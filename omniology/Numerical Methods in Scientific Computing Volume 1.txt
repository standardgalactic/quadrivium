
Numerical 
Methods
in Scientific 
Computing
Volume I


Numerical 
Methods
in Scientific 
Computing
Volume I
GERMUND DAHLQUIST
Royal Institute of Technology
Stockholm, Sweden
ÅKE BJÖRCK
Linköping University
Linköping, Sweden
Society for Industrial and Applied Mathematics • Philadelphia

Copyright © 2008 by the Society for Industrial and Applied Mathematics.
10 9 8 7 6 5 4 3 2 1
All rights reserved.  Printed in the United States of America.  No part of this book may be
reproduced, stored, or transmitted in any manner without the written permission of the 
publisher.  For information, write to the Society for Industrial and Applied Mathematics, 
3600 Market Street, 6th Floor, Philadelphia, PA 19104-2688 USA.
Trademarked names may be used in this book without the inclusion of a trademark symbol.
These names are used in an editorial context only; no infringement of trademark is intended.
Mathematica is a registered trademark of Wolfram Research, Inc.
MATLAB is a registered trademark of The MathWorks, Inc. For MATLAB product information,
please contact The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA 01760-2098 USA, 
508-647-7000, Fax: 508-647-7101, info@mathworks.com, www.mathworks.com.
Figure 4.5.2 originally appeared in Germund Dahlquist and Åke Björck. Numerical Methods.
Prentice-Hall, 1974. It appears here courtesy of the authors.
Library of Congress Cataloging-in-Publication Data
Dahlquist, Germund.
Numerical methods in scientific computing / Germund Dahlquist, Åke Björck.
p.cm.
Includes bibliographical references and index.
ISBN 978-0-898716-44-3 (v. 1 : alk. paper)
1.  Numerical analysis—Data processing.  I. Björck, Åke, 1934- II. Title.
QA297.D335 2008
518—dc22
2007061806
is a registered trademark.

To Marianne and Eva



Contents
List of Figures
xv
List of Tables
xix
List of Conventions
xxi
Preface
xxiii
1
Principles of Numerical Calculations
1
1.1 Common Ideas and Concepts . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1 Fixed-Point Iteration
. . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.3 Linearization and Extrapolation
. . . . . . . . . . . . . . . . . .
9
1.1.4 Finite Difference Approximations
. . . . . . . . . . . . . . . . .
11
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
15
1.2 Some Numerical Algorithms
. . . . . . . . . . . . . . . . . . . . . . .
16
1.2.1 Solving a Quadratic Equation . . . . . . . . . . . . . . . . . . . .
16
1.2.2 Recurrence Relations . . . . . . . . . . . . . . . . . . . . . . . .
17
1.2.3 Divide and Conquer Strategy . . . . . . . . . . . . . . . . . . . .
20
1.2.4 Power Series Expansions . . . . . . . . . . . . . . . . . . . . . .
22
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
23
1.3 Matrix Computations
. . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.3.1 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . .
26
1.3.2 Solving Linear Systems by LU Factorization . . . . . . . . . . . .
28
1.3.3 Sparse Matrices and Iterative Methods . . . . . . . . . . . . . . .
38
1.3.4 Software for Matrix Computations . . . . . . . . . . . . . . . . .
41
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
43
1.4 The Linear Least Squares Problem
. . . . . . . . . . . . . . . . . . . .
44
1.4.1 Basic Concepts in Probability and Statistics . . . . . . . . . . . .
45
1.4.2 Characterization of Least Squares Solutions . . . . . . . . . . . .
46
1.4.3 The Singular Value Decomposition . . . . . . . . . . . . . . . . .
50
1.4.4 The Numerical Rank of a Matrix . . . . . . . . . . . . . . . . . .
52
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
vii

viii
Contents
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
54
1.5 Numerical Solution of Differential Equations . . . . . . . . . . . . . . .
55
1.5.1 Euler’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
1.5.2 An Introductory Example . . . . . . . . . . . . . . . . . . . . . .
56
1.5.3 Second Order Accurate Methods . . . . . . . . . . . . . . . . . .
59
1.5.4 Adaptive Choice of Step Size . . . . . . . . . . . . . . . . . . . .
61
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
63
1.6 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
1.6.1 Origin of Monte Carlo Methods
. . . . . . . . . . . . . . . . . .
64
1.6.2 Generating and Testing Pseudorandom Numbers . . . . . . . . . .
66
1.6.3 Random Deviates for Other Distributions
. . . . . . . . . . . . .
73
1.6.4 Reduction of Variance . . . . . . . . . . . . . . . . . . . . . . . .
77
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
82
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
2
How to Obtain and Estimate Accuracy
87
2.1 Basic Concepts in Error Estimation . . . . . . . . . . . . . . . . . . . .
87
2.1.1 Sources of Error . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
2.1.2 Absolute and Relative Errors . . . . . . . . . . . . . . . . . . . .
90
2.1.3 Rounding and Chopping
. . . . . . . . . . . . . . . . . . . . . .
91
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.2 Computer Number Systems . . . . . . . . . . . . . . . . . . . . . . . .
93
2.2.1 The Position System
. . . . . . . . . . . . . . . . . . . . . . . .
93
2.2.2 Fixed- and Floating-Point Representation
. . . . . . . . . . . . .
95
2.2.3 IEEE Floating-Point Standard
. . . . . . . . . . . . . . . . . . .
99
2.2.4 Elementary Functions . . . . . . . . . . . . . . . . . . . . . . . .
102
2.2.5 Multiple Precision Arithmetic
. . . . . . . . . . . . . . . . . . .
104
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
105
2.3 Accuracy and Rounding Errors . . . . . . . . . . . . . . . . . . . . . .
107
2.3.1 Floating-Point Arithmetic . . . . . . . . . . . . . . . . . . . . . .
107
2.3.2 Basic Rounding Error Results
. . . . . . . . . . . . . . . . . . .
113
2.3.3 Statistical Models for Rounding Errors . . . . . . . . . . . . . . .
116
2.3.4 Avoiding Overﬂow and Cancellation . . . . . . . . . . . . . . . .
118
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
122
2.4 Error Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
2.4.1 Numerical Problems, Methods, and Algorithms . . . . . . . . . .
126
2.4.2 Propagation of Errors and Condition Numbers . . . . . . . . . . .
127
2.4.3 Perturbation Analysis for Linear Systems
. . . . . . . . . . . . .
134
2.4.4 Error Analysis and Stability of Algorithms . . . . . . . . . . . . .
137
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
142

Contents
ix
2.5 Automatic Control of Accuracy and Veriﬁed Computing . . . . . . . . .
145
2.5.1 Running Error Analysis . . . . . . . . . . . . . . . . . . . . . . .
145
2.5.2 Experimental Perturbations . . . . . . . . . . . . . . . . . . . . .
146
2.5.3 Interval Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . .
147
2.5.4 Range of Functions . . . . . . . . . . . . . . . . . . . . . . . . .
150
2.5.5 Interval Matrix Computations
. . . . . . . . . . . . . . . . . . .
153
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
155
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
3
Series, Operators, and Continued Fractions
157
3.1 Some Basic Facts about Series
. . . . . . . . . . . . . . . . . . . . . .
157
3.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
3.1.2 Taylor’s Formula and Power Series . . . . . . . . . . . . . . . . .
162
3.1.3 Analytic Continuation . . . . . . . . . . . . . . . . . . . . . . . .
171
3.1.4 Manipulating Power Series . . . . . . . . . . . . . . . . . . . . .
173
3.1.5 Formal Power Series . . . . . . . . . . . . . . . . . . . . . . . .
181
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
185
3.2 More about Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
3.2.1 Laurent and Fourier Series . . . . . . . . . . . . . . . . . . . . .
191
3.2.2 The Cauchy–FFT Method . . . . . . . . . . . . . . . . . . . . . .
193
3.2.3 Chebyshev Expansions . . . . . . . . . . . . . . . . . . . . . . .
198
3.2.4 Perturbation Expansions
. . . . . . . . . . . . . . . . . . . . . .
203
3.2.5 Ill-Conditioned Series . . . . . . . . . . . . . . . . . . . . . . . .
206
3.2.6 Divergent or Semiconvergent Series . . . . . . . . . . . . . . . .
212
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
215
3.3 Difference Operators and Operator Expansions . . . . . . . . . . . . . .
220
3.3.1 Properties of Difference Operators . . . . . . . . . . . . . . . . .
220
3.3.2 The Calculus of Operators
. . . . . . . . . . . . . . . . . . . . .
225
3.3.3 The Peano Theorem . . . . . . . . . . . . . . . . . . . . . . . . .
237
3.3.4 Approximation Formulas by Operator Methods . . . . . . . . . .
242
3.3.5 Single Linear Difference Equations . . . . . . . . . . . . . . . . .
251
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
261
3.4 Acceleration of Convergence . . . . . . . . . . . . . . . . . . . . . . .
271
3.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
3.4.2 Comparison Series and Aitken Acceleration . . . . . . . . . . . .
272
3.4.3 Euler’s Transformation . . . . . . . . . . . . . . . . . . . . . . .
278
3.4.4 Complete Monotonicity and Related Concepts . . . . . . . . . . .
284
3.4.5 Euler–Maclaurin’s Formula . . . . . . . . . . . . . . . . . . . . .
292
3.4.6 Repeated Richardson Extrapolation
. . . . . . . . . . . . . . . .
302
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
309

x
Contents
3.5 Continued Fractions and Padé Approximants . . . . . . . . . . . . . . .
321
3.5.1 Algebraic Continued Fractions . . . . . . . . . . . . . . . . . . .
321
3.5.2 Analytic Continued Fractions . . . . . . . . . . . . . . . . . . . .
326
3.5.3 The Padé Table . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
3.5.4 The Epsilon Algorithm . . . . . . . . . . . . . . . . . . . . . . .
336
3.5.5 The qd Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .
339
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
345
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
4
Interpolation and Approximation
351
4.1 The Interpolation Problem . . . . . . . . . . . . . . . . . . . . . . . . .
351
4.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
4.1.2 Bases for Polynomial Interpolation . . . . . . . . . . . . . . . . .
352
4.1.3 Conditioning of Polynomial Interpolation . . . . . . . . . . . . .
355
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
357
4.2 Interpolation Formulas and Algorithms . . . . . . . . . . . . . . . . . .
358
4.2.1 Newton’s Interpolation Formula . . . . . . . . . . . . . . . . . .
358
4.2.2 Inverse Interpolation
. . . . . . . . . . . . . . . . . . . . . . . .
366
4.2.3 Barycentric Lagrange Interpolation . . . . . . . . . . . . . . . . .
367
4.2.4 Iterative Linear Interpolation . . . . . . . . . . . . . . . . . . . .
371
4.2.5 Fast Algorithms for Vandermonde Systems
. . . . . . . . . . . .
373
4.2.6 The Runge Phenomenon
. . . . . . . . . . . . . . . . . . . . . .
377
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
380
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
380
4.3 Generalizations and Applications . . . . . . . . . . . . . . . . . . . . .
381
4.3.1 Hermite Interpolation . . . . . . . . . . . . . . . . . . . . . . . .
381
4.3.2 Complex Analysis in Polynomial Interpolation . . . . . . . . . . .
385
4.3.3 Rational Interpolation . . . . . . . . . . . . . . . . . . . . . . . .
389
4.3.4 Multidimensional Interpolation . . . . . . . . . . . . . . . . . . .
395
4.3.5 Analysis of a Generalized Runge Phenomenon . . . . . . . . . . .
398
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
407
4.4 Piecewise Polynomial Interpolation . . . . . . . . . . . . . . . . . . . .
410
4.4.1 Bernštein Polynomials and Bézier Curves . . . . . . . . . . . . .
411
4.4.2 Spline Functions
. . . . . . . . . . . . . . . . . . . . . . . . . .
417
4.4.3 The B-Spline Basis . . . . . . . . . . . . . . . . . . . . . . . . .
426
4.4.4 Least Squares Splines Approximation
. . . . . . . . . . . . . . .
434
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
437
4.5 Approximation and Function Spaces
. . . . . . . . . . . . . . . . . . .
439
4.5.1 Distance and Norm . . . . . . . . . . . . . . . . . . . . . . . . .
440
4.5.2 Operator Norms and the Distance Formula . . . . . . . . . . . . .
444
4.5.3 Inner Product Spaces and Orthogonal Systems . . . . . . . . . . .
450

Contents
xi
4.5.4 Solution of the Approximation Problem . . . . . . . . . . . . . .
454
4.5.5 Mathematical Properties of Orthogonal Polynomials
. . . . . . .
457
4.5.6 Expansions in Orthogonal Polynomials
. . . . . . . . . . . . . .
466
4.5.7 Approximation in the Maximum Norm . . . . . . . . . . . . . . .
471
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
479
4.6 Fourier Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
482
4.6.1 Basic Formulas and Theorems . . . . . . . . . . . . . . . . . . .
483
4.6.2 Discrete Fourier Analysis . . . . . . . . . . . . . . . . . . . . . .
487
4.6.3 Periodic Continuation of a Function . . . . . . . . . . . . . . . .
491
4.6.4 Convergence Acceleration of Fourier Series . . . . . . . . . . . .
492
4.6.5 The Fourier Integral Theorem
. . . . . . . . . . . . . . . . . . .
494
4.6.6 Sampled Data and Aliasing . . . . . . . . . . . . . . . . . . . . .
497
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
500
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
500
4.7 The Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . .
503
4.7.1 The FFT Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
503
4.7.2 Discrete Convolution by FFT . . . . . . . . . . . . . . . . . . . .
509
4.7.3 FFTs of Real Data . . . . . . . . . . . . . . . . . . . . . . . . . .
510
4.7.4 Fast Trigonometric Transforms . . . . . . . . . . . . . . . . . . .
512
4.7.5 The General Case FFT
. . . . . . . . . . . . . . . . . . . . . . .
515
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
517
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
518
5
Numerical Integration
521
5.1 Interpolatory Quadrature Rules . . . . . . . . . . . . . . . . . . . . . .
521
5.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
521
5.1.2 Treating Singularities . . . . . . . . . . . . . . . . . . . . . . . .
525
5.1.3 Some Classical Formulas . . . . . . . . . . . . . . . . . . . . . .
527
5.1.4 Superconvergence of the Trapezoidal Rule . . . . . . . . . . . . .
531
5.1.5 Higher-Order Newton–Cotes’ Formulas . . . . . . . . . . . . . .
533
5.1.6 Fejér and Clenshaw–Curtis Rules . . . . . . . . . . . . . . . . . .
538
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
542
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
542
5.2 Integration by Extrapolation . . . . . . . . . . . . . . . . . . . . . . . .
546
5.2.1 The Euler–Maclaurin Formula . . . . . . . . . . . . . . . . . . .
546
5.2.2 Romberg’s Method . . . . . . . . . . . . . . . . . . . . . . . . .
548
5.2.3 Oscillating Integrands . . . . . . . . . . . . . . . . . . . . . . . .
554
5.2.4 Adaptive Quadrature
. . . . . . . . . . . . . . . . . . . . . . . .
560
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
564
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
564
5.3 Quadrature Rules with Free Nodes . . . . . . . . . . . . . . . . . . . .
565
5.3.1 Method of Undetermined Coefﬁcients . . . . . . . . . . . . . . .
565
5.3.2 Gauss–Christoffel Quadrature Rules . . . . . . . . . . . . . . . .
568

xii
Contents
5.3.3 Gauss Quadrature with Preassigned Nodes . . . . . . . . . . . . .
573
5.3.4 Matrices, Moments, and Gauss Quadrature . . . . . . . . . . . . .
576
5.3.5 Jacobi Matrices and Gauss Quadrature . . . . . . . . . . . . . . .
580
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
585
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
585
5.4 Multidimensional Integration . . . . . . . . . . . . . . . . . . . . . . .
587
5.4.1 Analytic Techniques . . . . . . . . . . . . . . . . . . . . . . . . .
588
5.4.2 Repeated One-Dimensional Integration
. . . . . . . . . . . . . .
589
5.4.3 Product Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . .
590
5.4.4 Irregular Triangular Grids . . . . . . . . . . . . . . . . . . . . . .
594
5.4.5 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . .
599
5.4.6 Quasi–Monte Carlo and Lattice Methods . . . . . . . . . . . . . .
601
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
604
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
605
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
606
6
Solving Scalar Nonlinear Equations
609
6.1 Some Basic Concepts and Methods . . . . . . . . . . . . . . . . . . . .
609
6.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
609
6.1.2 The Bisection Method
. . . . . . . . . . . . . . . . . . . . . . .
610
6.1.3 Limiting Accuracy and Termination Criteria . . . . . . . . . . . .
614
6.1.4 Fixed-Point Iteration
. . . . . . . . . . . . . . . . . . . . . . . .
618
6.1.5 Convergence Order and Efﬁciency . . . . . . . . . . . . . . . . .
621
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
624
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
624
6.2 Methods Based on Interpolation . . . . . . . . . . . . . . . . . . . . . .
626
6.2.1 Method of False Position . . . . . . . . . . . . . . . . . . . . . .
626
6.2.2 The Secant Method . . . . . . . . . . . . . . . . . . . . . . . . .
628
6.2.3 Higher-Order Interpolation Methods . . . . . . . . . . . . . . . .
631
6.2.4 A Robust Hybrid Method . . . . . . . . . . . . . . . . . . . . . .
634
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
635
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
636
6.3 Methods Using Derivatives . . . . . . . . . . . . . . . . . . . . . . . .
637
6.3.1 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . .
637
6.3.2 Newton’s Method for Complex Roots
. . . . . . . . . . . . . . .
644
6.3.3 An Interval Newton Method
. . . . . . . . . . . . . . . . . . . .
646
6.3.4 Higher-Order Methods . . . . . . . . . . . . . . . . . . . . . . .
647
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
652
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
653
6.4 Finding a Minimum of a Function . . . . . . . . . . . . . . . . . . . . .
656
6.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
656
6.4.2 Unimodal Functions and Golden Section Search . . . . . . . . . .
657
6.4.3 Minimization by Interpolation
. . . . . . . . . . . . . . . . . . .
660
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
661
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
661

Contents
xiii
6.5 Algebraic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
662
6.5.1 Some Elementary Results . . . . . . . . . . . . . . . . . . . . . .
662
6.5.2 Ill-Conditioned Algebraic Equations . . . . . . . . . . . . . . . .
665
6.5.3 Three Classical Methods . . . . . . . . . . . . . . . . . . . . . .
668
6.5.4 Deﬂation and Simultaneous Determination of Roots . . . . . . . .
671
6.5.5 A Modiﬁed Newton Method
. . . . . . . . . . . . . . . . . . . .
675
6.5.6 Sturm Sequences . . . . . . . . . . . . . . . . . . . . . . . . . .
677
6.5.7 Finding Greatest Common Divisors
. . . . . . . . . . . . . . . .
680
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
682
Problems and Computer Exercises . . . . . . . . . . . . . . . . . . . . . . .
683
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
685
Bibliography
687
Index
707
A
Online Appendix: Introduction to Matrix Computations
A-1
A.1 Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A-1
A.1.1 Linear Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . .
A-1
A.1.2 Matrix and Vector Algebra . . . . . . . . . . . . . . . . . . . . .
A-3
A.1.3 Rank and Linear Systems . . . . . . . . . . . . . . . . . . . . . .
A-5
A.1.4 Special Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . .
A-6
A.2 Submatrices and Block Matrices
. . . . . . . . . . . . . . . . . . . . .
A-8
A.2.1 Block Gaussian Elimination
. . . . . . . . . . . . . . . . . . . . A-10
A.3 Permutations and Determinants . . . . . . . . . . . . . . . . . . . . . . A-12
A.4 Eigenvalues and Norms of Matrices . . . . . . . . . . . . . . . . . . . . A-16
A.4.1 The Characteristic Equation
. . . . . . . . . . . . . . . . . . . . A-16
A.4.2 The Schur and Jordan Normal Forms . . . . . . . . . . . . . . . . A-17
A.4.3 Norms of Vectors and Matrices . . . . . . . . . . . . . . . . . . . A-18
Review Questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-21
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-22
B
Online Appendix: A MATLAB Multiple Precision Package
B-1
B.1 The Mulprec Package . . . . . . . . . . . . . . . . . . . . . . . . . . .
B-1
B.1.1 Number Representation . . . . . . . . . . . . . . . . . . . . . . .
B-1
B.1.2 The Mulprec Function Library . . . . . . . . . . . . . . . . . . .
B-3
B.1.3 Basic Arithmetic Operations . . . . . . . . . . . . . . . . . . . .
B-3
B.1.4 Special Mulprec Operations
. . . . . . . . . . . . . . . . . . . .
B-4
B.2 Function and Vector Algorithms . . . . . . . . . . . . . . . . . . . . . .
B-4
B.2.1 Elementary Functions . . . . . . . . . . . . . . . . . . . . . . . .
B-4
B.2.2 Mulprec Vector Algorithms . . . . . . . . . . . . . . . . . . . . .
B-5
B.2.3 Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B-6
B.2.4 Using Mulprec
. . . . . . . . . . . . . . . . . . . . . . . . . . .
B-6
Computer Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B-6

xiv
Contents
C
Online Appendix: Guide to Literature
C-1
C.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C-1
C.2 Textbooks in Numerical Analysis . . . . . . . . . . . . . . . . . . . . .
C-1
C.3 Handbooks and Collections . . . . . . . . . . . . . . . . . . . . . . . .
C-5
C.4 Encyclopedias, Tables, and Formulas . . . . . . . . . . . . . . . . . . .
C-6
C.5 Selected Journals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C-8
C.6 Algorithms and Software
. . . . . . . . . . . . . . . . . . . . . . . . .
C-9
C.7 Public Domain Software . . . . . . . . . . . . . . . . . . . . . . . . . . C-10

List of Figures
1.1.1
Geometric interpretation of iteration xn+1 = F(xn). . . . . . . . . . . .
3
1.1.2
The ﬁxed-point iteration xn+1 = (xn + c/xn)/2, c = 2, x0 = 0.75. . . .
4
1.1.3
Geometric interpretation of Newton’s method.
. . . . . . . . . . . . .
7
1.1.4
Geometric interpretation of the secant method. . . . . . . . . . . . . .
8
1.1.5
Numerical integration by the trapezoidal rule (n = 4). . . . . . . . . .
10
1.1.6
Centered ﬁnite difference quotient.
. . . . . . . . . . . . . . . . . . .
11
1.3.1
Nonzero pattern of a sparse matrix from an eight stage chemical distilla-
tion column.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
1.3.2
Nonzero structure of the matrix A (left) and L + U (right). . . . . . . .
39
1.3.3
Structure of the matrix A (left) and L+U (right) for the Poisson problem,
N = 20 (rowwise ordering of the unknowns). . . . . . . . . . . . . . .
41
1.4.1
Geometric characterization of the least squares solution. . . . . . . . .
48
1.4.2
Singular values of a numerically singular matrix. . . . . . . . . . . . .
53
1.5.1
Approximate solution of the differential equation dy/dt = y, y0 = 0.25,
by Euler’s method with h = 0.5. . . . . . . . . . . . . . . . . . . . . .
56
1.5.2
Approximate trajectories computed with Euler’s method with h = 0.01.
58
1.6.1
Neutron scattering. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
1.6.2
Plots of pairs of 106 random uniform deviates (Ui, Ui+1) such that Ui <
0.0001. Left: MATLAB 4; Right: MATLAB 5. . . . . . . . . . . . . .
71
1.6.3
Random number with distribution F(x). . . . . . . . . . . . . . . . . .
74
1.6.4
Simulated two-dimensional Brownian motion. Plotted are 32 simulated
paths with h = 0.1, each consisting of 64 steps. . . . . . . . . . . . . .
76
1.6.5
The left part shows how the estimate of π varies with the number of
throws. The right part compares |m/n−2/π| with the standard deviation
of m/n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
1.6.6
Mean waiting times for doctor and patients at polyclinic. . . . . . . . .
81
2.2.1
Positive normalized numbers when β = 2, t = 3, and −1 ≤e ≤2.
. .
97
2.2.2
Positive normalized and denormalized numbers when β = 2, t = 3, and
−1 ≤e ≤2.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
2.3.1
Computed values for n = 10p, p = 1 : 14, of |(1 + 1/n)n −e| and
| exp(n log(1 + 1/n)) −e|. . . . . . . . . . . . . . . . . . . . . . . . .
110
2.3.2
Calculated values of a polynomial near a multiple root. . . . . . . . . .
117
2.3.3
The frequency function of the normal distribution for σ = 1. . . . . . .
118
xv

xvi
List of Figures
2.4.1
Geometrical illustration of the condition number. . . . . . . . . . . . .
134
2.5.1
Wrapping effect in interval analysis. . . . . . . . . . . . . . . . . . . .
152
3.1.1
Comparison of a series with an integral, (n = 5). . . . . . . . . . . . .
160
3.1.2
A series where Rn and Rn+1 have different signs. . . . . . . . . . . . .
161
3.1.3
Successive sums of an alternating series.
. . . . . . . . . . . . . . . .
161
3.1.4
Partial sums of the Maclaurin expansions for two functions. The upper
curves are for cos x, n = 0 : 2 : 26, 0 ≤x ≤10. The lower curves are
for 1/(1 + x2), n = 0 : 2 : 18, 0 ≤x ≤1.5. . . . . . . . . . . . . . . .
163
3.1.5
Relative error in approximations of the error function by a Maclaurin
series truncated after the ﬁrst term that satisﬁes the condition in (3.1.11).
165
3.2.1
Graph of the Chebyshev polynomial T20(x), x ∈[−1, 1]. . . . . . . . .
200
3.2.2
Example 3.2.7(A): Terms of (3.2.33), cn = (n + 1)−2, x = 40, no
preconditioner. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
3.2.3
Example 3.2.7(B): cn = (n + 1)−2, x = 40, with preconditioner
in (3.2.36). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
3.2.4
Error estimates of the semiconvergent series of Example 3.2.9 for x = 10;
see (3.2.43).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
3.2.5
The error of the expansion of f (x) = 1/(1 + x2) in a sum of Chebyshev
polynomials {Tn(x/1.5)}, n ≤12. . . . . . . . . . . . . . . . . . . . .
216
3.3.1
Bounds for truncation error RT and roundoff error RXF in numerical
differentiation as functions of h (U = 0.5 · 10−6). . . . . . . . . . . . .
248
3.4.1
Logarithms of the actual errors and the error estimates for MN,k in a
more extensive computation for the alternating series in (3.4.12) with
completely monotonic terms. The tolerance is here set above the level
where the irregular errors become important; for a smaller tolerance parts
of the lowest curves may become less smooth in some parts. . . . . . .
283
3.5.1
Best rational approximations {(p, q)} to the “golden ratio.” . . . . . . .
325
4.2.1
Error of interpolation in Pn for f (x) = xn, using n = 12: Chebyshev
points (solid line) and equidistant points (dashed line). . . . . . . . . .
378
4.2.2
Polynomial interpolation of 1/(1 + 25x2) in two ways using 11 points:
equidistant points (dashed curve), Chebyshev abscissae (dashed-dotted
curve).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
4.3.1
ℜψ(u), u ∈[−1, 1], for the processes based on equidistant nodes and
on Chebyshev nodes. For Chebyshev nodes, the convergence properties
are the same over the whole interval [−1, 1], because ℜψ(u) = −ln 2 =
−0.693 for all u ∈[−1, 1]. For equidistant nodes, the curve, which is
based on (4.3.61), partly explains why there are functions f such that the
interpolation process diverges fast in the outer parts of [−1, 1] and con-
verges fast in the central parts (often faster than Chebyshev interpolation). 400
4.3.2
Some level curves of the logarithmic potential for w(t) ≡1
2, t ∈[−1, 1].
Due to the symmetry only one quarter of each curve is shown. The value
of ℜψ(z) on a curve is seen to the left close to the curve. It is explained
in Example 4.3.11 how the curves have been computed.
. . . . . . . .
402

List of Figures
xvii
4.3.3
log10 |(f −Lnf )(u)| for Runge’s classical example f (u) = 1/(1+25u2)
with 30 equidistant nodes in [−1, 1].
The oscillating curves are the
empirical interpolation errors (observed at 300 equidistant points), for
u = x in the lower curve and for u = x+0.02i in the upper curve; in both
cases x ∈[−1, 1]. The smooth curves are the estimates of these quantities
obtained by the logarithmic potential model; see Examples 4.3.10 and
4.3.11.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
4.3.4
Some level curves of the logarithmic potential associated with Chebyshev
interpolation. They are ellipses with foci at ±1. Due to the symmetry
only a quarter is shown of each curve. The value of ℜψ(z) for a curve is
seen to the left, close to the curve. . . . . . . . . . . . . . . . . . . . .
404
4.4.1
The four cubic Bernštein polynomials. . . . . . . . . . . . . . . . . . .
412
4.4.2
Quadratic Bézier curve with control points. . . . . . . . . . . . . . . .
414
4.4.3
Cubic Bézier curve with control points p0, . . . , p3. . . . . . . . . . . .
414
4.4.4
De Casteljau’s algorithm for n = 2, t = 1
2.
. . . . . . . . . . . . . . .
416
4.4.5
A drafting spline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
4.4.6
Broken line and cubic spline interpolation.
. . . . . . . . . . . . . . .
419
4.4.7
Boundary slope errors eB,i for a cubic spline, e0 = em = −1; m = 20. .
426
4.4.8
Formation of a double knot for a linear spline.
. . . . . . . . . . . . .
428
4.4.9
B-splines of order k = 1, 2, 3. . . . . . . . . . . . . . . . . . . . . . .
429
4.4.10 The four cubic B-splines nonzero for x ∈(t0, t1) with coalescing exterior
knots t−3 = t−2 = t−1 = t0.
. . . . . . . . . . . . . . . . . . . . . . .
430
4.4.11
Banded structure of the matrices A and AT A arising in cubic spline ap-
proximation with B-splines (nonzero elements shown). . . . . . . . . .
435
4.4.12 Least squares cubic spline approximation of titanium data using 17 knots
marked on the axes by an “o.” . . . . . . . . . . . . . . . . . . . . . .
436
4.5.1
The Legendre polynomial P21. . . . . . . . . . . . . . . . . . . . . . .
463
4.5.2
Magnitude of coefﬁcients ci in a Chebyshev expansion of an analytic
function contaminated with roundoff noise. . . . . . . . . . . . . . . .
472
4.5.3
Linear uniform approximation. . . . . . . . . . . . . . . . . . . . . . .
475
4.6.1
A rectangular wave.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
486
4.6.2
Illustration of Gibbs’ phenomenon.
. . . . . . . . . . . . . . . . . . .
487
4.6.3
Periodic continuation of a function f outside [0, π] as an odd function.
491
4.6.4
The real (top) and imaginary (bottom) parts of the Fourier transform (solid
line) of e−x and the corresponding DFT (dots) with N = 32, T = 8. . .
499
5.1.1
The coefﬁcients |am,j| of the δ2-expansion for m = 2 : 2 : 14, j = 0 :
20. The circles are the coefﬁcients for the closed Cotes’ formulas, i.e.,
j = 1 + m/2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
5.2.1
The Filon-trapezoidal rule applied to the Fourier integral with f (x) = ex,
for h = 1/10, and ω = 1 : 1000; solid line: exact integral; dashed line:
absolute value of the error. . . . . . . . . . . . . . . . . . . . . . . . .
558
5.2.2
The oscillating function x−1 cos(x−1 ln x).
. . . . . . . . . . . . . . .
559
5.2.3
A needle-shaped function. . . . . . . . . . . . . . . . . . . . . . . . .
561
5.4.1
Region D of integration. . . . . . . . . . . . . . . . . . . . . . . . . .
590
5.4.2
A seven-point O(h6) rule for a circle. . . . . . . . . . . . . . . . . . .
593

xviii
List of Figures
5.4.3
Reﬁnement of a triangular grid.
. . . . . . . . . . . . . . . . . . . . .
594
5.4.4
Barycentric coordinates of a triangle.
. . . . . . . . . . . . . . . . . .
595
5.4.5
Correction for curved boundary segment. . . . . . . . . . . . . . . . .
598
5.4.6
The grids for I4 and I16.
. . . . . . . . . . . . . . . . . . . . . . . . .
599
5.4.7
Hammersley points in [0, 1]2.
. . . . . . . . . . . . . . . . . . . . . .
603
6.1.1
Graph of curves y = (x/2)2 and sin x. . . . . . . . . . . . . . . . . . .
611
6.1.2
The bisection method.
. . . . . . . . . . . . . . . . . . . . . . . . . .
612
6.1.3
Limited-precision approximation of a continuous function. . . . . . . .
615
6.1.4
The ﬁxed-point iteration xk+1 = e−xk, x0 = 0.3. . . . . . . . . . . . . .
619
6.2.1
The false-position method. . . . . . . . . . . . . . . . . . . . . . . . .
627
6.2.2
The secant method. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
628
6.3.1
The function x = u ln u. . . . . . . . . . . . . . . . . . . . . . . . . .
642
6.3.2
A situation where Newton’s method converges from any x0 ∈[a, b]. . .
643
6.4.1
One step of interval reduction, g(ck) ≥g(dk). . . . . . . . . . . . . . .
658
6.5.1
Suspect squares computed by Weyl’s quadtree method. Their centers
(marked by ×) approximate the ﬁve zeros marked by ∗.
. . . . . . . .
680

List of Tables
1.6.1
Simulation of waiting times for patients at a polyclinic. . . . . . . . . .
80
2.2.1
IEEE ﬂoating-point formats. . . . . . . . . . . . . . . . . . . . . . . .
100
2.2.2
IEEE 754 representation. . . . . . . . . . . . . . . . . . . . . . . . . .
101
2.4.1
Condition numbers of Hilbert matrices of order ≤12.
. . . . . . . . .
135
3.1.1
Maclaurin expansions for some elementary functions.
. . . . . . . . .
167
3.2.1
Results of three ways to compute
F(x) = (1/x)
 x
0 (1/t)(1 −e−t) dt. . . . . . . . . . . . . . . . . . . .
207
3.2.2
Evaluation of some Bessel functions.
. . . . . . . . . . . . . . . . . .
211
3.3.1
Bickley’s table of relations between difference operators. . . . . . . . .
231
3.3.2
Integrating y′′ = −y, y(0) = 0, y′(0) = 1; the letters U and S in the
headings of the last two columns refer to “Unstable” and “Stable.” . . .
258
3.4.1
Summation by repeated averaging. . . . . . . . . . . . . . . . . . . . .
278
3.4.2
Bernoulli and Euler numbers; B1 = −1/2, E1 = 1. . . . . . . . . . . .
294
4.5.1
Weight functions and recurrence coefﬁcients for some classical monic
orthogonal polynomials. . . . . . . . . . . . . . . . . . . . . . . . . .
465
4.6.1
Useful symmetry properties of the continuous Fourier transform. . . . .
495
4.7.1
Useful symmetry properties of the DFT. . . . . . . . . . . . . . . . . .
511
5.1.1
The coefﬁcients wi = Aci in the n-point closed Newton–Cotes’ formulas. 534
5.1.2
The coefﬁcients wi = Aci in the n-point open Newton–Cotes’ formulas.
535
5.3.1
Abscissae and weight factors for some Gauss–Legendre quadrature from
[1, Table 25.4]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
572
6.5.1
The qd scheme for computing the zeros of Ly(z). . . . . . . . . . . . .
674
6.5.2
Left: Sign variations in the Sturm sequence. Right: Intervals [lk, uk]
containing the zero xk. . . . . . . . . . . . . . . . . . . . . . . . . . .
679
xix


List of Conventions
Besides the generally accepted mathematical abbreviations and notations (see, e.g., James
and James, Mathematics Dictionary [1985, pp. 467–471]), the following notations are used
in the book:
MATLAB® has been used for this book in testing algorithms. We also use its notations
for array operations and the convenient colon notation.
.∗
A . ∗B element-by-element product A(i, j)B(i, j)
./
A ./B element-by-element division A(i, j)/B(i, j)
i : k
same as i, i + 1, . . . , k and empty if i > k
i : j : k
same as i, i + j, i + 2j, . . . , k
A(:, k), A(i, :)
the kth column, ith row of A, respectively
A(i : k)
same as A(i), A(i + 1), . . . , A(k)
⌊x⌋
ﬂoor, i.e., the largest integer ≤x
⌈x⌉
roof, i.e., the smallest integer ≥x
ex and exp(x)
both denote the exponential function
ﬂ(x + y)
ﬂoating-point operations; see Sec. 2.2.3
{xi}n
i=0
denotes the set {x0, x1, . . . , xn}
[a, b]
closed interval (a ≤x ≤b)
(a, b)
open interval (a < x < b)
sign (x)
+1 if x ≥0, else −1
int (a, b, c, . . . , w)
the smallest interval which contains a, b, c, . . . , w
f (x) = O(g(x)), x →a
|f (x)/g(x)| is bounded as x →a
(a can be ﬁnite, +∞, or −∞)
f (x) = o(g(x)), x →a
limx→a f (x)/g(x) = 0
f (x) ∼g(x), x →a
limx→a f (x)/g(x) = 1
k ≤i, j ≤n
means k ≤i ≤n and k ≤j ≤n
Pk
the set of polynomials of degree less than k
(f, g)
scalar product of functions f and g
∥· ∥p
p-norm in a linear vector or function space;
see Sec. 4.5.1–4.5.3 and Sec. A.3.3 in
Online Appendix A
En(f )
dist(f, Pn)∞,[a,b]; see Deﬁnition 4.5.6
The notations a ≈b, a ⪅b, and a ⪆b are deﬁned in Sec. 2.1.2. Matrices and
vectors are generally denoted by Roman letters A and b. AT and bT denote the transpose
of the matrix A and the vector b, respectively. (A, B) means a partitioned matrix; see Sec.
A.2 in Online Appendix A. Notation for matrix computation can also be found in Online
Appendix A. Notations for differences and difference operators, e.g., 42yn, [x0, x1, x2]f ,
δ2y, are deﬁned in Chapters 3 and 4.
xxi


Preface
In 1974 the book by Dahlquist and Björck, Numerical Methods, was published in the
Prentice–Hall Series in Automatic Computation, edited by George Forsythe. It was an
extended and updated English translation of a Swedish undergraduate textbook used at the
Royal Institute of Technology (KTH) in Stockholm. This book became one of the most
successful titles at Prentice–Hall. It was translated into several other languages and as late
as 1990 a Chinese edition appeared. It was reprinted in 2003 by Dover Publications.
In 1984 the authors were invited by Prentice–Hall to prepare a new edition of the
book. After some attempts it soon became apparent that, because of the rapid development
of the ﬁeld, one volume would no longer sufﬁce to cover the topics treated in the 1974 book.
Thus a large part of the new book would have to be written more or less from scratch. This
meant more work than we initially envisaged. Other commitments inevitably interfered,
sometimes for years, and the project was delayed. The present volume is the result of several
revisions worked out during the past 10 years.
Tragically, my mentor, friend, and coauthor Germund Dahlquist died on February 8,
2005, before this ﬁrst volume was ﬁnished. Fortunately the gaps left in his parts of the
manuscript were relatively few. Encouraged by his family, I decided to carry on and I have
tried to the best of my ability to ﬁll in the missing parts. I hope that I have managed to
convey some of his originality and enthusiasm for his subject. It was a great privilege for
me to work with him over many years. It is sad that he could never enjoy the fruits of his
labor on this book.
Today mathematics is used in one form or another within most areas of science and
industry. Although there has always been a close interaction between mathematics on the
one hand and science and technology on the other, there has been a tremendous increase in
the use of sophisticated mathematical models in the last decades. Advanced mathematical
models and methods are now also used more and more within areas such as medicine,
economics, and social sciences. Today, experiment and theory, the two classical elements
of the scientiﬁc method, are supplemented in many areas by computations that are an equally
important component.
The increased use of numerical methods has been caused not only by the continuing
advent of faster and larger computers. Gains in problem-solving capabilities through bet-
ter mathematical algorithms have played an equally important role. In modern scientiﬁc
computing one can now treat more complex and less simpliﬁed problems through massive
amounts of numerical calculations.
This volume is suitable for use in a basic introductory course in a graduate program in
numerical analysis. Although short introductions to numerical linear algebra and differential
xxiii

xxiv
Preface
equations are included, a more substantial treatment is deferred to later volumes. The book
can also be used as a reference for researchers in applied sciences working in scientiﬁc
computing. Much of the material in the book is derived from graduate courses given by
the ﬁrst author at KTH and Stanford University, and by the second author at Linköping
University, mainly during the 1980s and 90s.
We have aimed to make the book as self-contained as possible. The level of presenta-
tion ranges from elementary in the ﬁrst and second chapters to fairly sophisticated in some
later parts. For most parts the necessary prerequisites are calculus and linear algebra. For
some of the more advanced sections some knowledge of complex analysis and functional
analysis is helpful, although all concepts used are explained.
The choice of topics inevitably reﬂects our own interests. We have included many
methods that are important in large-scale computing and the design of algorithms. But
the emphasis is on traditional and well-developed topics in numerical analysis. Obvious
omissions in the book are wavelets and radial basis functions. Our experience from the
1974 book showed us that the most up-to-date topics are the ﬁrst to become out of date.
Chapter 1 is on a more elementary level than the rest of the book. It is used to introduce
a few general and powerful concepts and ideas that will be used repeatedly. An introduction
is given to some basic methods in the numerical solution of linear equations and least
squares problems, including the important singular value decomposition. Basic techniques
for the numerical solution of initial value problems for ordinary differential equations is
illustrated. An introduction to Monte Carlo methods, including a survey of pseudorandom
number generators and variance reduction techniques, ends this chapter.
Chapter 2 treats ﬂoating-point number systems and estimation and control of errors. It
is modeled after the same chapter in the 1974 book, but the IEEE ﬂoating-point standard has
made possible a much more satisfactory treatment. We are aware of the fact that this aspect
of computing is considered by many to be boring. But when things go wrong (and they
do!), then some understanding of ﬂoating-point arithmetic and condition numbers may be
essential. A new feature is a section on interval arithmetic, a topic which recently has seen
a revival, partly because the directed rounding incorporated in the IEEE standard simpliﬁes
the efﬁcient implementation.
In Chapter 3 different uses of inﬁnite power series for numerical computations are
studied, including ill-conditioned and semiconvergent series. Various algorithms for com-
puting the coefﬁcients of power series are given. Formal power series are introduced and
their convenient manipulation using triangular Toeplitz matrices is described.
Difference operators are handy tools for the derivation, analysis, and practical ap-
plication of numerical methods for many tasks such as interpolation, differentiation, and
quadrature. A more rigorous treatment of operator series expansions and the use of the
Cauchy formula and the fast Fourier transform (FFT) to derive the expansions are original
features of this part of Chapter 3.
Methods for convergence acceleration of series (sequences) are covered in detail.
For alternating series or series in a complex variable, Aitken extrapolation and Euler’s
transformationarethemostimportant. VariantsofAitken, Euler–Maclaurin, andRichardson
acceleration work for monotonic sequences. A partly new and more rigorous theoretical
analysis given for completely monotonic sequences reﬂects Dahlquist’s interest in analytic
function theory. Although not intended for the novice, this has been included partly because
it illustrates techniques that are of more general interest.

Preface
xxv
An exposition of continued fractions and Padé approximation, which transform a
(formal) power series into a sequence of rational functions, concludes this chapter. This
includes the ϵ-algorithm, the most important nonlinear convergence acceleration method,
as well as the qd algorithm.
Chapter 4 treats several topics related to interpolation and approximation. Different
bases for polynomial interpolation and related interpolation formulas are explained. The
advantages of the barycentric form of Lagrange interpolation formula are stressed. Complex
analysis is used to derive a general Lagrange–Hermite formula for polynomial interpolation
in the complex plane. Algorithms for rational and multidimensional interpolation are brieﬂy
surveyed.
Interpolation of an analytic function at an inﬁnite equidistant point set is treated from
the point of view of complex analysis. Applications made to the Runge phenomenon and
the Shannon sampling theorem. This section is more advanced than the rest of the chapter
and can be skipped in a ﬁrst reading.
Piecewise polynomials have become ubiquitous in computer aided design and com-
puter aided manufacturing. We describe how parametric Bézier curves are constructed from
piecewise Bernštein polynomials. A comprehensive treatment of splines is given and the
famous recurrence relation of de Boor and Cox for B-splines is derived. The use of B-splines
for representing curves and surfaces with given differentiability conditions is illustrated.
Function spaces are introduced in Chapter 4 and the concepts of linear operator and
operator norm are extended to general inﬁnite-dimensional vector spaces. The norm and
distance formula, which gives a convenient error bound for general approximation problems,
is presented. Inner product spaces, orthogonal systems, and the least squares approximation
problem are treated next. The importance of the three-term recurrence formula and the
Stieltjes procedure for numerical calculations is stressed. Chebyshev systems and theory
and algorithms for approximation in maximum norm are surveyed.
Basic formulas and theorems for Fourier series and Fourier transforms are discussed
next. Periodic continuation, sampled data and aliasing, and the Gibbs phenomenon are
treated. In applications such as digital signal and image processing, and time-series analysis,
the FFT algorithm (already used in Chapter 3) is an important tool. A separate section is
therefore devoted to a matrix-oriented treatment of the FFT, including fast trigonometric
transforms.
InChapter5theclassicalNewton–CotesrulesforequidistantnodesandtheClenshaw–
Curtis interpolatory rules for numerical integration are ﬁrst treated. Next, extrapolation
methods such as Romberg’s method and the use of the ϵ-algorithm are described. The
superconvergence of the trapezoidal rule in special cases and special Filon-type methods
for oscillating integrands are discussed. A short section on adaptive quadrature follows.
Quadrature rules with both free and prescribed nodes are important in many contexts.
A general technique of deriving formulas using the method of undetermined coefﬁcients is
given ﬁrst. Next, Gauss–Christoffel quadrature rules and their properties are treated, and
Gauss–Lobatto, Gauss–Radau, and Gauss–Kronrod rules are introduced. A more advanced
exposition of relations between moments, tridiagonal matrices, and Gauss quadrature is
included, but this part can be skipped at ﬁrst reading.
Product rules for multidimensional integration formulas use simple generalizations
of univariate rules and are applicable to rectangular domains. For more general domains,
integration using irregular triangular grids is more suitable. The basic linear and quadratic

xxvi
Preface
interpolation formulas on such grids are derived. Together with a simple correction for
curved boundaries these formulas are also very suitable for use in the ﬁnite element method.
Adiscussion of Monte Carlo and quasi–Monte Carlo methods and their advantages for high-
dimensional integration ends Chapter 5.
Chapter 6 starts with the bisection method. Next, ﬁxed-point iterations are introduced
and the contraction mapping theorem proved. Convergence order and the efﬁciency index
are discussed. Newton’s method is treated also for complex-valued equations and an interval
Newton method is described. Adiscussion of higher-order methods, including the Schröder
family of methods, is featured in this chapter.
Because of their importance for the matrix eigenproblem, algebraic equations are
treated at length. The frequent ill-conditioning of roots is illustrated. Several classical
methods are described, as well as an efﬁcient and robust modiﬁed Newton method due to
Madsen and Reid. Further, we describe the progressive qd algorithm and Sturm sequence
methods, both of which are also of interest for the tridiagonal eigenproblem.
Three Online Appendices are available from the Web page of the book, www.siam.
org/books/ot103. Appendix A is a compact survey of notations and some frequently
used results in numerical linear algebra. Volume II will contain a full treatment of these
topics. OnlineAppendix B describes Mulprec, a collection of MATLAB m-ﬁles for (almost)
unlimited high precision calculation. This package can also be downloaded from the Web
page. Online Appendix C is a more complete guide to literature, where advice is given on
not only general textbooks in numerical analysis but also handbooks, encyclopedias, tables,
software, and journals.
An important feature of the book is the large collection of problems and computer
exercises included. This draws from the authors’40+ year of experience in teaching courses
in numerical analysis. It is highly recommended that a modern interactive system such as
MATLAB is available to the reader for working out these assignments. The 1974 book also
contained answers and solutions to most problems. It has not been possible to retain this
feature because of the much greater number and complexity of the problems in the present
book.
We have aimed to make and the bibliography as comprehensive and up-to-date as
possible. A Notes and References section containing historical comments and additional
references concludes each chapter. To remind the reader of the fact that much of the the-
ory and many methods date one or several hundred years back in time, we have included
more than 60 short biographical notes on mathematicians who have made signiﬁcant con-
tributions. These notes would not have been possible without the invaluable use of the bi-
ographies compiled at the School of Mathematics and Statistics, University of St Andrews,
Scotland (www-history.mcs.st.andrews.ac.uk). Many of these full biographies are
fascinating to read.
IamverygratefulfortheencouragementreceivedfromMarianneandMartinDahlquist,
who graciously allowed me to access computer ﬁles from Germund Dahlquist’s personal
computer. Without their support the completion of this book would not have been possible.
Many people read early drafts at various stages of the evolution of this book and
contributed many corrections and constructive comments. I am particularly grateful to
Nick Higham, Lothar Reichel, Zdenek Strakos, and several anonymous referees whose
suggestions led to several major improvements. Other people who helped with proofreading
include Bo Einarsson, Tommy Elfving, Pablo Guerrero-Garcia, Sven-Åke Gustafsson, and

Preface
xxvii
Per Lötstedt. Thank you all for your interest in the book and for giving so much of your
valuable time!
The book was typeset in LATEX the references were prepared in BibTEX, and the index
with MakeIndex. These are all wonderful tools and my thanks goes to Donald Knuth for his
gift to mathematics. Thanks also to Cleve Moler for MATLAB, which was used in working
out examples and for generating ﬁgures.
It is a pleasure to thank Elizabeth Greenspan, Sarah Murphy, and other staff at SIAM
for their cheerful and professional support during all phases of the acquisition and production
of the book.
Åke Björck
Linköping, July 2007


Chapter 1
Principles of Numerical
Calculations
It is almost impossible to identify a mathematical
theory no matter how “pure,” that has never
inﬂuenced numerical reasoning.
—B. J. C. Baxter and Arieh Iserles
1.1
Common Ideas and Concepts
Although numerical mathematics has been used for centuries in one form or another within
many areas of science and industry,1 modern scientiﬁc computing using electronic comput-
ers has its origin in research and developments during the Second World War. In the late
1940s and early 1950s the foundation of numerical analysis was laid as a separate discipline
of mathematics. The new capability of performing billions of arithmetic operations cheaply
has led to new classes of algorithms, which need careful analysis to ensure their accuracy
and stability.
As a rule, applications lead to mathematical problems which in their complete form
cannot be conveniently solved with exact formulas, unless one restricts oneself to special
cases or simpliﬁed models. In many cases, one thereby reduces the problem to a sequence of
linear problems—for example, linear systems of differential equations. Such an approach
can quite often lead to concepts and points of view which, at least qualitatively, can be used
even in the unreduced problems.
Recent developments have enormously increased the scope for using numerical meth-
ods. Gains in problem solving capabilities mean that today one can treat much more complex
and less simpliﬁed problems through massive amounts of numerical calculations. This has
increased the interaction of mathematics with science and technology.
In most numerical methods, one applies a small number of general and relatively
simple ideas. These are then combined with one another in an inventive way and with such
1The Greek mathematician Archimedes (287–212 B.C.), Isaac Newton (1642–1727), and Carl Friedrich Gauss
(1777–1883) were pioneering contributors to numerical mathematics.
1

2
Chapter 1. Principles of Numerical Calculations
knowledge of the given problem as one can obtain in other ways—for example, with the
methods of mathematical analysis. Some knowledge of the background of the problem is
also of value; among other things, one should take into account the orders of magnitude of
certain numerical data of the problem.
In this chapter we shall illustrate the use of some general ideas behind numerical
methods on some simple problems. These may occur as subproblems or computational
details of larger problems, though as a rule they more often occur in a less pure form and
on a larger scale than they do here. When we present and analyze numerical methods,
to some degree we use the same approach which was ﬁrst mentioned above: we study in
detail special cases and simpliﬁed situations, with the aim of uncovering more generally
applicable concepts and points of view which can guide us in more difﬁcult problems.
It is important to keep in mind that the success of the methods presented depends on
the smoothness properties of the functions involved. In this ﬁrst survey we shall tacitly
assume that the functions have as many well-behaved derivatives as are needed.
1.1.1
Fixed-Point Iteration
One of the most frequently occurring ideas in numerical calculations is iteration (from
the Latin iterare, “to plow once again”) or successive approximation. Taken generally,
iteration means the repetition of a pattern of action or process. Iteration in this sense occurs,
for example, in the repeated application of a numerical process—perhaps very complicated
and itself containing many instances of the use of iteration in the somewhat narrower sense
to be described below—in order to improve previous results. To illustrate a more speciﬁc
use of the idea of iteration, we consider the problem of solving a (usually) nonlinear equation
of the form
x = F(x),
(1.1.1)
where F is assumed to be a differentiable function whose value can be computed for any
given value of a real variable x, within a certain interval. Using the method of iteration,
one starts with an initial approximation x0, and computes the sequence
x1 = F(x0),
x2 = F(x1),
x3 = F(x2), . . . .
(1.1.2)
Each computation of the type xn+1 = F(xn), n = 0, 1, 2, . . . , is called a ﬁxed-point
iteration. As n grows, we would like the numbers xn to be better and better estimates of
the desired root. If the sequence {xn} converges to a limiting value α, then we have
α = lim
n→∞xn+1 = lim
n→∞F(xn) = F(α),
and thus x = α satisﬁes the equation x = F(x). One can then stop the iterations when the
desired accuracy has been attained.
A geometric interpretation of ﬁxed point iteration is shown in Figure 1.1.1. A root of
(1.1.1) is given by the abscissa (and ordinate) of an intersecting point of the curve y = F(x)
and the line y = x. Starting from x0, the point x1 = F(x0) on the x-axis is obtained by ﬁrst
drawing a horizontal line from the point (x0, F(x0)) = (x0, x1) until it intersects the line
y = x in the point (x1, x1); from there we draw a vertical line to (x1, F(x1)) = (x1, x2),

1.1. Common Ideas and Concepts
3
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
x0
x1 x2
0 < F′(x) < 1
y = F(x)
y = x
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
x0
x1
x2
x3
x4
−1 < F′(x) < 0
y = F(x)
y = x
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
x0
x1
x2
x3
F′(x) > 1
y = F(x)
y = x
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
x0
x1
x2
x3
x4
F′(x) < −1
y = F(x)
y = x
(a)
(b)
(c)
(d)
Figure 1.1.1. Geometric interpretation of iteration xn+1 = F(xn).
and so on in a “staircase” pattern. In Figure 1.1.1(a) it is obvious that the sequence {xn}
converges monotonically to the root α. Figure 1.1.1(b) shows a case where F is a decreasing
function. There we also have convergence, but not monotone convergence; the successive
iterates xn lie alternately to the right and to the left of the root α. In this case the root is
bracketed by any two successive iterates.
There are also two divergent cases, exempliﬁed by Figures 1.1.1(c) and (d). One can
see geometrically that the quantity, which determines the rate of convergence (or diver-
gence), is the slope of the curve y = F(x) in the neighborhood of the root. Indeed, from
the mean value theorem of calculus we have
xn+1 −α
xn −α
= F(xn) −F(α)
xn −α
= F ′(ξn),
where ξn lies between xn and α. We see that if x0 is chosen sufﬁciently close to the root
(yet x0 ̸= α), the iteration will converge if |F ′(α)| < 1. In this case α is called a point of
attraction. The convergence is faster the smaller |F ′(α)| is. If |F ′(α)| > 1, then α is a
point of repulsion and the iteration diverges.

4
Chapter 1. Principles of Numerical Calculations
0.5
1
1.5
2
2.5
0.5
1
1.5
2
2.5
x0
x1
x2
Figure 1.1.2. The ﬁxed-point iteration xn+1 = (xn + c/xn)/2, c = 2, x0 = 0.75.
Example 1.1.1.
The square root of c > 0 satisﬁes the equation x2 = c, which can also be written
x = c/x or x = (x + c/x)12. This suggests the ﬁxed-point iteration
xn+1 = 1
2 (xn + c/xn) ,
n = 1, 2, . . . ,
(1.1.3)
which is the widely used Heron’s rule.2 The curve y = F(x) is in this case a hyperbola
(see Figure 1.1.2).
From (1.1.3) follows
xn+1 ± √c = 1
2

xn ± 2√c + c
xn

= (xn ± √c)2
2xn
;
that is,
xn+1 −√c
xn+1 + √c =
xn −√c
xn + √c
2
.
(1.1.4)
We can take en = xn−√c
xn+√c to be a measure of the error in xn. Then (1.1.4) reads en+1 = e2
n
and it follows that en = e2n
0 . If |x0 −√c| ̸= |x0 + √c|, then e0 < 1 and xn converges to a
square root of c when n →∞. Note that the iteration (1.1.3) can also be used for complex
values of c.
For c = 2 and x0 = 1.5, we get x1 = (1.5 + 2/1.5)12 = 15/12 = 1.4166666 . . . ,
and
x2 = 1.414215 686274,
x3 = 1.414213 562375
(correct digits shown in boldface). This can be compared with the exact value
√
2 =
1.414213 562373 . . . . As can be seen from Figure 1.1.2, a rough value for x0 sufﬁces. The
2Heron made important contributions to geometry and mechanics. He is believed to have lived in Alexandria,
Egypt, during the ﬁrst century A.D.

1.1. Common Ideas and Concepts
5
rapid convergence is due to the fact that for α = √c we have
F ′(α) = (1 −c/α2)/2 = 0.
One can in fact show that
lim
n→∞
|xn+1 −√c|
|xn −√c|2 = C
for some constant 0 < C < ∞, which is an example of what is known as quadratic
convergence. Roughly, if xn has t correct digits, then xn+1 will have at least 2t −1 correct
digits.
The above iteration method is used quite generally on both pocket calculators and
larger computers for calculating square roots.
Iteration is one of the most important aids for practical as well as theoretical treatment
of both linear and nonlinear problems. One very common application of iteration is to the
solution of systems of equations. In this case {xn} is a sequence of vectors, and F is a vector-
valued function. When iteration is applied to differential equations, {xn} means a sequence
of functions, and F(x) means an expression in which integration or other operations on
functions may be involved. A number of other variations on the very general idea of
iteration will be given in later chapters.
The form of (1.1.1) is frequently called the ﬁxed-point form, since the root α is a
ﬁxed point of the mapping F. An equation may not be given in this form originally. One
has a certain amount of choice in the rewriting of an equation f (x) = 0 in ﬁxed-point
form, and the rate of convergence depends very much on this choice. The equation x2 = c
can also be written, for example, as x = c/x. The iteration formula xn+1 = c/xn gives a
sequence which alternates between x0 (for even n) and c/x0 (for odd n)—the sequence does
not converge for any x0 ̸= √c!
1.1.2
Newton’s Method
Let an equation be given in the form f (x) = 0, and for any k ̸= 0, set
F(x) = x + kf (x).
Then the equation x = F(x) is equivalent to the equation f (x) = 0. Since F ′(α) =
1 + kf ′(α), we obtain the fastest convergence for k = −1/f ′(α). Because α is not known,
this cannot be applied literally. But if we use xn as an approximation, this leads to the choice
F(x) = x −f (x)/f ′(x), or the iteration
xn+1 = xn −f (xn)
f ′(xn).
(1.1.5)
This is the celebrated Newton’s method.3 We shall derive it in another way below.
3Isaac Newton (1642–1727), English mathematician, astronomer, and physicist invented inﬁnitesimal calculus
independently of the German mathematician and philosopher Gottfried W. von Leibniz (1646–1716).

6
Chapter 1. Principles of Numerical Calculations
The equation x2 = c can be written in the form f (x) = x2−c = 0. Newton’s method
for this equation becomes
xn+1 = xn −x2
n −c
2xn
= 1
2

xn + c
xn

,
n = 0, 1, 2, . . . ,
(1.1.6)
which is the fast method in Example 1.1.1. More generally, Newton’s method applied to
the equation f (x) = xp −c = 0 can be used to compute c1/p, p = ±1, ±2, . . . , from the
iteration
xn+1 = xn −xp
n −c
pxp−1
n
.
This can be written as
xn+1 = 1
p

(p −1)xn +
c
xp−1
n

=
xn
(−p)[(1 −p) −cx−p
n ].
(1.1.7)
It is convenient to use the ﬁrst expression in (1.1.7) when p > 0 and the second when
p < 0. With p = 2, 3, and −2, respectively, this iteration formula is used for calculating
√c,
3√c, and 1/√c. Also 1/c, (p = −1) can be computed by the iteration
xn+1 = xn + xn(1 −cxn) = xn(2 −cxn),
using only multiplication and addition. In some early computers, which lacked division in
hardware, this iteration was used to implement division, i.e., b/c was computed as b(1/c).
Example 1.1.2.
We want to construct an algorithm based on Newton’s method for the efﬁcient calcu-
lation of the square root of any given ﬂoating-point number a. If we ﬁrst shift the mantissa
so that the exponent becomes even, a = c · 22e, and 1/2 ≤c < 2; then
√a = √c· 2e.
We need only consider the reduced range 1/2 ≤c ≤1 since for 1 < c ≤2 we can compute
√1/c and invert.4
To ﬁnd an initial approximation x0 to start the Newton iterations when 1/2 ≤c < 1,
we can use linear interpolation of x = √c between the endpoints 1/2, 1, giving
x0(c) =
√
2(1 −c) + 2(c −1/2)
(
√
2 is precomputed). The iteration then proceeds using (1.1.6).
For c = 3/4 (√c = 0.86602540378444) the result is x0 = (
√
2 + 2)/4 and (correct
digits are in boldface)
x0 = 0.85355339059327,
x1 = 0.86611652351682,
x2 = 0.86602540857756,
x3 = 0.86602540378444,
4Since division is usually much slower than addition and multiplication, this may not be optimal.

1.1. Common Ideas and Concepts
7
The quadratic rate of convergence is apparent. Three iterations sufﬁce to give about 16
digits of accuracy for all x ∈[1/2, 1].
Newton’s method is based on linearization. This means that locally, i.e., in a small
neighborhood of a point, a more complicated function is approximated with a linear func-
tion. In the solution of the equation f (x) = 0, this means geometrically that we seek the
intersection point between the x-axis and the curve y = f (x); see Figure 1.1.3. Assume
x0
x1
x2
Figure 1.1.3. Geometric interpretation of Newton’s method.
that we have an approximating value x0 to the root. We then approximate the curve with its
tangent at the point (x0, f (x0)). Let x1 be the abscissa of the point of intersection between
the x-axis and the tangent. Since the equation for the tangent reads
y −f (x0) = f ′(x0)(x −x0),
by setting y = 0 we obtain the approximation
x1 = x0 −f (x0)/f ′(x0).
In many cases x1 will have about twice as many correct digits as x0. But if x0 is a poor
approximation and f (x) far from linear, then it is possible that x1 will be a worse approxi-
mation than x0.
If we combine the ideas of iteration and linearization, that is, substitute xn for x0 and
xn+1 for x1, we rediscover Newton’s method mentioned earlier. If x0 is close enough to α,
the iterations will converge rapidly (see Figure 1.1.3), but there are also cases of divergence.
An alternative to drawing the tangent to approximate a curve locally with a linear
function is to choose two neighboring points on the curve and to approximate the curve
with the secant which joins the two points; see Figure 1.1.4. The secant method for the
solutionofnonlinearequationsisbasedonthisapproximation. Thismethod, whichpreceded
Newton’s method, is discussed in more detail in Sec. 6.3.1.
Newton’s method can be generalized to yield a quadratically convergent method for
solving a system of nonlinear equations
fi(x1, x2, . . . , xn) = 0,
i = 1 : n.
(1.1.8)

8
Chapter 1. Principles of Numerical Calculations
x1
x0
x2
x3
Figure 1.1.4. Geometric interpretation of the secant method.
Such systems arise in many different contexts in scientiﬁc computing. Important examples
are the solution of systems of differential equations and optimization problems. We can
write (1.1.8) as f(x) = 0, where f and x are vectors in Rn. The vector-valued function f is
said to be differentiable at the point x if each component is differentiable with respect to all
the variables. The matrix of partial derivatives of f with respect to x,
J(x) = f′(x) =


∂f1
∂x1
. . .
∂f1
∂xn
...
...
∂fn
∂x1
. . .
∂fn
∂xn

∈Rn×n,
(1.1.9)
is called the Jacobian of f.
Let xk be the current approximate solution and assume that the matrix f′(xk) is nonsin-
gular. Then in Newton’s method for the system (1.1.8), the next iterate xn+1 is determined
from the unique solution to the system of linear equations
J(xk)(xn+1 −xk) = −f(xk).
(1.1.10)
The linear system (1.1.10) can be solved by computing the LU factorization of the matrix
J(x); see Sec. 1.3.2.
Each step of Newton’s method requires the evaluation of the n2 entries of the Jacobian
matrix J(xk). This may be a time consuming task if n is large. If either the iterates or
the Jacobian matrix are not changing too rapidly, it is possible to reevaluate J(xk) only
occasionally and use the same Jacobian in several steps. This has the further advantage that
once we have computed the LU factorization of the Jacobian matrix, the linear system can
be solved in only O(n2) arithmetic operations; see Sec. 1.3.2.
Example 1.1.3.
The following example illustrates the quadratic convergence of Newton’s method for
simple roots. The nonlinear system
x2 + y2 −4x = 0,
y2 + 2x −2 = 0

1.1. Common Ideas and Concepts
9
has a solution close to x0 = 0.5, y0 = 1. The Jacobian matrix is
J(x, y) =

2x −4
2y
2
2y

,
and Newton’s method becomes

xk+1
yk+1

=

xk
yk

−J(xk, yk)−1

x2
k + y2
k −4xn
y2
k + 2xk −2

.
We get the following results:
k
xk
yk
1
0.35
1.15
2
0.35424528301887
1.13652584085316
3
0.35424868893322
1.13644297217273
4
0.35424868893541
1.13644296914943
All digits are correct in the last iteration. The quadratic convergence is obvious; the number
of correct digits approximately doubles in each iteration.
Often, the main difﬁculty in solving a nonlinear system is to ﬁnd a sufﬁciently good
starting point for the Newton iterations. Techniques for modifying Newton’s method to
ensure global convergence are therefore important in several dimensions. These must
include techniques for coping with ill-conditioned or even singular Jacobian matrices at
intermediate points. Such techniques will be discussed in Volume II.
1.1.3
Linearization and Extrapolation
The secant approximation is useful in many other contexts; for instance, it is generally used
when one “reads between the lines” or interpolates in a table of numerical values. In this case
the secant approximation is called linear interpolation. When the secant approximation is
used in numerical integration, i.e., in the approximate calculation of a deﬁnite integral,
I =
 b
a
y(x) dx,
(1.1.11)
(see Figure 1.1.5) it is called the trapezoidal rule. With this method, the area between the
curve y = y(x) and the x-axis is approximated with the sum T (h) of the areas of a series
of parallel trapezoids. Using the notation of Figure 1.1.5, we have
T (h) = h1
2
n−1

i=0
(yi + yi+1),
h = b −a
n
.
(1.1.12)
(In the ﬁgure, n = 4.) We shall show in a later chapter that the error is very nearly
proportional to h2 when h is small.
One can then, in principle, attain arbitrarily high
accuracy by choosing h sufﬁciently small. But the computational work involved is roughly

10
Chapter 1. Principles of Numerical Calculations
a                                                                              b
y0
y1
y2
y3
y4
Figure 1.1.5. Numerical integration by the trapezoidal rule (n = 4).
proportional to the number of points where y(x) must be computed, and thus inversely
proportional to h. Hence the computational work grows rapidly as one demands higher
accuracy (smaller h).
Numerical integration is a fairly common problem because only seldom can the “prim-
itive” function be analytically calculated in a ﬁnite expression containing only elementary
functions. It is not possible for such simple functions as ex2 or (sin x)/x. In order to obtain
higher accuracy with signiﬁcantly less work than the trapezoidal rule requires, one can use
one of the following two important ideas:
(a) local approximation of the integrand with a polynomial of higher degree, or with a
function of some other class, for which one knows the primitive function;
(b) computation with the trapezoidal rule for several values of h and then extrapolation
to h = 0, the so-called Richardson extrapolation5 or deferred approach to the
limit, with the use of general results concerning the dependence of the error on h.
The technical details for the various ways of approximating a function with a poly-
nomial, including Taylor expansions, interpolation, and the method of least squares, are
treated in later chapters.
The extrapolation to the limit can easily be applied to numerical integration with the
trapezoidal rule. As was mentioned previously, the trapezoidal approximation (1.1.12) to
the integral has an error approximately proportional to the square of the step size. Thus,
using two step sizes, h and 2h, one has
T (h) −I ≈kh2,
T (2h) −I ≈k(2h)2,
and hence 4(T (h) −I) ≈T (2h) −I, from which it follows that
I ≈1
3(4T (h) −T (2h)) = T (h) + 1
3(T (h) −T (2h)).
5Lewis Fry Richardson (1881–1953) studied mathematics, physics, chemistry, botany, and zoology. He grad-
uated from King’s College, Cambridge in 1903. He was the ﬁrst (1922) to attempt to apply the method of ﬁnite
differences to weather prediction, long before the computer age!

1.1. Common Ideas and Concepts
11
Thus, by adding the corrective term 1
3(T (h) −T (2h)) to T (h), one should get an estimate
of I which is typically far more accurate than T (h). In Sec. 3.4.6 we shall see that the
improvement is in most cases quite striking. The result of the Richardson extrapolation
is in this case equivalent to the classical Simpson’s rule for numerical integration, which
we shall encounter many times in this volume. It can be derived in several different ways.
Section 3.4.5 also contains application of extrapolation to problems other than numerical
integration, as well as a further development of the extrapolation idea, namely repeated
Richardson extrapolation. In numerical integration this is also known as Romberg’s
method; see Sec. 5.2.2.
Knowledge of the behavior of the error can, together with the idea of extrapolation,
lead to a powerful method for improving results. Such a line of reasoning is useful not only
for the common problem of numerical integration, but also in many other types of problems.
Example 1.1.4.
The integral
 12
10
f (x) dx
is computed for f (x) = x3 by the trapezoidal method. With h = 1 we obtain T (h) = 2695,
T (2h) = 2728, and extrapolation gives T = 2684, equal to the exact result.
Similarly, for f (x) = x4 we obtain T (h) = 30,009, T (2h) = 30,736, and with
extrapolation T ≈29,766.7 (exact 29,766.4).
1.1.4
Finite Difference Approximations
The local approximation of a complicated function by a linear function leads to another
frequently encountered idea in the construction of numerical methods, namely the approx-
imation of a derivative by a difference quotient. Figure 1.1.6 shows the graph of a function
(n − 1)h
nh
(n + 1)h
yn−1
yn
yn+1
Figure 1.1.6. Centered ﬁnite difference quotient.

12
Chapter 1. Principles of Numerical Calculations
y(x) in the interval [xn−1, xn+1], where xn+1 −xn = xn −xn−1 = h; h is called the step
size. If we set yi = y(xi), i = n−1, n, n+1, then the derivative at xn can be approximated
by a forward difference quotient,
y′(xn) ≈yn+1 −yn
h
,
(1.1.13)
or a similar backward difference quotient involving yn and yn−1. The error in the approxi-
mation is called a discretization error.
But it is conceivable that the centered difference approximation
y′(xn) ≈yn+1 −yn−1
2h
(1.1.14)
usually will be more accurate. It is in fact easy to motivate this. By Taylor’s formula,
y(x + h) −y(x) = y′(x)h + y′′(x)h2/2 + y′′′(x)h3/6 + · · · ,
(1.1.15)
−y(x −h) + y(x) = y′(x)h −y′′(x)h2/2 + y′′′(x)h3/6 −· · · .
(1.1.16)
Set x = xn. Then, by the ﬁrst of these equations,
y′(xn) = yn+1 −yn
h
−h
2y′′(xn) −· · · .
Next, add the two Taylor expansions and divide by 2h. Then the ﬁrst error term cancels and
we have
y′(xn) = yn+1 −yn−1
2h
−h2
6 y′′′(xn) −· · · .
(1.1.17)
In what follows we call a formula (or a method), where a step size parameter h is involved,
accurate of order p, if its error is approximately proportional to hp. Since y′′(x) vanishes
for all x if and only if y is a linear function of x, and similarly, y′′′(x) vanishes for all x if
and only if y is a quadratic function, we have established the following important result.
Lemma 1.1.1.
The forward difference approximation (1.1.13) is exact only for a linear function, and
it is only ﬁrst order accurate in the general case. The centered difference approximation
(1.1.14) is exact also for a quadratic function, and is second order accurate in the general
case.
For the above reason the approximation (1.1.14) is, in most situations, preferable
to (1.1.13). But there are situations when these formulas are applied to the approximate
solution of differential equations where the forward difference approximation sufﬁces, but
where the centered difference quotient is entirely unusable, for reasons which have to do
with how errors are propagated to later stages in the calculation. We shall not examine this
phenomenon more closely here, but mention it only to intimate some of the surprising and
fascinating mathematical questions which can arise in the study of numerical methods.

1.1. Common Ideas and Concepts
13
Higher derivatives can be approximated with higher differences, that is, differences
of differences, another central concept in numerical calculations. We deﬁne
(4y)n = yn+1 −yn;
(42y)n = (4(4y))n = (yn+2 −yn+1) −(yn+1 −yn)
= yn+2 −2yn+1 + yn;
(43y)n = (4(42y))n = yn+3 −3yn+2 + 3yn+1 −yn;
etc. For simplicity one often omits the parentheses and writes, for example, 42y5 instead
of (42y)5. The coefﬁcients that appear here in the expressions for the higher differences
are, by the way, the binomial coefﬁcients. In addition, if we denote the step length by 4x
instead of by h, we get the following formulas, which are easily remembered:
dy
dx ≈4y
4x ,
d2y
dx2 ≈42y
(4x)2 ,
(1.1.18)
etc. Each of these approximations is second order accurate for the value of the derivative at
an x which equals the mean value of the largest and smallest x for which the corresponding
value of y is used in the computation of the difference. (The formulas are only ﬁrst order
accurate when regarded as approximations to derivatives at other points between these
bounds.) These statements can be established by arguments similar to the motivation for
(1.1.13) and (1.1.14).
Taking the difference of the Taylor expansions (1.1.15)–(1.1.16) with one more term
in each and dividing by h2, we obtain the following important formula:
y′′(xn) = yn+1 −2yn + yn−1
h2
−h2
12yiv(xn) −· · · .
Introducing the central difference operator
δyn = y

xn + 1
2h

−y

xn −1
2h

(1.1.19)
and neglecting higher order terms we get
y′′(xn) ≈1
h2 δ2yn −h2
12yiv(xn).
(1.1.20)
The approximation of (1.1.14) can be interpreted as an application of (1.1.18) with
4x = 2h, or as the mean of the estimates which one gets according to (1.1.18) for y′((n +
1
2)h) and y′((n −1
2)h).
When the values of the function have errors (for example, when they are rounded
numbers) the difference quotients become more and more uncertain the smaller h is. Thus
if one wishes to compute the derivatives of a function one should be careful not to use too
small a step length; see Sec. 3.3.4.
Example 1.1.5.
Assume that for y = cos x, function values correct to six decimal digits are known at
equidistant points:

14
Chapter 1. Principles of Numerical Calculations
x
y
4y
42y
0.59
0.830941
−5605
0.60
0.825336
−83
−5688
0.61
0.819648
,
where the differences are expressed in units of 10−6. This arrangement of the numbers is
called a difference scheme. Using (1.1.14) and (1.1.18) one gets
y′(0.60) ≈(0.819648 −0.830941)/0.02 = −0.56465,
y′′(0.60) ≈−83 · 10−6/(0.01)2 = −0.83.
The correct results are, with six decimals,
y′(0.60) = −0.564642,
y′′(0.60) = −0.825336.
In y′′ we got only two correct decimal digits. This is due to cancellation, which is an
important cause of loss of accuracy; see Sec. 2.3.4. Better accuracy can be achieved by
increasing the step h; see Problem 1.1.5 at the end of this section.
A very important equation of mathematical physics is Poisson’s equation:6
∂2u
∂x2 + ∂2u
∂y2 = f (x, y),
(x, y) ∈<.
(1.1.21)
Here the function f (x, y) is given together with some boundary condition on u(x, y).
Under certain conditions, gravitational, electric, magnetic, and velocity potentials satisfy
Laplace’s equation7 which is (1.1.21) with f (x, y) = 0.
Finite difference approximations are useful for partial derivatives. Suppose that < is
a rectangular region and introduce a rectangular grid that covers the rectangle. With grid
spacing h and k, respectively, in the x and y directions, respectively, this consists of the
points
xi = x0 + ih,
i = 0 : M,
yj = y0 + jk,
j = 0 : N.
By (1.1.20), a second order accurate approximation of Poisson’s equation is given by the
ﬁve-point operator
∇2
5u = ui+1,j −2ui,j + ui−1,j
h2
+ ui,j+1 −2ui,j + ui,j−1
k2
.
For k = h
∇2
5u = 1
h2

ui,j+1 + ui−1,j −4ui,j + ui+1,j + ui,j−1

,
6Siméon Denis Poisson (1781–1840), professor at École Polytechnique. He has also given his name to the
Poisson distribution in probability theory.
7Pierre-Simon, Marquis de Laplace (1749–1827), professor at École Militaire. Laplace was one of the most
inﬂuential scientists of his time and did major work in probability and celestial mechanics.

Problems and Computer Exercises
15
which corresponds to the “computational molecule”
1
h2


1
1
−4
1
1

.
If this is superimposed on each grid point we get one equation for the unknown values
u(xi, yj), i = 1 : M −1, j = 1 : N −1, at each interior point of the grid.
To get a solution we also need prescribed boundary conditions on u or ∂u/∂n on the
boundary. The solution can then be obtained in the interior by solving a system of linear
equations.
Review Questions
1.1.1 Make lists of the concepts and ideas which have been introduced. Review their use
in the various types of problems mentioned.
1.1.2 Discuss the convergence condition and the rate of convergence of the ﬁxed-point
iteration method for solving a nonlinear equation x = F(x).
1.1.3 What is meant by quadratic convergence of an iterative method for solving a nonlinear
equation?
1.1.4 What is the trapezoidal rule? What is said about the dependence of its error on the
step length?
1.1.5 How can Richardson extrapolation be used to improve the accuracy of the trapezoidal
rule?
Problems and Computer Exercises
1.1.1 Calculate
√
10 to seven decimal places using the method in Example 1.1.1. Begin
with x0 = 2.
1.1.2 Consider f (x) = x3 −2x −5. The cubic equation f (x) = 0 has been a standard test
problem, since Newton used it in 1669 to demonstrate his method. By computing
(say) f (x) for x = 1, 2, 3, we see that x = 2 probably is a rather good initial guess.
Iterate by Newton’s method until you trust that the result is correct to six decimal
places.
1.1.3 The equation x3 −x = 0 has three roots, −1, 0, 1. We shall study the behavior of
Newton’s method on this equation, with the notations used in Sec. 1.1.1 and Fig-
ure 1.1.3.
(a) What happens if x0 = 1/
√
3? Show that xn converges to 1 for any x0 > 1/
√
3.
What is the analogous result for convergence to −1?
(b) What happens if x0 = 1/
√
5?
Show that xn converges to 0 for any x0 ∈
(−1/
√
5, 1/
√
5).
Hint: Show ﬁrst that if x0 ∈(0, 1/
√
5), then x1 ∈(−x0, 0). What can then be said
about x2?

16
Chapter 1. Principles of Numerical Calculations
(c) Find, by a drawing (with paper and pencil), lim xn if x0 is a little less than 1/
√
3.
Find by computation lim xn if x0 = 0.46.
(d) A complete discussion of the question in (c) is rather complicated, but there is
an implicit recurrence relation that produces a decreasing sequence {a1 = 1/
√
3,
a2, a3, . . .}, by means of which one you can easily ﬁnd limn→∞xn for any x0 ∈
(1/
√
5, 1/
√
3). Try to ﬁnd this recurrence.
Answer: ai −f (ai)/f ′(ai) = −ai−1; limn→∞xn = (−1)i if x0 ∈(ai, ai+1);
a1 = 0.577, a2 = 0.462, a3 = 0.450, a4 ≈limi→∞ai = 1/
√
5 = 0.447.
1.1.4 Calculate
 1/2
0
ex dx
(a) to six decimals using the primitive function.
(b) with the trapezoidal rule, using step length h = 1/4.
(c) using Richardson extrapolation to h = 0 on the results using step lengths h = 1/2
and h = 1/4.
(d) the ratio between the error in the result of (c) to that of (b).
1.1.5 In Example 1.1.5 we computed y′′(0.6) for y = cos x, with step length h = 0.01.
Make similar calculations using h = 0.1, h = 0.05, and h = 0.001. Which value of
h gives the best result, using values of y to six decimal places? Discuss qualitatively
the inﬂuences of both the rounding errors in the function values and the error in the
approximation of a derivative with a difference quotient on the result for various
values of h.
1.1.6 Give an approximate expression of the form ahbf (c)(0) for the error of the esti-
mate of the integral
 h
−h f (x)dx obtained by Richardson extrapolation (according to
Sec. 1.1.3) from the trapezoidal values T (h) and T (2h).
1.2
Some Numerical Algorithms
For a given numerical problem one can consider many different algorithms. Even if they
just differ in small details they can differ in efﬁciency and reliability and give approximate
answers with widely varying accuracy. In the following we give a few examples of how
algorithms can be developed to solve some typical numerical problems.
1.2.1
Solving a Quadratic Equation
An early example of pitfalls in computation studied by G. E. Forsythe [121] is the following.
For computing the roots of the quadratic equation ax2 + bx + c = 0, a ̸= 0, elementary
textbooks usually give the well-known formula
r1,2 =

−b ±

b2 −4ac

(2a).
Using this for the quadratic equation x2 −56x + 1 = 0, we get the two approximate real
roots
r1 = 28 +
√
783 ≈28 + 27.982 = 55.982 ± 1
210−3,
r2 = 28 −
√
783 ≈28 −27.982 = 0.018 ± 1
210−3.

1.2. Some Numerical Algorithms
17
In spite of the fact that the square root used is given to ﬁve digits of accuracy, we get only
two signiﬁcant digits in r2, while the relative error in r1 is less than 10−5. This shows
that there can be very poor relative accuracy in the difference between two nearly equal
numbers. This phenomenon is called cancellation of terms. It is a very common reason
for poor accuracy in numerical calculations.
Notice that the subtraction in the calculation of r2 was carried out exactly.
The
cancellation in the subtraction only gives an indication of the unhappy consequence of a
loss of information in previous steps, due to the rounding of one of the operands, and is not
the cause of the inaccuracy.
In numerical calculations, if possible one should try to avoid formulas that give rise
to cancellation, as in the above example. For the quadratic equation this can be done by
rewriting of the formulas. Comparing coefﬁcients on both sides of
x2 + (b/a)x + c/a = (x −r1)(x −r2) = x2 −(r1 + r2)x + r1r2,
we get the relation between coefﬁcients and roots
r1 + r2 = −b/a,
r1r2 = c/a.
(1.2.1)
A more accurate value of the root of smaller magnitude is obtained by computing this root
from the latter of these relations. We then get
r2 = 1/55.982 = 0.0178629 ± 0.0000002.
Five signiﬁcant digits are now obtained also for this root.
1.2.2
Recurrence Relations
A common computational task is the evaluation of a polynomial
p(x) = a0xn + a1x2 + · · · + an−1x + an
at a given point. This can be reformulated as
p(x) = (· · · ((a0x + a1)x + a2)x + · · · + an−1)x + an,
and written as a recurrence relation:
bi(x) = bi−1(x)x + ai,
i = 1 : n.
(1.2.2)
We note that this recurrence relation can be used in two different ways:
• it can be used algebraically to generate a sequence of Horner polynomials bi(x) such
that bn(x) = p(x);
• it can be used arithmetically with a speciﬁc value x = x1, which is Horner’s rule
for evaluating p(x1) = bn(x1).

18
Chapter 1. Principles of Numerical Calculations
Horner’s rule requires n additions and multiplications for evaluating p(x) for x = x1. Note
that if the powers are calculated recursively by xi
1 = x1 · xi−1
1
and subsequently multiplied
by an−i, this requires twice as many multiplications.
When a polynomial p(x) is divided by x −x1 the remainder equals p(x1); i.e.,
p(x) = (x −x1)q(x) + p(x1). The quantities bi(x1) from the Horner scheme (1.2.2) are
of intrinsic interest because they are the coefﬁcients of the quotient polynomial q(x). This
algorithm therefore performs the synthetic division
p(x) −p(x1)
x −x1
=
n−1

i=0
bi(x1)xn−1−i.
(1.2.3)
The proof of this result is left as an exercise.
Synthetic division is used, for instance, in the solution of algebraic equations when
already computed roots are successively eliminated. After each elimination, one can deal
with an equation of lower degree. This process is called deﬂation; see Sec. 6.5.4. As
emphasized there, some care is necessary in the numerical application of this idea to prevent
the propagation of roundoff errors.
The proof of the following useful relation is left as an exercise for the reader.
Lemma 1.2.1.
Let bi be deﬁned by (1.2.2) and
c0 = b0,
ci = bi + xci−1,
i = 1 : n −1.
(1.2.4)
Then p′(x) = cn−1.
Due to their intrinsic constructive quality, recurrence relations are one of the basic
mathematical tools of computation. There is hardly a computational task which does not
use recursive techniques. One of the most important and interesting parts of the preparation
of a problem for a computer is therefore to ﬁnd a recursive description of the task. Often an
enormous amount of computation can be described by a small set of recurrence relations.
Although recurrence relations are a powerful tool they are also susceptible to error
growth. Each cycle of a recurrence relation not only generates its own errors but also
inherits errors committed in all previous cycles. If conditions are unfavorable, the result
may be disastrous. This aspect of recurrence relations and its prevention is therefore of
great importance in computations and has been studied extensively; see [139].
Example 1.2.1.
Unless used in the right way, errors committed in a recurrence relation can grow
exponentially and completely ruin the results. To compute the integrals
In =
 1
0
xn
x + 5 dx,
i = 1 : N,
one can use the recurrence relation
In + 5In−1 = 1
n,
(1.2.5)

1.2. Some Numerical Algorithms
19
which follows from
In + 5In−1 =
 1
0
xn + 5xn−1
x + 5
dx =
 1
0
xn−1 dx = 1
n.
Below we use this formula to compute I8, using six decimals throughout. For n = 0 we
have
I0 = [ln(x + 5)]1
0 ≈ln 6 −ln 5 = 0.182322.
Using the recurrence relation we get
I1 = 1 −5I0 = 1 −0.911610 = 0.088390,
I2 = 1/2 −5I1 = 0.500000 −0.441950 = 0.058050,
I3 = 1/3 −5I2 = 0.333333 −0.290250 = 0.043083,
I4 = 1/4 −5I3 = 0.250000 −0.215415 = 0.034585,
I5 = 1/5 −5I4 = 0.200000 −0.172925 = 0.027075,
I6 = 1/6 −5I5 = 0.166667 −0.135375 = 0.031292,
I7 = 1/7 −5I6 = 0.142857 −0.156460 = −0.013603.
It is strange that I6 > I5, and obviously absurd that I7 < 0! The reason for the absurd result
is that the roundoff error ϵ in I0 = 0.18232156 . . . , whose magnitude is about 0.44 · 10−6,
is multiplied by (−5) in the calculation of I1, which then has an error of −5ϵ. That error
produces an error in I2 of 52ϵ, and so forth. Thus the magnitude of the error in I7 is
57ϵ = 0.0391, which is larger than the true value of I7. On top of this are the roundoff
errors committed in the various steps of the calculation. These can be shown, in this case,
to be relatively unimportant.
If one uses higher precision, the absurd result will show up at a later stage. For
example, a computer that works with a precision corresponding to about 16 decimal places
gave a negative value to I22, although I0 had full accuracy. The above algorithm is an
example of an unpleasant phenomenon, called numerical instability. In this simple case,
one can avoid the numerical instability by reversing the direction of the recursion.
Example 1.2.2.
If we use the recurrence relation in the other direction,
In−1 = (1/n −In)/5,
(1.2.6)
the errors will be divided by −5 in each step. But we need a starting value. We can
directly see from the deﬁnition that In decreases as n increases. One can also surmise that
In decreases slowly when n is large (the reader is encouraged to motivate this). Thus we
try setting I12 = I11. It then follows that
I11 + 5I11 ≈1/12,
I11 ≈1/72 ≈0.013889
(show that 0 < I12 < 1/72 < I11). Using the recurrence relation we get
I10 = (1/11 −0.013889)/5 = 0.015404,
I9 = (1/10 −0.015404)/5 = 0.016919

20
Chapter 1. Principles of Numerical Calculations
and, further,
I8 = 0.018838,
I7 = 0.021232,
I6 = 0.024325,
I5 = 0.028468,
I4 = 0.034306,
I3 = 0.043139,
I2 = 0.058039,
I1 = 0.088392,
and ﬁnally I0 = 0.182322. Correct!
If one instead simply takes I12 = 0 as the starting value, one gets I11 = 0.016667,
I10 = 0.018889, I9 = 0, 016222, I8 = 0.018978, I7 = 0.021204, I6 = 0.024331, and
I5, . . . , I0 have the same values as above. The difference in the values for I11 is 0.002778.
The subsequent values of I10, I9, . . . , I0 are quite close because the error is divided by −5
in each step. The results for In obtained above have errors which are less than 10−3 for
n ≤8.
One should not draw erroneous conclusions from the above example. The use of a
recurrence relation “backwards” is not a universal recipe, as will be seen later. Compare
also Problems 1.2.7 and 1.2.8.
In Sec. 3.3.5 we will study the general linear homogeneous difference equation of kth
order
yn+k + a1yn+k−1 + · · · + akyn = 0,
(1.2.7)
with real or complex constant coefﬁcients a1, . . . , ak. The stability properties of this type of
equation are fundamental, since they arise in the numerical solution of ordinary and partial
differential equations.
1.2.3
Divide and Conquer Strategy
Apowerful strategy for solving large scale problems is the divide and conquer strategy (one
of the oldest military strategies). This is one of the most powerful algorithmic paradigms for
designing efﬁcient algorithms. The idea is to split a high-dimensional problem into multiple
problems (typically two for sequential algorithms) of lower dimension. Each of these is
then again split into smaller subproblems, and so forth, until a number of sufﬁciently small
problems are obtained. The solution of the initial problem is then obtained by combining
the solutions of the subproblems working backward in the hierarchy.
We illustrate the idea on the computation of the sum s = n
i=1 ai. The usual way to
proceed is to use the recursion
s0 = 0,
si = si−1 + ai,
i = 1 : n.
Another order of summation is as illustrated below for n = 23 = 8:
a1
↘
a2
↙
a3
↘
a4
↙
a5
↘
a6
↙
a7
↘
a8
↙
s1:2
↘
s3:4
↙
s5:6
↘
s7:8
↙
,
s1:4
↘
s5:8
↙
s1:8

1.2. Some Numerical Algorithms
21
where si:j = ai + · · · + aj. In this table each new entry is obtained by adding its two
neighbors in the row above. Clearly this can be generalized to compute an arbitrary sum
of n = 2k terms in k steps. In the ﬁrst step we perform n/2 sums of two terms, then n/4
partial sums each of four terms, etc., until in the kth step we compute the ﬁnal sum.
This summation algorithm uses the same number of additions as the ﬁrst one. But
it has the advantage that it splits the task into several subtasks that can be performed in
parallel. For large values of n this summation order can also be much more accurate than
the conventional order (see Problem 2.3.5).
The algorithm can also be described in another way. Consider the summation algo-
rithm
sum = s(i, j);
if j = i + 1 then sum = ai + aj;
else k = ⌊(i + j)/2⌋; sum = s(i, k) + s(k + 1, j);
end
for computing the sum s(i, j) = ai + · · · + aj, j > i. (Here and in the following ⌊x⌋
denotes the ﬂoor of x, i.e., the largest integer ≤x. Similarly, ⌈x⌉denotes the ceiling of
x, i.e., the smallest integer ≥x.)
This function deﬁnes s(i, j) in a recursive way; if the
sum consists of only two terms, then we add them and return with the answer. Otherwise
we split the sum in two and use the function again to evaluate the corresponding two partial
sums. Espelid [114] gives an interesting discussion of such summation algorithms.
The function above is an example of a recursive algorithm—it calls itself. Many
computer languages (for example, MATLAB) allow the deﬁnition of such recursive algo-
rithms. The divide and conquer is a top down description of the algorithm in contrast to
the bottom up description we gave ﬁrst.
Example 1.2.3.
Sorting the items of a one-dimensional array in ascending or descending order is one of
the most important problems in computer science. In numerical work, sorting is frequently
needed when data need to be rearranged. One of the best known and most efﬁcient sorting
algorithms, quicksort by Hoare [202], is based on the divide and conquer paradigm. To
sort an array of n items, a[0 : n −1], it proceeds as follows:
1. Select an element a(k) to be the pivot. Commonly used methods are to select the
pivot randomly or select the median of the ﬁrst, middle, and last element in the array.
2. Rearrange the elements of the array a into a left and right subarray such that no element
in the left subarray is larger than the pivot and no element in the right subarray is
smaller than the pivot.
3. Recursively sort the left and right subarray.
The partitioning of a subarray a[l : r], l < r, in step 2 can proceed as follows. Place
the pivot in a[l] and initialize two pointers i = l, j = r + 1. The pointer i is incremented
until an element a(i) is encountered which is larger than the pivot. Similarly, the pointer j
is decremented until an element a(j) is encountered which is smaller than the pivot. At this

22
Chapter 1. Principles of Numerical Calculations
point the elements a(i) and a(j) are exchanged. The process continues until the pointers
cross each other. Finally, the pivot element is placed in its correct position.
It is intuitively clear that this algorithm sorts the entire array and that no merging
phase is needed.
There are many other examples of the power of the divide and conquer approach.
It underlies the fast Fourier transform (Sec. 4.6.3) and is used in efﬁcient automatic paral-
lelization of many tasks, such as matrix multiplication; see [111].
1.2.4
Power Series Expansions
In many problems of applied mathematics, the solution of a given problem can be obtained
as a power series expansion. Often the convergence of these series is quite fast. As an
example we consider the task of computing, to ﬁve decimals, y(0.5), where y(x) is the
solution to the differential equation
y′′ = −xy,
with initial conditions y(0) = 1, y′(0) = 0. The solution cannot be simply expressed in
terms of elementary functions. We shall use the method of undetermined coefﬁcients.
Thus we try substituting a series of the form:
y(x) =
∞

n=0
cnxn = c0 + c1x + c2x2 + · · · .
Differentiating twice we get
y′′(x) =
∞

n=0
n(n −1)cnxn−2
= 2c2 + 6c3x + 12c4x2 + · · · + (m + 2)(m + 1)cm+2xm + · · · ,
−xy(x) = −c0x −c1x2 −c2x3 −· · · −cm−1xm −· · · .
Equating coefﬁcients of xm in these series gives
c2 = 0,
(m + 2)(m + 1)cm+2 = −cm−1,
m ≥1.
It follows from the initial conditions that c0 = 1, c1 = 0. Thus cn = 0, if n is not a multiple
of 3, and using the recursion we obtain
y(x) = 1 −x3
6 + x6
180 −
x9
12 960 + · · · .
(1.2.8)
This gives y(0.5) ≈0.97925. The x9 term is ignored, since it is less than 2·10−7. In this ex-
ample also the ﬁrst neglected term gives a rigorous bound for the error (i.e., for the remaining
terms), since the absolute value of the term decreases, and the terms alternate in sign.
Since the calculation was based on a trial substitution, one should, strictly speaking,
prove that the series obtained deﬁnes a function which satisﬁes the given problem. Clearly,

Problems and Computer Exercises
23
the series converges at least for |x| < 1, since the coefﬁcients are bounded. (In fact the
series converges for all x.) Since a power series can be differentiated term by term in the
interior of its interval of convergence, the proof presents no difﬁculty. Note, in addition,
that the ﬁnite series obtained for y(x) by breaking off after the x9-term is the exact solution
to the following modiﬁed differential equation:
y′′ = −xy −
x10
12 960,
y(0) = 1,
y′(0) = 0,
where the “perturbation term” −x10/12 960 has magnitude less than 10−7 for |x| ≤0.5. It
is possible to ﬁnd rigorous bounds for the difference between the solutions of a differential
system and a modiﬁed differential system.
The use of power series and rational approximations will be studied in depth in Chapter
3, where other more efﬁcient methods than the Maclaurin series for approximation by
polynomials will also be treated.
A different approximation problem, which occurs in many variants, is to approximate
a function f speciﬁed at a one- or two-dimensional grid by a member f ∗of a class of func-
tions which is easy to work with mathematically. Examples are (piecewise) polynomials,
rational functions, or trigonometric polynomials, where each particular function in the class
is speciﬁed by the numerical values of a number of parameters.
In computer aided design (CAD) curves and surfaces have to be represented math-
ematically so that they can be manipulated and visualized easily. For this purpose spline
functions are now used extensively with important applications in the aircraft and automo-
tive industries; see Sec. 4.4. The name spline comes from a very old technique in drawing
smooth curves, in which a thin strip of wood or rubber, called a draftsman’s spline, is bent
so that it passes through a given set of points. The points of interpolation are called knots
and the spline is secured at the knots by means of lead weights called ducks. Before the
computer age, splines were used in shipbuilding and other engineering designs.
Review Questions
1.2.1 What is a common cause of loss of accuracy in numerical calculations?
1.2.2 Describe Horner’s rule and synthetic division.
1.2.3 Give a concise explanation why the algorithm in Example 1.2.1 did not work and
why that in Example 1.2.2 did work.
1.2.4 Describe the basic idea behind the divide and conquer strategy. What is a main
advantage of this strategy? How do you apply it to the task of summing n numbers?
Problems and Computer Exercises
1.2.1 (a) Use Horner’s scheme to compute for x = 2
p(x) = x4 + 2x3 −3x2 + 2.

24
Chapter 1. Principles of Numerical Calculations
(b) Count the number of multiplications and additions required for the evaluation
of a polynomial p(z) of degree n by Horner’s rule. Compare with the work needed
when the powers are calculated recursively by xj = x · xj−1 and subsequently
multiplied by an−j.
1.2.2 P (x) = 1 −1
2x2 + 1
24x4 is a polynomial approximation to cos x for small values of
|x|. Estimate the errors of
P(x),
P ′(x),
1
x
 x
0
P(t) dt,
and compare them for x = 0.1.
1.2.3 Showhowrepeatedsyntheticdivisioncanbeusedtomovetheoriginofapolynomial;
i.e., given a1, a2, . . . , an, and z, ﬁnd c1, c2, . . . , cn so that
pn(x) =
n

j=1
ajxj−1 ≡
n

j=1
cj(x −z)j−1.
Write a program for synthetic division (with this ordering of the coefﬁcients) and
apply it to this algorithm.
Hint: Apply synthetic division to pn(x), pn−1(x) = (pn(x) −pn(z))/(x −z), and
so forth.
1.2.4 (a) Show that the transformation made in Problem 1.2.3 can also be expressed by
means of the matrix-vector equation
c = diag (1, z−1, . . . , z1−n) P diag (1, z, . . . , zn−1) a,
where a = [a1, a2, . . . , an]T , c = [c1, c2, . . . , cn]T , and diag (1, z, . . . , zn−1) is a
diagonal matrix with elements zj−1, j = 1 : n. The matrix P ∈Rn×n has elements
pi,j =
 j −1
i −1

if j ≥i,
0
otherwise.
By convention,
0
0

= 1 here.
(b) Note the relation of P to the Pascal triangle, and show how P can be generated
by a simple recursion formula. Also show how each element of P −1 can be expressed
in terms of the corresponding element of P. How is the origin of the polynomial
pn(x) moved, if you replace P by P −1 in the matrix-vector equation that deﬁnes c?
(c) If you reverse the order of the elements of the vectors a, c—this may sometimes
be a more convenient ordering—how is the matrix P changed?
Comment: With terminology to be used much in this book (see Sec. 4.1.2), we can
look upon a and c as different coordinate vectors for the same element in the n-
dimensional linear space Pn of polynomials of degree less than n. The matrix P
gives the coordinate transformation.

Problems and Computer Exercises
25
1.2.5 Derive recurrence relations and write a program for computing the coefﬁcients of
the product r of two polynomials p and q:
r(x) = p(x)q(x) =
 m

i=1
aixi−1
 

n

j=1
bjxj−1

=
m+n−1

k=1
ckxk−1.
1.2.6 Let a, b be nonnegative integers, with b ̸= 0. The division a/b yields the quotient q
and the remainder r. Show that if a and b have a common factor, then that number
is a divisor of r as well. Use this remark to derive the Euclidean algorithm for the
determination of the greatest common divisor of a and b.
1.2.7 Derive a forward and a backward recurrence relation for calculating the integrals
In =
 1
0
xn
4x + 1 dx.
Why is the forward recurrence stable and the backward recurrence unstable in this
case?
1.2.8 (a) Solve Example 1.2.1 on a computer, with the following changes: Start the re-
cursion (1.2.5) with I0 = ln 1.2, and compute and print the sequence {In} until In
becomes negative for the ﬁrst time.
(b) Start the recursion (1.2.6) ﬁrst with the condition I19 = I20, then with I29 = I30.
Compare the results you obtain and assess their approximate accuracy. Compare
also with the results of part (a).
1.2.9 (a) Write a program (or study some library program) for ﬁnding the quotient Q(x)
and the remainder R(x) of two polynomials A(x), B(x), i.e.,
A(x) = Q(x)B(x) + R(x),
deg R(x) < deg B(x).
(b) Write a program (or study some library program) for ﬁnding the coefﬁcients of
a polynomial with given roots.
1.2.10 (a) Write a program (or study some library program) for ﬁnding the greatest com-
mon divisor of two polynomials. Test it on a number of polynomials of your own
choice. Choose also some polynomials of a rather high degree, and do not choose
only polynomials with small integer coefﬁcients. Even if you have constructed the
polynomials so that they should have a common divisor, rounding errors may disturb
this, and some tolerance is needed in the decision whether a remainder is zero or
not. One way of ﬁnding a suitable size of the tolerance is to make one or several
runs where the coefﬁcients are subject to some small random perturbations, and ﬁnd
out how much the results are changed.
(b) Apply the programs mentioned in the last two problems for ﬁnding and elimi-
nating multiple zeros of a polynomial.
Hint: A multiple zero of a polynomial is a common zero of the polynomial and its
derivative.

26
Chapter 1. Principles of Numerical Calculations
1.3
Matrix Computations
Matrix computations are ubiquitous in scientiﬁc computing. A survey of basic notation and
concepts in matrix computations and linear vector spaces is given in Online Appendix A.
This is needed for several topics treated in later chapters of this book. A fuller treatment of
this topic will be given in Volume II.
In this section we focus on some important developments since the 1950s in the solu-
tion of linear systems. One is the systematic use of matrix notations and the interpretation of
Gaussian elimination as matrix factorization. This decompositional approach has several
advantages; e.g., a computed factorization can often be used with great savings to solve new
problems involving the original matrix. Another is the rapid development of sophisticated
iterative methods, which are becoming increasingly important as systems increase in size.
1.3.1
Matrix Multiplication
A matrix8 A is a collection of m × n numbers ordered in m rows and n columns:
A = (aij) =


a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn

.
We write A ∈Rm×n, where Rm×n denotes the set of all real m × n matrices. If m = n, then
the matrix A is said to be square and of order n. If m ̸= n, then A is said to be rectangular.
The product of two matrices A and B is deﬁned if and only if the number of columns
in A equals the number of rows in B. If A ∈Rm×p and B ∈Rp×n, then C = AB ∈Rm×n,
where
cij =
p

k=1
aikbkj,
1 ≤i ≤m,
1 ≤j ≤n.
(1.3.1)
The product BA is deﬁned only if m = n and then BA ∈Rp×p. Clearly, matrix multipli-
cation is in general not commutative. In the exceptional case where AB = BA holds, the
matrices A and B are said to commute.
Matrix multiplication satisﬁes the associative and distributive rules:
A(BC) = (AB)C,
A(B + C) = AB + AC.
However, the number of arithmetic operations required to compute the left- and right-hand
sides of these equations can be very different!
Example 1.3.1.
Let the three matrices A ∈Rm×p, B ∈Rp×n, and C ∈Rn×q be given. Then
computing the product ABC as (AB)C requires mn(p+q) multiplications, whereas A(BC)
requires pq(m + n) multiplications.
8The ﬁrst to use the term “matrix” was the English mathematician James Sylvester in 1850. Arthur Cayley
then published Memoir on the Theory of Matrices in 1858, which spread the concept.

1.3. Matrix Computations
27
If A and B are square n × n matrices and C = x ∈Rn×1, a column vector of length
n, then computing (AB)x requires n2(n + 1) multiplications, whereas A(Bx) only requires
2n2 multiplications. When n ≫1 this makes a great difference!
It is often useful to think of a matrix as being built up of blocks of lower dimensions.
The great convenience of this lies in the fact that the operations of addition and multiplication
can be performed by treating the blocks as noncommuting scalars and applying the deﬁnition
(1.3.1). Of course the dimensions of the blocks must correspond in such a way that the
operations can be performed.
Example 1.3.2.
Assume that the two n × n matrices are partitioned into 2 × 2 block form,
A =

A11
A12
A21
A22

,
B =

B11
B12
B21
B22

,
where A11 and B11 are square matrices of the same dimension. Then the product C = AB
equals
C =

A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22

.
(1.3.2)
Be careful to note that since matrix multiplication is not commutative the order of the factors
in the products cannot be changed. In the special case of block upper triangular matrices
this reduces to

R11
R12
0
R22
 
S11
S12
0
S22

=

R11S11
R11S12 + R12S22
0
R22S22

.
(1.3.3)
Note that the product is again block upper triangular, and its block diagonal simply equals
the products of the diagonal blocks of the factors.
It is important to know roughly how much work is required by different matrix algo-
rithms. By inspection of (1.3.1) it is seen that computing the mp elements cij in the product
C = AB requires mnp additions and multiplications.
In matrix computations the number of multiplicative operations (×, /) is usually about
the same as the number of additive operations (+, −). Therefore, in older literature, a ﬂop
was deﬁned to mean roughly the amount of work associated with the computation
s := s + aikbkj,
i.e., one addition and one multiplication (or division). In more recent textbooks (e.g., Golub
and Van Loan [169])a ﬂop is deﬁned as one ﬂoating-point operation, doubling the older ﬂop
counts.9 Hence, multiplication C = AB of two square matrices of order n requires 2n3
9Stewart [335, p. 96] uses ﬂam (ﬂoating-point addition and multiplication) to denote an “old” ﬂop.

28
Chapter 1. Principles of Numerical Calculations
ﬂops. The matrix-vector multiplication y = Ax, where A ∈Rn×n and x ∈Rn, requires
2mn ﬂops.10
Operation counts are meant only as a rough appraisal of the work and one should not
assign too much meaning to their precise value. On modern computer architectures the rate
of transfer of data between different levels of memory often limits the actual performance.
Also usually ignored is the fact that on many computers a division is ﬁve to ten times slower
than a multiplication.
An operation count still provides useful information and can serve as an initial basis
of comparison of different algorithms. It implies that the running time for multiplying two
square matrices on a computer will increase roughly cubically with the dimension n. Thus,
doubling n will approximately increase the work by a factor of eight; this is also apparent
from (1.3.2).
A faster method for matrix multiplication would give more efﬁcient algorithms for
many linear algebra problems such as inverting matrices, solving linear systems, and solv-
ing eigenvalue problems. An intriguing question is whether it is possible to multiply two
matrices A, B ∈Rn×n (or solve a linear system of order n) in less than n3 (scalar) multiplica-
tions. The answer is yes! Strassen [341] developed a fast algorithm for matrix multiplication
which, if used recursively to multiply two square matrices of dimension n = 2k, reduces
the number of multiplications from n3 to nlog2 7 = n2.807.... The key observation behind the
algorithm is that the block matrix multiplication (1.3.2) can be performed with only seven
block matrix multiplications and eighteen block matrix additions. Since for large dimen-
sions matrix multiplication is much more expensive (2n3 ﬂops) than addition (2n2 ﬂops),
this will lead to a savings in operations.
It is still an open (difﬁcult!) question what the minimum exponent ω is such that matrix
multiplication can be done in O(nω) operations. The fastest known algorithm, devised in
1987 by Coppersmith and Winograd [79], has ω < 2.376. Many believe that an optimal
algorithm can be found which reduces the number to essentially n2. For a review of recent
efforts in this direction using group theory, see Robinson [306]. (Note that for many of the
theoretically “fast” methods large constants are hidden in the O notation.)
1.3.2
Solving Linear Systems by LU Factorization
The solution of linear systems of equations is the most frequently encountered task in
scientiﬁc computing. One important source of linear systems is discrete approximations of
continuous differential and integral equations.
A linear system can be written in matrix-vector form as


a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn




x1
x2
...
xn

=


b1
b2
...
bm

,
(1.3.4)
where aij and bi, 1 ≤i ≤m, 1 ≤j ≤n, are known input data and the task is to compute
the unknowns xj, 1 ≤j ≤n. More compactly we write Ax = b, where A ∈Rm×n is a
matrix and x ∈Rn and b ∈Rm are column vectors.
10To add to the confusion, in computer literature “ﬂops” means ﬂoating-point operations per second.

1.3. Matrix Computations
29
Solving linear systems by Gaussian elimination11 is taught in elementary courses in
linear algebra. Although in theory this algorithm seems deceptively simple, the practical
solution of large linear systems is far from trivial. In the 1940s, at the beginning of the
computer age, there was a mood of pessimism among mathematicians about the possibility
of accurately solving systems even of modest order, say n = 100. Today there is a deeper
understanding of how Gaussian elimination performs in ﬁnite precision arithmetic. Linear
systems with hundred of thousands of unknowns are now routinely solved in scientiﬁc
computing.
Linear systems which (possibly after a permutation of rows and columns) are of
triangular form are particularly simple to solve. Consider a square upper triangular linear
system (m = n),


u11
. . .
u1,n−1
u1n
...
...
...
un−1,n−1
un−1,n
unn




x1
...
xn−1
xn

=


b1
...
bn−1
bn

.
The matrix U is nonsingular if and only if
det(U) = u11 · · · un−1,n−1unn ̸= 0.
If this is the case the unknowns can be computed by the following recursion:
xn = bn/unn,
xi =

bi −
n

k=i+1
uikxk

uii,
i = n −1 : −1 : 1.
(1.3.5)
Since the unknowns are solved in reverse order this is called back-substitution. Thus the
solution of a triangular system of order n can be computed in exactly n2 ﬂops; this is the
same amount of work as required for multiplying a vector by a triangular matrix.
Similarly, a square linear system of lower triangular form Lx = b,


l11
l21
l22
...
...
...
ln1
ln2
. . .
lnn




x1
x2
...
xn

=


b1
b2
...
bn

,
where L is nonsingular, can be solved by forward-substitution:
x1 = b1/l11,
xi =

bi −
i−1

k=1
likxk

lii,
i = 2 : n.
(1.3.6)
(Note that by reversing the order of the rows and columns an upper triangular system is
transformed into a lower triangular and vice versa.)
11Named after the German mathematician Carl Friedrich Gauss (1777–1855), but known already in China as
early as the ﬁrst century B.C. Gauss was one of the greatest mathematician of the nineteenth century. He spent
most of his life in Göttingen, where in his dissertation he gave the ﬁrst proof of the fundamental theorem of algebra.
He made fundamental contributions to number theory, differential geometry, celestial mechanics, and geodesy. He
introduced the method of least squares and put it on a solid foundation.

30
Chapter 1. Principles of Numerical Calculations
When implementing a matrix algorithm on a computer, the order of operations in
matrix algorithms may be important. One reason for this is the economizing of storage,
since even matrices of moderate dimensions have a large number of elements. When the
initial data are not needed for future use, computed quantities may overwrite data. To
resolve such ambiguities in the description of matrix algorithms, it is important to be able
to describe computations like those in (1.3.5) in a more precise form. For this purpose we
will use an informal programming language, which is sufﬁciently precise for our purpose
but allows the suppression of cumbersome details. We illustrate these concepts on the
back-substitution algorithm given above. In the following back-substitution algorithm the
solution x overwrites the data b.
Algorithm 1.1. Back-Substitution.
Given a nonsingular upper triangular matrix U ∈Rn×n and a vector b ∈Rn, the following
algorithm computes x ∈Rn such that Ux = b:
for i = n : (−1) : 1
s :=
n

k=i+1
uikbk;
bi := (bi −s)/uii;
end
Here x := y means that the value of y is evaluated and assigned to x. We use the convention
that when the upper limit in a sum is smaller than the lower limit the sum is set to zero.
In the above algorithm the elements in U are accessed in a rowwise manner. In another
possible sequencing of the operations the elements in U are accessed columnwise. This
gives the following algorithm:
for k = n : (−1) : 1
bk := bk/ukk;
for i = k −1 : (−1) : 1
bi := bi −uikbk;
end
end
Such differences in the sequencing of the operations can inﬂuence the efﬁciency of the
implementation depending on how the elements in the matrix U are stored.
Gaussian elimination uses the following elementary operations, which can be per-
formed without changing the set of solutions:
• interchanging two equations,
• multiplying an equation by a nonzero scalar α,
• adding a multiple α of the ith equation to the jth equation, j ̸= i.

1.3. Matrix Computations
31
These operations correspond in an obvious way to row operations carried out on the aug-
mented matrix (A, b). By performing a sequence of such elementary operations the system
Ax = b can be transformed into an upper triangular system which, as shown above, can be
solved by recursive substitution.
In Gaussian elimination the unknowns are eliminated in a systematic way so that at
the end an equivalent triangular system is produced, which can be solved by substitution.
Consider the system (1.3.4) with m = n and assume that a11 ̸= 0. Then we can eliminate x1
fromthelast(n−1)equationsbysubtractingfromtheithequationthemultipleli1 = ai1/a11,
of the ﬁrst equation. The last (n −1) equations then become


a(2)
22
· · ·
a(2)
2n
...
...
...
a(2)
n2
· · ·
a(2)
nn




x2
...
xn

=


b(2)
2
...
b(2)
n

,
where the new elements are given by
a(2)
ij = aij −ai1a1j
a11
= aij −li1a1j,
b(2)
i
= bi −li1b1,
i, j = 2 : n.
This is a system of (n −1) equations in the (n −1) unknowns x2, . . . , xn. All following
steps are similar. In step k, k = 1 : n −1, if a(k)
kk ̸= 0, we eliminate xk from the last (n −k)
equations giving a system containing only xk+1, . . . , xn. We take lik = a(k)
ik /a(k)
kk , and the
elements of the new system are given by
a(k+1)
ij
= a(k)
ij −
a(k)
ik a(k)
kj
a(k)
kk
= a(k)
ij −lika(k)
kj ,
(1.3.7)
b(k+1)
i
= b(k)
i
−likb(k)
k ,
i, j = k + 1 : n.
(1.3.8)
The diagonal elements a11, a(2)
22 , . . . , a(n)
n,n, which appear in the denominator in (1.3.7)
during the elimination are called pivotal elements. As long as these are nonzero, the
elimination can be continued. After (n −1) steps we get the single equation
a(n)
nn xn = b(n)
n .
Collecting the ﬁrst equation from each step we get


a(1)
11
a(1)
12
· · ·
a(1)
1n
a(2)
22
· · ·
a(2)
2n
...
...
a(n)
nn




x1
x2
...
xn

=


b(1)
1
b(2)
2...
b(n)
n

,
(1.3.9)
where we have introduced the notations a(1)
ij
= aij, b(1)
i
= bi for the coefﬁcients in the
originalsystem. Thus(1.3.4)hasbeenreducedtoanequivalentnonsingular, uppertriangular
system (1.3.9), which can be solved by back-substitution.

32
Chapter 1. Principles of Numerical Calculations
We remark that no extra memory space is needed to store the multipliers. When
lik = a(k)
ik /a(k)
kk is computed the element a(k+1)
ik
becomes equal to zero, so the multipliers
can be stored in the lower triangular part of the matrix. Note also that if the multipliers
lik are saved, then the operations on the vector b can be carried out at a later stage. This
observation is important in that it shows that when solving several linear systems with the
same matrix A but different right-hand sides,
AX = B,
X = (x1, . . . , xp),
B = (b1, . . . , bp),
the operations on A only have to be carried out once.
We now show another interpretation of Gaussian elimination. For notational conve-
nience we assume that m = n and that Gaussian elimination can be carried out without
pivoting. Then Gaussian elimination can be interpreted as computing the factorization
A = LU of the matrix A into the product of a unit lower triangular matrix L and an upper
triangular matrix U.
Depending on whether the element aij lies on, above, or below the principal diagonal
we have
a(n)
ij =

. . . = a(i+1)
ij
= a(i)
ij ,
i ≤j;
. . . = a(j+1)
ij
= 0,
i > j.
Thus the elements aij, 1 ≤i, j ≤n, are transformed according to
a(k+1)
ij
= a(k)
ij −lika(k)
kj ,
k = 1 : p,
p = min(i −1, j).
(1.3.10)
If these equations are summed for k = 1 : p, we obtain
p

k=1

a(k+1)
ij
−a(k)
ij

= a(p+1)
ij
−aij = −
p

k=1
lika(k)
kj .
This can also be written
aij =



a(i)
ij +
i−1

k=1
lika(k)
kj ,
i ≤j,
0 +
j

k=1
lika(k)
kj ,
i > j,
or, if we deﬁne lii = 1, i = 1 : n,
aij =
r

k=1
likukj,
ukj = a(k)
kj ,
r = min(i, j).
(1.3.11)
However, these equations are equivalent to the matrix equation
A = LU,
L = (lik),
U = (ukj).
Here L and U are lower and upper triangular matrices, respectively.
Hence Gaussian
elimination computes a factorization of A into a product of a lower and an upper triangular

1.3. Matrix Computations
33
matrix, the LU factorization of A. Note that since the unit diagonal elements in L need
not be stored, it is possible to store the L and U factors in an array of the same dimensions
as A.
Algorithm 1.2. LU Factorization.
Given a matrix A = A(1) ∈Rn×n and a vector b = b(1) ∈Rn, the following algorithm
computes the elements of the reduced system of upper triangular form (1.3.9). It is assumed
that a(k)
kk ̸= 0, k = 1 : n:
for k = 1 : n −1
for i = k + 1 : n
lik := a(k)
ik /a(k)
kk ; a(k+1)
ik
:= 0;
for j = k + 1 : n
a(k+1)
ij
:= a(k)
ij −lika(k)
kj ;
end
end
end
Although the LU factorization is just a different interpretation of Gaussian elimination
it turns out to have important conceptual advantages. It divides the solution of a linear system
into two independent steps:
1. the factorization A = LU,
2. solution of the systems Ly = b and Ux = y.
The LU factorization is a prime example of the decompositional approach to matrix
computation. This approach came into favor in the 1950s and early 1960s and has been
named as one of the ten algorithms having the most inﬂuence on science and engineering
in the twentieth century. This interpretation of Gaussian elimination has turned out to be
very fruitful. For example, it immediately follows that the inverse of A (if it exists) has the
factorization
A−1 = (LU)−1 = U −1L−1.
This shows that the solution of linear system Ax = b,
x = A−1b = U −1(L−1b),
can be computed by solving the two triangular systems Ly = b, Ux = y. Indeed it has
been said (see Forsythe and Moler [124]) that “Almost anything you can do with A−1 can
be done without it.”
Another example is the problem of solving the transposed system AT x = b. Since
AT = (LU)T = U T LT ,

34
Chapter 1. Principles of Numerical Calculations
we have that AT x = U T (LT x) = b. It follows that x can be computed by solving the two
triangular systems
U T c = b,
LT x = c.
(1.3.12)
In passing we remark that Gaussian elimination is also an efﬁcient algorithm for
computing the determinant of a matrix A. It can be shown that the value of the determinant
is unchanged if a row (column) multiplied by a scalar is added to another row (column)
(see (A.2.4) in the Online Appendix). Further, if two rows (columns) are interchanged, the
value of the determinant is multiplied by (−1). Since the determinant of a triangular matrix
equals the product of the diagonal elements it follows that
det(A) = det(L) det(U) = det(U) = (−1)qa(1)
11 a(2)
22 · · · a(n)
nn ,
(1.3.13)
where q is the number of row interchanges performed.
From Algorithm 1.2 it follows that (n −k) divisions and (n −k)2 multiplications and
additions are used in step k to transform the elements of A. Afurther (n−k) multiplications
and additions are used to transform the elements of b. Summing over k and neglecting low
order terms, we ﬁnd that the number of ﬂops required for the reduction of Ax = b to a
triangular system by Gaussian elimination is
n−1

k=1
2(n −k)2 ≈2n3/3,
n−1

k=1
2(n −k) ≈n2
for the transformation of A and the right-hand side b, respectively. Comparing this with the
n2 ﬂops needed to solve a triangular system we conclude that, except for very small values
of n, the LU factorization of A dominates the work in solving a linear system. If several
linear systems with the same matrix A but different right-hand sides are to be solved, then
the factorization needs to be performed only once.
Example 1.3.3.
Linear systems where the matrix A has only a few nonzero diagonals often arise. Such
matrices are called band matrices. In particular, band matrices of the form
A =


b1
c1
a1
b2
c2
...
...
...
an−2
bn−1
cn−1
an−1
bn


(1.3.14)
are called tridiagonal. Tridiagonal systems of linear equations can be solved by Gaussian
elimination with much less work than the general case. The following algorithm solves the
tridiagonal system Ax = g by Gaussian elimination without pivoting.
First compute the LU factorization A = LU, where
L =


1
γ1
1
γ2
1
...
...
γn−1
1


,
U =


β1
c1
β2
c2
...
...
βn−1
cn−1
βn


.

1.3. Matrix Computations
35
The new elements in L and U are obtained from the recursion: Set β1 = b1, and
γk = ak/βk,
βk+1 = bk+1 −γkck,
k = 1 : n −1.
(1.3.15)
(Check this by computing the product LU.) The solution to Ax = L(Ux) = g is then
obtained in two steps. First a forward-substitution to get y = Ux,
y1 = g1,
yk+1 = gk+1 −γkyk,
k = 1 : n −1,
(1.3.16)
followed by a backward recursion for x,
xn = yn/βn,
xk = (yk −ckxk+1)/βk,
k = n −1 : −1 : 1.
(1.3.17)
In this algorithm the LU factorization requires only about n divisions and n multiplications
and additions. The solution of the lower and upper bidiagonal systems require about twice
as much work.
Stability of Gaussian Elimination
If A is nonsingular, then Gaussian elimination can always be carried through provided row
interchanges are allowed. In this more general case, Gaussian elimination computes an LU
factorization of the matrix ˜A obtained by carrying out all row interchanges on A. In practice
row interchanges are needed to ensure the numerical stability of Gaussian elimination. We
now consider how the LU factorization has to be modiﬁed when such interchanges are
incorporated.
Consider the case when in step k of Gaussian elimination a zero pivotal element is
encountered, i.e., a(k)
kk = 0. (The equations may have been reordered in previous steps, but
we assume that the notations have been changed accordingly.) If A is nonsingular, then in
particular its ﬁrst k columns are linearly independent. This must also be true for the ﬁrst k
columns of the reduced matrix, and hence some element a(k)
ik , i = k : n, must be nonzero, say
a(k)
rk ̸= 0. By interchanging rows k and r this element can be taken as pivot and it is possible
to proceed with the elimination. The important conclusion is that any nonsingular system
of equations can be reduced to triangular form by Gaussian elimination, if appropriate row
interchanges are used.
Note that when rows are interchanged in A the same interchanges must be made in
the elements of the right-hand side b. Also, the computed factors L and U will be the same
as if the row interchanges are ﬁrst performed on A and the Gaussian elimination performed
without interchanges. Row interchanges can be expressed as premultiplication with certain
matrices, which we now introduce.
A permutation matrix P ∈Rn×n is a matrix whose columns are a permutation of
the columns of the unit matrix, that is,
P = (ep1, . . . , epn),
where (p1, . . . , pn) is a permutation of (1, . . . , n). Notice that in a permutation matrix every
row and every column contains just one unity element. The transpose P T of a permutation
matrix is therefore again a permutation matrix. A permutation matrix is orthogonal P T P =
I, and hence P T affects the reverse permutation.

36
Chapter 1. Principles of Numerical Calculations
A transposition matrix is a special permutation matrix,
Iij = (. . . , ei−1, ej, ei+1, . . . , ej−1, ei, ej+1),
which equals the identity matrix except with columns i and j interchanged. By construction
it immediately follows that Iij is symmetric. I 2
ij = I and hence I −1
ij
= Iij. If a matrix A is
premultiplied by Iij, this results in the interchange of rows i and j. Any permutation matrix
can be expressed as a product of transposition matrices.
If P is a permutation matrix, then PA is the matrix A with its rows permuted. Hence,
Gaussian elimination with row interchanges produces a factorization, which in matrix no-
tations can be written
PA = LU,
where P is a permutation matrix. Note that P is uniquely represented by the integer vector
(p1, . . . , pn) and need never be stored as a matrix.
Assume that in the kth step, k = 1 : n −1, we select the pivot element from row pk,
and interchange the rows k and pk. Notice that in these row interchanges also, previously
computed multipliers lij must take part. At completion of the elimination, we have obtained
lower and upper triangular matrices L and U. We now make the important observation
that these are the same triangular factors that are obtained if we ﬁrst carry out the row
interchanges k ↔pk, k = 1 : n −1, on the original matrix A to get a matrix PA, where
P is a permutation matrix, and then perform Gaussian elimination on PA without any
interchanges. This means that Gaussian elimination with row interchanges computes the
LU factors of the matrix PA. We now summarize the results and prove the uniqueness of
the LU factorization.
To ensure the numerical stability in Gaussian elimination, except for special classes
of linear systems, it will be necessary to perform row interchanges not only when a pivotal
element is exactly zero. Usually it sufﬁces to choose the pivotal element in step k as the
element of largest magnitude in the unreduced part of the kth column. This is called partial
pivoting.
Example 1.3.4.
The linear system

ϵ
1
1
1
 
x1
x2

=

1
0

is nonsingular for any ϵ ̸= 1 and has the unique solution x1 = −x2 = −1/(1 −ϵ). But
when ϵ = 0 the ﬁrst step in Gaussian elimination cannot be carried out. The remedy here is
obviously to interchange the two equations, which directly gives an upper triangular system.
Suppose that in the system above ϵ = 10−4. Then the exact solution, rounded to four
decimals, equals x = (−1.0001, 1.0001)T . But if Gaussian elimination is carried through
without interchanges, we obtain l21 = 104 and the triangular system
0.0001x1 + x2 = 1,
(1 −104)x2 = −104.
Suppose that the computation is performed using arithmetic with three decimal digits. Then
in the last equation the coefﬁcient a(2)
22 will be rounded to −104 and the solution computed
by back-substitution is ¯x2 = 1.000, ¯x1 = 0, which is a catastrophic result.

1.3. Matrix Computations
37
If before performing Gaussian elimination we interchange the two equations, then we
get l21 = 10−4 and the reduced system becomes
x1 + x2 = 0,
(1 −10−4)x2 = 1.
The coefﬁcient a(2)
22 is now rounded to 1, and the computed solution becomes ¯x2 = 1.000,
¯x1 = −1.000, which is correct to the precision carried.
In this simple example it is easy to see what went wrong in the elimination without
interchanges. The problem is that the choice of a small pivotal element gives rise to large
elements in the reduced matrix and the coefﬁcient a22 in the original system is lost through
rounding. Rounding errors which are small when compared to the large elements in the
reduced matrix are unacceptable in terms of the original elements. When the equations are
interchanged the multiplier is small and the elements of the reduced matrix are of the same
size as in the original matrix.
In general an algorithm is said to be backward stable (see Deﬁnition 2.4.10) if the
computed solution w equals the exact solution of a problem with “slightly perturbed data.”
The more or less ﬁnal form of error analysis of Gaussian elimination was given by J. H.
Wilkinson [375].12
Wilkinson showed that the computed triangular factors L and U of A obtained by
Gaussian elimination are the exact triangular factors of a slightly perturbed matrix A + E.
He further gave bounds on the elements of E. The essential condition for stability is that
no substantial growth occurs in the elements in L and U; see Theorem 2.4.12. Although
matrices can be constructed for which the element growth factor in Gaussian elimination
with partial pivoting equals 2n−1, quoting Kahan [219] we say that: “Intolerable pivot-
growth (with partial pivoting) is a phenomenon that happens only to numerical analysts
who are looking for that phenomenon.” Why large element growth rarely occurs in practice
with partial pivoting is a subtle and still not fully understood phenomenon. Trefethen and
Schreiber [360] show that for certain distributions of random matrices the average element
growth is close to n2/3 for partial pivoting.
It is important to note that the fact that a problem has been solved by a backward stable
algorithm does not mean that the error in the computed solution is small. If the matrix A is
close to a singular matrix, then the solution is very sensitive to perturbations in the data. This
is the case when the rows (columns) of A are almost linearly dependent. But this inaccuracy
is intrinsic to the problem and cannot be avoided except by using higher precision in the
calculations. Condition numbers for linear systems are discussed in Sec. 2.4.2.
An important special case of LU factorization is when the matrix A is symmetric,
AT = A, and positive deﬁnite, i.e.,
xT Ax > 0
∀x ∈Rn,
x ̸= 0.
(1.3.18)
Similarly A is said to be positive semideﬁnite if xT Ax ≥0 for all x ∈Rn. Otherwise it is
called indeﬁnite.
12James Hardy Wilkinson (1919–1986), English mathematician who graduated from Trinity College, Cam-
bridge. He became Alan Turing’s assistant at the National Physical Laboratory in London in 1946, where he
worked on the ACE computer project. He did pioneering work on numerical methods for solving linear systems
and eigenvalue problems, and developed software and libraries of numerical routines.

38
Chapter 1. Principles of Numerical Calculations
For symmetric positive deﬁnite matrices there always exists a unique factorization
A = RTR,
(1.3.19)
where R is an upper triangular matrix with positive diagonal elements. An important fact
is that no pivoting is needed for stability. Indeed, unless the pivots are chosen from the
diagonal, pivoting is harmful since it will destroy symmetry.
This is called the Cholesky factorization.13
The elements in the Cholesky factor
R = (rij) can be determined directly. The matrix equation A = RTR with R upper triangular
can be written elementwise as
aij =
i
k=1
rkirkj =
i−1

k=1
rkirkj + riirij,
1 ≤i ≤j ≤n.
(1.3.20)
These are n(n + 1)/2 equations for the unknown elements in R. Solving for rij from the
corresponding equation in (1.3.20), we obtain
rij =

aij −
i−1

k=1
rkirkj

rii,
i < j,
rjj =

ajj −
j−1

k=1
r2
kj
1/2
.
(1.3.21)
If properly sequenced, these equations can be used in a recursive fashion to compute the
elements in R, for example, one row at a time. The resulting algorithm requires n square
roots and approximately n3/3 ﬂops, which is about half the work of an LU factorization.
We remark that the Cholesky factorization can be carried out also for a symmetric
indeﬁnite matrix, if at each step a positive pivot is chosen from the diagonal. However, for
a symmetric indeﬁnite matrix, such as the matrix in Example 1.3.4 with ϵ < 1, no Cholesky
factorization can exist.
1.3.3
Sparse Matrices and Iterative Methods
Following Wilkinson and Reinsch [381], a matrix A will be called sparse if the percentage
of zero elements is large and its distribution is such that it is economical to take advantage of
their presence. The nonzero elements of a sparse matrix may be concentrated on a narrow
band centered on the diagonal. Alternatively, they may be distributed in a less systematic
manner.
Large sparse linear systems arise in numerous areas of application, such as the numeri-
cal solution of partial differential equations, mathematical programming, structural analysis,
chemical engineering, electrical circuits, and networks. Large could imply a value of n in
the range 1000–1,000,000. Figure 1.3.1 shows a sparse matrix of order n = 479 with 1887
nonzero elements (or 0.9%) that arises from a model of an eight stage chemical distillation
column.
The ﬁrst task in solving a sparse system by Gaussian elimination is to permute the rows
and columns so that not too many new nonzero elements are created during the elimination.
13André-Louis Cholesky (1875–1918), a French military ofﬁcer involved in geodesy and surveying in Crete and
North Africa just before World War One.

1.3. Matrix Computations
39
0 
50 
100 
150 
200 
250 
300 
350 
400 
450 
0 
50 
100 
150 
200 
250 
300 
350 
400 
450 
nz = 1887 
Figure 1.3.1. Nonzero pattern of a sparse matrix from an eight stage chemical
distillation column.
Equivalently, we want to choose permutation matrices P and Q such that the LU factors of
P AQ are as sparse as possible. Such a reordering will usually nearly minimize the number
of arithmetic operations.
To ﬁnd an optimal ordering which minimizes the number of nonzero elements in L
and U is unfortunately an intractable problem, because the number of possible orderings
of rows and columns are (n!)2. Fortunately, there are heuristic ordering algorithms which
do a good job. In Figure 1.3.2 we show the reordered matrix PAQ and its LU factors.
Here L and U contain together 5904 nonzero elements or about 2.6%. The column ordering
was obtained using a MATLAB version of the so-called column minimum degree ordering.
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 1887
0
50
100
150
200
250
300
350
400
450
0
50
100
150
200
250
300
350
400
450
nz = 5904
Figure 1.3.2. Nonzero structure of the matrix A (left) and L + U (right).

40
Chapter 1. Principles of Numerical Calculations
For the origin and details of this code we refer to Gilbert, Moler, and Schreiber [156].
We remark that, in general, some kind of stability check on the pivot elements must be
performed during the factorization.
For many classes of sparse linear systems iterative methods are more efﬁcient to
use than direct methods such as Gaussian elimination. Typical examples are those arising
when a differential equation in two or three dimensions is discretized. In iterative methods
a sequence of approximate solutions is computed, which in the limit converges to the exact
solution x. Basic iterative methods work directly with the original matrix A and therefore
have the added advantage of requiring only extra storage for a few vectors.
In a classical iterative method due to Richardson [302], starting from x(0) = 0, a
sequence x(k) is deﬁned by
x(k+1) = x(k) + ω(b −Ax(k)),
k = 0, 1, 2, . . . ,
(1.3.22)
where ω > 0 is a parameter to be chosen. It follows easily from (1.3.22) that the error in
x(k) satisﬁes x(k+1) −x = (I −ωA)(x(k) −x), and hence
x(k) −x = (I −ωA)k(x(0) −x).
It can be shown that, if all the eigenvalues λi of A are real and satisfy
0 < a ≤λi ≤b,
then x(k) will converge to the solution, when k →∞, for 0 < ω < 2/b.
Iterative methods are used most often for the solution of very large linear systems,
which typically arise in the solution of boundary value problems of partial differential
equations by ﬁnite difference or ﬁnite element methods. The matrices involved can be
huge, sometimes involving several million unknowns. The LU factors of matrices arising
in such applications typically contain orders of magnitude more nonzero elements than
A itself. Hence, because of the storage and number of arithmetic operations required,
Gaussian elimination may be far too costly to use. In a typical problem for the Poisson
equation (1.1.21) the function is to be determined in a plane domain D, when the values
of u are given on the boundary ∂D. Such boundary value problems occur in the study
of steady states in most branches of physics, such as electricity, elasticity, heat ﬂow, and
ﬂuid mechanics (including meteorology). Let D be a square grid with grid size h, i.e.,
xi = x0 + ih, yj = y0 + jh, 0 ≤i ≤N + 1, 0 ≤j ≤N + 1. Then the difference
approximation yields
ui,j+1 + ui−1,j + ui+1,j + ui,j−1 −4ui,j = h2f (xi, yj)
(1 ≤i, j ≤N). This is a huge system of linear algebraic equations; one equation for each in-
teriorgridpoint, altogetherN2 unknownsandequations. (Notethatui,0, ui,N+1, u0,j, uN+1,j
are known boundary values.) To write the equations in matrix-vector form we order the
unknowns in a vector,
u = (u1,1, . . . , u1,N, u2,1, . . . , u2,N, . . . , uN,1, . . . , uN,N),
the so-called natural ordering. If the equations are ordered in the same order we get a system
Au = b, where A is symmetric with all nonzero elements located in ﬁve diagonals; see
Figure 1.3.3 (left).

1.3. Matrix Computations
41
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
nz = 1958
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
nz = 15638
Figure 1.3.3. Structure of the matrix A (left) and L + U (right) for the Poisson
problem, N = 20 (rowwise ordering of the unknowns).
In principle Gaussian elimination can be used to solve such systems. But even taking
symmetry and the banded structure into account, this would require 1
2 ·N4 multiplications,
since in the LU factors the zero elements inside the outer diagonals will ﬁll in during the
elimination, as shown in Figure 1.3.3 (right).
The linear system arising from the Poisson equation has several features common to
boundary value problems for other linear partial differential equations. One of these is that
only a tiny fraction of the elements in each row of A are nonzero. Therefore, each iteration
in Richardson’s method requires only about kN2 multiplications, i.e., k multiplications per
unknown. Using iterative methods which take advantage of the sparsity and other features
does allow the efﬁcient solution of such systems. This becomes even more essential for
three-dimensional problems.
As early as 1954, a simple atmospheric model was used for weather forecasting on an
electronic computer. The net covered most of NorthAmerica and Europe. During a 48 hour
forecast, the computer solved (among other things) 48 Poisson equations (with different
right-hand sides). This would have been impossible at that time if the special features of
the system had not been used.
1.3.4
Software for Matrix Computations
In most computers in use today the key to high efﬁciency is to avoid as much as possible
data transfers between memory, registers, and functional units, since these can be more
costly than arithmetic operations on the data. This means that the operations have to be
carefully structured. One observation is that Gaussian elimination consists of three nested
loops, which can be ordered in 3 · 2 · 1 = 6 ways. Disregarding the right-hand side vector
b, each version does the operations
a(k+1)
ij
:= a(k)
ij −a(k)
kj a(k)
ik /a(k)
kk ,

42
Chapter 1. Principles of Numerical Calculations
and only the ordering in which they are done differs. The version given above uses row
operations and may be called the “kij” variant, where k refers to step number, i to row
index, and j to column index. This version is not suitable for programming languages like
Fortran 77, in which matrix elements are stored sequentially by columns. In such a language
the form kji should be preferred, as well as a column oriented back-substitution rather than
that in Algorithm 1.1.
The ﬁrst collection of high quality linear algebra software was a series of algorithms
written in Algol 60 that appeared in Wilkinson and Reinsch [381]. This contains 11 sub-
routines for linear systems, least squares, and linear programming, and 18 routines for the
algebraic eigenvalue problem.
The Basic Linear Algebra Subprograms (BLAS) have become an important tool for
structuring linear algebra computations. These are now commonly used to formulate matrix
algorithms and have become an aid to clarity, portability, and modularity in modern software.
The original set of BLAS [239], introduced in 1979, identiﬁed frequently occurring vector
operations in matrix computation such as scalar product, adding of a multiple of one vector
to another, etc. For example, the operation
y := αx + y
in single precision is named SAXPY. By carefully optimizing them for each speciﬁc com-
puter, performance was enhanced without sacriﬁcing portability. These BLAS were adopted
in the collections of Fortran subroutines LINPACK (see [101]) for linear systems and
EISPACK (see [133]) for eigenvalue problems.
For modern computers it is important to avoid excessive data movements between
differentpartsofmemoryhierarchy. Toachievethis, so-calledlevel3BLASwereintroduced
in the 1990s. These work on blocks of the full matrix and perform, for example, the
operations
C := αAB + βC,
C := αAT B + βC,
C := αABT + βC.
Level 3 BLAS use O(n2) data but perform O(n3) arithmetic operations. This gives a
surface-to-volume effect for the ratio of data movement to operations.
LAPACK (see [6]) is a linear algebra package initially released in 1992. LAPACK
was designed to supersede and integrate the algorithms in both LINPACK and EISPACK.
It achieves close to optimal performance on a large variety of computer architectures by
expressing as much of the algorithm as possible as calls to level 3 BLAS. This is also an
aid to clarity, portability, and modularity. LAPACK today is the backbone of the interactive
matrix computing system MATLAB.
Example 1.3.5.
In 1974 the authors wrote in [89, Sec. 8.5.3] that “a full 1000 × 1000 system of
equations is near the limit at what can be solved at a reasonable cost.” Today systems of
this size can easily be handled on a personal computer. The benchmark problem for the
Japanese Earth Simulator, one of the world’s fastest computers in 2004, was the solution
of a system of size 1,041,216 on which a speed of 35.6 × 1012 operations per second was
measured. This is a striking illustration of the progress in high speed matrix computing that
has occurred in these 30 years!

Problems and Computer Exercises
43
Review Questions
1.3.1 How many operations are needed (approximately) for
(a) the multiplication of two square matrices A, B ∈Rn×n?
(b) the LU factorization of a matrix A ∈Rn×n?
(c) the solution of Ax = b, when the triangular factorization of A is known?
1.3.2 Show that if the kth diagonal entry of an upper triangular matrix is zero, then its ﬁrst
k columns are linearly dependent.
1.3.3 What is the LU factorization of an n by n matrix A, and how is it related to Gaussian
elimination? Does it always exist? If not, give sufﬁcient conditions for its existence.
1.3.4 (a) For what type of linear systems are iterative methods to be preferred to Gaussian
elimination?
(b) Describe Richardson’s method for solving Ax = b. What can you say about the
error in successive iterations?
1.3.5 What does the acronym BLAS stand for? What is meant by level 3 BLAS, and why
are they used in current linear algebra software?
Problems and Computer Exercises
1.3.1 Let A be a square matrix of order n and k a positive integer such that 2p ≤k < 2p+1.
Show how Ak can be computed in at most 2pn3 multiplications.
Hint: Write k in the binary number system and compute A2, A4, A8, . . . , by succes-
sive squaring; e.g., 13 = (1101)2 and A13 = A8A4A.
1.3.2 (a) Let A and B be square upper triangular matrices of order n. Show that the product
matrix C = AB is also upper triangular. Determine how many multiplications are
needed to compute C.
(b) Show that if R is an upper triangular matrix with zero diagonal elements, then
Rn = 0.
1.3.3 Show that there cannot exist an LU factorization
A =

0
1
1
1

=

l11
0
l21
l22
 
u11
u12
0
u22

.
Hint: Equate the (1, 1)-elements and deduce that either the ﬁrst row or the ﬁrst column
in LU must be zero.
1.3.4 (a) Consider the special upper triangular matrix of order n,
Un(a) =


1
a
a
· · ·
a
1
a
· · ·
a
1
· · ·
a
...
...
1


.

44
Chapter 1. Principles of Numerical Calculations
Determine the solution x to the triangular system Un(a)x = en, where en = (0, 0, . . . ,
0, 1)T is the nth unit vector.
(b) Show that the inverse of an upper triangular matrix is also upper triangular. Deter-
mine for n = 3 the inverse of Un(a). Try also to determine Un(a)−1 for an arbitrary n.
Hint: Note that UU −1 = U −1U = I, the identity matrix.
1.3.5 A matrix Hn of order n such that hij = 0 whenever i > j + 1 is called an upper
Hessenberg matrix. For n = 5 it has the structure
H5 =


h11
h12
h13
h14
h15
h21
h22
h23
h24
h25
0
h32
h33
h34
h35
0
0
h43
h44
h45
0
0
0
h54
h55

.
(a) Determine the approximate number of operations needed to compute the LU
factorization of Hn without pivoting.
(b) Determine the approximate number of operations needed to solve the linear system
Hnx = b, when the factorization in (a) is given.
1.3.6 Compute the product |L| |U| for the LU factors with and without pivoting of the
matrix in Example 1.3.4. (Here |A| denotes the matrix with elements |aij|.)
1.3.7 Let A ∈Rn×n be a given matrix. Show that if Ax = y has at least one solution
for any y ∈Rn, then it has exactly one solution for any y ∈Rn. (This is a useful
formulation for showing uniqueness of approximation formulas.)
1.4
The Linear Least Squares Problem
A basic problem in science is to ﬁt a mathematical model to given observations subject to
errors. As an example, consider observations (ti, yi), i = 1 : m, to be ﬁtted to a model
described by a scalar function y(t) = f (c, t), where c ∈Rn is a parameter vector to be
determined. There are two types of shortcomings to take into account: errors in the input
data, and approximations made in the particular model (class of functions, form). We shall
call these measurement errors and errors in the model, respectively.
Clearly the more observations that are available the more accurately will it be possible
to determine the parameters in the model. One can also see this problem as analogous to
the task of a communication engineer, to ﬁlter away noise from the signal. These ques-
tions are connected with both mathematical statistics and the mathematical discipline of
approximation theory.
A simple example is when the model is linear in c and of the form
y(t) =
n

j=1
cjφj(t),
where φj(t) are given (possibly nonlinear) functions. Given m > n measurements the
resulting equations
yi =
n

j=1
cjφj(ti),
i = 1 : m,

1.4. The Linear Least Squares Problem
45
form an overdetermined linear system. If we set A ∈Rm×n, aij = φj(ti), then the system
can be written in matrix form as Ac = y. In general such a system is inconsistent and has
no solution. But we can try to ﬁnd a vector c ∈Rn such that Ac is the “best” approximation
to y. This is equivalent to minimizing the size of the residual vector r = y −Ac.
There are many possible ways of deﬁning the best solution to such an inconsistent
linear system. A choice which can often be motivated for statistical reasons, and which
also leads to a simple computational problem, is to take as the solution a vector c, which
minimizes the sum of the squared residuals, that is,
min
x
m

i=1
r2
i = min
x
∥y −Ac∥2
2.
(1.4.1)
Here we have used the notation
∥x∥2 = (|x1|2 + · · · + |xn|2)1/2 = (xT x)1/2
for the Euclidean length of a vector x (see OnlineAppendixA). We call (1.4.1) a linear least
squares problem and any minimizer x a least squares solution of the system Ax = b.
Gauss claims to have discovered the method of least squares in 1795 when he was 18
years old. He used it in 1801 to successively predict the orbit of the asteroid Ceres.
1.4.1
Basic Concepts in Probability and Statistics
We now introduce, without proofs, some basic concepts, formulas, and results from statistics
which will be used later. Proofs may be found in most texts on these subjects.
The distribution function of a random variable X is denoted by the nonnegative and
nondecreasing function
F(x) = Pr{X ≤x},
F(−∞) = 0,
F(∞) = 1.
If F(x) is differentiable, the (probability) density function14 is f (x) = F ′(x). Note that
f (x) ≥0,

R
f (x) dx = 1
and
Pr{X ∈[x, x + 4x]} = f (x) 4x + o(4x).
In the discrete case X can only take on discrete values xi, i = 1 : N, and
Pr{X = xi} = pi,
i = 1 : N,
where pi ≥0 and 
i pi = 1.
14In old literature a density function is often called a frequency function. The term cumulative distribution is also
used as a synonym for distribution function. Unfortunately, distribution or probability distribution is sometimes
used in the meaning of a density function.

46
Chapter 1. Principles of Numerical Calculations
The mean or the expectation of X is
E(X) =



 ∞
−∞
xf (x) dx,
continuous case,
N

i=1
pixi,
discrete case.
The variance of X equals
σ 2 = var(X) = E((X −µ)2),
µ = E(X),
and σ = √var(X) is the standard deviation. The mean and standard deviation are fre-
quently used as measures of the center and spread of a distribution.
Let Xk, k = 1 : n, be random variables with mean values µk. Then the covariance
matrix is V = {σjk}n
j,k=1, where
σjk = cov(Xj, Xk) = E((Xj −µj)(Xk −µk))
= E(Xj)E(Xk) −µjµk.
If cov(Xj, Xk) = 0, then Xj and Xk are said to be uncorrelated.
If the random variables Xk, k = 1 : n, are mutually uncorrelated, then V is a diagonal
matrix.
Some formulas for the estimation of mean, standard deviation, etc. from results of sim-
ulation experiments or other statistical data are given in the computer exercises of Sec. 2.3.
1.4.2
Characterization of Least Squares Solutions
Let y ∈Rm be a vector of observations that is related to a parameter vector c ∈Rn by the
linear relation
y = Ac + ϵ,
A ∈Rm×n,
(1.4.2)
where A is a known matrix of full column rank and ϵ ∈Rm is a vector of random errors. We
assume here that ϵi, i = 1 : m, has zero mean and that ϵi and ϵj, i ̸= j, are uncorrelated,
i.e.,
E(ϵ) = 0,
V (ϵ) = σ 2I.
The parameter c is then a random vector which we want to estimate in terms of the known
quantities A and y. Let yT c be a linear functional of the parameter c in (1.4.2). We say
that θ = θ(A, y) is an unbiased linear estimator of yT c if E(θ) = yT c. It is a best linear
unbiased estimator if θ has the smallest variance among all such estimators.
The following theorem15 places the method of least squares on a sound theoretical
basis.
15This theorem is originally due to C. F. Gauss (1821). His contribution was somewhat neglected until redis-
covered by the Russian mathematician A. A. Markov in 1912.

1.4. The Linear Least Squares Problem
47
Theorem 1.4.1 (Gauss–Markov Theorem).
Consider a linear model (1.4.2), where ϵ is an uncorrelated random vector with zero
mean and covariance matrix V = σ 2I. Then the best linear unbiased estimator of any
linear functional yT c is yT ˆc, where
ˆc = (ATA)−1AT y
(1.4.3)
is the least squares estimator obtained by minimizing the sum of squares ∥f −Ac∥2
2. Fur-
thermore, the covariance matrix of the least squares estimate ˆc equals
V (ˆc) = σ 2(ATA)−1,
(1.4.4)
and
s2 = ∥y −Aˆc∥2
2/(m −n)
(1.4.5)
is an unbiased estimate of σ 2, i.e. E(s2) = σ 2.
Proof. See Zelen [389, pp. 560–561].
The set of all least squares solutions can also be characterized geometrically. For this
purpose we introduce two fundamental subspaces of Rm, the range of A and the null space
of AT , deﬁned by
R(A) = {z ∈Rm| z = Ax, x ∈Rn},
(1.4.6)
N(AT ) = {y ∈Rm| AT y = 0}.
(1.4.7)
If z ∈R(A) and y ∈N(AT ), then zT y = xT AT y = 0, which shows that N(AT ) is the
orthogonal complement of R(A).
By the Gauss–Markov theorem any least squares solution to an overdetermined linear
system Ax = b satisﬁes the normal equations
ATAx = AT b.
(1.4.8)
The normal equations are always consistent, since the right-hand side satisﬁes
AT b ∈R(AT ) = R(ATA).
Therefore, a least squares solution always exists, although it may not be unique.
Theorem 1.4.2.
The vector x minimizes ∥b −Ax∥2 if and only if the residual vector r = b −Ax is
orthogonal to R(A) or, equivalently,
AT (b −Ax) = 0.
(1.4.9)
Proof. Let x be a vector for which AT (b −Ax) = 0. For any y ∈Rn, it holds that
b −Ay = (b −Ax) + A(x −y). Squaring this and using (1.4.9) we obtain
∥b −Ay∥2
2 = ∥b −Ax∥2
2 + ∥A(x −y)∥2
2 ≥∥b −Ax∥2
2,
where equality holds only if A(x −y) = 0.

48
Chapter 1. Principles of Numerical Calculations
Now assume that AT (b −Ax) = z ̸= 0. If x −y = −ϵz, we have for sufﬁciently
small ϵ ̸= 0
∥b −Ay∥2
2 = ∥b −Ax∥2
2 + ϵ2∥Az∥2
2 −2ϵ(Az)T (b −Ax)
= ∥b −Ax∥2
2 + ϵ2∥Az∥2
2 −2ϵ∥z∥2
2 < ∥b −Ax∥2
2,
and thus x does not minimize ∥b −Ax∥2.
From Theorem 1.4.2 it follows that a least squares solution x decomposes the right-
hand side into two orthogonal components
b = Ax + r,
r = b −Ax ∈N(AT ),
Ax ∈R(A).
(1.4.10)
This geometric interpretation is illustrated in Figure 1.4.1. Note that although the solution
x to the least squares problem may not be unique, the decomposition (1.4.10) always is
unique.
✶
✕
✲
Ax
b
r = b −Ax
R(A)
Figure 1.4.1. Geometric characterization of the least squares solution.
We now give a necessary and sufﬁcient condition for the least squares solution to be
unique.
Theorem 1.4.3.
The matrix ATA is positive deﬁnite and hence nonsingular if and only if the columns
of A are linearly independent, that is, when rank (A) = n. In this case the least squares
solution x is unique and given by
x = (ATA)−1AT b.
(1.4.11)
Proof. If the columns of A are linearly independent, then x ̸= 0 ⇒Ax ̸= 0. Therefore,
x ̸= 0 ⇒xT ATAx = ∥Ax∥2
2 > 0, and hence ATA is positive deﬁnite.
On the other hand, if the columns are linearly dependent, then for some x0 ̸= 0 we
have Ax0 = 0. Then xT
0 ATAx0 = 0, and therefore ATA is not positive deﬁnite. When ATA
is positive deﬁnite it is also nonsingular and (1.4.11) follows.
When A has full column rank ATA is symmetric and positive deﬁnite and the normal
equations can be solved by computing the Cholesky factorization ATA = RTR. The normal

1.4. The Linear Least Squares Problem
49
equations then become RTRx = ATb, which decomposes as
RT z = AT b,
Rx = z.
The ﬁrst system is lower triangular and z is computed by forward-substitution. Then x is
computed from the second upper triangular system by back-substitution. For many practical
problems this method of normal equations is an adequate solution method, although its
numerical stability is not the best.
Example 1.4.1.
The comet Tentax, discovered in 1968, is supposed to move within the solar system.
The following observations of its position in a certain polar coordinate system have been
made:
r
2.70
2.00
1.61
1.20
1.02
φ
48◦
67◦
83◦
108◦
126◦.
By Kepler’s ﬁrst law the comet should move in a plane orbit of elliptic or hyperbolic form,
if the perturbations from planets are neglected. Then the coordinates satisfy
r = p/(1 −e cos φ),
where p is a parameter and e the eccentricity. We want to estimate p and e by the method
of least squares from the given observations.
We ﬁrst note that if the relationship is rewritten as
1/p −(e/p) cos φ = 1/r,
it becomes linear in the parameters x1 = 1/p and x2 = e/p. We then get the linear system
Ax = b, where
A =


1.0000
−0.6691
1.0000
−0.3907
1.0000
−0.1219
1.0000
0.3090
1.0000
0.5878


,
b =


0.3704
0.5000
0.6211
0.8333
0.9804

.
The least squares solution is x = ( 0.6886
0.4839 )T giving p = 1/x1 = 1.4522 and
ﬁnally e = px2 = 0.7027.
By (1.4.10), if x is a least squares solution, then Ax is the orthogonal projection of b
onto R(A). Thus orthogonal projections play a central role in least squares problems. In
general, a matrix P1 ∈Rm×m is called a projector onto a subspace S ⊂Rm if and only if
it holds that
P1v = v ∀v ∈S,
P 2
1 = P1.
(1.4.12)

50
Chapter 1. Principles of Numerical Calculations
An arbitrary vector v ∈Rm can then be decomposed as v = P1v + P2v ≡v1 + v2, where
P2 = I −P1. In particular, if P1 is symmetric, P1 = P T
1 , we have
P T
1 P2v = P T
1 (I −P1)v = (P1 −P 2
1 )v = 0
∀v ∈Rm,
and it follows that P T
1 P2 = 0. Hence vT
1 v2 = vT P T
1 P2v = 0 for all v ∈Rm, i.e., v2 ⊥v1.
In this case P1 is the orthogonal projector onto S, and P2 = I −P1 is the orthogonal
projector onto S⊥.
In the full column rank case, rank (A) = n, of the least squares problem, the residual
r = b −Ax can be written r = b −PR(A)b, where
PR(A) = A(ATA)−1AT
(1.4.13)
is the orthogonal projector onto R(A). If rank (A) < n, then A has a nontrivial null space.
In this case if ˆx is any vector that minimizes ∥Ax −b∥2, then the set of all least squares
solutions is
S = {x = ˆx + y | y ∈N(A)}.
(1.4.14)
In this set there is a unique solution of minimum norm characterized by x ⊥N(A), which
is called the pseudoinverse solution.
1.4.3
The Singular Value Decomposition
In the past the conventional way to determine the rank of a matrix A was to compute the
row echelon form by Gaussian elimination. This would also show whether a given linear
system is consistent or not. However, in ﬂoating-point calculations it is difﬁcult to decide
if a pivot element, or an element in the transformed right-hand side, should be considered
as zero or nonzero. Such questions can be answered in a more satisfactory way by using
the singular value decomposition (SVD), which is of great theoretical and computational
importance.16
The geometrical signiﬁcance of the SVD is as follows: The rectangular matrix A ∈
Rm×n, m ≥n, represents a mapping y = Ax from Rn to Rm. The image of the unit sphere
∥x∥2 = 1 is a hyper ellipse in Rm with axes equal to σ1 ≥σ2 · · · ≥σn ≥0. In other words,
the SVD gives orthogonal bases in these two spaces such that the mapping is represented
by the generalized diagonal matrix H ∈Rm×n. This is made more precise in the following
theorem, a constructive proof of which will be given in Volume II.
Theorem 1.4.4 (Singular Value Decomposition).
Any matrix A ∈Rm×n of rank r can be decomposed as
A = UHV T ,
H =

Hr
0
0
0

∈Rm×n,
(1.4.15)
where Hr = diag (σ1, σ2, . . . , σr) is diagonal and
U = (u1, . . . , um) ∈Rm×m,
V = (v1, . . . , vn) ∈Rn×n
(1.4.16)
16The SVD was published by Eugenio Beltrami in 1873 and independently by Camille Jordan in 1874. Its use
in numerical computations is much more recent, since a stable algorithm for computing the SVD did not become
available until the publication of Golub and Reinsch [167] in 1970.

1.4. The Linear Least Squares Problem
51
are square orthogonal matrices, U T U = Im, V T V = In. Here
σ1 ≥σ2 ≥· · · ≥σr > 0
are the r ≤min(m, n) nonzero singular values of A. The vectors ui, i = 1 : m, and vj,
j = 1 : n, are left and right singular vectors. (Note that if r = n and/or r = m, some of
the zero submatrices in H disappear.)
The singular values of A are uniquely determined. For any distinct singular value
σj ̸= σi, i ̸= j, the corresponding singular vector vj is unique (up to a factor ±1). For
a singular value of multiplicity p the corresponding singular vectors can be chosen as any
orthonormal basis for the unique subspace of dimension p that they span. Once the singular
vectors vj, 1 ≤j ≤r, have been chosen, the vectors uj, 1 ≤j ≤r, are uniquely
determined, and vice versa, by
uj = 1
σj
Avj,
vj = 1
σj
AT uj,
j = 1 : r.
(1.4.17)
By transposing (1.4.15) we obtain AT = V HT U T , which is the SVD of AT . Ex-
panding (1.4.15), the SVD of the matrix A can be written as a sum of r matrices of rank
one,
A =
r

i=1
σiuivT
i .
(1.4.18)
The SVD gives orthogonal bases for the range and null space of A and AT . Suppose that
the matrix A has rank r < min(m, n). It is easy to verify that
R(A) = span (u1, . . . , ur),
N(AT ) = span (ur+1, . . . , um),
(1.4.19)
R(AT ) = span (v1, . . . , vr),
N(A) = span (vr+1, . . . , vn).
(1.4.20)
It immediately follows that
R(A)⊥= N(AT ),
N(A)⊥= R(AT );
(1.4.21)
i.e., N(AT ) is the orthogonal complement to R(A), and N(A) is the orthogonal complement
to R(AT ). This result is sometimes called the fundamental theorem of linear lgebra.
We remark that the SVD generalizes readily to complex matrices. The SVD of a
matrix A ∈Cm×n is
A = (U1 U2)H

V T
1
V T
2

,
H =

Hr
0
0
0

∈Rm×n,
(1.4.22)
where the singular values σ1, σ2, . . . , σr are real and nonnegative, and U and V are square
unitary matrices, U HU = Im, V HV = In. (Here AH denotes the conjugate transpose of A.)
The SVD can also be used to solve linear least squares problems in the case when the
columns in A are linearly dependent. Then there is a vector c ̸= 0 such that Ac = 0 and
the least squares solution is not unique, then there exists a unique least squares solution of
minimum Euclidean length, which solves the least squares problem
min
x∈S ∥x∥2,
S = {x ∈Rn| ∥b −Ax∥2 = min}.
(1.4.23)

52
Chapter 1. Principles of Numerical Calculations
In terms of the SVD (1.4.22) of A the solution to (1.4.23) can be written x = A†b, where
the matrix A† is
A† = (V1 V2)H†
 U T
1
U T
2

,
H† =
 H−1
r
0
0
0

∈Rn×m.
(1.4.24)
The matrix A† is unique and called the pseudoinverse of A, and x = A†b is the pseudoin-
verse solution. Note that problem (1.4.23) includes as special cases the solution of both
overdetermined and underdetermined linear systems.
The pseudoinverse A† is often called the Moore–Penrose inverse. Moore developed
the concept of the general reciprocal in 1920. In 1955 Penrose [288] gave an elegant
algebraic characterization and showed that X = A† is uniquely determined by the four
Penrose conditions:
(1)
AXA = A,
(2)
XAX = X,
(1.4.25)
(3)
(AX)T = AX,
(4)
(XA)T = XA.
(1.4.26)
It can be directly veriﬁed that X = A† given by (1.4.24) satisﬁes these four conditions. In
particular this shows that A† does not depend on the particular choices of U and V in the
SVD.
The orthogonal projections onto the four fundamental subspaces of A have the fol-
lowing simple expressions in terms of the SVD:
PR(A) = AA† = U1U T
1 ,
PN(AT ) = I −AA† = U2U T
2 ,
(1.4.27)
PR(AT ) = A†A = V1V T
1 ,
PN(A) = I −A†A = V2V T
2 .
These ﬁrst expressions are easily veriﬁed using the deﬁnition of an orthogonal projection
and the Penrose conditions.
1.4.4
The Numerical Rank of a Matrix
Let A be a matrix of rank r < min(m, n), and E a matrix of small random elements. Then
it is most likely that the perturbed matrix A + E has maximal rank min(m, n). However,
since A+E is close to a rank deﬁcient matrix, it should be considered as having numerical
rank equal to r. In general, the numerical rank assigned to a matrix should depend on some
tolerance δ, which reﬂects the error level in the data and/or the precision of the arithmetic
used.
It can be shown that perturbations of an element of a matrix A result in perturbations
of the same, or smaller, magnitude in its singular values. This motivates the following
deﬁnition of numerical rank.
Deﬁnition 1.4.5.
A matrix A ∈Rm×n is said to have numerical δ-rank equal to k if
σ1 ≥· · · ≥σk > δ ≥σk+1 ≥· · · ≥σp,
p = min(m, n),
(1.4.28)
where σi are the singular values of A. Then the right singular vectors (vk+1, . . . , vn) form
an orthogonal basis for the numerical null space of A.

1.4. The Linear Least Squares Problem
53
Deﬁnition 1.4.5 assumes that there is a well-deﬁned gap between σk+1 and σk. When
this is not the case the numerical rank of A is not well deﬁned.
Example 1.4.2.
Consider an integral equation of the ﬁrst kind,
 1
0
k(s, t)f (s) ds = g(t),
k(s, t) = e−(s−t)2,
on −1 ≤t ≤1. In order to solve this equation numerically it must ﬁrst be discretized. We
introduce a uniform mesh for s and t on [−1, 1] with step size h = 2/n, si = −1 + ih,
tj = −1 + jh, i, j = 0 : n. Approximating the integral with the trapezoidal rule gives
h
n

i=0
wik(si, tj)f (ti) = g(tj),
j = 0 : n,
where wi = 1, i ̸= 0, n, and w0 = wn = 1/2. These equations form a linear system
Kf = g,
K ∈R(n+1)×(n+1),
f, g ∈Rn+1.
For n = 100 the singular values σk of the matrix K were computed in IEEE double
precision with a unit roundoff level of 1.11 · 10−16 (see Sec. 2.2.3). They are displayed in
logarithmic scale in Figure 1.4.2. Note that for k > 30 all σk are close to roundoff level,
so the numerical rank of K certainly is smaller than 30. This means that the linear system
Kf = g is numerically underdetermined and has a meaningful solution only for special
right-hand sides g.
0
10
20
30
40
50
60
70
80
90
100
10
−18
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
Figure 1.4.2. Singular values of a numerically singular matrix.

54
Chapter 1. Principles of Numerical Calculations
Equation (1.4.28) is a Fredholm integral equation of the ﬁrst kind. It is known that
such equations are ill-posed in the sense that the solution f does not depend continuously on
the right-hand side g. This example illustrate how this inherent difﬁculty in the continuous
problem carries over to the discretized problem.
Review Questions
1.4.1 State the Gauss–Markov theorem.
1.4.2 Show that the matrix ATA ∈Rn×n of the normal equations is symmetric and positive
semideﬁnite, i.e., xT (ATA)x ≥0, for all x ̸= 0.
1.4.3 Give two geometric conditions which are necessary and sufﬁcient conditions for x to
be the pseudoinverse solution of Ax = b.
1.4.4 (a) Which are the four fundamental subspaces of a matrix? Which relations hold
between them?
(b) Show, using the SVD, that PR(A) = AA† and PR(AT ) = A†A.
1.4.5 (a) Construct an example where (AB)† ̸= B†A†.
(b) Show that if A is an m×r matrix, B is an r ×n matrix, and rank (A) = rank (B) =
r, then (AB)† = B†A†.
Problems and Computer Exercises
1.4.1 In order to estimate the height above sea level for three points A, B, and C, the
difference in altitude was measured between these points and points D, E, and F at
sea level. The measurements obtained form a linear system in the heights xA, xB, and
xC of A, B, and C:


1
0
0
0
1
0
0
0
1
−1
1
0
0
−1
1
−1
0
1


 xA
xB
xC

=


1
2
3
1
2
1


.
Determine the least squares solution and verify that the residual vector is orthogonal
to all columns in A.
1.4.2 Consider the least squares problem minx ∥Ax −b∥2
2, where A has full column rank.
Partition the problem as
min
x1,x2
(A1 A2)

x1
x2

−b

2
2.
By a geometric argument show that the solution can be obtained as follows. First

1.5. Numerical Solution of Differential Equations
55
compute x2 as the solution to the problem
min
x2 ∥P ⊥
A1(A2x2 −b)∥2
2,
where P ⊥
A1 = I −PA1 is the orthogonal projector onto N(AT
1 ). Then compute x2 as
the solution to the problem
min
x1 ∥A1x1 −(b −A2x2)∥2
2.
1.5
Numerical Solution of Differential Equations
1.5.1
Euler’s Method
The approximate solution of differential equations is a very important task in scientiﬁc
computing. Nearly all the areas of science and technology contain mathematical models
which lead to systems of ordinary (or partial) differential equations. For the step by step
simulation of such a system a mathematical model is ﬁrst set up; i.e., state variables are
set up which describe the essential features of the state of the system. Then the laws are
formulated, which govern the rate of change of the state variables, and other mathematical
relations between these variables. Finally, these equations are programmed for a computer
to calculate approximately, step by step, the development in time of the system.
The reliability of the results depends primarily on the quality of the mathematical
model and on the size of the time step. The choice of the time step is partly a question of
economics. Small time steps may give you good accuracy, but also long computing time.
More accurate numerical methods are often a good alternative to the use of small time steps.
The construction of a mathematical model is not trivial. Knowledge of numerical
methods and programming helps in that phase of the job, but more important is a good
understanding of the fundamental processes in the system, and that is beyond the scope
of this text. It is, however, important to realize that if the mathematical model is bad, no
sophisticated numerical techniques or powerful computers can stop the results from being
unreliable, or even harmful.
A mathematical model can be studied by analytic or computational techniques. Ana-
lytic methods do not belong to this text. We want, though, to emphasize that the comparison
of results obtained by applying analytic methods, in the special cases when they can be
applied, can be very useful when numerical methods and computer programs are tested. We
shall now illustrate these general comments using a particular example.
An initial value problem for an ordinary differential equation is to ﬁnd y(t) such that
dy
dt = f (t, y),
y(0) = c.
The differential equation gives, at each point (t, y), the direction of the tangent to the solution
curve which passes through the point in question. The direction of the tangent changes
continuously from point to point, but the simplest approximation (which was proposed as
early as the eighteenth century by Euler17) is that one studies the solution for only certain
17Leonhard Euler (1707–1783), incredibly proliﬁc Swiss mathematician. He gave fundamental contributions to
many branches of mathematics and to the mechanics of rigid and deformable bodies, as well as to ﬂuid mechanics.

56
Chapter 1. Principles of Numerical Calculations
values of t = tn = nh, n = 0, 1, 2, . . . , (h is called the “time step” or “step length”) and
assumes that dy/dt is constant between the points. In this way the solution is approximated
by a polygon (Figure 1.5.1) which joins the points (tn, yn), n = 0, 1, 2, . . . , where
y0 = c,
yn+1 −yn
h
= f (tn, yn).
(1.5.1)
Thus we have the simple difference equation known as Euler’s method:
y0 = c,
yn+1 = yn + hf (tn, yn),
n = 0, 1, 2, . . . .
(1.5.2)
During the computation, each yn occurs ﬁrst on the left-hand side, then recurs later on the
right-hand side of an equation. (One could also call (1.5.2) an iteration formula, but one
usually reserves the word “iteration” for the special case where a recursion formula is used
solely as a means of calculating a limiting value.)
0
0.5
1
1.5
2
2.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 1.5.1. Approximate solution of the differential equation dy/dt = y, y0 =
0.25, by Euler’s method with h = 0.5.
1.5.2
An Introductory Example
Consider the motion of a ball (or a shot) under the inﬂuence of gravity and air resistance.
It is well known that the trajectory is a parabola, when the air resistance is neglected and
the force of gravity is assumed to be constant. We shall still neglect the variation of the
force of gravity as well as the curvature and the rotation of the Earth. This means that
we forsake serious applications to, for example, satellites. We shall, however, take the air
resistance into account. We neglect the rotation of the shot around its own axis. Therefore,
we can treat the problem as motion in a plane, but we have to forsake the application to, for
example, table tennis, baseball, or a rotating projectile. Now we have introduced a number
of assumptions, which deﬁne our model of reality.
The state of the ball is described by its position (x, y) and velocity (u, v), each of
which has two Cartesian coordinates in the plane of motion. The x-axis is horizontal, and the
y-axis is directed upward. Assume that the air resistance is a force P such that the direction
is opposite to the velocity, and the strength z is proportional to the square of the speed and

1.5. Numerical Solution of Differential Equations
57
to the square of the radius R of the shot. If we denote by Px and Py the components of P
along the x and y directions, respectively, we can then write
Px = −mzu,
Py = −mzv,
z = cR2
m

u2 + v2,
(1.5.3)
where m is the mass of the ball.
For the sake of simplicity we assume that c is a constant. It actually depends on the
density and the viscosity of the air. Therefore, we have to forsake the application to cannon
shots, where the variation of the density with height is important. If one has access to a
good model of the atmosphere, the variation of c would not make the numerical simulation
much more difﬁcult. This contrasts with analytic methods, where such a modiﬁcation is
likely to mean a considerable complication. In fact, even with a constant c, a purely analytic
treatment offers great difﬁculties.
Newton’s law of motion tells us that
mdu/dt = Px,
mdv/dt = −mg + Py,
(1.5.4)
where the term −mg is the force of gravity. Inserting (1.5.3) into (1.5.4) and dividing by m
we get
du/dt = −zu,
dv/dt = −g −zv,
(1.5.5)
and by the deﬁnition of velocity,
dx/dt = u,
dy/dt = v.
(1.5.6)
Equations (1.5.5) and (1.5.6) constitute a system of four differential equations for the four
variables x, y, u, v. The initial state x0, y0 and u0, v0 at time t0 = 0 is assumed to be given.
Afundamental proposition in the theory of differential equations tells us that if initial values
of the state variables u, v, x, y are given at some initial time t = t0, then they will be
uniquely determined for all t > t0.
The simulation of the motion of the ball means that, at a sequence of time instances
tn, n = 0, 1, 2, . . . , we determine the approximate values un, vn, xn, yn. We ﬁrst look at the
simplest technique, using Euler’s method with a constant time step h. Therefore, set tn = nh.
We replace the derivative du/dt by the forward difference quotient (un+1 −un)/h, and
similarly for the other variables. Hence after multiplication by h, the differential equations
are replaced by the following system of difference equations:
xn+1 = xn + hun,
yn+1 = yn + hvn,
un+1 = un −hznun,
(1.5.7)
vn+1 = vn −h(g + znvn),
where
zn = cR2
m
 
u2n + v2n.
From this xn+1, yn+1, un+1, vn+1, etc. are solved, step by step, for n = 0, 1, 2, . . . , using
the provided initial values x0, y0, u0, and v0.

58
Chapter 1. Principles of Numerical Calculations
We performed these computations until yn+1 became negative for the ﬁrst time, with
g = 9.81, φ = 60◦, and the initial values
x0 = 0,
y0 = 0,
u0 = 100 cos φ,
v0 = 100 sin φ.
Curves obtained for h = 0.01 and cR2/m = 0.25i · 10−3, i = 0 : 4, are shown in
Figure 1.5.2. There is, in this graphical representation, also an error due to the limited
resolution of the plotting device.
0
100
200
300
400
500
600
700
800
−100
0
100
200
300
400
500
i = 0
1
2
3
4
Figure 1.5.2. Approximate trajectories computed with Euler’s method with h = 0.01.
In Euler’s method the state variables are locally approximated by linear functions
of time, one of the often recurrent ideas in numerical computation. We can use the same
idea for computing the coordinate x∗of the point where the shot hits the ground. Suppose
that yn+1 becomes negative for the ﬁrst time when n = N. For xN ≤x ≤xN+1 we
then approximate y by a linear function of x, represented by the secant through the points
(xN, yN) and (xN+1, yN+1), i.e.,
y = yN + (x −xN)yN+1 −yN
xN+1 −xN
.
By setting y = 0 we obtain
x∗= xN −yN
xN+1 −xN
yN+1 −yN
.
(1.5.8)
This is called (linear) inverse interpolation; see Sec. 4.2.2. The error from the linear
approximation in (1.5.8) used for the computation of x∗is proportional to h2. It is thus
approximately equal to the error committed in one single step with Euler’s method, and
hence of less importance than the other error.

1.5. Numerical Solution of Differential Equations
59
The case without air resistance (i = 0) can be solved exactly. In fact it can be shown
that
x∗= 2u0v0/9.81 = 5000 ·
√
3/9.81 ≈882.7986.
The computer produced x∗≈883.2985 for h = 0.01, and x∗≈883.7984 for h = 0.02.
The error for h = 0.01 is therefore 0.4999, and for h = 0.02 it is 0.9998. The approximate
proportionality to h is thus veriﬁed, actually more strikingly than could be expected!
It can be shown that the error in the results obtained with Euler’s method is also
proportional to h (not h2). Hence a disadvantage of the above method is that the step length
h must be chosen quite small if reasonable accuracy is desired. In order to improve the
method we can apply another idea mentioned previously, namely Richardson extrapolation.
(The application differs a little from the one we saw previously, because now the error
is approximately proportional to h, while for the trapezoidal rule it was approximately
proportional to h2.) For i = 4, the computer produced x∗≈500.2646 and x∗≈500.3845
for, respectively, h = 0.01 and h = 0.02. Now let x∗denote the exact horizontal coordinate
of the landing point. Then
x∗−500.2646 ≈0.01k,
x∗−500.3845 ≈0.02k.
By elimination of k we obtain
x∗≈2 · 500.2646 −500.3845 = 500.1447,
which should be a more accurate estimate of the coordinate. By a more accurate integration
method we obtained 500.1440. Thus, in this case we gained more than two decimal digits
by the use of Richardson extrapolation.
The simulations shown in Figure 1.5.2 required about 1500 time steps for each curve.
This may seem satisfactory, but we must not forget that this is a very small task, compared
with most serious applications. So we would like to have a method that allows much larger
time steps than Euler’s method.
1.5.3
Second Order Accurate Methods
In step by step computations we have to distinguish between the local error, the error that
is committed at a single step, and the global error, the error of the ﬁnal results. Recall that
we say that a method is accurate of order p if its global error is approximately proportional
to hp. Euler’s method is only ﬁrst order accurate; we shall present a method that is second
order accurate. To achieve the same accuracy as with Euler’s method the number of steps
can then be reduced to about the square root of the number of steps in Euler’s method. In
the above ball problem this means
√
1500 ≈40 steps. Since the amount of work is closely
proportional to the number of steps this is an enormous savings!
Another question is how the step size h is to be chosen. It can be shown that even for
rather simple examples (see below) it is adequate to use very different step sizes in different
parts of the computation. Hence the automatic control of the step size (also called adaptive
control) is an important issue.

60
Chapter 1. Principles of Numerical Calculations
Both requests can be met by an improvement of the Euler method (due to Runge18)
obtained by applying the Richardson extrapolation in every second step. This is different
from our previous application of the Richardson idea. We ﬁrst introduce a better notation
by writing a system of differential equations and the initial conditions in vector form
dy/dt = f(t, y),
y(a) = c,
(1.5.9)
where y is a column vector that contains all the state variables.19 With this notation methods
for large systems of differential equations can be described as easily as methods for a single
equation. The change of a system with time can then be thought of as a motion of the state
vector in a multidimensional space, where the differential equation deﬁnes the velocity ﬁeld.
This is our ﬁrst example of the central role of vectors and matrices in modern computing.
For the ball example, we have a = 0, and by (1.5.5) and (1.5.6),
y =


y1
y2
y3
y4

≡


x
y
u
v

,
f(t, y) =


y3
y4
−zy3
−g −zy4

,
c = 102


0
0
cos φ
sin φ

,
where
z = cR2
m

(y3)2 + (y4)2.
The computations in the step which leads from tn to tn+1 are then as follows:
i. One Euler step of length h yields the estimate:
y∗
n+1 = yn + hf(tn, yn).
ii. Two Euler steps of length 1
2h yield another estimate:
yn+1/2 = yn + 1
2hf(tn, yn),
y∗∗
n+1 = yn+1/2 + 1
2hf(tn+1/2, yn+1/2),
where tn+1/2 = tn + h/2.
iii. Then yn+1 is obtained by Richardson extrapolation:
yn+1 = y∗∗
n+1 + (y∗∗
n+1 −y∗
n+1).
It is conceivable that this yields a second order accurate method. It is left as an exercise
(Problem 1.5.2) to verify that this scheme is identical to the following somewhat simpler
scheme known as Runge’s second order method:
k1 = hnf(tn, yn),
k2 = hnf(tn + hn/2, yn + k1/2),
(1.5.10)
yn+1 = yn + k2,
18Carle DavidTolmé Runge (1856–1927), German mathematician. Runge was professor of applied mathematics
at Göttingen from 1904 until his death.
19The boldface notation is temporarily used for vectors in this section only, not in the rest of the book.

1.5. Numerical Solution of Differential Equations
61
where we have replaced h by hn in order to include the use of variable step size. Another
explanation of the second order accuracy of this method is that the displacement k2 equals
the product of the step size and a sufﬁciently accurate estimate of the velocity at the midpoint
of the time step. Sometimes this method is called the improved Euler method or Heun’s
method, but these names are also used to denote other second order accurate methods.
1.5.4
Adaptive Choice of Step Size
We shall now describe how the step size can be adaptively (or automatically) controlled by
means of a tolerance tol, by which the user tells the program how large an error he tolerates
in values of variables (relative to the values themselves).20 Compute
δ = max
i
δi,
δi = |k2,i −k1,i|/|3yi|,
where δi is related to the relative error of the ith component of the vector y at the current
step; see below.
A step size is accepted if δ ≤tol, and the next step should be
hnext = h min
!
1.5,

tol/(1.2δ)
"
,
where 1.2 is a safety factor, since the future is never exactly like the past! The square root
occurring here is due to the fact that this method is second order accurate; i.e., the global error
is almost proportional to the square of the step size and δ is approximately proportional to h2.
A step is rejected if δ > tol, and recomputed with the step size
hnext = h max
!
0.1,

tol/(1.2δ)
"
.
The program needs a suggestion for the size of the ﬁrst step. This can be a very rough
guess, because the step size control described above will improve it automatically so that an
adequate step size is found after a few steps (or recomputations, if the suggested step was
too big). In our experience, a program of this sort can efﬁciently handle guesses that are
wrong by several powers of 10. If y(a) ̸= 0 and y′(a) ̸= 0, you may try the initial step size
h = 1
4

i
|yi|
# 
i
|dyi/dt|
evaluated at the initial point t = a. When you encounter the cases y(a) = 0 or y′(a) = 0
for the ﬁrst time, you are likely to have gained enough experience to suggest something that
the program can handle. More professional programs take care of this detail automatically.
The request for a certain relative accuracy may cause trouble when some components
of y are close to zero. So, already in the ﬁrst version of your program, you had better replace
yi in the above deﬁnition of δ by
¯yi = max{|yi|, 0.001}.
(You may sometimes have to replace the default value 0.001 by something else.)
20With the terminology that will be introduced in the next chapter, tol is, with the step size control described
here, related to the global relative errors. At the time of writing, this contrasts to most codes for the solution of
ordinary differential equations, in which the local errors per step are controlled by the tolerance.

62
Chapter 1. Principles of Numerical Calculations
It is a good habit to make a second run with a predetermined sequence of step sizes
(if your program allows this) instead of adaptive control. Suppose that the sequence of time
instances used in the ﬁrst run is t0, t1, t2, . . . . Divide each subinterval [tn, tn+1] into two steps
of equal length. Thus, the second run still has variable step size and twice as many steps as
the ﬁrst run. The errors are therefore expected to be approximately 1
4 of the errors of the ﬁrst
run. The ﬁrst run can therefore use a tolerance that is four times as large than the error you
can tolerate in the ﬁnal result. Denote the results of the two runs by yI(t) and yII(t). You
can plot 1
3(yII(t) −yI(t)) versus t; this is an error curve for yII(t). Alternatively you can
add 1
3(yII(t) −yI(t)) to yII(t). This is another application of the Richardson extrapolation
idea. The cost is only 50% more work than the plain result without an error curve.
If there are no singularities in the differential equation, 1
3(yII(t) −yI(t)) strongly
overestimates the error of the extrapolated values—typically by a factor such as tol−1/2. It
is, however, a nontrivial matter to ﬁnd an error curve that strictly and realistically tells us
how good the extrapolated results are. The reader is advised to test experimentally how this
works on examples where the exact results are known.
An easier, though inferior, alternative is to run a problem with two different tolerances.
One reason why this is inferior is that the two runs do not “keep in step,” and then Richardson
extrapolation cannot be easily applied.
If you request very high accuracy in your results, or if you are going to simulate a
system over a very long time, you will need a method with a higher order of accuracy than
two. The reduction of computing time, if you replace this method by a higher order method
can be large, but the improvements are seldom as drastic as when you replace Euler’s method
by a second order accurate scheme like this. Runge’s second order method is, however,
no universal recipe. There are special classes of problems, notably the problems which are
called “stiff,” which need special methods.
One advantage of a second order accurate scheme when requests for accuracy are
modest is that the quality of the computed results is normally not ruined by the use of linear
interpolation at the graphical output, or in the postprocessing of numerical results. (After
you have used a more than second order accurate integration method, it may be necessary to
use more sophisticated interpolation at the graphical or numerical treatment of the results.)
We suggest that you write or try to ﬁnd a program that can be used for systems with (in
principle) any number of equations; see the preface.
Example 1.5.1.
The differential equation
dy/dt = −1
2y3,
with initial condition y(1) = 1, was treated by a program, essentially constructed as de-
scribed above, with tol = 10−4 until t = 104. When comparing the result with the exact
solution y(t) = t−1/2, it was found that the actual relative error remained at a little less
than 1.5 tol all the time when t > 10. The step size increased almost linearly with t from
h = 0.025 to h = 260. The number of steps increased almost proportionally to log t; the
total number of steps was 374. Only one step had to be recomputed (except for the ﬁrst
step, where the program had to ﬁnd an appropriate step size).

Problems and Computer Exercises
63
The computation was repeated with tol = 4 · 10−4. The experience was the same,
except that the steps were about twice as long all the time. This is what can be expected, since
the step sizes should be approximately proportional to
√
tol, for a second order accurate
method. The total number of steps was 194.
Example 1.5.2.
The example of the motion of a ball was treated by Runge’s second order method with
the constant step size h = 0.9. The x-coordinate of the landing point became x∗≈500.194,
which is more than twice as accurate than the result obtained by Euler’s method (without
Richardson extrapolation) with h = 0.01, which uses about 90 times as many steps.
We have now seen a variety of ideas and concepts which can be used in the develop-
ment of numerical methods. A small warning is perhaps warranted here: it is not certain
that the methods will work as well in practice as one might expect. This is because ap-
proximations and the restriction of numbers to a certain number of digits introduce errors
which are propagated to later stages of a calculation. The manner in which errors are propa-
gated is decisive for the practical usefulness of a numerical method. We shall examine such
questions in Chapter 2. Later chapters will treat propagation of errors in connection with
various typical problems.
The risk that error propagation may upstage the desired result of a numerical process
should, however, not dissuade one from the use of numerical methods. It is often wise,
though, to experiment with a proposed method on a simpliﬁed problem before using it in
a larger context. Developments in hardware as well as software has created a far better
environment for such work.
Review Questions
1.5.1 Explain the difference between the local and global error of a numerical method for
solving a differential equation. What is meant by the order of accuracy of a method?
1.5.2 Describe how Richardson extrapolation can be used to increase the order of accuracy
of Euler’s method.
1.5.3 Discuss some strategies for the adaptive control of step length and estimation of global
accuracy in the numerical solution of differential equations.
Problems and Computer Exercises
1.5.1 (a) Integrate numerically using Euler’s method the differential equation dy/dt = y,
with initial conditions y(0) = 1, to t = 0.4, with step length h = 0.2 and h = 0.1.
(b) Extrapolate to h = 0, using the fact that the error is approximately proportional to
the step length. Compare the result with the exact solution of the differential equation
and determine the ratio of the errors in the results in (a) and (b).

64
Chapter 1. Principles of Numerical Calculations
(c) How many steps would have been needed in order to attain, without using extrap-
olation, the same accuracy as was obtained in (b)?
1.5.2 (a) Write a program for the simulation of the motion of the ball using Euler’s method
and the same initial values and parameter values as above. Print only x, y at integer
values of t and at the last two points (i.e., n = N and n = N + 1) as well as the
x-coordinate of the landing point. Take h = 0.05 and h = 0.1. As postprocessing,
improve the estimates of x∗by Richardson extrapolation, and estimate the error by
comparison with the results given in the text above.
(b) In (1.5.7), in the equations for xn+1 and yn+1, replace the right-hand sides un and
vn by, respectively, un+1 and vn+1. Then proceed as in (a) and compare the accuracy
obtained with that obtained in (a).
(c) Choose initial values which correspond to what you think is reasonable for shot
put. Make experiments with several values of u0, v0 for c = 0. How much is x∗
inﬂuenced by the parameter cR2/m?
1.5.3 Verify that Runge’s second order method, as described by (1.5.10), is equivalent to the
scheme described a few lines earlier (with Euler steps and Richardson extrapolation).
1.5.4 Write a program for Runge’s second order method with automatic step size control
that can be applied to a system of differential equations. Store the results so that they
can be processed afterward, for example, to make a table of the results; draw curves
showing y(t) versus t, or (for a system) y2 versus y1; or draw some other interesting
curves.
Apply the program to Examples 1.5.1 and 1.5.2, and to the circle test, that is,
y′
1 = −y2,
y′
2 = y1,
with initial conditions y1(0) = 1, y2(0) = 0. Verify that the exact solution is a
uniform motion along the unit circle in the (y1, y2)-plane. Stop the computations
after 10 revolutions (t = 20π). Make experiments with different tolerances, and
determine how small the tolerance has to be in order that the circle on the screen does
not become “thick.”
1.6
Monte Carlo Methods
1.6.1
Origin of Monte Carlo Methods
In most of the applications of probability theory one makes a mathematical formulation of
a stochastic problem (i.e., a problem where chance plays some part) and then solves the
problem by using analytical or numerical methods. In the Monte Carlo method one does
the opposite; a mathematical or physical problem is given, and one constructs a numerical
game of chance, the mathematical analysis of which leads to the same equations as the given
problem for, e.g., the probability of some event, or for the mean of some random variable in
the game. One plays it N times and estimates the relevant quantities by traditional statistical
methods. Here N is a large number, because the standard deviation of a statistical estimate
typically decreases only inversely proportionally to
√
N.

1.6. Monte Carlo Methods
65
TheideabehindtheMonteCarlomethodwasusedbytheItalianphysicistEnricoFermi
to study neutron diffusion in the early 1930s. Fermi used a small mechanical adding machine
for this purpose. With the development of computers larger problems could be tackled. At
Los Alamos in the late 1940s the use of the method was pioneered by von Neumann,21
Ulam,22 and others for many problems in mathematical physics including approximating
complicated multidimensional integrals. The picturesque name of the method was coined
by Nicholas Metropolis.
The Monte Carlo method is now so popular that the deﬁnition is too narrow. For
instance, in many of the problems where the Monte Carlo method is successful, there is
already an element of chance in the system or process which one wants to study. Thus such
games of chance can be considered numerical simulations of the most important aspects.
In this wider sense the “Monte Carlo methods” also include techniques used by statisticians
since around 1900, under names like experimental or artiﬁcial sampling. For example,
statistical experiments were used to check the adequacy of certain theoretical probability
laws that had been derived mathematically by the eminent scientist W. S. Gosset. (He used
the pseudonym “Student” when he wrote on probability.)
Monte Carlo methods may be used when the changes in the system are described with
a much more complicated type of equation than a system of ordinary differential equations.
Note that there are many ways to combine analytical methods and Monte Carlo methods.
An important rule is that if a part of a problem can be treated with analytical or traditional
numerical methods, then one should use such methods.
The following are some areas where the Monte Carlo method has been applied:
(a) Problems in reactor physics; for example, a neutron, because it collides with other
particles, is forced to make a random journey. In infrequent but important cases the
neutron can go through a layer of (say) shielding material (see Figure 1.6.1).
(b) Technical problems concerning trafﬁc (in telecommunication systems and railway
networks; in the regulation of trafﬁc lights, and in other problems concerning auto-
mobile trafﬁc).
(c) Queuing problems.
(d) Models of conﬂict.
(e) Approximate computation of multiple integrals.
(f) Stochastic models in ﬁnancial mathematics.
Monte Carlo methods are often used for the evaluation of high-dimensional (10 to
100) integrals over complicated regions. Such integrals occur in such diverse areas as
21John von Neumann was born János Neumann in Budapest 1903, and died in Washington D.C. 1957. He
studied under Hilbert in Göttingen in 1926–27, was appointed professor at Princeton University in 1931, and in
1933 joined the newly founded Institute for Advanced Studies in Princeton. He built a framework for quantum
mechanics, worked in game theory, and was one of the pioneers of computer science.
22Stanislaw Marcin Ulam, born in Lemberg, Poland (now Lwow, Ukraine) 1909, and died in Santa Fe, New
Mexico, USA, 1984. Ulam obtained his Ph.D. in 1933 from the Polytechnic institute of Lwow, where he studied
under Banach. He was invited to Harvard University by G. D. Birkhoff in 1935 and left Poland permanently in
1939. In 1943 he was asked by von Neumann to come to Los Alamos, where he remained until 1965.

66
Chapter 1. Principles of Numerical Calculations
Inside
Shield
Outside
Figure 1.6.1. Neutron scattering.
quantum physics and mathematical ﬁnance. The integrand is then evaluated at random
points uniformly distributed in the region of integration. The arithmetic mean of these
function values is then used to approximate the integral; see Sec. 5.4.5.
In a simulation, one can study the result of various actions more cheaply, more quickly,
and with less risk of organizational problems than if one were to take the corresponding
actions on the actual system. In particular, for problems in applied operations research, it is
quite common to take a shortcut from the actual system to a computer program for the game
of chance, without formulating any mathematical equations. The game is then a model of
the system. In order for the term “Monte Carlo method” to be correctly applied, however,
random choices should occur in the calculations. This is achieved by using so-called
random numbers; the values of certain variables are determined by a process comparable
to dice throwing. Simulation is so important that several special programming languages
have been developed exclusively for its use.23
1.6.2
Generating and Testing Pseudorandom Numbers
In the beginning, coins, dice, and roulette wheels were used for creating the randomness.
For example, the sequence of 20 digits
11100 01001 10011 01100
is a record of 20 tosses of a coin where “heads” are denoted by 1 and “tails” by 0. Such digits
are sometimes called (binary) random digits, assuming that we have a perfect coin—i.e.,
that heads and tails have the same probability of occurring. We also assume that the tosses
of the coin are made in a statistically independent way.24
Similarly, decimal random digits could in principle be obtained by using a well-made
icosahedral (20 sided) dice and assigning each decimal digit to two of its sides. Such
23One notable early example is the SIMULA programming language designed and built by Ole-Johan Dahl and
Kristen Nygaard at the Norwegian Computing Center in Oslo 1962–67. It was originally built as a language for
discrete event simulation, but was also inﬂuential because it introduced object-oriented programming concepts.
24Of course, these assumptions cannot be obtained in practice, as shown in theoretical and experimental studies
by Persi Diaconis, Stanford University.

1.6. Monte Carlo Methods
67
mechanical (or analogous electronic) devices have been used to produce tables of random
sampling digits; the ﬁrst one by Tippett was published in 1927 and was to be considered
as a sequence of 40,000 independent observations of a random variable that equals one
of the integer values 0, 1, 2, . . . , 9, each with probability 1/10. In the early 1950s the
Rand Corporation constructed a million-digit table of random numbers using an electrical
“roulette wheel” ([295]). The wheel had 32 slots, of which 12 were ignored; the others
were numbered from zero to nine twice. To test the quality of the randomness several tests
were applied. Every block of a 1000 digits in the tables (and also the table as a whole)
were tested. The Handbook of Mathematical Functions [1, Table 26.11]25 provides 2500
ﬁve-digit random numbers compiled from this set.
Example 1.6.1.
The random number generator, used for drawing of prizes of Swedish Premium Sav-
ing Bonds, was developed in 1962 by Dahlquist [86]. Speed is not a major concern for this
application, since relatively few random decimal digits (about 50,000) are needed. There-
fore, an algorithm which is easier to analyze was chosen. This uses a primary series of less
than 240 decimal random digits produced by some other means. The length of this primary
series is n = p1 + p2 + · · · + pk, where pi are prime numbers and pi ̸= pj, i ̸= j. For the
analysis it is assumed that the primary series is perfectly random.
The primary series is used to generate a much longer secondary series of prime num-
bers in a way that is best described by a mechanical analogy. Think of k cogwheels with
pi cogs, i = 1 : k, and place the digits from the primary series on the cogs of these. The
ﬁrst digit in the secondary series is obtained by adding the k digits (modulus 10) that are at
the top position of each cogwheel. Then each wheel is turned one cog clockwise and the
second digit is obtained in the same way as the ﬁrst, etc. After p1 · p2 · · · pk steps we are
back in the original position. This is the minimum period of the secondary series of random
digits.
For the application mentioned above, k = 7 prime numbers, in the range 13 ≤pi ≤
53, are randomly selected. This gives a varying minimum period approximately equal
to 108, which is much more than the number of digits used to produce the drawing list.
Considering the public reaction, the primary series is generated by drawing from a tombola.
Random digits from a table can be packed together to give a sequence of equidis-
tributed integers. For example, the sequence
55693 02945 81723 43588 81350 76302 . . .
can be considered as six ﬁve-digit random numbers, where each element in the sequence
has a probability of 10−5 of taking on the value 0, 1, 2, . . . , 99,999. From the same digits
one can also construct the sequence
0.556935, 0.029455, 0.817235, 0.435885, 0.813505, 0.763025, . . . ,
(1.6.1)
which can be considered a good approximation to a sequence of independent observations
of a variable which is a sequence of uniform deviates in the interval [0, 1). The 5 in the
25This classical Handbook of Mathematical Functions, edited by Milton Abramowitz and Irene A. Stegun, is
used as a reference throughout this book. We will often refer to it as just “the Handbook.”

68
Chapter 1. Principles of Numerical Calculations
sixth decimal place is added in order to get the correct mean (without this the mean would
be 0.499995 instead of 0.5).
In a computer it is usually not appropriate to store a large table of random numbers.
Several physical devices for random number generation have been proposed, using for
instance electronic or radioactive noise, but very few seem to have been inserted in an actual
computer system. Instead random numbers are usually produced by arithmetic methods,
so-called random number generators (RNGs). The aim of a random number generator
is to generate a sequence of numbers u1, u2, u3, . . . that imitates the abstract mathematical
concept of a sequence of mutually independent random variables uniformly distributed over
the interval [0, 1). Sequences obtained in this way are uniquely determined by one or more
starting values called seeds, to be given by the user (or some default values). Random
number generators should be analyzed theoretically and be backed by practical evidence
from extensive statistical testing. According to a much quoted statement by D. H. Lehmer,26
A random sequence is a vague notion . . . in which each term is unpredictable to
the uninitiated and whose digits pass a certain number of tests traditional with
statisticians. . .
Because the set of ﬂoating-point numbers in [0, 1] is ﬁnite, although very large, there
will eventually appear a number that has appeared before, say ui+j = ui for some positive
i, j. The sequence {un} therefore repeats itself periodically for n ≥i; the length of the
period is j. A truly random sequence is, of course, never periodic. For this and other
reasons, a sequence generated like this is called a pseudorandom sequence. But the ability
to repeat exactly the same sequence of numbers, which is needed for program veriﬁcation
and variance reduction, is a major advantage over generation by physical devices.
There are two popular myths about the making of a random number generator: ﬁrst
that it is impossible; second that it is trivial. We have seen that the ﬁrst myth is correct,
unless we add the preﬁx “pseudo.”27 The second myth, however, is completely false.
In a computer the fundamental concept is not a sequence of decimal random digits,
but uniform random deviates, i.e., a sequence of mutually independent observations of a
random variable U with a uniform distribution on [0, 1); the density function of U is thus
(with a temporary notation)
f1(u) =
! 1
if u ∈[0, 1),
0
otherwise.
Random deviates for other distributions are generated by means of uniform deviates. For
example, the variable X = a+(b−a)U is a uniform deviate on [a, b). Its density function is
f (x) = f1((x −a)/(b−a)). If [a, b] = [0, 1], we usually write “uniform deviate” (without
mentioning the interval). We often write “deviate” instead of “random deviate” when the
meaning is evident from the context. Algorithms for generating deviates for several other
distributions are given in Sec. 1.6.3.
26Some readers may think that Lehmer’s deﬁnition is too vague. There have been many deep attempts for more
precise formulation. See Knuth [230, pp. 149–179], who catches the ﬂavor of the philosophical discussion of these
matters and contributes to it himself.
27“Anyone who considers arithmetic methods of producing random numbers is, of course, in a state of sin.”–John
von Neumann (1951).

1.6. Monte Carlo Methods
69
The most widely used generators for producing pseudorandom numbers are multiple
recursive generators. These are based on a linear recurrence of order k,
xn = λ1xn−1 + · · · + λkxn−k + c
mod P,
(1.6.2)
i.e., xn is the remainder obtained when the right-hand side is divided by the modulus m. Here
P is a positive integer and the coefﬁcients λ1, . . . , λk belong to the set {0, 1, . . . , m −1}.
The state at step n is sn = (xn−k+1, . . . , xn) and the generator is started from a seed sk−1 =
(x0, . . . , xk−1). When m is large the output can be taken as the number un = xn/m. For
k = 1 we obtain the classical mixed congruential method
xn = λxn−1 + c
mod P.
An important characteristic of an RNG is its period, which is the maximum length of
the sequence before it begins to repeat. Note that if the algorithm for computing xn depends
only on xn−1, then the entire sequence repeats once the seed x0 is duplicated. One can show
that if P = 2t (which is natural on a binary computer) the period of the mixed congruential
method is equal to 2t, assuming that c is odd and that λ gives remainder 1 when divided by
four. Also, if P is a prime number and if the coefﬁcients λj satisfy certain conditions, then
the generated sequence has the maximal period mk −1; see Knuth [230].
A good RNG should have a period that is guaranteed to be extremely long to make
sure that no wrap-around can occur in practice. The linear congruential generator deﬁned
by
xn = 16807xn−1 mod (231 −1),
(1.6.3)
with period (231 −2), was proposed originally by Lewis, Goodman, and Miller (1969). It
has been widely used in many software libraries for statistics, simulation, and optimization.
In the survey by Park and Miller [283] this generator was proposed as a “minimal standard”
against which other generators should be judged. Asimilar generator, but with the multiplier
77 = 823543, was used in MATLAB 4.
Marsaglia [258] pointed out a theoretical weakness of all linear congruential genera-
tors. He showed that if k successive random numbers (xi+1, . . . , xi+k) at a time are generated
and used to plot points in k-dimensional space, then they will lie on (k −1)-dimensional
hyperplanes and will not ﬁll up the space; see Figure 1.6.2 (left). More precisely, the values
will lie on a set of at most (k!m)1/k ≈(k/e)m1/k equidistant parallel hyperplanes in the
k-dimensional hypercube (0, 1)k. When the number of hyperplanes is too small, this obvi-
ously is a strong limitation to the k-dimensional uniformity. For example, for m = 231 −1
and k = 3, this is only about 1600 planes. This clearly may interfere with a simulation
problem.
If the constants m, a, and c are not very carefully chosen, there will be many fewer
hyperplanes than the maximum possible. One such infamous example is the linear congru-
ential generator with a = 65539, c = 0, and m = 231 used by IBM mainframe computers
for many years.
Another weakness of linear congruential generators is that their low order digits are
much less random than their high order digits. Therefore, when only part of a generated
random number is used, one should pick the high order digits.

70
Chapter 1. Principles of Numerical Calculations
One approach to better generators is to combine two RNGs. One possibility is to
use a second RNG to shufﬂe the output of a linear congruential generator. In this way it is
possible to get rid of some serial correlations in the output; see the generator ran1 described
in Press et al. [294, Chapter 7.1].
A good generator should have been analyzed theoretically and be supported by prac-
tical evidence from extensive statistical and other tests. Knuth [230, Chapter 3] points out
important ideas, concepts, and facts of the topic, but also mentions some scandalously poor
RNGs that were in widespread daily use for decades as standard tools in computer libraries.
Although the generators in daily use have improved, many are still not satisfactory. He
ends this masterly chapter on random numbers with the following exercise: “Look at the
subroutine library at your computer installation, and replace the random number generators
by good ones. Try to avoid being too shocked at what you ﬁnd.”
L’Ecuyer [244] writes in 2001:
Unfortunately, despite repeated warnings over the past years about certain
classes of generators, and despite the availability of much better alternatives,
simplistic and unsafe generators still abound in commercial software.
L’Ecuyer reports on tests of RNGs used in some popular software products. Microsoft Excel
used the linear congruential generator
ui = 9821.0un−1 + 0.211327 mod 1,
implemented directly for the ui in ﬂoating-point arithmetic. Its period length depends on
the precision of the arithmetic and it is not clear what it is. Microsoft Visual Basic used a
linear congruential generator with period 224, deﬁned by
xi = 1140671485xi−1 + 12820163 mod (224),
and takes ui = xi/224. The Unix standard library uses the recurrence
xi = 25214903917xi−1 + 12820163 mod (248),
with period 248 and sets ui = xi/248. The Java standard library uses the same recurrence
but constructs random deviates ui from x2i and x2i+1.
In MATLAB 5 and later versions the previous linear congruential generator has been
replaced with a much better generator, based on ideas of Marsaglia; see Figure 1.6.2 (right).
This generator has a 35 element state vector and can generate all the ﬂoating-point numbers
in the closed interval [2−53, 1 −2−53]. Theoretically it can generate 21492 values before
repeating itself; see Moler [266]. If one generates one million random numbers a second it
would take 10435 years before it repeats itself!
Some modern linear RNGs can generate huge samples of pseudorandom numbers very
fast and reliably. The multiple recursive generator MRG32k3a proposed by L’Ecuyer has
a period near 2191. The Mersenne twister MT19937 by Matsumoto and Nishimura [261],
the “world champion” of RNGs in the year 2000, has a period length of 219,937 −1!
Many statistical tests have been adapted and extended for the examination of arith-
metic methods of (pseudo)random number generation. In these, the observed frequencies

1.6. Monte Carlo Methods
71
0
0.2
0.4
0.6
0.8
1
x 10
−3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
x 10
−3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1.6.2. Plots of pairs of 106 random uniform deviates (Ui, Ui+1) such that
Ui < 0.0001. Left: MATLAB 4; Right: MATLAB 5.
for some random variable associated with the test are compared with the theoretical frequen-
cies on the hypothesis that the numbers are independent observations from a true sequence
of random digits without bias. This is done by means of the famous χ2-test of K. Pear-
son [287],28 which we now describe.
Suppose that the space S of the random variable is divided into a ﬁnite number r of
nonoverlapping parts S1, . . . , Sr. These parts may be groups into which the sample values
have been arranged for tabulation purposes. Let the corresponding group probabilities be
pi = Pr(Si),
i = 1, . . . , r,
r

i=1
pi = 1.
We now form a measure of the deviation of the observed frequencies ν1, . . . , νr, 
i νi = n,
from the expected frequencies
χ2 =
r

i=1
(νi −npi)2
npi
=
r

i=1
ν2
i
npi
−n.
(1.6.4)
It is known that as n tends to inﬁnity the distribution of χ2 tends to a limit independent of
P r(Si), which is the χ2-distribution with r −1 degrees of freedom.
Let χ2
p be a value such that Pr(χ2 > χ2
p) = p %. Here p is chosen so small that we
are practically certain that an event of probability p % will not occur in a single trial. The
28This paper, published in 1900 by the English mathematician Karl Pearson (1857–1936), is considered one of
the foundations of modern statistics. In it he gave several examples, such as proving that some runs at roulette
that he had observed during a visit to Monte Carlo were so far from expectations that the odds against an honest
wheel were about 1029 to one.

72
Chapter 1. Principles of Numerical Calculations
hypothesis is rejected if the observed value of χ2 is larger than χ2
p. Often a rejection level
of 5% or 1% is used.
Example 1.6.2.
In an experiment consisting of n = 4040 throws with a coin, ν1 = 2048 heads were
obtained and hence ν2 = n −ν1 = 1992 tails. Is this consistent with the hypothesis that
there is a probability of p1 = 1/2 of throwing tails? Computing
χ2 = (ν1 −np1)2
np1
+ (n −ν1 −np1)2
np1
= 2(2048 −2020)2
2020
= 0.776
and using a rejection level of 5%, we ﬁnd from a table of the χ2-distribution with one degree
of freedom that χ2
5 = 3.841. Hence the hypothesis is accepted at this level.
Several tests that have been used for testing RNGs are described in Knuth [230,
Sec. 3.3]. Some of them are the following:
1. Frequency test. This test is to ﬁnd out if the generated numbers are equidistributed.
One divides the possible outcomes into equal nonoverlapping intervals and tallies the
number of numbers in each interval.
2. Poker test. This test applies to generated digits, which are divided into nonoverlap-
ping groups of ﬁve digits. Within the groups we study some (unordered) combinations
of interest in poker. These are given below, together with their probabilities.
All different:
abcde
0.3024
One pair:
aabcd
0.5040
Two pairs:
aabbc
0.1080
Three of a kind:
aaabc
0.0720
Full house:
aaabb
0.0090
Four of a kind:
aaaab
0.0045
Five of a kind:
aaaaa
0.0001
3. Gap test. This test examines the length of “gaps” between occurrences of Uj in a
certain range. If α and β are two numbers with 0 ≤α < β ≤1, we consider the
length of consecutive subsequences Uj, Uj+1, . . . , Uj+r in which Uj+r lies between
α and β but Uj, Uj+1, . . . , Uj+r−1 do not. These subsequence then represents a gap
of length r.
The special cases (α, β) = (0, 1/2) or (1/2, 1) give rise to tests called “runs above
the mean” and “runs below the mean,” respectively.
Working with single digits, we see that the gap equals the distance between two equal
digits. The probability of a gap of length r in this case equals
pr = 0.1(1 −0.1)r = 0.1(0.9)r,
r = 0, 1, 2, . . . .

1.6. Monte Carlo Methods
73
Example 1.6.3.
To test the two-dimensional behavior of an RNG we generated 106 pseudorandom
numbers Ui. We then placed the numbers (Ui, Ui+1) in the unit square of the plot. A thin
slice of the surface of the square, 0.0001 wide by 1.0 high, was then cut on its left side and
stretched out horizontally. This corresponds to plotting only the pairs (Ui, Ui+1) such that
Ui < 0.0001 (about 1000 points).
In Figure 1.6.2 we show the two plots from the generators in MATLAB 4 and
MATLAB 5, respectively. The lattice structure is quite clear in the ﬁrst plot. With the
new generator no lattice structure is visible.
A statistical test studied by Knuth [230] is the collision test. In this test the interval
useful [0, 1) is ﬁrst cut into n equal intervals, for some positive integer n. This partitions the
hypercube [0, 1)d into k = nd cubic boxes. Then N random points are generated in [0, 1)d
and we record the number of times C that a point falls in a box that already has a point in it.
The expectation of the random number C is known to be of very good approximation when
N is large. Indeed, C follows approximatively a Poisson distribution with mean equal to
N2/(2k).
For this and other similar tests it has been observed that when the sample size N is
increased the test starts to fail when N reaches a critical value N0, and the failure is clear
for all larger values of N. For the collision test it was observed by L’Ecuyer [244] that
N0 ≈16ρ1/2 for good linear congruential generators,where ρ is the period of the RNG. For
another statistical test called the birthday spacing test the relation was N0 ≈16ρ1/3.
From such tests is can be concluded that when large sample sizes are needed many
RNGs are unsafe to use and can fail decisively. A period of 224 or even 248 may not be
enough. Linear RNGs are also unsuitable for cryptographic applications, because the output
is too predictable. For this reason, nonlinear generators have been developed, but these are
in general much slower than the linear generators.
1.6.3
Random Deviates for Other Distributions
We have so far discussed how to generate sequences that behave as if they were random
uniform deviates U on [0, 1). By arithmetic operations one can form random numbers with
other distributions. A simple example is that the random numbers
S = a + (b −a)U
will be uniformly distributed on [a, b).
Monte Carlo methods often call for other kinds of distributions. We shall show here
how to use uniform deviates to generating random deviates X for several other distributions.
Many of the tricks used were originally suggested by John von Neumann in the early 1950s,
but have since been improved and reﬁned.
Discrete Distributions
Making a random choice from a ﬁnite number k of equally probable possibilities is equiva-
lent to generating a random integer X between 1 and k. To do this we take a random deviate

74
Chapter 1. Principles of Numerical Calculations
U uniformly distributed on [0, 1), multiply it by k, and take the integer part
X = ⌈kU⌉;
here ⌈x⌉denotes the smallest integer larger than or equal to x. There will be a small error
because the set of ﬂoating-point numbers is ﬁnite, but this is usually negligible.
In a more general situation, we might want to give different probabilities to the values
of a variable. Suppose we assign the values X = xi, i = 1 : k the probabilities pi, i = 1 : k;
note that  pi = 1. We can then generate a uniform number U and let
X =



x1
if 0 ≤U < p1,
x2
if p1 ≤U < p1 + p2,
...
xk
if p1 + p2 + · · · pk−1 ≤U < 1.
If k is large, and the sequence {pi} is irregular, it may require some thought how to ﬁnd
x quickly for a given u. See the analogous question of ﬁnding a ﬁrst guess to the root of
(1.6.5) below and the discussion in Knuth [230, Sec. 3.4.1].
A General Transformation from U to X
Suppose we want to generate numbers for a random variable X with a given continuous
or discrete distribution function F(x). (In the discrete case, the graph of the distribution
function becomes a staircase; see the formulas above.) A general method for this is to solve
the equation
F(X) = U
(1.6.5)
or, equivalently, X = F −1(U); see Figure 1.6.3. Because F(x) is a nondecreasing function,
and P r{U ≤u} = u for all u ∈[0, 1], (1.6.5) is proved by the line
P r{X ≤x} = Pr{F(X) ≤F(x)} = Pr{U ≤F(x)} = F(x).
How to solve (1.6.5) efﬁciently is the main problem with this method. For some distributions
we shall describe better methods below.
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
F(x)
X
R
Figure 1.6.3. Random number with distribution F(x).

1.6. Monte Carlo Methods
75
Exponential Deviates
The exponential distribution with parameter λ > 0 occurs in queuing problems, for example,
in telecommunications, to model arrival and service times. The important property is that
the intervals of time between two successive events are a sequence of exponential deviates.
The exponential distribution with mean 1/λ has density function f (t) = λe−λt, t > 0, and
distribution function
F(x) =
 x
0
λe−λt dt = 1 −e−λx.
(1.6.6)
Using the general rule given above, exponentially distributed random numbers X can be
generated as follows: Let U be a uniformly distributed random number in [0, 1]. Solving
the equation 1 −e−λX = U, we obtain
X = −λ−1 ln(1 −U).
A drawback of this method is that the evaluation of the logarithm is relatively slow.
One important use of exponentially distributed random numbers is in the generation of
so-called Poisson processes. Such processes are often fundamental in models of telecom-
munication systems and other service systems. APoisson process with frequency parameter
λ is a sequence of events characterized by the property that the probability of occurrence
of an event in a short time interval (t, t + 4t) is equal to λ·4t + o(4t), independent of
the sequence of events previous to time t. An “event” can mean a call on a telephone line,
the arrival of a customer to a store, etc. For simulating a Poisson process one can use the
important property that the intervals of time between two successive events are independent
exponentially distributed random numbers.
Normal Deviates
A normal deviate N = N(0, 1) with zero mean and unit standard deviation has the density
function
f (x) =
1
√
2π
e−x2/2.
Then µ + σN is a normal deviate with mean µ and standard deviation σ with density
function 1
σ f ((x −µ)/σ). Since the normal distribution function
M(x) =
1
√
2π
 x
−∞
e−t2/2 dt
(1.6.7)
is not an elementary function, solving equation (1.6.5) would be time consuming.
Fortunately, random normal deviates can be obtained in easier ways. In the polar
algorithm a random point in the unit disk is ﬁrst generated as follows. Let U1, U2 be
two independent uniformly distributed random numbers on [0, 1]. Then the point (V1, V2),
where Vi = 2Ui −1, i = 1, 2, is uniformly distributed in the square [−1, 1] × [−1, 1]. If
we compute S = V 2
1 + V 2
2 and reject the point if it is outside the unit circle, i.e., if S > 1,
remaining points will be uniformly distributed on the unit disk. For each accepted point we
then form
N1 = τV1,
N2 = τV2,
τ =
$
−2 log S
S
.
(1.6.8)
It can be proved that N1, N2 are two independent normally distributed random numbers
with zero mean and unit standard deviation.

76
Chapter 1. Principles of Numerical Calculations
We point out that N1, N2 can be considered to be rectangular coordinates of a point
whose polar coordinates (r, φ) are determined by the equations
r2 = N2
1 + N2
2 = −2 ln S,
cos φ = U1/
√
S,
sin φ = U2/
√
S.
The correctness of the above procedure follows from the fact that the distribution function
for a pair of independent normally distributed random variables is rotationally symmetric
(uniformly distributed angle) and that their sum of squares is exponentially distributed with
mean 2. For a proof of this, see Knuth [230, p. 123].
The polar algorithm (used previously in MATLAB 4) is not optimal. First, about
1 −π/4 ≈21.5% of the uniform numbers are rejected because the generated point falls
outside the unit disk. Further, the calculation of the logarithm contributes signiﬁcantly to the
cost. From MATLAB version 5 and later, a more efﬁcient table look-up algorithm developed
by Marsaglia and Tsang [260] is used. This is called the “ziggurat” algorithm after the name
of ancient Mesopotamian terraced temple mounds which look like two-dimensional step
functions. A popular description of the ziggurat algorithm is given by Moler [267]; see also
[220].
Example 1.6.4.
Tosimulateatwo-dimensionalBrownianmotion, trajectoriesaregeneratedasfollows.
Initially the particle is located at the origin w0 = (0, 0)T . At each time step the particle
moves randomly,
wk+1 = wk + h

N1k
N2k

,
k = 0 : n,
where N1k and N2k are normal random deviates generated according to (1.6.8). Figure 1.6.4
shows plots of 32 simulated paths with h = 0.1, each consisting of n = 64 time steps.
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 1.6.4. Simulated two-dimensional Brownian motion. Plotted are 32 simu-
lated paths with h = 0.1, each consisting of 64 steps.

1.6. Monte Carlo Methods
77
Chi-Square Distribution
The chi-square distribution function P(χ2, n) is related to the incomplete gamma function
(see Abramowitz and Stegun [1, Sec. 6.5]):
P(χ2, n) = γ (n/2, χ2/2).
(1.6.9)
Its complement Q(χ2, n) = 1 −P(χ2, n) is the probability that the observed chi-square
will exceed the value χ2, even for a correct model. Subroutines for evaluating the χ2-
distribution function as well as other important statistical distribution functions are given
in [294, Sec. 6.2–6.3].
Numbers belonging to the chi-square distribution can also be obtained by using the
deﬁnition of the distribution. If N1, N2, . . . , Nn are normal deviates with zero mean and
unit variance, the number
Yn = N2
1 + N2
2 + · · · + N2
n
is distributed as χ2 with n degrees of freedom.
Other Methods
Several other methods to generate random deviates with Poisson, gamma, and binomial
distribution are described in Knuth [230, Sec. 3.4]) and Press et al. [294, Chapter 7.3]. The
rejection method is based on ideas of von Neumann (1951). A general method introduced
by Marsaglia [257] is the rectangle-wedge-tail method; see references in Knuth [230].
Powerful combinations of rejection methods and the rectangle-wedge-tail method have
been developed.
1.6.4
Reduction of Variance
From statistics, we know that if one makes n independent observations of a quantity whose
standard deviation is σ, then the standard deviation of the mean is σ/√n. Hence, to increase
the accuracy by a factor of 10 (say) we have to increase the number of experiments n by a
factor 100.
Often a more efﬁcient way than increasing the number of samples is to try to decrease
the value of σ by redesigning the experiment in various ways. Assume that one has two
ways (which require the same amount of work) of carrying out an experiment, and these
experiments have standard deviations σ1 and σ2 associated with them. If one repeats the
experiments n1 and n2 times (respectively), the same precision will be obtained if σ1/√n1 =
σ2/√n2, or
n1/n2 = σ 2
1 /σ 2
2 .
(1.6.10)
Thus if a variance reduction by a factor k can be achieved, then the number of experiments
needed is also reduced by the same factor k.

78
Chapter 1. Principles of Numerical Calculations
Example 1.6.5.
In 1777 Buffon29 carried out a probability experiment by throwing sticks over his
shoulder onto a tiled ﬂoor and counting the number of times the sticks fell across the lines
between the tiles. He stated that the favorable cases correspond “to the area of part of the
cycloid whose generating circle has diameter equal to the length of the needle.” To simulate
Buffon’s experiment we suppose a board is ruled with equidistant parallel lines and that a
needle ﬁne enough to be considered a segment of length l not longer than the distance d
between consecutive lines is thrown on the board. The probability is then 2l/(πd) that it
will hit one of the lines.
The Monte Carlo method and this game can be used to approximate the value of π.
Take the distance δ between the center of the needle and the lines and the angle φ between
the needle and the lines to be random numbers. By symmetry we can choose these to be
rectangularly distributed on [0, d/2] and [0, π/2], respectively. Then the needle hits the
line if δ < (l/2) sin φ.
We took l = d. Let m be the number of hits in the ﬁrst n throws in a Monte Carlo
simulation with 1000 throws. The expected value of m/n is therefore 2/π, and so 2n/m
is an estimate of π after n throws. In the left part of Figure 1.6.5 we see how 2n/m varies
with n in one simulation. The right part compares |m/n −2/π| with the standard deviation
of m/n, which equals
$
2
π

1 −2
π
1
n
and is, in the log-log diagram, represented by a straight line, the slope of which is −1/2.
This can be taken as a test that the RNG in MATLAB is behaving correctly! (The spikes,
directed downward in the ﬁgure, typically indicate where m/n −2/π changes sign.)
0
200
400
600
800
1000
2
2.5
3
3.5
4
4.5
n
estimate of pi 
10
0
10
1
10
2
10
3
10
−4
10
−3
10
−2
10
−1
10
0
n
|m/n−2/pi|
Figure 1.6.5. The left part shows how the estimate of π varies with the number of
throws. The right part compares |m/n −2/π| with the standard deviation of m/n.
29Comte de Buffon (1707–1788), French natural scientist who contributed to the understanding of probability.
He also computed the probability that the sun would continue to rise after having been observed to rise on n
consecutive days.

1.6. Monte Carlo Methods
79
An important means of reducing the variance of estimates obtained from the Monte
Carlo method is to use antithetic sequences. If Ui, i = 1 : n, is a sequence of random
uniform deviates on [0, 1], then U ′
i = 1−Ui, i = 1 : n, is an antithetic uniformly distributed
sequence. From the sequence in (1.6.1) we get the antithetic sequence
0.443065, 0.970545, 0.182765, 0.564115, 0.186495, 0.236975, . . . .
(1.6.11)
Antithetic sequences of normally distributed numbers with zero mean are obtained simply
by reversing the sign of the original sequence.
Roughly speaking, since the inﬂuence of chance has opposing effects in the two
antithetic experiments, one can presume that the effect of chance on the means is much less
than the effect of chance in the original experiments. In the following example we show
how to make a quantitative estimate of the reduction of variance accomplished with the use
of antithetic experiments.
Example 1.6.6.
Suppose the numbers xi are the results of statistically independent measurements of
a quantity with expected value µ, and standard deviation σ. Set
¯x = 1
n
n

i=1
xi,
s2 =
1
n −1
n

i=1
(xi −¯x)2.
Then ¯x is an estimate of µ, and s is an estimate of σ.
In ten simulations and their antithetic experiments of a service system, the following
values were obtained for the treatment time:
685
1 045
718
615
1 021
735
675
635
616
889 .
From this experiment the mean for the treatment time is estimated as 763.4, and the standard
deviation 51.5. Using an antithetic series, the following values were obtained:
731
521
585
710
527
574
607
698
761
532 .
The series means are thus
708
783
651.5
662.5
774
654.5
641
666.5
688.5
710.5 ,
from which one gets the estimate 694.0 ± 15.9.
When one instead supplements the ﬁrst sequence with 10 values using independent
random numbers, the estimate 704 ± 36 using all 20 values is obtained. These results
indicate that, in this example, using antithetical sequence produces the desired accuracy
with (15.9/36)2 ≈1/5 of the work required if completely independent random numbers
are used. This rough estimate of the work saved is uncertain, but indicates that it is very
proﬁtable to use the technique of antithetic series.
Example 1.6.7.
Monte Carlo methods have been used successfully to study queuing problems. A
well-known example is a study by Bailey [12] to determine how to give appointment times

80
Chapter 1. Principles of Numerical Calculations
to patients at a polyclinic. The aim is to ﬁnd a suitable balance between the mean waiting
times of both patients and doctors. This problem was in fact solved analytically—much
later—after Bailey already had the results that he wanted; this situation is not uncommon
when numerical methods (and especially Monte Carlo methods) have been used.
Suppose that k patients have been booked at the time t = 0 (when the clinic opens), and
that the rest of the patients (altogether 10) are booked at intervals of 50 time units thereafter.
The time of treatment is assumed to be exponentially distributed with mean 50. (Bailey used
a distribution function which was based on empirical data.) We use the following numbers
which are taken from a table of exponentially distributed random numbers with mean 100:
211
3
53
159
24
35
54
39
44
13 .
Three alternatives, k = 1, 2, 3, are to be simulated. By using the same random numbers for
each k (hence the same treatment times) one gets a reduced variance in the estimate of the
change in waiting times as k varies. s
The computations are shown in Table 1.6.1. The following abbreviations are used
in what follows: P = patient, D = doctor, T = treatment. An asterisk indicates that
the patient did not need to wait. In the table, Parr follows from the rule given previously
for booking patients. The treatment time Ttime equals R/2, where R are exponentially
distributed numbers with mean 100 taken from a table, i.e., the mean treatment time is 50.
Tbeg equals the larger of the number Parr (on the same row) and Tend (in the row just above),
where Tend = Tbeg + Ttime.
Table 1.6.1. Simulation of waiting times for patients at a polyclinic.
k = 1
k = 2
Pno
Parr
Tbeg
R
Ttime
Tend
Parr
Tend
1
0∗
0
211
106
106
0∗
106
2
50
106
3
2
108
0
108
3
100
108
53
26
134
50
134
4
150∗
150
159
80
230
100
214
5
200
230
24
12
242
150
226
6
250∗
250
35
18
268
200
244
7
300∗
300
54
27
327
250∗
277
8
350∗
350
39
20
370
300∗
320
9
400∗
400
44
22
422
350∗
372
10
450∗
450
13
6
456
400∗
406
H
2250
319
2663
1800
2407
From the table we ﬁnd that for k = 1 the doctor waited the time D = 456−319 = 137;
the total waiting time for patients was P = 2663 −2250 −319 = 94. For k = 2 the
corresponding waiting times were D = 406−319 = 87 and P = 2407−1800−319 = 288.
Similar calculations for k = 3 gave D = 28 and P = 553 (see Figure 1.6.6). For k ≥4 the
doctor never needs to wait.

Review Questions
81
0
10
20
30
40
50
60
2
4
6
8
10
12
14
k = 1
k = 2
k = 3
Mean waiting time for patients
Mean waiting time for doctor
Figure 1.6.6. Mean waiting times for doctor and patients at polyclinic.
One cannot, of course, draw any tenable conclusions from one experiment. More
experiments should be done in order to put the conclusions on statistically solid ground.
Even isolated experiments, however, can give valuable suggestions for the planning of
subsequent experiments, or perhaps suggestions of appropriate approximations to be made
in the analytic treatment of the problem. The large-scale use of Monte Carlo methods
requires careful planning to avoid drowning in enormous quantities of unintelligible results.
Two methods for reduction of variance have been introduced here: antithetic se-
quence of random numbers and the technique of using the same random numbers in corre-
sponding situations. The latter technique is used when studying the changes in behavior of a
system when a certain parameter is changed, for example, the parameter k in Example 1.6.7.
Note that for this we need to be able to restart the RNG using the same seed. Other effective
methods for reducing variance are importance sampling and splitting techniques; see
Hammersley and Handscomb [183].
Review Questions
1.6.1 What is meant by the Monte Carlo method? Describe the origin of the method and give
some typical applications. In general, how fast does the error decrease in estimates
obtained with the Monte Carlo method?
1.6.2 Describe a linear congruential generator for generating a sequence of uniformly dis-
tributed pseudorandom numbers.
What are some important properties of such a
generator?
1.6.3 Describe a general method for obtaining random numbers with a given discrete or
continuous distribution from uniformly distributed random numbers. Give examples
of its use.
1.6.4 Describe some statistical tests which can be applied to an RNG.

82
Chapter 1. Principles of Numerical Calculations
1.6.5 What are the most important properties of a Poisson process? How can one generate
a Poisson process with the help of random numbers?
1.6.6 What is the mixed congruential method for generating pseudorandom numbers? What
important difference is there between the numbers generated by this method and
“genuine” random numbers?
1.6.7 Explain what is meant by reduction of variance in estimates made with the Monte
Carlo method. Give three methods for reduction of variance. What is the quantitative
connection between reducing variance and decreasing the amount of computation
needed in a given problem?
Problems and Computer Exercises
1.6.1 (C. Moler) Consider the toy RNG, xi = axi mod m, with a = 13, m = 31, and start
with x0 = 1. Show that this generates a sequence consisting of a permutation of all
integers from 1 to 30, and then repeats itself. Conclude that this generator has period
m −1 = 30, equal to the maximum possible.
1.6.2 Simulate (say) 360 throws with two standard dice. Denote the sum of the number
of dots on the two dice on the nth throw by Yn, 2 ≤Yn ≤12. Tabulate or draw a
histogram of the (absolute) frequency of the occurrence of j dots versus j, j = 2 : 12.
Make a conjecture about the true value of Pr(Yn = j). Try to conﬁrm it by repeating
the experiment with fresh uniform random numbers. When you have found the right
conjecture, you will ﬁnd that it is not hard to prove.
1.6.3 (a) Let X, Y be independent uniform random numbers on the interval [0, 1]. Show that
P r(X2 +Y 2 ≤1) = π/4, and estimate this probability by a Monte Carlo experiment
with (say) 1000 pairs of random numbers. Produce a graphical output like in the
Buffon needle problem.
(b) Conduct an antithetic experiment, and take the average of the two results. Is the
average better than one could expect if the second experiment had been independent
of the ﬁrst one?
(c) Estimate similarly the volume of the four-dimensional unit ball. If you have
enough time, use more random numbers. (The exact volume of the unit ball is π2/2.)
1.6.4 A famous result by P. Diaconis asserts that it takes approximately 3
2 log2 52 ≈8.55
rifﬂe shufﬂes to randomize a deck of 52 cards, and that randomization occurs abruptly
according to a “cutoff phenomenon.” (After six shufﬂes the deck is still far from
random!)
The following deﬁnition can be used for simulating a rifﬂe shufﬂe. The deck of cards
is ﬁrst cut roughly in half according to a binomial distribution, i.e., the probability
that ν cards are cut is
n
ν

2−n. The two halves are then rifﬂed together by dropping
cards roughly alternately from each half onto a pile, with the probability of a card
being dropped from each half being proportional to the number of cards in it.
Write a program that uses uniform random numbers, and perhaps uses the formula
X = ⌈kR⌉, for several values of k, to simulate a random shufﬂe of a deck of 52 cards
according to the above precise deﬁnition. This is for a numerical game; do not spend
time drawing beautiful hearts, clubs, etc.

Notes and References
83
1.6.5 Brownian motion is the irregular motion of dust particles suspended in a ﬂuid, being
bombarded by molecules in a random way. Generate two sequences of random normal
deviates ai and bi, and use these to simulate Brownian motion by generating a path
deﬁned by the points (xi, yi), where x0 = y0 = 0, xi = xi−1 + ai, yi = yi−1 + bi.
Plot each point and connect the points with a straight line to visualize the path.
1.6.6 Repeat the simulation in the queuing problem in Example 1.6.7 for k = 1 and k = 2
using the sequence of exponentially distributed numbers R,
13
365
88
23
154
122
87
112
104
213 ,
antithetic to that used in Example 1.6.7. Compute the mean of the waiting times for
the doctor and for all patients for this and the previous experiment.
1.6.7 A target with depth 2b and very large width is to be shot at with a cannon. (The
assumption that the target is very wide makes the problem one-dimensional.) The
distance to the center of the target is unknown, but estimated to be D. The difference
between the actual distance and D is assumed to be a normally distributed random
variable X = N(0, σ1).
One shoots at the target with a salvo of three shots, which are expected to travel a
distance D−a, D, and D+a, respectively. The difference between the actual and the
expected distance traveled is assumed to be a normally distributed random variable
N(0, σ2); the resulting error component in the three shots is denoted by Y−1, Y0, Y1.
We further assume that these three variables are independent of each other and X.
One wants to know how the probability of at least one “hit” in a given salvo depends
on a and b. Use normally distributed pseudorandom numbers to shoot 10 salvos and
determine for each salvo the least value of b for which there is at least one hit in the
salvo. Show that this is equal to
min
k
|X −(Yk + ka)|,
k = −1, 0, 1.
Fire an antithetic salvo for each salvo.
Draw curves, for both a = 1 and a = 2, which give the probability of a hit as a
function of the depth of the target. Use σ1 = 3 and σ2 = 1, and the same random
numbers.
Notes and References
The methods and problems presented in this introductory chapter will be studied in greater
detail later in this volume and in Volume II. In particular, numerical quadrature methods are
studied in Chapter 5 and methods for solving a single nonlinear equation in Chapter 6. For
a survey of sorting algorithms we refer to [294, Chapter 8]. A comprehensive treatment of
sorting and searching is given in Knuth [231].
Although the history of Gaussian elimination goes back at least to Chinese mathemati-
cians (about 250 B.C.), there was no practical experience of solving large linear systems
until the advent of computers in the 1940s. Gaussian elimination was the ﬁrst numerical
algorithm to be subjected to a rounding error analysis. In 1946 there was a mood of pes-
simism about the stability of Gaussian elimination. Bounds had been produced showing

84
Chapter 1. Principles of Numerical Calculations
that the error in the solution would be proportional to 4n. This suggested that it would
be impossible to solve even systems of modest order. A few years later J. von Neumann
and H. H. Goldstein published more relevant error bounds. In 1948 A. M. Turing wrote a
remarkable paper [362], in which he formulated the LU factorization and introduced matrix
condition numbers.
Several of the great mathematicians at the turn of the nineteenth century worked on
methods for solving overdetermined linear systems. In 1799, Laplace used the principle of
minimizing the sum of absolute errors |ri|, with the added conditions that the errors sum to
zero. This leads to a solution x that satisﬁes at least n equations exactly. The method of least
squares was ﬁrst published as an algebraic procedure by Legendre in 1805 [245]. Gauss
justiﬁed the least squares principle as a statistical procedure in [138], where he claimed to
have used the method since 1795. This led to one of the most famous priority disputes in
the history of mathematics. Gauss further developed the statistical aspects in 1821–1823.
For an interesting account of the history of the invention of least squares, see Stigler [337].
For a comprehensive treatment of all aspects of random numbers we refer to
Knuth [230]. Another good reference on the state of the art is the monograph by Niederre-
iter [275]. Guidelines for choosing a good RNG are given in Marsaglia [259], the monograph
by Gentle [152], and in the two surveys L’Ecuyer [242, 243]. Hellekalek [191] explains how
to access RNGs for practitioners. An introduction to Monte Carlo methods and their appli-
cations is given by Hammersley and Handscomb [183]. There is a close connection between
random number generation and data encryptation; see Press et al. [294, Chapter 7.5].
Some later chapters in this book assume a working knowledge in numerical linear
algebra. Online Appendix A gives a brief survey of matrix computations. A more in-depth
treatment of direct and iterative methods for linear systems, least squares, and eigenvalue
problems is planned for Volume II. Some knowledge of modern analysis including analytic
functions is also needed for some more advanced parts of the book. The classical textbook
by Apostol [7] is highly recommended as a suitable reference.
The James & James Mathematics Dictionary [210] is a high-quality general mathe-
matics dictionary covering arithmetic to calculus, and it includes a multilingual index. CRC
Concise Encyclopedia of Mathematics [370] is a comprehensive compendium of mathe-
matical deﬁnitions, formulas, and references. A free Web encyclopedia containing surveys
and references is Eric Weisstein’s MathWorld at mathworld.wolfram.com.
The development of numerical analysis during the period when the foundation was
laid in the sixteenth through the nineteenth century is traced in Goldstine [160]. Essays
on the history of scientiﬁc computing can be found in Nash [274]. An interesting account
of the developments in the twentieth century is given in [56]. An eloquent essay on the
foundations of computational mathematics and its relation to other ﬁelds is given by Baxter
and Iserles [21].
In 2000–2001, the Journal of Computational and Applied Mathematics published a
series of papers on Numerical Analysis of the 20th Century, with the aim of presenting the
historical development of numerical analysis and reviewing current research. The papers
were arranged in seven volumes; see Online Appendix C3.
Wegivebelowaselectionoftextbooksandreviewpapersonnumericalmethods. Even
though the selection is by no means complete and reﬂects a subjective choice, we hope it can
serve as a guide for a reader who, out of interest (or necessity!), wishes to deepen his or her
knowledge. Both recent textbooks and older classics are included. Note that reviews of new

Notes and References
85
books can be found in Mathematical Reviews as well as in the journals SIAM Review and
Mathematics of Computation. A more complete guide to relevant literature and software is
given in Online Appendix C.
Many outstanding textbooks in numerical analysis were originally published in the
1960s and 70s. The classical text by Hildebrand [201] can still be used as an introduction.
Isaacson and Keller [208] give a rigorous mathematical treatment of classical topics, in-
cluding differential equations and orthogonal polynomials. The present authors’ textbook
[89] was used at many universities in the USA and is still available.
Hamming [184] is a more applied text and aims at combining mathematical theory,
heuristic analysis, and computing methods. It emphasizes the message that “the purpose
of computing is insight, not numbers.” An in-depth treatment of several areas such as
numerical quadrature and approximation is found in the comprehensive book by Ralston
and Rabinowitz [296]. This book also contains a large number of interesting and fairly
advanced problems.
The book by Forsythe, Malcolm, and Moler [123] is notable in that it includes a set
of Fortran subroutines of unusually high quality. Kahaner, Moler, and Nash [220] comes
with a disk containing software. A good introduction to scientiﬁc computing is given by
Golub and Ortega [166]. A matrix-vector approach is used in the MATLAB oriented text
of Van Loan [367]. Analysis is complemented with computational experiments using a
package of more than 200 m-ﬁles. Cheney and Kincaid [68] is an undergraduate text with
many examples and exercises. Kincaid and Cheney [226] is a related textbook but more
mathematically oriented. Two other good introductory texts are Eldén, Wittmeyer-Koch,
and Nielsen [109] and Süli and Mayers [344].
Heath [190] is a popular, more advanced, and comprehensive text. Gautschi [147] is
an elegant introductory text containing a wealth of computer exercises. Much valuable and
hard-to-ﬁnd information is included in notes after each chapter. The bestseller by Press et
al. [294] surveys contemporary numerical methods for the applied scientist, but is weak on
analysis.
Several good textbooks have been translated from German, notably the excellent book
by Stoer and Bulirsch [338]. This is particularly suitable for a reader with a good mathemati-
cal background. Hämmerlin and Hoffmann [182] and Deuﬂhard and Hohmann [96] are less
comprehensive but with a modern and careful treatment. Schwarz [318] is a mathematically
oriented text which also covers ordinary and partial differential equations. Rutishauser [312]
is an annotated translation of a highly original textbook by one of the pioneers of numerical
analysis. Though brief, the book by Tyrtychnikov [363] is original and thorough. It also
contains references to Russian literature unavailable in English.
Since numerical analysis is still in a dynamic stage it is important to keep track of new
developments. An excellent source of survey articles on topics of current interest can be
found in Acta Numerica, a Cambridge University Press Annual started in 1992. The journal
SIAM Review also publishes high-quality review papers.
Another collection of outstanding survey papers on special topics is being published in
a multivolume sequence in the Handbook of Numerical Analysis [70], edited by Philippe G.
Ciarlet and Jacques-Louis Lions. It offers comprehensive coverage in all areas of numerical
analysis as well as many actual problems of contemporary interest; see section C3 in Online
Appendix C.


Chapter 2
How to Obtain and
Estimate Accuracy
I always think I used computers for what
God had intended them for, to do arithmetic.
—Cleve Moler
2.1
Basic Concepts in Error Estimation
The main purpose of numerical analysis and scientiﬁc computing is to develop efﬁcient and
accurate methods to compute approximations to quantities that are difﬁcult or impossible to
obtain by analytic means. It has been convincingly argued (Trefethen [357]) that controlling
rounding errors is just a small part of this, and that the main business of computing is the
development of algorithms that converge rapidly. Even if we acknowledge the truth of this
statement, it is still necessary to be able to control different sources of errors, including
roundoff errors, so that these will not interfere with the computed results.
2.1.1
Sources of Error
Numerical results are affected by many types of errors. Some sources of error are difﬁcult
to inﬂuence; others can be reduced or even eliminated by, for example, rewriting formulas
or making other changes in the computational sequence. Errors are propagated from their
sources to quantities computed later, sometimes with a considerable ampliﬁcation or damp-
ing. It is important to distinguish between the new error produced at the computation of a
quantity (a source error), and the error inherited (propagated) from the data that the quantity
depends on.
A. Errors in Given Input Data.
Input data can be the result of measurements which have been contaminated by
different types of errors. In general one should be careful to distinguish between
systematic errors and random errors. A systematic error can, for example, be
produced by insufﬁciencies in the construction of an instrument of measurement;
87

88
Chapter 2. How to Obtain and Estimate Accuracy
such an error is the same in each trial. Random errors depend on the variation in the
experimental environment which cannot be controlled.
B. Rounding Errors During the Computations.
Arounding error occurs whenever an irrational number, for example π, is shortened
(“rounded off”) to a ﬁxed number of digits, or when a decimal fraction is converted
to the binary form used in the computer. The limitation of ﬂoating-point numbers in
a computer leads at times to a loss of information that, depending on the context, may
or may not be important. Two typical cases are
(i) If the computer cannot handle numbers which have more than, say, s digits, then
the exact product of two s-digit numbers (which contains 2s or 2s −1 digits) cannot
be used in subsequent calculations; the product must be rounded off.
(ii) In a ﬂoating-point computation, if a relatively small term b is added to a, then
some digits of b are “shifted out” (see Example 2.3.1), and they will not have any
effect on future quantities that depend on the value of a + b.
The effect of such rounding can be quite noticeable in an extensive calculation, or in
an algorithm which is numerically unstable.
C. Truncation Errors.
These are errors committed when a limiting process is truncated (broken off) before
one has come to the limiting value. A truncation error occurs, for example, when
an inﬁnite series is broken off after a ﬁnite number of terms, or when a derivative is
approximated with a difference quotient (although in this case the term discretization
error is better). Another example is when a nonlinear function is approximated with
a linear function, as in Newton’s method. Observe the distinction between truncation
error and rounding error.
D. Simpliﬁcations in the Mathematical Model.
In most of the applications of mathematics, one makes idealizations. In a mechanical
problem one might assume that a string in a pendulum has zero mass. In many other
types of problems it is advantageous to consider a given body to be homogeneously
ﬁlled with matter, instead of being built of atoms. For a calculation in economics,
one might assume that the rate of interest is constant over a given period of time. The
effects of such sources of error are usually more difﬁcult to estimate than the types
named in A, B, and C.
E. “Human” Errors and Machine Errors.
In all numerical work, one must expect that clerical errors, errors in hand calculation,
and misunderstandings will occur. One should even be aware that textbooks (!),
tables, and formulas may contain errors. When one uses computers, one can expect
errors in the program itself, typing errors in entering the data, operator errors, and
(less frequently) pure machine errors.
Errors which are purely machine errors are responsible for only a very small part of
the strange results which (occasionally with great publicity) are produced by computers.
Most of the errors depend on the so-called human factor. As a rule, the effect of this type

2.1. Basic Concepts in Error Estimation
89
of error source cannot be analyzed with the help of the theoretical considerations of this
chapter! We take up these sources of error in order to emphasize that both the person who
carries out a calculation and the person who guides the work of others can plan so that
such sources of error are not damaging. One can reduce the risk of such errors by suitable
adjustments in working conditions and routines. Stress and fatigue are common causes of
such errors.
Intermediate results that may reveal errors in a computation are not visible when
using a computer. Hence the user must be able to verify the correctness of his results or
be able to prove that his process cannot fail! Therefore, one should carefully consider
what kind of checks can be made, either in the ﬁnal result or in certain stages of the work,
to prevent the necessity of redoing a whole project just because a small error has been
made in an early stage. One can often discover whether calculated values are of the wrong
order of magnitude or are not sufﬁciently regular, for example, using difference checks (see
Sec. 3.3.1).
Occasionally one can check the credibility of several results at the same time by
checking that certain relations are true. In linear problems, one often has the possibility
of sum checks. In physical problems, one can check to see whether energy is conserved,
although because of error sourcesAto D one cannot expect that it will be exactly conserved.
In some situations, it can be best to treat a problem in two independent ways, although one
can usually (as intimated above) check a result with less work than this.
Errors of type E do occur, sometimes with serious consequences. The ﬁrst American
Venus probe was lost due to a program fault caused by the inadvertent substitution of a
statement in a Fortran program of the form DO 3 I = 1.3 for one of the form DO 3 I
= 1,3. Erroneously replacing the comma “,” with a dot “.” converts the intended loop
statement into an assignment statement! Ahardware error that got much publicity surfaced in
1994, when it was found that the INTEL Pentium processor gave wrong results for division
with ﬂoating-point numbers of certain patterns. This was discovered during research on
prime numbers (see Edelman [103]) and later ﬁxed.
From a different point of view, one may distinguish between controllable and uncon-
trollable (or unavoidable) error sources. Errors of type A and D are usually considered to
be uncontrollable in the numerical treatment (although feedback to the constructor of the
mathematical model may sometimes be useful). Errors of type C are usually controllable.
For example, the number of iterations in the solution of an algebraic equation, or the step
size in a simulation, can be chosen either directly or by setting a tolerance.
The rounding error in the individual arithmetic operation (type B) is, in a computer,
controllable only to a limited extent, mainly through the choice between single and double
precision. A very important fact is, however, that it can often be controlled by appropriate
rewriting of formulas or by other changes of the algorithm; see Example 2.3.3.
If it doesn’t cost too much, a controllable error source should be controlled so that
its effects are evidently negligible compared to the effects of the uncontrollable sources.
A reasonable interpretation of “full accuracy” is that the controllable error sources should
not increase the error of a result by more than about 20%. Sometimes, “full accuracy” may
be expensive, for example, in terms of computing time, memory space, or programming
efforts. Then it becomes important to estimate the relation between accuracy and these cost
factors. One goal of the rest of this chapter is to introduce concepts and techniques useful
for this purpose.

90
Chapter 2. How to Obtain and Estimate Accuracy
Many real-world problems contain some nonstandard features, where understanding
the general principles of numerical methods can save much time in the preparation of a
program as well as in the computer runs. Nevertheless, we strongly encourage the reader
to use quality library programs whenever possible, since a lot of experience and profound
theoretical analysis has often been built into these (sometimes far beyond the scope of this
text). It is not practical to “reinvent the wheel.”
2.1.2
Absolute and Relative Errors
Approximation is a central concept in almost all the uses of mathematics. One must often
be satisﬁed with approximate values of the quantities with which one works. Another
type of approximation occurs when one ignores some quantities which are small compared
to others. Such approximations are often necessary to ensure that the mathematical and
numerical treatment of a problem does not become hopelessly complicated.
We make the following deﬁnition.
Deﬁnition 2.1.1.
Let ˜x be an approximate value whose exact value is x. Then the absolute error in ˜x
is
4x = |˜x −x|,
and if x ̸= 0 the relative error is
4x/x = |(˜x −x)/x|.
In some books the error is deﬁned with the opposite sign to what we use here. It
makes almost no difference which convention one uses, as long as one is consistent. Note
that x −˜x is the correction which should be added to ˜x to get rid of the error. The correction
and the absolute error then have the same magnitude but may have different signs.
In many situations one wants to compute a strict or approximate bound for the absolute
or relative error. Since it is sometimes rather hard to obtain an error bound that is both strict
and sharp, one sometimes prefers to use less strict but often realistic error estimates. These
can be based on the ﬁrst neglected term in some expansion, or on some other asymptotic
considerations.
The notation x = ˜x ± ϵ means, in this book, |˜x −x| ≤ϵ. For example, if x =
0.5876 ± 0.0014, then 0.5862 ≤x ≤0.5890, and |˜x −x| ≤0.0014. In other texts, the
same plus–minus notation is sometimes used for the “standard error” (see Sec. 2.3.3) or
some other measure of deviation of a statistical nature. If x is a vector ∥· ∥, then the error
bound and the relative error bound may be deﬁned as bounds for
∥˜x −x∥
and
∥˜x −x∥/∥x∥,
respectively, where ∥· ∥denotes some vector norm (see Sec.A.3.3 in Online Appendix A).
Then a bound ∥˜x −x∥/∥x∥≤1/2 · 10−p implies that components ˜xi with |˜xi| ≈∥x∥have
about p signiﬁcant digits, but this is not true for components of smaller absolute value. An
alternative is to use componentwise relative errors,
max
i
|˜xi −xi|/|xi|,
(2.1.1)
but this assumes that xi ̸= 0 for all i.

2.1. Basic Concepts in Error Estimation
91
We will distinguish between the terms accuracy and precision. By accuracy we mean
the absolute or relative error of an approximate quantity. The term precision will be reserved
for the accuracy with which the basic arithmetic operations +, −, ∗, / are performed. For
ﬂoating-point operations this is given by the unit roundoff; see (2.2.8).
Numerical results which are not followed by any error estimations should often,
though not always, be considered as having an uncertainty of 1
2 of a unit in the last decimal
place. In presenting numerical results, it is a good habit, if one does not want to go through
the difﬁculty of presenting an error estimate with each result, to give explanatory remarks
such as
• “All the digits given are thought to be signiﬁcant.”
• “The data have an uncertainty of at most three units in the last digit.”
• “For an ideal two-atom gas, cP /cV = 1.4 (exactly).”
We shall also introduce some notations, useful in practice, though their deﬁnitions are
not exact in a mathematical sense:
a ≪b (a ≫b) is read “a is much smaller (much greater) than b.” What is meant by
“much smaller”(or “much greater”) depends on the context—among other things, on
the desired precision.
a ≈b is read “a is approximately equal to b” and means the same as |a −b| ≪c,
where c is chosen appropriate to the context. We cannot generally say, for example,
that 10−6 ≈0.
a ⪅b (or b ⪆a) is read “a is less than or approximately equal to b” and means the
same as “a ≤b or a ≈b.”
Occasionally we shall have use for the following more precisely deﬁned mathematical
concepts:
f (x) = O(g(x)), x →a, means that |f (x)/g(x)| is bounded as x →a
(a can be ﬁnite, +∞, or −∞).
f (x) = o(g(x)), x →a, means that limx→a f (x)/g(x) = 0.
f (x) ∼g(x), x →a, means that limx→a f (x)/g(x) = 1.
2.1.3
Rounding and Chopping
When one counts the number of digits in a numerical value one should not include zeros
in the beginning of the number, as these zeros only help to denote where the decimal point
should be. For example, the number 0.00147 has ﬁve decimals but is given with three digits.
The number 12.34 has two decimals but is given with four digits.
If the magnitude of the error in a given numerical value ˜a does not exceed 1
2 ·10−t,
then ˜a is said to have t correct decimals. The digits in ˜a which occupy positions where the
unit is greater than or equal to 10−t are then called signiﬁcant digits (any initial zeros are

92
Chapter 2. How to Obtain and Estimate Accuracy
not counted). Thus, the number 0.001234 ± 0.000004 has ﬁve correct decimals and three
signiﬁcant digits, while 0.001234 ± 0.000006 has four correct decimals and two signiﬁcant
digits. The number of correct decimals gives one an idea of the magnitude of the absolute
error, while the number of signiﬁcant digits gives a rough idea of the magnitude of the
relative error.
We distinguish here between two ways of rounding off a number x to a given number
t of decimals. In chopping (or round toward zero) one simply leaves off all the decimals to
the right of the tth. That way is generally not recommended since the rounding error has,
systematically, the opposite sign of the number itself. Also, the magnitude of the error can
be as large as 10−t.
In rounding to nearest (sometimes called “correct” or “optimal” rounding), one
chooses a number with s decimals which is nearest to x. Hence if p is the part of the
number which stands to the right of the sth decimal, one leaves the tth decimal unchanged
if and only if |p| < 0.5 ·10−s. Otherwise one raises the sth decimal by 1. In the case of a
tie, when x is equidistant to two s decimal digit numbers, then one raises the sth decimal if
it is odd or leaves it unchanged if it is even (round to even). In this way, the error is positive
or negative about equally often. The error in rounding a decimal number to s decimals will
always lie in the interval [−1
210−s, 1
210−s].
Example 2.1.1.
Shortening to three decimals,
0.2397
rounds to 0.240
(is chopped to 0.239),
−0.2397
rounds to −0.240
(is chopped to −0.239),
0.23750
rounds to 0.238
(is chopped to 0.237),
0.23650
rounds to 0.236
(is chopped to 0.236),
0.23652
rounds to 0.237
(is chopped to 0.236).
Observe that when one rounds off a numerical value one produces an error; thus it is
occasionally wise to give more decimals than those which are correct. Take a = 0.1237 ±
0.0004, which has three correct decimals according to the deﬁnition given previously. If
one rounds to three decimals, one gets 0.124; here the third decimal may not be correct,
since the least possible value for a is 0.1233.
Suppose that you are tabulating a transcendental function and that a particular entry
has been evaluated as 1.2845 correct to the digits given. You want to round the value to
three decimals. Should the ﬁnal digit be 4 or 5? The answer depends on whether there
is a nonzero trailing digit. You compute the entry more accurately and ﬁnd 1.28450, then
1.284500, then 1.2845000, etc. Since the function is transcendental, there clearly is no
bound on the number of digits that has to be computed before distinguishing if to round to
1.284 or 1.285. This is called the tablemaker’s dilemma.30
Example 2.1.2.
The difference between chopping and rounding can be important, as is illustrated by
the following story. The index of the Vancouver Stock Exchange, founded at the initial value
30This can be used to advantage in order to protect mathematical tables from illegal copying by rounding a few
entries incorrectly, where the error in doing so is insigniﬁcant due to several trailing zeros. An illegal copy could
then be exposed simply by looking up these entries!

2.2. Computer Number Systems
93
1000.000 in 1982, was hitting lows in the 500s at the end of 1983 even though the exchange
apparently performed well. It was discovered (The Wall Street Journal, Nov. 8, 1983, p. 37)
that the discrepancy was caused by a computer program which updated the index thousands
of times a day and used chopping instead of rounding to nearest! The rounded calculation
gave a value of 1098.892.
Review Questions
2.1.1 Clarify with examples the various types of error sources which occur in numerical
work.
2.1.2 (a) Deﬁne “absolute error” and “relative error” for an approximation ¯x to a scalar
quantity x. What is meant by an error bound?
(b) Generalize the deﬁnitions in (a) to a vector x.
2.1.3 How is “rounding to nearest” performed?
2.1.4 Give π to four decimals using (a) chopping; (b) rounding.
2.1.5 What is meant by the “tablemaker’s dilemma”?
2.2
Computer Number Systems
2.2.1
The Position System
In order to represent numbers in daily life, we use a position system with base 10 (the
decimal system). Thus, to represent the numbers we use ten different characters, and the
magnitude with which the digit a contributes to the value of a number depends on the digit’s
position in the number. If the digit stands n steps to the right of the decimal point, the value
contributed is a · 10−n. For example, the sequence of digits 4711.303 means
4 · 103 + 7 · 102 + 1 · 101 + 1 · 100 + 3 · 10−1 + 0 · 10−2 + 3 · 10−3.
Every real number has a unique representation in the above way, except for the possibility
of inﬁnite sequences of nines—for example, the inﬁnite decimal fraction 0.3199999 . . .
represents the same number as 0.32.
One can very well consider other position systems with bases other than 10. Any
integer β ≥2 (or β ≤−2) can be used as a base. One can show that every positive real
number a has, with exceptions analogous to the nines sequences mentioned above, a unique
representation of the form
a = dnβn + dn−1βn−1 + · · · + d1β1 + d0β0 + d−1β−1 + d−2β−2 + · · ·
or, more compactly, a = (dndn−1 . . . d0.d−1d−2 . . .)β, where the coefﬁcients di, the “digits”
in the system with base β, are positive integers di such that 0 ≤di ≤β −1.
One of the greatest advantages of the position system is that one can give simple,
general rules for the arithmetic operations. The smaller the base is, the simpler these rules

94
Chapter 2. How to Obtain and Estimate Accuracy
become. This is just one reason why most computers operate in base 2, the binary number
system. The addition and multiplication tables then take the following simple form:
0 + 0 = 0,
0 + 1 = 1 + 0 = 1,
1 + 1 = 10,
0 · 0 = 0,
0 · 1 = 1 · 0 = 0,
1 · 1 = 1.
In the binary system the number seventeen becomes 10001, since 1 · 24 + 0 · 23 + 0 · 22 +
0 · 21 + 1 · 20 = sixteen + one = seventeen. Put another way (10001)2 = (17)10, where
the index (in decimal representation) denotes the base of the number system. The numbers
become longer written in the binary system; large integers become about 3.3 times as long,
since N binary digits sufﬁce to represent integers less than 2N = 10N log10 2 ≈10N/3.3.
Occasionally one groups together the binary digits in subsequences of three or four,
which is equivalent to using 23 and 24, respectively, as the base. These systems are called the
octal and hexadecimal number systems, respectively. The octal system uses the digits from
0 to 7; in the hexadecimal system the digits 0 through 9 and the letters A, B, C, D, E, F
(“ten” through “ﬁfteen”) are used.
Example 2.2.1.
(17)10 = (10001)2 = (21)8 = (11)16,
(13.25)10 = (1101.01)2 = (15.2)8 = (D.4)16,
(0.1)10 = (0.000110011001 . . .)2 = (0.199999 . . .)16.
Note that the ﬁnite decimal fraction 0.1 cannot be represented exactly by a ﬁnite fraction in
the binary number system! (For this reason some pocket calculators use base 10.)
Example 2.2.2.
In 1991 a Patriot missile in Saudi Arabia failed to track and interrupt an incoming
Scud missile due to a precision problem. The Scud then hit an army barrack and killed 28
Americans. The computer used to control the Patriot missile was based on a design dating
from the 1970s using 24-bit arithmetic. For the tracking computations, time was recorded
by the system clock in tenths of a second but converted to a 24-bit ﬂoating-point number.
Rounding errors in the time conversions caused an error in the tracking. After 100 hours
of consecutive operations the calculated time in seconds was 359,999.6567 instead of the
correct value 360,000, an error of 0.3433 seconds leading to an error in the calculated range
of 687 meters; see Skeel [326]. Modiﬁed software was later installed.
In the binary system the “point” used to separate the integer and fractional part of a
number (corresponding to the decimal point) is called the binary point. The digits in the
binary system are called bits (binary digits).
We are so accustomed to the position system that we forget that it is built upon an
ingenious idea. The reader can puzzle over how the rules for arithmetic operations would
look if one used Roman numerals, a number system without the position principle described
above.
Recall that rational numbers are precisely those real numbers which can be expressed
as a quotient between two integers. Equivalently, rational numbers are those whose repre-
sentation in a position system have a ﬁnite number of digits or whose digits are repeating.

2.2. Computer Number Systems
95
We now consider the problem of conversion between two number systems with dif-
ferent bases. Since almost all computers use a binary system this problem arises as soon as
one wants to input data in decimal form or print results in decimal form.
Algorithm 2.1. Conversion between Number Systems.
Let a be an integer given in number systems with base α.
We want to determine its
representation in a number system with base β:
a = bnβm + bm−1βn−1 + · · · + b0,
0 ≤bi < β.
(2.2.1)
The computations are to be done in the system with base α, and thus β also is expressed in
this representation. The conversion is done by successive divisions of a with β: Set q0 = a,
and
qk/β = qk+1 + bk/β,
k = 0, 1, 2, . . .
(2.2.2)
(qk+1 is the quotient and bk is the remainder in the division).
If a is not an integer, we write a = b + c, where b is the integer part and
c = c−1β−1 + c−2β−2 + c−3β−3 + · · ·
(2.2.3)
is the fractional part, where c−1, c−2, . . . are to be determined. These digits are obtained as
the integer parts when successively multiplying c with β: Set p−1 = c, and
pk · β = ckβ + pk−1,
k = −1, −2, −3, . . . .
(2.2.4)
Since a ﬁnite fraction in a number system with base α usually does not correspond to a ﬁnite
fraction in the number system with base β, rounding of the result is usually needed.
When converting by hand between the decimal system and the binary system, all
computations are made in the decimal system (α = 10 and β = 2).
It is then more
convenient to convert the decimal number ﬁrst to octal or hexadecimal, from which the
binary representation easily follows. If, on the other hand, the conversion is carried out on
a binary computer, the computations are made in the binary system (α = 2 and β = 10).
Example 2.2.3.
Convert the decimal number 176.524 to ternary form (base β = 3). For the integer
part we get 176/3 = 58 with remainder 2; 58/3 = 19 with remainder 1; 19/3 = 6
with remainder 1; 6/3 = 2 with remainder 0; 2/3 = 0 with remainder 2. It follows that
(176)10 = (20112)3.
For the fractional part we compute .524 · 3 = 1.572, .572 · 3 = 1.716, .716 · 3 =
2.148, . . . . Continuing in this way we obtain (.524)10 = (.112010222 . . .)3. The ﬁnite
decimal fraction does not correspond to a ﬁnite fraction in the ternary number system.
2.2.2
Fixed- and Floating-Point Representation
Acomputer is, in general, built to handle pieces of information of a ﬁxed size called a word.
The number of digits in a word (usually binary) is called the word-length of the computer.

96
Chapter 2. How to Obtain and Estimate Accuracy
Typical word-lengths are 32 and 64 bits. A real or integer number is usually stored in a
word. Integers can be exactly represented, provided that the word-length sufﬁces to store
all the digits in its representation.
In the ﬁrst generation of computers calculations were made in a ﬁxed-point number
system; i.e., real numbers were represented with a ﬁxed number of t binary digits in the
fractional part. If the word-length of the computer is s + 1 bits (including the sign bit), then
only numbers in the interval I = [−2s−t, 2s−t] are permitted. Some common ﬁxed-point
conventions are t = s (fraction convention) and t = 0 (integer convention). This limitation
causes difﬁculties, since even when x ∈I, y ∈I, we can have x −y ̸∈I or x/y ̸∈I.
In a ﬁxed-point number system one must see to it that all numbers, even intermediate
results, remain within I. This can be attained by multiplying the variables by appropriate
scale factors, and then transforming the equations accordingly. This is a tedious process.
Moreover it is complicated by the risk that if the scale factors are chosen carelessly, certain
intermediate results can have many leading zeros which can lead to poor accuracy in the
ﬁnal results. As a consequence, current numerical analysis literature rarely deals with other
than ﬂoating-point arithmetic. In scientiﬁc computing ﬁxed-point representation is mainly
limited to computations with integers, as in subscript expressions for vectors and matrices.
On the other hand, ﬁxed-point computations can be much faster than ﬂoating-point,
especially since modern microprocessors have superscalar architectures with several ﬁxed-
point units but only one ﬂoating-point unit. In computer graphics, ﬁxed-point is used almost
exclusively once the geometry is transformed and clipped to the visible window. Fixed-point
square roots and trigonometric functions are also pretty quick and are easy to write.
By a normalized ﬂoating-point representation of a real number a, we mean a
representation in the form
a = ±m · βe,
β−1 ≤m < 1,
e an integer.
(2.2.5)
Such a representation is possible for all real numbers a and is unique if a ̸= 0. (The
number zero is treated as a special case.) Here the fraction part m is called the mantissa31
or signiﬁcand), e is the exponent, and β the base (also called the radix).
In a computer, the number of digits for e and m is limited by the word-length. Suppose
that t digits are used to represent m. Then we can only represent ﬂoating-point numbers of
the form
¯a = ±m · βe,
m = (0.d1d2 · · · dt)β,
0 ≤di < β,
(2.2.6)
where m is the mantissa m rounded to t digits, and the exponent is limited to a ﬁnite range
emin ≤e ≤emax.
(2.2.7)
A ﬂoating-point number system F is characterized by the base β, the precision t, and
the numbers emin, emax. Only a ﬁnite set F of rational numbers can be represented in the
form (2.2.6). The numbers in this set are called ﬂoating-point numbers. Since d1 ̸= 0 this
set contains, including the number zero, precisely
2(β −1)βt−1(emax −emin + 1) + 1
31Strictly speaking, “mantissa” refers to the decimal part of a logarithm.

2.2. Computer Number Systems
97
numbers. (Show this!) The limited number of digits in the exponent implies that a is limited
in magnitude to an interval which is called the range of the ﬂoating-point system. If a is
larger in magnitude than the largest number in the set F, then a cannot be represented at all
(exponent spill). The same is true, in a sense, of numbers smaller than the smallest nonzero
number in F.
Example 2.2.4.
Consider the ﬂoating-point number system for β = 2, t = 3, emin = −1, and emax = 2.
The positive normalized numbers in the corresponding set F are shown in Figure 2.2.1. The
set F contains exactly 2 · 16 + 1 = 33 numbers. In this example the nonzero numbers
of smallest magnitude that can be represented are (0.100)2 · 2−1 =
1
4 and the largest is
(0.111)2 · 22 = 7
2.
0
1
4
1
2
1
2
3
Figure 2.2.1. Positive normalized numbers when β = 2, t = 3, and −1 ≤e ≤2.
Notice that ﬂoating-point numbers are not equally spaced; the spacing jumps by a
factor of β at each power of β. This wobbling is smallest for β = 2.
Deﬁnition 2.2.1.
The spacing of ﬂoating-point numbers is characterized by the machine epsilon, which
is the distance ϵM from 1.0 to the next larger ﬂoating-point number.
The leading signiﬁcant digit of numbers represented in a number system with base β
has been observed to closely ﬁt a logarithmic distribution; i.e., the proportion of numbers
with leading digit equal to n is lnβ(1 + 1/n) (n = 0, 1, . . . , β −1). It has been shown
that, under this assumption, taking the base equal to 2 will minimize the mean square
representation error. A discussion of this intriguing fact, with historic references, is found
in Higham [199, Sec. 2.7].
Even if the operands in an arithmetic operation are ﬂoating-point numbers in F, the
exact result of the operation may not be in F. For example, the exact product of two
ﬂoating-point t-digit numbers has 2t or 2t −1 digits.
If a real number a is in the range of the ﬂoating-point system, the obvious thing to do
is to represent a by ¯a = f l (a), where f l (a) denotes a number in F which is nearest to a.
This corresponds to rounding of the mantissa m, and according to Sec. 2.1.3, we have
|m −m| ≤1
2β−t.
(There is one exception. If |m| after rounding should be raised to 1, then |m| is set equal to
0.1 and e is raised by 1.) Since m ≥0.1 this means that the magnitude of the relative error

98
Chapter 2. How to Obtain and Estimate Accuracy
in ¯a is at most equal to
1
2β−t · βe
m · βe
≤1
2β−t+1.
Even with the exception mentioned above this relative bound still holds. (If chopping is
used, this doubles the error bound above.) This proves the following theorem.
Theorem 2.2.2.
In a ﬂoating-point number system F = F(β, t, emin, emax) every real number in the
ﬂoating-point range of F can be represented with a relative error, which does not exceed
the unit roundoff u, which is deﬁned by
u =
% 1
2β−t+1
if rounding is used,
β−t+1
if chopping is used.
(2.2.8)
Note that in a ﬂoating-point system both large and small numbers are represented with
nearly the same relative precision. The quantity u is, in many contexts, a natural unit for
relative changes and relative errors. For example, termination criteria in iterative methods
usually depend on the unit roundoff.
To measure the difference between a ﬂoating-point number and the real number it
approximates we shall occasionally use “unit in last place” or ulp.
We shall often say
that “the quantity is perturbed by a few ulps.” For example, if in a decimal ﬂoating-point
system the number 3.14159 is represented as 0.3142 · 101, this has an error of 0.41 ulps.
Example 2.2.5.
Sometimes it is useful to be able to approximately determine the unit roundoff in a
program at run time. This may be done using the observation that u ≈µ, where µ is
the smallest ﬂoating-point number x for which f l (1 + x) > 1. The following program
computes a number µ which differs from the unit roundoff u by at most a factor of 2:
x := 1;
while 1 + x > 1
x := x/2; end;
µ := x;
One reason why u does not exactly equal µ is that so-called double rounding may occur.
This is when a result is ﬁrst rounded to extended format and then to the target precision.
Aﬂoating-point number system can be extended by including denormalized numbers
(also called subnormal numbers). These are numbers with the minimum exponent and with
the most signiﬁcant digit equal to zero. The three numbers
(.001)22−1 = 1/16,
(.010)22−1 = 2/16,
(.011)22−1 = 3/16
can then also be represented (see Figure 2.2.2). Because the representations of denormal-
ized numbers have initial zero digits these have fewer digits of precision than normalized
numbers.

2.2. Computer Number Systems
99
0
1
4
1
2
1
2
3
Figure 2.2.2. Positive normalized and denormalized numbers when β = 2, t = 3,
and −1 ≤e ≤2.
2.2.3
IEEE Floating-Point Standard
Actual computer implementations of ﬂoating-point representations may differ in detail from
the one given above. Although some pocket calculators use a ﬂoating-point number system
with base β = 10, almost all modern computers use base β = 2. Most current computers
now conform to the IEEE 754 standard for binary ﬂoating-point arithmetic.32 This standard
from 1985 (see [205]), which is the result of several years’ work by a subcommittee of the
IEEE, is now implemented on almost all chips used for personal computers and workstations.
There is also a standard IEEE 854 for radix independent ﬂoating-point arithmetic [206]. This
is used with base 10 by several hand calculators.
The IEEE 754 standard speciﬁes basic and extended formats for ﬂoating-point num-
bers, elementary operations and rounding rules available, conversion between different
number formats, and binary–decimal conversion. The handling of exceptional cases like
exponent overﬂow or underﬂow and division by zero are also speciﬁed.
Two main basic formats, single and double precision, are deﬁned using 32 and 64
bits, respectively. In single precision a ﬂoating-point number a is stored as the sign s (one
bit), the exponent e (8 bits), and the mantissa m (23 bits). In double precision 11 of the 64
bits are used for the exponent, and 52 bits are used for the mantissa. The value v of a is in
the normal case
v = (−1)s(1.m)22e,
−emin ≤e ≤emax.
(2.2.9)
Note that the digit before the binary point is always 1 for a normalized number. Thus the
normalization of the mantissa is different from that in (2.2.6). This bit is not stored (the
hidden bit). In that way one bit is gained for the mantissa. A biased exponent is stored and
no sign bit used for the exponent. In single precision emin = −126 and emax = 127, and
e + 127 is stored.
The unit roundoff equals
u =
%
2−24 ≈5.96 · 10−8
in single precision,
2−53 ≈1.11 · 10−16
in double precision.
(The machine epsilon is twice as large.) The largest number that can be represented is
approximately 2.0·2127 ≈3.4028×1038 in single precision and 2.0·21023 ≈1.7977×10308
in double precision. The smallest normalized number is 1.0 · 2−126 ≈1.1755 × 10−38 in
single precision and 1.0 · 2−1022 ≈2.2251 × 10−308 in double precision.
An exponent e = emin −1 and m ̸= 0 signiﬁes the denormalized number
v = (−1)s(0.m)22emin.
32W. Kahan, University of California, Berkeley, was given the Turing Award by the Association of Computing
Machinery for his contribution to this standard.

100
Chapter 2. How to Obtain and Estimate Accuracy
The smallest denormalized number that can be represented is 2−126−23 ≈1.4013 · 10−45 in
single precision and 2−1022−52 ≈4.9407 · 10−324 in double precision.
There are distinct representations for +0 and −0. ±0 is represented by a sign bit, the
exponent emin −1, and a zero mantissa. Comparisons are deﬁned so that +0 = −0. One
use of a signed zero is to distinguish between positive and negative underﬂowed numbers.
Another use occurs in the computation of complex elementary functions; see Sec. 2.2.4.
Inﬁnity is also signed and ±∞is represented by the exponent emax + 1 and a zero
mantissa. When overﬂow occurs the result is set to ±∞. This is safer than simply returning
the largest representable number, which may be nowhere near the correct answer. The result
±∞is also obtained from the illegal operations a/0, where a ̸= 0. The inﬁnity symbol
obeys the usual mathematical conventions such as ∞+ ∞= ∞, (−1) × ∞= −∞,
a/∞= 0.
The IEEE standard also includes two extended precision formats that offer extra
precision and exponent range. The standard only speciﬁes a lower bound on how many
extra bits it provides.33 Most modern processors use 80-bit registers for processing real
numbersandstoreresultsas64-bitnumbersaccordingtotheIEEEdoubleprecisionstandard.
Extendedformatssimplifytaskssuchascomputingelementaryfunctionsaccuratelyinsingle
or double precision. Extended precision formats are used also by hand calculators. These
will often display 10 decimal digits but use 13 digits internally—“the calculator knows more
than it shows.”
The characteristics of the IEEE formats are summarized in Table 2.2.1. (The hidden
bit in the mantissa accounts for the +1 in the table. Note that double precision satisﬁes the
requirements for the single extended format, so three different precisions sufﬁce.)
Table 2.2.1. IEEE ﬂoating-point formats.
Format
t
e
emin
emax
Single
32 bits
23 + 1
8 bits
−126
127
Single extended
≥43 bits
≥32
≥11 bits
≤−1022
≥1023
Double
64 bits
52 + 1
11 bits
−1022
1023
Double extended
≥79 bits
≥64
≥15 bits
≤−16,382
≥16,383
Example 2.2.6.
Although the exponent range of the ﬂoating-point formats seems reassuringly large,
even simple programs can quickly give exponent spill. If x0 = 2, xn+1 = x2
n, then already
x10 = 21024 is larger than what IEEE double precision permits. One should also be careful
in computations with factorials, e.g., 171! ≈1.24 · 10309 is larger than the largest double
precision number.
Four rounding modes are supported by the standard. The default rounding mode is
round to the nearest representable number, with round to even in the case of a tie. (Some
computers, in case of a tie, round away from zero, i.e., raise the absolute value of the
33Hardware implementation of extended precision normally does not use a hidden bit, so the double extended
format uses 80 bits rather than 79.

2.2. Computer Number Systems
101
number, because this is easier to realize technically.) Chopping is also supported as well
as directed rounding to ∞and to −∞. The latter mode simpliﬁes the implementation of
interval arithmetic; see Sec. 2.5.3.
The standard speciﬁes that all arithmetic operations should be performed as if they
were ﬁrst calculated to inﬁnite precision and then rounded to a ﬂoating-point number ac-
cording to one of the four modes mentioned above. This also includes the square root and
conversion between an integer and a ﬂoating-point. The standard also requires that the con-
version between internal formats and decimal be correctly rounded. This can be achieved
using extra guard digits in the intermediate result of the operation before normalization and
rounding. Using a single guard digit, however, will not always ensure the desired result.
However, by introducing a second guard digit and a third sticky bit (the logical OR of all
succeeding bits) the rounded exact result can be computed at only a slightly higher cost
(Goldberg [158]). One reason for specifying precisely the results of arithmetic operations
is to improve the portability of software. If a program is moved between two computers,
both supporting the IEEE standard, intermediate results should be the same.
IEEE arithmetic is a closed system; that is, every operation, even mathematically
invalid operations, such as 0/0 or
√
−1, produces a result. To handle exceptional situations
without aborting the computations some bit patterns (seeTable 2.2.2) are reserved for special
quantities like NaN (“Not a Number”) and ∞. NaNs (there are more than one NaN) are
represented by e = emax + 1 and m ̸= 0.
Table 2.2.2. IEEE 754 representation.
Exponent
Mantissa
Represents
e = emin −1
m = 0
±0
e = emin −1
m ̸= 0
±0.m · 2emin
emin < e < emax
±1.m · 2e
e = emax + 1
m = 0
±∞
e = emax + 1
m ̸= 0
NaN
Note that the gap between zero and the smallest normalized number is 1.0×2emin. This
is much larger than for the spacing 2−t+1×2emin for the normalized numbers for numbers just
larger than the underﬂow threshold; compare Figure 2.2.1. With denormalized numbers the
spacing becomes more regular and permits what is called gradual underﬂow. This makes
many algorithms well behaved close to the underﬂow threshold also. Another advantage of
having gradual underﬂow is that it makes it possible to preserve the property
x = y
⇔
x −y = 0
as well as other useful relations. Several examples of how denormalized numbers make
writing reliable ﬂoating-point code easier are analyzed by Demmel [94].
One illustration of the use of extended precision is in converting between IEEE 754
single precision and decimal. The converted single precision number should ideally be
converted with enough digits so that when it is converted back the binary single precision
number is recovered. It might be expected that since 224 < 108, eight decimal digits in the

102
Chapter 2. How to Obtain and Estimate Accuracy
converted number would sufﬁce. But it can be shown that nine decimal digits are needed to
recover the binary number uniquely (see Goldberg [158, Theorem 15] and Problem 2.2.4).
When converting back to binary form a rounding error as small as one ulp will give the
wrong answer. To do this conversion efﬁciently, extended single precision is needed.34
A NaN is generated by operations such as 0/0, +∞+ (−∞), 0 × ∞, and
√
−1. A
NaN compares unequal with everything including itself. (Note that x ̸= x is a simple way
to test if x equals a NaN.) When a NaN and an ordinary ﬂoating-point number are combined
the result is the same as the NaN operand. A NaN is also often used for uninitialized or
missing data.
Exceptional operations also raise a ﬂag. The default is to set a ﬂag and continue, but
it is also possible to pass control to a trap handler. The ﬂags are “sticky” in that they remain
set until explicitly cleared. This implies that without a log ﬁle everything before the last
setting is lost, which is why it is always wise to use a trap handler. There is one ﬂag for each
of the following ﬁve exceptions: underﬂow, overﬂow, division by zero, invalid operation,
and inexact. For example, by testing the ﬂags, it is possible to test if an overﬂow is genuine
or the result of division by zero.
Because of cheaper hardware and increasing problem sizes, double precision is used
more and more in scientiﬁc computing. With increasing speed and memory becoming
available, bigger and bigger problems are being solved and actual problems may soon
require more than IEEE double precision! When the IEEE 754 standard was deﬁned no one
expected computers able to execute more than 1012 ﬂoating-point operations per second.
2.2.4
Elementary Functions
Although the square root is included, the IEEE 754 standard does not deal with the imple-
mentation of other familiar elementary functions, such as the exponential function exp, the
natural logarithm log, and the trigonometric and hyperbolic functions sin, cos, tan, sinh,
cosh, tanh, and their inverse functions. With the IEEE 754 standard more accurate imple-
mentations are possible which in many cases give almost correctly rounded exact results.
To always guarantee correctly rounded exact results sometimes requires computing many
more digits than the target accuracy (cf. the tablemaker’s dilemma) and therefore is in gen-
eral too costly. It is also important to preserve monotonicity, e.g., 0 ≤x ≤y ≤π/2 ⇒
sin x ≤sin y, and range restrictions, e.g., sin x ≤1, but these demands may conﬂict with
rounded exact results!
The ﬁrst step in computing an elementary function is to perform a range reduction.
To compute trigonometric functions, for example, sin x, an additive range reduction is ﬁrst
performed, in which a reduced argument x∗, −π/4 ≤x∗≤π/4, is computed by ﬁnding an
integer k such that
x∗= x −kπ/2
(π/2 = 1.57079 63267 94896 61923 . . .).
(Quantities such as π/2, log(2), that are often used in standard subroutines, are listed in
decimal form to 30 digits and octal form to 40 digits in Hart et al. [187, Appendix C] and to
34It should be noted that some computer languages do not include input/output routines, but these are developed
separately. This can lead to double rounding, which spoils the carefully designed accuracy in the IEEE 754
standard. (Some banks use separate routines with chopping even today—you may guess why!)

2.2. Computer Number Systems
103
40 and 44 digits in Knuth [230, AppendixA].) Then sin x = ± sin x∗or sin x = ± cos x∗,
depending on if k mod 4 equals 0, 1, 2, or 3. Hence approximation for sin x and cos x need
only be provided for 0 ≤x ≤π/4. If the argument x is very large, then cancellation in the
range reduction can lead to poor accuracy; see Example 2.3.7.
To compute log x, x > 0, a multiplicative range reduction is used. If an integer k is
determined such that
x∗= x/2k,
x∗∈[1/2, 1],
then log x = log x∗+ k · log 2.
To compute the exponential function exp(x) an integer k is determined such that
x∗= x −k log 2,
x∗∈[0, log 2]
(log 2 = 0.69314 71805 59945 30942 . . .).
It then holds that exp(x) = exp(x∗)·2k and hence we only need an approximation of exp(x)
for the range x ∈[0, log 2].
Coefﬁcients of polynomial and rational approximations suitable for software imple-
mentations are tabulated in Hart et al. [187] and Cody and Waite [75]. But approximation
of functions can now be simply obtained using software such as Maple [65]. For example,
in Maple the commands
Digits = 40; minimax(exp(x), x = 0..1, [i,k],1,’err’)
mean that we are looking for the coefﬁcients of the minimax approximation of the exponen-
tial function on [0, 1] by a rational function with numerator of degree i and denominator of
degree k with weight function 1, and that the variable err should be equal to the approxi-
mation error. The coefﬁcients are to be computed to 40 decimal digits. A trend now is for
elementary functions to be increasingly implemented in hardware. Hardware implementa-
tions are discussed by Muller [272]. Carefully implemented algorithms for elementary func-
tions are available from www.netlib.org/fdlibm in the library package fdlibm (Freely
Distributable Math. Library) developed by Sun Microsystems and used by MATLAB.
Example 2.2.7.
On a computer using IEEE double precision arithmetic the roundoff unit is u = 2−53 ≈
1.1·10−16. One wishes to compute sinh x with good relative accuracy, both for small and
large |x|, at least moderately large. Assume that ex is computed with a relative error less
than u in the given interval. The formula (ex −e−x)/2 for sinh x is sufﬁciently accurate
except when |x| is very small and cancellation occurs. Hence for |x| ≪1, ex and e−x
and hence (ex −e−x)/2 can have absolute errors of order of magnitude (say) u. Then the
relative error in (ex −e−x)/2 can have magnitude ≈u/|x|; for example, this is more than
100% for x ≈10−16.
For |x| ≪1 one can instead use (say) two terms in the series expansion for sinh x,
sinh x = x + x3/3! + x5/5! + . . . .
Then one gets an absolute truncation error which is about x5/120, and a roundoff error of
the order of 2u|x|. Thus the formula x + x3/6 is better than (ex −e−x)/2 if
|x|5/120 + 2u|x| < u.

104
Chapter 2. How to Obtain and Estimate Accuracy
If 2u|x| ≪u, we have |x|5 < 120u = 15·2−50, or |x| < 151/5 ·2−10 ≈0.00168 (which
shows that 2u|x| really could be ignored in this rough calculation). Thus, if one switches
from (ex −e−x)/2 to x + x3/6 for |x| < 0.00168, the relative error will nowhere exceed
u/0.00168 ≈0.66·10−13. If one needs higher accuracy, one can take more terms in the
series, so that the switch can occur at a larger value of |x|.
For very large values of |x| one must expect a relative error of order of magnitude |xu|
because of roundoff error in the argument x. Compare the discussion of range reduction in
Sec. 2.2.4 and Problem 2.2.13.
For complex arguments the elementary functions have discontinuous jumps across
when the argument crosses certain branch cuts in the complex plane. They are represented
by functions which are single-valued excepts for certain straight lines called branch cuts.
Where to put these branch cuts and the role of IEEE arithmetic in making these choices are
discussed by Kahan [217].
Example 2.2.8.
The function √x is multivalued and there is no way to select the values so the function
is continuous over the whole complex plane. If a branch cut is made by excluding all real
negative numbers from consideration the square root becomes continuous. Signed zero
provides a way to distinguish numbers of the form x + i(+0) and x + i(−0) and to select
one or the other side of the cut.
To test the implementation of elementary functions, a Fortran package ELEFUNT
has been developed by Cody [73]. This checks the quality using identities like cos x =
cos(x/3)(4 cos2(x/3) −1). For complex elementary functions a package CELEFUNT
serves the same purpose; see Cody [74].
2.2.5
Multiple Precision Arithmetic
Hardly any quantity in the physical world is known to an accuracy beyond IEEE double
precision. Avalue of π correct to 20 decimal digits would sufﬁce to calculate the circumfer-
ence of a circle around the sun at the orbit of the Earth to within the width of an atom. There
seems to be little need for multiple precision calculations. Occasionally, however, one may
want to perform some calculations, for example, the evaluation of some mathematical con-
stant (such as π and Euler’s constant γ ) or elementary functions, to very high precision.35
Extremely high precision is sometimes needed in experimental mathematics when trying to
discover new mathematical identities. Algorithms which may be used for these purposes
include power series, continued fractions, solutions of equations with Newton’s method, or
other superlinearly convergent methods.
For performing such tasks it is convenient to use arrays to represent numbers in a
ﬂoating-point form with a large base and a long mantissa, and to have routines for performing
ﬂoating-point operations on such numbers. In this way it is possible to simulate arithmetic
of arbitrarily high precision using standard ﬂoating-point arithmetic.
35In October 1995 Yasumasa Kanada of the University of Tokyo computed π to 6,442,458,938 decimals on a
Hitachi supercomputer; see [11].

Problems and Computer Exercises
105
Brent [46, 45] developed the ﬁrst major such multiple precision package in Fortran
66. His package represents multiple precision numbers as arrays of integers and operates
on them with integer arithmetic. It includes subroutines for multiple precision evaluation of
elementary functions. A more recent package called MPFUN, written in Fortran 77 code,
is that of Bailey [9]. In MPFUN a multiple precision number is represented as a vector of
single precision ﬂoating-point numbers with base 224. Complex multiprecision numbers are
also supported. There is also a Fortran 90 version of this package [10], which is easy to use.
A package of MATLAB m-ﬁles called Mulprec for computations in, principally,
unlimited precision ﬂoating-point, has been developed by the ﬁrst-named author. Docu-
mentation of Mulprec and the m-ﬁles can be downloaded from the homepage of this book
at www.siam.org/books/ot103 together with some examples of its use.
Fortran routines for high precision computation are also provided in Press et al. [294,
Sec. 20.6], and are also supported by symbolic manipulation systems such as Maple [65]
and Mathematica [382]; see Online Appendix C.
Review Questions
2.2.1 What base β is used in the binary, octal, and hexadecimal number systems?
2.2.2 Show that any ﬁnite decimal fraction corresponds to a binary fraction that eventually
is periodic.
2.2.3 What is meant by a normalized ﬂoating-point representation of a real number?
2.2.4 (a) How large can the maximum relative error be in representation of a real number
a in the ﬂoating-point system F = F(β, p, emin, emax)? It is assumed that a is in the
range of F.
(b) How are the quantities “machine epsilon” and “unit roundoff” deﬁned?
2.2.5 What are the characteristics of the IEEE single and double precision formats?
2.2.6 What are the advantages of including denormalized numbers in the IEEE standard?
2.2.7 Give examples of operations that give NaN as their result.
Problems and Computer Exercises
2.2.1 Which rational numbers can be expressed with a ﬁnite number of binary digits to
the right of the binary point?
2.2.2 (a) Prove the algorithm for conversion between number systems given in Sec. 2.2.1.
(b) Give the hexadecimal form of the decimal numbers 0.1 and 0.3. What error is
incurred in rounding these numbers to IEEE 754 single and double precision?
(c) What is the result of the computation 0.3/0.1 in IEEE 754 single and double
precision?
2.2.3 (Kahan) An (over)estimate of u can be obtained for almost any computer by evalu-
ating |3 × (4/3 −1) −1| using rounded ﬂoating-point for every operation. Test this
on a calculator or computer available to you.

106
Chapter 2. How to Obtain and Estimate Accuracy
2.2.4 (Goldberg [158]) The binary single precision numbers in the half-open interval
[103, 1024) have 10 bits to the left and 14 bits to the right of the binary point. Show
that there are (210 −103)·214 = 393,216 such numbers, but only (210 −103)·104 =
240,000 decimal numbers with eight decimal digits in the same interval. Conclude
that eight decimal digits are not enough to uniquely represent single precision binary
numbers in the IEEE 754 standard.
2.2.5 Suppose one wants to compute the power An of a square matrix A, where n is a
positive integer. To compute Ak+1 = A · Ak for k = 1 : n −1 requires n −1 matrix
multiplications. Show that the number of multiplications can be reduced to less than
2⌊log2 n⌋by converting n into binary form and successively squaring A2k = (Ak)2,
k = 1 : ⌊log2 n⌋.
2.2.6 Give in decimal representation: (a) (10000)2; (b) (100)8; (c) (64)16; (d) (FF)16;
(e) (0.11)8; (f) the largest positive integer which can be written with 31 binary digits
(answer with one signiﬁcant digit).
2.2.7 (a) Show how the following numbers are stored in the basic single precision format
of the IEEE 754 standard: 1.0; −0.0625; 250.25; 0.1.
(b) Give in decimal notation the largest and smallest positive numbers which can be
stored in this format.
2.2.8 (Goldberg [158, Theorem 7]) When β = 2, if m and n are integers with m < 2p−1
(p is the number of bits in the mantissa) and n has the special form n = 2i +
2j, then f l((m/n) × n) = m provided that ﬂoating-point operations are exactly
rounded to nearest. The sequence of possible values of n starts with 1, 2, 3, 4, 5, 6, 8,
9, 10, 12, 16, 17. Test the theorem on your computer for these numbers.
2.2.9 Let pi be the closest ﬂoating-point number to π in double precision IEEE 754
standard. Find a sufﬁciently accurate approximation to π from a table and show that
π −pi ≈1.2246 · 10−16. What value do you get on your computer for sin π?
2.2.10 (Edelman) Let x, 1 ≤x < 2, be a ﬂoating-point number in IEEE double precision
arithmetic. Show that f l(x · f l(1/x)) is either 1 or 1 −ϵM/2, where ϵM = 2−52
(the machine epsilon).
2.2.11 (N. J. Higham) Let a and b be ﬂoating-point numbers with a ≤b. Show that the
inequalities a ≤f l((a + b)/2) ≤b can be violated in base 10 arithmetic. Show
that a ≤f l(a + (b −a)/2) ≤b in base β arithmetic.
2.2.12 (Muller) A rational approximation of tan x in [−π/4, π/4] is
r(x) =
(0.99999 99328 −0.09587 5045x2)x
1 −(0.42920 9672 + 0.00974 3234x2)x2 .
Determine the approximate maximum error of this approximation by comparing
with the function on your system on 100 equidistant points in [0, π/4].
2.2.13 (a) Show how on a binary computer the exponential function can be approximated
by ﬁrst performing a range reduction based on the relation ex = 2y, y = x/ log 2,
and then approximating 2y on y ∈[0, 1/2].

2.3. Accuracy and Rounding Errors
107
(b) Show that since 2y satisﬁes 2−y = 1/2y a rational function r(y) approximating
2y should have the form
r(y) = q(y2) + ys(y2)
q(y2) −ys(y2),
where q and s are polynomials.
(c) Suppose the r(y) in (b) is used for approximating 2y with
q(y) = 20.81892 37930 062 + y,
s(y) = 7.21528 91511 493 + 0.05769 00723 731y.
How many additions, multiplications, and divisions are needed in this case to eval-
uate r(y)? Investigate the accuracy achieved for y ∈[0, 1/2].
2.3
Accuracy and Rounding Errors
2.3.1
Floating-Point Arithmetic
It is useful to have a model of how the basic ﬂoating-point operations are carried out. If x
and y are two ﬂoating-point numbers, we denote by
f l (x + y),
f l (x −y),
f l (x × y),
f l (x/y)
the results of ﬂoating addition, subtraction, multiplication, and division, which the machine
stores in memory (after rounding or chopping). In what follows we will assume that under-
ﬂow or overﬂow does not occur, and that the following standard model for the arithmetic
holds.
Deﬁnition 2.3.1.
Assume that x, y ∈F. Then in the standard model it holds that
f l (x op y) = (x op y)(1 + δ),
|δ| ≤u,
(2.3.1)
where u is the unit roundoff and “op” stands for one of the four elementary operations: +,
−, ×, and /.
Thestandardmodelholdswiththedefaultroundingmodeforcomputersimplementing
the IEEE 754 standard. In this case we also have
f l (√x) = √x(1 + δ),
|δ| ≤u.
(2.3.2)
If a guard digit is lacking, then instead of (2.3.1) only the weaker model
f l (x op y) = x(1 + δ1) op y(1 + δ2),
|δi| ≤u,
(2.3.3)
holds for addition/subtraction. The lack of a guard digit is a serious drawback and can lead
to damaging inaccuracy caused by cancellation. Many algorithms can be proved to work

108
Chapter 2. How to Obtain and Estimate Accuracy
satisfactorily only if the standard model (2.3.1) holds. We remark that on current computers
multiplication is as fast as addition/subtraction. Division usually is ﬁve to ten times slower
than a multiply, and a square root about twice as slow as division.
Some earlier computers lack a guard digit in addition/subtraction. Notable examples
are several pre-1995 models of Cray computers (Cray 1,2, X-MP,Y-MP, and C90), which
were designed to have the highest possible ﬂoating-point performance. The IBM 360, which
used a hexadecimal system, lacked a (hexadecimal) guard digit between 1964 and 1967.
The consequences turned out to be so intolerable that a guard digit had to be retroﬁtted.
Sometimes the ﬂoating-point computation is more precise than what the standard
model assumes. An obvious example is that when the exact value x op y can be represented
as a ﬂoating-point number there is no rounding error at all.
Some computers can perform a fused multiply–add operation, i.e., an expression of
the form a ×x +y can be evaluated with just one instruction and there is only one rounding
error at the end:
f l (a × x + y) = (a × x + y)(1 + δ),
|δ| ≤u.
Fused multiply–add can be used to advantage in many algorithms. For example, Horner’s
rule to evaluate the polynomial p(x) = a0xn + a1xn−1 + · · · + an−1x + an, which uses the
recurrence relation b0 = a0, bi = bi−1 · x + ai, i = 1 : n, needs only n fused multiply–add
operations.
It is important to realize that ﬂoating-point operations have, to some degree, other
properties than the exact arithmetic operations. Floating-point addition and multiplication
are commutative but not associative, and the distributive law also fails for them. This makes
the analysis of ﬂoating-point computations quite difﬁcult.
Example 2.3.1.
To show that associativity does not, in general, hold for ﬂoating addition, consider
adding the three numbers
a = 0.1234567 · 100,
b = 0.4711325 · 104,
c = −b
in a decimal ﬂoating-point system with t = 7 digits in the mantissa. The following scheme
indicates how ﬂoating-point addition is performed:
f l (b + c) = 0,
f l (a + f l (b + c)) = a = 0.1234567 · 100
a =
0.0000123
4567 · 104
+b =
0.4711325
·104
f l (a + b) =
0.4711448
·104
c =
−0.4711325
·104
.
The last four digits to the right of the vertical line are lost by outshifting, and
f l (f l (a + b) + c) = 0.0000123 · 104 = 0.1230000 · 100 ̸= f l (a + f l (b + c)).
An interesting fact is that, assuming a guard digit is used, ﬂoating-point subtraction
of two sufﬁciently close numbers is always exact.

2.3. Accuracy and Rounding Errors
109
Lemma 2.3.2 (Sterbenz [333]).
Let the ﬂoating-point numbers x and y satisfy
y/2 ≤x ≤2y.
Then f l(x −y) = x −y, unless x −y underﬂows.
Proof. By the assumption the exponent of x and y in the ﬂoating-point representations of x
and y can differ by at most one unit. If the exponent is the same, then the exact result will be
computed. Therefore, assume the exponents differ by one. After scaling and, if necessary,
interchanging x and y, it holds that x/2 ≤y ≤x < 2 and the exact difference z = x −y
is of the form
x = x1.x2 . . . xt
y =
0 .y1 . . . yt−1yt
z = z1.z2 . . . ztzt+1
.
But from the assumption, x/2 −y ≤0 or x −y ≤y. Hence we must have z1 = 0, and
thus after shifting the exact result is also obtained in this case.
With gradual underﬂow, as in the IEEE 754 standard, the condition that x −y does
not underﬂow can be dropped.
Example 2.3.2.
A corresponding result holds for any base β. For example, using four-digit ﬂoating
decimal arithmetic we get with guard digit
f l (0.1000 · 101 −0.9999) = 0.0001 = 1.000 · 10−4
(exact), but without guard digit
f l (0.1000 · 101 −0.9999) = (0.1000 −0.0999)101 = 0.0001 · 101 = 1.000 · 10−3.
The last result satisﬁes (2.3.3) with |δi| ≤0.5 · 10−3 since 0.10005 · 101 −0.9995 = 10−3.
Outshiftings are common causes of loss of information that may lead to catastrophic
cancellation later, in the computations of a quantity that one would have liked to obtain
with good relative accuracy.
Example 2.3.3.
An example where the result of Lemma 2.3.2 can be used to advantage is in computing
compounded interest. Consider depositing the amount c every day in an account with an
interest rate i compounded daily. With the accumulated capital, the total at the end of the
year equals
c[(1 + x)n −1]/x,
x = i/n ≪1,
and n = 365. Using this formula does not give accurate results. The reason is that a
rounding error occurs in computing f l(1 + x) = 1 + ¯x and low order bits of x are lost.
For example, if i = 0.06, then i/n = 0.0001643836; in decimal arithmetic using six digits

110
Chapter 2. How to Obtain and Estimate Accuracy
when this is added to one we get f l(1 + i/n) = 1.000164, and thus four low order digits
are lost.
The problem then is to accurately compute (1+x)n = exp(n log(1 + x)). The formula
log(1 + x) =
 x
if f l (1 + x) = 1,
x log(1 + x)
(1 + x) −1
otherwise
(2.3.4)
can be shown to yield accurate results when x ∈[0, 3/4] and the computed value of
log(1 + x) equals the exact result rounded; see Goldberg [158, p. 12].
To check this formula we recall that the base e of the natural logarithm can be deﬁned
by the limit
e = lim
n→∞(1 + 1/n)n.
In Figure 2.3.1 we show computed values, using double precision ﬂoating-point arithmetic,
of the sequence |(1 + 1/n)n −e| for n = 10p, p = 1 : 14. More precisely, the expression
was computed as
| exp(n log(1 + 1/n)) −exp(1)|.
The smallest difference 3 · 10−8 occurs for n = 108, for which about half the number of bits
in x = 1/n are lost. For larger values of n rounding errors destroy the convergence. But
using (2.3.4) we obtain correct results for all values of n! (The Maclaurin series
log(1 + x) = x −x2/2 + x3/3 −x4/4 + · · ·
will also give good results.)
10
0
10
5
10
10
10
15
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Figure 2.3.1. Computed values for n = 10p, p = 1 : 14, of the sequences: solid
line |(1 + 1/n)n −e|; dashed line | exp(n log(1 + 1/n)) −e| using (2.3.4).

2.3. Accuracy and Rounding Errors
111
A fundamental insight from the above examples can be expressed in the following
way:
“mathematically equivalent” formulas or algorithms are not in general “numer-
ically equivalent.”
This adds a new dimension to calculations in ﬁnite precision arithmetic and it will be a
recurrent theme in the analysis of algorithms in this book.
By mathematical equivalence of two algorithms we mean here that the algorithms
give exactly the same results from the same input data, if the computations are made without
rounding error (“with inﬁnitely many digits”). One algorithm can then, as a rule, formally
be derived from the other using the rules of algebra for real numbers, and with the help
of mathematical identities. Two algorithms are numerically equivalent if their respective
ﬂoating-point results using the same input data are the same.
In error analysis for compound arithmetic expressions based on the standard model
(2.3.1), one often needs an upper bound for quantities of this form:
ϵ ≡|(1 + δ1)(1 + δ2) · · · (1 + δn) −1|,
|δi| ≤u,
i = 1 : n.
Then ϵ ≤(1 + u)n −1. Assuming that nu < 1, an elementary calculation gives
(1 + u)n −1 = nu + n(n −1)
2!
u2 + · · · +
n
k

uk + · · ·
< nu

1 + nu
2 + · · · +
nu
2
k−1
+ · · ·

=
nu
1 −nu/2.
(2.3.5)
Similarly, it can be shown that (1 −u)−n −1 < nu/(1 −nu), and the following useful
result follows (Higham [199, Lemma 3.1]).
Lemma 2.3.3.
Let |δi| ≤u, ρi = ±1, i = 1:n, and set
n
&
i=1
(1 + δi)ρi = 1 + θn.
(2.3.6)
If nu < 1, then |θn| < γn, where
γn = nu/(1 −nu).
(2.3.7)
Complex arithmetic can be reduced to real arithmetic. Let x = a +ib and y = c +id
be two complex numbers. Then we have
x ± y = a ± c + i(b ± d),
x × y = (ac −bd) + i(ad + bc),
(2.3.8)
x/y = ac + bd
c2 + d2 + i bc −ad
c2 + d2 .

112
Chapter 2. How to Obtain and Estimate Accuracy
Using the above formula, complex addition (subtraction) needs two real additions, and
multiplying two complex numbers requires four real multiplications.
Lemma 2.3.4.
Assume that the standard model (2.3.1) for ﬂoating-point arithmetic holds. Then,
provided that no overﬂow or underﬂow occurs, no denormalized numbers are produced and
the complex operations computed according to (2.3.8) satisfy
f l (x ± y) = (x ± y)(1 + δ),
|δ| ≤u,
f l (x × y) = x × y(1 + δ),
|δ| ≤
√
5u,
(2.3.9)
f l (x/y) = x/y(1 + δ),
|δ| ≤
√
2γ4,
where δ is a complex number and γn is deﬁned in (2.3.7).
Proof. See Higham [199, Sec. 3.6]. The result for complex multiplication is due to Brent
et al. [48].
The square root of a complex number u + iv = √x + iy is given by
u =
r + x
2
1/2
,
v =
r −x
2
1/2
,
r =

x2 + y2.
(2.3.10)
When x > 0 there will be cancellation when computing v, which can be severe if also
|x| ≫|y| (cf. Sec. 2.3.4). To avoid this we note that uv =
√
r2 −x2/2 = y/2, and thus v
can be computed from v = y/(2u). When x < 0 we instead compute v from (2.3.10) and
set u = y/(2v).
Most rounding error analyses given in this book are formulated for real arithmetic.
Since the bounds in Lemma 2.3.4 are of the same form as the standard model for real
arithmetic, these can simply be extended to complex arithmetic.
In some cases it may be desirable to avoid complex arithmetic when working with
complex matrices. This can be achieved in a simple way by replacing the complex matrices
and vectors by real ones of twice the order. Suppose that a complex matrix A ∈Cn×n and
a complex vector z ∈Cn are given, where
A = B + iC,
z = x + iy,
with real B, C, x, and y. Form the real matrix ˜A ∈R2n×2n and real vector ˜z ∈R2n deﬁned
by
˜A =

B
−C
C
B

,
˜z =

x
y

.
It is easy to verify the following rules:
'
(Az) = ˜A˜z,

(AB) = ˜A ˜B,

(A−1) = ( ˜A)−1.
Thus we can solve complex-valued matrix problems using algorithms for the real case. But
this incurs a penalty in storage and arithmetic operations.

2.3. Accuracy and Rounding Errors
113
2.3.2
Basic Rounding Error Results
We now use the notation of Sec. 2.3.1 and the standard model of ﬂoating-point arithmetic
(Deﬁnition 2.3.1) to carry out rounding error analysis of some basic computations. Most,
but not all, results are still true if only the weaker bound (2.3.3) holds for addition and
subtraction. Note that f l (x op y) = (x op y)(1 + δ), |δ| ≤u, can be interpreted for
multiplication to mean that f l (x · y) is the exact result of x · y(1 + δ) for some δ, |δ| ≤u.
In the same way, the results using the three other operations can be interpreted as the result
of exact operations where the operands have been perturbed by a relative amount which
does not exceed u. In backward error analysis (see Sec. 2.4.4) one applies the above
interpretation step by step backward in an algorithm.
By repeated use of the formula (2.3.1) in the case of multiplication, one can show that
f l (x1x2 · · · xn) = x1x2(1 + δ2)x3(1 + δ3) · · · xn(1 + δn),
|δi| ≤u,
i = 2 : n
holds; i.e., the computed product f l (x1x2 · · · xn) is exactly equal to a product of the factors
˜x1 = x1,
˜xi = xi(1 + δi),
i = 2 : n.
Using the estimate and notation of (2.3.7) it follows from this analysis that
|f l (x1x2 · · · xn) −x1x2 · · · xn| < γn−1|x1x2 · · · xn|,
(2.3.11)
which bounds the forward error of the computed result.
For a sum of n ﬂoating-point numbers similar results can be derived. If the sum is
computed in the natural order, we have
f l (· · · (((x1 + x2) + x3) + · · · + xn))
= x1(1 + δ1) + x2(1 + δ2) + · · · + xn(1 + δn),
where
|δ1| < γn−1,
|δi| < γn+1−i,
i = 2 : n,
and thus the computed sum is the exact sum of the numbers xi(1 + δi). This also gives an
estimate of the forward error
|f l (· · · (((x1 + x2) + x3) + · · · + xn)) −(x1 + x2 + x3 + · · · + xn)|
<
n

i=1
γn+1−i|xi| ≤γn−1
n

i=1
|xi|,
(2.3.12)
where the last upper bound holds independent of the summation order.
Notice that to minimize the ﬁrst upper bound in equation (2.3.12), the terms should
be added in increasing order of magnitude. For large n an even better bound can be shown
if the summation is done using the divide and conquer technique described in Sec. 1.2.3;
see Problem 2.3.5.

114
Chapter 2. How to Obtain and Estimate Accuracy
Example 2.3.4.
Using a hexadecimal machine (β = 16), with t = 6 and chopping (u = 16−5 ≈10−6),
we computed
10,000

n=1
n−2 ≈1.644834
in two different orders. Using the natural summation order n = 1, 2, 3, . . . the error was
1.317 · 10−3. Summing in the opposite order n = 10,000, 9,999, 9,998, . . . the error was
reduced to 2 · 10−6. This was not unexpected. Each operation is an addition, where the
partial sum s is increased by n−2. Thus, in each operation, one commits an error of about
s · u, and all these errors are added. Using the ﬁrst summation order, we have 1 ≤s ≤2
in every step, but using the other order of summation we have s < 10−2 in 9,900 of the
10,000 additions.
Similar bounds for roundoff errors can easily be derived for basic vector and matrix
operations; see Wilkinson [377, pp. 114–118]. For an inner product xT y computed in the
natural order we have
f l (xT y) = x1y1(1 + δ1) + x2y2(1 + δ2) + · · · + xnyn(1 + δn),
where
|δ1| < γn,
|δr| < γn+2−i,
i = 2 : n.
The corresponding forward error bound becomes
|f l (xT y) −xT y| <
n

i=1
γn+2−i|xi||yi| < γn
n

i=1
|xi||yi|.
If we let |x|, |y| denote vectors with elements |xi|, |yi| the last estimate can be written in
the simple form
|f l (xT y) −xT y| < γn|xT ||y|.
(2.3.13)
This bound is independent of the summation order and also holds for the weaker model
(2.3.3) valid with no guard digit rounding.
The outer product of two vectors x, y ∈Rn is the matrix xyT = (xiyj). In ﬂoating-
point arithmetic we compute the elements f l (xiyj) = xiyj(1 + δij), δij ≤u, and thus
|f l (xyT ) −xyT | ≤u |xyT |.
(2.3.14)
This is a satisfactory result for many purposes, but the computed result is not in general a rank
one matrix and it is not possible to ﬁnd 4x and 4y such that f l(xyT ) = (x+4x)(x+4y)T .
The product of two t-digit ﬂoating-point numbers can be exactly represented with at
most 2t digits. This allows inner products to be computed in extended precision without
much extra cost. If f le denotes computation with extended precision and ue the correspond-
ing unit roundoff, then the forward error bound for an inner product becomes
|f l (f le(xT y)) −xT y| < u|xT y| +
nue
1 −nue/2(1 + u)|xT ||y|,
(2.3.15)

2.3. Accuracy and Rounding Errors
115
where the ﬁrst term comes from the ﬁnal rounding. If |xT ||y| ≤u|xT y|, then the computed
inner product is almost as accurate as the correctly rounded exact result. These accurate inner
products can be used to improve accuracy by so-called iterative reﬁnement in many linear
algebra problems. But since computations in extended precision are machine dependent it
has been difﬁcult to make such programs portable.36 The recent development of Extended
and Mixed Precision BLAS (Basic Linear Algebra Subroutines; see [247]) may now make
this more feasible.
Similar error bounds can easily be obtained for matrix multiplication. Let A ∈Rm×n,
B ∈Rn×p, and denote by |A| and |B| matrices with elements |aij| and |bij|. Then it holds
that
|f l (AB) −AB| < γn|A||B|,
(2.3.16)
where the inequality is to be interpreted elementwise. Often we shall need bounds for some
norm of the error matrix. From (2.3.16) it follows that
∥f l (AB) −AB∥< γn∥|A| ∥∥|B| ∥.
(2.3.17)
Hence, for the 1-norm, ∞-norm, and the Frobenius norm we have
∥f l (AB) −AB∥< γn∥A∥∥B∥,
(2.3.18)
but unless A and B have only nonnegative elements, we get for the 2-norm only the weaker
bound
∥f l (AB) −AB∥2 < nγn∥A∥2 ∥B∥2.
(2.3.19)
To reduce the effects of rounding errors in computing a sum n
i=1 xi one can use
compensated summation. In this algorithm the rounding error in each addition is esti-
mated and then compensated for with a correction term. Compensated summation can be
useful when a large number of small terms are to be added as in numerical quadrature.
Another example is the case in the numerical solution of initial value problems for ordinary
differential equations. Note that in this application the terms have to be added in the order
in which they are generated.
Compensated summation is based on the possibility to simulate double precision
ﬂoating-point addition in single precision arithmetic. To illustrate the basic idea we take,
as in Example 2.3.1,
a = 0.1234567 · 100,
b = 0.4711325 · 104,
so that s = f l (a + b) = 0.4711448 · 104. Suppose we form
c = f l (f l (b −s) + a) = −0.1230000 · 100 + 0.1234567 · 100 = 4567000 · 10−3.
Note that the variable c is computed without error and picks up the information that was
lost in the operation f l (a + b).
36It was suggested that the IEEE 754 standard should require inner products to be precisely speciﬁed, but that
did not happen.

116
Chapter 2. How to Obtain and Estimate Accuracy
Algorithm 2.2. Compensated Summation.
The following algorithm uses compensated summation to accurately compute the sum
n
i=1 xi:
s := x1; c := 0;
for i = 2 : n
y := c + xi;
t := s + y;
c := (s −t) + y;
s := t;
end
It can be proved (see Goldberg [158]) that on binary machines with a guard digit the
computed sum satisﬁes
s =
n

i=1
(1 + ξi)xi,
|ξi| < 2u + O(nu2).
(2.3.20)
This formulation is a typical example of a backward error analysis; see Sec. 2.4.4. The ﬁrst
term in the error bound is independent of n.
2.3.3
Statistical Models for Rounding Errors
The bounds for the accumulated rounding error we have derived so far are estimates of
the maximal error. These bounds ignore the sign of the errors and tend to be much too
pessimistic when the number of variables is large. However, they can still give valuable
insight into the behavior of a method and be used for the purpose of comparing different
methods.
An alternative is a statistical analysis of rounding errors, which is based on the as-
sumption that rounding errors are independent and have some statistical distribution. It
was observed already in the 1950s that rounding errors occurring in the solution of differ-
ential equations are not random and are often strongly correlated. This does not in itself
preclude that useful information can sometimes be obtained by modeling them by random
uncorrelated variables! In many computational situations and scientiﬁc experiments, where
the error can be considered to have arisen from the addition of a large number of indepen-
dent error sources of about the same magnitude, an assumption that the errors are normally
distributed is justiﬁed.
Example 2.3.5.
Figure 2.3.2 illustrates the effect of rounding errors on the evaluation of two different
expressions for the polynomial p(x) = (x −1)5 for x ∈[0.999, 1.001], in IEEE double
precision (unit roundoff u = 1.1·10−16). Among other things it shows that the monotonicity
of a function can be lost due to rounding errors. The model of rounding errors as independent
random variables works well in this example. It is obvious that it would be impossible to
locate the zero of p(x) to a precision better than about (0.5 · 10−14)1/6 ≈0.0007 using the

2.3. Accuracy and Rounding Errors
117
0.999
0.9995
1
1.0005
1.001
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2 x 10
−15
Figure 2.3.2. Calculated values of a polynomial near a multiple root: solid line
p(x) = x5 −5x4 + 10x3 −10x2 + 5x −1 = 0; dashed line p(x) = (x −1)5.
expanded form of p(x). But using the expression p(x) = (1 −x)5 function values can be
evaluated with constant relative precision even close to x = 1, and the problem disappears!
This example shows that although multiple roots are in general ill-conditioned, an
important exception is when the function f (x) is given in such a form that it can be computed
with less absolute error as x approaches α.
The theory of standard error is based on probability theory and will not be treated in
detail here. The standard error of an estimate of a given quantity is the same as the standard
deviation of its sampling distribution.
If in a sum y = n
i=1 xi each xi has error |4i| ≤δ, then the maximum error bound
for y is nδ. Thus, the maximal error grows proportionally to n. If n is large—for example,
n = 1000—then it is in fact highly improbable that the real error will be anywhere near nδ,
since that bound is attained only when every 4xi has the same sign and the same maximal
magnitude. Observe, though, that if positive numbers are added, each of which has been
abridged to t decimals by chopping, then each 4xi has the same sign and a magnitude which
is on average 1
2δ, where δ = 10−t. Thus, the real error is often about 500δ.
If the numbers are rounded instead of chopped, and if one can assume that the errors in
the various terms are stochastically independent with standard deviation ϵ, then the standard
error in y becomes (see Theorem 2.4.5)
(ϵ2 + ϵ2 + · · · + ϵ2)1/2 = ϵ√n.
Thus the standard error of the sum grows only proportionally to √n. This supports the
rule of thumb, suggested by Wilkinson [376, p. 26], that if a rounding error analysis gives
a bound f (n)u for the maximum error, then one can expect the real error to be of size
√f (n)u.
If n ≫1, then the error in y is, under the assumptions made above, approximately
normally distributed with standard deviation σ = ϵ√n. The corresponding frequency
function,
f (t) =
1
√
2π
e−t2/2,

118
Chapter 2. How to Obtain and Estimate Accuracy
is illustrated in Figure 2.3.3; the curve shown there is also called the Gauss curve. The
assumption that the error is normally distributed with standard deviation σ means, for
example, that the statement “the magnitude of the error is greater than 2σ” (see the shaded
area of Figure 2.3.3) is true in only about 5% of all cases (the unshaded area under the
curve). More generally, the assertion that the magnitude of the error is larger than σ, 2σ,
and 3σ respectively, is about 32%, 5%, and 0.27%.
−4
−3
−2
−1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
x
y
Figure 2.3.3. The frequency function of the normal distribution for σ = 1.
One can show that if the individual terms in a sum y = n
i=1 xi have a uniform
probability distribution in the interval [−1
2δ, 1
2δ], then the standard deviation of an individual
term is δ/12. Therefore, in only about 5% of cases is the error in the sum of 1000 terms
greater than 2δ√1000/12 ≈18δ, which can be compared to the maximum error 500δ.
This shows that rounding can be far superior to chopping when a statistical interpretation
(especially the assumption of independence) can be given to the principal sources of errors.
Observe that, in the above, we have only considered the propagation of errors which were
present in the original data, and have ignored the effect of possible roundoff errors in the
additions themselves.
For rounding errors the formula for standard errors is used. For systematic errors,
however, the formula for maximal error (2.4.5) should be used.
2.3.4
Avoiding Overﬂow and Cancellation
In the rare cases when input and output data are so large or small in magnitude that the range
of the machine is not sufﬁcient, one can use higher precision or else work with logarithms
or some other transformation of the data. One should, however, keep in mind the risk that
intermediate results in a calculation can produce an exponent which is too large (overﬂow)
or too small (underﬂow) for the ﬂoating-point system of the machine. Different actions
may be taken in such situations, as well for division by zero. Too small an exponent is
usually, but not always, unproblematic. If the machine does not signal underﬂow, but
simply sets the result equal to zero, there is a risk, however, of harmful consequences.
Occasionally, “unexplainable errors” in output data are caused by underﬂow somewhere in
the computations.

2.3. Accuracy and Rounding Errors
119
The Pythagorean sum c =
√
a2 + b2 occurs frequently, for example, in conversion
to polar coordinates and in computing the complex modulus and complex multiplication. If
the obvious algorithm is used, then damaging underﬂows and overﬂows may occur in the
squaring of a and b even if a and b and the result c are well within the range of the ﬂoating-
point system used. This can be avoided by using instead the algorithm: If a = b = 0, then
c = 0; otherwise set p = max(|a|, |b|), q = min(|a|, |b|), and compute
ρ = q/p;
c = p

1 + ρ2.
(2.3.21)
Example 2.3.6.
The formula (2.3.8) for complex division suffers from the problem that intermediate
results can overﬂow even if the ﬁnal result is well within the range of the ﬂoating-point
system. This problem can be avoided by rewriting the formula as for the Pythagorean sum:
If |c| > |d|, then compute
a + ib
c + id = a + be
r
+ i b −ae
r
,
e = d
c ,
r = c + de.
If |d| > |c|, then e = c/d is computed and a corresponding formula used.
Similar precautions are also needed for computing the Euclidean length (norm) of a
vector ∥x∥2 =
n
i=1 x2
i
1/2, x ̸= 0. We could avoid overﬂows by ﬁrst ﬁnding xmax =
max1≤i≤n |xi| and then forming
s =
n

i=1
(xi/xmax)2,
∥x∥2 = xmax
√s.
(2.3.22)
This has the drawback of needing two passes through the data.
Algorithm 2.3.
The following algorithm, due to S. J. Hammarling, for computing the Euclidean length of
a vector requires only one pass through the data. It is used in the level-1 BLAS routine
xNRM2:
t = 0; s = 1;
for i = 1 : n
if |xi| > 0
if |xi| > t
s = 1 + s(t/xi)2; t = |xi|;
else
s = s + (xi/t)2;
end
end
end
∥x∥2 = t√s;
On the other hand this code does not vectorize and can therefore be slower if implemented
on a vector computer.

120
Chapter 2. How to Obtain and Estimate Accuracy
One very common reason for poor accuracy in the result of a calculation is that
somewhere a subtraction has been carried out in which the difference between the operands
is considerably less than either of the operands. This causes a loss of relative precision.
(Note that, on the other hand, relative precision is preserved in addition of nonnegative
quantities, multiplication, and division.)
Consider the computation of y = x1 −x2, where ˜x1 = x1 + 4x1, ˜x2 = x2 + 4x2
are approximations to the exact values. If the operation is carried out exactly the result is
˜y = y +4y, where 4y = 4x1 −4x2. But, since the errors 4x1 and 4x2 can have opposite
sign, the best error bound for ˜y is
|4y| ≤|4x1| + |4x2|.
(2.3.23)
Notice the plus sign! Hence for the relative error we have
((((
4y
y
(((( ≤|4x1| + |4x2|
|x1 −x2|
.
(2.3.24)
This shows that there can be very poor relative accuracy in the difference between two
nearly equal numbers. This phenomenon is called cancellation of terms.
In Sec. 1.2.1 it was shown that when using the well-known “textbook” formula
r1,2 =

−b ±

b2 −4ac

(2a)
for computing the real roots of the quadratic equation ax2+bx+c = 0 (a ̸= 0), cancellation
could cause a loss of accuracy in the root of smallest magnitude. This can be avoided by
computing the root of smaller magnitude from the relation r1r2 = c/a between coefﬁcients
and roots. The following is a suitable algorithm.
Algorithm 2.4. Solving a Quadratic Equation.
d := b2 −4ac;
if d ≥0 % real roots
r1 := −sign(b)

|b| +
√
d

/(2a);
r2 := c/(a · r1);
else % complex roots x + iy
x := −b/(2a);
y :=
√
−d/(2a);
end
Note that we deﬁne sign (b) = 1 if b ≥0, else sign (b) = −1.37 It can be proved that
in IEEE arithmetic this algorithm computes a slightly wrong solution to a slightly wrong
problem.
37In MATLAB sign (0) = 0, which can lead to failure of this algorithm.

2.3. Accuracy and Rounding Errors
121
Lemma 2.3.5.
Assume that Algorithm 2.4 is used to compute the roots r1,2 of the quadratic equation
ax2 + bx + c = 0. Denote the computed roots by ¯r1,2 and let ˜r1,2 be the exact roots of the
nearby equation ax2 + bx + ˜c = 0, where |˜c −c| ≤γ2|˜c|. Then |˜ri −¯ri| ≤γ5|˜ri|, i = 1, 2.
Proof. See Kahan [216].
More generally, if |δ| ≪x, then one should rewrite
√
x + δ −√x =
x + δ −x
√x + δ + √x =
δ
√x + δ + √x .
There are other exact ways of rewriting formulas which are as useful as the above;
for example,
cos(x + δ) −cos x = −2 sin(δ/2) sin(x + δ/2).
If one cannot ﬁnd an exact way of rewriting a given expression of the form f (x +δ)−f (x),
it is often advantageous to use one or more terms in the Taylor series
f (x + δ) −f (x) = f ′(x)δ + 1
2f ′′(x)δ2 + · · ·
Example 2.3.7 (Cody [73]).
To compute sin 22 we ﬁrst ﬁnd ⌊22/(π/2)⌋= 14. It follows that sin 22 = −sin x∗,
where x∗= 22 −14(π/2). Using the correctly rounded 10-digit approximation π/2 =
1.57079 6327 we obtain
x∗= 22 −14 · 1.57079 6327 = 8.85142 · 10−3.
Here cancellation has taken place and the reduced argument has a maximal error of 7·10−9.
The actual error is slightly smaller since the correctly rounded value is x∗= 8.85144 8711 ·
10−3, which corresponds to a relative error in the computed sin 22 of about 2.4 · 10−6, in
spite of using a 10-digit approximation to π/2.
For very large arguments the relative error can be much larger. Techniques for carrying
out accurate range reductions without actually needing multiple precision calculations are
discussed by Muller [272]; see also Problem 2.3.9.
In previous examples we got a warning that cancellation would occur, since x2 was
found as the difference between two nearly equal numbers each of which was, relatively,
much larger than the difference itself. In practice, one does not always get such a warning,
for two reasons: ﬁrst, in using a computer one has no direct contact with the individual steps
of calculation; second, cancellation can be spread over a great number of operations. This
may occur in computing a partial sum of an inﬁnite series. For example, in a series where
the size of some terms are many orders of magnitude larger than the sum of the series, small
relative errors in the computation of the large terms can then produce large errors in the
result.
It has been emphasized here that calculations where cancellation occurs should be
avoided. But there are cases where one has not been able to avoid it, and there is no time

122
Chapter 2. How to Obtain and Estimate Accuracy
to wait for a better method. Situations occur in practice where (say) the ﬁrst ten digits are
lost, and we need a decent relative accuracy in what will be left.38 Then, high accuracy is
required in intermediate results. This is an instance where the high accuracy in IEEE double
precision is needed!
Review Questions
2.3.1 What is the standard model for ﬂoating-point arithmetic? What weaker model holds
if a guard digit is lacking?
2.3.2 Give examples to show that some of the axioms for arithmetic with real numbers do
not always hold for ﬂoating-point arithmetic.
2.3.3 (a) Give the results of a backward and forward error analysis for computing f l (x1 +
x2 + · · · + xn). It is assumed that the standard model holds.
(b) Describe the idea in compensated summation.
2.3.4 Explain the terms “maximum error” and “standard error.” What statistical assumption
about rounding errors is often made when calculating the standard error in a sum due
to rounding?
2.3.5 Explain what is meant by “cancellation of terms.” Give an example of how this can
be avoided by rewriting a formula.
Problems and Computer Exercises
2.3.1 Rewrite the following expressions to avoid cancellation of terms:
(a) 1 −cos x, |x| ≪1; (b) sin x −cos x, |x| ≈π/4
2.3.2 (a) The expression x2 −y2 exhibits catastrophic cancellation if |x| ≈|y|. Show that
it is more accurate to evaluate it as (x + y)(x −y).
(b) Consider using the trigonometric identity sin2 x+cos2 x = 1 to compute cos x =
(1 −sin2 x)1/2. For which arguments in the range 0 ≤x ≤π/4 will this formula
fail to give good accuracy?
2.3.3 The polar representation of a complex number is
z = x + iy = r(sin φ + cos φ) ≡r · eiφ.
Develop accurate formulas for computing this polar representation from x and y
using real operations.
2.3.4 (Kahan) Show that with the use of fused multiply–add the algorithm
w = f l (b × c);
y := f l (a × d −w);
e := f l (b × c −w);
z = f l (y −e);
38G. Dahlquist has encountered just this situation in a problem of ﬁnancial mathematics.

Problems and Computer Exercises
123
computes with high relative accuracy
z = det

a
b
c
d

= ad −bc.
2.3.5 Suppose that the sum s = n
i=1 xi, n = 2k, is computed using the divide and conquer
technique described in Sec. 1.2.3. Show that this summation algorithm computes an
exact sum
¯s =
n

i=1
xi(1 + δi),
|δi| ≤˜u log2 n.
Hence for large values of n this summation order can be much more accurate than
the conventional order.
2.3.6 Show that for the evaluation of a polynomial p(x) = n
i=0 aixi by Horner’s rule
the following roundoff error estimate holds:
|f l (p(x)) −p(x)| < γ1
n

i=0
(2i + 1)|ai| |x|i,
(2nu ≤0.1).
2.3.7 In solving linear equations by Gaussian elimination expressions of the form s =
(c −n−1
i=1 aibi)/d often occur. Show that, by a slight extension of the result in the
previous problem, that the computed ¯s satisﬁes
(((¯sd −c +
n−1

i=1
aibi
((( ≤γn

|¯sd| +
n−1

i=1
|ai||bi|

,
where the inequality holds independent of the summation order.
2.3.8 The zeros of the reduced cubic polynomial z3 + 3qz −2r = 0 can be found from
the Cardano–Tartaglia formula:
z =

r +

q3 + r2
1/3
+

r −

q3 + r2
1/3
,
where the two cubic roots are to be chosen so that their product equals −q. One real
root is obtained if q3 + r2 ≥0, which is the case unless all three roots are real and
distinct.
The above formula can lead to cancellation. Rewrite it so that it becomes more
suitable for numerical calculation and requires the calculation of only one cubic
root.
2.3.9 (Eldén and Wittmeyer-Koch) In the interval reduction for computing sin x there
can be a loss of accuracy through cancellation in the computation of the reduced
argument x∗= x −k · π/2 when k is large. A way to avoid this without reverting
to higher precision has been suggested by Cody and Waite [75]). Write
π/2 = π0/2 + r,
where π0/2 is exactly representable with a few digits in the (binary) ﬂoating-point
system. The reduced argument is now computed as x∗= (x −k · π0/2) −kr. Here,

124
Chapter 2. How to Obtain and Estimate Accuracy
unless k is very large, the ﬁrst term can be computed without rounding error. The
rounding error in the second term is bounded by k|r| u, where u is the unit roundoff.
In IEEE single precision one takes
π0/2 = 201/128 = 1.573125 = (10.1001001)2,
r = 4.838267949 · 10−4.
Estimate the relative error in the computed reduced argument x∗when x = 1000
and r is represented in IEEE single precision.
2.3.10 (Kahan [218]) The area A of a triangle with sides equal to a, b, c is given by Heron’s
formula:
A =

s(s −a)(s −b)(s −c),
s = (a + b + c)/2.
Show that this formula fails for needle-shaped triangles, using ﬁve-digit decimal
ﬂoating arithmetic and a = 100.01, b = 99.995, c = 0.025.
Thefollowingformulacanbeprovedtoworkifaddition/subtractionsatisﬁes(2.3.21):
Order the sides so that a ≥b ≥c, and use
A = 1
4

(a + (b + c))(c −(a −b))(c + (a −b))(a + (b −c)).
Compute a correct result for the data above using this modiﬁed formula. If a person
tells you that this gives an imaginary result if a −b > c, what do you answer him?
2.3.11 As is well known, f (x) = (1 + x)1/x has the limit e = 2.71828 18284 59045 . . .
when x →∞. Study the sequences f (xn) for xn = 10−n and xn = 2−n, for
n = 1, 2, 3, . . . . Stop when xn < 10−10 (or when xn < 10−20 if you are using
double precision). Give your results as a table of n, xn, and the relative error gn =
(f (xn) −e)/e. Also plot log(|gn|) against log(|xn|). Comment on and explain your
observations.
2.3.12 (a) Compute the derivative of the exponential function ex at x = 0 by approximating
with the difference quotients (ex+h −ex)/h, for h = 2−i, i = 1 : 20. Explain your
results.
(b) Repeat (a), but approximate with the central difference approximation (ex+h −
ex−h)/(2h).
2.3.13 The hyperbolic cosine is deﬁned by cosh t = (et + e−t)/2, and its inverse function
t = arccosh (x) is the solution to
x = (et + e−t)/2.
Solving the quadratic equation (et)2 −2xet + 1, we ﬁnd et = x ± (x2 −1)1/2 and
arccos x = log(x ± (x2 −1)1/2).
(a) Show that this formula suffers from serious cancellation when the minus sign is
used and x is large. Try, e.g., x = cosh(10) using double precision IEEE. (Using
the plus sign will just transfer the problem to negative x.)
(b) A better formula is
arccos x = 2 log

((x + 1)/2)1/2 + ((x −1)/2)1/2
.

Problems and Computer Exercises
125
This also avoids the squaring of x which can lead to overﬂow. Derive this formula
and show that it is well behaved!
2.3.14 (Gautschi) Euler’s constant γ = 0.57721566490153286 . . . is deﬁned as the limit
γ = lim
n→∞γn,
where
γn = 1 + 1/2 + 1/3 + · · · + 1/n −log n.
Assuming that γ −γn ∼cn−d, n →∞, for some constants c and d > 0, try to
determine c and d experimentally on your computer.
2.3.15 In the statistical treatment of data, one often needs to compute the quantities
¯x = 1
n
n

i=1
xi,
s2 = 1
n
n

i=1
(xi −¯x)2.
If the numbers xi are the results of statistically independent measurements of a
quantity with expected value m, then ¯x is an estimate of m, whose standard deviation
is estimated by s/
√
n −1.
(a) The computation of ¯x and m using the formulas above have the drawback that
they require two passes through the data xi. Let α be a provisional mean, chosen as
an approximation to ¯x, and set x′
i = xi −α. Show that the formulas
¯x = α + 1
n
n

i=1
x′
i,
s2 = 1
n
n

i=1
(x′
i)2 −(¯x −α)2
hold for an arbitrary α.
(b) In 16 measurements of a quantity x one got the following results:
i
xi
i
xi
i
xi
i
xi
1
546.85
5
546.81
9
546.96
13
546.84
2
546.79
6
546.82
10
546.94
14
546.86
3
546.82
7
546.88
11
546.84
15
546.84
4
546.78
8
546.89
12
546.82
16
546.84
Compute ¯x and s2 to two signiﬁcant digits using α = 546.85.
(c) In the computations in (b), one never needed more than three digits. If one
uses the value α = 0, how many digits are needed in (x′
i)2 in order to get two
signiﬁcant digits in s2? If one uses ﬁve digits throughout the computations, why is
the cancellation in the s2 more fatal than the cancellation in the subtraction x′
i −α?
(One can even get negative values for s2!)
(d) If we deﬁne
mk = 1
k
k

i=1
xi,
qk =
k

i=1
(xi −mk)2 =
k

i=1
x2
i −1
k

k

i=1
xi
2
,
then it holds that ¯x = mn, and s2 = qn/n. Show the recursion formulas
m1 = x1,
mk = mk−1 + (xk −mk−1)/k
q1 = 0,
qk = qk−1 + (xk −mk−1)2(k −1)/k.

126
Chapter 2. How to Obtain and Estimate Accuracy
2.3.16 Compute the sum in Example 2.3.4 using the natural summation ordering in IEEE
754 double precision. Repeat the computations using compensated summation (Al-
gorithm 2.3).
2.4
Error Propagation
2.4.1
Numerical Problems, Methods, and Algorithms
By a numerical problem we mean here a clear and unambiguous description of the
functional connection between input data—that is, the “independent variables” in the
problem—and output data—that is, the desired results. Input data and output data consist
of a ﬁnite number of real (or complex) quantities and are thus representable by ﬁnite di-
mensional vectors. The functional connection can be expressed in either explicit or implicit
form. We require for the following discussion also that the output data should be uniquely
determined and depend continuously on the input data.
By an algorithm39 for a given numerical problem we mean a complete description
of well-deﬁned operations through which each permissible input data vector is transformed
into an output data vector. By “operations” we mean here arithmetic and logical operations,
which a computer can perform, together with references to previously deﬁned algorithms.
It should be noted that, as the ﬁeld of computing has developed, more and more complex
functions (for example, square root, circular, and hyperbolic functions) are built into the
hardware. In many programming environments operations such as matrix multiplication,
solution of linear systems, etc. are considered as “elementary operations” and for the user
appear as black boxes.
(The concept algorithm can be analogously deﬁned for problems completely different
from numerical problems, with other types of input data and fundamental operations—
for example, inﬂection, merging of words, and other transformations of words in a given
language.)
Example 2.4.1.
To determine the largest real root of the cubic equation
p(z) = a0z3 + a1z2 + a2z + a3 = 0,
with real coefﬁcients a0, a1, a2, a3, is a numerical problem.
The input data vector is
(a0, a1, a2, a3). The output is the desired root x; it is an implicitly deﬁned function of
the input data.
An algorithm for this problem can be based on Newton’s method, supplemented with
rules for how the initial approximation should be chosen and how the iteration process is to be
terminated. One could also use other iterative methods, or algorithms based on the formula
by Cardano–Tartaglia for the exact solution of the cubic equation (see Problem 2.3.8).
Since this uses square roots and cube roots, one needs to assume that algorithms for the
computation of these functions have been speciﬁed previously.
39The term “algorithm” is a Latinization of the name of theArabic ninth century mathematicianAl-Khowârizmî.
He also introduced the word “algebra” (Al-jabr). Western Europe became acquainted with the Hindu positional
number system from a Latin translation of his book entitled Algorithmi de Numero Indorum.

2.4. Error Propagation
127
One often begins the construction of an algorithm for a given problem by breaking
down the problem into subproblems in such a way that the output data from one subproblem
are the input data to the next subproblem.
Thus the distinction between problem and
algorithm is not always so clear-cut. The essential point is that, in the formulation of the
problem, one is only concerned with the initial state and the ﬁnal state. In an algorithm,
however, one should clearly deﬁne each step along the way, from start to ﬁnish.
We use the term numerical method in this book to mean a procedure either to approx-
imate a mathematical problem with a numerical problem or to solve a numerical problem
(or at least to transform it to a simpler problem). A numerical method should be more
generally applicable than an algorithm, and set lesser emphasis on the completeness of the
computational details. The transformation of a differential equation problem to a system of
nonlinear equations can be called a numerical method—even without instructions as to how
to solve the system of nonlinear equations. Newton’s method is a numerical method for
determining a root of a large class of nonlinear equations. In order to call it an algorithm,
conditions for starting and stopping the iteration process should be added.
For a given numerical problem one can consider many different algorithms. As we
have seen in Sec. 2.3 these can, in ﬂoating-point arithmetic, give approximations of widely
varying accuracy to the exact solution.
Example 2.4.2.
The problem of solving the differential equation
d2y
dx2 = x2 + y2
with boundary conditions y(0) = 0, y(5) = 1 is not a numerical problem according to the
deﬁnition stated above. This is because the output is the function y, which cannot in any
conspicuous way be speciﬁed by a ﬁnite number of parameters. The above mathematical
problem can be approximated with a numerical problem if one speciﬁes the output data to be
the values of y for x = h, 2h, 3h, . . . , 5−h. Also, the domain of variation of the unknowns
must be restricted in order to show that the problem has a unique solution. This can be
done, however, and there are a number of different algorithms for solving the problem
approximately, which have different properties with respect to the number of arithmetic
operations needed and the accuracy obtained.
Before an algorithm can be used it has to be implemented in an algorithmic program
language in a reliable and efﬁcient manner. We leave these aspects aside for the moment,
but this is far from a trivial task. Most amateur algorithm writers seem to think that an
algorithm is ready at the point where a professional realizes that the hard and tedious work
is just beginning (George E. Forsythe [120]).
2.4.2
Propagation of Errors and Condition Numbers
In scientiﬁc computing the given input data are usually imprecise. The errors in the input
will propagate and give rise to errors in the output. In this section we develop some general
tools for studying the propagation of errors. Error-propagation formulas are also of great
interest in the planning and analysis of scientiﬁc experiments.

128
Chapter 2. How to Obtain and Estimate Accuracy
Note that rounding errors from each step in a calculation are also propagated to give
errors in the ﬁnal result. For many algorithms a rounding error analysis can be given, which
shows that the computed result always equals the exact (or slightly perturbed) result of a
nearby problem, where the input data have been slightly perturbed (see, e.g, Lemma 2.3.5).
The effect of rounding errors on the ﬁnal result can then be estimated using the tools of this
section.
We ﬁrst consider two simple special cases of error propagation. For a sum of an
arbitrary number of terms we get the following lemma by induction from (2.3.23).
Lemma 2.4.1.
In addition (and subtraction) a bound for the absolute errors in the result is given by
the sum of the bounds for the absolute errors of the operands:
y =
n

i=1
xi,
|4y| ≤
n

i=1
|4xi|.
(2.4.1)
To obtain a corresponding result for the error propagation in multiplication and di-
vision, we start with the observations that for y = log x we have 4(log x) ≈4(x)/x.
In words, the relative error in a quantity is approximately equal to the absolute error in
its natural logarithm. This is related to the fact that displacements of the same length at
different places on a logarithmic scale mean the same relative change of the value. From
this we obtain the following result.
Lemma 2.4.2.
In multiplication and division, an approximate bound for the relative error is obtained
by adding the relative errors of the operands. More generally, for y = xm1
1 xm2
2 · · · xmn
n ,
((((
4y
y
(((( ⪅
n

i=1
|mi|
((((
4xi
xi
(((( .
(2.4.2)
Proof.
The proof follows by differentiating log y
= m1 log x1 + m2 log x2 + · · ·
+ mn log xn.
Example 2.4.3.
In Newton’s method for solving a nonlinear equation a correction is to be calculated
as a quotient 4x = f (xk)/f ′(xk). Close to a root the relative error in the computed value of
f (xk) can be quite large due to cancellation. How accurately should one compute f ′(xk),
assuming that the work grows as one demands higher accuracy? Since the limit for the
relative error in 4x is equal to the sum of the bounds for the relative errors in f (xk) and
f ′(xk), there is no gain in making the relative error in f ′(xk) very much less than the relative
error in f (xk). This observation is of great importance, particularly in the generalization of
Newton’s method to systems of nonlinear equations.
We now study the propagation of errors in more general nonlinear expressions. Con-
sider ﬁrst the case when we want to compute a function y = f (x) of a single real variable

2.4. Error Propagation
129
x.
How is the error in x propagated to y?
Let ˜x −x = 4x.
Then, a natural way
is to approximate 4y = ˜y −y with the differential of y. By the mean value theorem,
4y = f (x + 4x) −f (x) = f ′(ξ)4x, where ξ is a number between x and x + 4x.
Suppose that |4x| ≤ϵ. Then it follows that
|4y| ≤max
ξ
|f ′(ξ)|ϵ,
ξ ∈[x −ϵ, x + ϵ].
(2.4.3)
In practice, it is usually sufﬁcient to replace ξ by the available estimate of x. Even if high
precision is needed in the value of f (x), one rarely needs a high relative precision in an
error bound or an error estimate. (In the neighborhood of zeros of the ﬁrst derivative f ′(x)
one has to be more careful.)
By the implicit function theorem a similar result holds if y is an implicit function of
x deﬁned by g(x, y) = 0. If g(x, y) = 0 and ∂g
∂y (x, y) ̸= 0, then in a neighborhood of x, y
there exists a unique function y = f (x) such that g(x, f (x)) = 0 and it holds that
f ′(x) = −∂g
∂x (x, f (x))
#∂g
∂y (x, f (x)).
Example 2.4.4.
The result in Lemma 2.3.5 does not say whether the computed roots of the quadratic
equation are close to the exact roots r1, r2. To answer that question we must determine
how sensitive the roots are to a relative perturbation in the coefﬁcient c. Differentiating
ax2 + bx + c = 0, where x = x(c) with respect to c, we obtain (2ax + b)dx/dc + 1 = 0,
dx/dc = −1/(2ax + b). With x = r1 and using r1 + r2 = −b/a, r1r2 = c/a this can be
written as
dr1
r1
= −dc
c
r2
r1 −r2
.
This shows that when |r1 −r2| ≪|r2| the roots can be very sensitive to small relative
perturbations in c.
When r1 = r2, that is, when there is a double root, this linear analysis breaks down.
Indeed, it is easy to see that the equation (x −r)2 −4c = 0 has roots x = r ±
√
4c.
To analyze error propagation in a function of several variables f = f (x1, . . . , xn),
we need the following generalization of the mean value theorem.
Theorem 2.4.3.
Assume that the real-valued function f is differentiable in a neighborhood of the point
x = (x1, x2, . . . , xn), and let x = x +4x be a point in this neighborhood. Then there exists
a number θ such that
4f = f (x + 4x) −f (x) =
n

i=1
∂f
∂xi
(x + θ4x)4xi,
0 ≤θ ≤1.
Proof. The proof follows by considering the function F(t) = f (x + t4x), and using the
mean value theorem for functions of one variable and the chain rule.

130
Chapter 2. How to Obtain and Estimate Accuracy
From Theorem 2.4.3 it follows that the perturbation 4f is approximately equal to
the total differential. The use of this approximation means that the function f (x) is, in
a neighborhood of x that contains the point x + 4x, approximated by a linear function.
All the techniques of differential calculus, such as logarithmic differentiation and implicit
differentiation, may be useful for the calculation of the total differential; see the examples
and the problems at the end of this section.
Theorem 2.4.4 (General Formula for Error Propagation).
Let the real-valued function f = f (x1, x2, . . . , xn) be differentiable in a neighbor-
hood of the point x = (x1, x2, . . . , xn) with errors 4x1, 4x2, . . . , 4xn. Then it holds that
4f ≈
n

i=1
∂f
∂xi
4xi.
(2.4.4)
Then for the maximal error in f (x1, . . . , xn) we obtain the approximate upper bound
|4f | ⪅
n

i=1
((((
∂f
∂xi
(((( |4xi|,
(2.4.5)
where the partial derivatives are evaluated at x.
In order to get a strict bound for |4f |, one should use in (2.4.5) the maximum absolute
values of the partial derivatives in a neighborhood of the known point x. In most practical
situations it sufﬁces to calculate |∂f/∂xi| at x and then add a certain marginal amount (5
to 10 percent, say) for safety. Only if the 4xi are large or if the derivatives have a large
relative variation in the neighborhood of x need the maximal values be used. (The latter
situation occurs, for example, in a neighborhood of an extremal point of f (x).)
The bound in Theorem 2.4.4 is the best possible, unless one knows some depen-
dence between the errors of the terms. Sometimes it can, for various reasons, be a gross
overestimate of the real error.
Example 2.4.5.
Compute error bounds for f = x2
1 −x2, where x1 = 1.03 ± 0.01, x2 = 0.45 ± 0.01.
We obtain
((((
∂f
∂x1
(((( = |2x1| ≤2.1,
((((
∂f
∂x2
(((( = | −1| = 1,
and ﬁnd |4f | ≤2.1·0.01+1·0.01 = 0.031, or f = 1.061−0.450±0.032 = 0.611±0.032.
The error bound has been raised 0.001 because of the rounding in the calculation of x2
1.
One is seldom asked to give mathematically guaranteed error bounds. More often it
is satisfactory to give an estimate of the order of magnitude of the anticipated error. The
bound for |4f | obtained with Theorem 2.4.3 estimates the maximal error, i.e., covers the
worst possible cases, where the sources of error 4xi contribute with the same sign and
magnitudes equal to the error bounds for the individual variables.
In practice, the trouble with formula (2.4.5) is that it often gives bounds which are too
coarse. More realistic estimates are often obtained using the standard error introduced in

2.4. Error Propagation
131
Sec. 2.3.3. Here we give without proof the result for the general case, which can be derived
using probability theory and (2.4.4). (Compare with the result for the standard error of a
sum given in Sec. 2.3.3.)
Theorem 2.4.5.
Assume that the errors 4x1, 4x2, . . . , 4xn are independent random variables
with mean zero and standard deviations ϵ1, ϵ2, . . . , ϵn.
Then the standard error ϵ for
f (x1, x2, . . . , xn) is given by the formula
ϵ ≈
 n

i=1
 ∂f
∂xi
2
ϵ2
i
1/2
.
(2.4.6)
Analysis of error propagation is more than just a means for judging the reliability of
calculated results. As remarked above, it has an equally important function as a means for
the planning of a calculation or scientiﬁc experiment. It can help in the choice of algorithm,
and in making certain decisions during a calculation. An example of such a decision is the
choice of step length during a numerical integration. Increased accuracy often has to be
bought at the price of more costly or complicated calculations. One can also shed some light
on the degree to which it is advisable to obtain a new apparatus to improve the measurements
of a given variable when the measurements of other variables are subject to error as well.
It is useful to have a measure of how sensitive the output data are to small changes
in the input data. In general, if “small” changes in the input data can result in “large”
changes in the output data, we call the problem ill-conditioned; otherwise it is called well-
conditioned. (The deﬁnition of large may differ from problem to problem depending on
the accuracy of the data and the accuracy needed in the solution.)
Deﬁnition 2.4.6.
Consider a numerical problem y = f (x) ∈Rm, x ∈Rn, or in component form
yj = fj(x1, . . . , xn), j = 1 : m. Let ˆx be ﬁxed and assume that neither ˆx nor ˆy0f (ˆx) is
zero. The sensitivity of y with respect to small changes in x can be measured by the relative
condition number
κ(f ; ˆx) = lim
ϵ→0 sup
∥h∥=ϵ
%∥f (x + h) −f (x)∥
∥f (x)∥
#∥h∥
∥x∥
)
.
(2.4.7)
We have used a vector norm ∥· ∥to measure the size of a vector; see Sec. A.4.3 in
Online Appendix A. Common vector norms are the p-norms deﬁned by
∥x∥p = (x1|p + |x2|p + · · · + |xn|p)1/p,
1 ≤p < ∞,
where one usually takes p = 1, 2, or p = ∞.
The condition number (2.4.7) is a function of the input data ˆx and also depends on the
choice of norms in the data space and the solution space. It measures the maximal amount
which a given relative perturbation is magniﬁed by the function f , in the limit of inﬁnitely
small perturbations. For perturbations of sufﬁciently small size we have the estimate
∥˜y −y∥≤κϵ∥y∥+ O(ϵ2).

132
Chapter 2. How to Obtain and Estimate Accuracy
We can expect to have roughly s = log10κ less signiﬁcant decimal digits in the solution
than in the input data. However, this may not hold for all components of the output.
Assume that f has partial derivatives with respect to xi, i = 1 : n, and let J(x) be
the Jacobian matrix
Jij(x) = ∂fj(x)
∂xi
,
j = 1 : m,
i = 1 : n.
(2.4.8)
Then, for any matrix norm subordinate to the vector norm (see Online Appendix A.3.3), the
condition number deﬁned above can be expressed as
κ(f ; ˆx) = ∥J(ˆx)∥∥ˆx∥
∥f (ˆx)∥
.
(2.4.9)
For a composite function g ◦f the chain rule for derivatives can be used to show that
κ(g ◦f ; ˆx) ≤κ(g; ˆy)κ(f ; ˆx).
(2.4.10)
If the composite function is ill-conditioned we can infer from this that at least one of the
functions g and f must be ill-conditioned.
If y = f (x) is a linear (bounded) function y = Mx, where M ∈Rm×n, then according
to (2.4.9)
κ(M; x) = ∥M∥∥x∥
∥y∥.
This inequality is sharp in the sense that for any matrix norm and for any M and x there
exists a perturbation δb such that equality holds.
If M is a square and invertible matrix, then from x = M−1y we conclude that
∥x∥≤∥M−1∥∥y∥. This gives the upper bound
κ(M; x) ≤∥M∥∥M−1∥,
(2.4.11)
which is referred to as the condition number of M. For given x (or y), this upper bound may
not be achievable for any perturbation of x. The inequality (2.4.11) motivates the following
deﬁnition.
Theorem 2.4.7.
The condition number for a square nonsingular matrix M ∈Rn×n equals κ(M) =
∥M∥∥M−1∥, where ∥· ∥is a subordinate matrix norm. In particular, for the Euclidean
norm
κ(M) = κ2(M) = ∥M∥2 ∥M−1∥2 = σ1/σn,
(2.4.12)
where σ1 and σn are the largest and smallest singular value of M.
The last expression in (2.4.12) follows by the observation that if M has singular values
σi, i = 1 : n, then M−1 has singular values 1/σi, i = 1 : n; see Theorem 1.4.4.

2.4. Error Propagation
133
We note some simple properties of κ(M). From (αM)−1 = M−1/α it follows that
κ(αM) = κ(M); i.e., the condition number is invariant under multiplication of M by a
scalar. Matrix norms are submultiplicative, i.e., ∥KM∥≤∥K∥∥M∥. From the deﬁnition
and the identity MM−1 = I it follows that
κ(M) = ∥M∥2∥M−1∥2 ≥∥I∥= 1,
i.e., the condition number κ2 is always greater than or equal to one. The composite mapping
of z = Ky and y = Mx is represented by the matrix product KY, and we have
κ(KM) ≤κ(K)κ(M).
It is important to note that the condition number is a property of the mapping x →y
and does not depend on the algorithm used to evaluate y! An ill-conditioned problem is
intrinsically difﬁcult to solve accurately using any numerical algorithm. Even if the input
data are exact, rounding errors made during the calculations in ﬂoating-point arithmetic
may cause large perturbations in the ﬁnal result. Hence, in some sense an ill-conditioned
problem is not well posed.
Example 2.4.6.
If we get an inaccurate solution to an ill-conditioned problem, then often nothing can
be done about the situation. (If you ask a stupid question you get a stupid answer!) But
sometimes the difﬁculty depends on the form one has chosen to represent the input and
output data of the problem.
The polynomial
P (x) = (x −10)4 + 0.200(x −10)3 + 0.0500(x −10)2 −0.00500(x −10) + 0.00100
is identical with a polynomial Q which, if the coefﬁcients are rounded to six digits, becomes
˜Q(x) = x4 −39.8000x3 + 594.050x2 −3941.00x + 9805.05.
One ﬁnds that P(10.11) = 0.0015 ± 10−4, where only three digits are needed in the
computation, while ˜Q(10.11) = −0.0481 ± 1
2 · 10−4, in spite of the fact that eight digits
were used in the computation. The rounding to six digits of the coefﬁcients of Q has thus
caused an error in the polynomial’s value at x = 10.11; the erroneous value is more than 30
times larger than the correct value and has the wrong sign. When the coefﬁcients of Q are
input data, the problem of computing the value of the polynomial for x ≈10 is far more
ill-conditioned than when the coefﬁcients of P are input data.
The conditioning of a problem can to some degree be illustrated geometrically. A
numerical problem P means a mapping of the space X of possible input data onto the space
Y of the output data. The dimensions of these spaces are usually quite large. In Figure 2.4.1
we picture a mapping in two dimensions. Since we are considering relative changes, we
take the coordinate axis to be logarithmically scaled. A small circle of radius r is mapped
onto an ellipse whose ratio of major to minor axis is κr, where κ is the condition number
of the problem P.

134
Chapter 2. How to Obtain and Estimate Accuracy
Space of
Input data
Space of
Output data
X
Y
P
Figure 2.4.1. Geometrical illustration of the condition number.
2.4.3
Perturbation Analysis for Linear Systems
Consider the linear system y = Ax, where A is nonsingular and y ̸= 0. From the analysis
in the previous section we know that the condition number of the inverse mapping x =
A−1y ̸= 0 is bounded by the condition number
κ(A−1) = κ(A) = ∥A−1∥∥A∥.
Assume that the elements of the matrix A are given data and subject to perturbations
δA. The perturbed solution x + δx satisﬁes the linear system
(A + δA)(x + δx) = y.
Subtracting Ax = y we obtain (A + δA)δx = −δAx. Assuming also that the matrix
(A + δA) = A(I + A−1δA) is nonsingular and solving for δx yields
δx = −(I + A−1δA)−1A−1δAx,
(2.4.13)
which is the basic identity for the analysis. Taking norms gives
∥δx∥≤∥(I + A−1δA)−1∥∥A−1∥∥δA∥∥x∥.
It can be shown (see Problem 2.4.9) that if ∥A−1δA∥< 1, then A + δA is nonsingular and
∥(I + A−1δA)−1∥< 1/(1 −∥A−1δA∥).
Neglecting second order terms,
∥δx∥
∥x∥⪅κ(A)∥δA∥
∥A∥.
(2.4.14)
This shows that κ(A) is also the condition number of x = A−1y with respect to perturbations
in A.
For any real, orthogonal matrix Q we have
κ2(Q) = ∥Q∥2∥Q−1∥2 = 1,
soQisperfectlyconditioned. ByLemmaA.4.1(seeOnlineAppendixA)wehave∥QAP∥2 =
∥A∥2 for any orthogonal P and Q. It follows that
κ2(PAQ) = κ2(A),

2.4. Error Propagation
135
i.e., the condition number of a matrix A is invariant under orthogonal transformations. This
important fact is one reason why orthogonal transformations play a central role in numerical
linear algebra.
How large may κ be before we consider the problem to be ill-conditioned? That
depends on the accuracy of the data and the accuracy desired in the solution. If the data
have a relative error of 10−7, then we can guarantee a (normwise) relative error in the
solution ≤10−3 if κ ≤0.5·104. But to guarantee a (normwise) relative error in the solution
≤10−6 we need to have κ ≤5.
Example 2.4.7.
The Hilbert matrix Hn of order n with elements
Hn(i, j) = hij = 1/(i + j −1),
1 ≤i, j ≤n,
is a notable example of an ill-conditioned matrix. In Table 2.4.1 approximate condition
numbers of Hilbert matrices of order ≤12, computed in IEEE double precision, are given.
For n > 12 the Hilbert matrices are too ill-conditioned even for IEEE double precision!
From a result by G. Szegö (see Gautschi [147, p. 34]) it follows that
κ2(Hn) ≈(
√
2 + 1)4(n+1)
215/4√πn
∼e3.5n,
i.e., the condition numbers grow exponentially with n. Although the severe ill-conditioning
exhibited by the Hilbert matrices is rare, moderately ill-conditioned linear systems do occur
regularly in many practical applications!
Table 2.4.1. Condition numbers of Hilbert matrices of order ≤12.
n
κ2(Hn)
n
κ2(Hn)
1
1
7
4.753·108
2
19.281
8
1.526·1010
3
5.241·102
9
4.932·1011
4
1.551·104
10
1.602·1013
5
4.766·105
11
5.220·1014
6
1.495·107
12
1.678·1016
The normwise condition analysis in the previous section usually is satisfactory when
the linear system is “well scaled.” If this is not the case, then a componentwise analysis
may give sharper bounds. We ﬁrst introduce some notations. The absolute values |A| and
|b| of a matrix A and vector b are interpreted componentwise
|A|ij = (|aij|),
|b|i = (|bi|).
The partial ordering “≤” for the absolute values of matrices |A|, |B| and vectors |b|, |c| is
to be interpreted componentwise:40
|A| ≤|B| ⇐⇒|aij| ≤|bij|,
|b| ≤|c| ⇐⇒|bi| ≤|ci|.
40Note that A ≤B in other contexts means that B −A is positive semideﬁnite.

136
Chapter 2. How to Obtain and Estimate Accuracy
It follows easily that |AB| ≤|A| |B| and a similar rule holds for matrix-vector multiplica-
tion.
Taking absolute values in (2.4.13) gives componentwise error bounds for the corre-
sponding perturbations in x,
|δx| ≤|(I + A−1δA)−1| |A−1|(|δA||x| + |δb|).
The matrix (I −|A−1||δA|) is guaranteed to be nonsingular if ∥|A−1| |δA| ∥< 1.
Assume now that we have componentwise bounds for the perturbations in A and b,
say
|δA| ≤ω|A|,
|δb| ≤ω|b|.
(2.4.15)
Neglecting second order terms in ω and using (2.4.15) gives
|δx| ⪅|A−1|(|δA||x| + |δb|) ≤ω|A−1|(|A| |x| + |b|).
(2.4.16)
Taking norms in (2.4.16) we get
∥δx∥⪅ω∥|A−1|(|A| |x| + |b|) ∥+ O(ω2).
(2.4.17)
The scalar quantity
κ|A|(A) = ∥|A−1| |A| ∥
(2.4.18)
is called the Bauer–Skeel condition number of the matrix A.
A different way to examine the sensitivity of various matrix problems is the differen-
tiation of a parametrized matrix. Suppose that λ is a scalar and that A(λ) is a matrix with
elements aij(λ) that are differentiable functions of λ. Then by the derivative of the matrix
A(λ) we mean the matrix
A′(λ) = d
dλA(λ) =
daij
dλ

.
(2.4.19)
Many of the rules for differentiation of scalar functions are easily generalized to differenti-
ation of matrices. For differentiating a product of two matrices there holds
d
dλ[A(λ)B(λ)] = d
dλ[A(λ)]B(λ) + A(λ) d
dλ[B(λ)].
(2.4.20)
Assuming that A−1(λ) exists, using this rule on the identity A−1(λ)A(λ) = I we obtain
d
dλ[A−1(λ)]A(λ) + A−1(λ) d
dλ[A(λ)] = 0,
or, solving for the derivative of the inverse,
d
dλ[A−1(λ)] = −A−1(λ) d
dλ[A(λ)]A−1(λ).
(2.4.21)

2.4. Error Propagation
137
2.4.4
Error Analysis and Stability of Algorithms
One common reason for poor accuracy in the computed solution is that the problem is ill-
conditioned. But poor accuracy can also be caused by a poorly constructed algorithm. We
say in general that an algorithm is unstable if it can introduce large errors in the computed
solutions to a well-conditioned problem.
We consider in the following a ﬁnite algorithm with input data (a1, . . . , ar), which by
a sequence of arithmetic operations is transformed into the output data (w1, . . . , ws), There
are two basic forms of roundoff error analysis for such an algorithm, which are both useful:
(i) In forward error analysis, one attempts to ﬁnd bounds for the errors in the solution
|wi −wi|, i = 1 : s, where wi denotes the computed value of wi. The main tool used
in forward error analysis is the propagation of errors, as studied in Sec. 2.4.2.
(ii) In backward error analysis, one attempts to determine a modiﬁed set of data ai +4ai
such that the computed solution wi is the exact solution, and give bounds for |4ai|.
There may be an inﬁnite number of such sets; in this case we seek to minimize the
size of|4ai|. However, it can also happen, even for very simple algorithms, that no
such set exists.
Sometimes, when a pure backward error analysis cannot be achieved, one can show
that the computed solution is a slightly perturbed solution to a problem with slightly modiﬁed
input data. An example of such a mixed error analysis is the error analysis given in
Lemma 2.3.5 for the solution of a quadratic equation.
In backward error analysis no reference is made to the exact solution for the original
data. In practice, when the data are known only to a certain accuracy, the “exact” solution
may not be well deﬁned. Then any solution whose backward error is smaller than the domain
of uncertainty of the data may be considered to be a satisfactory result.
A frequently occurring backward error problem is the following. Suppose we are
given an approximate solution y to a linear system Ax = b. We want to ﬁnd out if y is the
exact solution to a nearby perturbed system (A + 4A)y = b + 4b. To this end we deﬁne
the normwise backward error of y as
η(y) = min{ϵ | (A + 4A)y = b + 4b, ∥4A∥≤ϵ∥A∥, ∥4b∥≤ϵ∥b∥}.
(2.4.22)
The following theorem tells us that the normwise backward error of y is small if the residual
vector b −Ay is small.
Theorem 2.4.8 (Rigal and Gaches [305]).
The normwise backward error of y is given by
η(y) =
∥r∥
∥A∥∥y∥+ ∥b∥,
(2.4.23)
where r = b −Ay, and ∥· ∥is any consistent norm.
Similarly, we deﬁne the componentwise backward error ω(y) of y by
ω(y) = min{ϵ | (A + 4A)y = b + 4b, |4A| ≤ϵ∥A∥, |4b| ≤ϵ|b|}.
(2.4.24)
As the following theorem shows, there is a simple expression also for ω(y).

138
Chapter 2. How to Obtain and Estimate Accuracy
Theorem 2.4.9 (Oettli and Prager [277]).
Let r = b −A¯x, E and f be nonnegative, and set
ω(y) = max
i
|ri|
(E|¯x| + f )i
,
(2.4.25)
where ξ/0 is interpreted as zero if ξ = 0 and inﬁnity otherwise.
By means of backward error analysis it has been shown, even for many quite compli-
cated matrix algorithms, that the computed results which the algorithm produces under the
inﬂuence of roundoff error are the exact output data of a problem of the same type in which
the relative change in data only are of the order of the unit roundoff u.
Deﬁnition 2.4.10.
An algorithm is backward stable if the computed solution w for the data a is the
exact solution of a problem with slightly perturbed data ¯a such that for some norm ∥· ∥it
holds that
∥¯a −a∥/∥a∥< c1u,
(2.4.26)
where c1 is a not too large constant and u is the unit roundoff.
We are usually satisﬁed if we can prove normwise forward or backward stability for
some norm, for example, ∥· ∥2 or ∥· ∥∞. Occasionally we may like the estimates to hold
componentwise,
|¯ai −ai|/|ai| < c2u,
i = 1 : r.
(2.4.27)
For example, by equation (2.3.16) the usual algorithm for computing an inner product xT y
is backward stable for elementwise relative perturbations.
We would like stability to hold for some class of input data. For example, a numerical
algorithm for solving systems of linear equations Ax = b is backward stable for a class of
matrices A if for each A ∈A and for each b the computed solution ¯x satisﬁes ¯A¯x = ¯b,
where ¯A and ¯b are close to A and b.
To yield error bounds for wi, a backward error analysis has to be complemented with
a perturbation analysis. For this the error propagation formulas in Sec. 2.4.2 can often be
used. If the condition number of the problem is κ, then it follows that
∥w −w∥≤c1uκ∥w∥+ O(u2).
(2.4.28)
Hence the error in the solution may still be large if the problem is ill-conditioned. But we
have obtained an answer which is the exact mathematical solution to a problem with data
close to the one we wanted to solve. If the perturbations ¯a −a are within the uncertainties
of the given data, the computed solution is as good as our data warrant!
A great advantage of backward error analysis is that, when it applies, it tends to give
much sharper results than a forward error analysis. Perhaps more important, it usually also
gives a better insight into the stability (or lack of it) of the algorithm.

2.4. Error Propagation
139
By the deﬁnition of the condition number κ it follows that backward stability implies
forward stability, but the converse is not true. Many important direct algorithms for solving
linear systems are known to be backward stable. The following result for the Cholesky
factorization is an important example.
Theorem 2.4.11 (Wilkinson [378]).
Let A ∈Rn×n be a symmetric positive deﬁnite matrix. Provided that
2n3/2uκ(A) < 0.1,
(2.4.29)
the Cholesky factor of A can be computed without breakdown, and the computed factor ¯R
satisﬁes
¯RT ¯R = A + E,
∥E∥2 < 2.5n3/2u∥A∥2,
(2.4.30)
and hence is the exact Cholesky factor of a matrix close to A.
For the LU factorization of matrix A the following componentwise backward error
result is known.
Theorem 2.4.12.
If the LU factorization of the matrix A ∈Rn×n runs to completion, then the computed
factors ¯L and ¯U satisfy
A + E = ¯L ¯U,
|E| ≤γn| ¯L|| ¯U|,
(2.4.31)
where γn = nu/(1 −nu), and u is the unit roundoff.
This shows that unless the elements in the computed factors | ¯L| and | ¯U| become large,
LU factorization is backward stable.
Example 2.4.8.
For ϵ = 10−6 the system

ϵ
1
1
1
 
x1
x2

=

1
0

is well-conditioned and has the exact solution x1 = −x2 = −1/(1 −ϵ) ≈−1. In Gaussian
elimination we multiply the ﬁrst equation by 106, and subtract from the second, giving
(1 −106)x2 = −106. Rounding this to x2 = 1 is correct to six digits. In the back-
substitution to obtain x1, we then get 10−6x1 = 1 −1, or x1 = 0, which is a completely
wrong result. This shows that Gaussian elimination can be an unstable algorithm unless
row (and/or column) interchanges are performed to limit element growth.
Some algorithms, including most iterative methods, are not backward stable. Then it
is necessary to weaken the deﬁnition of stability. In practice an algorithm can be considered
stable if it produces accurate solutions for well-conditioned problems. Such an algorithm
can be called weakly stable. Weak stability may be sufﬁcient for giving conﬁdence in an
algorithm.

140
Chapter 2. How to Obtain and Estimate Accuracy
Example 2.4.9.
In the method of normal equations for computing the solution of a linear least squares
problem one ﬁrst forms the matrix AT A. This product matrix can be expressed in outer
form as
AT A =
m

i=1
aiaT
i ,
where aT
i is the ith row of A, i.e., AT = ( a1
a2
. . .
am ). From (2.3.14) it follows that
this computation is not backward stable; i.e., it is not true that f l(AT A) = (A+E)T (A+E)
for some small error matrix E. In order to avoid loss of signiﬁcant information higher
precision needs to be used.
Backward stability is easier to prove when there is a sufﬁciently large set of input
data compared to the number of output data. When computing the outer product xyT (as in
Example 2.4.9) there are 2n data and n2 results. This is not a backward stable operation.
When the input data are structured rather than general, backward stability often does not
hold.
Example 2.4.10.
Many algorithms for solving a linear system Ax = b are known to be backward
stable; i.e., the computed solution is the exact solution of a system (A+E)x = b, where the
normwise relative error ∥E∥/∥A∥is not much larger than the machine precision. In many
applications the system matrix is structured. An important example is Toeplitz matrices
T , whose entries are constant along every diagonal:
T = (ti−j)1≤i,j≤n =


t0
t1
. . .
tn−1
t−1
t0
. . .
tn−2
...
...
...
...
t−n+1
t−n+2
. . .
t0

∈Rn×n.
(2.4.32)
Note that a Toeplitz matrix is completely speciﬁed by its ﬁrst row and column, i.e., the
2n −1 quantities t = (t−n+1, . . . , t0, . . . , tn−1).
Ideally, in a strict backward error analysis, we would like to show that a solution
algorithm always computes an exact solution to a nearby Toeplitz system deﬁned by T + S,
where S is small. It has been shown that no such algorithm can exist! We have to be content
with algorithms that (at best) compute the exact solution of (T + E)x = b, where ∥E∥is
small but E unstructured.
In the construction of an algorithm for a given problem, one often breaks down the
problem into a chain of subproblems, P1, P2, . . . , Pk, for which algorithms A1, A2, . . . , Ak
are known, in such a way that the output data from Pi−1 are the input data to Pi. Different
ways of decomposing the problem give different algorithms with, as a rule, different stability
properties. It is dangerous if the last subproblem in such a chain is ill-conditioned. On the
other hand, it need not be dangerous if the ﬁrst subproblem is ill-conditioned, if the problem
itself is well-conditioned. Even if the algorithms for all the subproblems are stable, we
cannot conclude that the composed algorithm is stable.

2.4. Error Propagation
141
Example 2.4.11.
The problem of computing the eigenvalues λi of a symmetric matrix A, given its ele-
ments (aij), is always a well-conditioned numerical problem with absolute condition number
equal to 1. Consider an algorithm which breaks down this problem into two subproblems:
• P1: compute the coefﬁcients of the characteristic polynomial of the matrix A p(λ) =
det(A −λI) of the matrix A.
• P2: compute the roots of the polynomial p(λ) obtained from P1.
It is well known that the second subproblem P2 can be very ill-conditioned. For
example, forasymmetricmatrixAwitheigenvalues±1, ±2, . . . , ±20theconditionnumber
for P2 is 1014 in spite of the fact that the origin lies exactly between the largest and smallest
eigenvalues, so that one cannot blame the high condition number on a difﬁculty of the same
type as that encountered in Example 2.4.7.
The important conclusion that eigenvalues should not be computed as outlined above
is further discussed in Sec. 6.5.2.
On the other hand, as the next example shows, it need not be dangerous if the ﬁrst sub-
problem of a decomposition is ill-conditioned, even if the problem itself is well-conditioned.
Example 2.4.12.
The ﬁrst step in many algorithms for computing the eigenvalues λi of a symmetric
matrix A is to use orthogonal similarity transformations to symmetric tridiagonal form,
QT AQ = T =


α1
β2
β2
α2
β3
...
...
...
βn−1
αn−1
βn
βn
αn


.
In the second step the eigenvalues of T , which coincide with those of A, are computed.
Wilkinson [377, Sec. 5.28] showed that the computed tridiagonal matrix can differ
signiﬁcantly from the matrix corresponding to exact computation. Hence here the ﬁrst
subproblem is ill-conditioned. (This fact is not as well known as it should be and still alarms
many users!) But the second subproblem is well-conditioned and the combined algorithm is
known to be backward stable, i.e., the computed eigenvalues are the exact eigenvalues of a
matrix A+E, where ∥E∥2 < c(n)u∥A∥2. This is a more complex example of a calculation,
where rounding errors cancel!
It should be stressed that the primary purpose of a rounding error analysis is to give
insight into the properties of the algorithm. In practice we can usually expect a much
smaller backward error in the computed solutions than the bounds derived in this section.
It is appropriate to recall here a remark by J. H. Wilkinson:
All too often, too much attention is paid to the precise error bound that has
been established. The main purpose of such an analysis is either to establish

142
Chapter 2. How to Obtain and Estimate Accuracy
the essential numerical stability of an algorithm or to show why it is unstable
and in doing so expose what sort of change is necessary to make it stable. The
precise error bound is not of great importance.
The treatment in this section is geared toward matrix problems and is not very useful,
for example, for time-dependent problems in ordinary and partial differential equations. In
Sec. 1.5 some methods for the numerical solution of an initial value problem
y′′ = −y,
y(0) = 0,
y′(0) = 1
were studied. As will be illustrated in Example 3.3.15, catastrophic error growth can occur
in such processes. The notion of stability is here related to the stability of linear difference
equations.
Review Questions
2.4.1 The maximal error bounds for addition and subtraction can for various reasons be a
coarse overestimate of the real error. Give two reasons, preferably with examples.
2.4.2 How is the condition number κ(A) of a matrix A deﬁned? How does κ(A) relate to
perturbations in the solution x to a linear system Ax = b, when A and b are perturbed?
2.4.3 Deﬁne the condition number of a numerical problem P of computing output data
y1, . . . , ym given input data x1, . . . , xn.
2.4.4 Give examples of well-conditioned and ill-conditioned problems.
2.4.5 What is meant by (a) a forward error analysis; (b) a backward error analysis;
(c) a mixed error analysis?
2.4.6 What is meant by (a) a backward stable algorithm; (b) a forward stable algorithm; (c)
a mixed stable algorithm; (d) a weakly stable algorithm?
Problems and Computer Exercises
2.4.1 (a) Determine the maximum error for y = x1x2
2/√x3, where x1 = 2.0 ± 0.1,
x2 = 3.0 ± 0.2, and x3 = 1.0 ± 0.1. Which variable contributes most to the error?
(b) Compute the standard error using the same data as in (a), assuming that the error
estimates for the xi indicate standard deviations.
2.4.2 One wishes to compute f = (
√
2 −1)6, using the approximate value 1.4 for
√
2.
Which of the following mathematically equivalent expressions gives the best result?
1
(
√
2 + 1)6 ;
(3 −2
√
2)3;
1
(3 + 2
√
2)3 ;
99 −70
√
2;
1
99 + 70
√
2
2.4.3 Analyze the error propagation in xα
(a) if x is exact and α in error; (b) if α is exact and x in error.

Problems and Computer Exercises
143
2.4.4 One is observing a satellite in order to determine its speed. At the ﬁrst observation,
R = 30,000 ± 10 miles. Five seconds later, the distance has increased by r =
125.0 ± 0.5 miles and the change in the angle is φ = 0.00750 ± 0.00002 radians.
What is the speed of the satellite, assuming that it moves in a straight line and with
constant speed in the interval?
2.4.5 Onehasmeasuredtwosidesandtheincludedangleofatriangletobea = 100.0±0.1,
b = 101.0 ± 0.1, and angle C = 1.00◦± 0.01◦. Then the third side is given by the
cosine theorem
c = (a2 + b2 −2ab cos C)1/2.
(a) How accurately is it possible to determine c from the given data?
(b) How accurately does one get c if one uses the value cos 1◦= 0.9998, which is
correct to four decimal places?
(c) Show that by rewriting the cosine theorem as
c = ((a −b)2 + 4ab sin2(C/2))1/2
it is possible to compute c to full accuracy using only a four-decimal table for the
trigonometric functions.
2.4.6 Consider the linear system

1
α
α
1
 
x
y

=

1
0

,
where α ̸= 1. What is the relative condition number for computing x? Using
Gaussian elimination and four decimal digits, compute x and y for α = 0.9950 and
compare with the exact solution x = 1/(1 −α2), y = −α/(1 −α2).
2.4.7 (a) Let two vectors u and v be given with components (u1, u2) and (v1, v2). The
angle φ between u and v is given by the formula
cos φ =
u1v1 + u2v2
(u2
1 + u2
2)1/2(v2
1 + v2
2)1/2 .
Show that computing the angle φ from the components of u and v is a well-
conditioned problem.
Hint: Take the partial derivative of cos φ with respect to u1, and from this compute
∂φ/∂u1. The other partial derivatives are obtained by symmetry.
(b) Show that the formula in (a) is not stable for small angles φ.
(c) Show that the following algorithm is stable. First normalize the vectors ˜u =
u/∥u∥2, ˜v = v/∥v∥2. Then compute α = ∥˜u −˜v∥2, β = ∥˜u + ˜v∥2 and set
φ =
%
2 arctan(α/β)
if α ≤β,
π −2 arctan(β/α)
if α > β.
2.4.8 For the integral
I(a, b) =
 1
0
e−bx
a + x2 dx,
the physical quantities a and b have been measured to be a = 0.4000 ± 0.003,

144
Chapter 2. How to Obtain and Estimate Accuracy
b = 0.340 ± 0.005. When the integral is computed for various perturbed values of
a and b, one obtains
a
b
I
0.39
0.34
1.425032
0.40
0.32
1.408845
0.40
0.34
1.398464
0.40
0.36
1.388198
0.41
0.34
1.372950
.
Estimate the uncertainty in I(a, b)!
2.4.9 (a) Let B ∈Rn×n be a matrix for which ∥B∥< 1. Show that the inﬁnite sum and
product
(I −B)−1 =
%
I + B + B2 + B3 + B4 · · · ,
(I + B)(I + B2)(I + B4)(I + B8) · · ·
both converge to the indicated limit.
Hint: Use the identity (I −B)(I + B + · · · + Bk) = I −Bk+1.
(b) Show that the matrix (I −B) is nonsingular and that
∥(I −B)−1∥≤1/(1 −∥B∥).
2.4.10 Solve the linear system in Example 2.4.8 with Gaussian elimination after exchanging
the two equations. Do you now get an accurate result?
2.4.11 Derive forward and backward recursion formulas for calculating the integrals
In =
 1
0
xn
4x + 1 dx.
Why is one algorithm stable and the other unstable?
2.4.12 (a) Use the results in Table 2.4.1 to determine constants c and q such that κ(Hn) ≈
c · 10q.
(b) Compute the Bauer–Skeel condition number cond (Hn) = ∥|H −1
n ||Hn| ∥2, of
the Hilbert matrices for n = 1 : 12. Compare the result with the values of κ(Hn)
given in Table 2.4.1.
2.4.13 Vandermonde matrices are structured matrices of the form
Vn =


1
1
· · ·
1
α1
α2
· · ·
αn
...
...
· · ·
...
αn−1
1
αn−1
2
· · ·
αn−1
n

.
Let αj = 1 −2(j −1)/(n −1), j = 1 : n. Compute the condition numbers κ2(Vn)
for n = 5, 10, 15, 20, 25. Is the growth in κ2(Vn) exponential in n?

2.5. Automatic Control of Accuracy and Veriﬁed Computing
145
2.5
Automatic Control of Accuracy and Veriﬁed
Computing
2.5.1
Running Error Analysis
A different approach to rounding error analysis is to perform the analysis automatically, for
each particular computation. This gives an a posteriori error analysis in contrast to the a
priori error analysis discussed above.
A simple form of a posteriori analysis, called running error analysis, was used in the
early days of computing; see Wilkinson [380]. To illustrate his idea we rewrite the basic
model for ﬂoating-point arithmetic as
x op y = f l (x op y)(1 + ϵ).
This holds for most implementations of ﬂoating-point arithmetic. Then, the actual error
can be estimated |f l (x op y) −x op y| ≤u|f l (x op y)|. Note that the error is now given
in terms of the computed result and is available in the computer at the time the operation
is performed. This running error analysis can often be easily implemented. We just take
an existing program and modify it, so that as each arithmetic operation is performed, the
absolute value of the computed results is added into the accumulating error bound.
Example 2.5.1.
The inner product f l (xT y) is computed by the program
s = 0;
η = 0;
for i = 1, 2, . . . , n
t = f l (xiyi);
η = η + |t|;
s = f l (s + t);
η = η + |s|;
end
For the ﬁnal error we have the estimate |f l (xT y) −xT y| ≤ηu. Note that a running error
analysis takes advantage of cancellations in the sum. This is in contrast to the previous
estimates, which we call a priori error analysis, where the error estimate is the same for all
distribution of signs of the elements xi and yi.
Efforts have been made to design the computational unit of a computer so that it
gives, in every arithmetic operation, only those digits of the result which are judged to be
signiﬁcant (possibly with a ﬁxed number of extra digits), so-called unnormalized ﬂoating
arithmetic. This method reveals poor construction in algorithms, but in many other cases it
gives a signiﬁcant and unnecessary loss of accuracy. The mechanization of the rules, which
a knowledgeable and experienced person would use for control of accuracy in hand calcu-
lation, is not as free from problems as one might expect. As a complement to arithmetical
operations of conventional type, the above type of arithmetic is of some interest, but it is
doubtful that it will ever be widely used.
A fundamental difﬁculty in automatic control of accuracy is that to decide how many
digits are needed in a quantity to be used in later computation, one needs to consider the entire

146
Chapter 2. How to Obtain and Estimate Accuracy
context of the computations. It can in fact occur that the errors in many operands depend on
each other in such a way that they cancel each other. Such a cancellation of error, which
is a completely different phenomenon from the previously discussed cancellation of terms,
is most common in larger problems, but will be illustrated here with a simple example.
Example 2.5.2.
Suppose we want to compute y = z1 + z2, where z1 =
√
x2 + 1, z2 = 200 −x,
x = 100 ± 1, with a rounding error which is negligible compared to that resulting from
the errors in z1 and z2. The best possible error bounds in the intermediate results are
z1 = 100 ± 1, z2 = 100 ± 1. It is then tempting to be satisﬁed with the result y = 200 ± 2.
But the errors in z1 and z2 due to the uncertainty in x will, to a large extent, cancel
each other! This becomes clear if we rewrite the expression as
y = 200 +

x2 + 1 −x

= 200 +
1
√
x2 + 1 + x
.
It follows that y = 200 + z, where 1/202 ⪅z ≤1/198. Thus y can be computed with
an absolute error less than about 2/(200)2 = 0.5 · 10−4. Therefore, using the expression
y = z1 + z2 the intermediate results z1 and z2 should be computed to four decimals even
though the last integer in these is uncertain. The result is y = 200.0050 ± 1
210−4.
In larger problems, such a cancellation of errors can occur even though one cannot
easily give a way to rewrite the expressions involved. The authors have seen examples
where the ﬁnal result, a sum of seven terms, was obtained correctly to eight decimals even
though the terms, which were complicated functions of the solution to a system of nonlinear
equations with 14 unknowns, were correct only to three decimals! Another nontrivial
example is given in Example 2.4.12.
2.5.2
Experimental Perturbations
In many practical problems, the functional dependence between input data and output data
is so complicated that it is difﬁcult to directly apply the general formulas for error propa-
gation derived in Sec. 2.4.3. One can then investigate the sensitivity of the output data for
perturbations in the input data by means of an experimental perturbational calculation.
One performs the calculations many times with perturbed input data and studies the per-
turbations in the output data. For example, instead of using the formula for standard error
of a function of many variables, given in Theorem 2.4.5, it is often easier to compute the
function a number of times with randomly perturbed arguments and to use them to estimate
the standard deviation of the function numerically.
Important data, such as the step length in a numerical integration or the parameter
which determines when an iterative process is going to be broken off, should be varied with
all the other data left unchanged. If one can easily vary the precision of the machine in the
arithmetic operations one can get an idea of the inﬂuence of rounding errors. It is generally
not necessary to make a perturbational calculation for each and every data component;
one can instead perturb many input data simultaneously—for example, by using random
numbers.

2.5. Automatic Control of Accuracy and Veriﬁed Computing
147
A perturbational calculation often gives not only an error estimate but also greater
insight into the problem. Occasionally, it can be difﬁcult to interpret the perturbational data
correctly, since the disturbances in the output data depend not only on the mathematical
problem but also on the choice of numerical method and the details in the design of the
algorithm. The rounding errors during the computation are not the same for the perturbed
and unperturbed problem. Thus if the output data react more sensitively than one had
anticipated, it can be difﬁcult to immediately point out the source of the error. It can then
be proﬁtable to plan a series of perturbation experiments with the help of which one can
separate the effects of the various sources of error. If the dominant source of error is the
method or the algorithm, then one should try another method or another algorithm. It is
beyond the scope of this book to give further comments on the planning of such experiments.
Imagination and the general insights regarding error analysis, which this chapter is meant
to give, play a large role.
2.5.3
Interval Arithmetic
In interval arithmetic one assumes that all input values are given as intervals and systemat-
ically calculates an inclusion interval for each intermediate result. It is partly an automation
of calculation with maximal error bounds. The importance of interval arithmetic is that it
provides a tool for computing validated answers to mathematical problems.
ThemoderndevelopmentofintervalarithmeticwasinitiatedbytheworkofR.E.Moore
[271]. Interval arithmetic has since been developed into a useful tool for many problems in
scientiﬁc computing and engineering. A noteworthy example of its use is the veriﬁcation
of the existence of a Lorenz attractor by W. Tucker [361]. Several extensive surveys on
interval arithmetic are available; see [3, 4, 225]. Hargreaves [186] gives a short tutorial on
INTLAB and also a good introduction to interval arithmetic.
Themostfrequentlyusedrepresentationsfortheintervalsaretheinﬁmum-supremum
representations
I = [a, b] := {x | a ≤x ≤b},
(a ≤b),
(2.5.1)
where x is a real number. The absolute value or the magnitude of an interval is deﬁned as
| [a, b] | = mag([a, b]) = max{|x| | x ∈[a, b]},
(2.5.2)
and the mignitude of an interval is deﬁned as
mig([a, b]) = min{|x| | x ∈[a, b]}.
(2.5.3)
In terms of the endpoints we have
mag([a, b]) = max{|a|, |b|},
mig([a, b]) =
! min{|a|, |b|}
if 0 /∈[a, b],
0
otherwise.
The result of an interval operation equals the range of the corresponding real operation.
For example, the difference between the intervals [a1, a2] and [b1, b2] is deﬁned as the
shortest interval which contains all the numbers x1 −x2, where x1 ∈[a1, a2], x2 ∈[b1, b2],

148
Chapter 2. How to Obtain and Estimate Accuracy
i.e., [a1, a2]−[b1, b2] := [a1−b2, a2−b1]. Other elementary interval arithmetic operations
are similarly deﬁned:
[a1, a2] op [b1, b2] := {x1 op x2 | x1 ∈[a1, a2], x2 ∈[b1, b2]},
(2.5.4)
where op ∈{+, −, ×, div }. The interval value of a function φ (for example, the elementary
functions sin, cos, exp, log) evaluated on an interval is deﬁned as
φ([a, b]) =
*
inf
x∈[a,b] φ(x), sup
x∈[a,b]
φ(x)
+
.
Operational Deﬁnitions
Although (2.5.4) characterizes interval arithmetic operations, we also need operational
deﬁnitions. We take
[a1, a2] + [b1, b2] = [a1 + b1, a2 + b2],
[a1, a2] −[b1, b2] = [a1 −b2, a2 −b1],
[a1, a2] × [b1, b2] =
,
min{a1b1, a1b2, a2b1, a2b2}, max{a1b1, a1b2, a2b1, a2b2}
-
,
1/[a1, a2] = [1/a2, 1/a1]
for
a1a2 > 0,
(2.5.5)
[a1, a2]/[b1, b2] = [a1, a2] · (1/[b1, b2]).
It is easy to prove that in exact interval arithmetic the operational deﬁnitions above give the
exact range (2.5.4) of the interval operations. Division by an interval containing zero is not
deﬁned and may cause an interval computation to come to a premature end.
A degenerate interval with radius zero is called a point interval and can be identiﬁed
with a single number a ≡[a, a]. In this way the usual arithmetic is recovered as a special
case. The intervals 0 = [0, 0] and 1 = [1, 1] are the neutral elements with respect to interval
addition and interval multiplication, respectively. A nondegenerate interval has no inverse
with respect to addition or multiplication. For example, we have
[1, 2] −[1, 2] = [−1, 1],
[1, 2]/[1, 2] = [0.5, 2].
For interval operations the commutative law
[a1, a2] op [b1, b2] = [b1, b2] op [a1, a2]
holds. But the distributive law has to be replaced by so-called subdistributivity:
[a1, a2]([b1, b2] + [c1, c2]) ⊆[a1, a2][b1, b2] + [a1, a2][c1, c2].
(2.5.6)
This unfortunately means that expressions, which are equivalent in real arithmetic, differ in
exact interval arithmetic. The simple example
[−1, 1]([1, 1] + [−1, −1]) = 0 ⊂[−1, 1][1, 1] + [−1, 1][−1, −1] = [−2, 2]
shows that −[−1, 1] is not the additive inverse to [−1, 1] and also illustrates (2.5.6).

2.5. Automatic Control of Accuracy and Veriﬁed Computing
149
The operations introduced are inclusion monotonic, i.e.,
[a1, a2] ⊆[c1, c2], [b1, b2] ⊆[d1, d2] ⇒[a1, a2] op [b1, b2] ⊆[c1, c2] op [d1, d2].
(2.5.7)
An alternative representation for an interval is the midpoint-radius representation,
for which we use brackets,
⟨c, r⟩:= {x
(( |x −c| ≤r}
(0 ≤r),
(2.5.8)
where the midpoint and radius of the interval [a, b] are deﬁned as
c = mid ([a, b]) = 1
2(a + b),
r = rad ([a, b]) = 1
2(b −a).
(2.5.9)
For intervals in the midpoint-radius representation we take as operational deﬁnitions
⟨c1, r1⟩+ ⟨c2, r2⟩= ⟨c1 + c2, r1 + r2⟩,
⟨c1, r1⟩−⟨c2, r2⟩= ⟨c1 −c2, r1 + r2⟩,
⟨c1, r1⟩× ⟨c2, r2⟩= ⟨c1c2, |c1|r2 + r1|c2| + r1r2⟩,
(2.5.10)
1/⟨c, r⟩= ⟨c/(|c|2 −r2), r/(|c|2 −r2)⟩,
(|c| > r),
⟨c1, r1⟩/⟨c2, r2⟩= ⟨c1, r1⟩× (1/⟨c2, r2⟩).
For addition, subtraction, and inversion, these give the exact range, but for multiplication
and division they overestimate the range (see Problem 2.5.2). For multiplication we have
for any x1 ∈⟨c1, r1⟩and x2 ∈⟨c2, r2⟩
|x1x2 −c1c2| = |c1(x2 −c2) + c2(x1 −c1) + (x1 −c1)(x2 −c2)|
≤|c1|r2 + |c2|r1 + r1r2.
In implementing interval arithmetic using ﬂoating-point arithmetic, the operational
interval results may not be exactly representable as ﬂoating-point numbers. Then if the
lower bound is rounded down to the nearest smaller machine number and the upper bound
rounded up, the exact result must be contained in the resulting interval. Recall that these
rounding modes (rounding to −∞and +∞) are supported by the IEEE 754 standard. For
example, using ﬁve signiﬁcant decimal arithmetic, we would like to get
[1, 1] + [−10−10, 10−10] = [0.99999, 1.0001]
or, in midpoint-radius representation,
⟨1, 0⟩+ ⟨0, 10−10⟩= ⟨1, 10−10⟩.
Note that in the conversion between decimal and binary representation rounding the appro-
priate rounding mode must also be used where needed. For example, converting the point
interval 0.1 to binary IEEE double precision we get an interval with radius 1.3878 · 10−17.
The conversion between the inﬁmum-supremum representation is straightforward but the
inﬁmum-supremum and midpoint may not be exactly representable.

150
Chapter 2. How to Obtain and Estimate Accuracy
Interval arithmetic applies also to complex numbers.
A complex interval in the
inﬁmum-supremum representation is
[z1, z2] = {z = x + iy | x ∈[x1, x2], y ∈[y1, y2]}.
This deﬁnes a closed rectangle in the complex plane deﬁned by the two real intervals,
[z1, z2] = [x1, x2] + i[y1, y2],
x1 ≤x2,
y1 ≤y2.
This can be written more compactly as [z1, z2] := {z | z1 ≤z ≤z2}, where we use the
partial ordering
z ≤w
⇐⇒
ℜz ≤ℜw
and
ℑz ≤ℑw.
Complex interval operations in the inﬁmum-supremum arithmetic are deﬁned in terms of the
real intervals in the same way as the complex operations are deﬁned for complex numbers
z = x+iy. For addition and subtraction the result coincides with the exact range. This is not
the case for complex interval multiplication, where the result is a rectangle in the complex
plane, whereas the actual range is not of this shape. Therefore, for complex intervals,
multiplication will result in an overestimation.
In the complex case the midpoint-radius representation is
⟨c, r⟩:= {z ∈C | |z −c| ≤r},
0 ≤r,
where the midpoint c is now a complex number. This represents a closed circular disk in
the complex plane. The operational deﬁnitions (2.5.10) are still valid, except that some
operations now are complex operations, and that inversion becomes
1/⟨c, r⟩=
.
¯c/(|c|2 −r2), r/(|c|2 −r2)
/
for
|c| > r,
where ¯c is the complex conjugate of c. Complex interval midpoint-radius arithmetic is also
called circular arithmetic.
For complex multiplications it generates less overestimation
than the inﬁmum-supremum notation.
Although the midpoint-radius arithmetic seems more appropriate for complex inter-
vals, most older implementations of interval arithmetic use inﬁmum-supremum arithmetic.
One reason for this is the overestimation also caused for real intervals by the operational
deﬁnitions for midpoint-radius multiplication and division. Rump [307] has shown that
the overestimation is bounded by a factor 1.5 in radius and that midpoint-radius arithmetic
allows for a much faster implementation for modern vector and parallel computers.
2.5.4
Range of Functions
One use of interval arithmetic is to enclose the range of a real-valued function. This can be
used, for example, for localizing and enclosing global minimizers and global minima of a
real function of one or several variables in a region. It can also be used for verifying the
nonexistence of a zero of f (x) in a given interval.
Let f (x) be a real function composed of a ﬁnite number of elementary operations and
standard functions. If one replaces the variable x by an interval [x, x] and evaluates the
resulting interval expression, one gets as the result an interval denoted by f ([x, x]). (It is

2.5. Automatic Control of Accuracy and Veriﬁed Computing
151
assumed that all operations can be carried out.) A fundamental result in interval arithmetic
is that this evaluation is inclusion monotonic, i.e.,
[x, x] ⊆[y, y]
⇒
f ([x, x]) ⊆f ([y, y]).
In particular this means that
x ⊆[x, x] ⇒f (x) ⊆f ([x, x]),
i.e., f ([x]) contains the range of f (x) over the interval [x, x]. A similar result holds also
for functions of several variables f (x1, . . . , xn).
An important case when interval evaluation gives the exact range of a function is
when f (x1, . . . , xn) is a rational expression, where each variable xi occurs only once in the
function.
Example 2.5.3.
In general narrow bounds cannot be guaranteed. For example, if f (x) = x/(1 −x),
then
f ([2, 3]) = [2, 3]/(1 −[2, 3]) = [2, 3]/[−2, −1] = [−3, −1].
The result contains but does not coincide with the exact range [−2, −3/2]. But if we rewrite
the expression as f (x) = 1/(1/x −1), where x only occurs once, then we get
f ([2, 3]) = 1/(1/[2, 3] −1) = 1/[−2/3, −1/2] = [−2, −3/2],
which is the exact range.
When interval analysis is used in a naive manner as a simple technique for simulating
forward error analysis, it does not usually give sharp bounds on the total computational error.
To get useful results the computations in general need to be arranged so that overestimation
is minimized as much as possible. Often a reﬁned design of the algorithm is required in
order to prevent the bounds for the intervals from becoming unacceptably coarse. The
answer [−∞, ∞] is of course always correct but not at all useful!
The remainder term in Taylor expansions includes a variable ξ, which is known to lie
in an interval ξ ∈[a, b]. This makes it suitable to treat the remainder term with interval
arithmetic.
Example 2.5.4.
Evaluate for [x] = [2, 3] the polynomial
p(x) = 1 −x + x2 −x3 + x4 −x5.
Using exact interval arithmetic we ﬁnd
p([2, 3]) = [−252, 49]
(verify this!). This is an overestimate of the exact range, which is [−182, −21]. Rewriting
p(x) in the form p(x) = (1 −x)(1 + x2 + x4) we obtain the correct range. In the ﬁrst

152
Chapter 2. How to Obtain and Estimate Accuracy
example there is a cancellation of errors in the intermediate results (cf. Example 2.5.2),
which is not revealed by the interval calculations.
Sometimes it is desired to compute a tiny interval that is guaranteed to enclose a
real simple root x∗of f (x).
This can be done using an interval version of Newton’s
method. Suppose that the function f (x) is continuously differentiable. Let f ′([x0]) denote
an interval containing f ′(x) for all x in a ﬁnite interval [x] := [a, b]. Deﬁne the Newton
operator N([x]), [x] = [a, b], by
N([x]) := m −f (m)
f ′([x]),
m = mid [x].
(2.5.11)
For the properties of the interval Newton’s method
[xk+1] = N([xk]),
k = 0, 1, 2, . . . ;
see Sec. 6.3.3.
Another important application of interval arithmetic is to initial value problems for
ordinary differential equations
y′ = f (x, y),
y(x0) = y0,
y ∈Rn.
Interval techniques can be used to provide for errors in the initial values, as well as truncation
and rounding errors, so that at each step intervals are computed that contain the actual
solution. But it is a most demanding task to construct an interval algorithm for the initial
value problem, and such algorithms tend to be signiﬁcantly slower than corresponding
point algorithms. One problem is that a wrapping effect occurs at each step and causes the
computed interval widths to grow exponentially. This is illustrated in the following example.
Example 2.5.5.
The recursion formulas
xn+1 = (xn −yn)/
√
2,
yn+1 = (xn + yn)/
√
2
meanaseriesof45-degreerotationsinthexy-plane(seeFigure2.5.1). Byatwo-dimensional
interval one means a rectangle whose sides are parallel to the coordinate axes.
Figure 2.5.1. Wrapping effect in interval analysis.
If the initial value (x0, y0) is given as an interval [x0] = [1−ϵ, 1+ϵ], [y0] = [−ϵ, ϵ]
(see the dashed square, in the leftmost portion of Figure 2.5.1), then (xn, yn) will, with exact

2.5. Automatic Control of Accuracy and Veriﬁed Computing
153
performance of the transformations, also be a square with side 2ϵ, for all n (see the other
squares in Figure 2.5.1). If the computations are made using interval arithmetic, rectangles
with sides parallel to the coordinate axis will, in each step, be circumscribed about the exact
image of the interval one had in the previous step. Thus the interval is multiplied by
√
2 in
each step. After 40 steps, for example, the interval has been multiplied by 220 > 106. This
phenomenon, intrinsic to interval computations, is called the wrapping effect. (Note that
if one uses disks instead of rectangles, there would not be any difﬁculties in this example.)
2.5.5
Interval Matrix Computations
In order to treat multidimensional problems we introduce interval vectors and matrices. An
interval vector is denoted by [x] and has interval components [xi] = [xi, xi]), i = 1 : n.
Likewise an interval matrix [A] = ([aij]) has interval elements
,
aij
-
=
,
aij, aij
-
,
i = 1 : m,
j = 1 : n.
Operations between interval matrices and interval vectors are deﬁned in an obvious manner.
The interval matrix-vector product [A][x] is the smallest interval vector, which contains the
set
{Ax | A ∈[A], x ∈[x]}
but normally does not coincide with it. By the inclusion property it holds that
{Ax | A ∈[A], x ∈[x]} ⊆[A][x] =

n

j=1
,
aij
-,
xj
-

.
In general, there will be an overestimation in enclosing the image with an interval vector,
caused by the fact that the image of an interval vector under a linear transformation in
general is not an interval vector. This phenomenon, intrinsic to interval computations, is
similar to the wrapping effect described in Example 2.5.5.
Example 2.5.6.
We have
A =

1
1
−1
1

,
[x] =

[0, 1]
[0, 1]

⇒
A[x] =

[0, 2]
[−1, 1]

.
Hence b = ( 2
−1 )T ∈A[x], but there is no x ∈[x] such that Ax = b. (The solution to
Ax = b is x = ( 3/2
1/2 )T .)
The magnitude of an interval vector or matrix is interpreted componentwise and
deﬁned by
| [x] | = (| [x1] |, | [x2] |, . . . , | [xn] |)T ,
where the magnitude of the components is deﬁned by
| [a, b] | = max{|x| | x ∈[a, b]}.
(2.5.12)

154
Chapter 2. How to Obtain and Estimate Accuracy
The ∞-norm of an interval vector or matrix is deﬁned as the ∞-norm of its magnitude,
∥[x] ∥∞= ∥| [x] | ∥∞,
∥[A] ∥∞= ∥| [A] | ∥∞.
(2.5.13)
In implementing matrix multiplication it is important to avoid case distinctions in the
inner loops, because that would make it impossible to use fast vector and matrix operations.
Using interval arithmetic it is possible to compute strict enclosures of the product of two
matrices. Note that this is also needed in the case of the product of two point matrices since
rounding errors will usually occur.
We assume that the command
setround(i),
i = −1, 0, 1,
sets the rounding mode to −∞, to nearest, and to +∞, respectively. (Recall that these
rounding modes are supported by the IEEE standard.) Let A and B be point matrices and
suppose we want to compute an interval matrix [C] such that
f l(A · B) ⊂[C] = [Cinf, Csup].
Then the following simple code, using two matrix multiplications, does that:
setround(−1); Cinf = A · B;
setround(1);
Csup = A · B;
We next consider the product of a point matrix A and an interval matrix [B] =
[Binf, Bsup]. The following code performs this using four matrix multiplications:
A−= min(A, 0); A+ = max(A, 0);
setround(−1);
Cinf = A+ · Binf + A−· Bsup;
setround(1);
Csup = A−· Binf + A+ · Bsup;
(Note that the commands A−= min(A, 0) and A+ = max(A, 0) act componentwise.) An
algorithm for computing the product of two interval matrices using eight matrix multipli-
cations is given by Rump [308].
Fast portable codes for interval matrix computations that make use of the Basic Linear
Algebra Subroutines (BLAS) and IEEE 754 standard are now available. This makes it
possible to efﬁciently use high-performance computers for interval computation. INTLAB
(INTerval LABoratory) by Rump [308, 307] is based on MATLAB, and it is particularly
easy to use. It includes many useful subroutines, for example, one to compute an enclosure
of the difference between the solution and an approximate solution xm = Cmid [b]. Veriﬁed
solutions of linear least squares problems can also be computed.
Review Questions
2.5.1 (a) Deﬁne the magnitude and mignitude of an interval I = [a, b].
(b) How is the ∞-norm of an interval vector deﬁned?

Notes and References
155
2.5.2 Describe the two different ways of representing intervals used in real and complex
interval arithmetic. Mention some of the advantages and drawbacks of each of these.
2.5.3 An important property of interval arithmetic is that the operations are inclusion mono-
tonic. Deﬁne this term.
2.5.4 What is meant by the “wrapping effect” in interval arithmetic and what are its impli-
cations? Give some examples of where it occurs.
2.5.5 Assume that the command
setround(i),
i = −1, 0, 1,
sets the rounding mode to −∞, to nearest, and to +∞, respectively. Give a simple
code that, using two matrix multiplications, computes an interval matrix [C] such
that for point matrices A and B
f l(A · B) ⊂[C] = [Cinf, Csup].
Problems and Computer Exercises
2.5.1 Carry out the following calculations in exact interval arithmetic:
(a) [0, 1] + [1, 2]; (b) [3, 3.1] −[0, 0, 2]; (c) [−4. −1] · [−6, 5];
(d) [2, 2] · [−1, 2]; (e) [−1, 1]/[−2, −0.5]; (f) [−3, 2] · [−3.1, 2.1].
2.5.2 Show that using the operational deﬁnitions (2.5.5) the product of the disks ⟨c1, r1⟩
and ⟨c2, r2⟩contains zero if c1 = c2 = 1 and r1 = r2 =
√
2 −1.
2.5.3 (Stoer) Using Horner’s rule and exact interval arithmetic, evaluate the cubic polyno-
mial
p(x) = ((x −3)x + 3)x,
[x] = [0.9, 1.1].
Compare the result with the exact range, which can be determined by observing that
p(x) = (x −1)3 + 1.
2.5.4 Treat Example 1.2.2 using interval analysis and four decimal digits. Starting with the
inclusion interval I10 = [0, 1/60] = [0, 0.01667] generate successively intervals Ik,
k = 9 : −1 : 5, using interval arithmetic and the recursion
In−1 = 1/(5n) −In/5.
Notes and References
Many different aspects of number systems and ﬂoating-point computations are treated in
Knuth [230, Chapter 4], including the historical development of number representation.
Leibniz (1703) seems to have been the ﬁrst to discuss binary arithmetic. He did not advocate
it for practical calculations, but stressed its importance for number-theoretic investigations.
King Charles XII of Sweden came upon the idea of radix 8 arithmetic in 1717. He felt this to

156
Chapter 2. How to Obtain and Estimate Accuracy
be more convenient than the decimal notation and considered introducing octal arithmetic
into Sweden. He died in battle before decreeing such a change.
In the early days of computing, ﬂoating-point computations were not built into the
hardware but implemented in software. The earliest subroutines for ﬂoating-point arithmetic
were probably those developed by J. H. Wilkinson at the National Physical Laboratory, Eng-
land, in 1947. A general source on ﬂoating-point computation is Sterbenz [333]. Goldberg
[158] is an excellent tutorial on the IEEE 754 standard for ﬂoating-point arithmetic deﬁned
in [205]. A self-contained, accessible, and easy to read introduction with many illustrating
examples is the monograph by Overton [280]. An excellent treatment on ﬂoating-point
computation, rounding error analysis, and related topics is given in Higham [199, Chapter
2]. Different aspects of accuracy and reliability are discussed in [108].
The fact that thoughtless use of mathematical formulas and numerical methods can
leadtodisastrousresultsareexempliﬁedbyStegunandAbramowitz[331]andForsythe[122].
Numerousexamplesinwhichincorrectanswersareobtainedfromplausiblenumericalmeth-
ods can be found in Fox [125].
Statistical analysis of rounding errors goes back to an early paper of Goldstine and
von Neumann [161]. Barlow and Bareiss [15] have investigated the distribution of round-
ing errors for different modes of rounding under the assumption that the mantissas of the
operands are from a logarithmic distribution.
Conditioning numbers of general differentiable functions were ﬁrst studied by Rice
[301]. Backward error analysis was developed and popularized by J. H. Wilkinson in the
1950s and 1960s, and the classic treatise on rounding error analysis is [376]. The more
recent survey [380] gives a good summary and a historical background. Kahan [216] gives
an in-depth discussion of rounding error analysis with examples of how ﬂaws in the design
of hardware and software in computer systems can have undesirable effects on accuracy.
The normwise analysis is natural for studying the effect of orthogonal transformations in
matrix computations; see Wilkinson [376]. The componentwise approach, used in pertur-
bation analysis for linear systems by Bauer [19], can give sharper results and has gained in
popularity.
Condition numbers are often viewed pragmatically as the coefﬁcients of the back-
ward errors in bounds on forward errors. Wilkinson in [376] avoids a precise deﬁnition of
condition numbers in order to use them more freely. The more precise limsup deﬁnition in
Deﬁnition 2.4.6 is usually attributed to Rice [301].
Even in the special literature, the discussion of planning of experimental perturbations
is surprisingly meager. An exception is the collection of software tools called PRECISE,
developed by Chaitin-Chatelin et al.; see [63, 64]. These are designed to help the user set
up computer experiments to explore the impact of the quality of convergence of numerical
methods. PRECISE involves a statistical analysis of the effect on a computed solution of
random perturbations in data.

Chapter 3
Series, Operators, and
Continued Fractions
No method of solving a computational problem
is really available to a user until it is completely
described in an algebraic computer language
and made completely reliable.
—George E. Forsythe
3.1
Some Basic Facts about Series
3.1.1
Introduction
Series expansions are a very important aid in numerical calculations, especially for quick
estimates made in hand calculation—for example, in evaluating functions, integrals, or
derivatives. Solutions to differential equations can often be expressed in terms of series
expansions. Since the advent of computers it has, however, become more common to
treat differential equations directly using, for example, ﬁnite difference or ﬁnite element
approximations instead of series expansions. Series have some advantages, especially in
problems containing parameters. As well, automatic methods for formula manipulation and
some new numerical methods provide new possibilities for series.
In this section we will discuss general questions concerning the use of inﬁnite series
for numerical computations including, for example, the estimation of remainders, power
series, and various algorithms for computing their coefﬁcients. Often a series expansion can
be derived by simple operations with a known series. We also give an introduction to formal
power series. The next section treats perturbation expansions, ill-conditioned expansions,
and semiconvergent expansions, from the point of view of computing.
Methods and results will sometimes be formulated in terms of series, sometimes in
terms of sequences. These formulations are equivalent, since the sum of an inﬁnite series
is deﬁned as the limit of the sequence {Sn} of its partial sums
Sn = a1 + a2 + · · · + an.
157

158
Chapter 3. Series, Operators, and Continued Fractions
Conversely, any sequence S1, S2, S3, . . . can be written as the partial sums of a series,
S1 + (S2 −S1) + (S3 −S2) + · · · .
In practice, one is seldom seriously concerned about a rigorous error bound when the
computed terms decrease rapidly, and it is “obvious” that the terms will continue to decrease
equally quickly. One can then break off the series and use either the last included term or a
coarse estimate of the ﬁrst neglected term as an estimate of the remainder.
This rule is not very precise. How rapidly is “rapidly”? Questions like this occur
everywhere in scientiﬁc computing. If mathematical rigor costs little effort or little extra
computing time, then it should, of course, be used. Often, however, an error bound that is
both rigorous and realistic may cost more than what is felt reasonable for (say) a one-off
problem.
In problems where guaranteed error bounds are not asked for, when it is enough to
obtain a feeling for the reliability of the results, one can handle these matters in the same
spirit as one handles risks in everyday life. It is then a matter of experience to formulate a
simple and sufﬁciently reliable termination criterion based on the automatic inspection of
the successive terms.41
The inexperienced scientiﬁc programmer may, however, ﬁnd such questions hard,
even in simple cases. In the production of general purpose mathematical software, or in a
context where an inaccurate numerical result can cause a disaster, such questions are serious
and sometimes hard for the experienced scientiﬁc programmer also. For this reason, we shall
formulate a few theorems with which one can often transform the feeling that “the remainder
is negligible” to a mathematical proof. There are, in addition, actually numerically useful
divergent series; see Sec. 3.2.6. When one uses such series, estimates of the remainder are
clearly essential.
Assume that we want to compute a quantity S, which can be expressed in a series
expansion, S = ∞
j=0 aj, and set
Sn = n
j=0 aj,
Rn = S −Sn.
We call ∞
j=n+1 aj the tail of the series; an is the “last included term” and an+1 is the “ﬁrst
neglected term.” The remainder Rn with reversed sign is called the truncation error.42
The tail of a convergent series can often be compared to a series with a known sum,
such as a geometric series, or with an integral which can be computed directly.
Theorem 3.1.1 (Comparison with a Geometric Series).
If |aj+1| ≤k|aj| for all j ≥n, where k < 1, then
|Rn| ≤|an+1|
1 −k ≤k|an|
1 −k .
In particular if k < 1/2, then it is true that the absolute value of the remainder is less than
the last included term.
41Termination criteria for iterative methods will be discussed in Sec. 6.1.3.
42In this terminology the remainder is the correction one has to make in order to eliminate the error.

3.1. Some Basic Facts about Series
159
Proof. By induction, one ﬁnds that |aj| ≤kj−1−n|an+1|, j ≥n + 1, since
|aj| ≤kj−1−n|an+1| ⇒|aj+1| ≤k|aj| ≤kj−n|an+1|.
Thus
|Rn| ≤
∞

j=n+1
|aj| ≤
∞

j=n+1
kj−1−n|an+1| = |an+1|
1 −k ≤k|an|
1 −k ,
according to the formula for the sum of an inﬁnite geometric series. The last statement
follows from the inequality k/(1 −k) < 1, when k < 1/2.
Example 3.1.1.
In a power series with slowly varying coefﬁcients, aj = j 1/2π−2j. Then a6 <
2.45·0.0000011 < 3·10−6, and
|aj+1|
|aj|
≤(j + 1)1/2
j 1/2
π−2j−2
π−2j
≤

1 + 1
6
1/2
π−2 < 0.11
for j ≥6. Thus, by Theorem 3.1.1, |R6| < 3·10−6
0.11
1−0.11 < 4·10−7.
Theorem 3.1.2 (Comparison of a Series with an Integral).
If |aj| ≤f (j) for all j ≥n + 1, where f (x) is a nonincreasing function for x ≥n,
then
|Rn| ≤
∞

j=n+1
|aj| ≤
 ∞
n
f (x) dx,
which yields an upper bound for |Rn|, if the integral is ﬁnite.
If aj+1 ≥g(j) > 0 for all j ≥n, we also obtain a lower bound for the error, namely
Rn =
∞

j=n+1
aj >
 ∞
n
g(x) dx.
Proof. See Figure 3.1.1.
Example 3.1.2.
When aj is slowly decreasing, the two error bounds are typically rather close to each
other; hence, they are rather realistic bounds, much larger than the ﬁrst neglected term an+1.
Let aj = 1/(j 3 + 1), f (x) = x−3. It follows that
0 < Rn ≤
 ∞
n
x−3 dx = n−2/2.
In addition this bound gives an asymptotically correct estimate of the remainder, as n →∞,
which shows that Rn is here signiﬁcantly larger than the ﬁrst neglected term.
For alternating series the situation is typically quite different.

160
Chapter 3. Series, Operators, and Continued Fractions
5
6
7
8
9
10
11
12
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
an+1
an+2
an+3
an+4
an+5
y = f(x)
y = g(x)
Figure 3.1.1. Comparison of a series with an integral, (n = 5).
Deﬁnition 3.1.3.
A series is alternating for j ≥n if, for all j ≥n, aj and aj+1 have opposite signs or,
equivalently, sign ajsign aj+1 ≤0, where sign x (read “signum” of x) is deﬁned by
sign x =
 +1
if x > 0,
0
if x = 0,
−1
if x < 0.
Theorem 3.1.4.
If Rn and Rn+1 have opposite signs, then S lies between Sn and Sn+1. Furthermore
S = 1
2(Sn + Sn+1) ± 1
2|an+1|.
We also have the weaker results:
|Rn| ≤|an+1|,
|Rn+1| ≤|an+1|,
sign Rn = sign an+1.
This theorem has nontrivial applications to practically important divergent sequences;
see Sec. 3.2.6.
Proof. The fact that Rn+1 and Rn have opposite signs means, quite simply, that one of
Sn+1 and Sn is too large and the other is too small, i.e., S lies between Sn+1 and Sn. Since
an+1 = Sn+1 −Sn, one has for positive values of an+1 the situation shown in Figure 3.1.2.
From this ﬁgure, and an analogous one for the case of an+1 < 0, the remaining assertions
of the theorem clearly follow.
The actual error of the average 1
2(Sn + Sn+1) is, for slowly convergent alternating
series, usually much smaller than the error bound 1
2|an+1|. For example, if Sn = 1 −1
2 +

3.1. Some Basic Facts about Series
161
✲
✲
✲
Sn
S
Sn+1
Rn
−Rn+1
an+1
Figure 3.1.2. A series where Rn and Rn+1 have different signs.
1
3 −· · · ± 1
n, lim Sn = ln 2 ≈0.6931, the error bound for n = 4 is 0.1, while the actual
error is less than 0.01. A systematic exploration of this observation, by means of repeated
averaging, is carried out in Sec. 3.4.3.
Theorem 3.1.5.
For an alternating series, the absolute values of whose terms approach zero mono-
tonically, the remainder has the same sign as the ﬁrst neglected term an+1, and the absolute
value of the remainder does not exceed |an+1|. (It is well known that such a series is
convergent.)
Proof. Sketch: That the theorem is true is almost clear from Figure 3.1.3. The ﬁgure shows
how Sj depends on j when the premises of the theorem are fulﬁlled. A formal proof is left
to the reader.
0
2
4
6
8
10
12
−0.5
0
0.5
1
1.5
Figure 3.1.3. Successive sums of an alternating series.
The use of this theorem will be illustrated in Example 3.1.3. An important general-
ization is given as Problem 3.3.2 (g).
In the preceding theorems the ideas of well-known convergence criteria are extended
to bounds or estimates of the error of a truncated expansion. In Sec. 3.4, we shall see a further
extension of these ideas, namely for improving the accuracy obtained from a sequence of
truncated expansions. This is known as convergence acceleration.

162
Chapter 3. Series, Operators, and Continued Fractions
3.1.2
Taylor’s Formula and Power Series
Consider an expansion into powers of a complex variable z, and suppose that it is convergent
for some z ̸= 0, and denote its sum by f (z),
f (z) =
∞

j=0
ajzj,
z ∈C.
(3.1.1)
It is then known from complex analysis that the series (3.1.1) either converges for all z,
or it has a circle of convergence with radius ρ such that it converges for all |z| < ρ, and
diverges for |z| > ρ. (For |z| = ρ convergence or divergence is possible.) The radius of
convergence is determined by the relation
ρ = lim sup |an|−1/n.
(3.1.2)
Another formula is ρ = lim |an|/|an+1|, if this limit exists.
The function f (z) can be expanded into powers of z – a around any point of analyticity,
f (z) =
∞

j=0
aj(z −a)j,
z ∈C.
(3.1.3)
By Taylor’s formula the coefﬁcients are given by
a0 = f (a),
aj = f (j)(a)/j!,
j ≥1.
(3.1.4)
In the general case this inﬁnite series is called a Taylor series; in the special case, a = 0, it
is by tradition called a Maclaurin series.43
The function f (z) is analytic inside its circle of convergence and has at least one
singular point on its boundary. The singularity of f , which is closest to the origin, can often
be found easily from the expression that deﬁnes f (z); thus the radius of convergence of a
Maclaurin series can often be easily found.
Note that these Taylor coefﬁcients are uniquely determined for the function f . This
is true also for a nonanalytic function, for example, if f ∈Cp[a, b], although in this case
the coefﬁcient aj exists only for j ≤p. In Figure 3.1.4 the partial sums of the Maclaurin
expansions for the functions f (x) = cos x and f (x) = 1/(1 + x2) are shown. The series
for cos x converges for all x, but rounding errors cause trouble for large values of x; see
Sec. 3.2.5. For 1/(1 + x2) the radius of convergence is 1.
There are several expressions for the remainder Rn(z) when the expansion for f (z) is
truncated after the term that contains zn−1. In order to simplify the notation, we put a = 0
and consider the Maclaurin series. The following integral form can be obtained by the
application of repeated integration by parts to the integral z
 1
0 f ′(zt) dt:
Rn(z) = zn
 1
0
(1 −t)n−1
(n −1)! f (n)(zt) dt;
(3.1.5)
43Brook Taylor (1685–1731), who announced his theorem in 1712, and Colin Maclaurin (1698–1746), were
British mathematicians.

3.1. Some Basic Facts about Series
163
0
1
2
3
4
5
6
7
8
9
10
−2
−1
0
1
2
26
22
18
14
10
6
2
4
8
12
16
20
24
0
0.5
1
1.5
0
0.5
1
1.5
2
2
6
4
8
Figure 3.1.4. Partial sums of the Maclaurin expansions for two functions. The
upper curves are for cos x, n = 0 : 2 : 26, 0 ≤x ≤10. The lower curves are for 1/(1+x2),
n = 0 : 2 : 18, 0 ≤x ≤1.5.
the details are left for Problem 3.2.10 (b). From this follows the upper bound
|Rn(z)| ≤1
n!|z|n max
0≤t≤1 |f (n)(zt)|.
(3.1.6)
This holds also in the complex case: if f is analytic on the segment from 0 to z, one
integrates along this segment, i.e., for 0 ≤t ≤1; otherwise another path is to be chosen.
The remainder formulas (3.1.5), (3.1.6) require only that f ∈Cn. It is thus not necessary
that the inﬁnite expansion converges or even exists.
For a real-valued function, Lagrange’s44 formula for the remainder term
Rn(x) = f (n)(ξ)xn
n!
,
ξ ∈[0, x],
(3.1.7)
is obtained by the mean value theorem of integral calculus. For complex-valued functions
and, more generally, for vector-valued functions, the mean value theorem and Lagrange’s
remainder term are not valid with a single ξ. (Sometimes componentwise application with
different ξ is possible.) A different form (3.2.11) for the remainder, valid in the complex
44Joseph Louis Lagrange (1736–1813) was born in Turin, Italy. When Euler returned from Berlin to St. Pe-
tersburg in 1766, Lagrange accepted a position in the Berlin Academy. He stayed there until 1787, when he
moved to Paris, where he remained until his death. Lagrange made fundamental contributions to most branches
of mathematics and mechanics.

164
Chapter 3. Series, Operators, and Continued Fractions
plane, is given in Sec. 3.2.2 in terms of the maximum modulus M(r) = max|z|=r |f (z)|,
which may sometimes be easier to estimate than the nth derivative. A power series is
uniformly convergent in any closed bounded region strictly inside its circle of convergence.
Roughly speaking, the series can be manipulated like a polynomial, as long as z belongs to
such a region:
• It can be integrated or differentiated term by term.
• Substitutions can be performed, and terms can be rearranged.
A power series can also be multiplied by another power series, as shown in the next
theorem.
Theorem 3.1.6 (Cauchy Product).
If f (z) = ∞
i=0 aizi and g(z) = ∞
j=0 bjzj, then f (z)g(z) = ∞
n=0 cnzn, where
cn = a0bn + a1bn−1 + · · · + anb0 =
n

i=0
aibn−i.
(3.1.8)
The expression on the right side of (3.1.8) is called the convolution or the Cauchy product
of the coefﬁcient sequences of f and g.
Example 3.1.3.
Many important functions in applied mathematics cannot be expressed in ﬁnite terms
of elementary functions and must be approximated by numerical methods. One such func-
tion is the error function deﬁned by
erf(x) =
2
√π
 x
0
e−t2 dt.
(3.1.9)
This function is encountered, for example, in computing the distribution function of a normal
deviate. It takes the values erf(0) = 0, erf(∞) = 1 and is related to the incomplete gamma
functions (see the Handbook [1, Sec. 6.5]) by erf(x) = γ (1/2, x2).
Suppose one wishes to compute erf(x) for x ∈[−1, 1] with a relative error less than
10−10. One can then approximate the function by a power series. Setting z = −t2 in the
well-known Maclaurin series for ez, truncating after n + 1 terms, and integrating term by
term we obtain
erf(x) ≈
2
√π
 x
0
n

j=0
(−1)j t2j
j! dt =
2
√π
n

j=0
ajx2j+1 =: p2n+1(x),
(3.1.10)
where
a0 = 1,
aj =
(−1)j
j!(2j + 1).
(Note that erf(x) is an odd function of x.) This series converges for all x, but is suitable for
numerical computations only for values of x which are not too large. To evaluate the series

3.1. Some Basic Facts about Series
165
we note that the coefﬁcients aj satisfy the recurrence relation
aj = −aj−1
(2j −1)
j(2j + 1).
This recursion shows that for x ∈[0, 1] the absolute values of the terms tj = ajx2j+1
decrease monotonically. By Theorem 3.1.5 this implies that the absolute error in a partial
sum is bounded by the absolute value of the ﬁrst neglected term anxn.
Apossible algorithm for evaluating the sum in (3.1.10) is as follows: Set s0 = t0 = x;
for j = 1, 2, . . . , compute
tj = −tj−1
(2j −1)
j(2j + 1)x2,
sj = sj−1 + tj
(3.1.11)
until |tj| ≤10−10sj. Here we have estimated the error by the last term added in the series.
Since we have to compute this term for the error estimate we might as well use it! Note
also that in this case, where the number of terms is not ﬁxed in advance, Horner’s rule is
not suitable for the evaluation. Figure 3.1.5 shows the graph of the relative error in the
computed approximation p2n+1(x). At most 12 terms in the series were needed.
0
0.2
0.4
0.6
0.8
1
10
−18
10
−17
10
−16
10
−15
10
−14
10
−13
10
−12
10
−11
x
error
Figure 3.1.5. Relative error in approximations of the error function by a Maclaurin
series truncated after the ﬁrst term that satisﬁes the condition in (3.1.11).
The use of the Taylor coefﬁcient formula and Lagrange’s form of the remainder may
be inconvenient, and it is often easier to obtain an expansion by manipulating some known
expansions. The geometric series
1
1 −z = 1 + z + z2 + z3 + · · · + zn−1 +
zn
1 −z,
z ̸= 1,
(3.1.12)

166
Chapter 3. Series, Operators, and Continued Fractions
is of particular importance; note that the remainder zn/(1 −z) is valid even when the
expansion is divergent.
Example 3.1.4.
Set z = −t2 in the geometric series, and integrate:
 x
0
(1 + t2)−1 dt =
n−1

j=0
 x
0
(−t2)j dt +
 x
0
(−t2)n(1 + t2)−1 dt.
Using the mean value theorem of integral calculus on the last term we get
arctan x =
n−1

j=0
(−1)jx2j+1
2j + 1
+ (1 + ξ 2)−1(−1)nx2n+1
2n + 1
(3.1.13)
for some ξ ∈int[0, x]. Both the remainder term and the actual derivation are much simpler
than what one would get by using Taylor’s formula with Lagrange’s remainder term. Note
also that Theorem 3.1.4 is applicable to the series obtained above for all x and n, even for
|x| > 1, when the inﬁnite power series is divergent.
Some useful expansions are collected in Table 3.1.1. These formulas will be used
often without a reference; the reader is advised to memorize the expansions. “Remainder
ratio” denotes the ratio of the remainder to the ﬁrst neglected term, if x ∈R; ξ means a
number between 0 and x. Otherwise these expansions are valid in the unit circle of C or in
the whole of C.
The binomial coefﬁcients are, also for noninteger k, deﬁned by
k
n

= k(k −1) · · · (k −n + 1)
1 · 2 · · · n
.
For example, setting k = 1/2 gives
(1 + x)1/2 = 1 + x
2 −x2
8 + x3
16 −· · ·
if |x| < 1.
Depending on the context, the binomial coefﬁcients may be computed by one of the follow-
ing well-known recurrences:

k
n + 1

=
k
n
(k −n)
(n + 1)
or
k + 1
n

=
k
n

+

k
n −1

,
(3.1.14)
with appropriate initial conditions. The latter recurrence follows from the matching of the
coefﬁcients of tn in the equation (1+t)k+1 = (1+t)(1+t)k. (Compare the Pascal triangle;
see Problem 1.2.4.) The explicit formula
k
n

= k!/(n!(k −n)!), for integers k, n, is to be
avoided if k can become large, because k! has overﬂow for k > 170 even in IEEE double
precision arithmetic.
The exponent k in (1+x)k is not necessarily an integer; it can even be an irrational or
a complex number. This function may be deﬁned as (1 + x)k = ek ln(1+x). Since ln(1 + x)

3.1. Some Basic Facts about Series
167
Table 3.1.1. Maclaurin expansions for some elementary functions.
Function
Expansion (x ∈C)
Remainder ratio (x ∈R)
(1 −x)−1
1 + x + x2 + x3 + · · · if |x| < 1
(1 −x)−1 if x ̸= 1
(1 + x)k
1 + kx +
k
2

x2 + · · · if |x| < 1
(1 + ξ)k−n if x > −1
ln(1 + x)
x −x2
2 + x3
3 −x4
4 + · · · if |x| < 1
(1 + ξ)−1 if x > −1
ex
1 + x + x2
2! + x3
3! + · · · all x
eξ, all x
sin x
x −x3
3! + x5
5! −x7
7! + · · · all x
cos ξ, all x, n odd
cos x
1 −x2
2! + x4
4! −x6
6! + · · · all x
cos ξ, all x, n even
1
2 ln
1 + x
1 −x

x + x3
3 + x5
5 + · · · if |x| < 1
1
1 −ξ 2 , |x| < 1, n even
arctan x
x −x3
3 + x5
5 + · · · if |x| < 1
1
1 + ξ 2 , all x
is multivalued, (1 + x)k is multivalued too, unless k is an integer. We can, however, make
them single-valued by forbidding the complex variable x to take real values less than −1.
In other words, we make a cut along the real axis from −1 to −∞that the complex variable
must not cross. (The cut is outside the circle of convergence.) We obtain the principal
branch by requiring that ln(1 + x) > 0 if x > 0. Let 1 + x = reiφ, r > 0, φ →±π. Note
that
1 + x →−r,
ln(1 + x) →ln r +
%
+iπ
if φ →π,
−iπ
if φ →−π.
(3.1.15)
Two important power series, not given in Table 3.1.1, are the following.
The Gauss hypergeometric function45
F(a, b, c; z) = 1 + ab
c
z
1! + a(a + 1)b(b + 1)
c(c + 1)
z2
2!
+ a(a + 1)(a + 2)b(b + 1)(b + 2)
c(c + 1)(c + 2)
z3
3! + . . . ,
(3.1.16)
where a and b are complex constants and c ̸= −1, −2, −3, . . . . The radius of convergence
for this series equals unity; see [1, Chap. 15].
45Gauss presented his paper on this series in 1812.

168
Chapter 3. Series, Operators, and Continued Fractions
Kummer’s conﬂuent hypergeometric function46
M(a, b; z) = 1 + a
b
z
1! + a(a + 1)
b(b + 1)
z2
2! + a(a + 1)(a + 2)
b(b + 1)(b + 2)
z3
3! + . . . ,
(3.1.17)
converges for all z (see [1, Chap. 13]). It is named “conﬂuent” because
M(a, c; z) = lim
b→∞F(a, b, c, z/b).
The coefﬁcients of these series are easily computed and the functions are easily eval-
uated by recurrence relations. (You also need some criterion for the truncation of the series,
adapted to your demands of accuracy.) In Sec. 3.5, these functions are also expressed in
terms of inﬁnite continued fractions that typically converge faster and in larger regions than
the power series do.
Example 3.1.5.
The following procedure can generally be used in order to ﬁnd the expansion of the
quotient of two expansions. We illustrate it in a case where the result is of interest to us
later.
The Bernoulli47 numbers Bn are deﬁned by the Maclaurin series
x
ex −1 ≡
∞

j=0
Bjxj
j! .
(3.1.18)
For x = 0 the left-hand side is deﬁned by l’Hôpital’s rule; the value is 1. If we multiply
this equation by the denominator, we obtain
x ≡
 ∞

i=1
xi
i!
 ∞

j=0
Bjxj
j!

.
By matching the coefﬁcients of xn, n ≥1, on both sides, we obtain a recurrence relation
for the Bernoulli numbers, which can be written in the form
B0 = 1,
n−1

j=0
1
(n −j)!
Bj
j! = 0,
n ≥2,
i.e.,
n−1

j=0
n
j

Bj = 0.
(3.1.19)
The last equation is a recurrence that determines Bn−1 in terms of Bernoulli numbers with
smaller subscripts; hence B0 = 1, B1 = −1
2, B2 =
1
6, B3 = 0, B4 = −1
30, B5 = 0,
B6 =
1
42, . . . .
46Ernst Eduard Kummer (1810–1893), a German mathematician, was a professor in Berlin from 1855 to his
death. He extended Gauss’s work on hypergeometric series. Together with Weierstrass and Kronecker, he made
Berlin into one of the leading centers of mathematics at that time.
47Jacob (or James) Bernoulli (1654–1705), a Swiss mathematician, was one of the earliest to realize the power
of inﬁnitesimal calculus. The Bernoulli numbers were published posthumously in 1713, in his fundamental work
Ars Conjectandi (on probability). The notation for Bernoulli numbers varies in the literature. Our notation seems
to be the most common in modern texts. Several members of the Bernoulli family enriched mathematics by their
teaching and writing. Their role in the history of mathematics resembles the role of the Bach family in the history
of music.

3.1. Some Basic Facts about Series
169
We see that the Bernoulli numbers are rational. We shall now demonstrate that Bn = 0,
when n is odd, except for n = 1:
x
ex −1 + x
2 = x
2
ex + 1
ex −1 = x
2
ex/2 + e−x/2
ex/2 −e−x/2 =
∞

n=0
B2nx2n
(2n)! .
(3.1.20)
Since the next-to-last term is an even function its Maclaurin expansion contains only even
powers of x, and hence the last expansion is also true.
The recurrence obtained for the Bernoulli numbers by the matching of coefﬁcients in
the equation
(ex/2 −e−x/2)
 ∞

n=0
B2nx2n/(2n)!

= 1
2x

ex/2 + e−x/2
is not the same as the one we found above. It turns out to have better properties of numerical
stability. We shall look into this experimentally in Problem 3.1.10 (g).
Thesingularitiesofthefunctionx/(ex−1)arepolesatx = 2nπi, n = ±1, ±2, ±3, . . . ;
hence the radius of convergence is 2π. Further properties of Bernoulli numbers and the
related Bernoulli polynomials and periodic functions are presented in Sec. 3.4.5, where they
occur as coefﬁcients in the important Euler–Maclaurin formula.
If r is large, the following formula is very efﬁcient; the series on its right-hand side
then converges rapidly:
B2r/(2r)! = (−1)r−12(2π)−2r

1 +
∞

n=2
n−2r

.
(3.1.21)
This is a particular case (t = 0) of a Fourier series for the Bernoulli functions that we
shall encounter in Lemma 3.4.9(c). In fact, you obtain IEEE double precision accuracy for
r > 26, even if the inﬁnite sum on the right-hand side is totally ignored. Thanks to (3.1.21)
we do not need to worry much over the instability of the recurrences. When r is very large,
however, we must be careful about underﬂow and overﬂow.
The Euler numbers En, which will be used later, are similarly deﬁned by the gener-
ating function
1
cosh z ≡
∞

n=0
Enzn
n! ,
|z| < π
2 .
(3.1.22)
Obviously En = 0 for all odd n. It can be shown that the Euler numbers are integers,
E0 = 1, E2 = −1, E4 = 5, E6 = −61; see Problem 3.1.7(c).
Example 3.1.6.
Let f (x) = (x3 + 1)−1
2 . Compute
 ∞
10 f (x) dx to nine decimal places, and f ′′′(10),
with at most 1% error. Since x−1 is fairly small, we expand in powers of x−1:
f (x) = x−3/2(1 + x−3)−1/2 = x−3/2
1 −1
2x−3 + 1·3
8 x−6 −· · ·

= x−1.5 −1
2x−4.5 + 3
8x−7.5 −· · · .

170
Chapter 3. Series, Operators, and Continued Fractions
By integration,
 ∞
10
f (x) dx = 2·10−0.5 −1
710−3.5 + 3
5210−6.5 + · · · = 0.632410375.
Each term is less than 0.001 of the previous term.
By differentiating the series three times, we similarly obtain
f ′′′(x) = −105
8 x−4.5 + 1 287
16 x−7.5 + · · · .
For x = 10 the second term is less than 1% of the ﬁrst; the terms after the second decrease
quickly and are negligible. One can show that the magnitude of each term is less than 8 x−3
of the previous term. We get f ′′′(10) = −4.12 ·10−4 to the desired accuracy. The reader is
advised to carry through the calculation in more detail.
Example 3.1.7.
One wishes to compute the exponential function ex with full accuracy in IEEE double
precision arithmetic (unit roundoff u = 2−53 ≈1.1·10−16). The method of scaling and
squaring is based on the following idea. If we let m ≥1 be an integer and set y = x/2m,
then
ex = (ey)2m.
Here the right-hand side can be computed by squaring ey m times. By choosing m large
enough, ey can be computed by a truncated Taylor expansion with k terms; see Sec. 3.1.2.
The integers m and k should be chosen so that the bound
1
k!yk ≤1
k!
log 2
2m
k
for the truncation error, multiplied by 2m to take into account the propagation of error due
to squaring ex∗, is bounded by the unit roundoff u. Subject to this constraint, m and k
are determined to minimize the computing time. If the Taylor expansion is evaluated by
Horner’s rule this is approximately proportional to (m + 2k). In IEEE double precision
arithmetic with u = 2−53 we ﬁnd that (k, m) = (7, 7) and (8, 5) are good choices. Note
that to keep the rounding error sufﬁciently small, part of the computations must be done in
extended precision.
We remark that rational approximations often give much better accuracy than poly-
nomial approximations. This is related to the fact that continued fraction expansions often
converge much faster than those based on power series; see Sec. 3.5.3, where Padé approx-
imations for the exponential function are given.
In numerical computation a series should be regarded as a ﬁnite expansion together
with a remainder. Taylor’s formula with the remainder (3.1.5) is valid for any function
f ∈Cn[a, a + x], but the inﬁnite series is valid only if the function is analytic in a complex
neighborhood of a.
If a function is not analytic at zero, it can happen that the Maclaurin expansion con-
verges to a wrong result. Aclassical example (see theAppendix to Chapter 6 in Courant [82])

3.1. Some Basic Facts about Series
171
is
f (x) =
%
e−1/x2
if x ̸= 0,
0
if x = 0.
It can be shown that all its Maclaurin coefﬁcients are zero. This trivial Maclaurin expansion
converges for all x, but the sum is wrong for x ̸= 0. There is nothing wrong with the use
of Maclaurin’s formula as a ﬁnite expansion with a remainder. Although the remainder that
in this case equals f (x) itself does not tend to 0 as n →∞for a ﬁxed x ̸= 0, it tends to 0
faster than any power of x, as x →0, for any ﬁxed n. The “expansion” gives, for example,
an absolute error less than 10−43 for x = 0.1, but the relative error is 100%. Also note that
this function can be added to any function without changing its Maclaurin expansion.
From the point of view of complex analysis, however, the origin is a singular point for
thisfunction. Notethat|f (z)| →∞asz →0alongtheimaginaryaxis, andthispreventsthe
application of any theorem that would guarantee that the inﬁnite Maclaurin series represents
the function. This trouble does not occur for a truncated Maclaurin expansion around a
point, where the function under consideration is analytic. The size of the ﬁrst nonvanishing
neglected term then gives a good hint about the truncation error, when |z| is a small fraction
of the radius of convergence.
The above example may sound like a purely theoretical matter of curiosity. We
emphasize this distinction between the convergence and the validity of an inﬁnite expansion
in this text as a background to other expansions of importance in numerical computation,
such as the Euler–Maclaurin expansion in Sec. 3.4.5, which may converge to the wrong
result, and in the application to a well-behaved analytic function. On the other hand, we
shall see in Sec. 3.2.6 that divergent expansions can sometimes be very useful. The universal
recipe in numerical computation is to consider an inﬁnite series as a ﬁnite expansion plus a
remainder term. But a more algebraic point of view on a series is often useful in the design
of a numerical method; see Sec. 3.1.5 (Formal Power Series) and Sec. 3.3.2 (The Calculus of
Operators). Convergence of an expansion is neither necessary nor sufﬁcient for its success
in practical computation.
3.1.3
Analytic Continuation
Analytic functions have many important properties that you may ﬁnd in any text on complex
analysis. A good summary for the purpose of numerical mathematics is found in the ﬁrst
chapter of Stenger [332]. Two important properties are contained in the following lemma.
We remark that the region of analyticity of a function f (z) is an open set. If we say
that f (z) is analytic on a closed real interval, it means that there exists an open set in C that
contains this interval, where f (z) is analytic.
Lemma 3.1.7.
An analytic function can only have a ﬁnite number of zeros in a compact subset of the
region of analyticity, unless the function is identically zero.
Suppose that two functions f1 and f2 are analytic in regions D1 and D2, respectively.
Suppose that D1 ∩D2 contains an interval throughout which f1(z) = f2(z). Then f1(z) =
f2(z) in the intersection D1 ∩D2.

172
Chapter 3. Series, Operators, and Continued Fractions
Proof. We refer, for the ﬁrst part, to any text on complex analysis. Here we closely follow
Titchmarsh [351]. The second part follows by the application of the ﬁrst part to the function
f1 −f2.
A consequence of this is known as the permanence of functional equations. That is,
in order to prove the validity of a functional equation (or “a formula for a function”) in a
region of the complex plane, it may be sufﬁcient to prove its validity in (say) an interval of
the real axis, under the conditions speciﬁed in the lemma.
Example 3.1.8 (The Permanence of Functional Equations).
We know from elementary real analysis that the functional equation
e(p+q)z = epzeqz,
(p, q ∈R),
holds for all z ∈R. We also know that all three functions involved are analytic for all z ∈C.
Set D1 = D2 = C in the lemma, and let “the interval” be any compact interval of R. The
lemma then tells us that the displayed equation holds for all complex z.
The right- and left-hand sides then have identical power series. Applying the convo-
lution formula and matching the coefﬁcients of zn, we obtain
(p + q)n
n!
=
n

j=0
pj
j!
qn−j
(n −j)!,
i.e.,
(p + q)n =
n

j=0
n!
j!(n −j)!pjqn−j.
This is not a very sensational result. It is more interesting to start from the following
functional equation:
(1 + z)p+q = (1 + z)p(1 + z)q.
The same argumentation holds, except that—by the discussion around Table 3.1.1—D1, D2
should be equal to the complex plane with a cut from −1 to −∞, and that the Maclaurin
series is convergent in the unit disk only. We obtain the equations
p + q
n

=
n

j=0
p
j

q
n −j

,
n = 0, 1, 2, . . . .
(3.1.23)
(They can also be proved by induction, but it is not needed.) This sequence of algebraic
identities, where each identity contains a ﬁnite number of terms, is equivalent to the above
functional equation.
We shall see that this observation is useful for motivating certain “symbolic compu-
tations” with power series, which can provide elegant derivations of useful formulas in
numerical mathematics.
Now we may consider the aggregate of values of f1(z) and f2(z) at points interior
to D1 or D2 as a single analytic function f . Thus f is analytic in the union D1 ∪D2, and
f (z) = f1(z) in D1, f (z) = f2(z) in D2.
The function f2 may be considered as extending the domain in which f1 is deﬁned,
and it is called a (single-valued) analytic continuation of f1. In the same way, f1 is an

3.1. Some Basic Facts about Series
173
analytic continuation of f2. Analytic continuation denotes both this process of extending
the deﬁnition of a given function and the result of the process. We shall see examples of
this, e.g., in Sec. 3.1.4. Under certain conditions the analytic continuation is unique.
Theorem 3.1.8.
Suppose that a region D is overlapped by regions D1, D2, and that (D1 ∩D2) ∩D
contains an interval. Let f be analytic in D, let f1 be an analytic continuation of f to D1,
and let f2 be an analytic continuation of f to D2 so that
f (z) = f1(z) = f2(z)
in
(D1 ∩D2) ∩D.
Then either of these functions provides a single-valued analytic continuation of f to D1∩D2.
The results of the two processes are the same.
Proof. Since f1 −f2 is analytic in D1 ∩D2, and f1 −f2 = 0 in the set (D1 ∩D2) ∩D,
which contains an interval, it follows from Lemma 3.1.7 that f1(z) = f2(z) in D1 ∩D2,
which proves the theorem.
If the set (D1 ∩D2) ∩D is void, the conclusion in the theorem may not be valid. We
may still consider the aggregate of values as a single analytic function, but this function can
be multivalued in D1 ∩D2.
Example 3.1.9.
For |x| < 1 the important formula
arctan x = 1
2i ln
1 + ix
1 −ix

easily follows from the expansions in Table 3.1.1. The function arctan x has an analytic
continuation as single-valued functions in the complex plane with cuts along the imaginary
axis from i to ∞and from −i to −∞. It follows from the theorem that “the important
formula” is valid in this set.
3.1.4
Manipulating Power Series
In some contexts, algebraic recurrence relations can be used for the computation of the
coefﬁcients in Maclaurin expansions, particularly if only a moderate number of coefﬁcients
are wanted. We shall study a few examples.
Example 3.1.10 (Expansion of a Composite Function).
Let g(x) = b0 + b1x + b2x2 + · · · , f (z) = a0 + a1z + a2z2 + · · · be given functions,
analytic at the origin. Find the power series
h(x) = f (g(x)) ≡c0 + c1x + c2x2 + · · · .
In particular, we shall study the case f (z) = ez.

174
Chapter 3. Series, Operators, and Continued Fractions
The ﬁrst idea we may think of is to substitute the expansion b0 + b1x + b2x2 + · · · for
z into the power series for f (z). This is, however, no good unless g(0) = b0 = 0, because
(g(x))k = bk
0 + kbk−1
0
b1x + · · ·
gives a contribution to c0, c1, · · · for every k, and thus we cannot successively compute the
cj by ﬁnite computation.
Now suppose that b0 = 0, b1 = 1, i.e., g(x) = x + b2x2 + b3x3 + · · · . (The
assumption that b1 = 1 is not important, but it simpliﬁes the writing.) Then cj depends
only on bk, ak, k ≤j, since (g(x))k = xk + kb2xk+1 + · · · . We obtain
h(x) = a0 + a1x + (a1b2 + a2)x2 + (a1b3 + 2a2b2 + a3)x3 + · · · ,
and the coefﬁcients of h(x) come out recursively,
c0 = a0,
c1 = a1,
c2 = a1b2 + a2,
c3 = a1b3 + 2a2b2 + a3, . . . .
Now consider the case f (z) = ez, i.e., an = 1/n!. We ﬁrst see that it is then also easy
to handle the case that b0 ̸= 0, since
eg(x) = eb0eb1x+b2x2+b3x3+···.
But there exists a more important simpliﬁcation if f (z) = ez. Note that h satisﬁes the
differential equation h′(x) = g′(x)h(x), h(0) = eb0. Hence
∞

n=0
(n + 1)cn+1xn ≡
∞

j=0
(j + 1)bj+1xj
∞

k=0
ckxk.
Set c0 = eb0, apply the convolution formula (3.1.8), and match the coefﬁcients of xn on the
two sides:
(n + 1)cn+1 = b1cn + 2b2cn−1 + · · · + (n + 1)bn+1c0,
(n = 0, 1, 2, . . .).
This recurrence relation is more easily programmed than the general procedure indicated
above. Other functions that satisfy appropriate differential equations can be treated simi-
larly; see Problem 3.1.8. More information is found in Knuth [230, Sec. 4.7].
Formulas like these are often used in packages for symbolic differentiation and for
automatic or algorithmic differentiation. Expanding a function into a Taylor series is
equivalent to ﬁnding the sequence of derivatives of the function at a given point.
The
goal of symbolic differentiation is to obtain analytic expressions for derivatives of functions
given in analytic form. This is handled by computer algebra systems, for example, Maple
or Mathematica.
In contrast, the goal of automatic or algorithmic differentiation is to extend an
algorithm (a program) for the computation of the numerical values of a few functions
to an algorithm that also computes the numerical values of a few derivatives of these
functions, without truncation errors. A simple example, Horner’s rule for computing values
and derivatives for a polynomial, was given in Sec. 1.2.1. At the time of this writing, lively

3.1. Some Basic Facts about Series
175
and active research is being performed on theory, software development, and applications
of automatic differentiation. Typical applications are in the solution of ordinary differential
equations by Taylor expansion; see the example in Sec. 1.2.4. Such techniques are also
used in optimization for partial derivatives of low order for the computation of Jacobian
and Hessian matrices.
Sometimes power series are needed with many terms, although rarely more than,
say 30.
(The ill-conditioned series are exceptions; see Sec. 3.2.5.) The determination
of the coefﬁcients can be achieved by the Toeplitz matrix method using ﬂoating-point
computation and an interactive matrix language. Computational details will be given in
Problems 3.1.10–3.1.13 for MATLAB. These problems are also available on the home page
of the book, www.siam.org/books/ot103. (Systems like Maple and Mathematica that
include exact arithmetic and other features are evidently also useful here.) An alternative
method, the Cauchy–FFT method, will be described in Sec. 3.2.2.
Both methods will be applied later in the book. See in particular Sec. 3.3.4, where
they are used for deriving approximation formulas in the form of expansions in powers of
elementary difference or differential operators. In such applications, the coefﬁcient vector,
v (say), is obtained in ﬂoating-point arithmetic (usually in a very short time).
Very accurate rational approximations to v, often even the exact values, can
be obtained (again in a very short time) by applying the MATLAB function [Nu,De] =
rat(v,Tol), whichreturnstwointegervectorssothat abs(Nu./De - v) <= Tol*abs(v)
results, with a few different values of the tolerance. This function is based on a continued
fraction algorithm, given in Sec. 3.5.1, for ﬁnding the best rational approximation to a real
number. This can be used for the “cleaning” of numerical results which have, for practical
reasons, been computed by ﬂoating-point arithmetic, although the exact results are known
to be (or strongly believed to be) rather simple rational numbers. The algorithm attempts to
remove the “dirt” caused by computational errors. In Sec. 3.5.1 you will also ﬁnd some com-
ments of importance for the interpretation of the results, for example, for judging whether
the rational numbers are exact results or only good approximations.
Let f (z) be a function analytic at z = 0 with power series
f (z) =
∞

j=0
ajzj.
We can associate this power series with an inﬁnite upper triangular semicirculant matrix
Cf =


a0
a1
a2
a3
. . .
a0
a1
a2
. . .
a0
a1
. . .
a0
. . .
...


.
(3.1.24)
This matrix has constant entries along each diagonal in Cf and is therefore also a Toeplitz
matrix.48 Atruncated power series fN(z) = N−1
j=0 ajzj is represented by the ﬁnite leading
48Otto Toeplitz (1881–1940), German mathematician. While in Göttingen 1906–1913, inﬂuenced by Hilbert’s
work on integral equations, he studied summation processes and discovered what is now known as Toeplitz
operators.

176
Chapter 3. Series, Operators, and Continued Fractions
principal N × N submatrix of Cf (see Deﬁnition A.2.1 in the online Appendix), which can
be written as
fN(SN) =
N−1

j=0
ajSj
N,
(3.1.25)
where SN is a shift matrix. For example, with N = 4,
fN(SN) =


a0
a1
a2
a3
0
a0
a1
a2
0
0
a0
a1
0
0
0
a0

,
SN =


0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0

.
The following properties of SN explain the term “shift matrix”:
SN


x1
x2
x3
x4

=


x2
x3
x4
0

,
(x1, x2, x3, x4)SN = (0, x1, x2, x3).
What do the powers of SN look like? Note that SN
N = 0, i.e., SN is a nilpotent matrix.
This is one of the reasons why the Toeplitz matrix representation is convenient for work
with truncated power series, since it follows that
f (SN) =
∞

j=0
ajSj
N =
N−1

j=0
ajSj
N = fN(SN).
It is easily veriﬁed that a product of upper triangular Toeplitz matrices is of the same type.
Also note that the multiplication of such matrices is commutative. It is also evident that a
linear combination of such matrices is of the same type. Further, it holds that
(f · g)(SN) = f (SN)g(SN) = fN(SN)gN(SN),
(αf + βg)(SN) = αfN(SN) + βgN(SN).
Similarly the quotient of two upper triangular Toeplitz matrices, say,
Q(SN) = f (SN) · g(SN)−1,
(3.1.26)
is also a matrix of the same type. (Ahint for a proof is given in Problem 3.1.10.)49 Note that
Q(SN)·g(SN) = f (SN). (In general, Toeplitz matrices are not nilpotent, and the product of
two nontriangular Toeplitz matrices is not a Toeplitz matrix; this also holds for the inverse.
In this section we shall deal only with upper triangular Toeplitz matrices.)
49In the terminology of algebra, the set of upper triangular N ×N Toeplitz matrices, i.e., {N−1
j=0 αjSj
N}, αj ∈C,
is a commutative integral domain, i.e., isomorphic with the set of polynomials N−1
j=0 αjxj modulo xN, where
x is an indeterminate.

3.1. Some Basic Facts about Series
177
Algorithm 3.1. Expand Row to Toeplitz Matrix.
An upper triangular Toeplitz matrix of order N is uniquely determined by its ﬁrst row r.
The following MATLAB function expands this row to a triangular Toeplitz matrix:
function T = toep(r,N);
% toep expands the row vector r into an upper triangular
% Toeplitz matrix T; N is an optional argument.
lr= length(r);
if (nargin==1 | lr > N), N = lr; end;
if lr < N,
r = [r, zeros(1,N-lr)]; end;
gs = zeros(N,N);
for i = 1:N
gs(i,i:N) = r(1:N-i+1);
end
T = gs;
CPU time and memory space can be saved by working with the ﬁrst rows of the
Toeplitz matrices instead of with the full triangular matrices. We shall denote by f 1, g1, the
row vectors with the ﬁrst N coefﬁcients of the Maclaurin expansions of f (z), g(z). They
are equal to the ﬁrst rows of the matrices f (SN), g(SN), respectively.
Suppose that f 1, g1 are given. We shall compute the ﬁrst row of f (SN) · g(SN) in a
similar notation. Then since
eT
1 (f (SN) · g(SN)) = (eT
1 f (SN)) · g(SN),
this can be written f1*toep(g1,N). Notice that you never have to multiply two triangular
matrices if you work with the ﬁrst rows only. Thus, only about N2 ﬂops and (typically) an
application of the toep(r, N) algorithm are needed.
With similar notations as above, the computation of the quotient in (3.1.26) can be
neatly written in MATLAB as
q1 = f1/toep(g1, N).
Note that this is the vector by matrix division of MATLAB. Although the discussion in
Sec. 1.3.2 is concerned with linear systems with a column as the unknown (instead of a
row), we draw from it the conclusion that only about N2 scalar ﬂops are needed, instead of
the N3/6 needed in the solution of the matrix equation Q · g(SN) = f (SN).
A library called toeplib is given in Problem 3.1.10 (a), which consists of short
MATLAB scripts mainly based on Table 3.1.1. In the following problems the series of
the library are combined by elementary operations to become interesting examples of the
Toeplitz matrix method. The convenience, the accuracy, and the execution time are probably
much better than you would expect; even the authors were surprised.
Next we shall study how a composite function h(z) = f (g(z)) can be expanded in
powers of z. Suppose that f (z) and g(z) are analytic at z = 0, f (z) = ∞
j=1 f 1(j)zj−1.

178
Chapter 3. Series, Operators, and Continued Fractions
An important assumption is that g(0) = 0. Then we can set g(z) = z ¯g(z), hence (g(z))n =
zn( ¯g(z))n and, because Sn
N = 0, n ≥N, we obtain
(g(SN))n = Sn
N · ( ¯g(SN))n = 0
if n ≥N and g(0) = 0,
h(SN) ≡f (g(SN)) =
N

j=1
f 1(j)(g(SN))j−1
if g(0) = 0.
(3.1.27)
This matrix polynomial can be computed by a matrix version of Horner’s rule. The row
vector version of this equation is written h1 = comp(f 1, g1, N).
If g(0) ̸= 0, (3.1.27) still provides an “expansion,” but it is wrong; see Prob-
lem 3.1.12 (c). Suppose that |g(0)| is less than the radius of convergence of the Maclaurin
expansion of f (x). Then a correct expansion is obtained by a different decomposition. Set
˜g(z) = g(z) −g(0), ˜f (x) = f (x + g(0)). Then ˜f , ˜g are analytic at z = 0, ˜g(0) = 0, and
˜f ( ˜g(z)) = f (g(z)) = h(z). Thus, (3.1.27) and its row vector implementations can be used
if ˜f , ˜g are substituted for f, g.
Algorithm 3.2. Expansion of Composite Function.
The following MATLAB function uses the Horner recurrence for the expansion of a com-
posite function and evaluates the matrix polynomial f (g(SN)) according to (3.1.27). If
g(0) = 0, the following MATLAB function evaluates the ﬁrst row of h(SN) = f (g(SN)).
function h1 = comp(f1,g1,N);
% INPUT: the integer N and the rows f1, g1, with
% the first N Maclaurin coefficients for the analytic
% functions f(z), g(z).
% OUTPUT: The row h1 with the first N Maclaurin coeffi-
% cients for the composite function h(z) = f(g(z)),
% where g1(1) = g(0) = 0.
% Error message if g(0) \ne 0.
if g1(1) ˜= 0,
error(‘g(0) ˜= 0 in a composite function f(g(z))’)
end
e1t = zeros(1,N);
e1t(1)=1;
r = f1(N)*e1t;
gs = toep(g1,N);
for j = N-1:-1:1,
r = r*gs + f1(j)*e1t;
end
h1 = r;
Analytic functions of matrices are deﬁned by their Taylor series. For example, the
series
eA = I + A + A2
2! + A3
3! + · · ·

3.1. Some Basic Facts about Series
179
converges elementwise for any matrix A. There exist several algorithms for computing
eA,
√
A, log A, where A is a square matrix. One can form linear combinations, products,
quotients, and composite functions of them. For example, a “principal matrix value” of
Y = (I + A)α is obtained by
B = log(I + A),
Y = eαB.
For a composite matrix function f (g(A)), it is not necessary that g(0) = 0, but it
is important that g(z) and f (g(z)) are analytic when z is an eigenvalue of A. We obtain
truncated power series if A = SN; note that SN has a multiple eigenvalue at zero. The
coding, and the manual handling in interactive computing, are convenient with matrix
functions, but the computer has to perform more operations on full triangular matrices than
with the row vector level algorithms described above. Therefore, for very long expansions
the earlier algorithms are notably faster.
If the given power series, f (x), g(x), . . . have rational coefﬁcients, then the exact
results of a sequence of additions, multiplications, divisions, compositions, differentiations,
and integrations will have rational coefﬁcients, because the algorithms are all formed by a
ﬁnite number of scalar additions, multiplications, and divisions. As mentioned above, very
accurate rational approximations, often even the exact values, can be quickly obtained by
applying a continued fraction algorithm (presented in Sec. 3.5.1) to the results of a ﬂoating-
point computation.
If f (x) is an even function, its power series contains only even powers of x. You gain
space and time by letting the shift matrix SN correspond to x2 (instead of x). Similarly, if
f (x) is an odd function, you can instead work with the even function f (x)/x, and let SN
correspond to x2.
Finally, we consider a classical problem of mathematics, known as power series
reversion. The task is to ﬁnd the power series for the inverse function x = g(y) of the
function y = f (x) = ∞
j=0 ajxj, in the particular case where a0 = 0, a1 = 1. Note that
even if the series for f (x) is ﬁnite, the series for g(y) is in general inﬁnite!
The following simple cases of power series reversion are often sufﬁcient and useful
in low order computations with paper and pencil.
y = x + axk + · · · ,
(k > 1),
⇒x = y −axk −· · · = y −ayk −· · · ;
(3.1.28)
y = f (x) ≡x + a2x2 + a3x3 + a4x4 + · · · ,
⇒x = g(y) ≡y −a2y2 + (2a2
2 −a3)y3 −(5a2
2 −5a2a3 + a4)y4 + · · · . (3.1.29)
An application of power series reversion occurs in the derivation of a family of iterative
methods of arbitrary high order for solving scalar nonlinear equations; see Sec. 6.2.3.
The radius of convergence depends on the singularities of g(y), which are typically
related to the singularities of f (x) and to the zeros of f ′(x) (Why?). There are other cases,
for example, if f ′(x) →0 as x →∞, then lim f (x) may be a singularity of g(y).
Knuth [230, Sec 4.7] presents several algorithms for power series reversion, including
a classical algorithm due to Lagrange (1768) that requires O(N3) operations to compute the
ﬁrst N terms. An algorithm due to Brent and Kung [47] is based on an adaptation to formal

180
Chapter 3. Series, Operators, and Continued Fractions
power series of Newton’s method (1.2.3) for solving a numerical algebraic equation. For
power series reversion, the equation to be solved reads
f (g(y)) = y,
(3.1.30)
where the coefﬁcients of g are the unknowns. The number of correct terms is roughly
doubled in each iteration, as long as N is not exceeded. In the usual numerical application
of Newton’s method to a scalar nonlinear equation (see Secs. 1.2 and 6.3) it is the number of
signiﬁcant digits that is (approximately) doubled, so-called quadratic convergence. Brent
and Kung’s algorithm can be implemented in about 150 (N log N)3/2 scalar ﬂops.
We now develop a convenient Toeplitz matrix implementation of the Brent and Kung
algorithm. It requires about cN3 log N scalar ﬂops with a moderate value of c. It is thus
much inferior to the original algorithm if N is very large. In some interesting interactive
applications, however, N rarelyexceeds30. Insuchcasesourimplementationissatisfactory,
unless (say) hundreds of series are to be reversed. Let
y = f (x) =
∞

j=1
f 1(j)xj−1,
where f 1(1) = f (0) = 0, f 1(2) = f ′(0) = 1 (with the notation used previously). Power
series reversion is to ﬁnd the power series for the inverse function
x = g(y) =
∞

j=1
g1(j)yj−1,
where g1(1) = g(0) = 0. We work with truncated series with N terms in theToeplitz matrix
representation. The inverse function relationship gives the matrix equation f (g(SN)) = SN.
Because g(0) = 0, we have, by (3.1.27),
f (g(SN)) =
N

j=1
f 1(j)g(SN)j−1.
Now Horner’s rule can be used for computing the polynomial and its derivative, the latter
being obtained by algorithmic differentiation; see Sec. 1.2.1.
Algorithm 3.3. Power Series Reversion.
The ﬁrst row of this matrix equation is treated by Newton’s method in the MATLAB function
breku50 listed below. The Horner algorithms are adapted to the ﬁrst row. The notations in
the code are almost the same as in the theoretical description, although lower case letters are
used, e.g., the matrix g(SN) is denoted gs, and fgs1 is the ﬁrst row of the matrix f (g(SN)).
50The name“breku” comes from Brent and Kung, who were probably the ﬁrst mathematicians to apply Newton’s
method to series reversion, although with a different formulation of the equation than ours (no Toeplitz matrices).

3.1. Some Basic Facts about Series
181
The equation reads fgs1 −s1 = 0.
function g1 = breku(f1,N);
% INPUT: The row vector f1 that represents a (truncated)
% Maclaurin series. N is optional input; by default
% N = length(f1). If length(f1) < N, f1 is extended to
% length N by zeros.
% OUTPUT: The row g1, i.e., the first N terms of the series
% x = g(y), where y = f(x).
% Note that f1(1) = 0, f1(2) = 1; if not, there will
% be an error message.
if ˜(f1(1) ˜= 0 | f1(2) ˜= 1),
error(‘wrong f1(1) or f1(2)’);
end
lf1 = length(f1);
if (nargin == 1|lf1 > N), N = lf1; end
if lf1 < N, f1 = [f1 zeros(1, N-lf1)]
end
maxiter = floor(log(N)/log(2));
e1t = [1, zeros(1,N-1)];
s1 = [0 1 zeros(1,N-2)];
gs1 = s1;
for iter = 0:maxiter
gs
= toep(gs1,N);
% Horner’s scheme for computing the first rows
% of f(gs) and f’(g(s)):
fgs1 = f1(N)*e1t;
der1 = zeros(1,N);
for j
=
N-1:-1:1
ofgs1 = fgs1;
%ofgs1 means "old" fgs1
fgs1 = ofgs1*gs + f1(j)*e1t ;
der1 = ofgs1 + der1*gs ;
end
% A Newton iteration for the equation fgs1 - s1 = 0:
gs1 = gs1 - (fgs1 - s1)/toep(der1,N);
end
g1 = gs1;
3.1.5
Formal Power Series
A power series is not only a means for numerical computation; it is also an aid for deriving
formulas in numerical mathematics and in other branches of applied mathematics. Then one
has another, more algebraic, aspect of power series that we shall brieﬂy introduce. A more
rigorous and detailed treatment is found in Henrici [196, Chapter 1], and in the literature
quoted there.
The set P of formal power series consists of all expressions of the form
P = a0 + a1x + a2x2 + · · · ,
where the coefﬁcients aj may be real or complex numbers (or elements in some other ﬁeld),
while x is an algebraic indeterminate; x and its powers can be viewed as place keepers.

182
Chapter 3. Series, Operators, and Continued Fractions
The sum of P and another formal power series, Q = b0 + b1x + b2x2 + · · ·, is deﬁned as
P + Q = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2 + · · · .
Similarly, the Cauchy product is deﬁned as
PQ = c0 + c1x + c2x2 + · · · ,
cn =
n

j=0
ajbn−j,
where the coefﬁcients are given by the convolution formula (3.1.8). The multiplicative
identity element is the series I := 1 + 0x + 0x2 + · · ·. The division of two formal power
series is deﬁned by a recurrence, as indicated in Example 3.1.5, if and only if the ﬁrst
coefﬁcient of the denominator is not zero. In algebraic terminology, the set P together with
the operations of addition and multiplication is an integral domain.
No real or complex values are assigned to x and P. Convergence, divergence, and
remainder term have no relevance for formal power series. The coefﬁcients of a formal
power series may even be such that the series diverges for any nonzero complex value that
you substitute for the indeterminate, for example, the series
P = 0!x −1!x2 + 2!x3 −3!x4 + · · · .
(3.1.31)
Other operations are deﬁned without surprises, for example, the derivative of P is deﬁned
as P′ = 1a1 + 2a2x + 3a3x2 + · · ·. The limit process, by which the derivative is deﬁned in
calculus, does not exist for formal power series. The usual rules for differentiation are still
valid, and as an exercise you may verify that the formal power series deﬁned by (3.1.31)
satisﬁes the formal differential equation x2P′ = x −P.
Formal power series can be used for deriving identities. In most applications in this
book difference operators or differential operators are substituted for the indeterminates, and
the identities are then used in the derivation of approximation formulas, and for interpolation,
numerical differentiation, and integration.
The formal deﬁnitions of the Cauchy product, (i.e., convolution) and division are
rarely used in practical calculation.
It is easier to work with upper triangular N × N
Toeplitz matrices, as in Sec. 3.1.4, where N is any natural number. Algebraic calculations
with these matrices are isomorphic with calculations with formal power series modulo xN.
If you perform operations on matrices fM(S), gM(S), . . . , where M < N, the results
are equal to the principal M × M submatrices of the results obtained with the matrices
fN(S), gN(S), . . . . This fact follows directly from the equivalence with power series
manipulations. It is related to the fact that in the multiplication of block upper triangular
matrices, the diagonal blocks of the product equal the products of the diagonal blocks, and
no new off-diagonal blocks enter; see Example A.2.1 in Online Appendix A.
So, wecaneasilydeﬁnetheproductoftwoinﬁniteuppertriangularmatrices, C = AB,
by stating that if i ≤j ≤n, then cij has the same value that it has in the N × N submatrix
CN = ANBN for every N ≥n. In particular C is upper triangular, and note that there are
no conditions on the behavior of the elements aij, bij as i, j →∞. One can show that
this product is associative and distributive. For the inﬁnite triangular Toeplitz matrices it is
commutative too.51
51For inﬁnite nontriangular matrices the deﬁnition of a product generally contains conditions on the behavior
of the elements as i, j →∞, but we shall not discuss this here.

3.1. Some Basic Facts about Series
183
The mapping of formal power series onto the set of inﬁnite semicirculant matrices
is an isomorphism. (see Henrici [196, Sec. 1.3]). If the formal power series a0 + a1x +
a2x2 +· · · and its reciprocal series, which exists if and only if a0 ̸= 0, are represented by the
semicirculants A and B, respectively, Henrici proves that AB = BA = I, where I is the
unit matrix of inﬁnite order. This indicates how to deﬁne the inverse of any inﬁnite upper
triangular matrix if all diagonal elements aii ̸= 0.
If a function f of a complex variable z is analytic at the origin, then we deﬁne52 f (x)
as the formal power series with the same coefﬁcients as the Maclaurin series for f (z). In
the case of a multivalued function we take the principal branch.
There is a kind of “permanence of functional equations” also for the generalization
from a function g(z) of a complex variable that is analytic at the origin, to the formal power
series g(x). We illustrate a general principle on an important special example that we
formulate as a lemma, since we shall need it in the next section.
Lemma 3.1.9.
(ex)θ = eθx,
(θ ∈R).
(3.1.32)
Proof. Let the coefﬁcient of xj in the expansion of the left-hand side be φj(θ). The
corresponding coefﬁcient for the right-hand side is θj/j!. If we replace x by a complex
variable z, the power series coefﬁcients are the same and we know that (ez)θ = eθz, hence
φj(θ) = θj/j!, j = 1, 2, 3 . . . , and therefore
∞

0
φj(θ)xj =
∞

0
(θj/j!)xj,
and the lemma follows.
Example 3.1.11.
Find (if possible) a formal power series Q = 0+b1x+b2x2 +b3x3 +· · · that satisﬁes
e−Q = 1 −x,
(3.1.33)
where e−Q = 1 −Q + Q2/2! −· · · .
We can, in principle, determine an arbitrarily long sequence b1, b2, b3, . . . , bk, by
matching the coefﬁcients of x, x2, x3, . . . , xk, in the two sides of the equation. We display
the ﬁrst three equations.
1 −(b1x + b2x2 + b3x3 + · · ·) + (b1x + b2x2 + · · ·)2/2 −(b1x + · · ·)3/6 + · · ·
= 1 −1x + 0x2 + 0x3 + · · · .
For any natural number k, the matching condition is of the form
−bk + φk(bk−1, bk−2, . . . , b1) = 0.
52Henrici (see reference above) does not use this concept—it may not be established.

184
Chapter 3. Series, Operators, and Continued Fractions
This shows that the coefﬁcients are uniquely determined.
−b1 = −1 ⇒b1 = 1,
−b2 + b2
1/2 = 0 ⇒b2 = 1/2,
−b3 + b1b2 −b1/6 = 0 ⇒b3 = 1/3.
There exists, however, a much easier way to determine the coefﬁcients. For the analogous
problem with a complex variable z, we know that the solution is unique,
q(z) = −ln(1 −z) =
∞

j=1
zj/j
(the principal branch, where b0 = 0), and hence ∞
1 xj/j is the unique formal power series
that solves the problem, and we can use the notation Q = −ln(1 −x) for it.53
Thetheoryofformalpowerseriescaninasimilarwayjustifymanyelegant“symbolic”
applications of power series for deriving mathematical formulas.
Review Questions
3.1.1 (a) Formulate three general theorems that can be used for estimating the remainder
term in numerical series.
(b) What can you say about the remainder term, if the nth term is O(n−k), k > 1?
Suppose in addition that the series is alternating. What further condition should you
add, in order to guarantee that the remainder term will be O(n−k)?
3.1.2 Give, with convergence conditions, the Maclaurin series for ln(1+x), ex, sin x, cos x,
(1 + x)k, (1 −x)−1, ln 1+x
1−x , arctan x.
3.1.3 Describe the main features of a few methods to compute the Maclaurin coefﬁcients
of, e.g.,
√
2ex −1.
3.1.4 Give generating functions of the Bernoulli and the Euler numbers. Describe generally
how to derive the coefﬁcients in a quotient of two Maclaurin series.
3.1.5 If a functional equation, for example, 4(cos x)3 = cos 3x + 3 cos x, is known to be
valid for real x, how do you know that it holds also for all complex x? Explain what
is meant by the statement that it holds also for formal power series, and why this is
true.
3.1.6 (a) Show that multiplying two arbitrary upper triangular matrices of order N uses
N
k=1 k(N −k) ≈N3/6 ﬂops, compared to N
k=1 k ≈N2/2 for the product of a row
vector and an upper triangular matrix.
(b) Show that if g(x) is a power series and g(0) = 0, then g(SN)n = 0, n ≥N.
Make an operation count for the evaluation of the matrix polynomial f (g(SN)) by
the matrix version of Horner’s scheme.
53The three coefﬁcients bj computed above agree, of course, with 1/j, j = 1 : 3.

Problems and Computer Exercises
185
(c) Consider the product f (SN)g(SN), where f (x) and g(x) are two power series.
Show, using rules for matrix multiplication, that for any M < N the leading M × M
block of the product matrix equals f (SM)g(SM).
3.1.7 Consider a power series y = f (x) = ∞
j=0 ajxj, where a0 = 0, a1 = 1. What is
meant by reversion of this power series? In the Brent–Kung method the problem of
reversion of a power series is formulated as a nonlinear equation. Write this equation
for the Toeplitz matrix representation of the series.
3.1.8 Let P = a0 + a1x + a2x2 + · · · and Q = b0 + b1x + b2x2 + · · · be two formal power
series. Deﬁne the sum P + Q and the Cauchy product PQ.
Problems and Computer Exercises
3.1.1 In how large a neighborhood of x = 0 does one get, respectively, four and six correct
decimals using the following approximations?
(a) sin x ≈x; (b) (1 + x2)−1/2 ≈1 −x2/2; (c) (1 + x2)−1/2e
√cos x ≈e(1 −3
4x2).
Comment: The truncation error is asymptotically qxp where you know p.
An alternative to an exact algebraic calculation of q is a numerical estimation of
q, by means of the actual error for a suitable value of x—neither too big nor too
small (!). (Check the estimate of q for another value of x.)
3.1.2 (a) Let a, b be the lengths of the two smaller sides of a right angle triangle, b ≪a.
Show that the hypotenuse is approximately a+b2/(2a) and estimate the error of this
approximation. If a = 100, how large is b allowed to be, in order that the absolute
error should be less than 0.01?
(b) How large a relative error do you commit when you approximate the length of
a small circular arc by the length of the chord? How big is the error if the arc is
100 km on a great circle of the Earth? (Approximate the Earth by a ball of radius
40,000/(2π) km.)
(c) How accurate is the formula arctan x ≈π/2 −1/x for x ≫1?
3.1.3 (a) Compute 10 −(999.999)1/3 to nine signiﬁcant digits by the use of the binomial
expansion. Compare your result with the result obtained by a computer in IEEE
double precision arithmetic, directly from the ﬁrst expression.
(b) How many terms of the Maclaurin series for ln(1 + x) would you need in order
to compute ln 2 with an error less than 10−6 ? How many terms do you need if you
use instead the series for ln((1 + x)/(1 −x)), with an appropriate choice of x?
3.1.4 It is well known that erf(x) →1 as x →∞. If x ≫1 the relative accuracy of the
complement 1−erf(x) is of interest. But the series expansion used in Example 3.1.3
for x ∈[0, 1] is not suitable for large values of x. Why?
Hint: Derive an approximate expression for the largest term.
3.1.5 Compute by means of appropriate expansions, not necessarily in powers of t, the
following integrals to (say) ﬁve correct decimals.

186
Chapter 3. Series, Operators, and Continued Fractions
(This is for paper, pencil, and a pocket calculator.)
(a)
 0.1
0
(1 −0.1 sin t)1/2 dt;
(b)
 ∞
10
(t3 −t)−1/2 dt.
3.1.6 (a) Expand arcsin x in powers of x by the integration of the expansion of (1−x2)−1/2.
(b) Use the result in (a) to prove the expansion
x = sinh x −1
2
sinh3 x
3
+ 1·3
2·4
sinh5 x
5
−1·3·5
2·4·6
sinh7 x
7
+ · · · .
3.1.7 (a) Consider the power series for
(1 + x)−α,
x > 0,
0 < α < 1.
Show that it is equal to the hypergeometric function F(α, 1, 1, −x). Is it true that
the expansion is alternating? Is it true that the remainder has the same sign as the
ﬁrst neglected term, for x > 1, where the series is divergent? What do the Theorems
3.1.4 and 3.1.5 tell you in the cases x < 1 and x > 1?
Comment: An application of the divergent case for α =
1
2 is found in Prob-
lem 3.2.9 (c).
(b) Express the coefﬁcients of the power series expansions of y cot y and ln(sin y/y)
in terms of the Bernoulli numbers.
Hint: Set x = 2iy into (3.1.20). Differentiate the second function.
(c) Find a recurrence relation for the Euler numbers En (3.1.22) and use it for show-
ing that these numbers are integers.
(d) Show that
1
2 ln
z + 1
z −1

= 1
z + 1
3z3 + 1
5z5 + · · · ,
|z| > 1.
Find a recurrence relation for the coefﬁcients of the expansion

ln
z + 1
z −1
−1
= 1
2z −µ1z−1 −µ3z−3 −µ5z−5 −· · · ,
|z| > 1.
Compute µ1, µ3, µ5 and determine ∞
0 µ2j+1 by letting z ↓1. (Full rigor is not
required.)
Hint: Look at Example 3.1.5.
3.1.8 The power series expansion g(x) = b1x +b2x2 +· · · is given. Find recurrence rela-
tions for the coefﬁcients of the expansion for h(x) ≡f (g(x)) = c0+c1x+c2x2+· · ·
in the following cases:
(a) h(x) = ln(1 + g(x)), f (x) = ln(1 + x).
Hint: Show that h′(x) = g′(x) −h′(x)g(x). Then proceed analogously to Exam-
ple 3.1.10.

Problems and Computer Exercises
187
Answer:
c0 = 0,
cn = bn −1
n
n−1

j=1
(n −j)cn−jbj.
(b) h(x) = (1 + g(x))k, f (x) = (1 + x)k, k ∈R, k ̸= 1.
Hint: Show that g(x)h′(x) = kh(x)g′(x) −h′(x). Then proceed analogously to
Example 3.1.10.
Answer:
c0 = 1,
cn = 1
n
n

j=1

(k + 1)j −n

cn−jbj,
n = 1, 2, . . . . The recurrence relation is known as the J. C. P. Miller’s formula.
(c) h1(y) = cos g(x), h2(y) = sin g(x), simultaneously.
Hint: Consider instead h(y) = eig(x), and separate real and imaginary parts after-
ward.
3.1.9 (a) If you want N > 3 terms in the power series expansion of the function f (x) =
(1 + x + x2)/(1 −x + x2), you must augment the expansions for the numerator and
denominator by sequences of zeros, so that the order of Toeplitz matrix becomes N.
Show experimentally and theoretically that the ﬁrst row of
(IN + SN + S2
N)/(IN −SN + S2
N)
is obtained by the statement
[1, 1, 1, zeros(1,N-3)]/toep([1, -1, 1, zeros(1,N-3)])
(b) Let f (z) = −z−1 ln(1 −z). Compute the ﬁrst six coefﬁcients of the Maclaurin
series for the functions f (z)k, k = 1 : 5, in ﬂoating-point, and convert them to
rational form. (The answer is given in (3.3.22) and an application to numerical
differentiation in Example 3.3.6.)
If you choose an appropriate tolerance in the MATLAB function rat you will obtain
an accurate rational approximation, but it is not necessarily exact. Try to judge
which of the coefﬁcients are exact.
(c) Compute in ﬂoating-point the coefﬁcients µ2j−1, j = 1 : 11, deﬁned in Prob-
lem 3.1.7 (d), and convert them to rational form.
Hint: First seek an equivalent problem for an expansion in ascending powers.
(d) Prove that Q = f (SN)g(SN)−1 is an upper triangular Toeplitz matrix.
Hint: Deﬁne Q = toep(q1, N), where q1 is deﬁned by (3.1.26), and show that each
row of the equation Q · g(SN) = f (SN) is satisﬁed.
3.1.10 (a) Study the following library of MATLAB lines for common applications of the
Toeplitz matrix method for arbitrary given values of N. All series are truncated to
N terms. The shift matrix SN corresponds to the variable x. You are welcome to
add new “cases,” e.g., for some of the exercises below.

188
Chapter 3. Series, Operators, and Continued Fractions
function y = toeplib(cas,N,par);
% cas is a string parameter; par is an optional real
% or complex scalar with default value 1.
%
if nargin == 2, par = 1; end
if cas == ’bin’,
y = [1
cumprod(par:-1:par-N+2)./cumprod(1:N-1)];
% y = 1st row of binomial series (1+x)ˆpar, par in R;
elseif
cas == ’con’,
y = cumprod([1
par*ones(1,N-1)]);
% The array multiplication y.*f1 returns the first
% row of f(par*S_N);
% sum(y.*f1) evaluates f(par). See also Problem˜(b).
elseif cas == ’exp’,
y = [1
cumprod(par./[1:(N-1)])];
% y = 1st row of exponential \exp(par*x).
% Since par can be complex, trigonometric functions
% can also be expanded.
elseif cas == ’log’,
y = [0
1./[1:(N-1)]].*cumprod ([-1 -par*ones(1:N-1)]);
% y = 1st row of logarithm \ln(1+par*x).
elseif cas == ’e1t’,
y = [1
zeros(1,N-1)];
% y = e_1ˆT
elseif cas == ’SN1’, y = [0
1
zeros(1,N-2)];
% y = 1st row of S_N.
elseif cas == ’dif’, y = [0
1:(N-1)];
% y.*f1 returns xf’(x).
else
cas == ’int’, y =1./[1:N];
% y.*f1 returns {1\over x}\int_0ˆx f(t) dt.
end
(b) Evaluation of f (x) Given N and f1 of your own choice, set
fterms = toeplib(’con’,N,x).*f1.
Whatis sum(fterms)and cumsum(fterms)? Whencan sum(fliplr(fterms))
be useful?
(c) Write a code that, for arbitrary given N, returns the ﬁrst rows of the Toeplitz
matrices for cos x and sin x, with SN corresponding to x, and then transforms them
to ﬁrst rows for Toeplitz matrices with SN corresponding to x2. Apply this for (say)
N = 36, to determine the errors of the coefﬁcients of 4(cos x)3 −3 cos x −cos 3x.
(d) Find out how a library “toeplib2” designed for Toeplitz matrices for even func-
tions, where SN corresponds to x2, must be different from toeplib. For example,
how are cas == ’dif’ and cas == ’int’ to be changed?
(e) Unfortunately, a toeplib “case” has at most one parameter, namely par. Write
a code that calls toeplib twice for ﬁnding the Maclaurin coefﬁcients of the three
parameter function y = (a + bx)α,
a > 0, b, α real. Compute the coefﬁcients in

Problems and Computer Exercises
189
two different ways for N = 24, a = 2, b = −1, α = ±3, and compare the results
for estimating the accuracy of the coefﬁcients.
(f) Compute the Maclaurin expansions for (1 −x2)−1/2 and arcsin x, and for y =
2arcsinh (x/2). Expand also dy/dx and y2. Convert the coefﬁcients to rational
numbers, as long as they seem to be reliable. Save the results, or make it easy to
reproduce them, for comparisons with the results of Problem 3.1.12 (a).
Comment: The last three series are fundamental for the expansions of differential
operators in powers of central difference operators, which lead to highly accurate
formulas for numerical differentiation.
(g)Two power series that generate the Bernoulli numbers are given in Example 3.1.5,
namely
x ≡
 ∞

i=1
xi
i!
 ∞

j=0
Bjxj
j!

,
x
2
ex/2 + e−x/2
ex/2 −e−x/2 =
∞

j=0
B2jx2j
(2j)! .
Compute B2j for (say) j ≤30 in ﬂoating-point using each of these formulas, and
compute the differences in the results, which are inﬂuenced by rounding errors. Try
to ﬁnd whether one of the sequences is more accurate than the other by means of the
formula in (3.1.21) for (say) j > 4. Then convert the results to rational numbers.
Use several tolerances in the function rat and compare with [1, Table 23.2]. Some
of the results are likely to disagree. Why?
(h) The Kummer conﬂuent hypergeometric function M(a, b, x) is deﬁned by the
power series (3.1.17). Kummer’s ﬁrst identity,
M(a, b, −x) = e−xM(b −a, b, x),
is important, for example, because the series on the left-hand side is ill-conditioned if
x ≫1, a > 0, b > 0, whiletheexpressionontheright-handsideiswell-conditioned.
Check the identity experimentally by computing the difference between the series on
the left-hand side and on the right for a few values of a, b, The computed coefﬁcients
are afﬂicted by rounding errors. Are the differences small enough to convince you
of the validity of the formula?
3.1.11 (a) Matrix functions in MATLAB. For h(z) = eg(z) it is convenient to use the matrix
function expm(g(SN)) or, on the vector level, h1 = e1t*expm(g(SN)), rather
than to use h1 = comp(f1,g1). If f (0) ̸= 0, you can analogously use the func-
tions
logm and
sqrtm.
They may be slower and less accurate than
h1 = comp(f1,g1), but they are typically fast and accurate enough.
Compare computing times and accuracy in the use of expm(k * logm(eye(N))
+ SN) and toeplib(’bin’,N,k) for a few values of N and k.
Comment: Note that for triangular Toeplitz matrices the diagonal elements are mul-
tiple eigenvalues.
(b) Expand esin(z) in powers of z in two ways: ﬁrst using the function in Prob-
lem 3.1.12 (a); second using the matrix functions of MATLAB. Show that the latter
can be written
HN = expm(imag(expm(i*SN))).

190
Chapter 3. Series, Operators, and Continued Fractions
Do not be surprised if you ﬁnd a dirty imaginary part of HN. Kill it!
Compare the results of the two procedures. If you have done the runs appropriately,
the results should agree excellently.
(c) Treat the series h(z) = √(1 + ez) in three different ways and compare the results
with respect to validity, accuracy, and computing time.
(i) Set ha(z) = h(z), and determine f (z), g(z), analytic at z = 0, so that g(0) = 0.
Compute ha1 = comp(f1,g1,N). Do you trust the result?
(ii) Set h(z) = H(z). Compute HN = sqrtm(eye(N) + expm(SN)).
In the ﬁrst test, i.e., for N = 6, display the matrix HN and check that HN is an
upper triangular Toeplitz matrix. For larger values of N, display the ﬁrst row only
and compare it to ha1. If you have done all this correctly, the agreement should be
extremely good, and we can practically conclude that both are very accurate.
(iii) Try the “natural,” although “illegal,” decomposition hb(z) = f (g(z)), with
f (x) = (1 + x)0.5, g(z) = ez. Remove temporarily the error stop. Demonstrate by
numerical experiment that hb1 is very wrong. If this is a surprise, read Sec. 3.1.4
once more.
3.1.12 (a) Apply the function breku for the reversion of power series to the computation
of g(y) for f (x) = sin x and for f (x) = 2 sinh(x/2). Compare with the results
of Problem 3.1.10 (f). Then reverse the two computed series g(y), and study how
you return to the original expansion of f (x), more or less accurately. Use “tic” and
“toc” to take the time for a few values of N.
(b) Compute g(y) for f (x) = ln(1 + x), f (x) = ex −1, f (x) = x + x2, and
f (x) = x+x2+x3. If you know an analytic expression for g(y), ﬁnd the Maclaurin
expansion for this, and compare with the expansions obtained from breku.
(c) Set y = f (x) and suppose that y(0) ̸= 0, y′(0) ̸= 0. Show how the function
breku can be used for expanding the inverse function in powers of (y−y(0))/y′(0).
Construct some good test examples.
(d) For the equation sin x −(1 −y)x = 0, express x2 = g(y) (why x2?), with
N = 12. Then express x in the form x ≈±y1/2P(y), where P(y) is a truncated
power series with (say) 11 terms.
3.1.13 The inverse function w(y) of y(w) = wew is known as the Lambert W function.54
The power series expansion for w(y) is
w(y) = y +
∞

n=2
(−1)n−1nn−2
(n −1)!
yn
= y −y2 + 3
2y3 −8
3y4 + 125
24 y5 −54
5 y6 + 16807
720 y7 −· · · .
Estimate the radius of convergence for f (x) = xex approximately by means of the
ratios of the coefﬁcients computed in (d), and exactly.
54Johann Heinrich Lambert (1728–1777), a German mathematician, physicist, and astronomer, was a colleague
of Euler and Lagrange at the Berlin Academy of Sciences. He is best known for his illumination laws and for the
continued fraction expansions of elementary functions; see Sec. 3.5.1. His W function was “rediscovered” a few
years ago; see [81].

3.2. More about Series
191
3.2
More about Series
3.2.1
Laurent and Fourier Series
A Laurent series is a series of the form
∞

n=−∞
cnzn.
(3.2.1)
Its convergence region is the intersection of the convergence regions of the expansions
∞

n=0
cnzn
and
∞

m=1
c−mz−m,
the interior of which are determined by conditions of the form |z| < r2 and |z| > r1. The
convergence region can be void, for example, if r2 < r1.
If 0 < r1 < r2 < ∞, then the convergence region is an annulus, r1 < |z| < r2. The
series deﬁnes an analytic function in the annulus. Conversely, if f (z) is a single-valued
analytic function in this annulus, it is represented by a Laurent series, which converges
uniformly in every closed subdomain of the annulus.
The coefﬁcients are determined by the following formula, due to Cauchy:55
cn =
1
2πi

|z|=r
z−n−1f (z)dz,
r1 < r < r2, −∞< n < ∞
(3.2.2)
and
|cn| ≤r−n max
|z|=r |f (z)|.
(3.2.3)
The extension to the case when r2 = ∞is obvious; the extension to r1 = 0 depends on
whether there are any terms with negative exponents or not. In the extension of formal
power series to formal Laurent series, however, only a ﬁnite number of terms with negative
indices are allowed to be different from zero; see Henrici [196, Sec. 1.8]. If you substitute
z for z−1 an inﬁnite number of negative indices is allowed, if the number of positive indices
is ﬁnite.
Example 3.2.1.
A function may have several Laurent expansions (with different regions of conver-
gence), for example,
(z −a)−1 =



−∞
n=0 a−n−1zn
if |z| < |a|,
∞
m=1 am−1z−m
if |z| > |a|.
The function 1/(z −1) + 1/(z −2) has three Laurent expansions, with validity conditions
|z| < 1, 1 < |z| < 2, 2 < |z|, respectively. The series contains both positive and negative
powers of z in the middle case only. The details are left for Problem 3.2.4 (a).
55Augustin Cauchy (1789–1857) is the father of modern analysis. He is the creator of complex analysis, in
which this formula plays a fundamental role.

192
Chapter 3. Series, Operators, and Continued Fractions
Remark 3.2.1.
The restriction to single-valued analytic functions is important in this
subsection. In this book we cannot entirely avoid working with multivalued functions such
as √z, ln z, zα, (α noninteger). We always work with such a function, however, in some
region where one branch of it, determined by some convention, is single-valued. In the
examples mentioned, the natural conventions are to require the function to be positive when
z > 1, and to forbid z to cross the negative real axis. In other words, the complex plane
has a cut along the negative real axis. The annulus mentioned above is incomplete in these
cases; its intersection with the negative real axis is missing, and we cannot use a Laurent
expansion.
For a function like ln( z+1
z−1), we can, depending on the context, cut out either the
interval [−1, 1] or the complement of this interval with respect to the real axis. We then
use an expansion into negative or into positive powers of z, respectively.
If r1 < 1 < r2, we set F(t) = f (eit).
Note that F(t) is a periodic function;
F(t + 2π) = F(t). By (3.2.1) and (3.2.2), the Laurent series then becomes for z = eit a
Fourier series:
F(t) =
∞

n=−∞
cneint,
cn = 1
2π
 π
−π
e−intF(t) dt.
(3.2.4)
Note that c−m = O(rm
1 ) for m →+∞, and cn = O(r−n
2 ) for n →+∞. The formulas
in (3.2.4), however, are valid in much more general situations, where cn →0 much more
slowly, and where F(t) cannot be continued to an analytic function f (z), z = reit, in an
annulus. (Typically, in such a case r1 = 1 = r2.)
A Fourier series is often written in the following form:
F(t) = 1
2a0 +
∞

k=1
(ak cos kt + bk sin kt).
(3.2.5)
Consider ckeikt +c−ke−ikt ≡ak cos kt +bk sin kt. Since e±ikt = cos kt ±i sin kt, we obtain
for k ≥0
ak = ck + c−k = 1
π
 π
−π
F(t) cos kt dt,
bk = i(ck −c−k) = 1
π
 π
−π
F(t) sin kt dt.
(3.2.6)
Also note that ak −ibk = 2ck. If F(t) is real for t ∈R, then c−k = ¯ck.
We mention without proof the important Riemann–Lebesgue theorem,56, 57 by which
the Fourier coefﬁcients cn tend to zero as n →∞for any function that is integrable (in the
sense of Lebesgue), a fortiori for any periodic function that is continuous everywhere. A
ﬁnite number of ﬁnite jumps in each period are also allowed.
A function F(t) is said to be of bounded variation in an interval if, in this interval,
it can be expressed in the form F(t) = F1(t) −F2(t), where F1 and F2 are nondecreasing
56George Friedrich Bernhard Riemann (1826–1866), a German mathematician, made fundamental contributions
to analysis and geometry. In his habilitation lecture 1854 in Göttingen, Riemann introduced the curvature tensor
and laid the groundwork for Einstein’s general theory of relativity.
57Henri Léon Lebesgue (1875–1941), a French mathematician, created path-breaking general concepts of mea-
sure and integral.

3.2. More about Series
193
bounded functions. A ﬁnite number of jump discontinuities are allowed. The variation of
F over the interval [a, b] is denoted
 b
a |dF(t)|. If F is differentiable the variation of F
equals
 b
a |F ′(t)| dt.
Another classical result in the theory of Fourier series reads as follows: If F(t) is of
bounded variation in the closed interval [−π, π], then cn = O(n−1); see Titchmarsh [351,
Secs. 13.21, 13.73]. This result can be generalized as the following theorem.
Theorem 3.2.1.
Suppose that F (p) is of bounded variation on [−π, π], and that F (j) is continuous
everywhere for j < p. Denote the Fourier coefﬁcients of F (p)(t) by c(p)
n . Then
cn = (in)−pc(p)
n
= O(n−p−1).
(3.2.7)
Proof. The theorem follows from the above classical result, after the integration of the
formula for cn in (3.2.2) by parts p times.
Bounds for the truncation error of a Fourier series can also be obtained from this. The
details are left for Problem 3.2.4 (d), together with a further generalization. A similar result
is that cn = o(n−p) if F (p) is integrable, hence a fortiori if F ∈Cp.
In particular, we ﬁnd for p = 1 (since  n−2 is convergent) that the Fourier series
(3.2.2) converges absolutely and uniformly in R. It can also be shown that the Fourier series
is valid, i.e., the sum is equal to F(t).
3.2.2
The Cauchy–FFT Method
An alternative method for deriving coefﬁcients of power series when many terms are needed
is based on the following classic result. Suppose that the value f (z) of an analytic function
can be computed at any point inside and on the circle Cr = {z : |z −a| = r}, and set
M(r) = max |f (z)|,
z = a + reiθ ∈Cr.
Then the coefﬁcients of the Taylor expansion around a are determined by Cauchy’s
formula,
an =
1
2πi

Cr
f (z)
(z −a)(n+1) dz = r−n
2π
 2π
0
f (a + reiθ)e−niθ dθ.
(3.2.8)
For a derivation, multiply the Taylor expansion (3.1.3) by (z −a)−n−1, integrate term by
term over Cr, and note that
1
2πi

Cr
(z −a)j−n−1 dz = 1
2π
 2π
0
rj−ne(j−n)iθ dθ =
%
1
if j = n,
0
if j ̸= n.
(3.2.9)
From the deﬁnitions and (3.2.8) it follows that
|an| ≤r−nM(r).
(3.2.10)

194
Chapter 3. Series, Operators, and Continued Fractions
Further, with z′ = a + r′eiθ, 0 ≤r′ < r, we have
|Rn(z′)| ≤
∞

j=n
|aj(z′ −a)j| ≤
∞

j=n
r−jM(r)(r′)j = M(r)(r′/r)n
1 −r′/r
.
(3.2.11)
This form of the remainder term of a Taylor series is useful in theoretical studies, and also
for practical purpose, if the maximum modulus M(r) is easier to estimate than the nth
derivative.
Set 4θ = 2π/N, and apply the trapezoidal rule (see (1.1.12)) to the second integral
in (3.2.8). Note that the integrand has the same value for θ = 2π as for θ = 0. The terms
1
2f0 and 1
2fN that appear in the general trapezoidal rule can therefore in this case be replaced
by f0. Then
an ≈˜an ≡
1
Nrn
N−1

k=0
f (a + reik4θ)e−ink4θ,
n = 0 : N −1.
(3.2.12)
The approximate Taylor coefﬁcients ˜an, or rather the numbers a⋆
n = ˜anNrn, are here ex-
pressed as a case of the (direct) discrete Fourier transform (DFT). More generally, this
transform maps an arbitrary sequence {αk}N−1
0
to a sequence {a⋆
n}N−1
0
, by the following
equations:
a⋆
n =
N−1

k=0
αke−ink4θ,
n = 0 : N −1.
(3.2.13)
It will be studied more systematically in Sec. 4.6.2.
If N is a power of 2, it is shown in Sec. 4.7 that, given the N values αk, k = 0 : N −1,
and e−i4θ, no more than N log2 N complex multiplications and additions are needed for
the computation of all the N coefﬁcients a⋆
n, if an implementation of the DFT known as
the fast Fourier transform (FFT) is used. This makes our theoretical considerations very
practical.
It is also shown in Sec. 4.7 that the inverse of the DFT(3.2.13) is given by the formulas
αk = (1/N)
N−1

n=0
a⋆
neink4θ,
k = 0 : N −1.
(3.2.14)
This looks almost like the direct DFT (3.2.13), except for the sign of i and the factor 1/N.
It can therefore also be performed by means of O(N log N) elementary operations, instead
of the O(N3) operations that the most obvious approach to this task would require (i.e., by
solving the linear system (3.2.13)).
In our context, i.e., the computation of Taylor coefﬁcients, we have, by (3.2.12) and
the line after that equation,
αk = f (a + reik4θ),
a⋆
n = ˜anNrn.
(3.2.15)
Set zk = a + reik4θ. Using (3.2.15), the inverse transformation then becomes58
f (zk) =
N−1

n=0
˜an(zk −a)n,
k = 0 : N −1.
(3.2.16)
58One interpretation of these equations is that the polynomial N−1
n=0 ˜an(z −a)n is the solution of a special,
although important, interpolation problem for the function f , analytic inside a circle in C.

3.2. More about Series
195
Since the Taylor coefﬁcients are equal to f (n)(a)/n!, this is de facto a method for
the accurate numerical differentiation of an analytic function.59 If r and N are chosen
appropriately, it is more well-conditioned than most methods for numerical differentiation,
such as the difference approximations mentioned in Chapter 1; see also Sec. 3.3. It requires,
however, complex arithmetic for a convenient implementation. We call this the Cauchy–
FFT method for Taylor coefﬁcients and differentiation.
The question arises of how to choose N and r. Theoretically, any r less than the
radius of convergence ρ would do, but there may be trouble with cancellation if r is small.
On the other hand, the truncation error of the numerical integration usually increases with
r. Scylla and Charybdis situations60 like this are very common with numerical methods.
Typically it is the rounding error that sets the limit for the accuracy; it is usually not
expensive to choose r and N such that the truncation error becomes much smaller. A rule
of thumb for this situation is to guess a value of ˆN, i.e., how many terms will be needed in
the expansion, and then to try two values for N (powers of 2) larger than ˆN. If ρ is ﬁnite try
r = 0.9ρ and r = 0.8ρ, and compare the results. They may or may not indicate that some
other values of N and r should also be tried. On the other hand, if ρ = ∞try, for example,
r = 1 and r = 3, and compare the results. Again, the results indicate whether or not more
experiments should be done.
One can also combine numerical experimentation with a theoretical analysis of a more
or less simpliﬁed model, including a few elementary optimization calculations. The authors
take the opportunity to exemplify below this type of “hard analysis” on this question.
We ﬁrst derive two lemmas, which are important also in many other contexts. First
we have a discrete analogue of (3.2.9).
Lemma 3.2.2.
Let p and N be integers. Then
N−1

k=0
e2πipk/N = 0,
unless p = 0 or p is a multiple of N. In these exceptional cases every term equals 1, and
the sum equals N.
Proof. If p is neither zero nor a multiple of N, the sum is a geometric series, the sum of
which is equal to
(e2πip −1)/(e2πip/N −1) = 0.
The rest of the statement is obvious.
We next show an error estimate for the approximation provided by the trapezoidal
rule (3.2.12).
59The idea of using Cauchy’s formula and FFT for numerical differentiation seems to have been ﬁrst suggested
by Lyness and Moler [252]; see Henrici [194, Sec. 3].
60According to the American Heritage Dictionary, Scylla is a rock on the Italian side of the Strait of Messina,
opposite to the whirlpool Charybdis, personiﬁed by Homer (Ulysses) as a female sea monster who devoured sailors.
The problem is to navigate safely between them.

196
Chapter 3. Series, Operators, and Continued Fractions
Lemma 3.2.3.
Suppose that f (z) = ∞
0 an(z −a)n is analytic in the disk |z −a| < ρ. Let ˜an be
deﬁned by (3.2.12), where 0 < r < ρ. Then
˜an −an = an+N rN + an+2N r2N + an+3N r3N + · · · ,
0 ≤n < N.
(3.2.17)
Proof. Since 4θ = 2π/N,
˜an =
1
Nrn
N−1

k=0
e−2πink/N
∞

m=0
am

re2πik/Nm
=
1
Nrn
∞

m=0
amrm
N−1

k=0
e2πi(−n+m)k/N.
By the previous lemma, the inner sum of the last expression is zero, unless m −n is a
multiple of N. Hence (recall that 0 ≤n < N),
˜an =
1
Nrn

anrnN + an+N rn+NN + an+2N rn+2NN + · · ·

,
from which (3.2.17) follows.
Lemma 3.2.3 can, with some modiﬁcations, be generalized to Laurent series (and to
complex Fourier series); for example, (3.2.17) becomes
˜cn −cn = · · · cn−2Nr−2N + cn−Nr−N + cn+NrN + cn+2Nr2N · · · .
(3.2.18)
Let M(r) be the maximum modulus for the function f (z) on the circle Cr, and denote
by M(r)U an upper bound for the error of a computed function value f (z), |z| = r, where
U ≪1. Assume that rounding errors during the computation of ˜an are of minor importance.
Then, by (3.2.12), M(r)U/rn is a bound for the rounding error of ˜an. (The rounding errors
during the computation can be included by a redeﬁnition of U.)
Next we shall consider the truncation error of (3.2.12). First we estimate the coef-
ﬁcients that occur in (3.2.17) by means of max |f (z)| on a circle with radius r′; r′ > r,
where r is the radius of the circle used in the computation of the ﬁrst N coefﬁcients. Thus,
in (3.2.8) we substitute r′, j for r, n, respectively, and obtain the inequality
|aj| ≤M(r′)(r′)−j,
0 < r < r′ < ρ.
The actual choice of r′ strongly depends on the function f . (In rare cases we may choose
r′ = ρ.) Put this inequality into (3.2.17), where we shall choose r < r′ < ρ. Then
|˜an −an| ≤M(r′)

(r′)−n−NrN + (r′)−n−2Nr2N + (r′)−n−3Nr3N + · · ·

= M(r′)(r′)−n 
(r/r′)N + (r/r′)2N + (r/r′)3N + · · ·

= M(r′)(r′)−n
(r′/r)N −1.

3.2. More about Series
197
We make a digression here, because this is an amazingly good result. The trapezoidal rule
that was used in the calculation of the Taylor coefﬁcients is typically expected to have an
error that is O((4θ)2) = O(N−2). (As before, 4θ = 2π/N.) This application is, however,
a very special situation: a periodic analytic function is integrated over a full period. We
shall return to results like this in Sec. 5.1.4. In this case, for ﬁxed values of r, r′, the
truncation error is
O

(r/r′)N
= O

e−η/4θ
,
η > 0,
4θ →0 + .
(3.2.19)
This tends to zero faster than any power of 4θ!
It follows that a bound for the total error of ˜an, i.e., the sum of the bounds for the
rounding and the truncation errors, is given by
UM(r)r−n + M(r′)(r′)−n
(r′/r)N −1,
r < r′ < ρ.
(3.2.20)
Example 3.2.2 (Scylla and Charybdis in the Cauchy–FFT).
We shall discuss how to choose the parameters r and N so that the absolute error
bound of an, given in (3.2.20) becomes uniformly small for (say) n = 0 : ˆn. 1 + ˆn ≫1
is thus the number of Taylor coefﬁcients requested. The parameter r′ does not belong to
the Cauchy–FFT method, but it has to be chosen well in order to make the bound for the
truncation error realistic.
The discussion is rather technical, and you may omit it at a ﬁrst reading. It may,
however, be useful to study this example later, because similar technical subproblems occur
in many serious discussions of numerical methods that contain parameters that should be
appropriately chosen.
First consider the rounding error. By the maximum modulus theorem, M(r) is an
increasing function; hence, for r > 1, maxn M(r)r−n = M(r) > M(1). On the other hand,
for r ≤1, maxn M(r)r−n = M(r)r−ˆn; ˆn was introduced in the beginning of this example.
Let r∗be the value of r, for which this maximum is minimal. Note that r∗= 1 unless
M′(r)/M(r) = ˆn/r for some r ≤1.
Then try to determine N and r′ ∈[r∗, ρ) so that, for r = r∗, the bound for the
second term of (3.2.20) becomes much smaller than the ﬁrst term, i.e., the truncation error
is made negligible compared to the rounding error. This works well if ρ ≫r∗. In such
cases, we may therefore choose r = r∗, and the total error is then just a little larger than
UM(r∗)(r∗)−ˆn.
For example, if f (z) = ez, then M(r) = er, ρ = ∞. In this case r∗= 1 (since
ˆn ≫1). Then we shall choose N and r′ = N so that er′/((r′)N −1) ≪eU. One can show
that it is sufﬁcient to choose N ≫| ln U/ ln | ln U||. For instance, if U = 10−16, this is
satisﬁed with a wide margin by N = 32. In IEEE double precision arithmetic, the choice
r = 1, N = 32 gave an error less than 2·10−16. The results were much worse for r = 10 and
for r = 0.1; the maximum error of the ﬁrst 32 coefﬁcients became 4 · 10−4 and 9 · 1013(!),
respectively. In the latter case the errors of the ﬁrst eight coefﬁcients did not exceed 10−10,
but the rounding error of an, due to cancellations, increased rapidly with n.
If ρ is not much larger than r∗, the procedure described above may lead to a value
of N that is much larger than ˆn. In order to avoid this, we set ˆn = αN. We now conﬁne
the discussion to the case that r < r′ < ρ ≤1, n = 0 : ˆn. Then, with all other parameters

198
Chapter 3. Series, Operators, and Continued Fractions
ﬁxed, the bound in (3.2.20) is maximal for n = ˆn. We simplify this bound; M(r) is replaced
by the larger quantity M(r′), and the denominator is replaced by (r′/r)N. Then, for given
r′, α, N, we set x = (r/r′)N and determine x so that
M(r′)(r′)−αN(Ux−α + x)
is minimized. The minimum is obtained for x = (αU)1/(1+α), i.e., for r = r′x1/N, and the
minimum is equal to61
M(r′)(r′)−nU 1/(1+α)c(α),
where
c(α) = (1 + α)α−α/(1+α).
We see that the error bound contains the factor U 1/(1+α). This is proportional to 2 U 1/2
for α = 1, and to 1.65 U 4/5 for α = 1
4. The latter case is thus much more accurate, but for
the same ˆn one has to choose N four times as large, which leads to more than four times as
many arithmetic operations. In practice, ˆn is usually given, and the order of magnitude of U
can be estimated. Then α is to be chosen to make a compromise between the requirements
for a good accuracy and for a small volume of computation. If ρ is not much larger than
r∗, we may choose
N = ˆn/α,
x = (αU)1/(1+α),
r = r′x1/N.
Experiments were conducted with
f (z) = ln(1 −z),
for which ρ = 1, M(1) = ∞. Take ˆn = 64, U = 10−15, r′ = 0.999. Then M(r′) =
6.9. For α = 1, 1/2, 1/4, we have N = 64, 128, 256, respectively. The above theory
suggests r = 0.764, 0.832, 0.894, respectively. The theoretical estimates of the absolute
errors become 10−9, 2.4·10−12, 2.7·10−14, respectively. The smallest errors obtained in
experiments with these three values of α are 6·10−10, 1.8·10−12, 1.8·10−14, which were
obtained for r = 0.766, 0.838, 0.898, respectively. So, the theoretical predictions of these
experimental results are very satisfactory.
3.2.3
Chebyshev Expansions
The Chebyshev62 polynomials of the ﬁrst kind are deﬁned by
Tn(z) = cos(n arccos z),
n ≥0,
(3.2.21)
that is, Tn(z) = cos(nφ), where z = cos φ. From the well-known trigonometric formula
cos(n + 1)φ + cos(n −1)φ = 2 cos φ cos nφ
61This is a rigorous upper bound of the error for this value of r, in spite of simpliﬁcations in the formulation of
the minimization.
62Pafnuty Lvovich Chebyshev (1821–1894), Russian mathematician, pioneer in approximation theory and the
constructive theory of functions. His name has many different transcriptions, for example, Tschebyscheff. This
may explain why the polynomials that bear his name are denoted Tn(x). He also made important contributions to
probability theory and number theory.

3.2. More about Series
199
follows, by induction, the important recurrence relation: T0(z) = 1, T1(z) = z,
Tn+1(z) = 2zTn(z) −Tn−1(z),
(n ≥1).
(3.2.22)
Using this recurrence relation we obtain
T2(z) = 2z2 −1,
T3(z) = 4z3 −3z,
T4(z) = 8z4 −8z2 + 1,
T5(z) = 16z5 −20z3 + 5z,
T6(z) = 32z6 −48z + 18z2 −1, . . . .
Clearly Tn(z) is the nth degree polynomial,
Tn(z) = zn −
n
2

zn−2(1 −z2) +
n
4

zn−4(1 −z2)2 −· · · .
The Chebyshev polynomials of the second kind,
Un−1(z) =
1
n + 1T ′(z) = sin(nφ)
sin φ ,
φ = arccos z,
(3.2.23)
satisfy the same recurrence relation, with the initial conditions U−1(z) = 0, U0(z) = 1; its
degree is n −1. (When we write just “Chebyshev polynomial,” we refer to the ﬁrst kind.)
The Chebyshev polynomial Tn(x) has n zeros in [−1, 1] given by
xk = cos
2k −1
n
π
2

,
k = 1 : n,
(3.2.24)
the Chebyshev points, and n + 1 extrema
x′
k = cos
kπ
n

,
k = 0 : n.
(3.2.25)
These results follow directly from the fact that cos(nφ) = 0 for φ = (2k + 1)π/(2n), and
that cos(nφ) = ±1 for φ = kπ/n.
Note that from (3.2.21) it follows that |Tn(x)| ≤1 for x ∈[−1, 1], even though its
leading coefﬁcient is as large as 2n−1.
Example 3.2.3.
Figure 3.2.1 shows a plot of the Chebyshev polynomial T20(x) for x ∈[−1, 1].
Setting z = 1 in the recurrence relation (3.2.22) and using T0(1) = T1(1) = 1, it follows
that Tn(1) = 1, n ≥0. From T ′
0(1) = 0, T ′
1(1) = 1 and differentiating the recurrence
relation we get
T ′
n+1(z) = 2(zT ′
n(z) + Tn(z)) −T ′
n−1(z),
(n ≥1).
ItfollowseasilybyinductionthatT ′
n(1) = n2, i.e., outsidetheinterval[−1, 1]theChebyshev
polynomials grow rapidly.

200
Chapter 3. Series, Operators, and Continued Fractions
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 3.2.1. Graph of the Chebyshev polynomial T20(x), x ∈[−1, 1].
The Chebyshev polynomials have a unique minimax property. (For a use of this
property, see Example 3.2.4.)
Lemma 3.2.4 (Minimax Property).
The Chebyshev polynomials have the following minimax property: Of all nth degree
polynomials with leading coefﬁcient 1, the polynomial 21−nTn(x) has the smallest magnitude
21−n in [−1, 1].
Proof.
Suppose there were a polynomial pn(x), with leading coefﬁcient 1 such that
|pn(x)| < 21−n for all x ∈[−1, 1]. Let x′
k, k = 0 : n, be the abscissae of the extrema of
Tn(x). Then we would have
pn(x′
0) < 21−nTn(x′
0),
pn(x′
1) > 21−nTn(x′
1),
pn(x′
2) < 21−nTn(x′
2), . . . ,
etc., up to x′
n. From this it follows that the polynomial
pn(x) −21−nTn(x)
changes sign in each of the n intervals (x′
k, x′
k+1), k = 0 : n −1. This is impossible, since
the polynomial is of degree n −1. This proves the minimax property.
The Chebyshev expansion of a function f (z),
f (z) =
∞

j=0
cjTj(z),
(3.2.26)
is an important aid in studying functions on the interval [−1, 1]. If one is working with a
function f (t), t ∈[a, b], then one should make the substitution
t = 1
2(a + b) + 1
2(b −a)x,
(3.2.27)
which maps the interval [−1, 1] onto [a, b].

3.2. More about Series
201
Consider the approximation to the function f (x) = xn on [−1, 1] by a polynomial
of lower degree. From the minimax property of Chebyshev polynomials it follows that the
maximum magnitude of the error is minimized by the polynomial
p(x) = xn −21−nTn(x).
(3.2.28)
From the symmetry property Tn(−x) = (−1)nTn(x), it follows that this polynomial has in
fact degree n −2. The error 21−nTn(x) assumes its extrema 21−n in a sequence of n + 1
points, xi = cos(iπ/n). The sign of the error alternates at these points.
Suppose that one has obtained, for example, by Taylor series, a truncated power series
approximation to a function f (x). By repeated use of (3.2.28), the series can be replaced
by a polynomial of lower degree with a moderately increased bound for the truncation
error. This process, called economization of power series often yields a useful polynomial
approximation to f (x) with a considerably smaller number of terms than the original power
series.
Example 3.2.4.
If the series expansion cos x = 1 −x2/2 + x4/24 −· · · is truncated after the x4-term,
the maximum error is 0.0014 in [−1, 1]. Since T4(x) = 8x4 −8x2 + 1, it holds that
x4/24 ≈x2/24 −1/192
with an error which does not exceed 1/192 = 0.0052. Thus the approximation
cos x = (1 −1/192) −x2(1/2 −1/24) = 0.99479 −0.45833x2
has an error whose magnitude does not exceed 0.0052 + 0.0014 < 0.007. This is less than
one-sixth of the error 0.042, which is obtained if the power series is truncated after the
x2-term.
Note that for the economized approximation cos(0) is not approximated by 1. It may
not be acceptable that such an exact relation is lost. In this example one could have asked
for a polynomial approximation to (1 −cos x)/x2 instead.
If a Chebyshev expansion converges rapidly, the truncation error is, by and large,
determined by the ﬁrst few neglected terms. As indicated by Figures 3.2.1 and 3.2.5 (see
Problem 3.2.3), the error curve is oscillating with slowly varying amplitude in [−1, 1].
In contrast, the truncation error of a power series is proportional to a power of x. Note
that f (z) is allowed to have a singularity arbitrarily close to the interval [−1, 1], and the
convergence of the Chebyshev expansion will still be exponential, although the exponential
rate deteriorates, as R ↓1.
Important properties of trigonometric functions and Fourier series can be reformulated
in the terminology of Chebyshev polynomials. For example, they satisfy certain orthog-
onality relations; see Example 4.5.10. Also, results like (3.2.7), concerning how the rate
of decrease of the coefﬁcients or the truncation error of a Fourier series is related to the
smoothness properties of its sum, can be translated to Chebyshev expansions. So, even if
f is not analytic, its Chebyshev expansion converges under amazingly general conditions
(unlike a power series), but the convergence is much slower than exponential. A typical

202
Chapter 3. Series, Operators, and Continued Fractions
result reads as follows: if f ∈Ck[−1, 1], k > 0, there exists a bound for the truncation
error that decreases uniformly like O(n−k log n). Sometimes convergence acceleration can
be successfully applied to such series.
Set w = eiφ = cos φ + i sin φ, where φ and z = cos φ may be complex. Then
w = z ±

z2 −1,
z = cos φ = 1
2(w + w−1),
and
Tn(z) = cos nφ = 1
2(wn + w−n),
(3.2.29)

z +

z2 −1
n
= Tn(z) + Un−1(z)

z2 −1,
where Un−1(z) is the Chebyshev polynomials of the second kind; see (3.2.23). It follows that
the Chebyshev expansion (3.2.26) formally corresponds to a symmetric Laurent expansion,
g(w) = f
 1
2(w + w−1)

=
∞

−∞
ajwj,
a−j = aj =
%
1
2cj
if j > 0,
c0
if j = 0.
It can be shown by the parallelogram law that |z + 1| + |z −1| = |w| + |w|−1. Hence, if
R > 1, z = 1
2(w + w−1) maps the annulus {w : R−1 < |w| < R}, twice onto an ellipse ER,
determined by the relation
ER = {z : |z −1| + |z + 1| ≤R + R−1},
(3.2.30)
with foci at 1 and −1. The axes are, respectively, R + R−1 and R −R−1, and hence R is
the sum of the semiaxes.
Note that as R →1, the ellipse degenerates into the interval [−1, 1]. As R →∞, it
becomes close to the circle |z| < 1
2R. It follows from (3.2.29) that this family of confocal
ellipses are level curves of |w| = |z ±
√
z2 −1|. In fact, we can also write
ER =
!
z : 1 ≤|z +

z2 −1| ≤R
"
.
(3.2.31)
Theorem 3.2.5 (Bernštein’s Approximation Theorem).
Let f (z) be real-valued for z ∈[−1, 1], analytic and single-valued for z ∈ER, R > 1.
Assume that |f (z)| ≤M for z ∈ER. Then63
(((f (x) −
n−1

j=0
cjTj(x)
((( ≤2MR−n
1 −1/R
for x ∈[−1, 1].
Proof. Set as before z = 1
2(w + w−1), g(w) = f ( 1
2(w + w−1)). Then g(w) is analytic in
the annulus R−1 + ϵ ≤|w| ≤R −ϵ, and hence the Laurent expansion (3.2.1) converges
there. In particular it converges for |w| = 1, hence the Chebyshev expansion for f (x)
converges when x ∈[−1, 1].
63A generalization to complex values of x is formulated in Problem 3.2.11.

3.2. More about Series
203
Set r = R −ϵ. By Cauchy’s formula we obtain, for j > 0,
|cj| = 2|aj| =
((( 2
2πi

|w|=r
g(w)w−(j+1)dw
((( ≤2
2π
 2π
0
Mr−j−1rdφ = 2Mr−j.
We then obtain, for x ∈[−1, 1],
(((f (x) −
n−1

j=0
cjTj(x)
((( =
(((
∞

n
cjTj(x)
((( ≤
∞

n
|cj| ≤2M
∞

n
r−j ≤2M
r−n
1 −1/r .
This holds for any ϵ > 0. We can here let ϵ →0 and thus replace r by R.
The Chebyshev polynomials are perhaps the most important example of a family of
orthogonal polynomials; see Sec. 4.5.5. The numerical value of a truncated Chebyshev
expansion can be computed by means of Clenshaw’s algorithm; see Theorem 4.5.21.
3.2.4
Perturbation Expansions
In the equations of applied mathematics it is often possible to identify a small dimensionless
parameter (say) ϵ, ϵ ≪1. The case when ϵ = 0 is called the reduced problem or the
unperturbed case, and one asks for a perturbation expansion, i.e., an expansion of the
solution of the perturbed problem into powers of the perturbation parameter ϵ. In many
cases it can be proved that the expansion has the form c0 + c1ϵ + c2ϵ2 + · · · , but there are
also important cases where the expansion contains fractional or a few negative powers.
In this subsection, we consider an analytic equation φ(z, ϵ) = 0 and seek expansions
for the roots zi(ϵ) in powers of ϵ. This has some practical interest in its own right, but it is
mainly to be considered as a preparation for more interesting applications of perturbation
methods to more complicated problems. A simple perturbation example for a differential
equation is given in Problem 3.2.9.
If zi(0) is a simple root, i.e., if ∂φ/∂z ̸= 0, for (z, ϵ) = (zi(0), 0), then a theorem of
complex analysis tells us that zi(ϵ) is an analytic function in a neighborhood of the origin.
Hence the expansion
zi(ϵ) −zi(0) = c1ϵ + c2ϵ2 + · · ·
has a positive (or inﬁnite) radius of convergence. We call this a regular perturbation
problem. The techniques of power series reversion, presented in Sec. 3.1.4, can often be
applied after some preparation of the equation. Computer algebra systems are also used in
perturbation problems, if expansions with many terms are needed.
Example 3.2.5.
We shall expand the roots of
φ(z, ϵ) ≡ϵz2 −z + 1 = 0
into powers of ϵ. The reduced problem −z + 1 = 0 has only one ﬁnite root, z1(0) = 1. Set
z = 1 + xϵ, x = c1 + c2ϵ + c3ϵ2 + · · · . Then φ(1 + xϵ, ϵ)/ϵ = (1 + xϵ)2 −x = 0, i.e.,
(1 + c1ϵ + c2ϵ2 + · · ·)2 −(c1 + c2ϵ + c3ϵ2 + · · ·) = 0.

204
Chapter 3. Series, Operators, and Continued Fractions
Matching the coefﬁcients of ϵ0, ϵ1, ϵ2, we obtain the system
1 −c1 = 0 ⇒c1 = 1,
2c1 −c2 = 0 ⇒c2 = 2,
2c2 + c2
1 −c3 = 0 ⇒c3 = 5;
hence z1(ϵ) = 1 + ϵ + 2ϵ2 + 5ϵ3 + · · · .
Now, the easiest way to obtain the expansion for the second root, z2(ϵ), is to use
the fact that the sum of the roots of the quadratic equation equals ϵ−1; hence z2(ϵ) =
ϵ−1 −1 −ϵ −2ϵ2 + · · · .
Note the appearance of the term ϵ−1. This is due to a characteristic feature of this
example. The degree of the polynomial is lower for the reduced problem than it is for ϵ ̸= 0;
one of the roots escapes to ∞as ϵ →0. This is an example of a singular perturbation
problem, an important type of problem for differential equations; see Problem 3.2.7.
If ∂φ/∂z = 0, for some zi, the situation is more complicated; zi is a multiple root, and
the expansions look different. If zi(0) is a k-fold root, then there may exist an expansion of
the form
zi(ϵ) = c0 + c1ϵ1/k + c2(ϵ1/k)2 + · · ·
for each of the k roots of ϵ, but this is not always the case. See (3.2.32) below, where the
expansions are of a different type. If one tries to determine the coefﬁcients in an expansion
of the wrong form, one usually runs into contradictions, but the question about the right
form of the expansions still remains.
The answers are given by the classical theory of algebraic functions, where Riemann
surfaces and Newton polygons are two of the key concepts; see, e.g., Bliss [35]. We shall,
for several reasons, not use this theory here. One reason is that it seems hard to generalize
some of the methods of algebraic function theory to more complicated equations, such as
differential equations. We shall instead use a general balancing procedure, recommended
in Lin and Segel [246, Sec. 9.1], where it is applied to singular perturbation problems for
differential equations too.
The basic idea is very simple: each term in an equation behaves like some power of
ϵ. The equation cannot hold unless there is a β such that a pair of terms of the equation
behave like Aϵβ (with different values of A), and the ϵ-exponents of the other terms are
larger than or equal to β. (Recall that larger exponents make smaller terms.)
Let us return to the previous example. Although we have already determined the
expansion for z2(ϵ) (by a trick that may not be useful for problems other than single analytic
equations), we shall use this task to illustrate the balancing procedure. Suppose that
z2(ϵ) ∼Aϵα,
(α < 0).
The three terms of the equation ϵz2 −z + 1 = 0 then get the exponents
1 + 2α,
α,
0.
Try the ﬁrst two terms as the candidates for being the dominant pair. Then 1 + 2α = α,
hence α = −1. The three exponents become −1, −1, 0. Since the third exponent is larger

3.2. More about Series
205
than the exponent of the candidates, this choice of pair seems possible, but we have not
shown that it is the only possible choice.
Now try the ﬁrst and the third terms as candidates. Then 1 + 2α = 0, hence α = −1
2.
The exponent of the noncandidate is −1
2 ≤0; this candidate pair is thus impossible. Finally,
try the second and the third terms. Then α = 0, but we are only interested in negative values
of α.
The conclusion is that we can try coefﬁcient matching in the expansion z2(ϵ) =
c−1ϵ−1 + c0 + c1ϵ + · · · . We don’t need to do it, since we know the answer already, but it
indicates how to proceed in more complicated cases.
Example 3.2.6.
First consider the equation z3 −z2 + ϵ = 0. The reduced problem z3 −z2 = 0 has a
single root, z1 = 1, and a double root, z2,3 = 0. No root has escaped to ∞. By a similar
coefﬁcient matching as in the previous example we ﬁnd that z1(ϵ) = 1−ϵ −2ϵ2 +· · · . For
the double root, set z = Aϵβ, β > 0. The three terms of the equation obtain the exponents
3β, 2β, 1. Since 3β is dominated by 2β we conclude that 2β = 1, i.e., β = 1/2,
z2,3(ϵ) = c0ϵ1/2 + c1ϵ + c2ϵ3/2 + · · · .
By matching the coefﬁcients of ϵ, ϵ3/2, ϵ2, we obtain the system
−c2
0 + 1 = 0 ⇒c0 = ±1,
−2c0c1 + c3
0 = 0 ⇒c1 = 1
2,
−2c0c2 −c2
1 + 2c2
0c1 + c1c2
0 = 0 ⇒c2 = ±5
8;
hence z2,3(ϵ) = ±ϵ1/2 + 1
2ϵ ± 5
8ϵ3/2 + · · · .
There are, however, equations with a double root, where the perturbed pair of roots
do not behave like ±c0ϵ1/2 as ϵ →0. In such cases the balancing procedure may help.
Consider the equation
(1 + ϵ)z2 + 4ϵz + ϵ2 = 0.
(3.2.32)
The reduced problem is z2 = 0, with a double root. Try z ∼Aϵα, α > 0. The exponents
of the three terms become 2α, α + 1, 2. We see that α = 1 makes the three exponents
all equal to 2; this is ﬁne. So, set z = ϵy. The equation reads, after division by ϵ2,
(1 + ϵ)y2 + 4y + 1 = 0, hence y(0) = a ≡−2 ±
√
3. Coefﬁcient matching yields the
result
z = ϵy = aϵ + (−a2/(2(a + 2)))ϵ2 + · · · ,
where all exponents are natural numbers.
If ϵ is small enough, the last term included can serve as an error estimate. A more
reliable error estimate (or even an error bound) can be obtained by inserting the truncated
expansion into the equation. It shows that the truncated expansion satisﬁes a modiﬁed
equation exactly. The same idea can be applied to equations of many other types; see
Problem 3.2.9.

206
Chapter 3. Series, Operators, and Continued Fractions
3.2.5
Ill-Conditioned Series
Slow convergence is not the only numerical difﬁculty that occurs in connection with inﬁnite
series. There are also series with oscillating terms and a complicated type of catastrophic
cancellation. The size of some terms is many orders of magnitude larger than the sum of
the series. Small relative errors in the computation of the large terms lead to a large relative
error in the result. We call such a series ill-conditioned.
Such series have not been subject to many systematic investigations. One simply
tries to avoid them. For the important “special functions” of applied mathematics, such as
Bessel functions, conﬂuent hypergeometric functions, etc., there usually exist expansions
into descending powers of z that can be useful, when |z| ≫1 and the usual series, in
ascending powers, are divergent or ill-conditioned. Another possibility is to use multiple
precision in computations with ill-conditioned power series; this is relatively expensive and
laborious (but the difﬁculties should not be exaggerated). There are, however, also other,
less well known possibilities that will now be exempliﬁed. The subject is still open for
fresh ideas, and we hope that the following pages and the related problems at the end of the
section will stimulate some readers to think about it.
First, we shall consider power series of the form
∞

n=0
(−x)ncn
n!
,
(3.2.33)
where x ≫1, although not so large that there is risk for overﬂow. We assume that the
coefﬁcients cn are positive and slowly varying (relative to (−x)n/n!). The ratio of two
consecutive terms is
cn+1
cn
−x
n + 1 ≈
−x
n + 1.
We see that the series converges for all x, and that the magnitude increases if and only
if n + 1 < |x|. The term of largest magnitude is thus obtained for n ≈|x|. Denote its
magnitude by M(x). Then, for x ≫1, the following type of approximations can be used
for crude estimates of the number of terms needed and the arithmetic precision that is to be
used in computations related to ill-conditioned power series: M(x) ≈cxex(2πx)−1/2; i.e.,
log10 M(x)/c0 ≈0.43x −1
2 log10(2πx).
(3.2.34)
This follows from the classical Stirling’s formula,
x! ∼
x
e
x √
2πx
0
1 +
1
12x +
1
288x2 + · · ·
1
,
x ≫1,
(3.2.35)
that gives x! with a relative error that is about 1/(12x). You ﬁnd a proof of this in most
textbooks on calculus. It will be used often in the rest of this book. A more accurate and
general version is given in Example 3.4.12 together with a few more facts about the gamma
function, T(z), an analytic function that interpolates the factorial T(n + 1) = n! if n is a
natural number. Sometimes the notation z! is used instead of T(z + 1) even if z is not an
integer.

3.2. More about Series
207
There exist preconditioners, i.e., transformations that can convert classes of ill-
conditionedpowerseries(withaccuratelycomputablecoefﬁcients)tomorewell-conditioned
problems. One of the most successful preconditioners known to the authors is the following:
∞

n=0
(−x)ncn
n!
= e−x
∞

n=0
xnbn
n! ,
bn = (−4)nc0.
(3.2.36)
Ahint for proving this identity is given in Problem 3.3.22. The notation 4ncn for high order
differences was introduced in Sec. 1.1.4.
For the important class of sequences {cn} which are completely monotonic, (−4)nc0
is positive and smoothly decreasing; see Sec. 3.4.4.
Example 3.2.7.
Consider the function
F(x) = 1
x
 x
0
1 −e−t
t
dt = 1 −
x
22 · 1! +
x2
32 · 2! −· · · ,
i.e., F(x) is a particular case of (3.2.33) with cn = (n+1)−2. We shall look at three methods
of computing F(x) for x = 10 : 10 : 50, named A, B, C. F(x) decreases smoothly from
0.2880 to 0.0898. The computed values of F(x) are denoted FA(x), FB(x), FC(x).
The coefﬁcients cn, n = 0 : 119, are given in IEEE ﬂoating-point, double precision.
The results in Table 3.2.1 show that (except for x = 50) 120 terms is much more than
necessary for the rounding of the coefﬁcients to become the dominant error source.
Table 3.2.1. Results of three ways to compute F(x) = (1/x)
 x
0 (1/t)(1 −e−t) dt.
x
10
20
30
40
50
F(x) ≈
0.2880
0.1786
0.1326
0.1066
0.0898
lasttermA
1 · 10−82
8 · 10−47
7 · 10−26
6 · 10−11
2 · 101
M(x; A)
3 · 101
1 · 105
9 · 108
1 · 1013
1 · 1017
|FA(x) −F(x)|
2 · 10−15
5 · 10−11
2 · 10−7
3 · 10−3
2 · 101
lasttermB
4 · 10−84
1 · 10−52
4 · 10−36
2 · 10−25
2 · 10−18
M(x; B)
4 · 10−2
2 · 10−2
1 · 10−2
7 · 10−3
5 · 10−3
|FC(x) −FB(x)|
7 · 10−9
2 · 10−14
6 · 10−17
0
1 · 10−16
(A) We use (3.2.33) without preconditioner. M(x; A) is the largest magnitude of the
terms of the expansion. M(x; A) · 10−16 gives the order of magnitude of the effect of the
rounding errors on the computed value FA(x). Similarly, the truncation error is crudely
estimated by lasttermA. See Figure 3.2.2. Since the largest term is 1013, it is no surprise that
the relative error of the sum is not better than 0.03, in spite of double precision ﬂoating-point
being used. Note the scale, and look also in the table.
(B) We use the preconditioner (3.2.36). In this example cn = (n + 1)−2. In Prob-
lem 3.3.3 (c) we ﬁnd the following explicit expressions, related to the series on the right-hand

208
Chapter 3. Series, Operators, and Continued Fractions
0
20
40
60
80
100
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1 x 10
12
Figure 3.2.2. Example 3.2.7(A): Terms of (3.2.33), cn = (n + 1)−2, x = 40, no
preconditioner.
side of the preconditioner for this example:
(−4)nc0 = (−4)ncm|m=0 = c0(−4)nx−2|x=1 =
c0
n + 1
n

k=0
1
k + 1,
F(x) = c0e−x
∞

n=0
xn
(n + 1)!
n

k=0
1
k + 1.
(3.2.37)
Note that (−4)mc0 is positive and smoothly decreasing.
Thelargesttermisthussmallerthanthesum, andtheseries(3.2.37)iswell-conditioned.
The largest term is now about 7·10−3 and the computed sum is correct to 16 decimal places.
Multiple precision is not needed here. It can be shown that if x ≫1, the mth term is
approximately proportional to the value at m of the normal probability density with mean
x and standard deviation equal to √x; note the resemblance to a Poisson distribution. The
terms of the right-hand side, including the factor e−x, become a so-called bell sum; see
Figure 3.2.3.
M(x; B) and lasttermB are deﬁned analogously to M(x; A) and lasttermA. The B-
values are very different from the A-values. In fact they indicate that all values of FB(x)
referred to in Table 3.2.1 give F(x) to full accuracy.
(C) The following expression for F(x),
xF(x) ≡
∞

n=1
(−x)n
nn!
= −γ −ln x −E1(x),
E1(x) =
 ∞
x
e−t
t
dt,
(3.2.38)
is valid for all x > 0; see [1, Sec. 5.1.11]. E1(x) is known as the exponential integral, and
γ = 0.57721 56649 01532 86061 . . .

3.2. More about Series
209
0
20
40
60
80
100
0
1
2
3
4
5
6
7 x 10
−3
Figure 3.2.3. Example 3.2.7(B): cn = (n + 1)−2, x = 40, with preconditioner in (3.2.36).
is the well-known Euler’s constant. In the next section, an asymptotic expansion for E1(x)
for x ≫1 is derived, the ﬁrst two terms of which are used here in the computation of F(x; C)
for Table 3.2.1.
E1(x) ≈e−x(x−1 −x−2),
x ≫1.
This approximation is the dominant part of the error of F(x; C); it is less than e−x2x−4.
F(x; C) gives full accuracy for (say) x > 25.
More examples of sequences, for which rather simple explicit expressions for the high
order differences are known, are given in Problem 3.3.3. Kummer’s conﬂuent hypergeo-
metric function M(a, b, x) was deﬁned in (3.1.17). We have
M(a, b, −x) = 1 +
∞

n=1
(−x)ncn
n!
,
cn = cn(a, b) = a(a + 1) . . . (a + n −1)
b(b + 1) . . . (b + n −1) .
In our context b > a > 0, n > 0. The oscillatory series for M(a, b, −x), x > 0, is
ill-conditioned if x ≫1.
ByProblem3.3.3, (−4)nc0(a, b) = cn(b−a, b) > 0, n > 0; hencethepreconditioner
(3.2.36) yields the equation
M(a, b, −x) = e−xM(b −a, b, x),
(3.2.39)
where the series on the right-hand side has positive terms, because b −a > 0, x > 0, and
is a well-conditioned bell sum. The mth term has typically a sharp maximum for m ≈x;
compare Figure 3.2.3. Equation (3.2.39), is in the theory of the conﬂuent hypergeometric
functions, known as Kummer’s ﬁrst identity. It is emphasized here because several func-
tions with famous names of their own are particular cases of the Kummer function. (Several
other particular cases are presented in Sec. 3.5.1 together with continued fractions.) These

210
Chapter 3. Series, Operators, and Continued Fractions
share the numerous useful properties of Kummer’s function, for example, the above iden-
tity; see the theory in Lebedev [240, Secs. 9.9–9.14]64 and the formulas in [1, Chap. 13,
particularly Table 13.6 of special cases]. An important example is the error function (see
Example 3.1.3) that can be expressed in terms of Kummer’s conﬂuent hypergeometric as
erf(x) =
2
√π
 x
0
e−t2 dt = 2x
√π M
1
2, 3
2, −x2

.
(3.2.40)
If we cannot ﬁnd explicit expressions for high-order differences, we can make a differ-
ence scheme by the recurrence 4m+1cn = 4mcn+1 −4mcn. Unfortunately the computation
of a difference scheme suffers from numerical instability. Suppose that the absolute errors
of the cn are bounded by ϵ. Then the absolute errors can become as large as 2ϵ in the ﬁrst dif-
ferences, 4ϵ in the second differences, etc. More generally, the absolute errors of (−4)mcn
can become as large as 2mϵ. (You will ﬁnd more about this in Examples 3.3.2 and 3.3.3.)
In connection with ill-conditioned series, this instability is much more disturbing than in
the traditional applications of difference schemes to interpolation where m is seldom much
larger than 10. Recall that m ≈x for the largest term of the preconditioned series. Thus, if
x > 53 even this term may not have any correct bit if IEEE double precision arithmetic is
used, and many terms are needed after this.
Therefore, during the computation of the new coefﬁcients (−4)mcn (only once for
the function F, and with double accuracy in the results), the old coefﬁcients cn must be
available with multiple accuracy, and multiple precision must be used in the computation of
their difference scheme. Otherwise, we cannot evaluate the series with decent accuracy for
much larger values of x than we could have done without preconditioning. Note, however,
that if satisfactory coefﬁcients have been obtained for the preconditioned series, double
precision is sufﬁcient when the series is evaluated for large values of x. (It is different for
method A above.)
Let F(x) be the function that we want to compute for x ≫1, where it is deﬁned
by an ill-conditioned power series F1(x). A more general preconditioner can be described
as follows. Try to ﬁnd a power series P(x) with positive coefﬁcients such that the power
series P (x)F1(x) has less severe cancellations than F1(x).
In order to distinguish between the algebraic manipulation and the numerical evalua-
tion of the functions deﬁned by these series, we introduce the indeterminate x and describe
a more general preconditioner as follows:
F∗
2(x) = P(x) · F1(x),
F2(x) = F ∗
2 (x)/P(x).
(3.2.41)
The second statement is a usual scalar evaluation (no boldface). Here P(x) may be evaluated
by some other method than the power series, if it is more practical. If P(x) = ex and F1(x)
is the series deﬁned by (3.2.33), then it can be shown that F2(x) is mathematically equivalent
to the right-hand side of (3.2.36). In these cases F2(x) has positive coefﬁcients.
If, however, F1(x) has a positive zero, this is also a zero of F ∗
2 (x), and hence it is
impossible that all coefﬁcients of the series F∗
2(x) have the same sign. Nevertheless, the
following example shows that the preconditioner (3.2.41) can sometimes be successfully
used in such a case too.
64Unfortunately, the formulation of Kummer’s ﬁrst identity in [240, eq. (9.11.2)] contains a serious sign error.

3.2. More about Series
211
Example 3.2.8.
The two functions
J0(x) =
∞

n=0
(−1)n (x2/4)n
(n!)2 ,
I0(x) =
∞

n=0
(x2/4)n
(n!)2
are examples of Bessel functions of the ﬁrst kind; I0 is nowadays called a modiﬁed Bessel
function. J0(x) is oscillatory and bounded, while I0(x) ∼ex/
√
2πx for x ≫1. Since all
coefﬁcients of I0 are positive, we shall set P = I0, F1 = J0, and try
F∗
2(x) = IJ(x) ≡I0(x) · J0(x),
F2(x) = F ∗
2 (x)/I0(x)
as a preconditioner for the power series for J0(x), which is ill-conditioned if x ≫1. In
Table 3.2.2, lines 2 and 7 are obtained from the fully accurate built-in functions for J0(x)
and I0(x). J(x; N1) is computed in IEEE double precision arithmetic from N1 terms of
the above power series for J0(x). N1 = N1(x) is obtained by a termination criterion that
should give full accuracy or, if the estimate of the effect of the rounding error is bigger than
10−16, the truncation error should be smaller than this estimate. We omit the details; see
Problem 3.2.9 (d).
Table 3.2.2. Evaluation of some Bessel functions.
1
x
10
20
30
40
50
2
J0(x) ≈
−2 · 10−1
2·10−1
−9 · 10−2
7 · 10−3
6 · 10−2
3
N1(x)
26
41
55
69
82
4
J(x; N1) −J0(x)
9 · 10−14
3 · 10−10
−2 · 10−6
−1 · 10−1
−2 · 102
5
N2(x)
16
26
36
46
55
6
IJ(x; N2) ≈
−7 · 102
7 · 106
−7 · 1010
1 · 1014
2 · 1019
7
I0(x) ≈
3 · 103
4 · 107
8 · 1011
1 · 1016
3 · 1020
8
IJ(x)/I0(x) −J0(x)
3 · 10−17
2 · 10−14
3 · 10−13
−5 · 10−12
2 · 10−10
The coefﬁcients of IJ(x) are obtained from the second expression for γm given in
Problem 3.2.9 (c). N2 = N2(x) is the number of terms used in the expansion of IJ(x), by
a termination criterion similar to the one described for J(x; N1). Compared to line 4, line
8 is a remarkable improvement, obtained without the use of multiple precision.
For series of the form
∞

n=0
an
(−x2)n
(2n)!
one can generate a preconditioner from P(x) = cosh x. This can also be applied to J0(x)
and other Bessel functions; see Problem 3.2.9 (e).
There are several procedures for transforming a series into an integral that can then
be computed by numerical integration or be expanded in another series that may have better
convergence or conditioning properties. An integral representation may also provide an

212
Chapter 3. Series, Operators, and Continued Fractions
analytic continuation of the function represented by the original series. Integral representa-
tions may be obtained in several different ways; we mention two of these. Either there exist
integral representations of the coefﬁcients,65 or one can use general procedures in complex
analysis that transform series into integrals. They are due to Cauchy, Plana, and Lindelöf;
see Dahlquist [87].
3.2.6
Divergent or Semiconvergent Series
That a series is convergent is no guarantee that it is numerically useful. In this section, we
shall see examples of the reverse situation: a divergent series can be of use in numerical
computations.
This sounds strange, but it refers to series where the size of the terms
decreases rapidly at ﬁrst and increases later, and where an error bound (see Figure 3.2.4),
can be obtained in terms of the ﬁrst neglected term. Such series are sometimes called
semiconvergent.66 An important subclass are the asymptotic series; see below.
Example 3.2.9.
We shall derive a semiconvergent series for the computation of Euler’s function
f (x) = exE1(x) = ex
 ∞
x
e−tt−1 dt =
 ∞
0
e−u(u + x)−1 du
for large values of x. (The second integral was obtained from the ﬁrst by the substitution
t = u + x.) The expression (u + x)−1 should ﬁrst be expanded in a geometric series with
remainder term, valid even for u > x,
(u + x)−1 = x−1(1 + x−1u)−1 = x−1
n−1

j=0
(−1)jx−juj + (−1)n(u + x)−1(x−1u)n.
We shall frequently use the well-known formula
 ∞
0
uje−u du = j! = T(j + 1).
We write f (x) = Sn(x) + Rn(x), where
Sn(x) = x−1
n−1

j=0
(−1)jx−j
 ∞
0
uje−udu = 1
x −1!
x2 + 2!
x3 −· · · + (−1)n−1 (n −1)!
xn
,
Rn(x) = (−1)n
 ∞
0
(u + x)−1u
x
n
e−udu.
The terms in Sn(x) qualitatively behave as in Figure 3.2.4. The ratio between the last
term in Sn+1 and the last term in Sn is
−n!
xn+1
xn
(n −1)! = −n
x ,
(3.2.42)
65For hypergeometric or conﬂuent hypergeometric series see Lebedev [240, Secs. 9.1 and 9.11] or [1, Secs. 15.3
and 13.2].
66A rigorous theory of semiconvergent series was developed by Stieltjes and Poincaré in 1886.

3.2. More about Series
213
2
4
6
8
10
12
14
16
18
20
−1.5
−1
−0.5
0
0.5
1
1.5 x 10
−3
n
Error estimate
Figure 3.2.4. Error estimates of the semiconvergent series of Example 3.2.9 for
x = 10; see (3.2.43).
and since the absolute value of that ratio for ﬁxed x is unbounded as n →∞, the sequence
{Sn(x)}∞
n=1 diverges for every positive x. But since sign Rn(x) = (−1)n for x > 0, it follows
fromTheorem 3.1.4 that
f (x) = 1
2

Sn(x) + Sn+1(x)

± 1
2
n!
xn+1 .
(3.2.43)
The idea is now to choose n so that the estimate of the remainder is as small as possible.
According to (3.2.42), this happens when n is equal to the integer part of x. For x = 5 we
choose n = 5,
S5(5) = 0.2 −0.04 + 0.016 −0.0096 + 0.00768 = 0.17408,
S6(5) = S5(5) −0.00768 = 0.16640,
which gives f (5) = 0.17024 ± 0.00384. The correct value is 0.17042, so the actual error
is only 5% of the error bound. For n = x = 10, the error estimate is 1.0144 · 10−5.
For larger values of x the accuracy attainable increases. One can show that the bound
for the relative error using the above computational scheme decreases approximately as
(π·x/2)1/2e−x, an extremely good accuracy for large values of x, if one stops at the smallest
term. It can even be improved further, by the use of the convergence acceleration techniques
presented in Sec. 3.4, notably the repeated averages algorithm, also known as the Euler
transformation; see Sec. 3.4.3. The algorithms for the transformation of a power series into
a rapidly convergent continued fraction, mentioned in Sec. 3.5.1, can also be successfully
applied to this example and to many other divergent expansions.
One can derive the same series expansion as above by repeated integration by parts.
This is often a good way to derive numerically useful expansions, convergent or semi-

214
Chapter 3. Series, Operators, and Continued Fractions
convergent, with a remainder in the form of an integral. For convenient reference, we
formulate this as a lemma that is easily proved by induction and the mean value theorem of
integral calculus. See Problem 3.2.10 for applications.
Lemma 3.2.6 (Repeated Integration by Parts).
Let F ∈Cp(a, b), let G0 be a piecewise continuous function, and let G0, G1, . . .
be a sequence of functions such that G′
j+1(x) = Gj(x) with suitably chosen constants of
integration. Then
 b
a
F(t)G0(t) dt =
p−1

j=0
(−1)jF (j)(t)Gj+1(t)
(((
b
t=a + (−1)p
 b
a
F (p)(t)Gp(t) dt.
The sum is the “expansion,” and the last integral is the “remainder.” If Gp(t) has a constant
sign in (a, b), the remainder term can also be written in the form
(−1)pF (p)(ξ)(Gp+1(b) −Gp+1(a)),
ξ ∈(a, b).
The expansion in Lemma 3.2.6 is valid as an inﬁnite series, if and only if the remainder
tends to 0 as p →∞. Even if the sum converges as p →∞, it may converge to the wrong
result.
The series in Example 3.2.9 is an expansion in negative powers of x, with the property
that for all n, the remainder, when x →∞, approaches zero faster than the last included
term. Such an expansion is said to represent f (x) asymptotically as x →∞. Such
an asymptotic series can be either convergent or divergent (semiconvergent). In many
branches of applied mathematics, divergent asymptotic series are an important aid, though
they are often needlessly surrounded by an air of mysticism.
It is important to appreciate that an asymptotic series does not deﬁne a sum uniquely.
For example f (x) = e−x is asymptotically represented by the series  j = 0∞0 · x−j, as
x →∞. Thus e−x (and many other functions) can be added to the function for which the
expansion was originally obtained.
Asymptotic expansions are not necessarily expansions into negative powers of x. An
expansion into positive powers of x −a,
f (x) ∼
n−1

ν=0
cν(x −a)ν + Rn(x),
represents f (x) asymptotically when x →a if
lim
x→a(x −a)−(n−1)Rn(x) = 0.
Asymptotic expansions of the error of a numerical method into positive powers of a step
length h are of great importance in the more advanced study of numerical methods. Such
expansions form the basis of simple and effective acceleration methods for improving nu-
merical results; see Sec. 3.4.

Problems and Computer Exercises
215
Review Questions
3.2.1 Give the Cauchy formula for the coefﬁcients ofTaylor and Laurent series, and describe
the Cauchy–FFT method. Give the formula for the coefﬁcients of a Fourier series.
For which of the functions in Table 3.1.1 does another Laurent expansion also exist?
3.2.2 Describe by an example the balancing procedure that was mentioned in the subsection
about perturbation expansions.
3.2.3 Deﬁne the Chebyshev polynomials, and tell some interesting properties of these and
of Chebyshev expansions. For example, what do you know about the speed of con-
vergence of a Chebyshev expansion for various classes of functions? (The detailed
expressions are not needed.)
3.2.4 Describe and exemplify what is meant by an ill-conditioned power series and a pre-
conditioner for such a series.
3.2.5 (a) Deﬁne what is meant when one says that the series ∞
0 anx−n
• converges to a function f (x) for x ≥R;
• represents a function f (x) asymptotically as x →∞.
(b) Give an example of a series that represents a function asymptotically as x →∞,
although it diverges for every ﬁnite positive x.
(c) What is meant by semiconvergence? Say a few words about termination criteria
and error estimation.
Problems and Computer Exercises
3.2.1 Some of the functions appearing in Table 3.1.1 and in other examples and problems
are not single-valued in the complex plane. Brush up your complex analysis and
ﬁnd out how to deﬁne the branches, where these expansions are valid, and (if nec-
essary) deﬁne cuts in the complex plane that must not be crossed. It turns out not
to be necessary for these expansions. Why?
(a) If you have access to programs for functions of complex variables (or to com-
mands in some package for interactive computation), ﬁnd out the conventions used
for functions like square root, logarithm, powers, arc tangent, etc. If the manual
does not give enough detail, invent numerical tests, both with strategically chosen
values of z and with random complex numbers in some appropriate domain around
the origin. For example, do you obtain
ln
z + 1
z −1

−ln(z + 1) + ln(z −1) = 0
∀z?
Or, what values of
√
z2 −1 do you obtain for z = ±i? What values should you
obtain, if you want the branch which is positive for z > 1?

216
Chapter 3. Series, Operators, and Continued Fractions
(b)WhatdoyouobtainifyouapplyCauchy’scoefﬁcientformulaortheCauchy–FFT
method to ﬁnd a Laurent expansion for √z? Note that √z is analytic everywhere in
an annulus, but that does not help. The expansion is likely to become weird. Why?
3.2.2 Apply (on a computer) the Cauchy–FFT method to ﬁnd the Maclaurin coefﬁcients
an of (say) ez, ln(1 −z), and (1 + z)1/2. Conduct experiments with different values
of r and N, and compare with the exact coefﬁcients. This presupposes that you
have access to good programs for complex arithmetic and FFT.
Try to summarize your experiences of how the error of an depends on r, N. You
may ﬁnd some guidance in Example 3.2.2.
3.2.3 (a) Suppose that r is located inside the unit circle; t is real. Show that
1 −r2
1 −2r cos t + r2 = 1 + 2
∞

n=1
rn cos nt,
2r sin t
1 −2r cos t + r2 = 2
∞

n=1
rn sin nt.
Hint: First suppose that r is real. Set z = reit. Show that the two series are the real
and imaginary parts of (1 + z)/(1 −z). Finally, make an analytic continuation of
the results.
(b) Let a be positive, x ∈[−a, a], while w is complex, w /∈[−a, a]. Let r = r(w),
|r| < 1 be a root of the quadratic r2 −(2w/a)r + 1 = 0. Show that (with an
appropriate deﬁnition of the square root)
1
w −x =
1
√
w2 −a2 ·

1 + 2
∞

n=1
rnTn
x
a

,
(w /∈[−a, a], x ∈[−a, a]).
(c) Find the expansion of 1/(1 + x2) for x ∈[−1.5, 1.5] into the polynomials
Tn(x/1.5). Explain the order of magnitude of the error and the main features of the
error curve in Figure 3.2.5.
Hint: Set w = i, and take the imaginary part. Note that r becomes imaginary.
0
0.5
1
1.5
−3
−2
−1
0
1
2
3 x 10−4
Figure 3.2.5. The error of the expansion of f (x) = 1/(1 + x2) in a sum of
Chebyshev polynomials {Tn(x/1.5)}, n ≤12.

Problems and Computer Exercises
217
3.2.4 (a) Find the Laurent expansions for
f (z) = 1/(z −1) + 1/(z −2).
(b) How do you use the Cauchy–FFT method for ﬁnding Laurent expansions? Test
your ideas on the function in the previous subproblem (and on a few other functions).
There may be some pitfalls with the interpretation of the output from the FFT
program, related to so-called aliasing; see Sec. 4.6.6 and Strang [339].
(c) As in Sec. 3.2.1, suppose that F (p) is of bounded variation in [−π, π] and
denote the Fourier coefﬁcients of F (p) by c(p)
n . Derive the following generalization
of (3.2.7):
cn = (−1)n−1
2π
p−1

j=0
F (j)(π) −F (j)(−π)
(in)j+1
+ c(p)
n
(in)p .
Show that if we add the condition that F ∈Cj[−∞, ∞], j < p, then the asymptotic
results given in (and after) (3.2.7) hold.
(d) Let z = 1
2(w + w−1). Show that |z −1| + |z + 1| = |w| + |w|−1.
Hint: Use the parallelogram law, |p −q|2 + |p + q|2 = 2(|p|2 + |q|2).
3.2.5 (a) The expansion of arcsinh t into powers of t, truncated after t7, is obtained from
Problem 3.1.6 (b). Using economization of a power series, construct from this a
polynomial approximation of the form c1t + c3t3 for the interval t ∈[−1
2, 1
2]. Give
bounds for the truncation error for the original truncated expansion and for the
economized expansion.
(b) The graph of T20(x) for x ∈[−1, 1] is shown in Figure 3.2.1. Draw the graph
of T20(x) for (say) x ∈[−1.1, 1.1].
3.2.6 Compute a few terms of the expansions into powers of ϵ or k of each of the roots
of the following equations, so that the error is O(ϵ2) or O(k−2) (ϵ is small and
positive; k is large and positive). Note that some terms may have fractional or
negative exponents. Also try to ﬁt an expansion of the wrong form in some of these
examples, and see what happens.
(a) (1 + ϵ)z2 −ϵ = 0;
(b) ϵz3 −z2 + 1 = 0;
(c) ϵz3 −z + 1 = 0;
(d) z4 −(k2 + 1)z2 −k2 = 0, (k2 ≫1).
3.2.7 The solution of the boundary value problem
(1 + ϵ)y′′ −ϵy = 0,
y(0) = 0, y(1) = 1,
has an expansion of the form y(t; ϵ) = y0(t) + y1(t)ϵ + y2(t)ϵ2 + · · · .
(a) By coefﬁcient matching, set up differential equations and boundary conditions
for y0, y1, y2, and solve them. You naturally use the boundary conditions of the
original problem for y0.
Make sure you use the right boundary conditions for
y1, y2.
(b) Set R(t) = y0(t) + ϵy1(t) −y(t; ϵ). Show that R(t) satisﬁes the (modiﬁed)
differential equation
(1 + ϵ)R′′ −ϵR = ϵ2(7t −t3)/6,
R(0) = 0,
R(1) = 0.

218
Chapter 3. Series, Operators, and Continued Fractions
3.2.8 (a) Apply Kummer’s ﬁrst identity (3.2.39) to the error function erf(x), to show that
erf(x) = 2x
√π e−x2M

1, 3
2, x2
= 2x
√π e−x2
1 + 2x2
3 + (2x2)2
3 · 5 + (2x2)3
3 · 5 · 7 + · · ·

.
Why is this series well-conditioned? (Note that it is a bell sum; compare Fig-
ure 3.2.3.) Investigate the largest term, rounding errors, truncation errors, and
termination criterion.
(b) erfc(x) has a semiconvergent expansion for x ≫1 that begins
erfc(x) = 1 −erf(x) =
2
√π
 ∞
x
e−t2 dt = e−x2
x√π

1 −
1
2x2 +
3
4x4 −15
8x6 + · · ·

.
Give an explicit expression for the coefﬁcients, and show that the series diverges
for every x. Where is the smallest term? Estimate its size.
Hint: Set t2 = x2 + u, and proceed analogously to Example 3.2.8. See Prob-
lem 3.1.7 (c), α =
1
2, about the remainder term. Alternatively, apply repeated
integration by parts; it may be easier to ﬁnd the remainder in this way.
3.2.9 Other notations for series, with application to Bessel functions.
(a) Set
f (x) =
∞

n=0
anxn
n! ,
g(x) =
∞

n=0
bnxn
n! ,
h(x) =
∞

n=0
cnxn
n! ,
φ(w) =
∞

n=0
αnwn
n!n! ,
ψ(w) =
∞

n=0
βnwn
n!n! ,
χ(w) =
∞

n=0
γnwn
n!n! .
Let h(x) = f (x) · g(x), χ(w) = φ(w) · ψ(w). Show that
cn =
n

j=0
n
j

ajbn−j,
γn =
n

j=0
n
j
2
αjβn−j.
Derive analogous formulas for series of the form ∞
n=0 anwn/(2n)!.
Suggest how to divide two power series in these notations.
(b) Let aj = (−1)ja′
j, g(x) = ex. Show that
cn =
n

j=0
n
j

(−1)ja′
j.
Comment: By (3.2.1), this can also be written cn = (−1)n4na0. This proves the
mathematical equivalence of the preconditioners (3.1.55) and (3.1.59) if P(x) = ex.
(c) Set, according to Example 3.2.8 and part (a) of this problem, w = −x2/4,
J0(x) =
∞

n=0
(−1)nwn
n!n!
,
I0(x) =
∞

n=0
wn
n!n!,
IJ(x) ≡I0(x)J0(x) =
∞

n=0
γnwn
n!n! .

Problems and Computer Exercises
219
Show that
γn =
n

j=0
(−1)j
n
j

n
n −j

=
%
(−1)m2m
m

if n = 2m,
0
if n = 2m + 1.
Hint: The ﬁrst expression for γn follows from (a). It can be interpreted as the
coefﬁcient of tn in the product (1 −t)n(1 + t)n. The second expression for γn is the
same coefﬁcient in (1 −t2)n.
(d) The second expression for γn in (c) is used in Example 3.2.8.67 Reconstruct
and extend the results of that example. Design a termination criterion. Where
is the largest modulus of a term of the preconditioned series, and how large is
it approximately? Make a crude guess in advance of the rounding error in the
preconditioned series.
(e) Show that the power series of J0(x) can be written in the form
∞

n=0
an
(−x2)n
(2n)! ,
where an is positive and decreases slowly and smoothly.
Hint: Compute an+1/an.
(f) It is known (see Lebedev [240, eq. (9.13.11)]) that
J0(x) = e−ixM
1
2, 1; 2ix

,
where M(a, b, c) is Kummer’s conﬂuent hypergeometric function, this time with
an imaginary argument. Show that Kummer’s ﬁrst identity is unfortunately of no
use here for preconditioning the power series.
Comment: Most of the formulas and procedures in this problem can be generalized
to the series for the Bessel functions of the ﬁrst kind of general integer order,
(z/2)−nJn(x). These belong to the most studied functions of applied mathematics,
andthereexistmoreefﬁcientmethodsforcomputingthem; see, e.g., Pressetal.[294,
Chapter 6]. This problem shows, however, that preconditioning can work well for
a nontrivial power series, and it is worth being tried.
3.2.10. (a) Derive the expansion of Example 3.2.5 by repeated integration by parts.
(b) Derive the Maclaurin expansion with the remainder according to (3.1.5) by the
application of repeated integration by parts to the equation
f (z) −f (0) = z
 1
0
f ′(zt) d(t −1).
3.2.11. Show the following generalization of Theorem 3.2.5. Assume that |f (z)| ≤M for
z ∈ER. Let |ζ| ∈Eρ, 1 < ρ < r ≤R −ϵ. Then the Chebyshev expansion of f (ζ)
67It is much better conditioned than the ﬁrst expression. This may be one reason why multiple precision is not
needed here.

220
Chapter 3. Series, Operators, and Continued Fractions
satisﬁes the inequality
((((f (ζ) −
n−1

j=0
cjTj(ζ)
(((( ≤2M(ρ/R)n
1 −ρ/R .
Hint: Set ω = ζ +

ζ 2 −1, and show that |Tj(ζ)| = | 1
2(ωj + ω−j)| ≤ρj.
3.3
Difference Operators and Operator Expansions
3.3.1
Properties of Difference Operators
Difference operators are handy tools for the derivation, analysis, and practical application
of numerical methods for many problems for interpolation, differentiation, and quadrature
of a function in terms of its values at equidistant arguments. The simplest notations for
difference operators and applications to derivatives were mentioned in Sec. 1.1.4.
Let y denote a sequence {yn}. Then we deﬁne the shift operator E (or translation
operator) and the forward difference operator 4 by the relations
Ey = {yn+1},
4y = {yn+1 −yn};
E and 4 are thus operators which map one sequence to another sequence. Note, however,
that if yn is deﬁned for a ≤n ≤b only, then Eyb is not deﬁned, and the sequence Ey
has fewer elements than the sequence y. (It is therefore sometimes easier to extend the
sequences to inﬁnite sequences, for example, by adding zeros in both directions outside the
original range of deﬁnition.)
These operators are linear, i.e., if α, β are real or complex constants and if y, z are
two sequences, then E(αy + βz) = αEy + βEz, and similarly for 4.
Powers of E and 4 are deﬁned recursively, i.e.,
Eky = E(Ek−1y),
4ky = 4(4k−1y).
By induction, the ﬁrst relation yields Eky = {yn+k}. We extend the validity of this relation
to k = 0 by setting E0y = y and to negative values of k. 4ky is called the kth difference
of the sequence y. We make the convention that 40 = 1. There will be little use of 4k for
negative values of k in this book, although 4−1 can be interpreted as a summation operator.
Note that 4y = Ey −y, and Ey = y + 4y for any sequence y. It is therefore
convenient to express these as equations between operators:
4 = E −1,
E = 1 + 4.
The identity operator is in this context traditionally denoted by 1. It can be shown that all
formulas derived from the axioms of commutative algebra can be used for these operators,
for example, the binomial theorem for positive integral k,
4k = (E −1)k =
k

j=0
(−1)k−j
k
j

Ej,
Ek = (1 + 4)k =
k

j=0
k
j

4j,
(3.3.1)

3.3. Difference Operators and Operator Expansions
221
giving
(4ky)n =
k

j=0
(−1)k−j
k
j

yn+j,
yn+k = (Eky)n =
k

j=0
k
j

(4jy)n.
(3.3.2)
We abbreviate the notation further and write, for example, Eyn = yn+1 instead of (Ey)n =
yn+1, and 4kyn instead of (4ky)n. But it is important to remember that 4 operates on
sequences and not on elements of sequences. Thus, strictly speaking, this abbreviation is
incorrect, though convenient. The formula for Ek will, in the next subsection, be extended
to an inﬁnite series for nonintegral values of k, but that is beyond the scope of algebra.
A difference scheme consists of a sequence and its difference sequences, arranged in
the following way:
y0
4y0
y1
42y0
4y1
43y0
y2
42y1
44y0
4y2
43y1
y3
42y2
4y3
y4
A difference scheme is best computed by successive subtractions; the formulas in (3.3.1)
are used mostly in theoretical contexts.
Inmanyapplicationsthequantitiesyn arecomputedinincreasingordern = 0, 1, 2, . . . ,
and it is natural that a difference scheme is constructed by means of the quantities previously
computed. One therefore introduces the backward difference operator
∇yn = yn −yn−1 = (1 −E−1)yn.
For this operator we have
∇k = (1 −E−1)k,
E−k = (1 −∇)k.
(3.3.3)
Note the reciprocity in the relations between ∇and E−1.
Any linear combination of the elements yn, yn−1, . . . , yn−k can also be expressed as
a linear combination of yn, ∇yn, . . . , ∇kyn, and vice versa.68 For example,
yn + yn−1 + yn−2 = 3yn −3∇yn + ∇2yn,
because 1 + E−1 + E−2 = 1 + (1 −∇) + (1 −∇)2 = 3 −3∇+ ∇2. By reciprocity, we
also obtain yn + ∇yn + ∇2yn = 3yn −3yn−1 + yn−2.
68An analogous statement holds for the elements yn, yn+1, . . . , yn+k and forward differences.

222
Chapter 3. Series, Operators, and Continued Fractions
In this notation the difference scheme reads as follows.
y0
∇y1
y1
∇2y2
∇y2
∇3y3
y2
∇2y3
∇4y4
∇y3
∇3y4
y3
∇2y4
∇y4
y4
In the backward difference scheme the subscripts are constant along diagonals directed
upward (backward) to the right, while in the forward difference scheme subscripts are
constant along diagonals directed downward (forward). Note, for example, that ∇kyn =
4kyn−k. In a computer, a backward difference scheme is preferably stored as a lower
triangular matrix.
Example 3.3.1.
Part of the difference scheme for the sequence y = {. . . , 0, 0, 0, 1, 0, 0, 0, . . .} is
given below.
0
1
−7
0
1
−6
28
0
1
−5
21
0
1
−4
15
−56
1
−3
10
−35
1
−2
6
−20
70
−1
3
−10
35
0
1
−4
15
−56
0
−1
5
−21
0
1
−6
28
0
−1
7
This example shows the effect of a disturbance in one element on the sequence of the
higher differences. Because the effect broadens out and grows quickly, difference schemes
are useful in the investigation and correction of computational and other errors, so-called
difference checks. Notice that, since the differences are linear functions of the sequence,
a superposition principle holds. The effect of errors can thus be estimated by studying
simple sequences such as the one above.

3.3. Difference Operators and Operator Expansions
223
Example 3.3.2.
The following is a difference scheme for a ﬁve-decimal table of the function f (x) =
tan x, x ∈[1.30, 1.36], with step h = 0.01. The differences are given with 10−5 as unit.
x
y
∇y
∇2y
∇3y
∇4y
∇5y
∇6y
1.30
3.60210
14498
1.31
3.74708
1129
15627
140
1.32
3.90335
1269
26
16896
166
2
1.33
4.07231
1435
28
9
18331
194
11
1.34
4.25562
1629
39
19960
233
1.35
4.45522
1862
21822
1.36
4.67344
We see that the differences decrease roughly by a factor of 0.1—that indicates that the step
size has been chosen suitably for the purposes of interpolation, numerical quadrature, etc.
until the last two columns, where the rounding errors of the function values have a visible
effect.
Example 3.3.3.
For the sequence yn = (−1)n one ﬁnds easily that
∇yn = 2yn, ∇2yn = 4yn, . . . ,
∇kyn = 2kyn.
If the errors in the elements of the sequence are bounded by ϵ, it follows that the errors of
the kth differences are bounded by 2kϵ. A rather small reduction of this bound is obtained
if the errors are assumed to be independent random variables (cf. Problem 3.4.24).
It is natural also to consider difference operations on functions not just on sequences.
E and 4 map the function f onto functions whose values at the point x are
E f (x) = f (x + h),
4f (x) = f (x + h) −f (x),
wherehisthestepsize. Ofcourse, 4f dependsonh; insomecasesthisshouldbeindicatedin
the notation. One can, for example, write 4hf (x), or 4f (x; h). If we set yn = f (x0 +nh),
the difference scheme of the function with step size h is the same as for the sequence {yn}.
Again it is important to realize that, in this case, the operators act on functions, not on the
values of functions. It would be more correct to write f (x0 + h) = (Ef )(x0). Actually,
the notation (x0)Ef would be even more logical, since the insertion of the value of the
argument x0 is the last operation to be done, and the convention for the order of execution
of operators proceeds from right to left.69
69The notation [x0]f occurs, however, naturally in connection with divided differences; see Sec. 4.2.1.

224
Chapter 3. Series, Operators, and Continued Fractions
Note that no new errors are introduced during the computation of the differences,
but the effects of the original irregular errors, for example, rounding errors in y, grow
exponentially. Note that systematic errors, for example, truncation errors in the numerical
solution of a differential equation, often have a smooth difference scheme. For example, if
the values of y have been produced by the iterative solution of an equation, where x is a
parameter, with the same number of iterations for every x and y and the same algorithm for
the ﬁrst approximation, then the truncation error of y is likely to be a smooth function of x.
Difference operators are in many respects similar to differentiation operators. Let f
be a polynomial. By Taylor’s formula,
4f (x) = f (x + h) −f (x) = hf ′(x) + 1
2h2f ′′(x) + · · · .
We see from this that deg 4f = deg f −1. Similarly, for differences of higher order, if f
is a polynomial of degree less than k, then
4k−1f (x) = constant,
4pf (x) = 0 ∀p ≥k.
The same holds for backward differences.
The following important result can be derived directly from Taylor’s theorem with
the integral form of the remainder. Assume that all derivatives of f up to kth order are
continuous. If f ∈Ck,
4kf (x) = hkf (k)(ζ),
ζ ∈[x, x + kh].
(3.3.4)
Hence h−k4kf (x) is an approximation to f (k)(x); the error of this approximation ap-
proaches zero as h →0 (i.e., as ζ →x). As a rule, the error is approximately proportional
to h. We postpone the proof to Sec. 4.2.1, where it appears as a particular case of a theorem
concerning divided differences.
Even though difference schemes do not have the same importance today that they had
in the days of hand calculations or calculation with desk calculators, they are still important
conceptually, and we shall also see how they are still useful in practical computing. In a
computer it is more natural to store a difference scheme as an array, with yn, ∇yn, ∇2yn, . . . ,
∇kyn in a row (instead of along a diagonal).
Many formulas for differences are analogous to formulas for derivatives, though
usually more complicated. The following results are among the most important.
Lemma 3.3.1.
It holds that
4k(ax) = (ah −1)kax,
∇k(ax) = (1 −a−h)kax.
(3.3.5)
For sequences, i.e., if h = 1,
4k{an} = (a −1)k{an},
4k{2n} = {2n}.
(3.3.6)
Proof. Let c be a given constant. For k = 1 we have
4(cax) = cax+h −cax = caxah −cax = c(ah −1)ax.
The general result follows easily by induction. The backward difference formula is derived
in the same way.

3.3. Difference Operators and Operator Expansions
225
Lemma 3.3.2 (Difference of a Product).
4(unvn) = un4vn + 4un vn+1.
(3.3.7)
Proof. We have
4(unvn) = un+1vn+1 −unvn
= un(vn+1 −vn) + (un+1 −un)vn+1.
Compare the above result with the formula for differentials, d(uv) = udv +vdu. Note that
we have vn+1 (not vn) on the right-hand side.
Lemma 3.3.3 (Summation by Parts).
N−1

n=0
un4vn = uNvN −u0v0 −
N−1

n=0
4un vn+1.
(3.3.8)
Proof. (Compare with the rule for integration by parts and its proof!) Notice that
N−1

n=0
4wn = (w1 −w0) + (w2 −w1) + · · · + (wN −wN−1)
= wN −w0.
Use this on wn = unvn. From the result in Lemma 3.3.1 one gets after summation
uNvN −u0v0 =
N−1

n=0
un4vn +
N−1

n=0
4unvn+1,
and the result follows. (For an extension, see Problem 3.3.2 (d).)
3.3.2
The Calculus of Operators
Formal calculations with operators, using the rules of algebra and analysis, are often an
elegant means of assistance in ﬁnding approximation formulas that are exact for all polyno-
mials of degree less than (say) k, and they should therefore be useful for functions that can be
accurately approximated by such a polynomial. Our calculations often lead to divergent (or
semiconvergent) series, but the way we handle them can usually be justiﬁed by means of the
theory of formal power series, of which a brief introduction was given at the end of Sec. 3.1.5.
The operator calculations also provide error estimates, asymptotically valid as the step size
h →0. Rigorous error bounds can be derived by means of Peano’s remainder theorem in
Sec. 3.3.3.
Operator techniques are sometimes successfully used (see Sec. 3.3.4) in a way that is
hard, or even impossible, to justify by means of formal power series. It is then not trivial
to formulate appropriate conditions for the success and to derive satisfactory error bounds
and error estimates, but it can sometimes be done.

226
Chapter 3. Series, Operators, and Continued Fractions
We make a digression about terminology. More generally, the word operator is in
this book used for a function that maps a linear space S into another linear space S′. S
can, for example, be a space of functions, a coordinate space, or a space of sequences. The
dimension of these spaces can be ﬁnite or inﬁnite. For example, the differential operator
D maps the inﬁnite-dimensional space C1[a, b] of functions with a continuous derivative,
deﬁned on the interval [a, b], into the space C[a, b] of continuous functions on the same
interval.
In the following we denote by Pn the set of polynomials of degree less than n.70 Note
that Pn is an n-dimensional linear space for which {1, x, x2, . . . , xn−1} is a basis called the
power basis; the coefﬁcients (c1, c2, . . . , cn) are then the coordinates of the polynomial p
deﬁned by p(x) = n
i=1 cixi−1.
For simplicity, we shall assume that the space of functions on which the operators
are deﬁned is C∞(−∞, ∞), i.e., the functions are inﬁnitely differentiable on (−∞, ∞).
This sometimes requires (theoretically) a modiﬁcation of a function outside the bounded
interval, where it is interesting. There are techniques for achieving this, but they are beyond
the scope of this book. Just imagine that they have been applied.
We deﬁne the following operators:
Ef (x) = f (x + h)
Shift (or translation) operator,
4f (x) = f (x + h) −f (x)
Forward difference operator,
∇f (x) = f (x) −f (x −h)
Backward difference operator,
Df (x) = f ′(x)
Differentiation operator,
δf (x) = f (x + 1
2h) −f (x −1
2h)
Central difference operator,
µf (x) = 1
2

f (x + 1
2h) + f (x −1
2h)

Averaging operator.
Suppose that the values of f are given on an equidistant grid only, e.g., xj = x0 + jh,
j = −M : N (j is an integer). Set fj = f (xj). Note that δfj, δ3fj, . . . (odd powers) and
µfj cannot be exactly computed; they are available halfway between the grid points. (A
way to get around this is given later; see (3.3.45).) The even powers δ2fj, δ4fj, . . . and
µδfj, µδ3fj, . . . can be exactly computed. This follows from the formulas
µδf (x) = 1
2

f (x + h) −f (x −h)

,
µδ = 1
2(4 + ∇),
δ2 = 4 −∇.
(3.3.9)
Several other notations are in use. For example, in the study of difference methods for partial
differential equations D+h, D0h, and D−h are used instead of 4, µδ, and ∇, respectively.
An operator P is said to be a linear operator if
P(αf + βg) = αPf + βPg
holds for arbitrary complex constants α, β and arbitrary functions f, g. The above six
operators are all linear.
The operation of multiplying by a constant α is also a linear
operator.
70Some authors use similar notations to denote the set of polynomials of degree less than or equal to n.

3.3. Difference Operators and Operator Expansions
227
If P and Q are two operators, then their sum and product can be deﬁned in the
following way:
(P + Q)f = Pf + Qf,
(P −Q)f = Pf −Qf,
(PQ)f = P(Qf ),
(αP)f = α(Pf ),
P nf = P · P · · · Pf,
n factors.
Two operators are equal, P = Q, if Pf = Qf , for all f in the space of functions considered.
Notice that 4 = E −1. One can show that the following rules hold for all linear operators:
P + Q = Q + P,
P + (Q + R) = (P + Q) + R,
P(Q + R) = PQ + PR,
P(QR) = (PQ)R.
The above six operators, E, 4, ∇, hD, δ, and µ, and the combinations of them by these
algebraic operations make a commutative ring. Thus, PQ = QP holds for these operators,
and any algebraic identity that is generally valid in such rings can be used.
If S = Rn, S′ = Rm, and the elements are column vectors, then the linear operators
are matrices of size [m, n]. They generally do not commute.
If S′ = R or C, the operator is called a functional. Examples of functionals are, if
x0 denotes a ﬁxed (though arbitrary) point,
Lf = f (x0),
Lf = f ′(x0),
Lf =
 1
0
e−xf (x) dx,
 1
0
|f (x)|2 dx;
all except the last one are linear functionals.
There is a subtle distinction here. For example, E is a linear operator that maps a
function to a function. Ef is the function whose value at the point x is f (x + h). If we
consider a ﬁxed point x0, then (Ef )(x0) is a scalar. This is therefore a linear functional.
We shall allow ourselves to simplify the notation and to write Ef (x0), but it must be
understood that E operates on the function f , not on the function value f (x0). This was
just one example; simpliﬁcations like this will be made with other operators than E, and
similar simpliﬁcations in notation were suggested earlier in this chapter. There are, however,
situations where it is, for the sake of clarity, advisable to return to the more speciﬁc notation
with a larger number of parentheses.
If we represent the vectors in Rn by columns y, the linear functionals in Rn are the
scalar products aT x = n
i=1 aiyi; every row aT thus deﬁnes a linear functional.
Examples of linear functionals in Pk are linear combinations of a ﬁnite number of
function values, Lf =  ajf (xj). If xj = x0 + jh the same functional can be expressed
in terms of differences, e.g.,  a′
j4jf (x0); see Problem 3.3.4. The main purpose of this
section is to show how operator methods can be used for ﬁnding approximations of this form
to linear functionals in more general function spaces. First, we need a general theorem.

228
Chapter 3. Series, Operators, and Continued Fractions
Theorem 3.3.4.
Let x1, x2, . . . , xk be k distinct real (or complex) numbers.
Then no nontrivial
relation of the form
k

j=1
ajf (xj) = 0
(3.3.10)
can hold for all f ∈Pk. If we add one more point (x0), there exists only one nontrivial
relation of the form k
j=0 a′
jf (xj) = 0 (except that it can be multiplied by an arbitrary
constant). In the equidistant case, i.e., if xj = x0 + jh, then
k

j=0
a′
jf (xj) ≡c4kf (x0),
c ̸= 0.
Proof. If (3.3.10) were valid for all f ∈Pk, then the linear system k
j=1 xi−1
j
aj = 0,
i = 1 : k, would have a nontrivial solution (a1, a2, . . . , ak). The matrix of the system,
however, is a Vandermonde matrix,71
V = [xi−1
j
]k
i,j=1 =


1
1
· · ·
1
x1
x2
· · ·
xk
...
...
· · ·
...
xk−1
1
xk−1
2
· · ·
xk−1
k


(3.3.11)
(see Problem 3.3.1). Its determinant can be shown to equal the product of all differences,
i.e.,
det(V ) =
&
1≤i<j≤k
(xi −xj).
(3.3.12)
This is nonzero if and only if the points are distinct.
Now we add the point x0. Suppose that there exist two relations,
k

j=0
bjf (xj) = 0,
k

j=0
cjf (xj) = 0,
with linearly independent coefﬁcient vectors. Then we can ﬁnd a (nontrivial) linear combi-
nation, where x0 has been eliminated, but this contradicts the result that we have just proved.
Hence the hypothesis is wrong; the two coefﬁcient vectors must be proportional.
We have seen above that, in the equidistant case, 4kf (x0) = 0 is such a relation.
More generally, we shall see in Chapter 4 that, for k + 1 arbitrary distinct points, the kth
order divided difference is zero for all f ∈Pk.
71Alexandre Théophile Vandermonde (1735–1796), member of the French Academy of Sciences, is regarded as
the founder of the theory of determinants. What is now referred to as the Vandermonde matrix does not seem to
appear in his writings!

3.3. Difference Operators and Operator Expansions
229
Corollary 3.3.5.
Suppose that a formula for interpolation, numerical differentiation, or integration has
been derived by an operator technique. If it is a linear combination of the values of f (x)
at k given distinct points xj, j = 1 : k, and is exact for all f ∈Pk, this formula is unique.
(If it is exact for all f ∈Pm, m < k, only, it is not unique.)
In particular, for any {cj}k
j=1, a unique polynomial P ∈Pk is determined by the
interpolation conditions P(xj) = cj, j = 1 : k.
Proof. The difference between two formulas that use the same function values would lead
to a relation that is impossible, by the theorem.
Now we shall go outside of polynomial algebra and consider also inﬁnite series of
operators. The Taylor series
f (x + h) = f (x) + hf ′(x) + h2
2! f ′′(x) + h3
3! f ′′′(x) + · · ·
can be written symbolically as
Ef =

1 + hD + (hD)2
2!
+ (hD)3
3!
+ · · ·

f.
We can here treat hD like an algebraic indeterminate, and consider the series inside the
parenthesis (without the operand) as a formal power series.72
For a formal power series the concepts of convergence and divergence do not exist.
When the operator series acts on a function f , and is evaluated at a point c, we obtain an
ordinary numerical series, related to the linear functional Ef (c) = f (c +h). We know that
this Taylor series may converge or diverge, depending on f , c, and h.
Roughly speaking, the last part of Sec. 3.1.5 tells us that, with some care, “analytic
functions” of one indeterminate can be handled with the same rules as analytic functions of
one complex variable.
Theorem 3.3.6.
ehD = E = 1 + 4,
e−hD = E−1 = 1 −∇,
2 sinh 1
2hD = ehD/2 −e−hD/2 = δ,
(1 + 4)θ = (ehD)θ = eθhD,
(θ ∈R).
Proof. The ﬁrst formula follows from the previous discussion. The second and the third
formulas are obtained in a similar way. (Recall the deﬁnition of δ.) The last formula follows
from the ﬁrst formula together with Lemma 3.1.9 (in Sec. 3.1.5).
It follows from the power series expansion that
(ehD)θf (x) = eθhDf (x) = f (x + θh),
72We now abandon the bold face notation for indeterminates and formal power series used in Sec. 3.1.5 for the
function ehD, which is deﬁned by this series. The reader is advised to take a look again at the last part of Sec. 3.1.5.

230
Chapter 3. Series, Operators, and Continued Fractions
when it converges. Since E = ehD it is natural to deﬁne
Eθf (x) = f (x + θh),
and we extend this deﬁnition to such values of θ that the power series for eθhDf (x) is
divergent. Note that, for example, the formula
Eθ2Eθ1f (x) = Eθ2+θ1f (x)
follows from this deﬁnition.
When one works with operators or functionals it is advisable to avoid notations like
4xn, Deαx, where the variables appear in the operands. For two important functions we
therefore set
Fα : Fα(x) = eαx,
fn : fn(x) = xn.
(3.3.13)
Let P be any of the operators mentioned above. When applied to Fα it acts like a scalar
that we shall call the scalar of the operator73 and denote by sc(P):
PFα = sc(P)Fα.
We may also write sc(P; hα) if it is desirable to emphasize its dependence on hα. (We
normalize the operators so that this is true; for example, we work with hD instead of D.)
Note that
sc(βP + γ Q) = βsc(P) + γ sc(Q),
(β, γ ∈C),
sc(PQ) = sc(P)sc(Q).
For our most common operators we obtain
sc(Eθ) = eθhα,
sc(∇) = sc(1 −E−1) = 1 −e−hα,
sc(4) = sc(E −1) = ehα −1,
sc(δ) = sc(E1/2 −E−1/2) = ehα/2 −e−hα/2.
Let Qh be one of the operators hD, 4, δ, ∇. It follows from the last formulas that
sc(Qh) ∼hα,
(h →0);
|sc(Qh)| ≤|hα|e|hα|.
The main reason for grouping these operators together is that each of them has the important
property (3.3.4), i.e., Qk
hf (c) = hkf (k)(ζ), where ζ lies in the smallest interval that contains
all the arguments used in the computation of Qk
hf (c). Hence,
f ∈Pk
⇒
Qn
hf = 0
∀n ≥k.
(3.3.14)
This property74 makes each of these four operators well suited to be the indeterminate in a
formal power series that, hopefully, will be able to generate a sequence of approximations,
73In applied Fourier analysis this scalar is, for α = iω, often called the symbol of the operator.
74The operators E and µ do not possess this property.

3.3. Difference Operators and Operator Expansions
231
L1, L2, L3 . . . , to a given linear operator L. Ln is the nth partial sum of a formal power
series for L. Then
f ∈Pk
⇒
Lnf = Lkf
∀n ≥k.
(3.3.15)
We shall see in the next theorem that, for expansion into powers of Qh,
lim
n→∞Lnf (x) = Lf (x)
if f is a polynomial. This is not quite self-evident because it is not true for all functions f ,
and we have seen in Sec. 3.1.5 that it can happen that an expansion converges to a “wrong
result.” We shall see more examples of that later. Convergence does not necessarily imply
validity.
Suppose that z is a complex variable, and that φ(z) is analytic at the origin, i.e., φ(z)
is equal to its Maclaurin series, (say)
φ(z) = a0 + a1z + a2z2 + · · · ,
if |z| < ρ for some ρ > 0. For multivalued functions we always refer to the principal
branch. The operator function φ(Qh) is usually deﬁned by the formal power series,
φ(Qh) = a0 + a1Qh + a2Q2
h + · · · ,
where Qh is treated like an algebraic indeterminate.
The operators E, hD, 4, δ, ∇, and µ are related to each others. See Table 3.3.1, which
is adapted from an article by the eminent blind British mathematician W. G. Bickley [25].
Some of these formulas follow almost directly from the deﬁnitions; others are derived in
this section. We ﬁnd the value sc(·) for each of these operators by substituting α for D in
the last column of the table. (Why?)
Table 3.3.1. Bickley’s table of relations between difference operators.
E
4
δ
∇
hD
E
E
1 + 4
1 + 1
2δ2 + δ
 
1 + 1
4δ2
1
1 −∇
ehD
4
E −1
4
δ
 
1 + 1
4 δ2 + 1
2δ2
∇
1 −∇
ehD −1
δ
E1/2 −E−1/2
4(1 + 4)−1/2
δ
∇(1 −∇)−1/2
2 sinh 1
2hD
∇
1 −E−1
4
1 + 4
δ
 
1 + 1
4 δ2 −1
2δ2
∇
1 −e−hD
hD
ln E
ln(1 + 4)
2 sinh−1 1
2δ
−ln(1 −∇)
hD
µ
1
2(E1/2 + E−1/2)
1 + 1
24
(1 + 4)1/2
 
1 + 1
4δ2
1 −1
2∇
(1 −∇)1/2
cosh 1
2 hD
Example 3.3.4.
The deﬁnition of ∇reads in operator form E−1 = 1 −∇. This can be looked upon as
a formal power series (with only two nonvanishing terms) for the reciprocal of E with ∇as

232
Chapter 3. Series, Operators, and Continued Fractions
the indeterminate. By the rules for formal power series mentioned in Sec. 3.1.5, we obtain
uniquely
E = (E−1)−1 = (1 −∇)−1 = 1 + ∇+ ∇2 + · · · .
We ﬁnd in Table 3.3.1 an equivalent expression containing a fraction line. Suppose that we
have proved the last column of the table. Thus, sc(∇) = 1 −e−hα, hence
sc((1 −∇)−1) = (e−hα)−1 = ehα = sc(E).
Example 3.3.5.
Suppose that we have proved the ﬁrst and the last columns of Bickley’s table (except
for the equation hD = ln E). We shall prove one of the formulas in the second column,
namely the equation
δ = 4(1 + 4)−1/2.
By the ﬁrst column, the right-hand side is equal to (E −1)E−1/2 = E1/2 −E−1/2 = δ.
We shall also compute sc(4(1 + 4)−1/2). Since sc(4) = ehα −1 we obtain
sc(4(1 + 4)−1/2) = (ehα −1)(ehα)−1/2 = ehα/2 −e−hα/2
= 2 sinh 1
2hα = sc(δ).
With the aid of Bickley’s table, we are in a position to transform L into the form
φ(Qh)Rh. (A sum of several such expressions with different indeterminates can also be
treated.)
• Qh is the one of the four operators, hD, 4, δ, ∇, which we have chosen to be the
“indeterminate.”
Lf ≃φ(Qh)f = (a0 + a1Qh + a2Q2
h + · · ·)f.
(3.3.16)
The coefﬁcients aj are the same as the Maclaurin coefﬁcients of φ(z), z ∈C, if
φ(z) is analytic at the origin. They can be determined by the techniques described in
Sec. 3.1.4 and Sec. 3.1.5. The meaning of the relation ≃will hopefully be clear from
the following theorem.
• Rh is, e.g., µδ or Ek, k integer, or more generally any linear operator with the
properties that RhFα = sc(Rh)Fα, and that the values of Rhf (xn) on the grid xn =
x0 + nh, n integer, are determined by the values of f on the same grid.
Theorem 3.3.7.
Recall the notation Qh for either of the operators 4, δ£, ∇, hD, and the notations
Fα(x) = eαx, fn(x) = xn. Note that
Fα(x) =
∞

n=0
αn
n! fn (x).
(3.3.17)
Also recall the scalar of an operator and its properties, for example,
LFα = sc(L)Fα,
Qj
hFα = (sc(Qh))jFα;
for the operators under consideration the scalar depends on hα.

3.3. Difference Operators and Operator Expansions
233
We make the following assumptions:
(i) A formal power series equation L = ∞
j=0 ajQj
h has been derived.75 Furthermore,
|sc(Qh)| < ρ, where ρ is the radius of convergence of the series  ajzj, z ∈C, and
sc(L) =
∞

j=0
aj(sc(Qh))j.
(3.3.18)
(ii) At α = 0 it holds that
L ∂n
∂αn Fα(x) = ∂n
∂αn (LFα)(x)
or, equivalently,
L

C
Fα(x) dα
αn+1
=

C
(LFα)(x) dα
αn+1
,
(3.3.19)
where C is any circle with the origin as center.
(iii) The domain of x is a bounded interval I1 in R.
Then it holds that
LFα =
 ∞

j=0
ajQj
h

Fα
if |sc(Qh)| < ρ,
(3.3.20)
Lf (x) =
k−1

j=0
ajQj
hf (x)
if f ∈Pk,
(3.3.21)
for any positive integer k.
A rigorous error bound for (3.3.21), if f /∈Pk, is obtained in Peano’s theorem (3.3.8).
An asymptotic error estimate (as h →0 for ﬁxed k) is given by the ﬁrst neglected
nonvanishing term arQr
hf (x) ∼ar(hD)rf (x), r ≥k, if f ∈Cr[I], where the interval I
must contain all the points used in the evaluation of Qr
hf (x).
Proof. By assumption (i),
LFα = sc(L)Fα = lim
J→∞
J−1

j=0
ajsc(Qj
h)Fα = lim
J→∞
J−1

j=0
ajQj
hFα = lim
J→∞
 J−1

j=0
ajQj
h

Fα,
hence LFα = (∞
j=0 Qj
h)Fα. This proves the ﬁrst part of the theorem.
By (3.3.17), Cauchy’s formula (3.2.8), and assumption (ii),
2πi
n! Lfn(x) = L

C
Fα(x) dα
αn+1
=

C
(LFα)(x) dα
αn+1
=

C
J−1

j=0
ajQj
hFα(x) dα
αn+1
+

C
∞

j=J
ajsc(Qh)jFα(x) dα
αn+1
.
75To simplify the writing, the operator Rh is temporarily neglected. See one of the comments below.

234
Chapter 3. Series, Operators, and Continued Fractions
Let ϵ be any positive number. Choose J so that the modulus of the last term becomes
ϵθn2π/n!, where |θn| < 1. This is possible, since |sc(Qh)| < ρ; see assumption (i). Hence,
for every x ∈I1,
Lfn(x) −ϵθn = n!
2πi
J−1

j=0
ajQj
h

C
Fα(x) dα
αn+1
=
J−1

j=0
ajQj
hfn(x) =
k−1

j=0
ajQj
hfn(x).
The last step holds if J ≥k > n because, by (3.3.14), Qj
hfn = 0 for j > n. It follows that
((((Lfn(x) −
k−1

j=0
ajQj
hfn(x)
(((( < ϵ
∀ϵ > 0,
and hence Lfn = k−1
j=0 ajQj
hfn.
If f ∈Pk, f is a linear combination of fn, n = 0 : k −1. Hence Lf = k−1
j=0 ajQj
hf
if f ∈Pk. This proves the second part of the theorem.
The error bound is derived in Sec. 3.3.1. Recall the important formula (3.3.4) that
expresses the kth difference as the value of the kth derivative in a point located in an interval
that contains all the points used in the computation of the kth difference; i.e., the ratio of
the error estimate ar(hD)rf (x) to the true truncation error tends to 1, as h →0.
Remark 3.3.1. This theorem is concerned with series of powers of the four operators
collectively denoted Qh. One may try to use operator techniques also to ﬁnd a formula
involving, for example, an inﬁnite expansion into powers of the operator E. Then one
should try afterward to ﬁnd sufﬁcient conditions for the validity of the result. This procedure
will be illustrated in connection with Euler–Maclaurin’s formula in Sec. 3.4.5.
Sometimes, operator techniques which are not covered by this theorem can, after
appropriate restrictions, be justiﬁed (or even replaced) by transform methods, for example,
z-, Laplace, or Fourier transforms.
The operator Rh that was introduced just before the theorem was neglected in the
proof in order to simplify the writing. We now have to multiply the operands by Rh in the
proof and in the results. This changes practically nothing for Fα, since RhFα = sc(Rh)Fα.
In (3.3.21) there is only a trivial change, because the polynomials f and Rhf may not have
the same degree. For example, if Rh = µδ and f ∈Pk, then Rhf ∈Pk−1. The veriﬁcation
of the assumptions typically offers no difﬁculties.
It follows from the linearity of (3.3.20) that it is satisﬁed also if Fα is replaced by a
linear combination of exponential functions Fα with different α, provided that |sc(Qh)| < ρ
for all the occurring α. With some care, one can let the linear combination be an inﬁnite
series or an integral.
There are two things to note in connection with the asymptotic error estimates. First,
the step size should be small enough; this means in practice that, in the beginning, the
magnitude of the differences should decrease rapidly, as their order increases. When the
order of the differences becomes large, it often happens that the moduli of the differences
also increase. This can be due to two causes: semiconvergence (see the next comment)
and/or rounding errors.

3.3. Difference Operators and Operator Expansions
235
The rounding errors of the data may have such large effects on the high-order dif-
ferences (recall Example 3.3.2) that the error estimation does not make sense. One should
then use a smaller value of the order k, where the rounding errors have a smaller inﬂuence.
An advantage with the use of a difference scheme is that it is relatively easy to choose the
order k adaptively, and sometimes the step size h also.
This comment is of particular importance for numerical differentiation. Numerical
illustrations and further comments are given below in Example 3.3.6 and Problem 3.3.7 (b),
and in several other places.
The sequence of approximations to Lf may converge or diverge, depending on f
and h. It is also often semiconvergent (recall Sec. 3.2.6), but in practice the rounding errors
mentioned in the previous comment have often, though not always, taken over already,
when the truncation error passes its minimum; see Problem 3.3.7 (b).
By Theorem 3.3.6, e−hD = 1 −∇. We look upon this as a formal power series; the
indeterminate is Qh = ∇. By Example 3.1.11,
L = hD = −ln(1 −∇) = ∇+ 1
2∇2 + 1
3∇3 + · · · .
(3.3.22)
Now we present veriﬁcation of the assumptions of Theorem 3.3.7:76
(i) sc(∇) = 1 −e−hα; the radius of convergence is ρ = 1.
sc(L) = sc(hD) = hα;
∞

j=1
sc(∇)j/j = −ln(1 −(1 −e−hα)) = hα.
The convergence condition |sc(∇)| < 1 reads hα > −ln 2 = −0.69 if α is real,
|hω| < π/3 if α = iω.
(ii) For α = 0, D ∂n
∂αn (eαx) = Dxn = nxn−1. By Leibniz’ rule
∂n
∂αn (αeαx) = 0xn + nxn−1.
By the theorem, we now obtain the backward differentiation formula that is exact
for all f ∈Pk:
hf ′(x) =

∇+ 1
2∇2 + 1
3∇3 + · · · +
1
k −1∇k−1

f (x).
(3.3.23)
By Theorem 3.3.4, this is the unique formula of this type that uses the values of f (x) at the
k points xn : −h : xn−k+1. The same approximation can be derived in many other ways,
perhaps with a different appearance; see Chapter 4. This derivation has several advantages;
the same expansion yields approximation formulas for every k, and if f ∈Ck, f /∈Pk, the
ﬁrst neglected term, i.e., 1
k∇k
hf (xn), provides an asymptotic errorestimate if f (k)(xn) ̸= 0.
76Recall the deﬁnition of the scalar sc(·), given after (3.3.13).

236
Chapter 3. Series, Operators, and Continued Fractions
Example 3.3.6.
We now apply formula (3.3.23) to the table in Example 3.3.2, where f (x) = tan x,
h = 0.01, k = 6,
0.01f ′(1.35) ≈0.1996 + 0.0163
2
+ 0.0019
3
+ 0.0001
4
−0.0004
5
;
i.e., we obtain a sequence of approximate results,
f ′(1.35) ≈19.96, 20.78, 20.84, 20.84, 20.83.
The correct value to 3D is (cos 1.35)−2 = 20.849. Note that the last result is worse than
the next to last. Recall the last comments on the theorem. In this case this is due to the
rounding errors of the data. Upper bounds for their effect of the sequence of approximate
values of f ′(1.35) are, by Example 3.3.3, shown in the series
10−2
1 + 2
2 + 4
3 + 8
4 + 16
5 + · · ·

.
Alarger version of this problem was run on a computer with the machine unit 2−53 ≈10−16;
f (x) = tan x, x = 1.35 : −0.01 : 1.06. In the beginning the error decreases rapidly, but
after 18 terms the rounding errors take over, and the error then grows almost exponentially
(with constant sign). The eighteenth term and its rounding error have almost the same
modulus (but opposite sign). The smallest error equals 5 · 10−10, and is obtained after 18
terms; after 29 terms the actual error has grown to 2 · 10−6. Such a large number of terms is
seldom used in practice, unless a very high accuracy is demanded; see also Problem 3.3.7 (b),
a computer exercise that offers both similar and different experiences.
Equation (3.3.22)—or its variable step size variant in Chapter 4—is the basis of the
important backward differentiation formula (BDF) method for the numerical integration
of ordinary differential equations.
Coefﬁcients for backward differentiation formulas for higher derivatives are obtained
from the equations
(hD/∇)k = (−ln(1 −∇)/∇)k.
The following formulas were computed by means of the matrix representation of a truncated
power series:


hD/∇
(hD/∇)2
(hD/∇)3
(hD/∇)4
(hD/∇)5

=


1
1/2
1/3
1/4
1/5
1
1
11/12
5/6
137/180
1
3/2
7/4
15/8
29/15
1
2
17/6
7/2
967/240
1
5/2
25/6
35/6
1069/144


·


1
∇
∇2
∇3
∇4

.
(3.3.24)
The rows of the matrix are the ﬁrst rows taken from the matrix representation of each of the
expansions (hD/∇)k, k = 1 : 5.
When the effect of the irregular errors of the data on a term becomes larger in
magnitude than the term itself, the term should, of course, be neglected; it does more harm

3.3. Difference Operators and Operator Expansions
237
than good. This happens relatively early for the derivatives of high order; see Problem 3.3.7.
When these formulas are to be used inside a program (rather than during an interactive post-
processing of results of an automatic computation), some rules for automatic truncation
have to be designed, an interesting kind of detail in scientiﬁc computing.
The forward differentiation formula, which is analogously based on the operator series
hD = ln(1 + 4) = 4 −1
242 + 1
343 −· · ·
(3.3.25)
is sometimes useful too. We obtain the coefﬁcients for derivatives of higher order by
inserting minus signs in the second and fourth columns of the matrix in (3.3.24).
Astraightforward solution to this problem is to use the derivative of the corresponding
interpolation polynomial as the approximation to the derivative of the function. This can
also be done for higher-order derivatives.
A grid (or a table) may be too sparse to be useful for numerical differentiation and for
the computation of other linear functionals. For example, we saw above that the successive
backward differences of eiωx increase exponentially if |ωh| > π/3. In such a case the
grid, where the values are given, gives insufﬁcient information about the function. One
also says that “the grid does not resolve the function.” This is often indicated by a strong
variation in the higher differences. But even this indication can sometimes be absent. An
extreme example is f (x) = sin(πx/h), on the grid xj = jh, j = 0, ±1, ±2, . . . . All the
higher differences, and thus the estimates of f ′(x) at all grid points, are zero, but the correct
values of f ′(xj) are certainly not zero. Therefore, this is an example where the expansion
(trivially) converges, but it is not valid! (Recall the discussion of a Maclaurin expansion
for a nonanalytic function at the end of Sec. 3.1.2. Now a similar trouble can also occur for
an analytic function.)
A less trivial example is given by the functions
f (x) =
20

n=1
an sin(2πnx),
g(x) =
10

n=1
(an + a10+n) sin(2πnx).
On the grid f (x) = g(x), hence they have the same difference scheme, but f ′(x) ̸= g′(x)
on the grid, and typically f (x) ̸= g(x) between the grid points.
3.3.3
The Peano Theorem
One can often, by a combination of theoretical and numerical evidence, rely on asymptotic
errorestimates. Sincethereareexceptions, itisinterestingthattherearetwogeneralmethods
for deriving strict error bounds. We call one of them the norms and distance formula.
This is not restricted to polynomial approximation, and it is typically easy to use, but it
requires some advanced concepts and often overestimates the error. We therefore postpone
the presentation of that method to Sec. 4.5.2.

238
Chapter 3. Series, Operators, and Continued Fractions
We shall now give another method, due to Peano.77 Consider a linear functional
˜Lf =
p

j=1
bjf (xj)
for the approximate computation of another linear functional, for example,
Lf =
 1
0
√xf (x) dx.
Suppose that it is exact when it is applied to any polynomial of degree less than k: In other
words, ˜Lf = Lf for all f ∈Pk. The remainder is then itself a linear functional, R = L−˜L,
with the special property that
Rf = 0
if
f ∈Pk.
The next theorem gives a representation for such functionals which provides a universal
device for deriving error bounds for approximations of the type that we are concerned with.
Let f ∈Cn[a, b]. In order to make the discussion less abstract we conﬁne it to functionals
of the following form, 0 ≤m < n,
Rf =
 b
a
φ(x)f (x) dx +
p

j=1

bj,0f (xj) + bj,1f ′(xj) + · · · + bj,mf (m)(xj)

,
(3.3.26)
where the function φ is integrable, the points xj lie in the bounded real interval [a, b], and
bj,m ̸= 0 for at least one value of j. Moreover, we assume that
Rp = 0
∀p ∈Pk.
(3.3.27)
We deﬁne the function78
t+ = max(t, 0),
tj
+ =

t+
j,
t0
+ = 1 + sign t
2
.
(3.3.28)
The function t0
+ is often denoted H(t) and is known as the Heaviside unit step function.79
The function sign is deﬁned as in Deﬁnition 3.1.3, i.e., sign x = 0, if x = 0. Note that
tj
+ ∈Cj−1, (j ≥1).
The Peano kernel K(u) of the functional R is deﬁned by the equation
K(u) =
1
(k −1)!Rx(x −u)k−1
+ ,
x ∈[a, b],
u ∈(−∞, ∞).
(3.3.29)
The subscript in Rx indicates that R acts on the variable x (not u).
77Giuseppe Peano (1858–1932) was an Italian mathematician and logician.
78We use the neutral notation t here for the variable, to avoid tying up the function too closely with the variables
x and u, which play a special role in the following.
79Oliver Heaviside (1850–1925), English physicist.

3.3. Difference Operators and Operator Expansions
239
The function K(u) vanishes outside [a, b] because
• if u > b, then u > x; hence (x −u)k−1
+
= 0 and K(u) = 0.
• if u < a, then x > u. It follows that (x −u)k−1
+
= (x −u)k−1 ∈Pk;
hence K(u) = 0, by (3.3.29) and (3.3.27).
If φ(x) is a polynomial, then K(u) becomes a piecewise polynomial; the points xj
are the joints of the pieces. In this case K ∈Ck−m−2; the order of differentiability may be
lower, if φ has singularities.
We are now in a position to prove an important theorem.
Theorem 3.3.8 (Peano’s Remainder Theorem).
Suppose that Rp = 0 for all p ∈Pk. Then,80 for all f ∈Ck[a, b],
Rf =
 ∞
−∞
f (k)(u)K(u) du.
(3.3.30)
The deﬁnition and some basic properties of the Peano kernel K(u) were given above.
Proof. By Taylor’s formula,
f (x) =
k−1

j=1
f (j)(a)
j!
(x −a)j +
 x
a
f (k)(u)
(k −1)!(x −u)k−1 du.
This follows from putting n = k, z = x −a, t = (u −a)/(x −u) into (3.1.5). We rewrite
the last term as
 ∞
a f (k)(u)(x −u)k−1
+ du. Then apply the functional R = Rx to both sides.
Since we can allow the interchange of the functional R with the integral, for the class of
functionals that we are working with, this yields
Rf = 0 + R
 ∞
a
f (k)(u)(x −u)k−1
+
(k −1)!
du =
 ∞
a
f (k)(u)Rx(x −u)k−1
+
(k −1)!
du.
The theorem then follows from (3.3.29).
Corollary 3.3.9.
Suppose that Rp = 0 for all p ∈Pk. Then
Rx(x −a)k = k!
 ∞
−∞
K(u) du.
(3.3.31)
For any f ∈Ck[a, b], Rf = f (k)(ξ)
k!
Rx((x −a)k) holds for some ξ ∈(a, b) if and only if
K(u) does not change its sign.
If K(u) changes its sign, the best possible error bound reads
|Rf | ≤sup
u∈[a,b]
|f (k)(u)|
 ∞
−∞
|K(u)| du;
a formula with f (k)(ξ) is not generally true in this case.
80The deﬁnition of f (k)(u) for u /∈[a, b] is arbitrary.

240
Chapter 3. Series, Operators, and Continued Fractions
Proof. First suppose that K(u) does not change sign. Then, by (3.3.30) and the mean value
theorem of integral calculus, Rf = f (k)(ξ)
 ∞
−∞K(u)du, ξ ∈[a, b]. For f (x) = (x −a)k
this yields (3.3.31). The “if” part of the corollary follows from the combination of these
formulas for Rf and R(x −a)k.
If K(u) changes its sign, the “best possible bound” is approached by a sequence of
functions f chosen so that (the continuous functions) f (k)(u) approach (the discontinuous
function) sign K(u). The “only if” part follows.
Example 3.3.7.
The remainder of the trapezoidal rule (one step of length h) reads
Rf =
 h
0
f (x) dx −h
2(f (h) + f (0)).
We know that Rp = 0 for all p ∈P2. The Peano kernel is zero for u /∈[0, h], while for
u ∈[0, h]
K(u) =
 h
0
(x −u)+ dx −h
2((h −u)+ + 0) = (h −u)2
2
−h(h −u)
2
= −u(h −u)
2
< 0.
We also compute
Rx2
2!
=
 h
0
x2
2 dx −h · h2
2 · 2 = h3
6 −h3
4 = −h3
12.
Since the Peano kernel does not change sign, we conclude that
Rf = −h3
12f ′′(ξ),
ξ ∈(0, h).
Example 3.3.8 (Peano Kernels for Difference Operators).
Let Rf = 43f (a), and set xi = a + ih, i = 0 : 3. Note that Rp = 0 for p ∈P3.
Then
Rf = f (x3) −3f (x2) + 3f (x1) −f (x0),
2K(u) = (x3 −u)2
+ −3(x2 −u)2
+ + 3(x1 −u)2
+ −(x0 −u)2
+;
i.e.,
2K(u) =



0
if u > x3,
(x3 −u)2
if x2 ≤u ≤x3,
(x3 −u)2 −3(x2 −u)2
if x1 ≤u ≤x2,
(x3 −u)2 −3(x2 −u)2 + 3(x1 −u)2 ≡(u −x0)2
if x0 ≤u ≤x1,
(x3 −u)2 −3(x2 −u)2 + 3(x1 −u)2 −(x0 −u)2 ≡0
if u < x0.
For the simpliﬁcation of the last two lines we used that 43
u(x0 −u)2 ≡0. Note that K(u)
is a piecewise polynomial in P3 and that K′′(u) is discontinuous at u = xi, i = 0 : 3.

3.3. Difference Operators and Operator Expansions
241
It can be shown (numerically or analytically) that K(u) > 0 in the interval (u0, u3).
This is no surprise because, by (3.3.4), 4nf (x) = hnf (n)(ξ) for any integer n, and, by the
above corollary, this could not be generally true if K(u) changes its sign. These calculations
can be generalized to 4kf (a) for an arbitrary integer k. This example will be generalized
in Sec. 4.4.2 to divided differences of nonequidistant data.
In general it is rather laborious to determine a Peano kernel. Sometimes one can show
that the kernel is a piecewise polynomial, that it has a symmetry, and that it has a simple
form in the intervals near the boundaries. All this can simplify the computation, and might
have been used in these examples.
It is usually much easier to compute R((x −a)k), and an approximate error estimate
is often given by
Rf ∼f (k)(a)
k!
R

(x −a)k
,
f (k)(a) ̸= 0.
(3.3.32)
For example, suppose that x ∈[a, b], where b −a is of the order of magnitude of a step
size parameter h, and that f is analytic in [a, b]. By Taylor’s formula,
f (x) = p(x) + f (k)(a)
k!
(x −a)k + f (k+1)(a)
(k + 1)! (x −a)k+1 + · · · ,
f (k)(a) ̸= 0,
where p ∈Pk; hence Rp = 0. Most of the common functionals can be applied term by
term. Then
Rf = 0 + f (k)(a)
n!
Rx(x −a)k + f (k+1)(a)
(k + 1)! Rx(x −a)k+1 + · · · .
Assume that, for some c, Rx(x−a)k = O(hk+c) for k = 1, 2, 3, . . . . (This is often the case.)
Then (3.3.32) becomes an asymptotic error estimate as h →0. It was mentioned above
that for formulas derived by operator methods, an asymptotic error estimate is directly
available anyway, but if a formula is derived by other means (see Chapter 4) this error
estimate is important.
Asymptotic error estimates are frequently used in computing, because they are often
much easier to derive and apply than strict error bounds. The question is, however, how to
know that “the computation is in the asymptotic regime,” where an asymptotic estimate is
practically reliable. Much can be said about this central question of applied mathematics.
Let us here just mention that a difference scheme displays well the quantitative properties
of a function needed to make the judgment.
If Rp = 0 for p ∈Pk, then a fortiori Rp = 0 for p ∈Pk−i, i = 0 : k. We may
thus obtain a Peano kernel for each i, which is temporarily denoted by Kk−i(u). They are
obtained by integration by parts,
Rkf =
 ∞
−∞
Kk(u)f (k)(u) du =
 ∞
−∞
Kk−1(u)f (k−1)(u) du
(3.3.33)
=
 ∞
−∞
Kk−2(u)f (k−2)(u) du = . . . ,
(3.3.34)
where Kk−i = (−D)iKk, i = 1, 2, . . . , as long as Kk−i is integrable. The lower-order
kernels are useful, e.g., if the actual function f is not as smooth as the usual remainder
formula requires.

242
Chapter 3. Series, Operators, and Continued Fractions
For the trapezoidal rule we obtained in Example 3.3.7
K1(u) = h
2u0
+ + h
2 −u + h
2(u −h)0
+.
A second integration by parts can only be performed within the framework of Dirac’s
delta functions (distributions); K0 is not integrable. A reader who is familiar with these
generalized functions may enjoy the following formula:
Rf =
 ∞
−∞
K0(u)f (u)du ≡
 ∞
−∞

−h
2δ(u) + 1 −h
2δ(u −h)

f (u)du.
This is for one step of the trapezoidal rule, but many functionals can be expressed analo-
gously.
3.3.4
Approximation Formulas by Operator Methods
We shall now demonstrate how operator methods are very useful for deriving approximation
formulas. For example, in order to ﬁnd interpolation formulas we consider the operator
expansion
f (b −γ h) = E−γ f (b) = (1 −∇)γ f (b) =
∞

j=0
γ
j

(−∇)jf (b).
The veriﬁcation of the assumptions of Theorem 3.3.7 offers no difﬁculties, and we omit
the details. Truncate the expansion before (−∇)k. By the theorem we obtain, for every
γ , an approximation formula for f (b −γ h) that uses the function values f (b −jh) for
j = 0 : k −1; it is exact if f ∈Pk and is unique in the sense of Theorem 3.3.4. We
also obtain an asymptotic error estimate if f /∈Pk, namely the ﬁrst neglected term of the
expansion, i.e.,
γ
k

(−∇)kf (b) ∼
γ
k

(−h)kf (k)(b).
Note that the binomial coefﬁcients are polynomials in the variable γ , and hence also in the
variable x = b −γ h.
It follows that the approximation formula yields a unique polynomial PB ∈Pk that
solves the interpolation problem: PB(b −hj) = f (b −hj), j = 0 : k −1 (B stands for
backward). If we set x = b −γ h, we obtain
PB(x) = E−γ f (b) = (1 −∇)γ f (a) =
k−1

j=0
γ
j

(−∇)jf (b)
(3.3.35)
= f (b −γ h) + O(hkf (k)).
Similarly, the interpolation polynomial PF ∈Pk that uses forward differences based on the
values of f at a, a + h, . . . , a + (k −1)h reads, if we set x = a + θh,
PF(x) = Eθf (a) = (1 + 4)θf (a)
k−1

j=0
θ
j

4jf (a)
(3.3.36)
= f (a + θh) + O(hkf (k)).

3.3. Difference Operators and Operator Expansions
243
These formulas are known as Newton’s interpolation formulas for constant step size,
backward and forward. The generalization to variable step size will be found in Sec. 4.2.1.
There exists a similar expansion for central differences. Set
φ0(θ) = 1,
φ1(θ) = θ,
φj(θ) = θ
j
θ + 1
2j −1
j −1

,
(j > 1).
(3.3.37)
φj is an even function if j is even, and an odd function if j is odd. It can be shown that
δjφk(θ) = φk−j(θ) and δjφk(0) = δj,k (Kronecker’s delta). The functions φk have thus an
analogous relation to the operator δ as, for example, the functions θj/j! and
θ
j

have to the
operators D and 4, respectively. We obtain the following expansion, analogous to Taylor’s
formula and Newton’s forward interpolation formula. The proof is left for Problem 3.3.5 (b).
Then
Eθf (a) =
k−1

j=0
φj(θ)δjf (a) = f (a + θh) + O(hkf (k)).
(3.3.38)
The direct practical importance of this formula is small, since δjf (a) cannot be expressed
as a linear combination of the given data when j is odd. There are several formulas in
which this drawback has been eliminated by various transformations. They were much in
use before the computer age; each formula had its own group of fans. We shall derive only
one of them, by a short break-neck application of the formal power series techniques.81
Note that
Eθ = eθhD = cosh θhD + sinh θhD,
δ2 = ehD −2 + e−hD,
ehD −e−hD = 2µδ,
cosh θhD = 1
2(Eθ + E−θ) =
∞

j=0
φ2j(θ)δ2j,
sinh θhD = 1
θ
d(cosh θhD)
d(hD)
=
∞

j=0
φ2j(θ)1
θ
dδ2j
dδ2
dδ2
d(hD)
=
∞

j=0
φ2j(θ)jδ2(j−1)
θ
(ehD −e−hD) =
∞

j=0
φ2j(θ)2j
θ µδ2j−1.
Hence,
f (x0 + θh) = f0 + θµδf0 + θ2
2! δ2f0 +
∞

j=2
φ2j(θ)
2j
θ µδ2j−1f0 + δ2jf0

.
(3.3.39)
This is known as Stirling’s interpolation formula.82 The ﬁrst three terms have been taken
out from the sum, in order to show their simplicity and their resemblance toTaylor’s formula.
They yield the most practical formula for quadratic interpolation; it is easily remembered
81Differentiation of a formal power series with respect to an indeterminate has a purely algebraic deﬁnition. See
the last part of Sec. 3.1.5.
82James Stirling (1692–1770), British mathematician perhaps most famous for his amazing approximation to n!.

244
Chapter 3. Series, Operators, and Continued Fractions
and worth being remembered. An approximate error bound for this quadratic interpolation
reads |0.016δ3f | if |θ| < 1.
Note that
φ2j(θ) = θ2(θ2 −1)(θ2 −4) · · · (θ2 −(j −1)2)/(2j)!.
The expansion yields a true interpolation formula if it is truncated after an even power of
δ. For k = 1 you see that f0 + θµδf0 is not a formula for linear interpolation; it uses three
data points instead of two. It is similar for all odd values of k.
Strict error bounds can be found by means of Peano’s theorem, but the remainder given
by Theorem 4.2.3 for Newton’s general interpolation formula (that does not require equidis-
tant data) typically give the answer easier. Both are typically of the form ck+1f (k+1)(ξ) and
require a bound for a derivative of high order. The assessment of such a bound typically
costs much more work than performing interpolation in one point.
A more practical approach is to estimate a bound for this derivative by means of a
bound for the differences of the same order. (Recall the important formula in (3.3.4).) This
is not a rigorous bound, but it typically yields a quite reliable error estimate, in particular if
you put a moderate safety factor on the top of it. There is much more to be said about the
choice of step size and order; we shall return to these kinds of questions in later chapters.
You can make error estimates during the computations; it can happen sooner or later
that it does not decrease when you increase the order. You may just as well stop there, and
accept the most recent value as the result. This event is most likely due to the inﬂuence
of irregular errors, but it can also indicate that the interpolation process is semiconvergent
only.
The attainable accuracy of polynomial interpolation applied to a table with n equidis-
tant values of an analytic function depends strongly on θ; the results are much poorer near
the boundaries of the data set than near the center. This question will be illuminated in
Sec. 4.7 by means of complex analysis.
Example 3.3.9.
The continuation of the difference scheme of a polynomial is a classical application of
a difference scheme for obtaining a smooth extrapolation of a function outside its original
domain. Given the values yn−i = f (xn −ih) for i = 1 : k and the backward differences,
∇jyn−1, j = 1 : k −1. Recall that ∇k−1y is a constant for y ∈Pk. Consider the algorithm
∇k−1yn = ∇k−1yn−1;
for j = k −1 : −1 : 1
∇j−1yn = ∇j−1yn−1 + ∇jyn;
(3.3.40)
end
yn = ∇0yn;
It is left for Problem 3.3.2 (g) to show that the result yn is the value at x = xn of the
interpolation polynomial which is determined by yn−i, i = 1 : k. This is a kind of inverse
use of a difference scheme; there are additions from right to left along a diagonal, instead
of subtractions from left to right.
This algorithm, which needs additions only, was used long ago for the production of
mathematical tables, for example, for logarithms. Suppose that one knows, by means of a

3.3. Difference Operators and Operator Expansions
245
series expansion, a relatively complicated polynomial approximation to (say) f (x) = ln x,
that is accurate enough in (say) the interval [a, b], and that this has been used for the
computation of k very accurate values y0 = f (a), y1 = f (a + h), . . . , yk−1, needed for
starting the difference scheme. The algorithm is then used for n = k, k + 1, k + 2,
. . . , (b −a)/h. k −1 additions only are needed for each value yn. Some analysis must
have been needed for the choice of the step h to make the tables useful with (say) linear
interpolation, and for the choice of k to make the basic polynomial approximation accurate
enough over a substantial number of steps. The precision used was higher when the table
was produced than when it was used. When x = b was reached, a new approximating
polynomial was needed for continuing the computation over another interval (at least a new
value of ∇k−1yn).83
The algorithm in (3.3.40) can be generalized to the case of nonequidistant with the
use of divided differences; see Sec. 4.2.1.
We now derive some central difference formulas for numerical differentiation. From
the deﬁnition and from Bickley’s table (Table 3.3.1),
δ ≡E1/2 −E−1/2 = 2 sinh
1
2hD

.
(3.3.41)
We may therefore put x = 1
2hD, sinh x = 1
2δ into the expansion (see Problem 3.1.7)
x = sinh x −1
2
sinh3 x
3
+ 1 · 3
2 · 4
sinh5 x
5
−1 · 3 · 5
2 · 4 · 6
sinh7 x
7
+ · · · ,
with the result
hD = 2arcsinh δ
2 = δ −δ3
24 + 3δ5
640 −5δ7
7168 +
35δ9
294,912 −
63δ11
2,883,584 + · · · .
(3.3.42)
The veriﬁcation of the assumptions of Theorem 3.3.7 follows the pattern of the proof of
(3.3.23), and we omit the details. Since arcsinh z, z ∈C, has the same singularities as its
derivative (1 + z2)−1/2, namely z = ±i, it follows that the expansion in (3.3.42), if sc(δ/2)
is substituted for δ/2, converges if sc(δ/2) < 1; hence ρ = 2.
By squaring the above relation, we obtain
(hD)2 = δ2 −δ4
12 + δ6
90 −δ8
560 + δ10
3150 −
δ12
16,632 + · · · ,
f ′′(x0) ≈

1 −δ2
12 + δ4
90 −δ6
560 +
δ8
3150 −
δ10
16,632 + · · ·
 δ2f0
h2 .
(3.3.43)
By Theorem 3.3.7 (3.3.43) holds for all polynomials. Since the ﬁrst neglected nonvanishing
term of (3.3.43) when applied to f is (asymptotically) cδ12f ′′(x0), the formula for f ′′(x)
83This procedure was the basis of the unﬁnished Difference Engine project of the great nineteenth century British
computer pioneer Charles Babbage. He abandoned it after a while in order to spend more time on his hugeAnalytic
Engine project, which was also unﬁnished. He documented a lot of ideas, where he was (say) 100 years ahead
of his time. “Difference engines” based on Babbage’s ideas were, however, constructed in Babbage’s own time,
by the Swedish inventors Scheutz (father and son) in 1834 and by Wiberg in 1876. They were applied to, among
other things, the automatic calculation and printing of tables of logarithms; see Goldstine [159].

246
Chapter 3. Series, Operators, and Continued Fractions
is exact if f ′′ ∈P12, i.e., if f ∈P14, although only 13 values of f (x) are used. We thus
gain one degree and, in the application to functions other than polynomials, one order of
accuracy, compared to what we may have expected by counting unknowns and equations
only; see Theorem 3.3.4. This is typical for a problem that has a symmetry with respect to
the hull of the data points.
Suppose that the values f (x) are given on the grid x = x0 + nh, n integer. Since
(3.3.42) contains odd powers of δ, it cannot be used to compute f ′
n on the same grid, as
pointed out in the beginning of Sec. 3.3.2. This difﬁculty can be overcome by means of
another formula given in Bickley’s table, namely
µ =

1 + δ2/4.
(3.3.44)
This is derived as follows. The formulas
µ = cosh hD
2 ,
δ
2 = sinh hD
2
follow rather directly from the deﬁnitions; the details are left for Problem 3.3.6 (a). The
formula (cosh hD)2 −(sinh hD)2 = 1 holds also for formal power series. Hence
µ2 −1
4δ2 = 1
or
µ2 = 1 + 1
4δ2,
from which the relation (3.3.44) follows.
If we now multiply the right-hand side of (3.3.42) by the expansion
1 = µ

1 + 1
4δ2−1/2
= µ

1 −δ2
8 + 3δ4
128 −
5δ6
1,024 +
35δ8
32,768 + · · ·

,
(3.3.45)
we obtain
hD =

1 −δ2
6 + δ4
30 −δ6
140 + δ8
630 −· · ·

µδ.
(3.3.46)
This leads to a useful central difference formula for the ﬁrst derivative (where we have used
more terms than we displayed in the above derivation):
f ′(x0) =

1 −δ2
6 + δ4
30 −δ6
140 + δ8
630 −δ10
2772 + · · ·
f1 −f−1
2h
.
(3.3.47)
If you truncate the operator expansion in (3.3.47) after the δ2k term, you obtain exactly the
derivative of the interpolation polynomial of degree 2k + 1 for f (x) that is determined by
the 2k + 2 values fi, i = ±1, ±2, . . . , ±(k + 1). Note that all the neglected terms in the
expansion vanish when f (x) is any polynomial of degree 2k + 2, independent of the value
of f0. (Check the statements ﬁrst for k = 0; you will recognize a familiar property of the
parabola.) So, although we search for a formula that is exact in P2k+2, we actually ﬁnd a
formula that is exact in P2k+3.
By the multiplication of the expansions in (3.3.43) and (3.3.46), we obtain the fol-
lowing formulas, which have applications in other sections:
(hD)3 =

1 −1
4δ2 +
7
120δ4 + · · ·

µδ3,
(hD)5 =

1 −1
3δ2 + · · ·

µδ5,
(3.3.48)
(hD)7 = µδ7 + · · · .

3.3. Difference Operators and Operator Expansions
247
Another valuable feature typical for expansions in powers of δ2 is the rapid convergence.
It was mentioned earlier that ρ = 2, hence ρ2 = 4, (while ρ = 1 for the backward
differentiation formula). The error constants of the differentiation formulas obtained by
(3.3.43) and (3.3.47) are thus relatively small.
All this is typical for the symmetric approximation formulas which are based on
central differences; see, for example, the above formula for f ′′(x0), or the next example.
In view of this, can we forget the forward and backward difference formulas altogether?
Well, this is not quite the case, since one must often deal with data that are unsymmetric
with respect to the point where the result is needed. For example, given f−1, f0, f1, how
would you compute f ′(x1)? Asymmetry is also typical for the application to initial value
problems for differential equations. In such applications methods based on symmetric rules
for differentiation or integration have sometimes inferior properties of numerical stability.
We shall study the computation of f ′(x0) using the operator expansion (3.3.47). The
truncation error (called RT ) can be estimated by the ﬁrst neglected term, where
1
hµδ2k+1f0 ≈h2kf (2k+1)(x0).
The irregular errors in the values of f (x) are of much greater importance in numerical
differentiation than in interpolation and integration. Suppose that the function values have
errors whose magnitude does not exceed 1
2U. Then the error bound on µδf0 = 1
2(f1−f−1) is
also equal to 1
2U. Similarly, one can show that the error bounds in µδ(2k+1)f0, for k = 1 : 3,
are 1.5U, 5U, 417.5U, respectively. Thus one gets the upper bounds U/(2h), 3U/(4h),
and 11U/(12h) for the roundoff error RXF with one, two, and three terms in (3.3.47).
Example 3.3.10.
Assume that k terms in the formula above are used to approximate f ′(x0), where
f (x) = ln x, x0 = 3, and U = 10−6. Then
f (2k+1)(3) = (2k)!/32k+1,
and for the truncation and roundoff errors we get
k
1
2
3
RT
0.0123h2
0.00329h4
0.00235h6
RXF
(1/2h)10−6
(3/4h)10−6
(11/12h)10−6
.
The plots of RT and RXF versus h in a log-log diagram in Figure 3.3.1 are straight lines
that well illustrate quantitatively the conﬂict between truncation and roundoff errors. The
truncation error increases, and the effect of the irregular error decreases with h. One sees
how the choice of h, which minimizes the sum of the bounds for the two types of error,
depends on U and k, and tells us what accuracy can be obtained. The optimal step lengths for
k = 1, 2, 3 are h = 0.0344, h = 0.1869, and h = 0.3260, giving error bounds 2.91 · 10−5,
8.03 · 10−6, and 5.64 · 10−6. Note that the optimal error bound with k = 3 is not much
better than that for k = 2.

248
Chapter 3. Series, Operators, and Continued Fractions
10
−3
10
−2
10
−1
10
0
10
−25
10
−20
10
−15
10
−10
10
−5
10
0
h
Error
RXF
RT
Figure 3.3.1. Bounds for truncation error RT and roundoff error RXF in numerical
differentiation as functions of h (U = 0.5 · 10−6).
Theeffectofthepureroundingerrorsisimportant, thoughitshouldnotbeexaggerated.
Using IEEE double precision with u = 1.1 · 10−16, one can obtain the ﬁrst two derivatives
very accurately by the optimal choice of h. The corresponding ﬁgures are h = 2.08 · 10−5,
h = 2.19 · 10−3, and h = 1.36 · 10−2, giving the optimal error bounds 1.07 · 10−11,
1.52 · 10−13, and 3.00 · 10−14, respectively.
It is left to the user (Problem 4.3.8) to check and modify the experiments and conclu-
sions indicated in this example.
When a problem has a symmetry around some point x0, you are advised to try to
derive a δ2-expansion. The ﬁrst step is to express the relevant operator in the form M(δ2),
where the function M is analytic at the origin.
To ﬁnd a δ2-expansion for M(δ2) is algebraically the same thing as expanding M(z)
into powers of a complex variable z. Thus, the methods for the manipulation of power
series mentioned in Sec. 3.1.4 and Problem 3.1.8 are available, and so is the Cauchy–FFT
method. For suitably chosen r, N you evaluate
M(re2πik/N),
k = 0 : N −1,
and obtain the coefﬁcients of the δ2-expansion by the FFT! You can therefore derive a long
expansion, and later truncate it as needed. You also obtain error estimates for all these
truncated expansions for free. By the assumed symmetry there will be even powers of δ
only in the expansion. Some computation and storage can be saved by working with F(√z)
instead.
Suppose that you have found a truncated δ2-expansion, (say)
A(δ2) ≡a1 + a2δ2 + a3δ4 + · · · + ak+1δ2k,

3.3. Difference Operators and Operator Expansions
249
but you want instead an equivalent symmetric expression of the form
B(E) ≡b1 + b2(E + E−1) + b3(E2 + E−2) + · · · + bk+1(Ek + E−k).
Note that δ2 = E −2 + E−1. The transformation A(δ2) 9→B(E) can be performed
in several ways. Since it is linear it can be expressed by a matrix multiplication of the
form b = Mk+1a, where a, b are column vectors for the coefﬁcients, and Mk+1 is the
(k + 1) × (k + 1) upper triangular submatrix in the northwest corner of a matrix M that
turns out to be
M =


1
−2
6
−20
70
−252
924
−3432
1
−4
15
−56
210
−792
3003
1
−6
28
−120
495
−2002
1
−8
45
−220
1001
1
−10
66
−364
1
−12
91
1
−14
1


.
(3.3.49)
This 8 × 8 matrix is sufﬁcient for a δ2-expansion up to the term a8δ14. Note that the
matrix elements are binomial coefﬁcients that can be generated recursively (Sec. 3.1.2). It
is easy to extend by the recurrence that is mentioned in the theorem below. Also note that
the matrix can be looked upon as the lower part of a thinned Pascal triangle.
Theorem 3.3.10.
The elements of M are
Mij =
%
(−1)j−12j−2
j−1

if 1 ≤i ≤j,
0
if i > j.
(3.3.50)
We extend the deﬁnition by setting M0,j = M2,j. Then the columns of M are obtained by
the recurrence
Mi,j+1 = Mi+1,j −2Mi,j + Mi−1,j.
(3.3.51)
Proof. Recall that δ = (1 −E−1)E1/2 and put m −ν = µ. Hence
δ2m = (1 −E−1)2mEm =
2m

ν=0
(−1)ν
2m
ν

Em−ν
= (−1)m
2m
m

+
m

µ=1
(−1)m−µ
 2m
m −µ

(Eµ + E−µ).
(3.3.52)
Since
( 1
δ2
δ4
. . . ) = ( 1
(E −E−1)
(E2 −E−2)
. . . ) M,

250
Chapter 3. Series, Operators, and Continued Fractions
we have in the result of (3.3.52) an expression for column m+1 of M. By putting j = m+1
and i = µ + 1, we obtain (3.3.50). The proof of the recurrence is left to the reader. (Think
of Pascal’s triangle.)
The integration operator D−1 is deﬁned by the relation
(D−1f )(x) =
 x
f (t) dt.
The lower limit is not ﬁxed, so D−1f contains an arbitrary integration constant. Note that
DD−1f = f , while D−1Df = f + C, where C is the integration constant. A difference
expression like
D−1f (b) −D−1f (a) =
 b
a
f (t) dt
is uniquely deﬁned. So is δD−1f , but D−1δf has an integration constant.
A right-hand inverse can be also deﬁned for the operators 4, ∇, and δ. For example,
(∇−1u)n = j=n uj has an arbitrary summation constant but, for example, ∇∇−1 = 1,
and 4∇−1 = E∇∇−1 = E are uniquely deﬁned.
One can make the inverses unique by restricting the class of sequences (or functions).
For example, if we require that ∞
j=0 uj is convergent, and make the convention that
(4−1u)n →0 as n →∞, then 4−1un = −∞
j=n uj; notice the minus sign. Also notice
that this is consistent with the following formal computation:
(1 + E + E2 + · · ·)un = (1 −E)−1un = −4−1un.
We recommend, however, some extra care with inﬁnite expansions into powers of operators
like E that is not covered by Theorem 3.3.7, but the ﬁnite expansion
1 + E + E2 + · · · + En−1 = (En −1)(E −1)−1
(3.3.53)
is valid.
In Chapter 5 we will use operator methods together with the Cauchy–FFT method
for ﬁnding the Newton–Cotes’ formulas for symmetric numerical integration. Operator
techniques can also be extended to functions of several variables. The basic relation is
again the operator form of Taylor’s formula, which in the case of two variables reads
u(x0 + h, y0 + k) = exp

h ∂
∂x + k ∂
∂y

u(x0, y0)
= exp

h ∂
∂x

exp

k ∂
∂y

u(x0, y0).
(3.3.54)

3.3. Difference Operators and Operator Expansions
251
3.3.5
Single Linear Difference Equations
Historically, the term difference equation was probably ﬁrst used in connection with an
equation of the form
b04kyn + b14k−1yn + · · · + bk−14yn + bkyn = 0,
n = 0, 1, 2, . . . ,
which resembles a linear homogeneous differential equation. It follows, however, from the
discussion after (3.3.1) and (3.3.3) that this equation can also be written in the form
yn+k + a1yn+k−1 + · · · + akyn = 0,
(3.3.55)
and nowadays this is what one usually means by a single homogeneous linear difference
equation of kth order with constant coefﬁcients; a difference equation without differences.
More generally, if we let the coefﬁcients ai depend on n we have a linear difference equation
with variable coefﬁcients. If we replace the zero on the right-hand side with some known
quantity rn, we have an inhomogeneous linear difference equation.
These types of equations are the main topic of this section. The coefﬁcients and the
unknown are real or complex numbers. We shall occasionally see examples of more general
types of difference equations, e.g., a nonlinear difference equation
F(yn+k, yn+k−1, . . . , yn) = 0,
and ﬁrst order systems of difference equations, i.e.,
yn+1 = Anyn + rn,
where rn and yn are vectors while An is a square matrix. Finally, partial difference equations,
where you have two (or more) subscripts in the unknown, occur often as numerical methods
for partial differential equations, but they have many other important applications too.
A difference equation can be viewed as a recurrence relation. With given values of
y0, y1, . . . , yk−1, called the initial values or the seed of the recurrence, we can succes-
sively compute yk, yk+1, yk+2, . . .; we see that the general solution of a kth order difference
equation contains k arbitrary constants, just like the general solution of the kth order differ-
ential equation. There are other important similarities between difference and differential
equations, for example, the following superposition result.
Lemma 3.3.11.
The general solution of a nonhomogeneous linear difference equation (also with vari-
able coefﬁcients) is the sum of one particular solution of it, and the general solution of the
corresponding homogeneous difference equation.
In practical computing, the recursive computation of the solution of difference equa-
tions is most common. It was mentioned at the end of Sec. 3.2.3 that many important
functions, e.g., Bessel functions and orthogonal polynomials, satisfy second order linear
difference equations with variable coefﬁcients (although this terminology was not used
there). Other important applications are the multistep methods for ordinary differential
equations.

252
Chapter 3. Series, Operators, and Continued Fractions
In such an application you are usually interested in the solution for one particular initial
condition, but due to rounding errors in the initial values you obtain another solution. It is
therefore of interest to know the behavior of the solutions of the corresponding homogeneous
difference equation. The questions are
• Can we use a recurrence to ﬁnd the desired solution accurately?
• How shall we use a recurrence, forward or backward?
Forward recurrence is the type we described above. In backward recurrence we choose
some large integer N, and give (almost) arbitrary values of yN+i, i = 0 : k −1, as seeds,
and compute yn for n = N −1 : −1 : 0.
We have seen this already in Example 1.2.1 for an inhomogeneous ﬁrst order recur-
rence relation. There it was found that the forward recurrence was useless, while backward
recurrence, with a rather naturally chosen seed, gave satisfactory results.
It is often like
this, though not always. In Problem 1.2.7 it is the other way around: the forward recurrence
is useful, and the backward recurrence is useless.
Sometimes boundary values are prescribed for a difference equation instead of initial
values, (say) p values at the beginning and q = k −p values at the end, e.g., the values
of y0, y1, . . . , yp−1 and yN−q, . . . ,N−1 , yN are given. Then the difference equation can be
treated as a linear system with N −k unknown. This also holds for a difference equation
with variable coefﬁcients and for an inhomogeneous difference equation. From the point
of view of numerical stability, such a treatment can be better than either recurrence. The
amount of work is somewhat larger, not very much though, for the matrix is a band matrix.
For a ﬁxed number of bands the amount of work to solve such a linear system is proportional
to the number of unknowns. An important particular case is when k = 2, p = q = 1; the
linear system is then tridiagonal. An algorithm for tridiagonal linear systems is described
in Example 1.3.3.
Another similarity for differential and difference equations is that the general solu-
tion of a linear equation with constant coefﬁcients has a simple closed form. Although,
in most cases real-world problems have variable coefﬁcients (or are nonlinear), one can
often formulate a class of model problems with constant coefﬁcients with similar features.
The analysis of such model problems can give hints, e.g., whether forward or backward
recurrence should be used, or other questions related to the design and the analysis of the
numerical stability of a numerical method for a more complicated problem.
We shall therefore now study how to solve a single homogeneous linear difference
equation with constant coefﬁcients (3.3.55), i.e.,
yn+k + a1yn+k−1 + · · · + akyn = 0.
It is satisﬁed by the sequence {yj}, where yj = cuj (u ̸= 0, c ̸= 0) if and only if
un+k + a1un+k−1 + · · · + akun = 0, i.e., when
φ(u) ≡uk + a1uk−1 + · · · + ak = 0.
(3.3.56)
Equation (3.3.56) is called the characteristic equation of (3.3.55); φ(u) is called the char-
acteristic polynomial.

3.3. Difference Operators and Operator Expansions
253
Theorem 3.3.12.
If the characteristic equation has k different roots, u1, . . . , uk, then the general solu-
tion of (3.3.55) is given by the sequences {yn}, where
yn = c1un
1 + c2un
2 + · · · + ckun
k,
(3.3.57)
where c1, c2, . . . , ck are arbitrary constants.
Proof. That {yn} satisﬁes (3.3.55) follows from the previous comments and from the fact
that the equation is linear. The parameters c1, c2, . . . , ck can be adjusted to arbitrary initial
conditions y0, y1, . . . , yk−1 by solving the system of equations


1
1
· · ·
1
u1
u2
· · ·
uk
...
...
...
uk−1
1
uk−1
2
· · ·
uk−1
k




c1
c2
...
ck

=


y0
y1
...
yk−1

.
The matrix is a Vandermonde matrix and its determinant is thus equal to the product of
all differences (ui −uj), i ≥j, 1 < i ≤k, which is nonzero; see the proof of Theorem
3.3.4.
Example 3.3.11.
Consider the difference equation yn+2 −5yn+1 + 6yn = 0 with initial conditions
y0 = 0, y1 = 1. Forward recurrence yields y2 = 5, y3 = 19, y4 = 65, . . . .
The characteristic equation u2 −5u + 6 = 0 has roots u1 = 3, u2 = 2; Hence, the
general solution is yn = c13n + c22n. The initial conditions give the system of equations
c1 + c2 = 0,
3c1 + 2c2 = 1,
with solution c1 = 1, c2 = −1; hence yn = 3n −2n.
As a check we ﬁnd y2 = 5, y3 = 19 in agreement with the results found by using
forward recurrence.
Example 3.3.12.
Consider the difference equation
Tn+1(x) −2xTn(x) + Tn−1(x) = 0,
n ≥1,
−1 < x < 1,
with initial conditions T0(x) = 1, T1(x) = x. We obtain T2(x) = 2x2−1, T3(x) = 4x3−3x,
T4(x) = 8x4 −8x2 + 1, . . . . By induction, Tn(x) is an nth degree polynomial in x.
We can obtain a simple formula for Tn(x) by solving the difference equation. The
characteristic equation is u2 −2xu + 1 = 0, with roots u = x ± i
√
1 −x2. Set x = cos φ,
0 < x < π. Then u = cos φ ± i sin φ, and thus u1 = eiφ, u2 = e−iφ, u1 ̸= u2. The general
solution is Tn(x) = c1einφ + c2e−inφ, and the initial conditions give
c1 + c2 = 1,
c1eiφ + c2e−iφ = cos φ,
with solution c1 = c2 = 1/2. Hence, Tn(x) = cos(nφ), x = cos φ. These polynomials
are thus identical to the important Chebyshev polynomials Tn(x) that were introduced in
(3.2.21).

254
Chapter 3. Series, Operators, and Continued Fractions
We excluded the cases x = 1 and x = −1, i.e., φ = 0 and φ = π, respectively. For the
particular initial values of this example, there are no difﬁculties; the solution Tn(x) = cos nφ
depends continuously on φ, and as φ →0 or φ →π, Tn(x) = cos nφ converges to 1 for
all n or (−1)n for all n, respectively.
When we ask for the general solution of the difference equation matters are a little
more complicated, because the characteristic equation has in these cases a double root:
u = 1 for x = 1, u = −1 for x = −1. Although they are thus covered by the next theorem,
we shall look at them directly because they are easy to solve, and they are a good preparation
for the general case.
If x = 1, the difference equation reads Tn+1 −2Tn + Tn−1 = 0, i.e., 42Tn = 0. We
know from before (see, e.g., Theorem 3.3.4) that this is satisﬁed if and only if Tn = an + b.
The solution is no longer built up by exponentials; a linear term is there too.
If x = −1, the difference equation reads Tn+1 + 2Tn + Tn−1 = 0. Set Tn = (−1)nVn.
The difference equation becomes, after division by (−1)n+1, Vn+1 −2Vn + Vn−1 = 0, with
the general solution Vn = an + b; hence Tn = (−1)n(an + b).
Theorem 3.3.13.
When ui is an mi-fold root of the characteristic equation, then the difference (3.3.55)
is satisﬁed by the sequence {yn}, where
yn = Pi(n)un
i
and Pi is an arbitrary polynomial in Pmi. The general solution of the difference equation is
a linear combination of solutions of this form using all the distinct roots of the characteristic
equation.
Proof. We can write the polynomial P ∈Pmi in the form
Pi(n) = b1 + b2n + b3n(n −1) + · · · + bmin(n −1) · · · (n −mi + 2).
Thus it is sufﬁcient to show that (3.3.55) is satisﬁed when
yn = n(n −1) · · · (n −p + 1)un
i = (up∂p(un)/∂up)u=ui,
p = 1 : mi −1.
(3.3.58)
Substitute this in the left-hand side of (3.3.55):
up ∂p
∂up

un+k + a1un+k−1 + · · · + akun
= up ∂p
∂up

φ(u)un
= up

φ(p)(u)un +
p
1

φ(p−1)(u)nun−1 + · · · +
p
p

φ(u) ∂p
∂up (un)

.
The last manipulation was made using Leibniz’s rule.
Now φ and all the derivatives of φ which occur in the above expression are zero for
u = ui, since ui is an mi-fold root. Thus the sequences {yn} in (3.3.58) satisfy the difference
equation. We obtain a solution with  mi = k parameters by the linear combination of
such solutions derived from the different roots of the characteristic equation.
It can be shown (see Henrici [192, p. 214]) that these solutions are linearly indepen-
dent. (This also follows from a different proof, where a difference equation of higher order

3.3. Difference Operators and Operator Expansions
255
is transformed to a system of ﬁrst order difference equations. This transformation also
leads to other ways of handling inhomogeneous difference equations than those which are
presented in this section.)
Note that the double root cases discussed in the previous example are completely in
accordance with this theorem. We look at one more example.
Example 3.3.13.
Consider the difference equation yn+3 −3yn+2 +4yn = 0. The characteristic equation
is u3 −3u2 + 4 = 0 with roots u1 = −1, u2 = u3 = 2. Hence, the general solution reads
yn = c1(−1)n + (c2 + c3n)2n.
For a nonhomogeneous linear difference equation of order k, one can often ﬁnd a
particular solution by the use of an Ansatz84 with undetermined coefﬁcients; thereafter,
by Lemma 3.3.11 one can get the general solution by adding the general solution of the
homogeneous difference equation.
Example 3.3.14.
Consider the difference equation yn+1 −2yn = an, with initial condition y0 = 1. Try
the Ansatz yn = can. One gets
can+1 −2can = an,
c = 1/(a −2),
a ̸= 2.
Thus the general solution is yn = an/(a −2) + c12n. By the initial condition, c1 =
1 −1/(a −2), hence
yn = an −2n
a −2 + 2n.
(3.3.59)
When a →2, l’Hôpital’s rule gives yn = 2n + n2n−1. Notice how the Ansatz must be
modiﬁed when a is a root of the characteristic equation.
The general rule when the right-hand side is of the form P(n)an (or a sum of such
terms), where P is a polynomial, is that the contribution of this term to yn is Q(n)an, where
Q is a polynomial. If a does not satisfy the characteristic equation, then deg Q = deg P;
if a is a single or a double root of the characteristic equation, then deg Q = deg P + 1 or
deg Q = deg P + 2, respectively, and so on. The coefﬁcients of Q are determined by the
insertion of yn = Q(n)an on the left-hand side of the equation and matching the coefﬁcients
with the right-hand side.
Another way to ﬁnd a particular solution is based on the calculus of operators. Let an
inhomogeneous difference equation be given in the form ψ(Q)yn = bn, where Q is one of
the operators 4, δ, and ∇, or an operator easily derived from these, for example, 1
6δ2 (see
Problem 3.3.27(d)). In Sec. 3.1.5 ψ(Q)−1 was deﬁned by the formal power series with the
same coefﬁcients as the Maclaurin series for the function 1/ψ(z), z ∈C, ψ(0) ̸= 0. In
simple cases, e.g., if ψ(Q) = a0 + a1Q, these coefﬁcients are usually easily found. Then
84AnAnsatz (German term) is an assumed form for a mathematical statement that is not based on any underlying
theory or principle.

256
Chapter 3. Series, Operators, and Continued Fractions
ψ(Q)−1bn is a particular solution of the difference equation ψ(Q)yn = bn; the truncated
expansions approximate this. Note that if Q = δ or ∇, the inﬁnite expansion demands that
bn is also deﬁned if n < 0.
Note that a similar technique, with the operator D, can also be applied to linear
differential equations. Today this technique has to a large extent been replaced by the
Laplace transform,85 which yields essentially the same algebraic calculations as operator
calculus.
In some branches of applied mathematics it is popular to treat nonhomogeneous dif-
ference equations by means of a generating function, also called the z-transform, since
both the deﬁnition and the practical computations are analogous to the Laplace transform.
The z-transform of the sequence y = {yn}∞
0 is
Y(z) =
∞

n=0
ynz−n.
(3.3.60)
Note that the sequence {Ey} = {yn+1} has the z-transform zY(z) −y0, {E2y} = {yn+2} has
the z-transform z2Y(z) −y0z −y1, etc.
If Y(z) is available in analytic form, it can often be brought to a sum of functions whose
inverse z-transforms are known by means of various analytic techniques, notably expansion
into partial fractions if Y(z) is a rational function. On the other hand, if numerical values
of Y(z) have been computed for complex values of z on some circle in C by means of
an algorithm, then yn can be determined by an obvious modiﬁcation of the Cauchy–FFT
method described in Sec. 3.2.2 (for expansions into negative powers of z). More information
about the z-transform can be found in Strang [339, Sec. 6.3].
We are now in a position to exemplify in more detail the use of linear difference
equations to studies of numerical stability, of the type mentioned above.
Theorem 3.3.14 (Root Condition).
Necessary and sufﬁcient for boundedness (stability) of all solutions of the difference
(3.3.55) for all positive n is the following root condition: (We shall say either that a
difference equation or that a characteristic polynomial satisﬁes the root condition; the
meaning is the same.)
i. All roots of characteristic (3.3.56) are located inside or on the unit circle |z| ≤1;
ii. The roots on the unit circle are simple.
Proof. The proof follows directly from Theorem 3.3.13.
This root condition corresponds to cases where it is the absolute error that matters.
It is basic in the theory of linear multistep methods for ordinary differential equations.
Computer graphics and an algebraic criterion due to Schur are useful for investigations
of the root condition, particularly, if the recurrence relation under investigation contains
parameters.
85The Laplace transform is traditionally used for similar problems for linear differential equations, for example,
in electrical engineering.

3.3. Difference Operators and Operator Expansions
257
There are important applications of single linear difference equations to the study of
the stability of numerical methods. When a recurrence is used one is usually interested
in the solution for one particular initial condition. But a rounding error in an initial value
produces a different solution, and it is therefore of interest to know the behavior of other
solutions of the corresponding homogeneous difference equation. We have seen this already
in Example 1.2.1 for an inhomogeneous ﬁrst order recurrence relation, but it is even more
important for recurrence relations of higher order.
The following example is based on a study done by Todd86 in 1950 (see [352]).
Example 3.3.15.
Consider the initial value problem
y′′(x) = −y,
y(0) = 0,
y′(0) = 1,
(3.3.61)
with the exact solution y(x) = sin x. To compute an approximate solution yk = y(xk) at
equidistant points xk = kh, where h is a step length, we approximate the second derivative
according to (3.3.43):
h2y′′
k = δ2yk + δ4yk
12 + δ6yk
90 + · · · .
(3.3.62)
We ﬁrst use the ﬁrst term only; the second term shows that the truncation error of this
approximation of y′′
k is asymptotically h2y(4)/12. We then obtain the difference equation
h−2δ2yk = −yk or, in other words,
yk+2 = (2 −h2)yk+1 −yk,
y0 = 0,
(3.3.63)
where a suitable value of y1 is to be assigned. In the third column of Table 3.3.2 we show
the results obtained using this recursion formula with h = 0.1 and y1 = sin 0.1. All
computations in this example were carried out using IEEE double precision arithmetic. We
obtain about three digits of accuracy at the end of the interval x = 1.2.
Since the algorithm was based on a second order accurate approximation of y′′ one
may expect that the solution of the differential equation is also second order accurate. This
turns out to be correct in this case; for example, if we divide the step size by two, the errors
will approximately be divided by four. We shall, however, see that we cannot always draw
conclusions of this kind; we also have to take the numerical stability into account.
In the hope of obtaining a more accurate solution, we shall now use one more term in
the expansion (3.3.62); the third term then shows that the truncation error of this approxi-
mation is asymptotically h4y(6)/90. The difference equation now reads
δ2yk −1
12δ4yk = −h2yk,
(3.3.64)
or
yk+2 = 16yk+1 −(30 −12h2)yk + 16yk−1 −yk−2,
k ≥2,
y0 = 0,
(3.3.65)
86John Todd (1911–2007), born in Ireland, was a pioneer in computing and numerical analysis. During World
War II he was head of the British Admirality Computing Services. At the end of the war he earned his nickname
“Savior of Oberwolfach” by protecting the Mathematical Research Institute at Oberwolfach in Germany from
destruction by Moroccan troops. In 1947 he joined the National Bureau of Standards (NBS) in Washington, DC,
where he became head of the Computation Laboratory and in 1954 Chief of the Numerical Analysis Section. In
1957 he took up a position as Professor of Mathematics at the California Institute of Technology.

258
Chapter 3. Series, Operators, and Continued Fractions
Table 3.3.2. Integrating y′′ = −y, y(0) = 0, y′(0) = 1; the letters U and S in the
headings of the last two columns refer to “Unstable” and “Stable.”
xk
sin xk
2nd order
4th order U
4th order S
0.1
0.0998334166
0.0998334
0.0998334166
0.0998334166
0.2
0.1986693308
0.1986685
0.1986693307
0.1986693303
0.3
0.2955202067
0.2955169
0.2955202067
0.2955202050
0.4
0.3894183423
0.3894101
0.3894183688
0.3894183382
0.5
0.4794255386
0.4794093
0.4794126947
0.4794255305
0.6
0.5646424734
0.5646143
0.5643841035
0.5646424593
0.7
0.6442176872
0.6441732
0.6403394433
0.6442176650
0.8
0.7173560909
0.7172903
0.6627719932
0.7173560580
0.9
0.7833269096
0.7832346
0.0254286676
0.7833268635
1.0
0.8414709848
0.8413465
−9.654611899
0.8414709226
1.1
0.8912073601
0.8910450
−144.4011267
0.8912072789
1.2
0.9320390860
0.9318329
−2010.123761
0.9320389830
where starting values for y1, y2, and y3 need to be assigned. We choose the correct values
of the solution rounded to double precision. The results from this recursion are shown in
the fourth column of Table 3.3.2. We see that disaster has struck—the recursion is severely
unstable! Already for x = 0.6 the results are less accurate than the second order scheme.
For x ≥0.9 the errors completely dominate the unstable method.
We shall now look at these difference equations from the point of view of the root
condition. The characteristic equation for (3.3.63) reads u2 −(2 −h2)u + 1 = 0, and since
|2 −h2| < 2, direct computation shows that it has simple roots of unit modulus. The root
condition is satisﬁed. By Example 3.3.12, the solution of (3.3.63) is yn = Tn(1−h2/2). For
the second order method the absolute error at x = 1.2 is approximately 2.1 · 10−4, whereas
for the stable fourth order method the error is 1.0 · 10−7.
For (3.3.65) the characteristic equation reads u4−16u3+(30−12h2)u2−16u+1 = 0.
We see immediately that the root condition cannot be satisﬁed. Since the sum of the roots
equals 16, it is impossible that all roots are inside or on the unit circle. In fact, the largest root
equals 13.94. So, a tiny error at x = 0.1 has been multiplied by 13.9414 ≈1016 at the end.
Astable fourth order accurate method can easily be constructed. Using the differential
equation we replace the term δ4yk in (3.3.64) by h2δ2y′′
k = −h2δ2yk. This leads to the
recursion formula87
yk+1 =

2 −
h2
1 + h2/12

yk −yk−1,
y0 = 0,
(3.3.66)
which can be traced back at least to B. Numerov (1924) (cf. Problem 3.4.27). This difference
equation satisﬁes the root condition if h2 < 6 (see Problem 3.3.25 (a)). It requires y0,
y1 ≈y(h)astheseed. Theresultsusingthisrecursionformulawithh = 0.1andy1 = sin 0.1
87Boris Vaisyevich Numerov (1891–1941) Russian astronomer and professor at the University of Leningrad.

3.3. Difference Operators and Operator Expansions
259
are shown in the ﬁfth column of Table 3.3.2. The error at the end is about 2·10−7, which is
much better than the 3.7·10−4 obtained with the second order method.
Remark 3.3.2.
If the solution of the original problem is itself strongly decreasing or
strongly increasing, one should consider the location of the characteristic roots with respect
to a circle in the complex plane that corresponds to the interesting solution. For example, if
the interesting root is 0.8, then a root equal to −0.9 causes oscillations that may eventually
become disturbing if one is interested in relative accuracy in a long run, even if the oscillating
solution is small in the beginning.
Manyproblemscontainhomogeneousornonhomogeneouslineardifferenceequations
with variable coefﬁcients, for which the solutions are not known in a simple closed form.
We now conﬁne the discussion to the cases where the original problem is to compute a
particular solution of a second order difference equation with variable coefﬁcients; several
interesting problems of this type were mentioned above, and we formulated the questions
of whether we can use a recurrence to ﬁnd the desired solution accurately, and how we
shall use a recurrence, forward or backward. Typically the original problem contains some
parameter, and one usually wants to make a study for an interval of parameter values.
Suchquestionsaresometimesstudiedwithfrozencoefﬁcients, i.e., themodelproblems
are in the class of difference equations with constant coefﬁcients in the range of the actual
coefﬁcients of the original problem. If one of the types of recurrence is satisfactory (i.e.,
numerically stable in some sense) for all model problems, one would like to conclude that
they are satisfactory also for the original problem, but the conclusion is not always valid
without further restrictions on the coefﬁcients—see a counterexample in Problem 3.3.27.
The technique with frozen coefﬁcients provides just a hint that should always be
checked by numerical experiments on the original problem. It is beyond the scope of this
text to discuss what restrictions are needed. If the coefﬁcients of the original problem are
slowly varying, however, there is a good chance that the numerical tests will conﬁrm the
hint—but again, how slowly is “slowly”? A warning against the use of one of the types of
recurrence may also be a valuable result of a study, although it is negative.
The following lemma exempliﬁes a type of tool that may be useful in such cases. The
proof is left for Problem 3.3.24 (a). Another useful tool is presented in Problem 3.3.26 (a)
and applied in Problem 3.3.26 (b).
Lemma 3.3.15.
Suppose that the wanted sequence y∗
n satisﬁes a difference equation (with constant
coefﬁcients),
αyn+1 + βyn −γyn−1 = 0,
(α > γ > 0, β > 0),
and that y∗
n is known to be positive for all sufﬁciently large n. Then the characteristic roots
can be written 0 < u1 < 1, u2 < 0, and |u2| > u1. Then y∗
n is unique apart from a positive
factor c; y∗
n = cun
1, c > 0.
A solution ¯yn, called the trial solution, that is approximately of this form can be
computed for n = N : −1 : 0 by backward recurrence starting with the “seed” yN+1 = 0,
yN = 1. If an accurate value of y∗
0 is given, the desired solution is
y∗
n = ¯yny∗
0/ ¯y0,

260
Chapter 3. Series, Operators, and Continued Fractions
with a relative error approximately proportional to (u2/u1)n−N (neglecting a possible error
in y∗
0). (If y∗
n is deﬁned by some other condition, one can proceed analogously.)
The forward recurrence is not recommended for ﬁnding y∗
n in this case, since the
positive term c1un
1 will eventually be drowned by the oscillating term c2un
2 that will be
introduced by the rounding errors. The proof is left for Problem 3.3.27. Even if y0 (in the
use of the forward recurrence) has no rounding errors, such errors committed at later stages
will yield similar contributions to the numerical results.
Example 3.3.16.
The “original problem” is to compute the parabolic cylinder function U(a, x) which
satisﬁes the difference equation

a + 1
2

U(a + 1, x) + xU(a, x) −U(a −1, x) = 0;
see Handbook [1, Chap. 19, in particular Example 19.28.1].
To be more precise, we consider the case x = 5. Given U(3, 5) = 5.2847 · 10−6
(obtained from a table in [1, p. 710]), we want to determine U(a, 5) for integer values of
a, a > 3, as long as |U(a, 5)| > 10−15. We guess (a priori) that the discussion can be
restricted to the interval (say) a = [3, 15]. The above lemma then gives the hint of a
backward recurrence, for a = a′ −1 : −1 : 3 for some appropriate a′ (see below), in order
to obtain a trial solution ¯Ua with the seed ¯Ua′ = 1, ¯Ua′+1 = 0. Then the wanted solution
becomes, by the lemma (with changed notation),
U(a, 5) = ¯UaU(3, 5)/ ¯U3.
The positive characteristic root of the frozen difference equation varies from 0.174 to 0.14
for a = 5 : 15, while the modulus of the negative root is between 6.4 and 3.3 times as
large. This motivates a choice of a′ ≈4 + (−9 −log 5.3)/ ln 0.174 ≈17 for the backward
recursion; it seems advisable to choose a′ (say) four units larger than the value where U
becomes negligible.
Forward recurrence with correctly rounded starting values U(3, 5) = 5.2847 · 10−6,
U(4, 5) = 9.172 · 10−7 gives oscillating (absolute) errors of relatively slowly decreasing
amplitude, approximately 10−11, that gradually drown the exponentially decreasing true
solution. The estimate of U(a, 5) itself became negative for a = 10, and then the results
oscillated with approximate amplitude 10−11, while the correct results decrease from the
order of 10−11 to 10−15 as a = 10 : 15. The details are left for Problem 3.3.25 (b).
It is conceivable that this procedure can be used for all x in some interval around ﬁve,
but we refrain from presenting the properties of the parabolic cylinder function needed for
determining the interval.
If the problem is nonlinear, one can instead solve the original problem with two
seeds, (say) y′
N, y′′
N, and study how the results deviate. The seeds should be so close that a
linearization like f (y′
n) −f (y′′
n) ≈rn(y′
n −y′′
n) is acceptable, but y′
n −y′′
n should be well
above the rounding error level. A more recent and general treatment of these matters is
found in [96, Chapter 6].

Problems and Computer Exercises
261
Review Questions
3.3.1 Give expressions for the shift operator Ek in terms of 4, ∇, and hD, and expressions
for the central difference operator δ2 in terms of E and hD.
3.3.2 Derive the best upper bound for the error of 4ny0, if we only know that the absolute
value of the error of yi, i = 0, . . . , n does not exceed ϵ.
3.3.3 Thereisatheorem(andacorollary)aboutexistenceanduniquenessofapproximation
formulas of a certain type that are exact for polynomials of certain class. Formulate
these results, and sketch the proofs.
3.3.4 What bound can be given for the kth difference of a function in terms of a bound for
the kth derivative of the same function?
3.3.5 Formulate the basic theorem concerning the use of operator expansions for deriving
approximation formulas for linear operators.
3.3.6 Discuss how various sources of error inﬂuence the choice of step length in numerical
differentiation.
3.3.7 Formulate Peano’s remainder theorem, and compute the Peano kernel for a given
symmetric functional (with at most four subintervals).
3.3.8 Express polynomial interpolation formulas in terms of forward and backward dif-
ference operators.
3.3.9 Give Stirling’s interpolation formula for quadratic interpolation with approximate
bounds for truncation error and irregular error.
3.3.10 Derive central difference formulas for f ′(x0) and f ′′(x0) that are exact for f ∈P4.
They should only use function values at xj, j = 0, ±1, ±2, . . . , as many as needed.
Give asymptotic error estimates.
3.3.11 Derive the formula for the general solution of the difference equation yn+k +
a1yn+k−1 + · · · + akyn = 0, when the characteristic equation has simple roots only.
What is the general solution when the characteristic equation has multiple roots?
3.3.12 What is the general solution of the difference equation 4kyn = an + b?
Problems and Computer Exercises
3.3.1 Prove the formula (3.3.12) for the determinant of the Vandermonde matrix V =
V (x1, . . . , xk). For deﬁnition and properties of a determinant, SectionA.3 in Online
Appendix A.
Hint: Considered as a function of x1, det V is a polynomial of degree k −1. Since
the determinant is zero if two columns are identical, this polynomial has the roots
x1 = xj, j = 2 : k. Hence
det V = c(x2, . . . , xk)(x1 −x2) · · · (x1 −xk),

262
Chapter 3. Series, Operators, and Continued Fractions
where c does not depend on x1. Similarly, viewed as a polynomial of x2 the deter-
minant must contain the factor (x2 −x1)(x2 −x3) · · · (x2 −xk), etc.
3.3.2 (a) Show that (1 + 4)(1 −∇) = 1, 4 −∇= 4∇= δ2 = E −2 + E−1, and that
δ2yn = yn+1 −2yn + yn−1.
(b) Let 4pyn, ∇pym, δpyk all denote the same quantity. How are n, m, k connected?
Along which lines in the difference scheme are the subscripts constant?
(c) Given the values of yn, ∇yn, . . . , ∇kyn, for a particular value of n, ﬁnd a recur-
rence relation for computing yn, yn−1, . . . , yn−k, by simple additions only. On the
way you obtain the full difference scheme of this sequence.
(d) Repeated summation by parts. Show that if u1 = uN = v1 = vN = 0, then
N−1

n=1
un42vn−1 = −
N−1

n=1
4un4vn =
N−1

n=1
vn42un−1.
(e) Show that if 4kvn →0, as n →∞, then ∞
n=m 4kvn = −4k−1vm.
(f) Show that (µδ3 + 2µδ)f0 = f2 −f−2.
(g) Show the validity of the algorithm in (3.3.40). Babbage’s favorite example was
f (x) = x2 +x +41. Given f (x) for x = 0, 1, 2, compute the backward differences
for x = 2 and use the algorithm to obtain f (3). Then compute f (x) for (say)
x = 4 : 10, by repeated use of the algorithm. (This is simple enough for paper and
pencil, since the algorithm contains only additions.)
3.3.3 (a) Prove by induction, the following two formulas:
4j
x
x
k

=

x
k −j

,
j ≤k,
where 4x means differencing with respect to x, with h = 1, and
4jx−1 =
(−h)jj!
x(x + h) · · · (x + jh).
Find the analogous expression for ∇jx−1.
(b) What formulas with derivatives instead of differences are these formulas analo-
gous to?
(c) Show the following formulas if x, a are integers:
x−1

n=a

n
k −1

=
x
k

−
a
k

,
∞

n=x
1
n(n + 1) · · · (n + j) = 1
j ·
1
x(x + 1) · · · (x + j −1).
Modify these results for noninteger x; x −a is still an integer.

Problems and Computer Exercises
263
(d) Suppose that b ̸= 0, −1, −2, . . . , and set
c0(a, b) = 1,
cn(a, b) = a(a + 1) . . . (a + n −1)
b(b + 1) . . . (b + n −1) ,
n = 1, 2, 3, . . . .
Show by induction that
(−4)kcn(a, b) = ck(b −a, b)cn(a, b + k),
and that hence (−4)nc0(a, b) = cn(b −a, b).
(e) Compute for a = e, b = π (say), cn(a, b), n = 1 : 100. How do you avoid
overﬂow? Compute 4nc0(a, b), both numerically by the difference scheme and
according to the formula in (d). Compare the results and formulate your experiences.
Do the same with a = e, b = π2.
Do the same with 4jx−1 for various values of x, j, and h.
3.3.4 Set
Yord = (yn−k, yn−k+1, . . . , yn−1, yn),
Ydif = (∇kyn, ∇k−1yn, . . . , ∇yn, yn).
Note that the results of this problem also hold if the yj are column vectors.
(a) Find a matrix P such that Ydif = YordP. Show that
Yord = Ydif P,
hence
P −1 = P.
How do you generate this matrix by means of a simple recurrence relation?
Hint: P is related to the Pascal matrix, but do not forget the minus signs in this
triangular matrix. Compare Problem 1.2.4.
(b) Suppose that k
j=0 αjE−j and k
j=0 aj∇j represent the same operator. Set
α = (αk, αk−1, . . . , α0)T and a = (ak, ak−1, . . . , a0)T , i.e., Yord · α ≡Ydif · a.
Show that Pa = α, Pα = a.
(c) The matrix P depends on the integer k. Is it true that the matrix which is obtained
for a certain k is a submatrix of the matrix you obtain for a larger value of k?
(d) Compare this method of performing the mapping Yord 9→Ydif with the ordinary
construction of a difference scheme. Consider the number of arithmetic operations,
the kind of arithmetic operations, rounding errors, convenience of programming in
a language with matrix operations as primary operations, etc. In the same way,
compare this method of performing the inverse mapping with the algorithm in Prob-
lem 3.3.2 (c).
3.3.5 (a) Set f (x) = tan x. Compute by using the table of tan x (in Example 3.3.2) and
the interpolation and differentiation formulas given in the above examples (almost)
as accurately as possible the quantities
f ′(1.35), f (1.322), f ′(1.325), f ′′(1.32).
Estimate the inﬂuence of rounding errors of the function values and estimate the
truncation errors.

264
Chapter 3. Series, Operators, and Continued Fractions
(b) Write a program for computing a difference scheme. Use it for computing the
difference scheme for more accurate values of tan x, x = 1.30 : 0.01 : 1.35, and
calculate improved values of the functionals in (a). Compare the error estimates
with the true errors.
(c) Verify the assumptions of Theorem 3.3.7 for one of the three interpolation for-
mulas in Sec. 3.3.4.
(d) It is rather easy to ﬁnd the values at θ = 0 of the ﬁrst two derivatives of Stirling’s
interpolation formula. You ﬁnd thus explicit expressions for the coefﬁcients in the
formulas for f ′(x0) and f ′′(x0) in (3.3.47) and (3.3.43), respectively. Check nu-
merically a few coefﬁcients in these equations, and explain why they are reciprocals
of integers. Also note that each coefﬁcient in (3.3.47) has a simple relation to the
corresponding coefﬁcient in (3.3.43).
3.3.6 (a) Study Bickley’s table (Table 3.3.1) and derive some of the formulas, in particular
the expressions for δ and µ in terms of hD, and vice versa.
(b) Show that h−kδk −Dk has an expansion into even powers of h when k is even.
Find an analogous result for h−kµδk −Dk when k is odd.
3.3.7 (a) Compute
f ′(10)/12, f (3)(10)/720, f 5(10)/30,240
by means of (3.3.24), given values of f (x) for integer values of x. (This is asked for
in applications of Euler–Maclaurin’s formula, Sec. 3.4.5.) Do this for f (x) = x−3/2.
Compare with the correct derivatives. Then do the same for f (x) = (x3 + 1)−1/2.
(b) Study the backward differentiation formula; see (3.3.23) on a computer. Compute
f ′(1) for f (x) = 1/x, for h = 0.02 and h = 0.03, and compare with the exact
result. Make a semilogarithmic plot of the total error after n terms, n = 1 : 29.
Study also the sign of the error. For each case, try to ﬁnd out whether the achievable
accuracy is set by the rounding errors or by the semiconvergence of the series.
Hint: A formula mentioned in Problem 3.3.3 (a) can be helpful. Also note that this
problem is both similar and very different from the function tan x that was studied
in Example 3.3.6.
(c) Set xi = x0 + ih, t = (x −x2)/h. Show that
y(x) = y2 + t4y2 + t(t −1)
2
42y2 + t(t −1)(t −2)
6
43y1
equalstheinterpolationpolynomialinP4 determinedbythevalues(xi, yi), i = 1 : 4.
(Note that 43y1 is used instead of 43y2 which is located outside the scheme. Is this
ﬁne?)
3.3.8 A well-known formula reads
P(D)(eαtu(t)) = eαtP(D + α)u(t),
where P is an arbitrary polynomial. Prove this, as well as the following analogous
formulas:
P(E)(anun) = anP(aE)un,
P(4/h)

(1 + αh)nun

= (1 + αh)nP((1 + αh)4/h + α)un.
Can you ﬁnd a more beautiful or more practical variant?

Problems and Computer Exercises
265
3.3.9 Find the Peano kernel K(u) for the functional 42f (x0). Compute

R K(u) du both
by direct integration of K(u) and by computing 42f (x0) for a suitably chosen
function f .
3.3.10 Set yj = y(tj), y′
j = y′(tj). The following relations, due to John Adams,88 are of
great interest in the numerical integration of the differential equations y′ = f (y).
(a) Adams–Moulton’s implicit formula:
yn+1 −yn = h

a0y′
n+1 + a1∇y′
n+1 + a2∇2y′
n+1 + · · ·

.
Show that ∇= −ln(1 −∇)  ai∇i, and ﬁnd a recurrence relation for the coefﬁ-
cients. The coefﬁcients ai, i = 0 : 6, read as follows (check a few of them):
ai = 1, −1
2, −1
12, −1
24, −19
720, −3
160, −863
60,480.
Alternatively, derive the coefﬁcients by means of the matrix representation of a
truncated power series.
(b) Adams–Bashforth’s explicit formula:
yn+1 −yn = h

b0y′
n + b1∇y′
n + b2∇2y′
n + · · ·

.
Show that  bi∇iE−1 =  ai∇i, and that bn−bn−1 = an (n ≥1). The coefﬁcients
bi, i = 0 : 6, read as follows (check a few of them):
bi = 1, 1
2,
5
12, 3
8, 251
720,
95
288, 19,087
60,480.
(c) Apply the second order explicit Adams’ formula,
yn+1 −yn = h

y′
n + 1
2∇y′
n

,
to the differential equation y′ = −y2, with initial condition y(0) = 1 and step size
h = 0.1. Two initial values are needed for the recurrence: y0 = y(0) = 1, of course,
and we choose89 y1 = 0.9090. Then compute y′
0 = −y2
0, y′
1 = −y2
1. The explicit
Adams’ formula then yields yk, k ≥2. Compute a few steps, and compare with the
exact solution.90
3.3.11 Let yj = y0 + jh. Find the asymptotic behavior as h →0 of
(5(y1 −y0) + (y2 −y1))/(2h) −y′
0 −2y′
1.
Comment: This is of interest in the analysis of cubic spline interpolation in Sec. 4.4.2.
88John Couch Adams (1819–1892) was an English mathematician. While still an undergraduate he calculated
the irregularities of the motion of the planet Uranus, showing the existence of Neptune. He held the position as
Professor of Astronomy and Geometry at Cambridge for 32 years.
89There are several ways of obtaining y1 ≈y(h), for example, by one step of Runge’s second order method,
see Sec. 1.5.3, or by a series expansion, as in Sec. 1.2.4.
90For an implicit Adams’ formula it is necessary, in this example, to solve a quadratic equation in each step.

266
Chapter 3. Series, Operators, and Continued Fractions
3.3.12 It sometimes happens that the values of some function f (x) can be computed by
some very time-consuming algorithm only, and that one therefore computes it much
sparser than is needed for the application of the results. It was common in the pre-
computer age to compute sparse tables that needed interpolation by polynomials of
a high degree; then one needed a simple procedure for subtabulation, i.e., to obtain
a denser table for some section of the table. Today a similar situation may occur in
connection with the graphical output of the results of (say) a numerical solution of
a differential equation.
Deﬁne the operators ∇and ∇k by the equations
∇f (x) = f (x) −f (x −h),
∇kf (x) = f (x) −f (x −kh),
(k < 1),
and set
∇r
k =
∞

s=r
crs(k)∇s.
(a) Suppose that 4rf (x), r = 0 : m, has been computed. Suppose that k has
been chosen, that the coefﬁcients crs(k) are known for r ≤m, s ≤m, and that
4r
kf (a), r = 0 : m, has been computed. Design an algorithm for obtaining f (x),
x = a : kh : a + mkh, and ∇r
kf (a + mkh), r = 0 : m. (You can here, e.g., modify
the ideas of (3.3.40).) Then you can apply (3.3.40) directly to obtain a tabulation of
f (x), x = a + mkh : kh : b.
(b) In order to compute the coefﬁcients crs, r ≤s ≤m, you are advised to use a
subroutine for ﬁnding the coefﬁcients in the product of two polynomials, truncate
the result, and apply the subroutine m −1 times.
(c) Given
fn
∇fn
∇2fn
∇3fn
∇4fn
1
0.181269
0.032858
0.005956
0.001080
,
compute for k = 1
2, fn = f (xn), ∇j
k fn for j = 1 : 4. Compute f (xn −h) and
f (xn −2h) by means of both {∇jfn} and {∇j
k fn} and compare the results. How big
a difference in the results did you expect, and how big a difference do you obtain?
3.3.13 (a) Check Example 3.3.10 and the conclusions about the optimal step length in the
text. Investigate how the attainable accuracy varies with u, for these three values of
k, if u = 1.1 · 10−16.
(b) Study the analogous question for f ′′(x0) using the formula
f ′′(x0) ≈

1 −δ2
12 + δ4
90 −δ6
560 +
δ8
3150 −· · ·
 δ2f0
h2 .
3.3.14 Solve the following difference equations. A solution in complex form should be
transformed to real form. As a check, compute (say) y2 both by recurrence and by
your closed form expression.
(a) yn+2 −2yn+1 −3yn = 0, y0 = 0, y1 = 1
(b) yn+2 −4yn+1 + 5yn = 0, y0 = 0, y1 = 2
(c) There exist problems with two-point boundary conditions for difference equa-
tions, as for differential equations yn+2 −2yn+1 −3yn = 0, y0 = 0, y10 = 1

Problems and Computer Exercises
267
(d) yn+2 + 2yn+1 + yn = 0, y0 = 1, y1 = 0
(e) yn+1 −yn = 2n, y0 = 0
(f) yn+2 −2yn+1 −3yn = 1 + cos πn
3 , y0 = y1 = 0
Hint: The right-hand side is ℜ(1 + an), where a = eπi/3.
(g) yn+1 −yn = n, y0 = 0
(h) yn+1 −2yn = n2n, y0 = 0
3.3.15 (a) Prove Lemma 3.3.11.
(b) Consider the difference equation yn+2 −5yn+1 +6yn = 2n+3(−1)n. Determine
a particular solution of the form yn = an + b + c(−1)n.
(c) Solve the difference equation yn+2 −6yn+1 + 5yn = 2n + 3(−1)n. Why and
how must you change the form of the particular solution?
3.3.16 (a) Show that the difference equation k
i=0 bi4iyn = 0 has the characteristic equa-
tion k
i=0 bi(u −1)i = 0.
(b) Solve the difference equation 42yn −34yn + 2yn = 0, with initial condition
4y0 = 1.
(c) Find the characteristic equation for the equation k
i=0 bi∇iyn = 0.
3.3.17 The inﬂuence of wrong boundary slopes for cubic spline interpolation (with equidis-
tant data)—see Sec. 4.4.2—is governed by the difference equation
en+1 + 4en + en−1 = 0,
0 < n < m,
with e0, em given. Show that en ≈une0 + um−nem, u =
√
3 −2 ≈−0.27. More
precisely,
((en −(une0 + um−nem)
(( ≤2|u3m/2|
1 −|u|m max(|e0|, |em|).
Generalize the simpler of these results to other difference and differential equations.
3.3.18 The Fibonacci sequence is deﬁned by the recurrence relation
yn = yn−1 + yn−2,
y0 = 0,
y1 = 1.
(a) Calculate limn→∞yn+1/yn.
(b) The error of the secant method (see Sec. 6.2.2) satisﬁes approximately the differ-
ence equation ϵn = Cϵn−1ϵn−2. Solve this difference equation. Determine p such
that ϵn+1/ϵp
n tends to a ﬁnite nonzero limit as n →∞. Calculate this limit.
3.3.19 For several algorithms using the divide and conquer strategy, such as the FFT and
some sorting methods, one can ﬁnd that the work W(n) for the application of them
to data of size n satisﬁes a recurrence relation of the form
W(n) = 2W(n/2) + kn,
where k is a constant. Find W(n).
3.3.20 When the recursion
xn+2 = (32xn+1 −20xn)/3,
x0 = 3, x1 = 2,

268
Chapter 3. Series, Operators, and Continued Fractions
was solved numerically in low precision (23 bits mantissa), one obtained for xi,
i = 2 : 12, the (rounded) values
1.33, 0.89, 0.59, 0.40, 0.26, 0.18, 0.11, 0.03, −0.46, −5.05, −50.80.
Explain the difference from the exact values xn = 3(2/3)n.
3.3.21 (a) k, N are given integers 0 ≤k <≤N. A “discrete Green’s function” Gn,k, 0 ≤
n ≤N, for the central difference operator −4∇together with the boundary condi-
tions given below, is deﬁned as the solution un = Gn,k of the difference equation
with boundary conditions
−4∇un = δn,k,
u0 = uN = 0
(δn,k is Kronecker’s delta). Derive a fairly simple expression for Gn,k.
(b) Find (by computer) the inverse of the tridiagonal Toeplitz matrix91
A =


2
−1
−1
2
−1
−1
2
−1
−1
2
−1
−1
2


.
What is the relation between Problems 3.3.21 (a) and (b)? Find a formula for the
elements of A−1. Express the solution of the inhomogeneous difference equation
−4∇un = bn, u0 = uN = 0, both in terms of the Green function Gn,k and in terms
of A−1 (for general N).
(c)Trytoﬁndananalogousformula92 forthesolutionofaninhomogeneousboundary
value problem for the differential equation −u′′ = f (x), u(0) = u(1) = 0.
3.3.22 (a) Demonstrate the formula
∞

n=0
(−x)ncn
n!
= e−x
∞

n=0
xn(−4)nc0
n!
.
(3.3.67)
Hint: Use the relation e−xE = e−x(1+4) = e−xe−x4.
(b) For completely monotonic sequences {cn} and {(−4)nc0} are typically positive
anddecreasingsequences. Forsuchsequences, theleft-handsidebecomesextremely
ill-conditioned for large x, (say) x = 100, while the graph of the terms on the right-
hand side (if exactly computed) is bell-shaped, almost like the normal probability
density with mean x and standard deviation √x. We have called such a sum a bell
sum. Such positive sums can be computed with little effort and no trouble with
rounding errors, if their coefﬁcients are accurate.
Compute the left-hand side of (3.3.67), for cn = 1/(n + 1), x = 10 : 10 : 100, and
compute the right-hand side, both with numerically computed differences and with
91The inverse is a so-called semiseparable matrix.
92In a differential equation, analogous to Problem 3.3.21 (a), the Kronecker delta is to be replaced by the Dirac
delta function. Also note that the inverse of the differential operator here can be described as an integral operator
with the Green’s function as the “kernel.”

Problems and Computer Exercises
269
exact differences; the latter are found in Problem 3.3.3 (a). (In this particular case
you can also ﬁnd the exact sum.)
Suppose that the higher differences {(−4)nc0} have been computed recursively from
rounded values of cn. Explain why one may fear that the right-hand side of (3.3.67)
does not provide much better results than the left-hand side.
(c) Use (3.3.67) to derive the second expansion for erf(x) in Problem 3.2.8 from the
ﬁrst expansion.
Hint: Use one of the results of Problem 3.3.3 (a).
(d) If cn = cn(a, b) is deﬁned as in Problem 3.3.3 (d), then the left-hand side becomes
theMaclaurinexpansionoftheKummerfunctionM(a, b, −x); seetheHandbook[1,
Chap. 13]. Show that
M(a, b, −x) = e−xM(b −a, b, x)
by means of the results of Problems 3.3.23 (a) and 3.3.2 (d).93
3.3.23 (a) The difference equation yn + 5yn−1 = n−1 was discussed in Example 1.2.1. It
can also be written thus: (6 + 4)yn−1 = n−1. The expansion of (6 + 4)−1n−1 into
powers of 4/6 provides a particular solution of the difference equation.
Compute this numerically for a few values of n. Try to prove the convergence, with
or without the expression in Problem 3.3.3 (b). Is this the same as the particular
solution In =
 1
0 xn(x + 5)−1 dx that was studied in Example 1.2.1?
Hint: What happens as n →∞? Can more than one solution of this difference
equation be bounded as n →∞?
(b) Make a similar study of the difference equation related to the integral in Prob-
lem 1.2.7. Why does the argument suggested by the hint in (a) not work in this case?
Try another proof.
3.3.24 (a) Prove Lemma 3.3.15. How is the conclusion to be changed if we do not suppose
that γ < α, even though the coefﬁcients are still positive? Show that a backward
recurrence is still to be recommended.
(b) Work out on a computer the numerical details of Example 3.3.16, and compare
with the Handbook [1, Example 19.28.1]. (Some deviations are to be expected,
since Miller used other rounding rules.) Try to detect the oscillating component by
computing the difference scheme of the computed U(a, 5), and estimate roughly the
error of the computed values.
3.3.25 (a) For which constant real a does the difference equation
yn+1 −2ayn + yn−1 = 0
satisfy the root condition? For which values of the real constant a does there exist
a solution such that limn→∞yn = 0? For these values of a, how do you construct
a solution yn = y∗
n by a recurrence and normalization so that this condition as well
as the condition y∗
0 + 2 ∞
m=1 y∗
2m = 1 are satisﬁed? Is y∗
n unique? Give also an
explicit expression for y∗
n.
93This formula is well known in the theory of the conﬂuent hypergeometric functions, where it is usually proved
in other ways.

270
Chapter 3. Series, Operators, and Continued Fractions
(b) For the other real values of a, show that y∗
n does not exist, but that for any
given y0, y1 a solution can be accurately constructed by forward recurrence. Give
an explicit expression for this solution in terms of Chebyshev polynomials (of the
ﬁrst and the second kind). Is it true that backward recurrence is also stable, though
more complicated than forward recurrence?
3.3.26 (a) The Bessel function Jk(z) satisﬁes the difference equation
Jk+1(z) −(2k/z)Jk(z) + Jk−1(z) = 0,
k = 1, 2, 3, . . . ,
and the identities
J0(z) + 2J2(z) + 2J4(z) + 2J6(z) + · · · = 1,
J0(z) −2J2(z) + 2J4(z) −2J6(z) + · · · = cos z;
see the Handbook [1, Sec. 9.1.27, 9.1.46, and 9.1.47]. Show how one of the identities
can be used for normalizing the trial sequence obtained by a backward recurrence.
Under what condition does Lemma 3.3.15 give the hint to use the backward recur-
rence for this difference equation?
(b) Study the section on Bessel functions of integer order in [294]. Apply this
technique for z = 10, 1, 0.1 (say). The asymptotic formula (see [1, Sec. 9.3.1])
Jk(z) ∼
1
√
2πk
 ez
2k
k
,
k ≫1, z ﬁxed,
may be useful in deciding where to start the backward recurrence. Use at least two
starting points, and subtract the results (after normalization).
Comment: The above difference equation for Jk(z) is also satisﬁed by a function
denoted Yk(z):
Yk(z) ∼
−2
√
2πk
 ez
2k
−k
,
(k ≫1).
How do these two solutions interfere with each other when forward or backward
recurrence is used?
3.3.27 A counterexample to the technique with frozen coefﬁcients. Consider the difference
equation yn+1 −(−1)nyn + yn−1 = 0. The technique with frozen coefﬁcients leads
to the consideration of the difference equations
zn+1 −2azn + zn−1 = 0,
a ∈[−0.5, 0.5];
all of them have only bounded solutions. Find by numerical experiment that, never-
theless, there seems to exist unbounded solutions yn of the ﬁrst difference equation.
Comment: A proof of this is found by noting that the mapping (y2n, y2n+1) 9→
(y2n+2, y2n+3) is represented by a matrix that is independent of n and has an eigen-
value that is less than −1.
3.3.28 Let {bn}∞
−∞be a given sequence, and consider the difference equation
yn−1 + 4yn + yn+1 = bn,
which can also be written in the form (6 + δ2)yn = bn.

3.4. Acceleration of Convergence
271
(a) Show that the difference equation has at most one solution that is bounded for
−∞< n < +∞. Find a particular solution in the form of an expansion into powers
of the operator δ2/6. (This is, hopefully, bounded.)
(b) Apply it numerically to the sequence bn = (1 + n2h2)−1 for a few values of the
step size h, e.g., h = 0.1, 0.2, 0.5, 1. Study for n = 0 the rate of decrease (?) of the
terms in the expansion. Terminate when you estimate that the error is (say) 10−6.
Check how well the difference equation is satisﬁed by the result.
(c) Study theoretically bounds for the terms when bn = exp(iωhn), ω ∈R. Does
the expansion converge? Compare your conclusions with numerical experiments.
Extend to the case when bn = B(nh), where B(t) can be represented by an absolutely
convergent Fourier integral,
B(t) =
 ∞
−∞
eiωtβ(ω)dω.
Note that B(t) = (1 + t2)−1 if β(ω) = 1
2e−|ω|. Compare the theoretical results with
the experimental results in (b).
(d) Put Q = δ2/6. Show that ˜yn ≡(1 −Q + Q2 + · · · ± Qk−1)bn/6 satisﬁes the
difference equation (1 + Q)( ˜yn −yn) = Qkbn/6.
Comment: This procedure is worthwhile if the sequence bn is so smooth that (say)
two or three terms give satisfactory accuracy.
3.4
Acceleration of Convergence
3.4.1
Introduction
We have seen that in applied mathematics the solution to many problems can be obtained
from a series expansion or a sequence converging to the exact solution. But sometimes the
convergence of the series is so slow that the effective use of it is limited.
If a sequence {sn}∞
0
converges slowly toward a limit s, but has a sort of regular
behavior when n is large, it can under certain conditions be transformed into another inﬁnite
sequence {s′
n}, which converges much faster to the same limit. Here s′
n usually depends on
the ﬁrst n elements of the original sequence only. This is called convergence acceleration.
Such a sequence transformation may be iterated to yield a sequence of inﬁnite sequences,
{s′′
n}, {s′′′
n }, and so forth, hopefully with improved convergence toward the same limit s.
For an inﬁnite series convergence acceleration means the convergence acceleration of its
sequence of partial sums, because
lim
n→∞sn = a
⇐⇒a = sj +
∞

p=1
(sp+j −sp+j−1).
Some algorithms are most easily discussed in terms of sequences, others in terms of series.
Several transformations, linear as well as nonlinear, have been suggested and are suc-
cessful under various conditions. Some of them, such as Aitken transformation, repeated
averages, and Euler’s transformation, are most successful on oscillating sequences (alternat-
ing series or series in a complex variable). Others, such as variants of Aitken acceleration,

272
Chapter 3. Series, Operators, and Continued Fractions
Euler–Maclaurin, and Richardson, work primarily on monotonic sequences (series with
positive terms). Some techniques for convergence acceleration such as continued fractions,
Padé approximation, and the ϵ algorithm transform a power series into a sequence of rational
functions.
Some of these techniques may even sometimes be successfully applied to semi-
convergent sequences. Several of them can also use a limited number of coefﬁcients of
a power series for the computation of values of an analytic continuation of a function,
outside the circle of convergence of the series that deﬁned it.
Convergence acceleration cannot be applied to “arbitrary sequences”; some sort of
conditions are necessary that restrict the variation of the future elements of the sequence,
i.e., the elements which are not computed numerically. In this section, these conditions are
of a rather general type, in terms of monotonicity, analyticity, or asymptotic behavior of
simple and usual types.
In addition to the “general purpose” techniques to be discussed in this chapter, there are
other techniques of convergence acceleration based on the use of more speciﬁc knowledge
about a problem. For example, the Poisson summation formula
∞

n=−∞
f (n) =
∞

j=−∞
ˆf (j),
ˆf (ω) =
 ∞
−∞
f (ω)e−2πiωx dx
(3.4.1)
( ˆf is the Fourier transform of f ) can be amazingly successful for a certain class of series
 a(n), namely if a(x) has a rapidly decreasing Fourier transform. The Poisson formula
is also an invaluable tool for the design and analysis of numerical methods for several
problems; see Theorem 3.4.10.
Irregular errors are very disturbing when these techniques are used. They sometimes
set the limit for the reachable accuracy. For the sake of simplicity we therefore use IEEE
double precision arithmetic in most examples.
3.4.2
Comparison Series and Aitken Acceleration
Suppose that the terms in the series ∞
j=1 aj behave, for large j, like the terms of a series
∞
j=1 bj, i.e., limj→∞aj/bj = 1. Then, if the sum s = ∞
j=1 bj is known one can write
S =
∞

j=1
aj = s +
∞

j=1
(aj −bj),
where the series on the right-hand side converges more quickly than the given series. We
call this making use of a simple comparison problem. The same idea is used in many other
contexts—for example, in the computation of integrals where the integrand has a singularity.
Usual comparison series are
∞

j=1
n−2 = π2/6,
∞

j=1
n−4 = π4/90, etc.
A general expression for ∞
j=1 n−2r is given in (3.4.32). No simple closed form is known
for ∞
j=1 n−3.

3.4. Acceleration of Convergence
273
Example 3.4.1.
The term aj = (j 4 + 1)−1/2 behaves, for large j, like bj = j −2, whose sum is π2/6.
Thus
∞

j=1
aj = π2/6 +
∞

j=1

(j 4 + 1)−1/2 −j −2
= 1.64493 −0.30119 = 1.3437.
Five terms on the right-hand side are sufﬁcient for four-place accuracy in the ﬁnal result.
Using the series on the left-hand side, one would not get four-place accuracy until after
20,000 terms.
This technique is unusually successful in this example. The reader is advised to ﬁnd
out why, and why it is less successful for aj = (j 4 + j 3 + 1)−1/2.
An important comparison sequence is a geometric sequence
yn = s + bkn
for which ∇yn = yn −yn−1 = bkn−1(k −1). If this is ﬁtted to the three most recently
computed terms of a given sequence, yn = sn for (say) n = j, j −1, j −2, then ∇yj = ∇sj,
∇yj−1 = ∇sj−1, and
k = ∇sj/∇sj−1,
∇sj = bkj−1(k −1).
Hence
bkj =
∇sj
1 −1/k =
∇sj
1 −∇sj−1/∇sj
= (∇sj)2
∇2sj
.
This yields a comparison sequence for each j. Suppose that |k| < 1. Then the comparison
sequence has the limit limn→∞yn = s = yj −bkj, i.e.,
s ≈s′
j = sj −(∇sj)2
∇2sj
.
(3.4.2)
This nonlinear acceleration method is called Aitken acceleration.94
Notice that the denominator equals sj −2sj−1 +sj−2, but to minimize rounding errors
it should be computed as
∇sj −∇sj−1 = (sj −sj−1) −(sj−1 + sj−2)
(cf. Lemma 2.3.2). If {sn} is exactly a geometric sequence, i.e., if sn −s = k(sn−1 −s)
for all n, then s′
j = s for all j. Otherwise it can be shown (Henrici [193]) that under the
assumptions
lim
j→∞sj = s,
lim sj+1 −sj
sj −sj−1
= k∗,
|k∗| < 1,
(3.4.3)
the sequence {s′
j} converges faster than the sequence {sj}. The above assumptions can often
be veriﬁed for sequences arising from iterative processes and for many other applications.
Note also that Aitken extrapolation is exact for sequences {sn} such that
α(sn −s) + β(sn+1 −s) = 0 ∀n,
with αβ ̸= 0, α + β ̸= 0. This leads to a generalization to be discussed in Sec. 3.5.4.
94Named after Alexander Craig Aitken (1895–1967), a Scottish mathematician born in New Zealand.

274
Chapter 3. Series, Operators, and Continued Fractions
If you want the sum of slowly convergent series, then it may seem strange to compute
the sequence of partial sums, and compute the ﬁrst and second differences of rounded values
of this sequence in order to apply Aitken acceleration. The a-version of Aitken acceleration
works on the terms aj of an inﬁnite series instead of on its partial sums sj.
Clearly we have aj = ∇sj, j = 1 : N. The a-version of Aitken acceleration thus
reads
s′
j = sj −a2
j/∇aj,
j = 1 : N.
(3.4.4)
We want to determine a′
j so that
j

k=1
a′
k = s′
j,
j = 1 : N.
Then
a′
1 = 0,
a′
j = aj −∇(a2
j/∇aj),
j = 2 : N,
ands′
N = sN−a2
N/∇aN (showthis). Wemayexpectthatthisa-versionofAitkenacceleration
handles rounding errors better.
The condition |k∗| < 1 is a sufﬁcient condition only. In practice, Aitken acceleration
seems most efﬁcient if k∗= −1. Indeed, it often converges even if k∗< −1; see Prob-
lem 3.4.7. It is much less successful if k∗≈1, for example, for slowly convergent series
with positive terms.
TheAitken acceleration process can often be iterated to yield sequences {s′′
n}∞
0 , {s′′′
n }∞
0 ,
etc., deﬁned by the formulas
s′′
j = s′
j −
(∇s′
j)2
∇2s′
j
,
s
′′′
j = s′′
j −
(∇s′′
j )2
∇2s′′
j
. . . .
(3.4.5)
j
sj
ej
e′
j
e′′
j
e′′′
j
6
0.820935
3.5536e−2
7
0.754268
−3.1130e−2
−1.7783e−4
8
0.813092
2.7693e−2
1.1979e−4
9
0.760460
−2.4938e−2
−8.4457e−5
−1.3332e−6
10
0.808079
2.2681e−2
6.1741e−5
7.5041e−7
11
0.764601
−2.0797e−2
−4.6484e−5
−4.4772e−7
−1.0289e−8
Example 3.4.2.
By (3.1.13), it follows that for x = 1
1 −1/3 + 1/5 −1/7 + 1/9 −· · · = arctan 1 = π/4 ≈0.7853981634.
This series converges very slowly. Even after 500 terms there still occur changes in the third
decimal. Consider the partial sums sj = j
n0(−1)j(2n + 1)−1, with n0 = 5, and compute
the iterated Aitken sequences as indicated above.

3.4. Acceleration of Convergence
275
The (sufﬁcient) theoretical condition mentioned above is not satisﬁed, since here
∇sn/∇sn−1 →−1 as n →∞. Nevertheless, we shall see that the Aitken acceleration
works well, and that the iterated accelerations converge rapidly. One gains two digits for
every pair of terms, in spite of the slow convergence of the original series. The results in
the table above were obtained using IEEE double precision arithmetic. The errors of s′
j,
s′′
j , . . . , are denoted by e′
j, e′′
j, . . . .
Example 3.4.3.
Set an = e−
√
n+1, n ≥0. As before, we denote by sn the partial sums of  an,
s = lim sn = 1.67040681796634, and use the same notations as above. Note that
∇sn
∇sn−1
=
an
an−1
≈1 −1
2n−1/2,
(n ≫1),
so this series is slowly convergent. Computations with plain and iterated Aitken in IEEE
double precision arithmetic gave the results below.
j
e2j
e(j)
2j
1
−0.882
−4.10e−1
2
−0.640
−1.08e−1
3
−0.483
−3.32e−2
2
−0.374
−4.41e−3
5
−0.295
−7.97e−4
6
−0.237
−1.29e−4
7
−0.192
−1.06e−5
The sequence {e(j)
2j } is monotonic until j = 8. After this |e(j)
2j | is mildly ﬂuctuating
around 10−5 (at least until j = 24), and the differences ∇s(j)
2j
= ∇e(j)
2j are sometimes
several powers of 10 smaller than the actual errors and are misleading as error estimates.
The rounding errors have taken over, and it is almost no use to compute more terms.
It is possible to use more terms for obtaining higher accuracy by applying iterated
Aitken acceleration to a thinned sequence, for example, s4, s8, s12, . . . ; cf. Problem 3.4.4.
Note the thinning is performed on a sequence that converges to the limit to be computed, for
example, the partial sums of a series. Only in so-called bell sums (see Problem 3.4.29) shall
we do a completely different kind of thinning, namely a thinning of the terms of a series.
The convergence ratio of the thinned sequence are much smaller; for the series of the
previous example they become approximately

1 −1
2n−1/2
4
≈1 −2n−1/2,
n ≫1.
The most important point though, is that the rounding errors become more slowly ampliﬁed,
so that terms far beyond the eighth one of the unthinned sequence can be used in the
acceleration, resulting in a much improved ﬁnal accuracy.

276
Chapter 3. Series, Operators, and Continued Fractions
How to realize the thinning depends on the sequence; a different thinning will be used
in the next example.
Example 3.4.4.
We shall compute, using IEEE double precision arithmetic,
s =
∞

n=1
n−3/2 = 2.612375348685488.
If all partial sums are used inAitken acceleration, it turns out that the error |e(j)
2j | is decreasing
until j = 5, when it is 0.07, and it remains on approximately this level for a long time.
j
0
1
2
3
4
5
E2j+1
−1.61
−0.94
−4.92e−1
−2.49e−1
−1.25e−1
−6.25e−2
E(j)
2j+1
−1.61
−1.85
−5.06e−2
−2.37e−4
−2.25e−7
2.25e−10
A much better result is obtained by means of thinning, but since the convergence is
much slower here than in the previous case, we shall try “geometric” thinning rather than
the “arithmetic” thinning used above; i.e., we now set Sm = s2m. Then
∇Sm =
2m

1+2m−1
an,
Sj = S0 +
j

m=1
∇Sm,
Ej = Sj −s.
(If maximal accuracy is wanted, it may be advisable to use the divide and conquer technique
for computing these sums (see Problem 2.3.5), but it has not been used here.) By the
approximation of the sums by integrals one can show that ∇Sm/∇Sm−1 ≈2−1/2, m ≫1.
The table above shows the errors of the ﬁrst thinned sequence and the results after iterated
Aitken acceleration. The last result has used 1024 terms of the original series, but since
sn −s = −
∞

j=n
j −3/2 ≈−
 ∞
n
t−3/2 dt = −2
3n−1/2,
(3.4.6)
1020 terms would have been needed for obtaining this accuracy without convergence accel-
eration.
For sequences such that
sn −s = c0n−p + c1n−p−1 + O(n−p−2),
p > 0,
wheres, c0, c1 areunknown, thefollowingvariantofAitkenacceleration(Bjørstad, Dahlquist,
and Grosse [33]) is more successful:
s′
n = sn −p + 1
p
4sn∇sn
4sn −∇sn
.
(3.4.7)

3.4. Acceleration of Convergence
277
It turns out that s′
n is two powers of n more accurate than sn, s′
n −s = O(n−p−2); see
Problem 3.4.12. More generally, suppose that there exists a longer (unknown) asymptotic
expansion of the form
sn = s + n−p(c0 + c1n−1 + c2n−2 + · · ·),
n →∞.
(3.4.8)
This is a rather common case. Then we can extend this to an iterative variant, where p is
to be increased by two in each iteration; i = 0, 1, 2, . . . is a superscript, i.e.,
si+1
n
= si
n −p + 2i + 1
p + 2i
4si
n∇si
n
4sin −∇sin
.
(3.4.9)
If p is also unknown, it can be estimated by means of the equation
1
p + 1 = −4
4sn
4sn −∇sn
+ O(n−2).
(3.4.10)
Example 3.4.5.
We consider the same series as in the previous example, i.e., s =  n−3/2. We use
(3.4.9) without thinning. Here p = −1/2; see Problem 3.4.13. As usual, the errors are
denoted ej = sj −s, ei
2j = si
2j −s. In the right column of the table below, we show the
errors from a computation with 12 terms of the original series.
j
e2j
ej
2j
0
−1.612
−1.612
1
−1.066
−8.217e−3
2
−0.852
−4.617e−5
3
−0.730
+2.528e−7
4
−0.649
−1.122e−9
5
−0.590
−0.634e−11
From this point the errors were around 10−10 or a little below. The rounding errors
have taken over, and the differences are misleading for error estimation. If needed, higher
accuracy can be obtained by arithmetic thinning with more terms.
In this computation only 12 terms were used. In the previous example a less accurate
result was obtained by means of 1024 terms of the same series, but we must appreciate that
the technique of Example 3.4.4 did not require the existence of an asymptotic expansion for
sn and may therefore have a wider range of application.
There are not yet so many theoretical results that do justice to the practically observed
efﬁciency of iterated Aitken accelerations for oscillating sequences. One reason for this
can be that the transformation (3.4.2) which the algorithm is based on is nonlinear. For

278
Chapter 3. Series, Operators, and Continued Fractions
methods of convergence acceleration that are based on linear transformations, theoretical
estimates of rates of convergence and errors are closer to the practical performance of the
methods.
3.4.3
Euler’s Transformation
In 1755 Euler gave the ﬁrst version of what is now called Euler’s transformation. Euler
showed that for an alternating series (uj ≥0), it holds that
S =
∞

j=0
(−1)juj =
∞

k=0
1
2k 4kuk.
(3.4.11)
Often it is better to apply Euler’s transformation to the tail of a series.
We shall now apply another method of acceleration based on repeated averaging of
the partial sums. Consider again the same series as in Example 3.4.2, i.e.,
∞

j=0
(−1)j(2j + 1)−1 = 1 −1
3 + 1
5 −1
7 + 1
9 −· · · = π
4 .
(3.4.12)
Let SN be the sum of the ﬁrst N terms. The columns to the right of the SN-column in the
scheme given in Table 3.4.1 are formed by building averages.
Each number in a column is the mean of the two numbers which stand to the left
and upper left of the number itself. In other words, each number is the mean of its “west”
and “northwest” neighbor. The row index of M equals the number of terms used from the
original series, while the column index minus one is the number of repeated averaging.
Only the digits which are different from those in the previous column are written out.
Table 3.4.1. Summation by repeated averaging.
N
SN
M2
M3
M4
M5
M6
M7
6
0.744012
7
0.820935
782474
8
0.754268
787602
5038
9
0.813092
783680
5641
340
10
0.760460
786776
5228
434
387
11
0.808079
784270
5523
376
405
396
12
0.764601
786340
5305
414
395
400
398
Notice that the values in each column oscillate. In general, for an alternating series, it
follows from the next theorem together with (3.3.4) that if the absolute value of the jth term,
considered as a function of j, has a kth derivative which approaches zero monotonically
for j > N0, then every other value in column Mk+1 is larger than the sum, and every other

3.4. Acceleration of Convergence
279
is smaller. This premise is satisﬁed here, since if f (j) = (2j + 1)−1, then f (k)(j) =
ck(2j + 1)−1−k, which approaches zero monotonically.
If roundoff is ignored, it follows from column M6 that 0.785396 ≤π/4 ≤0.785400.
To take account of roundoff error, we set π/4 = 0.785398 ± 3 · 10−6. The actual error
is only 1.6 · 10−7. In Example 3.4.2 iterated Aitken accelerations gave about one decimal
digit more with the same data. It is evident how the above method can be applied to any
alternating series. The diagonal elements are equivalent to the results from using Euler’s
transformation.
Euler’s transformation and the averaging method can be generalized for the conver-
gence acceleration of a general complex power series
S(z) =
∞

j=1
ujzj−1.
(3.4.13)
For z = −1 an alternating series is obtained. Other applications include Fourier series.
They can be brought to this form with z = eiφ, −π ≤φ ≤π; see Sec. 4.6.2 and Prob-
lem 4.6.7.
The irregular errors of the coefﬁcients play a big role if |φ| ≪π, and it is impor-
tant to reduce their effects by means of a variant of the thinning technique described (for
Aitken acceleration) in the previous section. Another interesting application is the analytic
continuation of the power series outside its circle of convergence; see Example 3.4.7.
Theorem 3.4.1.
The tail of the power series in (3.4.13) can formally be transformed into the following
expansion, where (z ̸= 1):
S(z) −
n

j=1
ujzj−1 =
∞

j=n+1
ujzj−1 =
zn
1 −z
∞

s=0
P sun+1,
P =
z
1 −z4.
(3.4.14)
Set N = n + k −1, and set
Mn,1 =
n

j=1
ujzj−1,
MN,k = Mn,1 +
zn
1 −z
k−2

s=0
P sun+1,
n = N −k + 1.
(3.4.15)
These quantities can be computed by the following recurrence formula that yields several
estimates based on N terms from the original series.95 This is called the generalized Euler
transformation:
MN,k = MN,k−1 −zMN−1,k−1
1 −z
,
k = 2 : N.
(3.4.16)
For z = −1, this is the repeated average algorithm described above, and P = −1
24.
95See Algorithm 3.4 for an adaptive choice of a kind of optimal output.

280
Chapter 3. Series, Operators, and Continued Fractions
Assume that |z| ≤1, that  ujzj−1 converges, and that 4suN →0, s = 0 : k, as
N →∞. Then MN,k →S(z) as N →∞. If, moreover, 4k−1uj has a constant sign for
j ≥N −k + 2, then the following strict error bounds are obtained:
|MN,k −S(z)| ≤|z(MN,k −MN−1,k−1)| = |MN,k −MN,k−1|, (k ≥2).
(3.4.17)
Proof. We ﬁrst note that as N →∞, P suN →0, s = 0 : k, and hence, by (3.4.15),
lim MN,k = lim MN,0 = S(z).
Euler’s transformation can be formally derived by operators as follows:
S(z) −Mn,1 = zn
∞

i=0
(zE)iun+1 =
zn
1 −zE un+1
=
zn
1 −z −z4un+1 =
zn
1 −z
∞

s=0
P sun+1.
In order to derive (3.4.16), note that this relation can be written equivalently be written as
MN,k −MN,k−1 = z(MN,k −MN−1,k−1),
(3.4.18)
MN,k−1 −MN−1,k−1 = (1 −z)(MN,k −MN−1,k−1).
(3.4.19)
Remembering that n = N −k + 1, we obtain, by (3.4.15),
MN,k −MN−1,k−1 = zN−k+1
1 −z P k−2uN−k+2,
(3.4.20)
and it can be shown (Problem 3.4.16) that
MN,k−1 −MN−1,k−1 = znP k−2un+1 = zN−k+1P k−2uN−k+2.
(3.4.21)
By (3.4.20) and (3.4.21), we now obtain (3.4.19) and hence also the equivalent equations
(3.4.18) and (3.4.16).
Now substitute j for N into (3.4.21), and add the p equations obtained for j = N +1,
. . . , N + p. We obtain
MN+p,k−1 −MN,k−1 =
N+p

j=N+1
zj−k+1P k−2uj−k+2.
Then substitute k + 1 for k, and N + 1 + i for j. Let p →∞, while k is ﬁxed. It follows
that
S(z) −MN,k =
∞

j=N+1
zj−kP k−1uj−k+1
= zN−k+1 · zk−1
(1 −z)k−1
∞

i=0
zi4k−1uN−k+2+i;
(3.4.22)

3.4. Acceleration of Convergence
281
hence
|S(z) −MN,k| ≤
(((z/(1 −z))k−1zN−k+1((
∞

i=0
((4k−1uN−k+2+i
(( .
We now use the assumption that 4k−1uj has constant sign for j ≥N −k + 2.
Since ∞
i=0 4k−1uN−k+2+i = −4k−2uN−k+2, it follows that
|S(z) −MN,k| ≤
((((zN−k+1 zk−14k−2uN−k+2
(1 −z)k−1
((((
=
((((
z · zN−k+1
1 −z
P k−2uN−k+2
(((( .
Now, by (3.4.20),
|S(z) −MN,k| ≤|z| · |MN,k −MN−1,k−1|.
This is the ﬁrst part of (3.4.17). The second part then follows from (3.4.18).
Remark 3.4.1. Note that the elements MN,k become rational functions of z for ﬁxed N,
k. If the term un, as a function of n, belongs to Pk, then the classical Euler transformation
(for n = 0) yields the exact value of S(z) after k terms if |z| < 1. This follows from
(3.4.14), because  ujzj is convergent, and P sun+1 = 0 for s ≥k. In this particular case,
S(z) = Q(z)(1 −z)−k, where Q is a polynomial; in fact, the Euler transformation gives
S(z) correctly for all z ̸= 1.
The advantage of using the recurrence formula (3.4.16) instead of a more direct use
of (3.4.14) is that it provides a whole lower triangular matrix of estimates so that one can,
by means of a simple test, decide when to stop. This yields a result with strict error bound,
if 4k−1uj has a constant sign (for all j with a given k), and if the effect of rounding errors
is evidently smaller than Tol. If these conditions are not satisﬁed, there is a small risk that
the algorithm may terminate if the error estimate is accidentally small, for example, near a
sign change of 4k−1uj.
The irregular errors of the initial data are propagated to the results. In the long run, they
are multiplied by approximately |z/(1−z)| from a column to the next—this is less than one
if ℜz < 1/2—but in the beginning this growth factor can be as large as (1 + |z|)/|1 −z|. It
plays no role for alternating series; its importance when |1−z| is smaller will be commented
on in Sec. 4.7.2.
The following algorithm is mainly based onTheorem 3.4.1 with a termination criterion
based on (3.4.17). The possibility of the irregular errors becoming dominant has been taken
into account (somewhat) in the third alternative of the termination criterion.
The classical Euler transformation would only consider the diagonal elements MNN,
N = 1, 2, . . . , and the termination would have been based on |MNN −MN−1,N−1|. The
strategy used in this algorithm is superior for an important class of series.

282
Chapter 3. Series, Operators, and Continued Fractions
Algorithm 3.4. Generalized Euler Transformation.
function [sum,errest,N,kk] = euler(z,u,Tol);
% EULER applies the generalized Euler transform to a power
% series with terms u(j)zˆj. The elements of M are inspected
% in a certain order, until a pair of neighboring elements
% are found that satisfies a termination criterion.
%
Nmax = length(u);
errest = Inf; olderrest = errest;
N = 1;
kk = 2; M(1,1) = u(1);
while (errest > Tol) & (N < Nmax) & (errest <= olderrest)
N = N+1;
M(N,1) = M(N-1,1)+ u(N)*zˆ(N-1); % New partial sum
for k = 2:N,
M(N,k) = (M(N,k-1) - z*M(N-1,k-1))/(1-z);
temp = abs(M(N,k) - M(N,k-1))/2;
if temp < errest,
kk = k; errest = temp;
end
end
end
sum = (M(N,kk) + M(N,kk-1))/2;
An oscillatory behavior of the values |MN,k −MN,k−1| in the same row indicates that
the irregular errors have become dominant. The smallest error estimates may then become
unreliable.
Remark 3.4.2. If the purpose of the computation is to study the convergence properties of
the method rather than to get a numerical result of desired accuracy as quickly as possible,
you had better replace the while statement by (say) for N=1:Nmax, change a few lines in
the program, and produce graphical output such as Figure 3.4.1.
The above algorithm gives a strict error bound if, in the notation used in the theorem,
4k−1ui has a constant sign for i ≥N −k + 2 (in addition to the other conditions of the
theorem). We recall that a sequence for which this condition is satisﬁed for every k is called
completely monotonic; see Deﬁnition 3.4.2.
It may seem difﬁcult to check if this condition is satisﬁed. It turns out that many
sequences that can be formed from sequences such as {n−α}, {e−αn} by simple operations and
combinations belong to this class. The generalized Euler transformation yields a sequence
that converges at least as fast as a geometric series. The convergence ratio depends on z; it
is less than one in absolute value for any complex z, except for z > 1 on the real axis. Thus,
the generalized Euler transformation often provides an analytic continuation of a power
series outside its circle of convergence.
For alternating series, with completely monotonic terms, i.e., for z = −1, the con-
vergence ratio typically becomes 1
3. This is in good agreement with Figure 3.4.1. Note that
the minimum points for the errors lie almost on a straight line and that the optimal value of
k
N is approximately 2
3, if N ≫1 and if there are no irregular errors.

3.4. Acceleration of Convergence
283
0
10
20
30
−15
−10
−5
0
j
log10  |error|
i=26
24
22
20
18
16
14
12
10
 8
 6
 4
i=2
0
10
20
30
−14
−12
−10
−8
−6
−4
−2
0
j
log10  |errest|
i=26
24
22
20
18
16
14
12
10
 8
 6
i=4
Figure 3.4.1. Logarithms of the actual errors and the error estimates for MN,k in a
more extensive computation for the alternating series in (3.4.12) with completely monotonic
terms. The tolerance is here set above the level where the irregular errors become important;
for a smaller tolerance parts of the lowest curves may become less smooth in some parts.
Example 3.4.6.
A program, essentially the same as Algorithm 3.4, is applied to the series
∞

j=1
(−1)jj −1 = 1 −1
2 + 1
3 −1
4 + 1
5 −· · · = ln 2 = 0.69314 71805 599453
with tol = 10−6, It stops when N = 12, kk = 9. The errors ek = MN,k −ln 2 and the
differences 1
2∇kMN,k along the last row of M read as shown in the following table.
k
1
2
3
. . .
10
11
12
ek
−3.99·10−2 1.73·10−3 −1.64·10−4
… 5.35·10−7 −9.44·10−7 2.75·10−6
∇/2
2.03·10−2 −9.47·10−4
… 4.93·10−7 −7.40·10−7 1.85·10−6
Note that |errest| = 4.93 · 10−7 and sum −ln 2 = 1
2(e9 + e8) = 4.2 · 10−8. Almost
full accuracy is obtained for Tol = 10−16, Nmax = 40. The results are N = 32, kk = 22,
errest = 10−16, |error| = 2 · 10−16. Note that errest < |error|; this can happen when
we ask for such a high accuracy that the rounding errors are not negligible.
Example 3.4.7.
We consider the application to a divergent power series (analytic continuation),
S(z) =
∞

n=1
unzn−1,
|z| > 1.
As in the previous example we study in detail the case of un = 1/n. It was mentioned above
that in exact arithmetic the generalized Euler transformation converges in the z-plane, cut
along the interval [1, ∞]. The limit is −z−1 ln(1−z), a single-valued function in this region.

284
Chapter 3. Series, Operators, and Continued Fractions
For various z outside the unit circle, we shall see that rounding causes bigger problems here
than for Fourier series. The error estimate of Algorithm 3.4, usually underestimated the
error, sometimes by a factor of ten. The table below reports some results from experiments
without thinning.
z
−2
−4
−10
−100
2i
8i
1 + i
2 + i
|error|
2·10−12 2·10−8 4·10−5 3·10−3 8·10−11 10−3
10−7
2·10−2
N
38
41
43
50
40
39
38
39
kk
32
34
39
50
28
34
22
24
Thinning can be applied in this application, but here not only the argument φ is
increased (this is good), but also |z| (this is bad). Nevertheless, for z = 1 + i, the error
becomes 10−7, 3 ·10−9, 10−9, 4· 10−8, for τ = 1, 2, 3, 4, respectively. For z = 2 + i,
however, thinning improved the error only from 0.02 to 0.01. All this is for IEEE double
precision arithmetic.
3.4.4
Complete Monotonicity and Related Concepts
For the class of completely monotonic sequences and some related classes of analytic func-
tions the techniques of convergence acceleration can be put on a relatively solid theoretical
basis.
Deﬁnition 3.4.2.
A sequence {un} is completely monotonic (c.m.) for n ≥a if and only if
un ≥0,
(−4)jun ≥0
∀j ≥0, n ≥a (integers).
Such sequences are also called totally monotonic. The abbreviation c.m. will be used,
both as an adjective and as a noun, and both in singular and in plural. The abbreviation
d.c.m. will similarly be used for the difference between two completely monotonic sequences.
(These abbreviations are not generally established.)
Ac.m. sequence {un}∞
0 is minimal if and only if it ceases to be a c.m. if u0 is decreased
while all the other elements are unchanged. This distinction is of little importance to us, since
we usually deal with a tail of some given c.m. sequence, and it can be shown that if {un}∞
0
is c.m., then {un}∞
1 is a minimal c.m. sequence. Note that, e.g., the sequence {1, 0, 0, 0, . . .}
is a nonminimal c.m., while {0, 0, 0, 0, . . .} is a minimal c.m. Unless it is stated otherwise
we shall only deal with minimal c.m. without stating this explicitly all the time.
Deﬁnition 3.4.3.
A function u(s) is c.m. for s ≥a, s ∈R, if and only if
u(s) ≥0,
(−1)(j)u(j)(s) ≥0,
s ≥a ∀j ≥0 (integer), ∀s ≥a (real).
u(s) is d.c.m. if it is a difference of two c.m. on the same interval.

3.4. Acceleration of Convergence
285
We also need variants with an open interval. For example, the function u(s) = 1/s is
c.m. in the interval [a, ∞) for any positive a, but it is not c.m. in the interval [0, ∞].
The simplest relation of c.m. functions and c.m. sequences reads as follows: if the
function u(s) is c.m. for s ≥s0, then the sequence deﬁned by un = u(s0 + hn), (h > 0),
n = 0, 1, 2, . . . , is also c.m. since, by (3.3.4), (−4)jun = (−hD)ju(ξ) ≥0 for some
ξ ≥s0.
A function is absolutely monotonic in an (open or closed) interval if the function and
all its derivatives are nonnegative there.
The main reason why the analysis of a numerical method is convenient for c.m. and
d.c.m. sequences is that they are “linear combinations of exponentials,” according to the
theorem below. The more precise meaning of this requires the important concept of a
Stieltjes integral.96
Deﬁnition 3.4.4.
The Stieltjes integral
 b
a f (x) dα(x) is deﬁned as the limit of sums of the form

i
f (ξi)

α(xi+1) −α(xi)

,
ξi ∈[xi, xi+1],
(3.4.23)
where
a = x0 < x1 < x2 < · · · < xN = b
is a partition of [a, b]. Here f (x) is bounded and continuous, and α(x) is of bounded vari-
ation in [a, b], i.e., the difference between two nondecreasing and nonnegative functions.
The extension to improper integrals where, for example, b = ∞, α(b) = ∞, is
made in a similar way as for Riemann or Lebesgue integrals. The Stieltjes integral is much
used also in probability and mechanics, since it uniﬁes the treatment of continuous and
discrete (and mixed) distributions of probability or mass. If α(x) is piecewise differentiable,
then dα(x) = α′(x) dx, and the Stieltjes integral is simply
 b
a f (x)α′(x) dx. If α(x) is
a step function, with jumps (also called point masses) mi at x = xi, i = 1 : n, then
dα(xi) = limϵ↓0 α(xi + ϵ) −α(xi −ϵ) = mi,
 b
a
f (x) dα(x) =
n

i=1
mif (xi).
(It has been assumed that f (x) is continuous at xi, i = 1 : n.
Integration by parts is as usual; the following example is of interest to us. Suppose
that α(0) = 0, α(x) = o(ecx) as x →∞, and that ℜs ≥c. Then
 ∞
0
e−sxdα(x) = s
 ∞
0
α(x)e−sx dx.
(3.4.24)
96Thomas Jan Stieltjes (1856–1894) was born in the Netherlands. After working with astronomical calculations
at the observatory in Leiden, he accepted a position in differential and integral calculus at the University of
Toulouse, France. He did important work on continued fractions and the moment problem, and invented a new
concept of the integral.

286
Chapter 3. Series, Operators, and Continued Fractions
The integral on the left side is called a Laplace–Stieltjes transform, while the integral on
the right side is an ordinary Laplace transform. Many properties of power series, though
not all, can be generalized to Laplace–Stieltjes integrals—set z = e−s. Instead of a disk
of convergence, the Laplace–Stieltjes integral has a (right) half-plane of convergence. A
difference is that the half-plane of absolute convergence may be different from the half-plane
of convergence.
We shall be rather brief and concentrate on the applicability to the study of numerical
methods. We refer to Widder [373, 374] for proofs and more precise information concerning
Stieltjes integrals, Laplace transforms, and complete monotonicity. Dahlquist [87] gives
more details about applications to numerical methods.
The sequence deﬁned by
un =
 1
0
tn dβ(t),
n = 0, 1, 2, . . . ,
(3.4.25)
is called a moment sequence if β(t) is nondecreasing. We make the convention that t0 = 1
also for t = 0, since the continuity of f is required in the deﬁnition of the Stieltjes integral.
Consider the special example where β(0) = 0, β(t) = 1 if t > 0. This means a unit
point mass at t = 0, and no more mass for t > 0. Then u0 = 1, un = 0 for n > 0. It is
then conceivable that making a sequence minimal just means removing a point mass from
the origin; thus minimality means requiring that β(t) is continuous at t = 0. (For a proof,
see [373, Sec. 4.14].)
The following theorem combines parts of several theorems in the books by Widder. It
is important that the functions called α(x) and β(t) in this theorem need not to be explicitly
known for an individual series for applications of an error estimate or a convergence rate of
a method of convergence acceleration. Some criteria will be given below that can be used
for simple proofs that a particular series is (or is not) c.m. or d.c.m.
Theorem 3.4.5.
1. The sequence {un}∞
0 is c.m. if and only if it is a moment sequence; it is minimal if in
addition β(t) is continuous at t = 0, i.e., if there is no point mass at the origin. It is
a d.c.m. if and only if (3.4.25) holds for some β(t) of bounded variation.
2. The function u(s) is c.m. for s ≥0 if and only if it can be represented as a Laplace–
Stieltjes transform,
u(s) =
 ∞
0
e−sx dα(x),
s ≥0,
(3.4.26)
with a nondecreasing and bounded function α(x). For the open interval s > 0 we
have the same, except for the boundedness of α(x). For a d.c.m. the same is true
with α(x) of bounded variation (not necessarily bounded as x →∞). The integral
representation provides an analytic continuation of u(s) from a real interval to a
half-plane.
3. The sequence {un}∞
0 is a minimal c.m. if and only if there exists a c.m. function u(s)
such that un = u(n), n = 0, 1, 2, . . . .

3.4. Acceleration of Convergence
287
4. Suppose that u(s) is c.m. in the interval s > a. Then the Laplace–Stieltjes integral
converges absolutely and uniformly if ℜs ≥a′, for any a′ > a, and deﬁnes an analytic
continuation of u(s) that is bounded for ℜs ≥a′ and analytic for ℜs > a. This is
true also if u(s) is a d.c.m.
Proof. The “only if” parts of these statements are deep results mainly due to Hausdorff97
and Bernštein,98 and we omit the rather technical proofs. The relatively simple proofs of
the “if” parts of the ﬁrst three statements will be sketched, since they provide some useful
insight.
1. Assume that un is a moment sequence, β(0) = 0, β is continuous at t = 0 and non-
decreasing for t > 0. Note that multiplication by E or 4 outside the integral sign in
(3.4.25)correspondstomultiplicationbyt ort−1inside. Then, forj, n = 0, 1, 2, . . . ,
(−1)j4jun = (−1)j
 1
0
(t −1)jtn dβ(t) =
 1
0
(1 −t)jtn dβ(t) ≥0,
and hence un is c.m.
2. Assume that u(s) satisﬁes (3.4.26). It is rather easy to legitimate the differentiation
under the integral sign in this equation. Differentiation j times with respect to s
yields, for j = 1, 2, 3, . . . ,
(−1)ju(j)(s) = (−1)j
 ∞
0
(−x)je−sx dα(x) =
 ∞
0
xje−sx dα(x) ≥0;
and hence u(s) is c.m.
3. Assume that un = u(n) =
 ∞
0 e−nx dα(x). Deﬁne t = e−x, β(0) = 0, β(t) ≡
β(e−x) = u(0) −α(x), and note that
t = 1 ⇔x = 0,
t = 0 ⇔x = ∞,
and that u(0) = limx→∞α(x). It follows that β(t) is nonnegative and nondecreasing,
since x decreases as t increases. Note that β(t) ↓β(0) as t ↓0. Then
un = −
 0
1
tn dβ(t) =
 1
0
tn dβ(t),
hence {un} is a minimal c.m.
4. The distinction is illustrated for α′(x) = eax, u(s) = (s −a)−1, for a real a. u(s) is
analytic for ℜs > a and bounded only for ℜs ≥a′ for any a′ > a.
97Felix Hausdorff (1868–1942), a German mathematician, is mainly known for having created a modern theory
of topological and metric spaces.
98Sergei Natanoviˇc Bernštein (1880–1968), Russian mathematician. Like his countryman Chebyshev, he made
major contributions to polynomial approximation.

288
Chapter 3. Series, Operators, and Continued Fractions
The basic formula for the application of complete monotonicity to the summation of
power series reads
S(z) ≡
∞

i=0
uizi =
∞

0
 1
0
ziti dβ(t) =
 1
0
∞

0
ziti dβ(t) =
 1
0
(1 −zt)−1 dβ(t).
(3.4.27)
The inversion of the summation and integration is legitimate when |z| < 1. Note that the
last integral exists for more general z; a classical principle of complex analysis then yields
the following interesting result.
Lemma 3.4.6.
If the sequence {ui} is d.c.m., then the last integral of formula (3.4.27) provides the
unique single-valued analytic continuation of S(z) to the whole complex plane, save for a
cut along the real axis from 1 to ∞.
Remark 3.4.3. When z is located in the cut, (1 −zt)−1 has a nonintegrable singularity
at t = 1/z ∈[0, 1] unless, e.g., β(t) is constant in the neighborhood of this point. If we
remove the cut, S(z) will not be single-valued. Check that this makes sense for β(t) = t.
Next we shall apply the above results to ﬁnd interesting properties of the (generalized)
Euler transformation. For example, we shall see that, for any z outside the cut, there is an
optimal strategy for the generalized Euler transformation that provides the unique value of
the analytic continuation of S(z). The classical Euler transformation, however, reaches only
the half-plane ℜz < 1
2.
After that we shall see that there are a number of simple criteria for ﬁnding out
whether a given sequence is c.m., d.c.m., or neither. Many interesting sequences are c.m.,
for example, un = e−kn, un = (n+c)−k, (k ≥0, c ≥0), all products of these, and all linear
combinations (i.e., sums or integrals) of such sequences with positive coefﬁcients.
The convergence of a c.m. toward zero can be arbitrarily slow, but an alternating series
with c.m. terms will, after Euler’s transformation, converge as rapidly as a geometric series.
More precisely, the following result on the optimal use of a generalized Euler transformation
will be shown.
Theorem 3.4.7.
We use the notation of Theorem 3.4.1 and (3.4.22). Suppose that the sequence {uj}
is either c.m. or d.c.m. Consider
S(z) =
∞

j=0
ujzj,
z ∈C,
and its analytic continuation (according to the above lemma). Then for the classical Euler
transformation the following holds: If z = −1, a sequence along a descending diagonal of
the scheme M or (equivalently) the matrix ¯M, i.e., {Mn0,k}∞
k=0 for a ﬁxed n0, converges at
least as fast as 2−k. More generally, the error behaves like (z/(1−z))k, (k ≫1). Note that
|z/(1 −z)| < 1 if and only if ℜz < 1
2. The classical Euler transformation diverges outside
this half-plane. If z = e±it, π
3 < t ≤π, it converges as fast as (2 sin t
2)−k.

3.4. Acceleration of Convergence
289
For the generalized Euler transformation we have the following: If z = −1, the
smallest error in the ith row of ¯M is O(3−i), as i →∞. More generally, this error is
O((|z|/(1 + |1 −z|))i), hence the smallest error converges exponentially, unless z −1 is
real and positive; i.e., the optimal application of the generalized Euler’s transformation
provides the analytic continuation, whenever it exists according to Lemma 3.4.6. If N ≫1,
the optimal value99 of k/N is |1 −z|/(1 + |1 −z|). If z = e±it, 0 < t ≤π, the error is
O((1 + 2 sin t
2)−i).
Proof. Sketch: The result of the generalized Euler transformation is in Sec. 3.4.3, denoted
by Mn,k(z). The computation uses N = n + k terms (or partial sums) of the power series
for S(z); n terms of the original series—the head—are added, and Euler’s transformation
is applied to the next k terms—the tail. Set n/N = µ, i.e., n = µN, k = (1 −µ)N,
and denote the error of Mn,k by RN,µ(z). Euler’s transformation is based on the operator
P = P (z) =
z
1−z4. A multiplication by the operator P corresponds to a multiplication by
z
1−z(t −1) inside the integral sign.
First suppose that |z| < 1. By the deﬁnitions of S(z) and Mn,k(z) in Theorem 3.4.1,
RN,µ(z) ≡S −Mn,k =
zn
1 −z
∞

s=k
P sun =
zn
1 −z
 1
0
∞

s=k
z(t −1)
(1 −z)
s
tn dβ(t)
=
zn
1 −z
 1
0
z(t −1)
(1 −z)
k
tn dβ(t)
1 −z(t −1)/(1 −z)
(3.4.28)
= (−1)k
zN
(1 −z)k
 1
0
(1 −t)ktn dβ(t)
1 −zt .
We see that the error oscillates as stated in Sec. 3.4.3. Again, by analytic continuation, this
holds for all z except for the real interval [1, ∞]. Then
|RN,µ(z)|1/N ≤|z/(1 −z)1−µ| max
t∈[0,1]

(1 −t)1−µtµ
c1/N,
c =
 1
0
|dβ(t)|
|1 −zt|.
The ﬁrst part of the theorem has n = 0, hence µ = 0. We obtain
lim
N→∞|RN,0|1/N ≤|z/(1 −z)|
as stated. This is less than unity if |z| < |1 −z|, i.e., if ℜ(z) < 1
2.
Now we consider the second part of the theorem. The maximum occurring in the
above expression for |RN,µ(z)|1/N (with N, µ ﬁxed) takes place at t = µ. Hence
|RN,µ(z)|1/N ≤|z/(1 −z)1−µ|c1/N(1 −µ)1−µµµ.
An elementary optimization shows that the value of µ that minimizes this bound for
|RN,µ(z)|1/N is µ = 1/(|1 −z| + 1), i.e.,
k = (1 −µ)N =
N|1 −z|
|1 −z| + 1,
99In practice this is found approximately by the termination criterion of Algorithm 3.4.

290
Chapter 3. Series, Operators, and Continued Fractions
and the minimum equals |z|/(|1 −z| + 1). The details of these two optimizations are left
for Problem 3.4.34. This proves the second part of the theorem.
This minimum turns out to be a rather realistic estimate of the convergence ratio of the
optimal generalized Euler transformation for power series with d.c.m. coefﬁcients, unless
β(t) is practically constant in some interval around t = µ; the exception happens, e.g., if
un = an, 0 < a < 1, a ̸= µ; see Problem 3.4.33.
Here we shall list a few criteria for higher monotonicity, by which one can often answer
the question of whether a function is c.m. or d.c.m. or neither. When several c.m. or d.c.m.
are involved, the intervals should be reduced to the intersection of the intervals involved.
By Theorem 3.4.5, the question is then also settled for the corresponding sequence. In
simple cases the question can be answered directly by means of the deﬁnition or the above
theorem, e.g., for u(s) = e−ks, s−k, (k ≥0), for ℜs ≥0 in the ﬁrst case, for ℜs > 0 in the
second case.
(A) If u(s) is c.m., and a, b ≥0, then g(s) = u(as + b) and (−1)ju(j)(s) are c.m.,
j = 1, 2, 3, . . . . The integral
 ∞
s
u(t) dt is also c.m., if it is convergent. (The interval
of complete monotonicity may not be the same for g as for f .) Analogous statements
hold for sequences.
(B) The product of two c.m. is c.m. Similarly, the product of two d.c.m. is d.c.m. This
can evidently be extended to products of any number of factors, and hence to every
positive integral power of a c.m. or d.c.m. The proof is left for Problem 3.4.34.
(C) A uniformly convergent positive linear combination of c.m. is itself c.m. The same
criterion holds for d.c.m. without the requirement of positivity. The term “positive
linear combination” includes sums with positive coefﬁcients and, more generally,
Stieltjes integrals

u(s; p) dγ (p), where γ (p) is nondecreasing.
(D) Suppose that u(s) is a d.c.m. for s ≥a. F(u(s)) is then a d.c.m. for s > a, if the
radius of convergence of the Taylor expansion for F(z) is greater than max |u(s)|.
Suppose that u(s) is c.m. for s ≥a. We must then add the assumption that the
coefﬁcients of the Taylor expansion of F(z) are nonnegative, in order to make sure
that F(u(s)) is c.m. for s ≥a.
These statements are important particular cases of (C). We also used (B), according
to which each term u(s)k is c.m. (or a d.c.m. in the ﬁrst statement). Two illustrations:
g(s) = (1−e−s)−1 is c.m. for s > 0; h(s) = (s2 +1)−1 is a d.c.m. at least for s > 1 (choose
z = s−2). The expansion into powers of s−2 also provides an explicit decomposition,
h(s) = (s−2 + s−6 + · · ·) −(s−4 + s−8 + · · ·) = s2/(s4 −1) −1/(s4 −1),
where the two components are c.m. for s > 1. See also Example 3.4.8.
(E) If g′(s) is c.m. for s > a, and if u(z) is c.m. in the range of g(s) for s > a, then
F(s) = u(g(s)) is c.m. for s > a. (Note that g(s) itself is not c.m.)
For example, we shall show that 1/ ln s is c.m. for s > 1. Set g(s) = ln s, u(z) = z−1,
a = 1. Then u(z) is completely monotonic for z > 0, and g′(s) = s−1 is c.m. for
s > 0, a fortiori for s > 1 where ln s > 0. Then the result follows from (E).

3.4. Acceleration of Convergence
291
The problems of Sec. 3.4 contain many interesting examples that can be treated by
means of these criteria. One of the most important is that every rational function that is
analytic and bounded in a half-plane is d.c.m. there; see Problem 3.4.35. Sometimes a table
of Laplace transforms (see, e.g., the Handbook [1, Chap. 29]) can be useful in combination
with the criteria below.
Another set of criteria is related to the analytic properties of c.m. and d.c.m. functions.
Let u(s) be d.c.m. for s > a. According to statement 4 of Theorem 3.4.5, u(s) is analytic
and bounded for s ≥a′ for any a′ > a. The converse of this is not unconditionally true. If,
however, we add the conditions that
 ∞
−∞
|u(σ + iω)| dω < ∞,
u(s) →0,
as |s| →∞,
σ ≥a′,
(3.4.29)
then it can be shown that u(s) is a d.c.m. for s > a. This condition is rather restrictive;
there are many d.c.m. that do not satisfy it, for example, functions of the form e−ks or
k + b(s −c)−γ (k ≥0, b ≥0, c > a, 0 < γ ≤1). The following is a reasonably powerful
criterion: u(s) is a d.c.m. for s > a, e.g., if we can make a decomposition of the form
u(s) = f1(s) + f2(s)
or
u(s) = f1(s)f2(s),
where f1(s) is known to be d.c.m. for s > a, and f2(s) satisﬁes the conditions in (3.4.29).
Theorem 3.4.8.
Suppose that u(s) is c.m. for some s though not for all s. Then a singularity on the
real axis, at (say) s = a, must be among the rightmost singularities; u(s) is c.m. for s > a,
hence analytic for ℜs > a.
The statement in the theorem is not generally true if u(s) is only d.c.m. Suppose that
u(s) is d.c.m. for s > a, though not for any s < a. Then we cannot even be sure that there
exists a singularity s∗such that ℜs∗= a.
Example 3.4.8.
This theorem can be used for establishing that a given function is not a c.m. For
example, u(s) = 1/(1 + s2) is not c.m. since the rightmost singularities are s = ±i, while
s = 0 is no singularity. u(s) is a d.c.m. for s > 0; however, since it is analytic and bounded,
and satisﬁes (3.4.29) for any positive a′. This result also comes from the general statement
about rational functions bounded in a half-plane; see Problem 3.4.35.
Another approach: in any text about Laplace transforms you ﬁnd that, for s > 0,
1
s2 + 1 =
 ∞
0
e−sx sin x dx =
 ∞
0
e−sx(1 + sin x) dx −
 ∞
0
e−sx dx.
Now α′(x) ≥0 in both terms. Hence the formula (1/s + 1/(s2 + 1)) −1/s expresses
1/(s2 + 1) as the difference of two c.m. sequences for s > 0.
The easy application of criterion (D) above gave a smaller interval (s > 1), but a
faster decrease of the c.m. terms as s →∞.
Another useful criterion for this kind of negative conclusion is that a c.m. sequence
cannot decrease faster than every exponential as s →+∞, for s ∈R, unless it is identically

292
Chapter 3. Series, Operators, and Continued Fractions
zero. For there exists a number ξ such that α(ξ) > 0, hence
u(s) =
 ∞
0
e−sxdα(x) ≥
 ξ
0
e−sxdα(x) ≥e−sξα(ξ).
For example, e−s2 and 1/T(s) are not c.m. Why does this not contradict the fact that s−1e−s
is c.m.?
These ideas can be generalized. Suppose that {ci}∞
i=0 is a given sequence such that
the sum C(t) ≡∞
i=0 citi is known, and that ui is c.m. or d.c.m. (ci and C(t) may depend
on a complex parameter z too). Then
Sc =
∞

i=0
ciui =
∞

i=0
ci
 1
0
ti dβ(t)
 1
0
C(t) dβ(t).
It is natural to ask how well Sc is determined if ui has been computed for i < N, if
{un}∞
0 is constrained to be c.m. A systematic way to obtain very good bounds is to ﬁnd a
polynomial Q ∈PN such that |C(t) −Q(t)| ≤ϵN for all t ∈[0, 1]. Then
|Sc −Q(E)u0| =
((((
 1
0

C(t) −Q(t)

dβ(t)
(((( ≤ϵN
 1
0
|dβ(t)|.
Note that Q(E)u0 is a linear combination of the computed values ui, i < N, with coefﬁ-
cients independent of {un}. For C(t; z) = (1 −tz)−1 the generalized Euler transformation
(implicitly) works with a particular array of polynomial approximations, based on Taylor
expansion, ﬁrst at t = 0 and then at t = 1.
Canweﬁndbetterpolynomialapproximations? ForC(t; z) = (1−tz)−1, Gustafson’s
Chebyshev acceleration (GCA) [177] is in most respects, superior to Euler transformation.
Like Euler’s transformation this is based on linear transformations of sequences and has the
same range of application as the optimal Euler transformation. For GCA
ϵ1/N
N
→1/(3 +
√
8)
if z = −1. The number of terms needed for achieving a certain accuracy is thus for GCA
about ln(3 +
√
8)/ ln 3 ≈1.6 times as large as for the optimal Euler transformation.
3.4.5
Euler–Maclaurin’s Formula
In the summation of series with essentially positive terms the tail of the sum can be approx-
imated by an integral by means of the trapezoidal rule.
As an example, consider the sum S = ∞
j=1 j −2. The sum of the ﬁrst nine terms is,
to four decimal places, 1.5398. This suggests that we compare the tail of the series with
the integral of x−2 from 10 to ∞. We approximate the integral according to the trapezoidal
rule (see Sec. 1.1.3),
 ∞
10
x−2 dx = 1
2(10−2 + 11−2) + 1
2(11−2 + 12−2) + · · · =
∞

j=10
j −2 −1
210−2.

3.4. Acceleration of Convergence
293
Hence it follows that
∞

j=1
j −2 ≈1.53977 + [−x−1]∞
10 + 0.0050 = 1.53977 + 0.1050 = 1.64477.
The correct answer is π2/6 = 1.64493 40668 4823. We would have needed about 10,000
terms to get the same accuracy by direct addition of the terms!
The above procedure is not a coincidental trick, but a very useful method. A fur-
ther systematic development of the idea leads to the important Euler–Maclaurin summation
formula. We ﬁrst derive this heuristically by operator techniques and exemplify its use,
including a somewhat paradoxical example that shows that a strict treatment with the con-
sideration of the remainder term is necessary for very practical reasons. Since this formula
has several other applications, for example, in numerical integration (see Sec. 5.2), we
formulate it more generally than needed for the summation of inﬁnite series.
First, consider a rectangle sum on the ﬁnite interval [a, b], with n steps of equal length
h, a + nh = b; with the operator notation introduced in Sec. 3.3.2,
h
n−1

i=0
f (a + ih) = h
n−1

i=0
Eif (a) = hEn −1
E −1 f (a) = (En −1)
D
hD
ehD −1f (a).
We apply, to the second factor, the expansion derived in Example 3.1.5, with the Bernoulli
numbers Bν (recall that a + nh = b, Enf (a) = f (b)):
h
n−1

i=0
f (a + ih) = (En −1)
D

1 +
∞

ν=1
Bν(hD)ν
ν!

f (a)
(3.4.30)
=
 b
a
f (x) dx +
k

ν=1
hνBν
ν!

f (ν−1)(b) −f (ν−1)(a)

+ Rk+1.
Here Rk+1 is a remainder term that will be discussed thoroughly in Theorem 3.4.10. Set
h = 1, and assume that f (b), f ′(b), . . . tend to zero as b →∞. Recall that B1 = −1
2,
B2j+1 = 0 for j > 0, and set k = 2r + 1. This yields Euler–Maclaurin’s summation
formula,100
∞

i=0
f (a + i) =
 ∞
a
f (x) dx + f (a)
2
−
r

j=1
B2jf (2j−1)(a)
(2j)!
+ R2r+2
(3.4.31)
=
 ∞
a
f (x) dx + f (a)
2
−f ′(a)
12
+ f (3)(a)
720
−· · · ,
in a form suitable for the convergence acceleration of series of essentially positive terms.
We give in Table 3.4.2 a few coefﬁcients related to the Bernoulli and the Euler numbers.
There are some obscure points in this operator derivation, but we shall consider it as
a heuristic calculation only and shall not try to legitimate the various steps of it. With an
100Leonhard Euler and the British mathematician Colin Maclaurin apparently discovered the summation formula
independently; see Goldstine [159, p. 84]. Euler’s publication came in 1738.

294
Chapter 3. Series, Operators, and Continued Fractions
Table 3.4.2. Bernoulli and Euler numbers; B1 = −1/2, E1 = 1.
2j
0
2
4
6
8
10
12
B2j
1
1
6
−1
30
1
42
−1
30
5
66
−691
2730
B2j
(2j)!
1
1
12
−1
720
1
30,240
−
1
1,209,600
1
47,900,160
B2j
2j(2j −1)
1
1
12
−1
360
1
1260
−
1
1680
1
1188
−
691
360,360
E2j
1
−1
5
−61
1385
−50,521
2,702,765
appropriate interpretation, a more general version of this formula will be proved by other
means in Theorem 3.4.10. A general remainder term is obtained there, if you let b →∞
in (3.4.37). You do not need it often, because the following much simpler error bound is
usually applicable—but there are exceptions.
The Euler–Maclaurin expansion (on the right-hand side) is typically semiconvergent
only. Nevertheless a few terms of the expansion often give surprisingly high accuracy with
simple calculations. For example, if f (x) is c.m., i.e., if
(−1)jf (j)(x) ≥0,
x ≥a,
j ≥0,
then the partial sums oscillate strictly around the true result; the ﬁrst neglected term is then
a strict error bound. (This statement also follows from the theorem below.)
Before we prove the theorem we shall exemplify how the summation formula is used
in practice.
Example 3.4.9.
We return to the case of computing S = ∞
j=1 j −2 and treat it with more precision
and accuracy. With f (x) = x−2, a = 10, we ﬁnd
 ∞
a f (x) dx = a−1, f ′(a) = −2a−3,
f ′′′(a) = −24a−5, . . . . By (3.4.31), (r = 2),
∞

x=1
x−2 =
9

x=1
x−2 +
∞

i=0
(10 + i)−2
= 1.53976 7731 + 0.1 + 0.005 + 0.00016 6667 −0.00000 0333 + R6
= 1.64493 4065 + R6.
Sincef (x) = x−2 isc.m.(seeDeﬁnition3.4.2), theﬁrstneglectedtermisastricterrorbound;
it is less than 720 · 10−7/30,240 < 3·10−9. (The actual error is approximately 2·10−9.)
Although the Euler–Maclaurin expansion in this example seems to converge rapidly,
it is in fact only semiconvergent for any a > 0, and this is rather typical. We have, namely,
f (2r−1)(a) = −(2r)!a−2r−1,

3.4. Acceleration of Convergence
295
and, by Example 3.1.5,
B2r/(2r)! ≈(−1)r+12(2π)−2r.
The ratio of two successive terms is thus −(2r + 2)(2r + 1)/(2πa)2, hence the modulus of
terms increases when 2r + 1 > 2πa.
The “rule” that one should terminate a semiconvergent expansion at the term of small-
est magnitude is, in general, no good for Euler–Maclaurin applications, since the high-order
derivatives (on the right-hand side) are typically much more difﬁcult to obtain than a few
more terms in the expansion on the left-hand side. Typically, you ﬁrst choose r, r ≤3,
depending on how tedious the differentiations are, and then you choose a in order to meet
the accuracy requirements.
In this example we were lucky to have access to simple closed expressions for the
derivatives and the integral of f . In other cases, one may use the possibilities for the
numerical integration on an inﬁnite interval mentioned in Chapter 5. In Problem 3.4.19
you ﬁnd two formulas that result from the substitution of the formulas (3.3.48) that express
higher derivatives in terms of central differences into the Euler–Maclaurin expansion.
An expansion of f (x) into negative powers of x is often useful both for the integral
and for the derivatives.
Example 3.4.10.
We consider f (x) = (x3 + 1)−1/2, for which the expansion
f (x) = x−3/2(1 + x−3)−1/2 = x−1.5 −1
2x−4.5 + 3
8x−7.5 −· · ·
was derived and applied in Example 3.1.6. It was found that
 ∞
10 f (x) dx = 0.632410375,
correctly rounded, and that f ′′′(10) = −4.13·10−4 with less than 1% error. The f ′′′(10)-term
in the Euler–Maclaurin expansion is thus −5.73 10−7, with absolute error less than 6·10−9.
InsertingthisintoEuler–Maclaurin’ssummationformula, togetherwiththenumericalvalues
of 9
n=0 f (n) and 1
2f (10) −1
12f ′(10), we obtain ∞
n=0 f (n) = 3.7941 1570 ± 10−8. The
reader is advised to work out the details as an exercise.
Example 3.4.11.
Let f (x) = e−x2, a = 0. Since all derivatives of odd order vanish at a = 0, then
the expansion (3.4.31) may give the impression that ∞
j=0 e−j 2 =
 ∞
0 e−x2 dx + 0.5 =
1.386 2269, but the sum (that is easily computed without any convergence acceleration) is
actually 1.386 3186, hence the remainder R2r+2 cannot tend to zero as r →∞. The inﬁnite
Euler–Maclaurin expansion, where all terms but two are zero, is convergent but is not valid.
Recall the distinction between the convergence and the validity of an inﬁnite expansion
made in Sec. 3.1.2.
In this case f (x) is not c.m.; for example, f ′′(x) changes sign at x = 1. With
appropriate choice of r, the general error bound (3.4.37) will tell us that the error is very
small, but it cannot be used for proving that it is zero—because this is not true.

296
Chapter 3. Series, Operators, and Continued Fractions
The mysteries of these examples have hopefully raised the appetite for a more sub-
stantial theory, including an error bound for the Euler–Maclaurin formula. We ﬁrst need
some tools that are interesting in their own right.
The Bernoulli polynomial Bn(t) is an nth degree polynomial deﬁned by the sym-
bolic relation Bn(t) = (B + t)n, where the exponents of B become subscripts after the
expansion according to the binomial theorem. The Bernoulli numbers Bj were deﬁned in
Example 3.1.5. Their recurrence relation (3.1.19) can be written in the form
n−1

j=0
n
j

Bj = 0,
n ≥2,
or “symbolically” (B + 1)n = Bn = Bn (for the computation of Bn−1), n ̸= 1, hence
B0(t) = 1, B1(t) = t + B1 = t −1/2, and
Bn(1) = Bn(0) = Bn,
n ≥2.
The Bernoulli function ˆBn(t) is a piecewise polynomial deﬁned for t ∈R by the equation
ˆBn(t) = Bn(t −⌊t⌋).101 (Note that ˆBn(t) = Bn(t) if 0 ≤t < 1.)
Lemma 3.4.9.
(a) ˆB′
n+1(t)/(n + 1)! = ˆBn(t)/n!, (n > 0),
ˆBn(0) = Bn. (For n = 1 this is the limit from the right.)
 1
0
Bn(t)
n!
dt =
! 1
if n = 0,
0
otherwise.
(b) The piecewise polynomials ˆBp(t) are periodic; ˆBp(t + 1) = ˆBp(t). ˆB1(t) is contin-
uous, except when t is an integer. For n ≥2, ˆBn ∈Cn−2(−∞, ∞).
(c) The Bernoulli functions have the following (modiﬁed) Fourier expansions, (r ≥1),
ˆB2r−1(t)
(2r −1)! = (−1)r2
∞

n=1
sin 2nπt
(2nπ)2r−1 ,
ˆB2r(t)
(2r)! = (−1)r−12
∞

n=1
cos 2nπt
(2nπ)2r .
Note that ˆBn(t) is an even (odd) function, when n is even (odd).
(d) | ˆB2r(t)| ≤|B2r|.
Proof. Statement (a) follows directly from the symbolic binomial expansion of the Bernoulli
polynomials.
The demonstration of statement (b) is left for a problem. The reader is advised to
draw the graphs of a few low-order Bernoulli functions.
101The function ⌊t⌋is the ﬂoor function deﬁned as the largest integer ≤t, i.e., the integer part of t. In many
older and current works the symbol [t] is used instead, but this should be avoided.

3.4. Acceleration of Convergence
297
The Fourier expansion for ˆB1(t) follows from the Fourier coefﬁcient formulas (3.2.6)
(modiﬁed for the period 1 instead of 2π). The expansions for ˆBp(t) are then obtained by
repeated integrations, term by term, with the use of (a). Statement (d) then follows from
the Fourier expansion, because ˆB2r(0) = B2r.
Remark 3.4.4. For t = 0 we obtain an interesting classical formula, together with a useful
asymptotic approximation that was obtained in a different way in Sec. 3.1.2:
∞

n=1
1
n2r = |B2r|(2π)2r
2(2r)!
,
|B2r|
(2r)! ∼
2
(2π)2r .
(3.4.32)
Also note how the rate of decrease of the Fourier coefﬁcients is related to the type of
singularity of the Bernoulli function at the integer points. (It does not help that the functions
are smooth in the interval [0, 1].)
The Bernoulli polynomials have a generating function that is elegantly obtained by
means of the following “symbolic” calculation:
∞

0
Bn(y)xn
n!
=
∞

0
(B + y)nxn
n!
= e(B+y)x = eBxeyx = xeyx
ex −1.
(3.4.33)
If the series is interpreted as a power series in the complex variable x, the radius of conver-
gence is 2π.
Theorem 3.4.10 (The Euler–Maclaurin Formula).
Set xi = a + ih, xn = b, suppose that f ∈C2r+2(a, b), and let ˆT (a : h : b)f be the
trapezoidal sum
ˆT (a : h : b)f =
n

i=1
h
2

f (xi−1) + f (xi)

= h
 n−1

i=0
f (xi) + 1
2(f (b) −f (a))

. (3.4.34)
Then
ˆT (a : h : b)f −
 b
a
f (x) dx = h2
12

f ′(b) −f ′(a)

−h4
720

f ′′′(b) −f ′′′(a)

(3.4.35)
+ · · · + B2rh2r
(2r)!

f (2r−1)(b) −f (2r−1)(a)

+ R2r+2(a, h, b)f.
The remainder R2r+2(a, h, b)f is O(h2r+2). It is represented by an integral with a kernel
of constant sign in (3.4.36). An upper bound for the remainder is given in (3.4.37). The
estimation of the remainder is very simple in certain important, particular cases:
• If f (2r+2)(x) does not change sign in the interval [a, b], then R2r+2(a, h, b)f has the
same sign as the ﬁrst neglected term.102
102If r = 0 all terms of the expansion are “neglected.”

298
Chapter 3. Series, Operators, and Continued Fractions
• If f (2r+2)(x) and f (2r)(x) have the same constant sign in [a, b], then the value of the
left-hand side of (3.4.35) lies between the values of the partial sum of the expansion
displayed in (3.4.35) and the partial sum with one term less.103
In the limit, as b →∞, these statements still hold—also for the summation formula
(3.4.31)—provided that the left-hand side of (3.4.35) and the derivatives f (ν)(b) (ν = 1 :
2r + 1) tend to zero, if it is also assumed that
 ∞
a
|f (2r+2)(x)| dx < ∞.
Proof. To begin with we consider a single term of the trapezoidal sum, and set x = xi−1+ht,
t ∈[0, 1], f (x) = F(t). Suppose that F ∈Cp[0, 1], where p is an even number.
Weshallapplyrepeatedintegrationbyparts, Lemma3.2.6, totheintegral
 1
0 F(t) dt =
 1
0 F(t)B0(t) dt. Use statement (a) of Lemma 3.4.9 in the equivalent form,

Bj(t)/j! dt =
Bj+1(t)/(j + 1)!
Consider the ﬁrst line of the expansion in the next equation. Recall that Bν = 0 if ν is
odd and ν > 1. Since Bj+1(1) = Bj+1(0) = Bj+1, j will thus be odd in all nonzero terms,
except for j = 0. Then, with no loss of generality, we assume that p is even.
 1
0
F(t) dt =
p−1

j=0
(−1)jF (j)(t) Bj+1(t)
(j + 1)!
((((
1
t=0
+ (−1)p
 1
0
F (p)(t)Bp(t)
p!
dt
= F(1) + F(0)
2
+
p−1

j=1
−Bj+1
(j + 1)!

F (j)(1) −F (j)(0)

+
 1
0
F (p)(t)Bp(t)
p!
dt
= F(1) + F(0)
2
−
p−3

j=1
Bj+1
(j + 1)!

F (j)(1) −F (j)(0)

−
 1
0
F (p)(t)Bp −Bp(t)
p!
dt.
The upper limit of the sum is reduced to p −3, since the last term (with j = p −1) has been
moved under the integral sign, and all values of j are odd. Set j + 1 = 2k and p = 2r + 2.
Then k is an integer that runs from 1 to r. Hence
p−3

j=1
Bj+1
(j + 1)!(F (j)(1) −F (j)(0)) =
r

k=1
B2k
(2k)!(F (2k−1)(1) −F (2k−1)(0)).
Now set F(t) = f (xi−1 + ht), t ∈[0, 1]. Then F (2k−1)(t) = h2k−1f (2k−1)(xi−1 + ht), and
make abbreviations such as fi = f (xi), f (j)
i
= f (j)(xi).
 xi
xi−1
f (x) dx = h
 1
0
F(t) dt = h(fi−1 + fi)
2
−
r

k=1
B2kh2k
(2k)! (f (2k−1)
i
−f (2k−1)
i−1
) −R,
103Formally this makes sense for r ≥2 only, but if we interpret f (−1) as “the empty symbol,” it makes sense
also for r = 1. If f is c.m. the statement holds for every r ≥1. This is easy to apply, because simple criteria for
complete monotonicity are given in Sec. 3.4.4.

3.4. Acceleration of Convergence
299
whereR isthelocalremainderthatisnowanintegralover[xi−1, xi]. Addingtheseequations,
for i = 1 : n, yields a result equivalent to (3.4.35), namely
 b
a
f (x) dx = ˆT (a : h : b)f −
r

k=1
B2kh2k
(2k)! f (2k−1)(x)
((((
b
x=a
−R2r+2(a, h, b)f,
R2r+2(a, h, b)f = h2r+2
 b
a

B2r+2 −ˆB2r+2((x −a)/h)
f (2r+2)(x)
(2r + 2)! dx.
(3.4.36)
By Lemma 3.4.9, | ˆB2r+2(t)| ≤|B2r+2|, hence the kernel B2r+2 −ˆB2r+2((x −a)/h) has the
same sign as B2r+2. Suppose that f (2r+2)(x) does not change sign on (a, b). Then
sign f (2r+2)(x) = sign

f (2r+1)(b) −f (2r+1)(a)

,
hence R2r+2(a, h, b)f has the same sign as the ﬁrst neglected term. The second statement
about “simple estimation of the remainder” then follows from Theorem 3.1.4, since the
Bernoulli numbers (with even subscripts) have alternating signs.
If sign f (2r+2)(x) is not constant, then we note instead that
|B2r+2 −ˆB2r+2((x −a)/h)| ≤|2B2r+2|,
and hence
|R2r+2(a, h, b)f | ≤h2r+2 |2B2r+2|
(2r + 2)!
 b
a
|f (2r+2)(x)| dx
≈2
 h
2π
2r+2  b
a
|f (2r+2)(x)| dx.
(3.4.37)
If
 ∞
a |f (2r+2)(x)| dx < ∞this holds also in the limit as b →∞.
Note that there are (at least) three parameters here that can be involved in different
natural limit processes. For example, one of the parameters can tend to its limit, while the
two others are kept ﬁxed. The remainder formula (3.4.37) contains all you need for settling
various questions about convergence.
• b →∞: natural when Euler–Maclaurin’s formula is used as a summation formula,
or for deriving an approximation formula valid when b is large.
• h →0: natural when Euler–Maclaurin’s formula is used in connection with numerical
integration. You see how the values of derivatives of f at the endpoints a, b can highly
improvetheestimateoftheintegraloff , obtainedbythetrapezoidalrulewithconstant
step size. Euler–Maclaurin’s formula is also useful for the design and analysis of other
methods for numerical integration; see Romberg’s method, Sec. 5.2.2.
• r →∞: limr→∞R2r+2(a, h, b)f = 0 can be satisﬁed only if f (z) is an entire
function such that |f (n)(a)| = o((2π/h)n) as n →∞. Fortunately, this type of
convergence is rarely needed in practice. With appropriate choice of b and h, the

300
Chapter 3. Series, Operators, and Continued Fractions
expansion is typically rapidly semiconvergent. Since the derivatives of f are typi-
cally more expensive to compute than the values of f , one frequently reduces h (in
integration) or increases b (in summation or integration over an inﬁnite interval) and
truncates the expansion several terms before one has reached the smallest term that
is otherwise the standard procedure with alternating semiconvergent expansion.
Variations of the Euler–Maclaurin summation formula, with ﬁnite differences instead
of derivatives in the expansion, are given in Problem 3.4.19, where you also ﬁnd a more
general form of the formula, and two more variations of it.
Euler–Maclaurin’s formula can also be used for ﬁnding an algebraic expression for a
ﬁnite sum (see Problem 3.4.31) or, as in the following example, for ﬁnding an expansion
that determines the asymptotic behavior of a sequence or a function.
Example 3.4.12.
To derive an expansion that generalizes Stirling’s formula (3.2.35), we shall use the
Euler–Maclaurin formula for f (x) = ln x, a = m > 0, h = 1, b = n ≥m. We obtain
ˆT (m : 1 : n)f =
n

i=m+1
ln i −1
2 ln n + 1
2 ln m = ln(n!) −1
2 ln n −ln(m!) + 1
2 ln m,
f (2k−1)(x) = (2k −2)!x1−2k,
 n
m
f (x) dx = n ln n −n −m ln m + m.
Note that ˆT (m : 1 : n)f and
 n
m f (x) dx are unbounded as n →∞, but their difference
is bounded. Putting these expressions into (3.4.35) and separating the terms containing n
from the terms containing m gives
ln(n!) −

n + 1
2

ln n + n −
r

k=1
B2k
2k(2k −1)n2k−1
(3.4.38)
= ln(m!) −

m + 1
2

ln m + m −
r

k=1
B2k
2k(2k −1)m2k−1 −R2r+2(m : 1 : n).
By (3.4.37),
|R2r+2(m : 1 : n)| ≤
 n
m
|2B2r+2|
(2r + 2)x2r+2 dx
≤
|2B2r+2|
(2r + 2)(2r + 1)|m2r+1| ≈
(2r)!
π|2πm|2r+1 .
(3.4.39)
Now let n →∞with ﬁxed r, m. First, note that the integral in the error bound converges.
Next, in most texts of calculus Stirling’s formula is derived in the following form:
n! ∼
√
2πnn+ 1
2 e−n,
(n →∞).
(3.4.40)
If you take the natural logarithm of this, it follows that the left-hand side of (3.4.38) tends

3.4. Acceleration of Convergence
301
to 1
2 ln(2π), and hence
ln(m!) =

m + 1
2

ln m −m + 1
2 ln(2π) +
r

k=1
B2k
2k(2k −1)m2k−1 + R,
(3.4.41)
where a bound for R is given by (3.4.39). The numerical values of the coefﬁcients are found
in Table 3.4.2.
Remark 3.4.5. You may ask why we refer to (3.4.40). Why not? Well, it is not necessary,
because it is easy to prove that the left-hand side of (3.4.38) increases with n and is bounded;
it thus tends to some limit C (say). The proof that C = ln
√
2π exactly is harder, without
the Wallis product idea (from 1655) or something equally ingenious or exotic. But if you
compute the right-hand side of (3.4.38) for m = 17, r = 5 (say), and estimate the remainder,
you will obtain C to a fabulous guaranteed accuracy, in negligible computer time after a
rather short programming time. And you may then replace 1
2 ln 2π by your own C in
(3.4.41), if you like.
Remark 3.4.6. Almost the same derivation works also for f (x) = ln(x +z), m = 0, where
z is a complex number not on the negative real axis. A few basic facts about the gamma
function are needed; see details in Henrici [197, Sec. 11.11, Example 3].
The result is that you just replace the integer m by the complex number z in the
expansion (3.4.41). According to the Handbook [1, Sec. 6.1.42] R is to be multiplied by
K(z) = supu≥0 |z2/(u2 + z2)|. For z real and positive, K(z) = 1, and since f ′(x) =
(z + x)−1 is c.m., it follows from Theorem 3.4.10 that, in this case, R is less in absolute
value than the ﬁrst term neglected and has the same sign.
It is customary to write ln T(z + 1) instead of ln(z!). The gamma function is one
of the most important transcendental functions; see, e.g., the Handbook [1, Sec. 6.5] and
Lebedev [240].
This formula (with m = z) is useful for the practical computation of ln T(z + 1). Its
semiconvergence is best if ℜz is large and positive. If this condition is not satisﬁed, the
situation can easily be improved by means of logarithmic forms of the
• reﬂection formula: T(z)T(1 −z) = π/ sin πz,
• recurrence formula: T(z + 1) = zT(z).
By simple applications of these formulas the computation of ln T(z+1) for an arbitrary
z ∈C is reduced to the computation of the function for a number z′ such that |z′| ≥17,
ℜz′ >
1
2, for which the total error, if r = 5, becomes typically less than 10−14. See
Problem 3.4.23.
Remark 3.4.7. As you may have noted, we write “the Euler–Maclaurin formula” mainly
for (3.4.35), which is used in general theoretical discussions, or if applications other than
the summation of an inﬁnite series are the primary issue. The term “the Euler–Maclaurin
summation formula” is mainly used in connection with (3.4.31), i.e., when the summation

302
Chapter 3. Series, Operators, and Continued Fractions
of an inﬁnite series is the issue. “The Euler–Maclaurin expansion” denotes both the right-
hand side of (3.4.35), except for the remainder and for the corresponding terms of (3.4.31).
These distinctions are convenient for us, but they are neither important nor in general use.
Although, in this section, the main emphasis is on the application of the Euler–
Maclaurin formula to the computation of sums and limits, we shall comment a little on
its possibilities for other applications.
• It shows that the global truncation error of the trapezoidal rule for
 b
a f (x) dx with
step size h has an expansion into powers of h2. Note that although the expansion
contains derivatives at the boundary points only, the remainder requires that |f (2r+2)|
is integrable in the interval [a, b]. The Euler–Maclaurin formula is thus the theoretical
basis for the application of repeated Richardson extrapolation to the results of the
trapezoidal rule, known as Romberg’s method; see Sec. 5.2.2. Note that the validity
depends on the differentiability properties of f .
• The Euler–Maclaurin formula can be used for highly accurate numerical integration
when the values of some derivatives of f are known at x = a and x = b. More about
this in Sec. 5.2.1.
• Theorem 3.4.10 shows that the trapezoidal rule is second order accurate, unless
f ′(a) = f ′(b), but there exist interesting exceptions. Suppose that the function
f is inﬁnitely differentiable for x ∈R, and that f has [a, b] as an interval of peri-
odicity, that is f (x + b −a) = f (x) for all x ∈R. Then f (k)(b) = f (k)(a), for
k = 0, 1, 2, . . . , hence every term in the Euler–Maclaurin expansion is zero for the
integral over the whole period [a, b]. One could be led to believe that the trapezoidal
rule gives the exact value of the integral, but this is usually not the case; for most pe-
riodic functions f , limr→∞R2r+2f ̸= 0; the expansion converges, of course, though
not necessarily to the correct result.
We shall illuminate these amazing properties of the trapezoidal rule from different
points of view in several places in this book, for example, in Sec. 5.1.4. See also applications
to the so-called bell sums in Problem 3.4.29.
3.4.6
Repeated Richardson Extrapolation
Let F(h) denote the value of a certain quantity obtained with step length h. In many
calculations one wants to know the limiting value of F(h) as the step length approaches
zero. But the work to compute F(h) often increases sharply as h →0. In addition, the
effects of roundoff errors often set a practical bound for how small h can be chosen.
Often, one has some knowledge of how the truncation error F(h) −F(0) behaves
when h →0. If
F(h) = a0 + a1hp + O(hr),
h →0,
r > p,
where a0 = F(0) is the quantity we are trying to compute and a1 is unknown, then a0 and
a1 can be estimated if we compute F for two step lengths, h and qh, q > 1,
F(h) = a0 + a1hp + O(hr),
F(qh) = a0 + a1(qh)p + O(hr),

3.4. Acceleration of Convergence
303
from which, eliminating a1, we get
F(0) = a0 = F(h) + F(h) −F(qh)
qp −1
+ O(hr).
(3.4.42)
This formula is called Richardson extrapolation, or the deferred approach to the limit.104
Examples of this were mentioned in Chapter 1—the application of the above process to
the trapezoidal rule for numerical integration (where p = 2, q = 2), and for differential
equations p = 1, q = 2 for Euler’s method, p = 2, q = 2 for Runge’s second order
method.
We call the term (F(h) −F(qh))/(qp −1) the Richardson correction. It is used in
(3.4.42) for improving the result. Sometimes it is used only for estimating the error. This
can make sense, for example, if the values of F are afﬂicted by other errors, usually irregular,
suspected of being comparable in size to the correction. If the irregular errors are negligible,
this error estimate is asymptotically correct. More often, the Richardson correction is used as
an error estimate for the improved (or extrapolated) value F(h)+(F(h)−F(qh))/(qp −1).
This is typically a strong overestimate; the error estimate is O(hp), while the error is O(hr),
(r > p).
Suppose that a more complete expansion of F(h) in powers of h is known to exist,
F(h) = a0 + a1hp1 + a2hp2 + a3hp3 + · · · ,
0 < p1 < p2 < p3 < · · · ,
(3.4.43)
where the exponents are known while the coefﬁcients are unknown. Then one can re-
peat the use of Richardson extrapolation in a way described below. This process is, in
many numerical problems—especially in the numerical treatment of integral and differ-
ential equations—one of the simplest ways to get results which have tolerable truncation
errors. The application of this process becomes especially simple when the step lengths
form a geometric sequence H, H/q, H/q2, . . . , where q > 1 and H is the basic step
length.
Theorem 3.4.11.
Suppose that there holds an expansion of the form of (3.4.43), for F(h), and set
F1(h) = F(h),
Fk+1(h) = qpkFk(h) −Fk(qh)
qpk −1
= Fk(h) + Fk(h) −Fk(qh)
qpk −1
,
(3.4.44)
for k = 1 : (n −1), where q > 1. Then Fn(h) has an expansion of the form
Fn(h) = a0 + a(n)
n hpn + a(n)
n+1hpn+1 + · · · ,
a(n)
ν
=
n−1
&
k=1
qpk −qpν
qpk −1 aν.
(3.4.45)
Note that a(n)
ν
= 0 for all ν < n.
104The idea of a deferred approach to the limit is sometimes used also in the experimental sciences—for example,
when some quantity is to be measured in a complete vacuum (difﬁcult or expensive to produce). It can then be more
practical to measure the quantity for several different values of the pressure. Expansions analogous to (3.4.43) can
sometimes be motivated by the kinetic theory of gases.

304
Chapter 3. Series, Operators, and Continued Fractions
Proof. Temporarily set Fk(h) = a0 + a(k)
1 hp1 + a(k)
2 hp2 + · · · + a(k)
ν hpν + · · · . Put this
expansion into the ﬁrst expression on the right-hand side of (3.4.44) and, substituting k + 1
for k, put it into the left-hand side. By matching the coefﬁcients for hpν we obtain
a(k+1)
ν
= a(k)
ν (qpk −qpν)/(q(pk) −1).
By (3.4.43), the expansion holds for k = 1, with a(1)
ν
= aν.
The recursion formula
then yields the product formula for a(n)
ν . Note that a(ν+1)
ν
= 0, hence a(n)
ν
= 0 for all
ν < n.
Theproductformulaisfortheoreticalpurposes. Therecurrenceformulaisforpractical
use. If an expansion of the form of (3.4.43) is known to exist, the above theorem gives a way
to compute increasingly better estimates of a0. The leading term of Fn(h) −a0 is a(n)
n hpn;
the exponent of h increases with n. A moment’s reﬂection on (3.4.44) will convince the
reader that (using the notation of the theorem) Fk+1(h) is determined by the k + 1 values
F1(H), F1(H/q), . . . , F1(H/qk).
With some changes in notation we obtain the following algorithm.
Algorithm 3.5. Repeated Richardson Extrapolation.
Set
Tm,1 = F(H/qm−1),
m = 1 : N,
(3.4.46)
and for m = 2 : N, k = 1 : m −1, compute
Tm,k+1 = qpkTm,k −Tm−1,k
qpk −1
= Tm,k + Tm,k −Tm−1,k
qpk −1
,
(3.4.47)
where the second expression is usually preferred.
The computations for repeated Richardson extrapolation can be set up in the following
scheme,
T11
T21
T22
T31
T32
T33
T41
T42
T43
T44
,
where an extrapolated value in the scheme is obtained by using the quantity to its left and
the correction diagonally above. (In a computer the results are simply stored in a lower
triangular matrix.)
According to the argument above, one continues the process until two values in the
same row agree to the desired accuracy, i.e.,
|Tm,k −Tm,k−1| < Tol −CU,
where Tol is the permissible error and CU is an upper bound of the irregular error (see
below). (Tol should, of course, be chosen larger than CU.) If no other error estimate is

3.4. Acceleration of Convergence
305
available, mink |Tm,k −Tm,k−1| + CU is usually chosen as the error estimate, even though
it is typically a strong overestimate.
Typically, k = m and Tmm is accepted as the numerical result, but this is not always
the case. For instance, if H has been chosen so large that the use of the basic asymptotic
expansion is doubtful, then the uppermost diagonal of the extrapolation scheme contains
nonsense and should be ignored, except for its element in the ﬁrst column. Such a case is
detected by inspection of the difference quotients in a column. If for some k, where Tk+2,k
has been computed and the modulus of the relative irregular error of Tk+2,k −Tk+1,k is less
than (say) 20%, and, most important, the difference quotient (Tk+1,k−Tk,k)/(Tk+2,k−Tk+1,k)
is very different from its theoretical value qpk, then the uppermost diagonal is to be ignored
(except for its ﬁrst element). In such a case, one says that H is outside the asymptotic
regime.
In this discussion a bound for the inherited irregular error is needed. We shall now
derive such a bound. Fortunately, it turns out that the numerical stability of the Richardson
scheme is typically very satisfactory (although the total error bound for Tmk will never be
smaller than the largest irregular error in the ﬁrst column).
Denote by ϵ1 the column vector with the irregular errors of the initial data. We neglect
the rounding errors committed during the computations.105 Then the inherited errors satisfy
the same linear recursion formula as the Tm,k, i.e.,
ϵm,k+1 = qpkϵm,k −ϵm−1,k
qpk −1
.
Denote the kth column of errors by ϵk, and set ∥ϵk∥∞= maxm |ϵm,k|. Then
∥ϵk+1∥∞≤qpk + 1
qpk −1∥ϵk∥∞.
Hence, for every k, ∥ϵk+1∥∞≤CU, where ∥ϵ1∥∞= U and C is the inﬁnite product,
C =
∞
&
k=1
qpk + 1
qpk −1 =
∞
&
k=1
1 + q−pk
1 −q−pk ,
that converges as fast as  q−pk; the multiplication of ten factors are thus more than enough
for obtaining a sufﬁciently accurate value of C.
The most common special case is an expansion where pk = 2k,
F(h) = a0 + a1h2 + a2h4 + a3h6 + · · · .
(3.4.48)
ThisexpansionholdsfortheerrorincompositetrapezoidalruleandisthebasisforRomberg’s
method for numerical integration. The Richardson corrections then become 4/3, 4/15,
4/63, . . . . In this case we ﬁnd that C = 5
3 · 7
15 · · · < 2 (after less than ten factors).
For (systems of) ordinary differential equations there exist some general theorems,
according to which the form of the asymptotic expansion (3.4.43) of the global error can be
found.
105They are usually of less importance for various reasons. One can also make them smaller by subtracting a
suitable constant from all initial data. This is applicable to all linear methods of convergence acceleration.

306
Chapter 3. Series, Operators, and Continued Fractions
• ForNumerov’smethod forordinarydifferentialequations, discussedinExample3.3.15
and Problem 3.4.27, one can show that we have the same exponents in the expansion
for the global error, but a1 = 0 (and the ﬁrst heading disappears). We thus have the
same product as above, except that the ﬁrst factor disappears, hence C < 2 · 3
5 = 1.2.
• ForEuler’smethodforordinarydifferentialequations, presentedinSec. 1.5.1, pk = k;
the Richardson corrections are 4/1, 4/3, 4/7, 4/15, . . . . Hence C = 3· 5
3 · 9
7 · · · =
8.25.
• For Runge’s second order method, presented in Sec. 1.5.3, the exponents are the same,
but a1 = 0. We thus have the same product as for Euler’s method, except that the
ﬁrst factor disappears, and C = 8.25/3 = 2.75.
In the special case that pj = j · p, j = 1, 2, 3, . . . in (3.4.43), i.e., for expansions of
the form
F(h) = a0 + a1hp + a2h2p + a3h3p + · · · ,
(3.4.49)
it is not necessary that the step sizes form a geometric progression. We can choose any
increasing sequence of integers q1 = 1, q2, . . . , qk, set hi = H/qi, and use an algorithm
that looks very similar to repeated Richardson extrapolation. Alternative sequences, that
may be suitable in the common case that the cost of evaluating F(h) for small h is high, are
the harmonic sequence 1, 2, 3, 4, 5, 6, 7, . . . and the sequence 1, 2, 3, 4, 8, 12, 16, 24, . . . ,
suggested by Bulirsch.
Note that the expansion (3.4.49) is a usual power series in the variable x = hp, which
can be approximated by a polynomial in x. Suppose that k+1 values F(H), F(H/q2), . . . ,
F(H/qk) are known. Then by the corollary to Theorem 4.2.1, they are uniquely determined
by the interpolation conditions
Q(xi) = F(H/qi),
xi = (H/qi)p,
i = 1 : k.
Our problem is to ﬁnd Q(0). Neville’s algorithm for iterative linear interpolation, which
will be derived in Sec. 4.2.4, is particularly convenient in this situation. After a change of
notation, Neville’s algorithm yields the following recursion: For m = 1 : N, set Tm,1 =
F(H/qm), where 1 = q1 < q2 < q3 . . . is any increasing sequence of integers, and compute,
for m = 2 : N, k = 1 : m −1,
Tm,k+1 = Tm,k +
Tm,k −Tm−1,k
(qm/qm−k)p −1.
(3.4.50)
ThecomputationscanbesetupinatrianglematrixasforrepeatedRichardsonextrapolations.
We remark that Richardson extrapolation does not require an expansion of the form
(3.4.43). Let Tn,0 = Sn be a sequence converging to S and xn a sequence of parameters
converging to zero when n →∞. Then Richardson extrapolation can be written as
Tn,k+1 = Tn,k −Tn+1,k −Tn,k
xn+k+1 −xn
.

3.4. Acceleration of Convergence
307
There are conditions (obtained by P. J. Laurent) such that the columns and the diagonals
converge to the same limit S, and conditions for the convergence to be accelerated; see
Brezinski [49, Sec. II.3].
Example 3.4.13.
The ancient Greeks computed approximate values of the circumference of the unit cir-
cle, 2π, byinscribingaregularpolygonandcomputingitsperimeter. Archimedesconsidered
the inscribed 96-sided regular polygon, whose perimeter is 6.28206 . . . = 2 · 3.14103 . . . .
In general, a regular n-sided polygon inscribed (circumscribed) in a circle with radius
1 has perimeter 2an (2bn), where
an = n sin(π/n),
bn = n sin(tan /n).
Clearly an < π < bn, giving lower and upper bound for π. Setting h = 1/n, we have
an = 1
h sin πh = π −π3
3! h2 + π5
5! h4 −π7
7! h6 + · · · ,
bn = 1
h tan πh = π + π3
3 h2 + 2π5
15 h4 −17π7
315 h6 + · · · .
We ﬁrst derive a recursion formula that leads from an and bn to a2n and b2n.
Setting
nm = n1 · 2m−1 and
sm = 1/ sin(π/nm),
tm = 1/ tan(π/nm),
we have anm = nm/sm, bnm = nm/tm.
Using the trigonometric formula tan(x/2) =
sin x/(1 + cos x), we obtain the recursion
tm = sm−1 + tm−1,
sm =
 
t2m + 1,
m = 1, 2, . . . .
(3.4.51)
Note that no trigonometric functions are used—only the square root, which can be computed
by Newton’s method.
Taking n1 = 6 gives a6 = 6/2 = 3, and b6 = 6/
√
3 = 3.4641 . . . . The following
table gives anm for n1 = 6, m = 1 : 5, computed using IEEE double precision arithmetic.
m
nm
anm
bnm
1
6
3.00000000000000
3.00000000000000
2
12
3.10582854123025
3.21539030917347
3
24
3.13262861328124
3.15965994209750
4
48
3.13935020304687
3.14608621513143
5
96
3.14103195089051
3.14271459964537
From this we can deduce that 3.1410 < π < 3.1427, or the famous, slightly weaker rational
lower and upper bounds of Archimedes, 3 10
71 < π < 3 1
7.

308
Chapter 3. Series, Operators, and Continued Fractions
The sequences a(h) and b(h) satisfy the assumptions for repeated Richardson extrap-
olation with pk = 2k. Since the coefﬁcients in the Taylor expansion for a(h) decay faster
we use the Richardson scheme with this sequence, giving the results shown in the next table.
A correctly rounded value of π to 20 digits reads
π = 3.14159 26535 89793 23846,
and correct digits are shown in boldface.
3.14110472164033
3.14156197063157
3.14159245389765
3.14159073296874
3.14159265045789
3.14159265357789
3.14159253350506
3.14159265354081
3.14159265358975
3.14159265358979
The errors in successive columns decay as 4−2k, 4−3k, 4−4k, and the ﬁnal number is
correct to all 14 decimals shown. Hence the accuracy used in computing values in the
previous table, which could be thought excessive, has been put to good use!106
Example 3.4.14 (Application to Numerical Differentiation).
From Bickley’s table (Table 3.3.1) for difference operators in Sec. 3.3.2, we know that
δ
h = 2 sinh(hD/2)
h
= D + a2h2D3 + a4h4D5 + · · · ,
µ = cosh(hD/2) = 1 + b2h2D2 + b4h4D4 + · · · ,
where the values of the coefﬁcients are now unimportant to us. Hence
f ′(x) −f (x + h) −f (x −h)
2h
= Df (x) −µδf (x)
h
and
f ′′(x) −δ2f (x)
h2
have expansions into even powers of h. Repeated Richardson extrapolation can thus be
used with step sizes H, H/2, H/4, . . . and headings 4/3, 4/15, 4/63, . . . . For numerical
examples, see the problems for this section.
Richardson extrapolation can be applied in the same way to the computation of higher
derivatives. Because of the division by hk in the difference approximation of f (k), irregular
errors in the values of f (x) are of much greater importance in numerical differentiation than
in interpolation and integration. It is therefore important to use high-order approximations
in numerical differentiation, so that larger values of h can be used.
Suppose that the irregular errors of the values of f are bounded in magnitude by u.
These errors are propagated to µδf (x), δ2f (x), . . . with bounds equal to u/h, 4u/h2, . . . .
As mentioned earlier, the Richardson scheme (in the version used here) is benevolent; it
multiplies the latter bounds by a factor less than two.
106An extension of this example was used as a test problem for Mulprec, a package for (in principle) arbitrarily
high precision ﬂoating-point arithmetic in MATLAB. For instance, π was obtained to 203 decimal places with 22
polygons and 21 Richardson extrapolations in less than half a minute. The extrapolations took a small fraction of
this time. Nevertheless they increased the number of correct decimals from approximately 15 to 203.

Problems and Computer Exercises
309
Review Questions
3.4.1 (a) Aitken acceleration is based on ﬁtting three successive terms of a given sequence
{sn} to a certain comparison series. Which?
(b) Give sufﬁcient conditions for the accelerated sequence {s′
j} to converge faster than
{sn}.
(c) Aitken acceleration is sometimes applied to a thinned sequence. Why can this
give a higher accuracy in the computed limit?
3.4.2 (a) State the original version of Euler’s transformation for summation of an alternat-
ing series S = ∞
j=0(−1)juj, uj ≥0.
(b) State the modiﬁed Euler’s transformation for this case and discuss suitable ter-
mination criteria. What is the main advantage of the modiﬁed algorithm over the
classical version?
3.4.3 (a) What pieces of information appear in the Euler–Maclaurin formula? Give the
generating function for the coefﬁcients. What do you know about the remainder
term?
(b) Give at least three important uses of the Euler–Maclaurin formula.
3.4.4 The Bernoulli polynomial Bn(t) have a key role in the proof of the Euler–Maclaurin
formula. They are deﬁned by the symbolic relation
Bn(t) = (B + t)n.
How is this relation to be interpreted?
3.4.5 (a) Suppose that an expansion of F(h)
F(h) = a0 + a1hp1 + a2hp2 + a3hp3 + · · · ,
0 < p1 < p2 < p3 < · · · ,
is known to exist. Describe how F(0) = a0 can be computed by repeated Richardson
extrapolation from known values of F(h), h = H, H/q, H/q2, . . . for some q > 1.
(b) Discuss the choice of q in the procedure in (a). What is the most common case?
Give some applications of repeated Richardson extrapolation.
Problems and Computer Exercises
3.4.1 (a) Compute ∞
n=1
1
(n+1)3 to eight decimal places by using
∞

n=N
1
n(n + 1)(n + 2),
for a suitable N, as a comparison series. Estimate roughly how many terms you
would have to add without and with the comparison series.
Hint: You found the exact sum of this comparison series in Problem 3.3.3.

310
Chapter 3. Series, Operators, and Continued Fractions
(b) Compute the sum also by Euler–Maclaurin’s formula or one of its variants in
Problem 3.4.19.
3.4.2 Study, or write yourself, programs for some of the following methods:107
• iterated Aitken acceleration,
• modiﬁed iterated Aitken, according to (3.4.9) or an a-version,
• generalized Euler transformation,
• one of the central difference variants of Euler–Maclaurin’s formula, given in
Problem 3.4.19.
The programs are needed in two slightly different versions.
Version i: For studies of the convergence rate, for a series (sequence) where one
knowsasufﬁcientlyaccuratevalueexa ofthesum(thelimit). Theriskofdrowningin
ﬁgures becomes smaller if you make graphical output, for example, like Figure 3.4.1.
Version ii: For a run controlled by a tolerance, as in Algorithm 3.4, appropriately
modiﬁed for the various algorithms. Print also i and, if appropriate, jj. If exa is
known, it should be subtracted from the result, because it is of interest to compare
errest with the actual error.
Comment: If you do not know exa, ﬁnd a sufﬁciently good exa by a couple of
runs with very small tolerances, before you study the convergence rates (for larger
tolerances).
3.4.3 The formula for Aitken acceleration is sometimes given in the form
sn −(4sn)2
42sn
or
sn −
4sn∇sn
4sn −∇sn
.
Show that these are equivalent to s′
n+2 or s′
n+1, respectively, in the notations of
(3.4.2). Also note that the second formula is limp→∞s′
n (not s′
n+1) in the notation of
(3.4.7).
3.4.4 (a) Try iterated Aitken with thinning for ∞
1 e−√n, according to the suggestions
after Example 3.4.3.
(b) Study the effect of small random perturbations to the terms.
3.4.5 Oscillatory series of the form ∞
n=1 cnzn. Suggested examples:
cn = e−√n,
1/(1 + n2),
1/n, 1/(2n −1),
n/(n2 + n + 1),
1/√n,
1/ ln(n + 1),
where z = −1, −0.9, ei3π/4, i, eiπ/4, eiπ/16, for the appropriate algorithms mentioned
in Problem 3.4.2 above. Apply thinning. Also try classical Euler transformation on
some of the cases.
Study how the convergence ratio depends on z, and compare with theoretical results.
Compare the various methods with each other.
107We have MATLAB in mind, or some other language with complex arithmetic and graphical output.

Problems and Computer Exercises
311
3.4.6 Essentially positive series of the form ∞
n=1 cnzn, where
cn = e−√n,
1/(1 + n2),
1/(5 + 2n + n2),
(n · ln(n + 1))−2,
1/

n3 + n, n−4/3,
1/((n + 1)(ln(n + 1))2),
z = 1, 0.99, 0.9, 0.7, eiπ/16, eiπ/4, i. Use appropriate algorithms from Problem 3.4.2.
Try also Euler–Maclaurin’s summation formula, or one of its variants, if you can
handle the integral with good accuracy. Also try to ﬁnd a good comparison series;
it is not always possible.
Study the convergence rate. Try to apply thinning to the ﬁrst two methods.
3.4.7 Divergent series. Apply, if possible, Aitken acceleration and the generalized Euler
transformation to the following divergent series ∞
1 cnzn. Compare the numerical
results with the results obtained by analytic continuation using the analytic expres-
sion for the sum as a function of z.
(a) cn = 1, z = −1;
(b) cn = n, z = −1;
(c) cn is an arbitrary polynomial in n;
(d) cn = 1, z = i;
(e) cn = 1, z = 2;
(f) cn = 1, z = −2.
3.4.8 Let yn be the Fibonacci sequence deﬁned in Problem 3.3.18 by the recurrence relation
yn = yn−1 + yn−2,
y0 = 0,
y1 = 1.
Show that the sequence {yn+1/yn}∞
0
satisﬁes the sufﬁcient condition for Aitken
acceleration given in the text. Compute a few terms, compute the limit by Aitken
acceleration(s), and compare with the exact result.
3.4.9 When the current through a galvanometer changes suddenly, its indicator begins
to oscillate with an exponentially damped simple harmonic motion toward a new
stationary value s. The relation between the successive turning points v0, v1, v2, . . .
is vn −s ≈A · (−k)n, 0 < k < 1. Determine, from the following series of
measurements, Aitken extrapolated values v′
2, v′
3, v′
4 which are all approximations
to s:108
v0 = 659,
v1 = 236,
v2 = 463,
v3 = 340,
v4 = 406.
3.4.10 (a) Show that the a-version ofAitken acceleration can be iterated, for i = 0 : N −2,
a(i+1)
i+1
= 0,
a(i+1)
j
= a(i)
j −∇

(a(i)
j )2/∇a(i)
j

,
j = i + 2 : N,
s(i+1)
N
= s(i)
N −(a(i)
N )2/∇a(i)
N .
(Note that a(0)
j
= aj, s(0)
j
= sj.) We thus obtain N estimates of the sum s. We
cannot be sure that the last estimate s(N−1)
N
is the best, due to irregular errors in the
terms and during the computations. Therefore, accept the average of a few estimates
108James Clark Maxwell used Aitken acceleration for this purpose already in 1892 in his “Treatise on Electricity
and Magnetism.”

312
Chapter 3. Series, Operators, and Continued Fractions
that are close to each other, or do you have a better suggestion? This also gives you
a (not quite reliable) error estimate.
(b)Although we may expect that the a-version ofAitken acceleration handles round-
ing errors better than the s-version, the rounding errors may set a limit for the accu-
racy of the result. It is easy to combine thinning with this version. How?
(c) Study or write yourself a program for the a-version, and apply it to one or two
problems where you have used the s-version earlier. Also use thinning on a problem,
where it is needed. We have here considered N as given. Can you suggest a better
termination criterion, or a process for continuing the computation, if the accuracy
obtained is disappointing?
3.4.11 A function g(t) has the form
g(t) = c −kt +
∞

n=1
ane−λnt,
where c, k, an, and 0 < λ1 < λ2 < · · · < λn are unknown constants and g(t) is
known numerically for tν = νh, ν = 0, 1, 2, 3, 4.
Find out how to eliminate c in such a way that a sufﬁcient condition for estimating kh
by Aitken acceleration is satisﬁed. Apply this to the following data, where h = 0.1,
gν = g(tν):
g0 = 2.14789,
g1 = 1.82207,
g2 = 1.59763,
g3 = 1.40680,
g4 = 1.22784.
Then, estimate c.
3.4.12 Suppose that the sequence {sn} satisﬁes the condition sn −s = c0n−p + c1n−p−1 +
O(n−p−2), p > 0, n →∞, and set
s′
n = sn −p + 1
p
4sn∇sn
4sn −∇sn
.
It was stated without proof in Sec. 3.4.2 that s′
n −s = O(n−p−2).
(a) Design an a-version of this modiﬁed Aitken acceleration, or look it up in [33].
(b) Since the difference expressions are symmetrical about n one can conjecture
that this result would follow from a continuous analogue with derivatives instead
of differences. It has been shown [33] that this conjecture is true, but we shall not
prove that. Our (easier) problem is just the continuous analogue: suppose that a
function s(t) satisﬁes the condition s(t)−s = c0t−p +c1t−p−1 +O(t−p−2), p > 0,
t →∞, and set
y(t) = s(t) −p + 1
p
s′(t)2
s′′(t) .
Show that y(t) −s = O(t−p−2). Formulate and prove the continuous analogue to
(3.4.10).

Problems and Computer Exercises
313
3.4.13 (a) Consider, as in Example 3.4.5, the sum  n−3/2. Show that the partial sum sn
has an asymptotic expansion of the form needed in that example, with p = −1/2.
Hint: Apply Euler–Maclaurin’s formula (theoretically).
(b) Suppose that  an is convergent, and that an = a(n) where a(z) is an analytic
function at z = ∞(for example a rational function), multiplied by some power of
z −c. Show that such a function has an expansion such as (3.4.8), and that the same
holds for a product of such functions.
3.4.14 Compute and plot
F(x) =
∞

n=0
Tn(x)/(1 + n2),
x ∈[−1, 1].
Find out experimentally or theoretically how F ′(x) behaves near x = 1 and x = −1.
3.4.15 Compute to (say) six decimal places the double sum
S =
∞

m=1
∞

n=1
(−1)m+n
(m2 + n2) =
∞

n=1
(−1)mf (m),
where
f (m) =
∞

n=1
(−1)n(m2 + n2)−1.
Compute, to begin with, f (m) for m = 1 : 10 by the generalized Euler transforma-
tion. Do you need more values of f (m)?
Comment: There exists an explicit formula for f (m) in this case, but you can solve
this problem easily without using that.
3.4.16 We use the notation of Sec. 3.4.3 (the generalized Euler transformation). Assume
that N ≥k ≥1, and set n = N −k + 1. A sum is equal to zero if the upper index
is smaller than the lower index.
(a) Prove (3.4.21), which is given without proof in the text; i.e.,
MN,k−1 −MN−1,k−1 = znP k−2un+1
(k ≥2).
Hint: By subscript transformations in the deﬁnition of MN,k, prove that
MN,k−1 −MN−1,k−1 = un+1zn +
zn
1 −z
k−3

s=0
(zE −1)P sun+1.
Next, show that zE −1 = (1 −z)(P −1), and use this to simplify the expression.
(b) Derive the formulas
Mk−1,k =
1
1 −z
k−2

s=0
P su1,
MN,k = Mk−1,k +
n−1

j=0
zjP k−1uj+1.

314
Chapter 3. Series, Operators, and Continued Fractions
Comment: The ﬁrst formula gives the partial sums of the classical Euler transfor-
mation. The second formula relates the kth column to the partial sums of the power
series with the coefﬁcients P k−1uj+1.
3.4.17 (a) If uj = aj, z = eiφ, φ ∈[0, π], for which real values of a ∈[0, 1] does the
series on the right of (3.4.14) converge faster than the series on the left?
(b) Find how the classical Euler transformation works if applied to the series

zn,
|z| = 1,
z ̸= 1.
Compare how it works on  unzn, for un = an, z = z1, and for un = 1, z = az1.
Consider similar questions for other convergence acceleration methods, which are
primarily invented for oscillating sequences.
3.4.18 Compute ∞
k=1 k1/2/(k2 + 1) with an error of less than 10−6. Sum the ﬁrst ten
terms directly. Then expand the summand in negative powers of k and use Euler–
Maclaurin’s summation formula. Or try the central difference variant of Euler–
Maclaurin’s summation formula given in the next problem; then you do not have to
compute derivatives.
3.4.19 Variations on the Euler–Maclaurin Theme. Set xi = a + ih, also for noninteger
subscripts, and xn = b.
Two variants with central differences instead of derivatives are interesting alterna-
tives, if the derivatives needed in the Euler–Maclaurin formula are hard to compute.
Check a few of the coefﬁcients on the right-hand side of the formula
∞

j=1
B2j(hD)2j−1
(2j)!
≈µδ
12 −11µδ3
720
+ 191µδ5
60,480 −2497µδ7
3,628,800 + · · · .
(3.4.52)
Use the expansion for computing the sum given in the previous problem. This
formula is given by Fröberg [128, p. 220], who attributes it to Gauss.
Compare the size of its coefﬁcients with the corresponding coefﬁcients of the Euler–
Maclaurin formula.
Suppose that h = 1, and that the terms of the given series can be evaluated also for
noninteger arguments. Then another variant is to compute the central differences
for (say) h = 1/2 in order to approximate each derivative needed more accurately
by means of (3.3.48). This leads to the formula109
∞

j=1
B2jD2j−1
(2j)!
∼µδ
6 −7µδ3
180 + 71µδ5
7560 −521µδ7
226,800 + · · · .
(3.4.53)
(h = 1/2 for the central differences; h = 1 in the series.) Convince yourself of the
reliability of the formula, either by deriving it or by testing it for (say) f (x) = e0.1 x.
Show that the rounding errors of the function values cause almost no trouble in the
numerical evaluation of these difference corrections.
109The formula is probably very old, but we have not found it in the literature.

Problems and Computer Exercises
315
3.4.20 (a) Derive formally in a similar way the following formula for an alternating series.
Set xi, h = 1, b = ∞, and assume that limx→∞f (x) = 0.
∞

i=0
(−1)if (a + i) = 1
2f (a) −1
4f ′(a) + 1
48f ′′′(a) −· · ·
−(22r −1)B2r
(2r)!
f (2r−1)(a) −· · · .
(3.4.54)
Of course, the integral of f is not needed in this case.110 Compare it with some of
the other methods for alternating series on an example of your own choice.
(b) Derive by using operators (without the remainder R) the following more general
form of the Euler–Maclaurin formula (Handbook [1, Sec. 23.1.32]):
m−1

k=0
hf (a + kh + ωh) =
 b
a
f (t)d t +
p

j=1
hj
j! Bj(ω)(f (j−1)(b) −f (j−1)(a))
−hp
p!
 1
0
ˆBp(ω −t)
m−1

k=0
f (p)(a + kh + th) dt.
If you use this formula for deriving the midpoint variant in (a) you will ﬁnd a quite
different expression for the coefﬁcients; nevertheless, it is the same formula. See
how this is explained in the Handbook [1, Sec. 23.1.10], that is by the “multiplication
theorem.”111
Bn(mx) = mn−1
m−1

k=0
Bn(x + k/m),
n = 0, 1, 2, . . . ,
m = 1, 2, 3, . . . .
3.4.21 Prove statement (b) of Lemma 3.4.9 (concerning the periodicity and the regularity
of the Bernoulli functions).
3.4.22 Euler’s constant is deﬁned by γ = limN→∞F(N), where
F(N) = 1 + 1
2 + 1
3 + · · · +
1
N −1 + 1
N −ln N.
(a) Use the Euler–Maclaurin formula with f (x) = x−1, h = 1, to show that, for any
integer N,
γ = F(N) + 1
12N−2 −
6
720N−4 +
120
30,240N−6 −· · · ,
where every other partial sum is larger than γ , and every other is smaller.
110Note that the right-hand side yields a ﬁnite value if f is a constant or, more generally, if f is a polynomial,
although the series on the left-hand side diverges. The same happens to other summation methods.
111That formula and the remainder R are derived on p. 21 and p. 30, respectively, in Nörlund [276].

316
Chapter 3. Series, Operators, and Continued Fractions
(b) Compute γ to seven decimal places, using N = 10, 10
n=1 n−1 = 2.92896825,
ln 10 = 2.30258509.
(c) Show how repeated Richardson extrapolation can be used to compute γ from
the following values.
N
1
2
4
8
F(N)
0.5
0.55685
0.57204
0.57592
(d) Extend (c) to a computation where a larger number of values of F(N) have been
computed as accurately as possible, and so that the ﬁnal accuracy of γ is limited by
the effects of rounding errors. Check the result by looking it up in an accurate table
of mathematical constants, for example, in the Handbook [1].
3.4.23 A digression about the gamma function.
(a) The Handbook [1, Sec. 6.1.40] gives an expansion for ln T(z) that agrees with
formula (3.4.41) for ln z! (if we substitute z for m), except that the Handbook writes
(z −1
2) ln z, where we have (m + 1
2) ln m. Explain concisely and completely that
there is no contradiction here.
(b) An asymptotic expansion for computing ln T(z + 1), z ∈C, is derived in Exam-
ple 3.4.12. If r terms are used in the asymptotic expansion, the remainder reads
R(z) = K(z)
(2r)!
π|2πz|2r+1 ,
K(z) = sup
u≥0
|z2|
|u2 + z2|
(see also Remark 3.4.6). Set z = x + iy. Show the following more useful bound
for K(z), valid for x > 0,
K(z) ≤
% 1
if x ≥|y|,
1
2(x/|y| + |y|/x)
otherwise.
Find a uniform upper bound for the remainder if r = 5, x ≥1
2, |z| ≥17.
(c) Write a MATLAB program for the computation of ln T(z+1). Use the reﬂection
and recurrence formulas to transform the input value z to another z = x + iy that
satisﬁes x ≥half , |z| ≥17, for which this asymptotic expansion is to be used with
r = 5.
Test the program by computing the following quantities, and compare with their
exact values:
n!,
T

n + 1
2

/√π,
n = 0, 1, 2, 3, 10, 20;
((((T
1
2 + iy
((((
2
=
π
cosh(πy),
y = ±10, ±20.
If the original input value has a small modulus, there is some cancellation when the
output from the asymptotic expansion is transformed to ln(1 + zinput), resulting in
a loss of (say) one or two decimal digits.

Problems and Computer Exercises
317
Comment: It is often much better to work with ln T(z) than with T(z). For example,
onecanavoidexponentoverﬂowinthecalculationofabinomialcoefﬁcientoravalue
of the beta function, B(z, w) = T(z)T(w)/T(z + w), where (say) the denominator
can become too big, even if the ﬁnal result is of a normal order of magnitude.
3.4.24 (a) Show that
2n
n

∼
22n
√πn,
n →∞,
and give an asymptotic estimate of the relative error of this approximation. Check
the approximation as well as the error estimate for n = 5 and n = 10.
(b) Random errors in a difference scheme. We know from Example 3.3.3 that if the
items yj of a difference scheme are afﬂicted with errors less than ϵ in absolute value,
then the inherited error of 4nyj is at most 2nϵ in absolute value. If we consider
the errors as independent random variables, uniformly distributed in the interval
[−ϵ, ϵ], show that the error of 4nyj has the variance
2n
n
 1
3ϵ2, hence the standard
deviation is approximately
2nϵ(9πn)−1/4,
n ≫1.
Check the result on a particular case using a Monte Carlo study.
Hint: It is known from probability theory that the variance of n
j=0 ajϵj is equal
to σ 2 n
j=0 a2
j, and that a random variable, uniformly distributed in the interval
[−ϵ, ϵ], has the variance σ 2 = ϵ2/3. Finally, use (3.1.23) with p = q = n.
3.4.25 The following table of values of a function f (x) is given.
x
0.6
0.8
0.9
1.0
1.1
1.2
1.4
f (x) 1.820365 1.501258 1.327313 1.143957 0.951849 0.752084 0.335920
Compute f ′(1.0) and f ′′(1.0) using repeated Richardson extrapolation.
3.4.26 Compute an approximation to π using Richardson extrapolation with Neville’s algo-
rithm, based on three simple polygons, with n = 2, 3, and 6 sides, not in geometric
progression. A 2-sided polygon can be interpreted as a diameter described up and
down. Its “ perimeter” is thus equal to four. Show that this gives even a little
better value than the result (3.14103) obtained for the 96-sided polygon without
extrapolations.
3.4.27 Numerov’s method with Richardson extrapolations.112
(a) Show that the formula
h−2(yn+1 −2yn + yn−1) = y′′
n + a(y′′
n+1 −2y′′
n + y′′
n−1)
is exact for polynomials of as high degree as possible, if a = 1/12. Show that
the error has an expansion into even powers of h, and determine the ﬁrst (typically
non-vanishing) term of this expansion.
112See also Example 3.3.15.

318
Chapter 3. Series, Operators, and Continued Fractions
(b) This formula can be applied to the differential equation y′′ = p(x)y with given
initial values y(0), y′(0). Show that this yields the recurrence relation
yn+1 = (2 + 10
12pnh2)yn −(1 −1
12pn−1h2)yn−1
1 −1
12pn+1h2
.
Comment: If h is small, information about p(t) is lost by outshifting in the factors
1 −
1
12pn−1h2. It is possible to rewrite the formulas in order to reduce the loss
of information, but in the application below this causes no trouble in IEEE double
precision.
(c) You proved in (a) that the local error has an expansion containing even powers
of h only. It can be shown that the same is true for the global error too. Assume
(without proof) that
y(x, h) = y(x) + c1(x)h4 + c2(x)h6 + c3(x)h8 + O(h10).
Apply this method, together with two Richardson extrapolations in (d), to the prob-
lem of computing the solution to the differential equation y′′ = −xy with initial
values y(0) = 1, y′(0) = 0, this time over the interval 0 ≤x ≤4.8. Denote the
numerical solution by y(x; h), i.e., yn = y(xn; h).
Compute the seeds y1 = y(h, h) by the Taylor expansion given in (1.2.8). The error
of y(0.2, 0, 2) should be less than 10−10, since we expect that the (global) errors
after two Richardson extrapolations can be of that order of magnitude.
Compute y(x; h), x = 0 : h : 4.8, for h = 0.05, h = 0.1, h = 0.2. Store these data
in a 100 × 3 matrix (where you must put zeros into some places). Plot y(x; 0.05)
versus x for x = 0 : 0.05 : 4.8.
(d) Express with the aid of the Handbook [1, Sec. 10.4] the solution of this initial
value problem in terms of Airy functions:113
y(x) = Ai(−x) + Bi(−x)/
√
3
2 · 0.3550280539
.
Check a few of your results of the repeated Richardson extrapolation by means of
[1, Table 10.11] that, unfortunately, gives only eight decimal places.
3.4.28 (a) Determine the Bernoulli polynomials B2(x) and B3(x), and ﬁnd the values and
the derivatives at zero and one. Factorize the polynomial B3(x). Draw the graphs
of a few periods of ˆBi(x), i = 1, 2, 3.
(b) In an old textbook, we found a “symbolic” formula, essentially
h
n−1

j=0
g′(a + jh) = g(b + hB) −g(a + hB).
(3.4.55)
The expansion of the right-hand side into powers of hB has been followed by the
replacement of the powers of B by Bernoulli numbers; the resulting expansion is
113Airy functions are special functions (related to Bessel functions) with many applications to mathematical
physics, for example, the theory of diffraction of radio waves along the Earth’s surface.

Problems and Computer Exercises
319
not necessarily convergent, even if the ﬁrst power series converges for any complex
value of hB.
Show that the second expansion is equivalent to the Euler–Maclaurin formula, and
that it is to be interpreted according to Theorem 3.4.10.
(c) If g is a polynomial, the expansion is ﬁnite. Show the following important
formulas, and check them with known results for k = 1 : 3.
n−1

j=0
j k−1 = (B + n)k −Bk
k
= Bk(n) −Bk
k
.
(3.4.56)
Also ﬁnd that (3.4.55) makes sense for g(x) = eαx, with the “symbolic” interpreta-
tion of the power series for eBx, if you accept the formula e(B+α)x = eBxeαx.
3.4.29 We have called  an a bell sum if an as a function of n has a bell-shaped graph,
and you must add many terms to get the desired accuracy. Under certain conditions
you can get an accurate result by adding (say) every tenth term and multiplying this
sum by ten, because both sums can be interpreted as trapezoidal approximations to
the same integral, with different step size. Inspired by Euler–Maclaurin’s formula,
we may hope to be able to obtain high accuracy using an integer step size h, i.e.,
(say) one quarter of the half-width of “the bell.” In other words, we do not have to
compute and add more than every hth term. We shall study a class of series
S(t) =
∞

n=0
cntn/n!,
t ≫1,
(3.4.57)
where cn > 0, log cn is rather slowly varying for n large; (say that) 4p log cn =
O(n−p). Let c(·) be a smooth function such that c(n) = cn. We consider S(t) as an
approximation to the integral
 ∞
0
c(n)tn/T(n + 1)dn,
with a smooth and bell-shaped integrand, almost like the normal frequency function,
with standard deviation σ ≈k√t.
(a) For p = 1 : 5, t = 4p, plot y =
√
2πte−ttn/n! versus x = n/t, 0 ≤x ≤3;
include all ﬁve curves on the same picture.
(b) For p = 1 : 5, t = 4p, plot y = ln(e−ttn/n!) versus x = (n −t)/√t,
max(0, t −8√t) ≤n ≤t + 8√t; include all ﬁve curves on the same picture. Give
bounds for the error committed if you neglect the terms of the series e−t ∞
0 tn/n!,
which are cut out in your picture.
(c) With the same notation as in (b), use Stirling’s asymptotic expansion to show
theoretically that, for t →∞,
e−ttn
n!
= e−x2/2
1 + O(1/√t)

√
2πt
,
(3.4.58)
where the O(1/√t)-term depends on x. Compare this with the plots.

320
Chapter 3. Series, Operators, and Continued Fractions
(d) Test these ideas by making numerical experiments with the series
e−t 
n∈N
tn/n!,
N = {round(t −8
√
t) : h : round(t + 8
√
t)},
for some integers h in the neighborhood of suitable fractions of √t, inspired by the
outcome of the experiments. Do this for t =1000, 500, 200, 100, 50, 30. Compare
with the exact result, see how the trapezoidal error depends on h, and try to formulate
an error estimate that can be reasonably reliable, in cases where the answer is not
known. How large must t be, in order that it should be permissible to choose h > 1
if you want (say) six correct decimals?
(e) Compute, with an error estimate, e−t ∞
n=1 tn/(n · n!), with six correct decimals
for the values of t mentioned in (d). You can also check your result with tables and
formulas in the Handbook [1, Chap. 5].
3.4.30 If you have a good program for generating primes, denote the nth prime by pn and
try convergence acceleration to series such as

(−1)n/pn,

1/p2
n.
Due to the irregularity of the sequence of primes, you cannot expect the spectacular
accuracy of the previous examples. It can be fun to see how these methods work
in combination with some comparison series derived from asymptotic results about
primes. The simplest one reads pn ∼n ln n, (n →∞), which is equivalent to the
classical prime number theorem.
3.4.31 A summation formula based on the Euler numbers. The Euler numbers En were
introduced by (3.1.22). The ﬁrst values read
E0 = 1, E2 = −1, E4 = 5, E6 = −61.
They are all integers (Problem 3.1.7(c)). En = 0 for odd n, and the sign is alternating
for even n. Their generating function reads
1
cosh z =
∞

j=0
Ejzj
j! .
(a) Show by means of operators the following expansion:
∞

k=m
(−1)k−mf (k) ≈
q

p=0
E2pf (2p)(m −1
2)
22p+1(2p)!
.
(3.4.59)
No discussion of convergence is needed; the expansion behaves much like the Euler–
Maclaurin expansion, and so does the error estimation; see [87].
The coefﬁcient of f (2p)(m −1
2) is approximately 2(−1)p/π2p+1 when p ≫1; e.g.,
for p = 3 the approximation yields −6.622 · 10−4, while the exact coefﬁcient is
61/92,160 ≈6.619 · 10−4.

3.5. Continued Fractions and Padé Approximants
321
(b)Apply(3.4.59)toexplainthefollowingcuriousobservation, reportedbyBorwein,
Borwein, and Dilcher [43].
50

k=1
4(−1)k
2k −1 = 3.12159465259 . . . ,
(π = 3.14159265359 . . .).
Note that only three digits disagree. There are several variations on this theme.
Reference [43] actually displays the case with 40 decimal places based on 50,000
terms. Make an educated guess concerning how few digits disagreed.
3.4.32 What is β(t) (in the notation of (3.4.25)), if un = an, 0 < a < 1?
3.4.33 Work out the details of the two optimizations in the proof of Theorem 3.4.7.
3.4.34 (a) Show that every rational function f (s) that is analytic and bounded for ℜs ≥a
is d.c.m. for s ≥a.
(b) Show criterion (B) for higher monotonicity (concerning products).
(c) Which of the coefﬁcient sequences {cn} mentioned in Problems 3.4.5 and 3.4.6
are c.m.? Which are d.c.m.?
(d) Show criterion (E) for higher monotonicity.
3.4.35 Suppose that un =
 1
0 tn dβ(t), where β(t) is of bounded variation in [0, 1]. Show
that lim un = 0 if β(t) is continuous at t = 1, but that it is not true if β(t) has a
jump at t = 1.
3.5
Continued Fractions and Padé Approximants
3.5.1
Algebraic Continued Fractions
Somefunctionscannotbewellapproximatedbyapowerseries, butcanbewellapproximated
by a quotient of power series. In order to study such approximations we ﬁrst introduce
continued fractions, i.e., expressions of the form
r = b0 +
a1
b1 +
a2
b2 +
a3
b3 + . . .
= b0 + a1
b1+
a2
b2+
a3
b3+ · · · .
(3.5.1)
The second expression is a convenient compact notation. If the number of terms is inﬁnite,
r is called an inﬁnite continued fraction.
Continued fractions were applied in the seventeenth century to the rational approxi-
mation of various algebraic numbers. In such algebraic continued fractions r and the entries
ai, bi are numbers. Beginning with work by Euler, analytic continued fraction expansions
r(z) = b0 + a1z
b1+
a2z
b2+
a3z
b3+ . . .
(3.5.2)
involving functions of a complex variable r(z) became an important tool in the approxima-
tion of special classes of analytic functions of a complex variable.

322
Chapter 3. Series, Operators, and Continued Fractions
We ﬁrst study some algebraic properties of continued fractions. The partial fraction
rn = pn
qn
= b0 + a1
b1+
a2
b2+ · · · an
bn
(3.5.3)
iscalledthenthapproximantofthecontinuedfraction. Thereareseveralessentiallydifferent
algorithms for evaluating a partial fraction. It can be evaluated backward in n divisions
using the recurrence
yn = bn,
yi−1 = bi−1 + ai/yi,
i = n : −1 : 1,
(3.5.4)
for which r = y0. It can happen that in an intermediate step the denominator yi becomes
zero and yi−1 = ∞. This does no harm if in the next step when you divide by yi−1 the result
is set equal to zero. If it happens in the last step, the result is ∞.114
A drawback of evaluating an inﬁnite continued fraction expansion by the backward
recursion (3.5.4) is that you have to decide where to stop in advance. The following theorem
shows how forward (or top down) evaluation can be achieved.
Theorem 3.5.1.
For the nth convergent rn = pn/qn of the continued fraction (3.5.1), pn and qn,
n ≥1, satisfy the recursion formulas
pn = bnpn−1 + anpn−2,
p−1 = 1,
p0 = b0,
(3.5.5)
qn = bnqn−1 + anqn−2,
q−1 = 0,
q0 = 1.
(3.5.6)
Another useful formula reads
pnqn−1 −pn−1qn = (−1)n−1a1a2 · · · an.
(3.5.7)
If we substitute anx for an in (3.5.5)–(3.5.6), then pn(x) and qn(x) become polynomials in
x of degree n and n −1, respectively.
Proof. We prove the recursion formulas by induction. First, for n = 1, we obtain
p1
q1
= b1p0 + a1p−1
b1q0 + a1q−1
= b1b0 + a1
b1 + 0
= b0 + a1
b1
= r1.
Next, assume that the formulas are valid up to pn−1, qn−1 for every continued fraction. Note
that pn/qn can be obtained from pn−1/qn−1 by the substitution of bn−1 + an/bn for bn−1.
Hence
pn
qn
= (bn−1 + an/bn)pn−2 + an−1pn−3
(bn−1 + an/bn)qn−2 + an−1qn−3
= bn(bn−1pn−2 + an−1pn−3) + anpn−2
bn(bn−1qn−2 + an−1qn−3) + anqn−2
= bnpn−1 + anpn−2
bnqn−1 + anqn−2
.
This shows that the formulas are valid also for pn, qn. The proof of (3.5.7) is left for Problem
3.5.2.
114Note that this works automatically in IEEE arithmetic, because of the rules of inﬁnite arithmetic; see Sec. 2.2.3.

3.5. Continued Fractions and Padé Approximants
323
Theevaluationofacontinuedfractionbyforwardrecursionrequires4nmultiplications
and one division. It is sometimes convenient to write the recursion formulas in matrix form;
see Problem 3.5.2. One must also be careful about the numerical stability of these recurrence
relations.
In practice the forward recursion for evaluating a continued fraction often generates
very large or very small values for the numerators and denominators. There is a risk of
overﬂow or underﬂow with these formulas. Since we are usually not interested in the pn, qn
themselves, but in the ratios only, we can normalize pn and qn by multiplying them by the
same factor after they have been computed. If we shall go on and compute pn+1, qn+1,
however, we have to multiply pn−1, qn−1 by the same factor also. The formula
a1
b1+
a2
b2+
a3
b3+ · · · = k1a1
k1b1+
k1k2a2
k2b2+
k2k3a3
k3b3+ · · · ,
(3.5.8)
where the ki are any nonzero numbers, is known as an equivalence transformation. The
proof of (3.5.8) is left for Problem 3.5.6.
Suppose we are given a rational function R(z) = R0(z)/R1(z), where R0(z) and
R1(z) are polynomials. Then by the following division algorithm R(z) can be expressed
as a continued fraction that can be evaluated by backward recursion in fewer arithmetic
operations; see Cheney [66, p. 151]. The degree of a polynomial Rj(z) is denoted by dj.
By successive divisions (of Rj−1(z) by Rj(z)) we obtain quotients Qjf (z) and remainders
Rj+1(z) as follows.
For j = 1, 2, . . . , until dj+1 = 0, set
Rj−1(z) = Rj(z)Qj(z) + Rj+1(z)
(dj+1 < dj).
(3.5.9)
Then
R(z) = R0(z)
R1(z) = Q1(z) +
1
R1(z)/R2(z) = · · ·
(3.5.10)
= Q1(z) +
1
Q2(z)+
1
Q3(z)+ · · ·
1
Qk(z).
(3.5.11)
By means of an equivalence transformation (see (3.5.8)), this fraction can be transformed
into a slightly more economic form, where the polynomials in the denominators have leading
coefﬁcient unity, while the numerators are in general different from 1.
Example 3.5.1.
In the rational form
r(z) = 7z4 −101z3 + 540z2 −1204z + 958
z4 −14z3 + 72z2 −151z + 112
,
the numerator and denominator can be evaluated by Horner’s rule. Alternatively, the above
algorithm can be used to convert the rational form to the ﬁnite continued fraction
r(z) = 7 −
3
z −2−
1
z −7+
10
z −2−
2
z −3.

324
Chapter 3. Series, Operators, and Continued Fractions
To evaluate this by backward recursion requires fewer operations than the rational form, but
a division by zero occurs at the four points z = 1, 2, 3, 4. In IEEE arithmetic the continued
fraction evaluates correctly also at these points because of the rules of inﬁnite arithmetic!
Indeed, the continued fraction form can be shown to have smaller errors for z ∈[0, 4] and
to be immune to overﬂow; see Higham [199, Sec. 27.1].
Every positive number x can be expanded into a regular continued fraction with integer
coefﬁcients of the form
x = b0 +
1
b1+
1
b2+
1
b3+ · · · .
(3.5.12)
Set x0 = x, p−1 = 1, q−1 = 0. For n = 0, 1, 2, . . . we construct a sequence of numbers,
xn = bn +
1
bn+1+
1
bn+2+
1
bn+3+ · · · .
Evidently bn = ⌊xn⌋, the integer part of xn, and xn+1 = 1/(xn −bn). Compute pn, qn,
according to the recursion formulas of Theorem 3.5.1, which can be written in vector form,
(pn, qn) = (pn−2, qn−2) + bn(pn−1, qn−1)
(since an = 1). Stop when |x −pn/qn| < tol or n > nmax. If the number x is rational this
expansion is ﬁnite. The details are left for Problem 3.5.1. Note that the algorithm is related
to the Euclidean algorithm; see Problem 1.2.6.
The above algorithm has been used several times in the previous sections, where
some coefﬁcients, known to be rational, have been computed in ﬂoating-point. It is also
useful for ﬁnding near commensurabilities between events with different periods;115 see
Problem 3.5.1 (c).
The German mathematician Felix Klein [228]116 gave the following illuminating
description of the sequence {(pn, qn)} obtained by this algorithm (adapted to our notation):
Imagine pegs or needles afﬁxed at all the integral points (pn, qn), and wrap a
tightly drawn string about the sets of pegs to the right and to the left of the ray,
p = xq. Then the vertices of the two convex string-polygons which bound our
two point sets will be precisely the points (pn, qn) . . . , the left polygon having
the even convergents, the right one the odd.
Klein also points out that “such a ray makes a cut in the set of integral points” and thus
makes Dedekind’s deﬁnition of irrational numbers very concrete. This construction, shown
in Figure 3.5.1, illustrates in a concrete way that the successive convergents are closer to x
than any numbers with smaller denominators, and that the errors alternate in sign. We omit
the details of the proof that this description is correct.
Note that, since aj = 1 for all j, (3.5.7) reads
pnqn−1 −pn−1qn = (−1)n−1.
115One of the convergents for log 2/ log 3 reads 12/19. This is, in a way, basic for Western music, where 13
quints make 7 octaves, i.e., (3/2)12 ≈27.
116Felix Christian Klein (1849–1925), a German mathematician, was born 4/25. He delighted in pointing out
that the day (52), month (22), and year (432) of his birth was the square of a prime number.

3.5. Continued Fractions and Padé Approximants
325
0
2
4
6
8
10
0
1
2
3
4
5
6
p
q
(p0,q0)
(p2,q2)
(p4,q4)
(p−1,q−1)
(p1,q1)
(p3,q3)
Figure 3.5.1. Best rational approximations {(p, q)} to the “golden ratio.”
This implies that the triangle with vertices at the points (0, 0), (qn, pn), (qn−1, pn−1) has
the smallest possible area among triangles with integer coordinates, and hence there can be
no integer points inside or on the sides of this triangle.
Comment: If we know or guess that a result x of a computation is a rational number
with a reasonably sized denominator, although it was practical to compute it in ﬂoating-point
arithmetic (afﬂicted by errors of various types), we have a good chance of reconstructing
the exact result by applying the above algorithm as a postprocessing.
If we just know that the exact x is rational, without any bounds for the number of
digits in the denominator and numerator, we must be conservative in claiming that the last
fraction that came out of the above algorithm is the exact value of x, even if |x −pn/qn|
is very small. In fact, the fraction may depend on tol that is to be chosen with respect to
the expected order of magnitude of the error of x. If tol has been chosen smaller than the
error of x, it may happen that the last fraction obtained at the termination is wrong, while
the correct fraction (with smaller numerator and denominator) may have appeared earlier
in the sequence (or it may not be there at all).
So a certain judgment is needed in the application of this algorithm. The smaller the
denominator and numerator are, the more likely it is that the fraction is correct. In a serious
context, it is advisable to check the result(s) by using exact arithmetic. If x is the root of
an equation (or a component of the solution of a system of equations), it is typically much
easier to check afterward that a suggested result is correct than to perform the whole solution
process in exact arithmetic.
The following theorem due to Seidel117 gives a necessary and sufﬁcient condition for
convergence of a continued fraction of the form (3.5.12).
117Philipp Ludwig von Seidel (1821–1896), German mathematician and astronomer. In 1846 he submitted
his habilitation dissertation entitled “Untersuchungen über die Konvergenz and Divergenz der Kettenbrüche”
(Investigations of the Convergence and Divergence of Continued Fractions).

326
Chapter 3. Series, Operators, and Continued Fractions
Theorem 3.5.2.
Let all bn be positive in the continued fraction
b0 +
1
b1+
1
b2+
1
b3+ · · · .
Then this converges if and only if the series  bn diverges.
Proof. See Cheney [66, p. 184].
Example 3.5.2.
The following are continued fraction expansions of some important irrational num-
bers:
π = 3 + 1
7+
1
15+
1
1+
1
292+
1
1+
1
1+
1
1+ · · · ,
e = 2 + 1
1+
1
2+
1
1+
1
1+
1
4+
1
1+
1
1+
1
6+ · · · .
For e there is a regular pattern in the expansion, but for π a general formula for the expansion
is not known. The partial fractions for π converge rapidly. For example, the error in the
third convergent π ≈355/113 is 0.266 · 10−6.
Figure 3.5.1 corresponds to the expansion
√
5 + 1
2
= 1 + 1
1+
1
1+
1
1+
1
1+
1
1+
1
1+ · · · .
(3.5.13)
Then, note that x = 1 + 1/x, x > 0, hence x = (
√
5 + 1)/2, which is the “golden section
ratio” (see also Problem 3.5.3). Note also that, by (3.5.7) with aj = 1,
((((x −pn
qn
(((( ≤
((((
pn+1
qn+1
−pn
qn
(((( = |pn+1qn −pnqn+1|
qn+1qn
=
1
qn+1qn
< 1
q2n
.
(3.5.14)
3.5.2
Analytic Continued Fractions
Continued fractions have also important applications in analysis. Alarge number of analytic
functions are known to have continued fraction representations. Indeed, some of the best
algorithms for the numerical computation of important analytic functions are based on
continued fractions. We shall not give complete proofs but refer to classical books of
Perron [289], Wall [369], and Henrici [196, 197].
Acontinued fraction is said to be equivalent to a given series if and only if the sequence
of convergents is equal to the sequence of partial sums. There is typically an inﬁnite number
of such equivalent fractions. The construction of the continued fraction is particularly simple
if we require that the denominators qn = 1 for all n ≥1. For a power series we shall thus
have
pn = c0 + c1z + c2z2 + · · · + cnzn,
n ≥1.
We must assume that cj ̸= 0 for all j ≥1.

3.5. Continued Fractions and Padé Approximants
327
We shall determine the elements an, bn by means of the recursion formulas of Theo-
rem 3.5.1 (for n ≥2) with initial conditions. We thus obtain the following equations:
pn = bnpn−1 + anpn−2,
p0 = b0,
p1 = b0b1 + a1,
1 = bn + an,
b1 = 1.
The solution reads b0 = p0 = c0, b1 = 1, a1 = p1 −p0 = c1z, and for n ≥2,
an = (pn −pn−1)/(pn−2 −pn−1) = −zcn/cn−1,
bn = 1 −an = 1 + zcn/cn−1,
c0 + c1z + · · · + cnzn · · · = c0 + zc1
1−
zc2/c1
1 + zc2/c1−. . .
zcn/cn−1
1 + zcn/cn−1−. . . .
Of course, an equivalent continued fraction gives by itself no convergence accelera-
tion, just because it is equivalent. We shall therefore leave the subject of continued fractions
equivalent to a series, after showing two instances of the numerous pretty formulas that can
be obtained by this construction. For
f (z) = ez = 1 + z + z2
2! + z3
3! + · · ·
and
f (z) = arctan √z
√z
= 1 −z
3 + z2
5 −z3
7 + · · · ,
we obtain for z = −1 and z = 1, respectively, after simple equivalence transformations,
e−1 = 1 −1
1+
1
1 + y =
1
2 + y ⇒e = 2 + 2
2+
3
3+
4
4+
5
5+ . . . ,
π
4 = 1
1+
1
2+
9
2+
25
2+
49
2+ . . . .
There exist, however, other methods to make a correspondence between a power series
and a continued fraction. Some of them lead to a considerable convergence acceleration that
often makes continued fractions very efﬁcient for the numerical computation of functions.
We shall return to such methods in Sec. 3.5.3.
Gauss developed a continued fraction for the ratio of two hypergeometric functions
(see (3.1.16)),
F(a, b + 1, c + 1; z)
F(a, b, c; z)
= 1
1+
a1z
1+
a2z
1+
a3z
1+ . . . ,
(3.5.15)
where
a2n+1 =
(a + n)(c −b + n)
(c + 2n)(c + 2n + 1),
a2n =
(b + n)(c −a + n)
(c + 2n −1)(c + 2n).
(3.5.16)
Although the power series converge only in the disk |z| < 1, the continued fraction of Gauss
converges throughout the complex z-plane cut along the real axis from 1 to +∞. It provides
an analytic continuation in the cut plane.

328
Chapter 3. Series, Operators, and Continued Fractions
If we set b = 0 in (3.5.15), we obtain a continued fraction for F(a, 1, c +1; z). From
this, many continued fractions for elementary functions can be derived, for example,
arctan z =
z
1+
z2
3+
22z2
5+
32z2
7+
42z2
9+ · · · ,
(3.5.17)
tan z =
z
1−
z2
3−
z2
5−
z2
7−· · · .
(3.5.18)
The expansion for tan z is valid everywhere, except in the poles. For arctan z the continued
fraction represents a single-valued branch of the analytic function in a plane with cuts along
the imaginary axis extending from +i to +i∞and from −i to −i∞. A continued fraction
expansion for arctanhz is obtained by using the relation arctanhz = −i arctan iz. In all
these cases the region of convergence as well as the speed of convergence is considerably
larger than for the power series expansions. For example, the sixth convergent for tan π/4
is almost correct to 11 decimal places.
For the natural logarithm we have
log(1 + z) =
z
1+
z
2+
z
3+
22z
4+
22z
5+
32z
6+ · · · ,
(3.5.19)
1
2 log
1 + z
1 −z

= z + z3
3 + z5
5 + z7
7 + · · ·
(3.5.20)
=
z
1−
z2
3−
22z2
5−
32z2
7−
42z2
9−· · · .
(3.5.21)
The fraction for the logarithm can be used in the whole complex plane except for the cuts
(−∞, −1] and [1, ∞). The convergence is slow when z is near a cut. For elementary
functions such as these, properties of the functions can be used for moving z to a domain
where the continued fraction converges rapidly.
Example 3.5.3.
Consider the continued fraction for ln(1 + z) and set z = 1. The successive approxi-
mations to ln 2 = 0.69314 71806 are the following.
1/1
2/3
7/10
36/52
208/300
1572/2268
12,876/18,576
1.000000
0.66667
0.700000
0.692308
0.69333
0.693122
0.693152
Note that the fractions give alternatively upper and lower bounds for ln 2. It can be shown
that this is the case when the elements of the continued fraction are positive. To get the
accuracy of the last approximation above would require as many as 50,000 terms of the
series ln 2 = ln(1 + 1) = 1 −1/2 + 1/3 −1/4 + · · ·.
Continued fraction expansions for the gamma function and the incomplete gamma
function are found in the Handbook [1, Sec. 6.5]. For the sake of simplicity we assume that
x > 0, although the formulas can be used also in an appropriately cut complex plane. The

3.5. Continued Fractions and Padé Approximants
329
parameter a may be complex in T(a, x).118
T(a, x) =
 ∞
x
ta−1e−t dt,
T(a, 0) = T(a),
γ (a, x) = T(a) −T(a, x) =
 x
0
ta−1e−t dt,
ℜa > 0,
T(a, x) = e−xxa 1
x+
1 −a
1+
1
x+
2 −a
1+
2
x+ · · ·

,
(3.5.22)
γ (a, x) = e−xxaT(a)
∞

n=0
xn
T(a + 1 + n).
We mention these functions because they have many applications. Several other
important functions can, by simple transformations, be brought to particular cases of this
function, for example, the normal probability function, the chi-square probability function,
the exponential integral, and the Poisson distribution.
The convergence behavior of continued fraction expansions is much more complicated
than for power series. Gautschi [142] exhibits a phenomenon of apparent convergence to
the wrong limit for a continued fraction of Perron for ratios of Kummer functions. The
sequence of terms initially decreases rapidly, then increases, and ﬁnally again decreases to
zero at a supergeometric rate.
Continued fractions such as these can often be derived by a theorem of Stieltjes which
relates continued fractions to orthogonal polynomials that satisfy a recurrence relation of the
same type as the one given above. Another method of derivation is the Padé approximation,
studied in the next section, that yields a rational function. Both techniques can be looked
upon as a convergence acceleration of an expansion into powers of z or z−1.
3.5.3
The Padé Table
Toward the end of the nineteenth century Frobenius and Padé developed a more general
scheme for expanding a formal power series into rational functions, which we now describe.
Let f (z) be a formal power series
f (z) = c0 + c1z + c2z2 + · · · =
∞

i=0
cizi.
(3.5.23)
Consider a complex rational form with numerator of degree at most m and denominator
of degree at most n such that its power series expansion agrees with that of f (z) as far as
possible. Such a rational form is called an (m, n) Padé119 approximation of f (z).
118There are plenty of other notations for this function.
119Henri Eugène Padé (1863–1953), French mathematician and student of Charles Hermite, gave a systematic
study of Padé forms in his thesis in 1892.

330
Chapter 3. Series, Operators, and Continued Fractions
Deﬁnition 3.5.3.
The (m, n) Padé approximation of the formal power series f (z) is, if it exists, deﬁned
to be a rational function
[m, n]f (z) = Pm,n(z)
Qm,n(z) ≡
m
j=0 pjzj
n
j=0 qjzj
(3.5.24)
that satisﬁes
f (z) −[m, n]f (z) = Rzm+n+1 + O(zm+n+2),
z →0.
(3.5.25)
The rational fractions [m, n]f , m, n ≥0 for f (z) can be arranged in a doubly inﬁnite
array, called a Padé table.
m\n
0
1
2
3
· · ·
0
[0, 0]f
[0, 1]f
[0, 2]f
[0, 3]f
· · ·
1
[1, 0]f
[1, 1]f
[1, 2]f
[1, 3]f
· · ·
2
[2, 0]f
[2, 1]f
[2, 2]f
[2, 3]f
· · ·
3
[3, 0]f
[3, 1]f
[3, 2]f
[3, 3]f
· · ·
...
...
...
...
...
The ﬁrst column in the table contains the partial sums m
j=0 cjzj of f (z).
Example 3.5.4.
The Padé approximants to the exponential function ez are important because of their
relation to methods for solving differential equations. The Padé approximants for m, n =
0 : 2 for the exponential function f (z) = ez are as follows.
m\n
0
1
2
0
1
1
1 −z
1
1 −z + 1
2z2
1
1 + z
1 + 1
2z
1 −1
2z
1 + 1
3z
1 −2
3z + 1
6z2
2
1 + z + 1
2z2
1 + 2
3z + 1
6z2
1 −1
3z
1 + 1
2z + 1
12z2
1 −1
2z + 1
12z2
There may not exist a rational function that satisﬁes (3.5.25) for all (m, n). We may
have to be content with k < 1. However, the closely related problem of ﬁnding Qm,n and
Pm,n(z) such that
Qm,nf (z) −Pm,n(z) = O(zm+n+1),
z →0,
(3.5.26)

3.5. Continued Fractions and Padé Approximants
331
always has a solution. The corresponding rational expression is called a Padé form of type
(m, n).
Using (3.5.23) and (3.5.24) gives
∞

k=0
ckzk
n

j=0
qjzj =
m

i=0
pizi + O(zm+n+1).
Matching the coefﬁcients of zi, i = 0 : m + n, gives
n

j=0
ci−jqj =
%
pi
if i = 0 : m,
0
if i = m + 1 : m + n,
(3.5.27)
where ci = 0 for i < 0. This is m + n + 1 linear equations for the m + n + 2 unknowns
p0, p1, . . . , pm, q0, q1, . . . , qn.
Theorem 3.5.4 (Frobenius).
There always exist Padé forms of type (m, n) for f (z). Each such form is a repre-
sentation of the same rational function [m, n]f . A reduced representation is possible with
Pm,n(z) and Qm,n(z) relatively prime, q0 = 1, and p0 = c0.
We now consider how to determine Padé approximants. With q0 = 1 the last n linear
equations in (3.5.27) are
n

j=1
ci−jqj + ci = 0,
i = m + 1 : m + n,
(3.5.28)
where ci = 0, i < 0. The system matrix of this linear system is
Cm,n =


cm
cm−1
· · ·
cm−n+1
cm+1
cm
· · ·
cm−n+2
...
...
· · ·
...
cm+n−1
cm+n−2
· · ·
cm


.
(3.5.29)
If cm,n = det(Cm,n) ̸= 0, then the linear system (3.5.28) has a solution q1, . . . , qn.
The coefﬁcients p0, . . . , pn of the numerator are then obtained from
pi =
min(i,n)

j=0
ci−jqj,
i = 0 : m.
(3.5.30)
In the regular case k = 1 the error constant R in (3.5.25) is given by
R = pi =
n

j=0
ci−jqj,
i = m + n + 1.

332
Chapter 3. Series, Operators, and Continued Fractions
Note that [m, n]f uses cl for l = 0 : m + n only; R uses cm+n+1 also. Thus, if cl is given
for l = 0 : r, then [m, n]f is deﬁned for m + n ≤r, m ≥0, n ≥0.
If n is large, the heavy part of the computation of a Padé approximant
[m, n]f (z) = Pm,n(z)/Qm,n(z)
of f (z) in (3.5.23) is the solution of the linear system (3.5.28). We see that if m or n is
decreased by one, most of the equations of the system will be the same. There are therefore
recursive relations between the polynomials Qm,n(z) for adjacent values of m, n, which can
be used for computing any sequence of adjacent Padé approximants. These relations have
been subject to intensive research that has resulted in several interesting algorithms; see the
next section on the epsilon algorithm, as well as the monographs of Brezinski [50, 51] and
the literature cited there.
There are situations where the linear system (3.5.28) is singular, i.e.,
cm,n = det(Cm,n) = 0.
We shall indicate how such singular situations can occur. These matters are discussed more
thoroughly in Cheney [66, Chap. 5].
Example 3.5.5.
Let f (z) = cos z = 1 −1
2z2 + · · ·, set m = n = 1, and try to ﬁnd
[1, 1]f (z) = (p0 + p1z)/(q0 + q1z),
q0 = 1.
The coefﬁcient matching according to (3.5.27) yields the equations
p0 = q0,
p1 = q1,
0 · q1 = −1
2q0.
The last equation contradicts the condition that q0 = 1. This single contradictory equation
is in this case the “system” (3.5.28).
If this equation is ignored, we obtain
[1, 1]f (z) = (1 + q1z)/(1 + q1z) = 1,
with error ≈1
2z2, in spite of the fact that we asked for an error that is O(zm+n+1) = O(z3).
If we instead allow that q0 = 0, then p0 = 0, and we obtain a solution
[1, 1]f (z) = z/z
which satisﬁes (3.5.26) but not (3.5.25). After dividing out the common factor z we get the
same result [1, 1]f (z) = 1 as before.
In a sense, this singular case results from a rather stupid request: we ask to approximate
the even function cos z by a rational function where the numerator and the denominator end
with odd powers of z. One should, of course, ask for the approximation by a rational
function of z2. What would you do if f (z) is an odd function?
It can be shown that these singular cases occur in square blocks of the Padé table,
where all the approximants are equal. For example, in Example 3.5.5 we will have [0, 0]f =

3.5. Continued Fractions and Padé Approximants
333
[0, 1]f = [1, 0]f = [1, 1]f = 1. This property, investigated by Padé, is known as the block
structure of the Padé table. For a proof of the following theorem, see Gragg [172].
Theorem 3.5.5.
Suppose that a rational function
r(z) = P(z)
Q(z),
where P (z) and Q(z) are relatively prime polynomials, occurs in the Padé table. Further
suppose that the degrees of P(z) and Q(z) are m and n, respectively. Then the set of all
places in the Padé table in which r(z) occurs is a square block. If
Q(z)f (z) −P(z) = O(zm+n+r+1),
(3.5.31)
then r ≥0 and the square block consists of (r + 1)2 places
(m + r1, n + r2),
r1, r2 = 0, 1, . . . , r.
An (m, n) Padé approximant is said to be normal if the degrees of Pm,n and Qm,n
are exactly m and n, respectively, and (3.5.31) holds with r = 0. The Padé table is called
normal if every entry in the table is normal. In this case all the Padé approximants are
different.
Theorem 3.5.6.
An (m, n) Padé approximant [m, n]f (z) is normal if and only if the determinants
cm,n,
cm1,n+1,
cm+1,n,
cm+1,n+1
are nonzero.
A Padé table is normal if and only if
cm,n ̸= 0,
m, n = 0, 1, 2, . . . .
In particular each Taylor coefﬁcient cm, 1 = cm, must be nonzero.
Proof. See Gragg [172].
Imagine a case where [m−1, n−1]f (z) happens to be a more accurate approximation
to f (z) than usual; say that
[m −1, n −1]f (z) −f (z) = O(zm+n+1).
(Forinstance, letf (z)betheratiooftwopolynomialsofdegreem−1andn−1, respectively.)
Let b be an arbitrary number, and choose
Qm,n(z) = (z + b)Qm−1,n−1(z),
Pm,n(z) = (z + b)Pm−1,n−1(z).
(3.5.32)

334
Chapter 3. Series, Operators, and Continued Fractions
Then
[m, n]f (z) = Pm,n(z)/Qm,n(z)
= Pm−1,n−1(z)/Qm−1,n−1(z) = [m −1, n −1]f (z),
which is an O(zm+n+1) accurate approximation to f (z). Hence our request for this accuracy
is satisﬁed by more than one pair of polynomials, Pm,n(z), Qm,n(z), since b is arbitrary. This
is impossible, unless the system (3.5.28) (that determines Qm,n) is singular.
Numerically singular cases can occur in a natural way. Suppose that one wants to
approximate f (z) by [m, n]f (z), although already [m−1, n−1]f (z) would represent f (z)
as well as possible with the limited precision of the computer. In this case we must expect the
system (3.5.28) to be very close to a singular system. A reasonable procedure for handling
this is to compute the Padé forms for a sequence of increasing values of m, n, to estimate
the condition numbers and to stop when it approaches the reciprocal of the machine unit.
This illustrates a fact of some generality. Unnecessary numerical trouble can be avoided
by means of a well-designed termination criterion.
For f (z) = −ln(1 −z), we have ci = 1/i, i > 0. When m = n the matrix of the
system (3.5.28) turns out to be the notorious Hilbert matrix (with permuted columns), for
which the condition number grows exponentially like 0.014 · 101.5n; see Example 2.4.7.
(The elements of the usual Hilbert matrix are aij = 1/(i + j −1).)
There is a close connection between continued fractions and Padé approximants.
Suppose that in a Padé table the staircase sequence
[0, 0]f , [1, 0]f , [1, 1]f , [2, 1]f , [2, 2]f , [3, 2]f , . . .
are all normal. Then there exists a regular continued fraction
1 + a1z
1+
a2z
1+
a3z
1+ . . . ,
an ̸= 0,
n = 1, 2, 3, . . . ,
with its nth convergent fn satisfying
f2m = [m, m]f ,
f2m+1 = [m + 1, m]f ,
m = 0, 1, 2, . . . ,
and vice versa. For a proof, see [214, Theorem 5.19].
Historically the theory of orthogonal polynomials, to be discussed later in Sec. 4.5.5.
originated from certain types of continued fractions.
Theorem 3.5.7.
Let the coefﬁcients of a formal power series (3.5.23) be the moments
cn =
 ∞
−∞
xnw(x) dx,
where w(t) ≥0. Let Qm,n be the denominator polynomial in the corresponding Padé
approximation [m, n]f . Then the reciprocal polynomials
Q∗
n,n+1(z) = zn+1Qn,n+1(1/z),
n ≥0,

3.5. Continued Fractions and Padé Approximants
335
are the orthogonal polynomials with respect to the inner product
(f, g) =
 ∞
−∞
f (x)g(x)w(x) dx.
Example 3.5.6.
The successive convergents of the continued fraction expansion in (3.5.3)
1
2z log
1 + z
1 −z

= 1
1−
z2
3−
22z2
5−
32z2
7−· · ·
are even functions and staircase Padé approximants. The ﬁrst few are
s01 =
3
3 −z2 ,
s11 = 15 + 4z2
3(5 −3z2),
s12 =
105 −55z2
3(35 −30z2 + 3z4),
s22 = 945 −735z2 + 64z4
15(63 −70z2 + 15z4).
These Padé approximants can be used to evaluate ln(1 + x) by setting z = x/(2 + x).
The diagonal approximants smm are of most interest. For example, the approximation s22
matches the Taylor series up to the term z8 and the error is approximately equal to the term
z10/11.
Chebyshev proved that the denominators in the above Padé approximants are the
Legendre polynomials in 1/z. These polynomials are orthogonal on [−1, 1] with respect to
the uniform weight distribution w(x) = 1; see Sec. 4.5.5.
Explicit expressions for the Padé approximants for ez were given by Padé (1892) in
his thesis. They are
Pm,n(z) =
m

j=0
(m + n −j)! m!
(m + n)! (m −j)!
zj
j! ,
(3.5.33)
Qm,n(z) =
n

j=0
(m + n −j)! n!
(m + n)! (n −j)!
(−z)j
j!
,
(3.5.34)
with the error
ez −Pm,n(z)
Qm,n(z) = (−1)n
m!n!
(m + n)!(m + n + 1)!zm+n+1 + O(zm+n+2).
(3.5.35)
Note that Pm,n(z) = Qn,m(−z), which reﬂects the property that e−z = 1/ez. Indeed, the
numerator and denominator polynomials can be shown to approximate (less accurately) ez/2
and e−z/2, respectively.
There are several reasons for preferring the diagonal Padé approximants (m = n) for
which
pj =
(2m −j)! m!
(2m)! (m −j)!j!,
qj = (−1)jpj,
j = 0 : m.
(3.5.36)

336
Chapter 3. Series, Operators, and Continued Fractions
These coefﬁcients satisfy the recursion
p0 = 1,
pj+1 =
(m −j)pj
(2m −j)(j + 1),
j = 0 : m −1.
(3.5.37)
For the diagonal Padé approximants the error Rm,n(z) satisﬁes |Rm,n(z)| < 1, for
ℜz < 0. This is an important property in applications for solving differential equations.120
To evaluate a diagonal Padé approximant of even degree we write
P2m,2m(z) = p2mz2m + · · · + p2z2 + p0
+ z(p2m−1z2m−2 + · · · + p3z2 + p1) = u(z) + v(z)
and evaluate u(z) and v(z) separately. Then Q2m(z) = u(z) −v(z). A similar splitting can
be used for an odd degree.
Recall that in order to compute the exponential function a range reduction should ﬁrst
be performed. If an integer k is determined such that
z∗= z −k ln 2,
|z∗| ∈[0, ln 2],
(3.5.38)
then exp(z) = exp(z∗) · 2k. Hence only an approximation of exp(z) for |z| ∈[0, ln 2] is
needed; see Problem 3.5.6.
The problem of convergence of a sequence of Padé approximants when at least one of
the degrees tends to inﬁnity is a difﬁcult problem and outside the scope of this book. Padé
proved that for the exponential function the poles of the Padé approximants [mi, ni]f tend
to inﬁnity when mi + ni tends to inﬁnity and
lim
i→∞[mi, ni]f (z) = ez
uniformly on any compact set of C. For a survey of other results, see [54].
3.5.4
The Epsilon Algorithm
One extension of the Aitken acceleration uses a comparison series with terms of the form
cj =
p

ν=1
α′
νkj
ν,
j ≥0,
kν ̸= 0.
(3.5.39)
Here α′
ν and kν are 2p parameters, to be determined, in principle, by means of cj, j = 0 :
2p −1. The parameters may be complex. The power series becomes
S(z) =
∞

j=0
cjzj =
p

ν=1
α′
ν
∞

j=0
kj
νzj =
p

ν=1
α′
ν
1 −kνz,
which is a rational function of z, and thus related to Padé approximation. Note, however,
that the poles at k−1
ν
should be simple and that m < n for S(z), because S(z) →0 as z →∞.
120Diagonal Padé approximants are also used for the evaluation of the matrix exponential eA, A ∈Rn×n; see
Volume II, Chapter 9.

3.5. Continued Fractions and Padé Approximants
337
Recall that the calculations for the Padé approximation determine the coefﬁcients of S(z)
without calculating the 2n parameters α′
ν and kν. It can happen that m becomes larger than
n, and if α′
ν and kν are afterward determined, by the expansion of S(z) into partial fractions,
it can turn out that some of the kν are multiple poles. This suggests a generalization of this
approach, but how?
If we consider the coefﬁcients qj, j = 1 : n, occurring in (3.5.28) as known quantities,
then (3.5.28) can be interpreted as a linear difference equation.121 The general solution of
this is given by (3.5.39) if the zeros of the polynomial
Q(x) := 1 +
n

j=1
qjxj
are simple. If multiple roots are allowed, the general solution is, by Theorem 3.3.13 (after
some change of notation),
cl =

ν
pν(l)kn
ν,
where kν runs through the different zeros of Q(x) and pν is an arbitrary polynomial, the
degree of which equals the multiplicity −1 of the zero kν. Essentially the same mathe-
matical relations occur in several areas of numerical analysis, such as interpolation and
approximation by a sum of exponentials (Prony’s method), and in the design of quadrature
rules with free nodes (see Sec. 5.3.1).
Shanks [322] considered the sequence transformation
ek(sn) =
((((((((((
sn
sn+1
· · ·
sn+k
sn+1
sn+2
· · ·
sn+k+1
...
...
· · ·
...
sn+k
sn+k+1
· · ·
sn+2k
((((((((((
((((((((
42sn
· · ·
42sn+k−1
...
· · ·
...
42sn+k−1
· · ·
42sn+2k−2
((((((((
,
k = 1, 2, 3, . . . ,
(3.5.40)
and proved that it is exact if and only if the values sn+i satisfy a linear difference equation
a0(sn −a) + · · · + ak(sn+k −a) = 0 ∀n,
(3.5.41)
with a0ak ̸= 0, a0+· · ·+ak ̸= 0. For k = 1, Shanks’transformation reduces toAitken’s 42
process (the proof is left as Problem 3.5.7). The Hankel determinants122 in the deﬁnition
of ek(sn) satisfy a ﬁve-term recurrence relationship, which can be used for implementing
the transformation.
121This can also be expressed in terms of the z-transform; see Sec. 3.3.5.
122A matrix with constant elements in the antidiagonals is called a Hankel matrix, after Hermann Hankel (1839–
1873), German mathematician. In his thesis [185] he studied determinants of the class of matrices now named
after him.

338
Chapter 3. Series, Operators, and Continued Fractions
Here we are primarily interested in the use of Padé approximants as a convergence
accelerator in the numerical computation of values of f (z) for (say) z = eiφ. A natural
question is then whether it is possible to omit the calculation of the coefﬁcients pj, qj and
ﬁnd a recurrence relation that gives the function values directly. A very elegant solution
to this problem, called the epsilon algorithm, was found in 1956 by Wynn [384], after
complicated calculations. We shall present the algorithm, but refer to the survey paper by
Wynn [386] for proof and more details.
A two-dimensional array of numbers ϵ(n)
k
is computed by the nonlinear recurrence
relation,
ϵ(p)
k+1 = ϵ(p+1)
k−1
+
1
ϵ(p+1)
k
−ϵ(p)
k
,
p, k = 0, 1, . . . ,
(3.5.42)
which involves four quantities in a rhombus:
ϵ(p)
k
ϵ(p+1)
k−1
ϵ(p)
k+1
ϵ(p+1)
k
.
The sequence transformation of Shanks can be computed by using the boundary conditions
ϵ(p)
−1 = 0, ϵ(p)
0
= sp in the epsilon algorithm. Then
ϵ(p)
2k = ek(sp),
ϵ(p)
2k+1 = 1/ek(4sp),
p = 0, 1, . . . ;
i.e., the ϵ’s with even lower index give the sequence transformation (3.5.40) of Shanks. The
ϵ’s with odd lower index are auxiliary quantities only.
The epsilon algorithm transforms the partial sums of a series into its Padé quotients
or, equivalently, is a process by means of which a series may be transformed into the con-
vergents of its associated and corresponding continued fractions. It is a quite powerful
all-purpose acceleration process for slowly converging sequences and usually fully exploits
the numerical precision of the data. For an application to numerical quadrature, see Exam-
ple 5.2.3.
If the boundary conditions
ϵ(p)
−1 = 0,
ϵ(p)
0
= rp,0(z) = p
j=0 cjzj
(3.5.43)
are used in the epsilon algorithm, this yields for even subscripts
ϵ(p)
2n = rp+n,n(z)
(3.5.44)
Thus the epsilon algorithm can be used to compute recursively the lower half of the Padé
table. The upper half can be computed by using the boundary conditions
ϵ(−n)
2n
= r0,n(z) =
1
n
j=0 djzj .
(3.5.45)
The polynomials r0,n(z) are obtained from the Taylor expansion of 1/f (z). Several proce-
dures for obtaining this were given in Sec. 3.1.

3.5. Continued Fractions and Padé Approximants
339
It seems easier to program this application of the ϵ-algorithm after a slight change of
notation. We introduce an r × 2r matrix A = [aij], where
aij = ϵ(p)
k ,
k = j −2,
p = i −j + 1.
Conversely, i = k + p + 1, j = k + 2. The ϵ algorithm, together with the boundary
conditions, now takes the following form:
for i = 1 : r
ai,1 = 0;
ai,2 = ri−1,0(z);
ai,2i = r0,i−1(z);
for j = 2 : 2(i −1)
ai,j+1 = ai−1,j−1 + 1/(aij −ai−1,j).
end
end
Results:
[m, n]f (z) = am+n+1,2n+2,
(m, n ≥0,
m + n + 1 ≤r).
The above program sketch must be improved for practical use. For example, something
should be done about the risk for a division by zero.
3.5.5
The qd Algorithm
Let {cn} be a sequence of real or complex numbers and
C(z) = c0 + c1z + c2z2 + · · ·
(3.5.46)
the formal power series formed with these coefﬁcients. The qd algorithm of Rutishauser
[310]123 forms from this sequence a two-dimensional array, similar to a difference scheme,
by alternately taking difference and quotients as follows. Take as initial conditions
e(n)
0
= 0,
n = 1, 2, . . . ,
q(n)
1
= cn+1
cn
,
n = 0, 1, . . . ,
(3.5.47)
and form the quotient-difference scheme, or qd scheme:
q(0)
1
0
e(0)
1
q(1)
1
q(0)
2
0
e(1)
1
e(0)
2
q(2)
1
q(1)
2
q(0)
3
0
e(2)
1
e(1)
2
...
q(2)
2
...
...
...
,
123Heinz Rutishauser (1912–1970) was a Swiss mathematician, a pioneer in computing, and the originator of
many important algorithms. The qd algorithm has had great impact in eigenvalue calculations.

340
Chapter 3. Series, Operators, and Continued Fractions
where the quantities are connected by the two rhombus rules
e(n)
m = q(n+1)
m
−q(n)
m + e(n+1)
m−1 ,
(3.5.48)
q(n)
m+1 = e(n+1)
m
e(n)
m
q(n+1)
m
,
m = 1, 2, . . . , n = 0, 1, . . . .
(3.5.49)
Each of the rules connects four adjacent elements of the qd scheme. The ﬁrst rule states
that in any rhombus-like conﬁguration of four elements centered in a q-column the sum of
the two NE and the two SW elements are equal. Similarly, the second rule states that in
any rhombus-like conﬁguration in an e-column the product of the two NE and the two SW
elements are equal.
The initial conditions (3.5.47) give the ﬁrst two columns in the qd scheme. The
remaining elements in the qd scheme, if it exists, can then be generated column by column
using the rhombus rules. Note the computations break down if one of the denominators in
(3.5.49) is zero. If one of the coefﬁcients cn is zero even the very ﬁrst q-column fails to exist.
The rhombus rules are based on certain identities between Hankel determinants, which
we now describe. These also give conditions for the existence of the qd scheme. The Hankel
determinants associated with the formal power series (3.5.46) are, for arbitrary integers n
and k ≥0, deﬁned by H (n)
0
= 1,
H (n)
k
=
((((((((((
cn
cn+1
· · ·
cn+k−1
cn+1
cn+2
· · ·
cn+k
...
· · ·
· · ·
...
cn+k−1
cn+k−2
· · ·
cn+2k−2
((((((((((
,
k > 1,
(3.5.50)
where ck = 0 for k < 0. This deﬁnition is valid also for negative values of n. Note,
however, that if n + k ≤0, then the entire ﬁrst row of H (n)
k
is zero, i.e.,
H (n)
k
= 0,
k ≤−n.
(3.5.51)
Aformal power series is called normal if its associated Hankel determinants H (n)
m
̸= 0
for all m, n ≥0; it is called k-normal if H (n)
m
̸= 0 for m = 0 : k and for all n ≥0.
In the following theorem (Henrici [196, Theorem 7.6a]) the elements in the qd scheme
can be expressed in terms of Hankel determinants.
Theorem 3.5.8.
Let H (n)
k
be the Hankel determinants associated with a formal power series C =
c0 + c1z + c2z2 + · · ·. If there exists a positive integer k such that the series is k-normal,
the columns q(n)
m of the qd scheme associated with C exist for m = 1 : k, and
q(n)
m = H (n+1)
m
H (n)
m−1
H (n)
m H (n+1)
m−1
,
e(n)
m = H (n)
m+1H (n+1)
m−1
H (n)
m H (n+1)
m
(3.5.52)
for m = 1 : k and all n ≥0.
The above result is related to Jacobi’s identity for Hankel matrices, that for all integers
n and k ≥1,
(H (n)
k )2 −H (n−1)
k
H (n+1)
k
+ H (n−1)
k+1 H (n+1)
k−1
= 0.
(3.5.53)
This identity can be derived from the following very useful determinant identity.

3.5. Continued Fractions and Padé Approximants
341
Theorem 3.5.9 (Sylvester’s Determinant Identity).
Let ˆA ∈Cn×n, n ≥2, be partitioned:
ˆA =
 α11
aT
1
α12
ˆa11
A
ˆa2
α21
aT
2
α22

=

A11
∗
∗
α22

=

∗
A12
α21
∗

=

∗
α12
A21
∗

=

α11
∗
∗
A21

.
Then we have the identity
det(A) · det( ˆA) = det(A11) · det(A22) −det(A21) · det(A12).
(3.5.54)
Proof. If the matrix A is square and nonsingular, then
det(Aij) = ± det

A
ˆaj
aT
i
αij

= ± det(A) · (αij −aT
i A−1 ˆaj),
(3.5.55)
with negative sign only possible if i ̸= j. Then, similarly,
det(A) · det( ˆA) = det(A) · det


A
ˆa11
ˆa2
aT
1
α11
α12
aT
2
α21
α22


= (det(A))2 · det
0
α11
α12
α21
α22

−

aT
1
aT
2

A−1 ( ˆa1
ˆa2 )
1
= det

β11
β12
β21
β22

,
where βij = αij −aT
i A−1 ˆaj. Using (3.5.55) gives (3.5.54), which holds even when A is
singular.
If the Hankel determinants H (n)
k
are arranged in a triangular array,
1
1
H (0)
1
= c0
1
H (1)
1
= c1
H (0)
2
1
H (2)
1
= c2
H (1)
2
H (0)
3
1
H (3)
1
= c3
H (2)
2
H (1)
3
H (0)
4
,
then Jacobi’s identity links together the entries in a star-like conﬁguration. Since the two
ﬁrst columns are trivial, (3.5.53) may be used to calculate the Hankel determinants recur-
sively from left to right. Further properties of Hankel determinants are given in Henrici [196,
Sec. 7.5].

342
Chapter 3. Series, Operators, and Continued Fractions
We state without proof an important analytical property of the Hankel determinants
that shows how the poles of a meromorphic124 function can be determined from the coefﬁ-
cients of its Taylor expansion at z = 0.
Theorem 3.5.10.
Let f (z) = c0 + c1z + c2z2 + · · · be the Taylor series of a function meromorphic in
the disk D : |z| < σ and let the poles zi = u−1
i
of f in D be numbered such that
0 < |z1| ≤|z2| ≤· · · < σ.
Then for each m such that |zm| < |zm+1|, if n is sufﬁciently large, H (n)
m
̸= 0, and
lim
n→∞H (n+1)
m
/H (n)
m
= u1u2 · · · um.
(3.5.56)
In the special case that f is a rational function with a pole of order p at inﬁnity and
the sum of orders of all its ﬁnite poles is k, then
H (n)
k
= Ck(u1u2 · · · uk)n,
n > p,
(3.5.57)
where Ck ̸= 0; furthermore Hm(n) = 0, n > p, m > k.
Proof. The result is a corollary of Theorem 7.5b in Henrici [196].
The above results are related to the qd scheme as follows; see Henrici [196, Theorem
7.6b].
Theorem 3.5.11.
Under the hypothesis of Theorem 3.5.10 and assuming that the Taylor series at z = 0
is ultimately k-normal for some integer k > 0, the qd scheme for f has the following
properties:
(a) For each m such that 0 < m ≤k and |zm−1| < |zm| < |zm+1|,
lim
n→∞q(n)
m = um;
(b) For each m such that 0 < m ≤k and |zm| < |zm+1|,
lim
n→∞e(n)
m = 0.
From the above results it seems that, under certain restrictions, an algorithm for
simultaneously computing all the poles of a meromorphic function f directly from its
Taylor series at the origin could be constructed, where the qd scheme is computed from left
to right. Any q-column corresponding to a simple pole of isolated modulus would tend to
the reciprocal value of that pole. The e-columns on both sides would tend to zero. If f is
rational, the last e-column would be zero, which could serve as a test of accuracy.
124A function which is analytic in a region <, except for poles, is said to be meromorphic in <.

3.5. Continued Fractions and Padé Approximants
343
Unfortunately, as outlined, this algorithm is unstable, i.e., oversensitive to rounding
errors, and useless numerically. This fact is related to the occurrence in (3.5.49) of a division
of two small quantities, which can have large relative errors. (Recall that e-columns tends
to zero.)
A more stable way of constructing the qd scheme is obtained by writing the rhombus
rules as
q(n+1)
m
=
*
e(n)
m −e(n+1)
m−1
+
+ q(n)
m ,
(3.5.58)
e(n+1)
m
= q(n)
m+1
q(n+1)
m
e(n)
m .
(3.5.59)
Written in this form, the rules can be used to construct the qd scheme row by row. The
problem now is how to start the algorithm. As seen from the scheme below, to do this it
sufﬁces to know the ﬁrst two rows of q’s and e’s. This, together with the ﬁrst column of
zeros, allows us to proceed along diagonals slanted SW; see scheme below.
q(0)
1
q(−1)
2
q(−2)
3
· · ·
0
e(0)
1
e(−1)
2
e(−2)
3
· · ·
×
×
×
0
×
×
×
×
0
×
×
0
This is called the progressive form of the qd algorithm. The starting values q(n)
m and e(n)
m
for negative values of n can be computed from the relations (3.5.52). In this form the qd
algorithm can be used to simultaneously determine the zeros of a polynomial; see Sec. 6.5.4.
The qd algorithm is related to Padé approximants. Consider a continued fraction of
the form
c(z) = a1
1+
a2z
1+
a3z
1+ · · · .
(3.5.60)
The nth approximant
wn(z) = Pn(z)/Qn(z),
n = 1, 2, . . . ,
(3.5.61)
is the ﬁnite continued fraction obtained by setting an+1 = 0. In the special case that all
ai > 0, the continued fraction is called a Stieltjes fraction.125 The sequence of numerators
{Pn(z)} and denominators {Qn(z)} in (3.5.61) satisfy the recurrence relations
P0 = 0,
P1 = 1,
Pn = zanPn−2 + Pn−1,
Q0 = Q1 = 1,
Qn = zanQn−2 + Qn−1,
n ≥2.
125The theory of such fractions was ﬁrst expounded by Stieltjes in a famous memoir which appeared in 1894,
the year of his death.

344
Chapter 3. Series, Operators, and Continued Fractions
Hence both Pn and Qn are polynomials in z of degree ⌊(n −1)/2⌋and ⌊n/2⌋, respectively.
It can be shown that the polynomials Pn and Qn have no common zero for n = 1, 2, . . . .
From the initial conditions and recurrence relations it follows that Qn(0) = 1, n =
0, 1, 2, . . . . Hence the rational function wn(z) = Pn(z)/Qn(z) is analytic at z = 0. Hence
it can be expanded in a Taylor series
Pn(z)
Qn(z) = c(n)
0
+ c(n)
1 z + c(n)
2 z2 + · · ·
(3.5.62)
that converges for z sufﬁciently small. The coefﬁcients c(n)
k
in (3.5.62) can be shown to
be independent of n for k < n. We denote by ck := c(n+1)
k
the ultimate value of c(n)
k
for
increasing values n and let
C(z) = c0 + c1z + c2z2 + · · ·
(3.5.63)
be the formal power series formed with these coefﬁcients. Then the power series C(z) and
the fraction c(z) are said to correspond to each other. Note that the formal power series
C(z) corresponding to a given fraction c(z) converges for any z ̸= 0.
The qd algorithm can be used to solve the following problem: Given a (formal) power
series C(z) = c0 + c1z + c2z2 + · · ·, ﬁnd a continued fraction c(z) of the form (3.5.60)
corresponding to it. Note that we do not require that the formal power series corresponding
to the continued fraction converges, merely that the nth approximant wn of the continued
fraction satisﬁes
C(z) −wn(z) = O(zn).
Theorem 3.5.12 (Henrici [197, Theorem 12.4c]).
Given a formal power series C(z) = c0 + c1z + c2z2 + · · ·, there exists at most one
corresponding continued fraction of the form
a0
1−
a1z
1−
a2z
1−
a3z
1−
4z
1−· · · .
There exists precisely one such fraction if and only if the Hankel determinants satisfy Hk(n) ̸=
0 for n = 0, 1 and k = 1, 2, . . . . If q(n)
k
and e(n)
k
are the elements of the qd scheme associated
with C, then
c0
1−
q(0)
1 z
1−
e(0)
1 z
1−
q(0)
2 z
1−
e(0)
2 z
1−
· · · .
(3.5.64)
Conversely, this shows that knowing the coefﬁcients of the continued fraction cor-
responding to f allows us to compute the qd scheme starting from the ﬁrst diagonal and
proceeding in the SW direction. This is called the progressive qd algorithm.
Example 3.5.7.
For the power series
c(z) = 0! + 1!z + 2!z2 + 3!z3 + · · · ,

Problems and Computer Exercises
345
we obtain using the rhombus rules (3.5.48)–(3.5.49) the qd scheme
1
0
1
2
2
0
1
2
3
3
3
0
1
2
...
4
4
...
0
1
...
5
...
.
Hence the corresponding continued fraction is
c(z) = 1
1+
z
1+
z
1+
2z
1+
2z
1+
3z
1+
3z
1+ · · · .
Review Questions
3.5.1 Deﬁne a continued fraction. Show how the convergents can be evaluated either
backward or forward.
3.5.2 Show how any positive number can be expanded into a continued fraction with integer
elements. In what sense are the convergents the best approximations? How accurate
are they?
3.5.3 What is the Padé table? Describe how the Padé approximants can be computed, if
they exist. Tell something about singular and almost singular situations that can be
encountered, and how to avoid them.
3.5.4 Describe the ϵ algorithm, and tell something about its background.
3.5.5 What are the rhombus rules for the qd algorithm? What is the difference between the
standard and the progressive qd algorithm?
3.5.6 Sketch how the qd algorithm, under some restrictions, can be used to compute the
zeros of a polynomial. Give necessary conditions for this to work. What governs the
rate of convergence?
Problems and Computer Exercises
3.5.1 (a)Write a program for the algorithm of best rational approximations to a real number
in Sec. 3.5.1.

346
Chapter 3. Series, Operators, and Continued Fractions
Apply it to ﬁnd a few coefﬁcients of the continued fractions for
1
2(
√
5 + 1),
√
2, e, π,
log 2
log 3, 2j/12
for a few integers j, 1 ≤j ≤11.
(b) Check the accuracy of the convergents. What happens when you apply your
program to a rational number, e.g., 729/768?
(c) The metonic cycle used for calendrical purposes by the Greeks consists of 235
lunar months, which nearly equal 19 solar years. Show, using the algorithm in
Sec. 3.5.1, that 235/19 is the sixth convergent of the ratio 365.2495/29.53059 of
solar period and the lunar phase (synodic) period.
3.5.2 A matrix formalism for continued fractions.
(a) We use the same notations as in Sec. 3.5.1, but set, with no loss of generality,
b0 = 0. Set
P(n) =

pn−1
pn
qn−1
qn

,
A(n) =

0
an
1
bn

.
Show that P(0) = I,
P (n) = P(n −1)A(n),
P(n) = A(1)A(2) · · · A(n −1)A(n),
n ≥1.
Comment: This does not minimize the number of arithmetic operations but, in a
matrix-oriented programming language, it often gives very simple programs.
(b) Write a program for this with some termination criterion and test it on a few
cases, such as
1 + 1
1+
1
1+
1
1+ · · · ,
2 + 1
3+
1
2+
1
3+
1
2+
1
3+ · · · ,
2 + 2
2+
3
3+
4
4+ · · · .
As a postprocessing, apply Aitken acceleration in the ﬁrst two cases in order to
obtain a very high accuracy. Does the result look familiar in the last case? See
Problem 3.5.3 concerning the exact results in the two other cases.
(c) Write a version of the program with some strategy for scaling P(n) in order to
eliminate the risk of overﬂow and underﬂow.
Hint: Note that the convergents xn = pn/qn are unchanged if you multiply the P(n)
by arbitrary scalars.
(d) Use this matrix form for working out a short proof of (3.5.7).
Hint: What is the determinant of a matrix product?
3.5.3 (a) Explain that x = 1 + 1/x for the continued fraction in (3.5.13).
(b) Compute the periodic continued fraction
2 + 1
3+
1
2+
1
3+
1
2+
1
3+ · · ·
exactly (by paper and pencil). (The convergence is assured by Seidel’s theorem
(Theorem 3.5.2).)

Problems and Computer Exercises
347
(c) Suggest a generalization of (a) and (b), where you can always obtain a quadratic
equation with a positive root.
(d) Show that
1
√
x2 −1
=
1
x−
1
2
x −y ,
where
y =
1
4
x−
1
4
x−
1
4
x−· · · .
3.5.4 (a) Prove the equivalence transformation (3.5.8). Show that the errors of the con-
vergents have alternating signs if the elements of the continued fraction are positive.
(b) Show how to bring a general continued fraction to the special form of equation
(3.5.12).
3.5.5 Show that the (1, 1) Padé approximant of
√
1 + x equals (4 + 3x)/(4 + x). What
is the (2, 2) Padé approximant?
3.5.6 Let Pm,m(z)/Qm,m(z) be the diagonal Padé approximants of the exponential func-
tion.
(a) Show that the coefﬁcients for Pm,m(z) satisfy the recursion
p0 = 1,
pj+1 =
m −j
(2m −j)(j + 1)pj,
j = 0 : m −1.
(3.5.65)
(b) Show that for m = 6 we have
P6,6(z) = 1 + 1
2z + 5
44z2 + 1
66z3 +
1
792z4 +
1
15,840z5 +
1
665,280z6
and Q6,6(z) = P6,6(−z). How many operations are needed to evaluate this approx-
imation for a given z?
(c) Use the error estimate in (3.5.35), neglecting higher-order terms, to compute a
bound for the relative error of the approximation in (b) when |z| ∈[0, ln 2]. What
degree of the diagonal Padé approximant is needed for the relative error to be of the
order of the unit roundoff 2−53 = 1.11 · 10−16 in IEEE double precision arithmetic?
3.5.7 For k = 1, Shanks’ sequence transformation (3.5.40) becomes
e1(sn) =
((((
sn
sn+1
sn+1
sn+2
((((
#
42sn.
Show that this is mathematically equivalent to the result s′
n+2 from Aitken extrapo-
lation. Why is the direct use of the above expression not safe numerically?
3.5.8 (a) Write a program for computing a Padé approximant and its error term. Apply it
(perhaps after a transformation) for various values of m, n to, e.g., ez, arctan z, tan z.
(Note that two of these examples are odd functions.) Use the algorithm of Sec. 3.5.1
for expressing the coefﬁcients as rational numbers. For how large m and n can you
use your program (in these examples) without severe trouble with rounding errors?
(b) Let m be an odd number. Try to transform the (m, m + 1) Padé approximants of
arctan z and tan z to continued fractions of the form given in Sec. 3.5.1.
(c) Try to determine for which other functions the Padé table has a similar symmetry
as shown in the text for the exponential function ez.

348
Chapter 3. Series, Operators, and Continued Fractions
3.5.9 (a) Show that there is at most one rational function R(z), where the degrees of the
numerator and denominator do not exceed, respectively, m and n such that
f (z) −R(z) = O(zm+n+1)
as
z →0,
even if the system (3.5.28) is singular. (Note, however, that Pm and Qn are not
uniquely determined if the system is singular; they have common factors.)
(b) Is it true that if f (z) is a rational function of degrees m′, n′, then
[m, n]f (z) = f (z)
∀m ≥m′,
n ≥n′?
3.5.10 Write a program for evaluating the incomplete gamma function. Use the continued
fraction (3.5.22) for x greater than about a + 1. For x less than about a + 1 use the
power series for γ (a, x).
3.5.11 Compute the inﬁnite sum 1 −1
3 + 1
5 −1
7 + 1
9 −· · · with the epsilon algorithm, and
estimate (empirically) the speed of convergence.
3.5.12 Write a program for determining the zeros of a polynomial p(z) of degree n with
simple positive zeros. Test it by computing the zeros of some orthogonal polyno-
mials. Discuss how you can shift the zeros so that convergence to a particular zero
is enhanced.
Notes and References
Much work on approximations to special functions, for example, the Gauss hypergeomet-
ric function and the Kummer function, was done around the end of World War II. A most
comprehensive source of information on useful mathematical functions and formulas is the
Handbook ﬁrst published in 1964 by the National Bureau of Standards (renamed National
Institute of Standards and Technology (NIST) in 1988), of which more than 150,000 copies
have been sold. Tables and formulas in this handbook can be useful in preliminary sur-
veys before turning to computer programs. Methods that are important for the numerical
computation of special functions are surveyed in Temme [349].
Although still available and among one of the most cited references, the Handbook
is increasingly becoming out of date. A replacement more suited to the needs of today
is being developed at NIST. This is planned to be made available both in print and as a
free electronic publication on the World Wide Web; see http://dlmf.nist.gov. An
outline of the features of the new NIST Digital Library of Mathematical Functions is given
by D. W. Lozier [249]. The Internet version will come with hyperlinks, interactive graph-
ics, and tools for downloading and searching. The part of the old Handbook devoted to
massive tables of values will be superseded. To summarize, data-intensive and operation-
preserving methods are replaced by data-conserving and operation-intensive techniques. A
complete survey of the available software for computing special functions was given by
Lozier and Olver [250]. The latest update of this project appeared in December 2000; see
http://math.nist.gov/mcsd/Reports/2001/nesf/.
The basic properties of the Gauss hypergeometric function are derived in Lebedev’s
monograph on special functions [240]. Lebedev’s compact book provides a good back-
ground to many of the applications of advanced analysis that lack complete proofs in our

Notes and References
349
book. For example, the chapter on the gamma function contains numerous instances of the
use of series expansions and analytic continuation that are efﬁcient as well as instructive,
important, and beautiful. Codes and other interesting information concerning the evaluation
of special functions are also found in [294, Chap. 5 and 6].
A thorough treatment of polynomial interpolation of equidistant data is found in Stef-
fensen [330]; see in particular Sec. 18 about “the calculus of symbols.” The history of
this topic is presented in Goldstine [159]. Different aspects of automatic differentiation are
discussed by Rall [297], Griewank [175], and Corliss et al. [80].
A classic exposition of the theory of inﬁnite series is given in the monograph by
Knopp [229]. An exposition of the long and interesting historical development of conver-
gence accelerating methods is given by Brezinski [53]. The use of extrapolation methods
in numerical analysis up to 1970 is surveyed in Joyce [215], which contains an extensive
bibliography. The book by Brezinski and Redivo-Zaglia [55] covers more recent develop-
ments. It also surveys properties of completely monotonic sequences, and how to construct
such sequences.
A general extrapolation algorithm that includes almost all known convergence ac-
celeration methods has been given by Håvie [188]. For acceleration of vector sequences
several generalizations of scalar sequence transformations have been suggested; see [173].
Used for solving linear equations, these are related to the biconjugate gradient algorithm and
projection methods; see the monograph by Brezinski [52]. Some convergence acceleration
methods (due to Lindelöf, Plana, and others) transform an inﬁnite series to an integral in the
complex plane. With appropriate numerical procedures for computing the integral, these
methods can compete with the methods treated in Sec. 3.4. In particular, they are applicable
to some difﬁcult ill-conditioned series; see Dahlquist [87].
The theory of continued fractions started to develop already in the seventeenth century.
The main contributors were Euler, Lambert, and Lagrange; see Brezinski [51]. Algebraic
continued fractions and applications to number theory are treated in Riesel [303].
The analytic theory of continued fractions has earlier origins, and contributors include
Chebyshev, A. A. Markov, and Stieltjes. Hermite was able to prove the transcendence of e
in 1873 using a kind of Padé approximants. His proof was extended in 1892 by Lindemann,
who showed that π is a transcendental number, answering a question that had been an
open problem for 2000 years. An important survey of theory and applications of continued
fractions is given by Jones and Thron [214]; see also Lorenzen and Waadeland [248].
Continued fraction expansions of many special functions are found in Abramowitz and
Stegun [1]. Codes and further references are given in Press et al. [294, Chapters 5 and 6].
The basic algorithmic aspects of what we today call Padé approximants were estab-
lished by Frobenius [127]. Padé [281] gave a systematic study of these approximants and
introduced the table named after him. The most complete reference on Padé approximation
is Baker and Graves-Morris [14]. An easier to read introduction is Baker [13]. The numer-
ical evaluation of continued fractions is surveyed in Blanche [34]. Gragg [172] gives an
excellent survey of the use of the Padé table in numerical analysis.
ThetheoryoftheqdalgorithmistreatedindepthbyHenrici[196, Chap. 7]. Rutishauser
[311] got the idea for his LR algorithm for computing the eigenvalues of a matrix from the
qd algorithm. For recent developments and applications to the matrix eigenvalue problem
see Parlett [285].


Chapter 4
Interpolation and
Approximation
Far better an approximate answer to the right question,
which is often vague,
than an exact answer to the wrong question,
which can always be made more precise.
—John W. Tukey
4.1
The Interpolation Problem
4.1.1
Introduction
Polynomials are used as the basic means of approximation in nearly all areas of numerical
analysis. We have encountered in Sec. 3.3.4 the problem of interpolating the values of a
function f (x) in n equidistant points by a polynomial p(x) ∈Pn.126 We have also studied
application of polynomial approximations to numerical differentiation and integration. It is
de facto so, although the polynomials are invisible in the derivations of formulas by operator
methods. In the following sections we shall go deeper into the nonequidistant polynomial
interpolation problem.
Let a = x1 < x2 < · · · < xn = b be a grid of distinct points xi. Find a polynomial
p ∈Pn such that
p(xi) = f (xi),
i = 1 : n.
(4.1.1)
By Theorem 3.3.4, the interpolation polynomial p is uniquely determined.
This
theorem is general, although the rest of Sec. 3.3 dealt with interpolation polynomials in
the equidistant case only and their application to numerical differentiation. Note that the
formulation and the solution of this problem are independent of the ordering of the points xi.
126Recall the deﬁnition of Pn as the space of polynomials in one variable of degree less than n; the dimension
of the linear space Pn is n.
351

352
Chapter 4. Interpolation and Approximation
4.1.2
Bases for Polynomial Interpolation
A set of polynomials {p1(x), p2(x), . . . , pn(x)} such that any polynomial p ∈Pn can be
expressed as a linear combination
p(x) =
n

j=1
cjpj(x)
is called a basis in Pn. The column vector c = (c1, c2, . . . , cn)T can be viewed as a
coordinate vector of p in the space Pn, with respect to this basis. The interpolation problem
(4.1.1) leads to a linear system of equations
c1p1(xi) + c2p2(xi) + · · · + cnpn(xi) = f (xi),
i = 1 : n.
(4.1.2)
If we introduce the matrix
Pn = [pj(xi)]n
i,j=1,
(4.1.3)
and the column vector f = (f (x1), f (x2), . . . , f (xn))T , then the linear system becomes
Pnc = f.
(4.1.4)
Mathematically, the choice of basis (for a ﬁnite-dimensional space) makes no differ-
ence. Computationally, working with rounded values of the coefﬁcients, the choice of basis
can make a great difference. If the purpose is to compute derivatives or integrals of the inter-
polation polynomial, the power basis or the shifted power basis, where pj(x) = (x−c)j−1,
i.e.,
p(x) =
n

j=1
cj(x −c)j−1,
is convenient although not always the best. If a shifted power basis is to be used for
polynomial approximation on an interval [a, b], it is often best to choose c = (a + b)/2,
i.e., equal to the midpoint of the interval.
For the power basis pj(x) = xj−1, the coefﬁcients of the interpolation polynomial
are given by the solution of the linear system V T
n c = f , where Vn is the Vandermonde
matrix
Vn = [xi−1
j
]n
i,j=1 =


1
1
· · ·
1
x1
x2
· · ·
xn
...
...
· · ·
...
xn−1
1
xn−1
2
· · ·
xn−1
n

.
(4.1.5)
By Theorem 3.3.4 this matrix is nonsingular, since the Vandermonde determinant equals
(see (3.3.12))
det(Vn) =
&
1≤i<j≤n
(xi −xj).
Let {p1(x), p2(x), . . . , pn(x)} and {q1(x), q2(x), . . . , qn(x)} be two bases for Pn.
Then the qj must be linear combinations of the pk, k = 1 : n. This can be expressed in
vector-matrix form:

q1(x), q2(x), . . . , qn(x)

=

p1(x), p2(x), . . . , pn(x)

S
∀x,
(4.1.6)

4.1. The Interpolation Problem
353
where S is a constant matrix. S must be nonsingular, since if S were singular, then there
would exist a nontrivial vector v such that Sv = 0, hence
(q1(x), q2(x), . . . , qn(x))v = (p1(x), p2(x), . . . , pn(x))Sv = 0
∀x,
and (q1(x), q2(x), . . . , qn(x)) would thus not be a basis.
Let Pn = [pj(xi)]n
i,j=1 and Qn = [qj(xi)]n
i,j=1. By putting x = xi, i = 1 : m,
into (4.1.6), we see that Qn = PnS, and Qn is nonsingular for every basis. If we set
p(x) =  djqj(x), the system (4.1.2) becomes for this basis Qnd = f , and then
Pnc = f = Qnd = PnSd,
c = P −1
n f = Sd.
(4.1.7)
The matrix S for the transformation between representations is thus like a coordinate trans-
formation in geometry. Matrix representations of various common bases transformations
are given by Gander [130].
The power basis has a bad reputation which is related to the ill-conditioning of the
corresponding Vandermonde matrix; see Sec. 4.1.3. There are other bases in Pn which are
often more advantageous to use. By a triangle family of polynomials we mean a sequence
of polynomials
q1(x) = s11,
q2(x) = s12 + s22x,
q3(x) = s13 + s23x + s33x2,
(4.1.8)
. . .
qn(x) = s1n + s2nx + s3nx2 + · · · + snnxn−1,
where sjj ̸= 0 for all j. Note that the coefﬁcients form a lower triangular matrix S.
Conversely, for any j, pj(x) = xj−1 can be expressed recursively and uniquely as
linear combinations of q1(x), . . . , qj(x). We obtain a triangular scheme also for the inverse
transformation:
1 = t11q1(x),
x = t12q1 + t22q2,
x2 = s13q1 + t23q2 + t33q3,
(4.1.9)
. . .
xn = t1nq1 + t2nq2 + t3nq3 + · · · + tnnqn,
where tjj ̸= 0 for all j, and the coefﬁcients form a lower triangular matrix T = S−1. Thus
every triangle family is a basis for Pm. (Recall the well-known fact that the inverse of a
triangular matrix with nonzero diagonal exists and is triangular.) Among interesting triangle
families are the shifted power basis (x −c)j, the Chebyshev polynomials Tj(x), and many
other families of orthogonal polynomials.
Atriangle family which is often very convenient for solving the interpolation problem
is the family of Newton polynomials
p1(x) = 1,
pj(x) = (x −x1)(x −x2) . . . (x −xj−1),
j = 2 : n,
(4.1.10)

354
Chapter 4. Interpolation and Approximation
which has unit leading coefﬁcients. Since pj(xk) = 0, if k < j we obtain, using the
representation
p(x) = c1p1 + c2p2(x) + c3p3(x) + · · · + cnpn(x),
(4.1.11)
lower triangular system Lc = f for the coefﬁcients, where
L =


1
1
(x2 −x1)
1
(x3 −x1)
(x3 −x1)(x3 −x2)
...
...
...
...
1
(xn −x1)
(xn −x1)(xn −x2)
· · ·
2n−1
j=1(xn −xj)


.
(4.1.12)
Hence the coefﬁcients can be computed by forward substitution. In the next section we
shall see how this basis leads to Newton’s interpolation formula. This is one of the best
interpolation formulas with respect to ﬂexibility, computational economy, and numerical
stability.
If a polynomial p(x) is given in the form (4.1.11), then it can be evaluated using only
n multiplications and 2n additions for a given numerical value x using the nested form
p(x) = (· · · (cn(x −xn−1) + cn−1)(x −xn−2)
+ · · · + c3)(x −x2) + c2)(x −x1) + c1.
This can be evaluated by a recursion formula similar to Horner’s rule (see Sec. 1.2.2).
Another basis of Pn that has many advantages is the Lagrange basis of polynomials
ℓj(x). If xi, i = 1 : n, are distinct interpolation points, these are
ℓj(x) =
n
&
i=1
i̸=j
(x −xi)
(xj −xi),
j = 1 : n.
(4.1.13)
These bases polynomials of degree n −1 satisfy
ℓj(xi) = δij =
%
1
if i = j,
0
if i ̸= j.
(4.1.14)
From this property directly follows Lagrange’s interpolation formula, which directly
displays the solution of the interpolation problem for n distinct points.127
Theorem 4.1.1.
The unique interpolation polynomial p ∈Pn interpolating the function f at the
distinct points xi, i = 1 : n, can be written
p(x) =
n

j=1
f (xj)
n
&
i=1
i̸=j
(x −xi)
(xj −xi).
(4.1.15)
127Lagrange published this formula in 1794.

4.1. The Interpolation Problem
355
It is an easy exercise to show that by l’Hôpital’s rule the Lagrange polynomials can
be written
ℓj(x) =
Mn(x)
(x −xj)M′n(xj),
Mn(x) =
n
&
i=1
(x −xi).
(4.1.16)
This property characterizes what, in a more general context, is known as a cardinal basis.
Lagrange’s interpolation formula has been widely regarded as being more suitable
for deriving theoretical results than for practical computation. However, in Sec. 4.2.3 two
modiﬁed forms of Lagrange’s interpolation formula will be given, which are also very
attractive computationally.
A natural extension of the interpolation problem is to determine a polynomial p(x) =
n
j=1 cjpj(x) ∈Pn that, in some sense, best ﬁts to the data (xi, f (xi)), i = 1 : m, where
m > n. Since the number of data points is larger than the number of parameters, the
corresponding linear system Pc = f is overdetermined and can typically be satisﬁed only
approximately. Overdetermination can be used to attain two different types of smoothing:
(i) to reduce the effect of random or other irregular errors in the values of the function;
(ii) to give the polynomial a smoother behavior between the grid points.
In least squares approximation one determines the coefﬁcient vector c so that the sum
of squared residuals
S(c) =
m

i=1
(p(xi) −f (xi))2
(4.1.17)
is minimized; see Sec. 1.4. This can in many applications be motivated by statistical ar-
guments; see the Gauss–Markov theorem (Theorem 1.4.1). It also leads to rather simple
computations. The conditions for the minimization are
∂S(c)
∂ck
= 2
m

i=1
pk(xi)

p(xi) −f (xi)

= 0,
k = 1 : n.
Astable method for discrete least squares polynomial approximation, based on using a basis
of orthogonal polynomials, will be given in Sec. 4.5.5.
We mention here that a large part of the theory of polynomial interpolation and ap-
proximation is valid also for generalized polynomials
u(x) =
n

k=1
akuk(x),
where {u1, u2, . . . , un} are continuous real-valued functions that form a Chebyshev system
on a closed ﬁnite interval [a, b]; see Sec. 4.5.7.
4.1.3
Conditioning of Polynomial Interpolation
Let p ∈Pn be the unique polynomial that interpolates the function values f at the distinct
points xj, j = 1 : n. Consider the problem to determine the value of p at the ﬁxed point ˜x.

356
Chapter 4. Interpolation and Approximation
With the terminology of Sec. 2.4.3 the input is the vector f = (f1, . . . , fn)T ∈Rn and the
output is the scalar p(˜x).
Using the Lagrange basis (4.1.13) we have
p(˜x) =
n

j=1
fjℓj(˜x),
ℓj(˜x) =
n
&
i=1
i̸=j
(˜x −xi)
(xj −xi).
If the data are perturbed by 4f , where |4f | ≤ϵ, then
|4p(˜x)| ≤ϵ
n

j=1
|ℓj(˜x)|
(4.1.18)
gives an upper bound for the perturbation in p(˜x).
The Lebesgue constant for polynomial interpolation at distinct points xi ∈[a, b],
i = 1 : n, is deﬁned as
Xn = max
a≤x≤b
n

j=1
|ℓj(x)|.
(4.1.19)
From (4.1.18) it follows that Xn can be interpreted as the condition number for the interpo-
lation problem with ˜x ∈[a, b]. We have the inequality
max
a≤x≤b |p(x)| ≤Xn max
a≤x≤b |f (x)|,
where equality can be obtained for f (x) piecewise linear and such that f (xi) = sign ℓi(xi).
For equally spaced points Xn grows at a rate proportional to 2n/(n log n); see Cheney
and Light [67, Chap. 3]. For the Chebyshev points in [−1, 1], on the other hand,
Xn ≤2
π log(n) + 1.
The basis consisting of the Lagrange polynomials for Chebyshev nodes is optimally condi-
tioned among all Lagrangian bases. It is indeed optimal among all polynomial bases in the
sense of attaining the optimal growth rate O(log n); see Gautschi [144].
Usually the interpolation problem is solved in two steps. First, a suitable basis pj(x),
j = 1 : n, for the space Pn is chosen and the coefﬁcients cj in the expansion
p(x) =
n

j=1
cjpj(x)
(4.1.20)
are determined. Second, the right-hand side of (4.1.20) is evaluated at some point x ∈[a, b].
Then we also need to consider the condition number κ(Pn) of the matrix Pn which maps
the coefﬁcient vector c = (c1, . . . , cn)T to f .
For the power basis Pn = V T
n . Many bounds and asymptotic estimates of κ(Vn) are
known; see [147, Sec. 1.3.2]; [199, Sec. 22.1]. For equidistant points xi = −1 + 2(i −
1)/(n −1) on [−1, 1], it holds that
κ∞(Vn) = ∥V −1
n ∥∞∥Vn∥∞∼π−1eπ/4(3.1)n;
(4.1.21)

Problems and Computer Exercises
357
e.g., κ∞(V20) ≈1.05 · 109. Other point distributions are even worse. For the harmonic
points xi = 1/i, i = 1 : n, we have κ∞(Vn) > nn+1, which is faster than exponential
growth! Surprisingly, some Vandermonde systems, which are so ill-conditioned that Gaus-
sian elimination with pivoting fails to produce a single correct digit, can be solved to full
relative accuracy by a fast algorithm given in Sec. 4.2.5.
For the Chebyshev points on [−1, 1]
xi = cos
2i −1
n
π
2

,
i = 1 : n,
(4.1.22)
i.e., the zeros of Tn−1(x), the Vandermonde matrix is much better conditioned and
κ∞(V ) ∼0.2533/4(1 +
√
2)n.
(4.1.23)
In contrast to the power basis the condition of bases of orthogonal polynomials exhibits
only polynomial growth in n. For Chebyshev polynomials pj = Tj on [−1, 1] it holds that
κ(Pn) ≤
√
2 n,
which is a big improvement on the power basis.
It should be stressed that κ(Pn) measures only the sensitivity of the coefﬁcients ci in
the polynomial p(x) = n
j=1 cjxj−1 to perturbations in the given data fi. It is possible
that even when these coefﬁcients are inaccurately determined, the interpolation polynomial
p(x) does still reproduce the true interpolation polynomial well. For further discussion of
these points, see Sec. 4.2.5 and [176].
Review Questions
4.1.1 The interpolation problem in Pn leads to a linear system V T c = f , where V is a
Vandermonde matrix. Write down the expression for the element vij.
4.1.2 What is meant by the method of undetermined coefﬁcients? Give an example.
4.1.3 What is meant by a triangle family q1(x), q2(x), . . . , qn(x) of polynomials? Are all
such families a basis for Pn?
4.1.4 What property characterizes a cardinal basis for Pn?
4.1.5 What good effects can be achieved by using overdetermination in polynomial inter-
polation?
4.1.6 How is the Lebesgue constant deﬁned and what is its signiﬁcance for the conditioning
of the polynomial interpolation problem?
Problems and Computer Exercises
4.1.1 (a) Study experimentally interpolation in Pn, n = 2 : 2 : 16, for f (x) = (3 + x)−1,
x ∈[−1, 1]. Use the linear system V T c = f and the power basis. Study both

358
Chapter 4. Interpolation and Approximation
equidistant points and Chebyshev points
xi = −1 + 2 i −1
n −1,
xi = cos
2i −1
n
π
2

,
i = 1 : n,
respectively. Plot the error curve y = |f (x) −p(x)| in semilogarithmic scale. For
the larger values of n, also conduct experiments to illuminate the effects of random
perturbations of the function values on the values of p(x).
(b) Also do a few experiments with a random vector f , for n = 16 and n = 8, in
order to compare the grid data and the order of magnitude of p(x) between the grid
points.
4.1.2 A warning for polynomial extrapolation of empirical functions, or . . .?
(a) Write a program c = polyapp(x, y, n) that ﬁnds the coefﬁcient vector c for a
polynomial in p ∈Pn, in a shifted power basis, such that yi ≈p(xi), i = 1 : m,
m ≥n, in the least squares sense, or study a program that does almost this.
(b) The following data show the development of the Swedish gross domestic pro-
duction (GDP), quoted from a table made by a group associated with the Swedish
Employer’s Confederation. (The data are expressed in prices of 1985 and scaled so
that the value for 1950 is 100.)
1950
1955
1960
1965
1970
1975
1980
1985
1990
100.0
117.7
139.3
179.3
219.3
249.1
267.5
291.5
326.4
1952
1957
1962
1967
1972
1977
1982
1987
1992
104.5
124.6
153.5
189.2
226.4
247.7
270.2
307.6
316.6
(c) For the upper pairs of data, compute and plot p(x), x ∈[1950, 2000] (say).
Mark the given data points. Do this for m = 9, and for (say) n = 9, and then for
n = 8 : −2 : 2. Store the results so that comparisons can be made afterward.
(d) Do the same for the lower pairs of data. Organize the plots so that interesting
comparisons become convenient. How well are the data points of one of the sets
interpolated by the results from the other set?
(e) Make forecasts for 1995 and 2000 with both data sets. Then, use a reduced data
set, e.g., for the years 1982 and earlier (so that m = 7), and compare the forecasts
for 1987 and 1992 with the given data. (Isn’t it a reasonable test for every suggested
forecast model to study its ability to predict the present from the past?)
(f) See if you obtain better results with the logarithms of the GDP values.
4.2
Interpolation Formulas and Algorithms
4.2.1
Newton’s Interpolation Formula
Newton’s interpolation formula uses the triangle family of Newton polynomials (4.1.10).
Let p ∈Pn be the unique polynomial interpolating a given function f (x) at n distinct real

4.2. Interpolation Formulas and Algorithms
359
or complex points x1, x2, . . . , xn. Suppose that an expansion
f (x) = c1 + c2(x −x1) + · · · + cn(x −x1)(x −x2) · · · (x −xn−1)
+ An(x)(x −x1)(x −x2) · · · (x −xn)
(4.2.1)
holds, where An(x) is the coefﬁcient of the remainder term. If f ∈Pn, we know from
Sec. 4.1.2 that such a formula holds with An(x) ≡0. We shall see that it is correct in
general.
For x = x1 we get c1 = f (x1). Further,
[x1, x]f = c2 + c3(x −x2) + · · · + cn(x −x2) · · · (x −xn−1)
+ An(x)(x −x2) · · · (x −xn),
where we have set
[x1, x]f = f (x) −f (x1)
x −x1
.
This shows that c2 = [x1, x2]f .
We now deﬁne divided differences128 for k > 1, by the recursion
[x1, . . . , xk−1xk, x]f = [x1, . . . , xk−1, x]f −[x1, . . . , xk−1, xk]f
x −xk
.
(4.2.2)
We obtain, for k = 2,
[x1, x2, x]f = c3 + c4(x −x3) + · · · + cn(x −x3) · · · (x −xn−1)
+ An(x)(x −x3) · · · (x −xn),
and c3 = [x1, x2, x3]f . By induction it follows that
ck = [x1, . . . , xk−1, xk]f,
k = 1 : n;
(4.2.3)
i.e., ck in (4.2.1) equals the (k −1)th divided difference of f . Further,
An(x) = [x1, x2, . . . , xn, x]f
for the coefﬁcient of the remainder term.
We are now ready to state Newton’s interpolation formula with exact remainder.
Theorem 4.2.1.
The unique interpolation polynomial p ∈Pn such that p(xi) = f (xi), i = 1 : n,
where the xi are distinct points, can be written as
p(x) =
n

k=1
ckMk−1(x),
(4.2.4)
where ck = [x1, x2, . . . , xk]f is the divided difference and
M0 = 1,
Mk(x) = Mk−1(x)(x −xk),
k = 1 : n.
(4.2.5)
128We prefer the modern notation [. . .]f to the older notations f [. . .] or f (. . .), since it emphasizes that [. . .] is
an operator. Note that the interpretation [x]f = f (x) is consistent with this.

360
Chapter 4. Interpolation and Approximation
The formula
f (x) =
n

k=1
[x1, x2, . . . , xk]f Mk−1(x) + [x1, x2, . . . , xn, x]f Mn(x)
(4.2.6)
is an identity, i.e., the exact remainder equals
f (x) −p(x) = [x1, x2, . . . , xn, x]f Mn(x).
(4.2.7)
These formulas are valid also for complex xi and x.
Proof. For f ∈Pn we know that (4.2.1) is correct, hence we can trust the coefﬁcients
ck = −[x1, . . . , xk]f .
Moreover, since p(xk) = f (xk), k = 1 : n, it follows that
[x1, x2, . . . , xk]p = [x1, x2, . . . , xk]f , and hence (4.2.4) holds.
We prove the identity (4.2.6) by induction. For n = 1 it is true because the right-hand
side becomes f (x1) + [x1, x]f · (x −x1) = f (x), which equals the left-hand side. Next
suppose that it is true for n = m. The difference between the right-hand side for n = m + 1
and n = m is

[x1, . . . , xm+1]f −[x1, . . . , xm, x]f

Mm(x) + [x1, . . . , xm+1, x]f Mm+1(x)
=

[x1, . . . , xm+1]f −[x1, . . . , xm, x]f + [x1, . . . , xm+1, x]f (x −xm+1)

Mm(x)
=

[x1, . . . , xm+1, x]f (xm+1 −x) + [x1, . . . , xm+1, x]f (x −xm+1)

Mm(x) = 0.
Hence the conjecture is true for n = m + 1.
Note that to obtain the interpolation polynomial of the next higher degree with New-
ton’s formula, we need only add a term similar to the last term, but involving a new divided
difference of one higher order.
In particular, if f ∈Pn, then it follows from (4.2.7) that
[x1, x2, . . . , xn, x]f = 0
∀x.
For x = xn+1, this equation is, by Theorem 3.3.4 the only nontrivial relation of the form
n+1
j=1 ajf (xj) = 0 that holds for all f ∈Pn.
Theorem 4.2.2.
Let xi, i = 1 : n, be pairwise distinct interpolation points. For every n, the divided
difference [x1, x2, . . . , xn]f is the unique coefﬁcient of xn−1 in the interpolation polynomial
p ∈Pn.129 Further, we have
[x1, x2, . . . , xn]f =
m

j=1
f (xj)
n
&
i=1
i̸=j
1
(xj −xi).
(4.2.8)
Proof. The ﬁrst statement follows from (4.2.3). The right-hand side of (4.2.8) is the coefﬁ-
cients
of
xn−1
obtained
by
using
Lagrange’s
interpolation
formula,
Theorem
4.1.1.
129Some authors take this as the deﬁnition of the divided difference.

4.2. Interpolation Formulas and Algorithms
361
It follows from (4.2.8) that the divided differences are symmetric functions of its
arguments and continuous functions of its arguments, as long as the points xi are distinct
and f (x) is continuous.
Assume k > i. By the deﬁnition of divided differences,
[xi+1, . . . , xk−1, xk, x]f = [xi+1, . . . , xk−1, x]f −[xi+1, . . . , xk−1, xk]f
x −xk
.
Now set x = xi and use the symmetry property (Theorem 4.2.2). We obtain the formula
[xi, xi+1, . . . , xk−1, xk]f = [xi, xi+1, . . . , xk−1]f −[xi+1, . . . , xk−1, xk]f
xi −xk
.
(4.2.9)
This formula can be used recursively to compute the divided differences. The computation
is conveniently arranged in the table below for n = 5 (recall that [xi]f = f (xi)).
x1
[x1]f
[x1, x2]f
x2
[x2]f
[x1, x2, x3]f
[x2, x3]f
[x1, x2, x3, x4]f
x3
[x3]f
[x2, x3, x4]f
[x1, x2, x3, x4, x5]f
[x3, x4]f
[x2, x3, x4, x5]f
x4
[x4]f
[x3, x4, x5]f
[x4, x5]f
x5
[x5]f
This table is called a divided-difference table. Each entry in the table is computed from
the two entries above and below in the previous column. Hence the complete table can be
constructed, for example, column by column or diagonal by diagonal.
The exact remainder term in Theorem 4.2.1 is not directly useful since unknown value
f (x) occurs in the divided difference [x1, . . . , xn, x]f . We now derive another expression
for the remainder term. In the following, int (x, x1, . . . , xn) denotes the smallest interval
that contains the points x and x1, . . . , xn.
Theorem 4.2.3 (The Remainder Term for Interpolation).
Let f be a given real function, with f (n) continuous in int (x, x1, x2, . . . , xn). Denote
by p the polynomial of degree n −1 for which p(xi) = f (xi), i = 1 : n. Then
f (x) −p(x) = [x1, x2, . . . , xn, x]f Mn(x) = f (n)(ξx)
n!
Mn(x),
(4.2.10)
Mn(x) = 2n
i=1(x −xi), for some point ξx ∈int (x, x1, x2, . . . , xn), and
[x1, x2, . . . , xn, xn+1]f = f (n)(ξ)
n!
,
ξ ∈int (x1, . . . , xn+1).
(4.2.11)
Proof. Following a proof due to Cauchy, we introduce a new variable z and set
G(z) = f (z) −p(z) −[x1, x2, . . . , xn, x]f Mn(z).

362
Chapter 4. Interpolation and Approximation
We know by Theorem 4.2.1 that
f (x) −p(x) = [x1, x2, . . . , xn, x]f Mn(x),
(4.2.12)
hence G(x) = 0. Then G(z) = 0 for z = x, x1, x2 . . . , xn. From repeated use of Rolle’s
theoremitfollowsthatthereexistsapointξx ∈int (x, x1, x2, . . . , xn)suchthatG(n)(ξx) = 0.
Since p(n)(z) = 0 and M(n)
n (z) = n! for all z, G(n)(z) = f (n)(z) −[x1, x2, . . . , xn, x]f n!.
If we now put z = ξx, we obtain
[x1, x2, . . . , xn, x]f = f (n)(ξx)
n!
.
(4.2.13)
Put this into the deﬁnition of G(z), and set z = x. Since G(x) = 0, the ﬁrst statement
follows. The second statement follows from (4.2.13) for x = xn+1.
Notice the similarity to the remainder term in Taylor’s formula. Notice also that the
right-hand side of (4.2.10) is zero at the grid points—as it should be.
In the proof of this theorem we have assumed that xi, x, and f (x) are real. In Sec. 4.3.1
we shall derive a remainder term for the general case that xi are complex interpolation points
and also consider the case when the points are allowed to coincide.
Theorem 4.2.4.
For equidistant points xi = x1 + (i −1)h, fi = f (xi), it holds that
[xi, xi+1, . . . , xi+k]f = 4kfi
hkk! .
(4.2.14)
Proof. The proof is obtained by induction, with the use of (4.2.9). The details are left to
the reader.
We have noted above that in the notation for the equidistant case ∇kfn ≈hkf (k),
while in the divided difference notation f [xn, xn−1, . . . , xn−k] ≈f (k)/k!. For the basis
functions of the interpolation formulas we have, respectively,
x
k

= O(1),
(x −xn)(x −xn−1) · · · (x −xn−k+1) = O(hk),
provided that x −xn−j = O(h), j = 0 : k −1.
We are now in a position to give a short proof of the important formula (3.3.4) that
we now formulate as a theorem.
Theorem 4.2.5.
Assume that f ∈Ck. Then
4kf (x) = hkf (k)(ζ),
ζ ∈[x, x + kh].
(4.2.15)
If f ∈Pk, then 4kf (x) = 0. Analogous results hold, mutatis mutandis, for backward and
central differences.

4.2. Interpolation Formulas and Algorithms
363
Proof.
Combine the result in Theorem 4.2.4 with (4.2.11), after making appropriate
substitutions.
Example 4.2.1.
Suppose we want to compute by linear interpolation the value f (x) at a point x =
x0 + θh, h = x1 −x0. Since θ(1 −θ) takes on its maximum value 1/4 for θ = 1/2, it
follows from (4.2.10) that for 0 ≤θ ≤1 the remainder satisﬁes
|f (x) −p(x)| ≤h2M2/8,
M2 =
max
x∈int [x0,x1] |f ′′(x)|.
(4.2.16)
If the values f0 and f1 are given to t correct decimal digits, then the roundoff error in
evaluating
p(x) = (1 −θ)f0 + θf1,
0 ≤θ ≤1,
is bounded by 1
210−t. Further, if h2M2
8
≤1
2 · 10−t, then the total error in p(x) is bounded
by 10−t.
This analysis motivates the rule of thumb that linear interpolation sufﬁces if |42fn|/8
is a tolerable truncation error.
To form the Newton interpolation polynomial we only need one diagonal of the
divided-difference table. It is not necessary to store the entire table. The following al-
gorithm replaces (overwrites) the given function values
fi = f (xi) = [x1]f,
i = 1 : n,
by the downward diagonal of divided differences of the divided-difference table,
[x1]f, [x1, x2]f, . . . , [x1, x2, . . . , xn]f.
At step j the jth column of the table is computed as follows:
% Compute downward diagonal of the divided-difference
% table. Function values are overwritten
for j = 1:n-1
for i = n:(-1):j+1
f(i) = (f(i) - f(j-1))/(x(i) - x(i-j));
end
end
Note that to avoid overwriting data needed later it is necessary to proceed from the
bottom of each column. The algorithm uses n(n−1)/2 divisions and n(n−1) subtractions.
Newton’s interpolation formula has the advantage that if it is not known in advance
how many interpolation points are needed to achieve the required accuracy, then one inter-
polation point can be added at a time. The following progressive algorithm computes the
divided-difference table one diagonal at a time. In the jth step, j = 2 : n, the entries
[xj−1, xj]f, . . . , [x1, x2, . . . , xj]f

364
Chapter 4. Interpolation and Approximation
on the upward diagonal of the divided-difference table overwrite the function values
fj−1, . . . , f1.
% Compute upward diagonal of the divided-difference
% table. Function values are overwritten
for j = 2:n
for i = j:(-1):2
f(i-1) = (f(i) - f(i-1))/(x(j) - x(i-1));
end
end
By Theorem 4.2.1 the Newton polynomial has the form
p(x) = c1 +
n

j=2
cj
j−1
&
i=1
(x −xi),
cj = [x1, . . . , xj]f.
(4.2.17)
Substituting z for x we have, by a simple generalization of Horner’s rule, that p(z) = b1(z),
where
bn = cn,
bi(z) = bi+1(z)(z −xi) + ci,
i = n −1 : −1 : 1.
(4.2.18)
This recursion can be used algebraically or to evaluate p(x) for a given numerical value
x = z. It is straightforward to show that in the latter case the computed result is the exact
value corresponding to slightly perturbed divided differences; cf. Problem 2.3.6.
The auxiliary quantities bn(z), . . . , b2(z) are of independent interest, since
p(x) = b1 + (x −z)

b2 +
n−1

j=2
bj+1φj−1(x)

.
(4.2.19)
Hence they give the coefﬁcients of the quotient polynomial in synthetic division of p(x)
with (x −z). The proof of this result is left as an exercise. Derivatives of a Newton
polynomial can be evaluated by repeated applications of the Horner scheme.
Example 4.2.2.
Compute the interpolation polynomial for the following table:
x1 = 1
0
2
x2 = 2
2
1
5
0
x3 = 4
12
1
8
x4 = 5
20
(the entries appearing in the Newton forward interpolation formula are boldface). We get
p(x) = 0 + 2(x −1) + 1(x −1)(x −2) + 0(x −1)(x −2)(x −4)
= x2 −x.

4.2. Interpolation Formulas and Algorithms
365
Note that for these particular data the unique interpolation polynomial in P4 actually belongs
to the subspace P3.
Example 4.2.3.
Set f (x; z) = 1/(z −x); x is the variable, z is a parameter, and both may be com-
plex. The following elementary, though remarkable, expansion can be proved directly by
induction.
1
z −x =
1
z −x1
+
x −x1
(z −x1)(z −x2) + · · · + (x −x1)(x −x2) · · · (x −xn−1)
(z −x1)(z −x2) · · · (z −xn)
+
(x −x1) · · · (x −xn)
(z −x1) · · · (z −xn)(z −x)
=
n

j=1
Mj−1(x)
Mj(z)
+
Mn(x)
Mn(z)(z −x).
(4.2.20)
When we match this with Newton’s interpolation formula we ﬁnd that
[x1, x2, . . . , xj]f (x; z) =
1
Mj(z),
(4.2.21)
[x1, x2, . . . , xj, x]f (x; z) =
1
Mj(z)(z −x).
(4.2.22)
These formulas can also be proved by induction, by working algebraically with 1/(z−x) in
the divided-difference table (Problem 4.2.4). See also Problem 3.3.3 (a) for the equidistant
case.
This is more than a particular example. Since 1/(z −x) is the kernel of Cauchy’s
integral (and several other integral representations), this expansion is easily generalized to
arbitrary analytic functions; see Sec. 4.3.2.
An interesting feature is that these formulas do not require that the points xi be distinct.
(They are consistent with the extension to nondistinct points that will be made in Sec. 4.3.1.)
Everything is continuous except if z = xi, i = 1 : n, or, of course, if z = x. If all the xi
coincide, we obtain a geometric series with a remainder.
For given interpolation points the divided differences in Newton’s interpolation for-
mula depend on the ordering in which the points xi are introduced. Mathematically all
orderings give the same unique interpolation polynomial. But the condition number for
the coefﬁcients c in the Newton polynomial can depend strongly on the ordering of the
interpolation points.
For simple everyday interpolation problems the increasing order
x1 < x2 < · · · < xn will give satisfactory results.
If the point ˜x where the polynomial is to be evaluated is known, then an ordering such
that
|˜x −x1| ≤|˜x −x2| ≤· · · ≤|˜x −xn|
can be recommended. In the equidistant case this corresponds to using Stirling’s interpola-
tion formula (3.3.39). In case convergence is slow and an interpolation polynomial of high

366
Chapter 4. Interpolation and Approximation
order has to be used, another suitable choice is the Leja ordering, deﬁned by
x1 = max
1≤i≤n |xi|,
j−1
&
k=1
|xj −xk| = max
i≥j
j−1
&
k=1
|xi −xk|,
j = 2 : n −1.
(4.2.23)
Let K be a compact set in the complex plane with a connected complement. Any sequence
of points ξ1, ξ2, . . . which satisﬁes the conditions
|ξ1| = max
ξ∈K |ξ|,
j−1
&
k=1
|ξj −ξk| = max
ξ∈K
j−1
&
k=1
|ξ −ξk|,
j = 2, 3, . . . ,
(4.2.24)
are Leja points for K. The points may not be uniquely deﬁned by (4.2.24). For a real
interval [a, b] the Leja points are distributed similarly to the Chebyshev points. The main
advantage of the Leja points is that it is easy to add new Leja points successively to an
already computed sequence of Leja points; see Reichel [299].
4.2.2
Inverse Interpolation
It often happens that one has a sequence of pairs {(xi, yi)} and wants to determine a point
where y(x) = c. We saw an example in the simulation of the motion of a ball (Sec. 1.5.2),
where the landing point was computed by linear inverse interpolation.
In general a natural approach is to reverse the roles of x and y, i.e., to compute the
inverse function x(y) for y = c by means of Newton’s interpolation formula with the divided
differences [yi, yi+1, . . . , yi+j]x. This is called inverse interpolation. It is convenient to
order the points so that
· · · < y5 < y3 < y1 < c < y2 < y4 < · · · .
This approach is successful if the function x(y) is suitable for local approximation by a
polynomial.
Sometimes, however, the function y(x) is much better suited for local approximation
by a polynomial than the inverse function x(y). Then we can instead, for some m, solve
the following equation:
y1 + [x1, x2]y · (x −x1) +
n−1

j=2
[x1, x2, . . . , xj+1]y Mj(x) = c.
(4.2.25)
Again it is convenient to order the points so that the root α comes in the middle, for example,
· · · < x5 < x3 < x1 < α < x2 < x4 < · · · .
Suppose that xi −x1 = O(h), i > 1, where h is some small parameter in the context
(usually some step size). Then
Mj(x) = O(hj),
M′
j(x) = O(hj−1).

4.2. Interpolation Formulas and Algorithms
367
The divided differences are O(1), and we assume that [x1, x2]y is bounded away from zero.
Then the terms of the sum decrease like hj.
Writing the equation in the form x = x1 + F(x), where
F(x) ≡
(c −y1) −n−1
j=2[x1, x2, . . . , xj+1]y Mj(x)
[x1, x2]y
,
(4.2.26)
we can use the iteration xk+1 = x1 + F(xk) to determine x. We ignore the sum to get the
ﬁrst guess x0; this means the same as linear inverse interpolation. We stop when xk and xk−1
are sufﬁciently close. A more careful termination criterion will be suggested in Chapter 6,
where the effect on the result of errors like the interpolation error is also discussed. From
the discussion of ﬁxed-point iteration in Sec. 1.1.1, we conclude that the iteration converges
with linear ratio equal to
F ′(x) ≈M′
2(x)[x1, x2, x3]y
[x1, x2]y
= O(h).
Thus, if h is small enough, the iterations converge rapidly. If more than two iterations are
needed, Aitken acceleration (Sec. 3.4.2) may be practical.
4.2.3
Barycentric Lagrange Interpolation
Lagrange’s interpolation formula was introduced in Sec. 4.1.2. For distinct real interpolation
points xi, i = 1 : n, this formula uses the cardinal basis of Pn consisting of the Lagrange
polynomials of degree n −1:
ℓj(x) =
n
&
i=1
i̸=j
(x −xi)
(xj −xi),
j = 1 : n,
(4.2.27)
with the property that
ℓj(xi) = δij =
%
1
if i = j,
0
if i ̸= j.
Quite often it is asserted that the Lagrange form is a bad choice for practical compu-
tations,130 since for each new value of x the functions ℓj(x) have to be recomputed at a cost
O(n2). Further, adding a new data point xn+1, fn+1 will require a new computation from
scratch. Therefore, it is concluded that Lagrange’s formula is not as efﬁcient as Newton’s
interpolation formula.
The above assertions are, however, not well-founded. The Lagrange representation
can easily be rewritten in two more attractive forms which are both eminently suitable for
computation; see Rutishauser [312] and Berrut and Trefethen [24]. The key idea is to take
out the common factor Mn(x) in (4.1.15) and introduce the support coefﬁcients
wj =
1
&
i=1
i̸=j
(xj −xi)
,
j = 1 : n.
(4.2.28)
130“For actual numerical interpolation Lagrange’s formula is, however, as a rule not very suitable.” (Stef-
fensen [330, p. 25].)

368
Chapter 4. Interpolation and Approximation
The Lagrange polynomials can then be written as
ℓj(x) = Mn(x)
wj
x −xj
.
Taking out the common factor gives the modiﬁed form of Lagrange’s interpolation formula:
p(x) = Mn(x)
n

j=1
wj
x −xj
f (xj).
(4.2.29)
Here wj depend only on the given points xj, j = 1 : n, and can be computed in 2(n −1)
ﬂops. This is slightly more than the 3
2n(n −1) ﬂops required to compute the divided
differences for Newton’s interpolation formula. Then, to evaluate p(x) from (4.2.29) for a
new value of x we only need to compute Mn(x) and wj/(x −xj), j = 1 : n, which just
requires O(n) operations.
The product factor Mn(x) in (4.2.29) can be eliminated as follows. Since the interpo-
lation formula is exact for f (x) ≡1, we have
1 = Mn(x)
n

j=1
wj
x −xj
.
Substituting this in (4.2.29),
p(x) =
n

j=1
wj
x −xj
f (xj)
n

j=1
wj
x −xj
if x ̸= xj,
j = 1 : n,
(4.2.30)
which is the barycentric form of Lagrange’s interpolation formula. This expresses the
value p(x) as a weighted mean of the values fi. (Note that the coefﬁcients wj/(x −xj)
need not be positive, so the term “barycentric” is not quite appropriate.)
The barycentric formula (4.2.30) has a beautiful symmetric form and is “eminently
suitable for machine computation” (Henrici [195, p. 237]). Unlike Newton’s interpolation
formula, it does not depend on how the nodes are ordered. The numerical stability of the two
modiﬁed Lagrange interpolation formulas is, contrary to what is often stated, very good.
Note that the interpolation property of p(x) is preserved even if the coefﬁcients wi are
perturbed, but then p(x) is usually no longer a polynomial but a rational function.
There seems to be a stability problem for the formula (4.2.30) when x is very close
to one of the interpolation points xi. In this case wi/(x −xi) will be very large and not
accurately computed because of the cancellation in the denominator. But this is in fact no
problem, since there will be exactly the same error in the denominator. Further, in case
4i = ﬂ(x −xi) is exactly zero, we can simply put 4i = u, where u is the unit roundoff;
see Theorem 2.2.2.
The barycentric form of Lagrange’s interpolation formula can be as efﬁciently updated
as Newton’s formula. Suppose the support coefﬁcients w(k−1)
i
, i = 1 : k −1, for the points
x1, . . . , xk−1 are known. Adding the point xk, the ﬁrst k −1 new support coefﬁcients can
be calculated from
w(k)
i
= w(k−1)
i
/(xi −xk),
i = 1 : k −1,

4.2. Interpolation Formulas and Algorithms
369
using (k −1) divisions and subtractions. Finally we have w(k)
k
= 1
 2k−1
i=1(xk −xi). The
computation of the support coefﬁcients is summarized in the following program:
% Compute support coefficients
w(1) = 1;
for k = 2:n
w(k) = 1;
for i = 1:k-1
w(i) = w(i)/(x(i) - x(k));
w(k) = w(k)/(x(k) - x(i));
end
end
Note that the support coefﬁcients wi do not depend on the function to be interpolated.
Once they are known interpolating a new function f only requires O(n) operations. This
contrasts favorably with Newton’s interpolation formula, which requires the calculation of
a new table of divided differences for each new function.
Suppose that we use interpolation points in an interval [a, b] of length 2C. Then as
n →∞the scale of the weights will grow or decay exponentially at the rate C−n. If n is
large or C is far from 1, the result may underﬂow or overﬂow even in IEEE double precision.
In such cases there may be a need to rescale the support coefﬁcients.
The computation of the support coefﬁcients can be done in only n(n−1) ﬂops by using
the relation (see Problem 4.2.5 and [318, Sec. 3.2.1]) n
i=1 wi = 0, n > 1, to compute
wn = n−1
i=1 wi. But using this identity destroys the symmetry and can lead to stability
problems for large n. Serious cancellation in the sum will occur whenever maxi |wi| is
much larger than |wn|. Hence the use of this identity is not recommended in general.
Theorem 4.2.6 (Higham [200]).
Assume that xi, fi, and x are ﬂoating-point numbers. Then the computed value ˜p(x)
of the interpolation polynomial using the modiﬁed Lagrange formula (4.2.29) satisﬁes
˜p(x) = Mn(x)
n

i=1
wi
x −xi
f (xi)
5(n+1)
&
j=1
(1 + δij)±1,
(4.2.31)
where |δij| ≤u.
Thus the formula (4.2.29) computes the exact value of an interpolating polynomial
corresponding to slightly perturbed function values f (xi). Hence this formula is backward
stable in the sense of Deﬁnition 2.4.10.
The barycentric formula is not backward stable. A forward error bound similar to
that for the modiﬁed formula but containing an extra term proportional to n
j=1 |ℓj(x)|
can be shown. Hence the barycentric formula can be signiﬁcantly less accurate than the
modiﬁed Lagrange formula (4.2.29) only for a poor choice of interpolation points with a
large Lebesgue constant Xn.
For various important distributions of interpolating points, it is possible to compute
the support coefﬁcients wi analytically.

370
Chapter 4. Interpolation and Approximation
Example 4.2.4.
For interpolation at the equidistant points x1, xi = x1 + (i −1)h, i = 2 : n, the
support coefﬁcients are
wi = 1

((xi −x1) · · · (xi −xi−1)(xi −xi+1) · · · (xi −xn))
=
(−1)n−i

hn−1(i −1)! (n −i)!

=
(−1)n−i
hn−1(n −1)!
n −1
i

.
In the barycentric formula (4.2.30) a common factor in the coefﬁcients wi cancels and we
may use instead the modiﬁed support coefﬁcients
w∗
i = (−1)i+1
n −1
i

.
(4.2.32)
For a given n these can be evaluated in only 2n operations using the recursion
w∗
1 = n −1,
w∗
i = w∗
i−1
n −i
i
,
i = 2 : n.
Example 4.2.5.
Explicit support coefﬁcients are also known for the Chebyshev points of the ﬁrst and
second kind on [−1, 1]. For the Chebyshev points of the ﬁrst kind they are
wi = (−1)i sin (2i −1)
n
π
2 ,
xi = cos (2i −1)
n
π
2 ,
i = 1 : n.
(4.2.33)
For the Chebyshev points of the second kind they are
wi = (−1)iδj,
xi = cos (i −1)
(n −1)π,
i = 1 : n,
(4.2.34)
where δj = 1/2 if i = 1 or i = n, and 1 otherwise. Note that all but two weights are equal.
This will be considered from another point of view in Sec. 4.6.
Foraninterval[a, b]theChebyshevpointscanbegeneratedbyalineartransformation.
The corresponding weight wi then gets multiplied by 2n(b −a)n. But this factor cancels
out in the barycentric formula, and there is no need to include it. Indeed, by not doing so
the risk of overﬂow or underﬂow, when |b −a| is far from 1 and n is large, is avoided.
The two examples above show that with equidistant or Chebyshev points only O(n)
operations are needed to get the weights wi. For these cases the barycentric formula seems
superior to all other interpolation formulas.
Lagrange’s interpolation formula can be used to compute the inverse of the Vander-
monde matrix V in (4.1.5) in O(n2) operations. If we set V −1 = W = (wij)n
i,j=1, then
WV = I, the ith row of which can be written
n

j=1
wijxj
k = δik,
k = 1 : n.

4.2. Interpolation Formulas and Algorithms
371
This is an interpolation problem that is solved by the Lagrange basis polynomial
ℓi(x) =
n
&
k=1
k̸=i
(x −xk)
(xi −xk) =
n

j=1
wijxj,
j = 1 : n.
(4.2.35)
Clearly V is nonsingular if and only if the points xi are distinct.
The elements wij in inverse Vandermonde matrix W = V −1 can be computed as
follows: First compute the coefﬁcients of the polynomial
Mn(x) = (x −x1)(x −x2) · · · (x −xn) =
n+1

j=1
ajxj−1.
This can be done by the following MATLAB script:
% Compute inverse Vandermonde matrix
a(1) = -x(1); a(2) = 1;
for k = 2:n
a(k+1) = 1;
for j = k:-1:2
a(j) = a(j-1) - x(k)*a(j);
end
a(1) = -x(k)*a(1);
end
Then compute the coefﬁcients of the polynomials
qi(x) = Mn(x)/(x −xi),
i = 1 : n,
by synthetic division (see Sec. 1.2.2). By (4.2.35) the n2 elements in W equal the coefﬁcients
of the Lagrange polynomials
ℓi(x) = qi(x)/qi(xi),
i = 1 : n.
Here the scalars qi(xi) are computed by Horner’s rule. The cost of computing the n2
elements in W by this algorithm is only 6n2 ﬂops.
4.2.4
Iterative Linear Interpolation
There are other recursive algorithms for interpolation. Of interest are those based on suc-
cessive linear interpolations. The basic formula is given in the following theorem.
Theorem 4.2.7.
Assume that the two polynomials pn−1(x) and qn−1(x), both in Pn−1, interpolate f (x)
at the points x1, . . . , xn−1 and x2, . . . , xn, respectively. If the n points x1, x2, . . . , xn−1, xn
are distinct, then
pn(x) = (x −x1)qn−1(x) −(x −xn)pn−1(x)
xn −x1
is the unique polynomial in Pn that interpolates f (x) at the m points x1, x2, . . . , xn−1, xn.

372
Chapter 4. Interpolation and Approximation
Proof. Since qn−1(x) and pn−1(x) both interpolate f (x) at the points x2, . . . , xn−1 and
(x −x1) −(x −xn)
xn −x1
= 1,
it follows that pn(x) also interpolates f (x) at these points. Further, pn(x1) = pn−1(x1)
and hence interpolates f (x) at x1. A similar argument shows that pn(x) interpolates f (x)
at x = xn. Hence pn(x) is the unique polynomial interpolating f (x) at the distinct points
x1, x2, . . . , xn.
A variety of schemes use Theorem 4.2.7 to construct successively higher order inter-
polation polynomials. Denote by Pj,j+1,...,k(x), k > j, the polynomial interpolating f (x)
at the points xj, xj+1, . . . , xk. The calculations in Neville’s algorithm can be arranged in a
triangular table.
x1
f (x1)
x2
f (x2)
P1,2(x)
x3
f (x3)
P2,3(x)
P1,2,3(x)
x4
f (x4)
P3,4(x)
P2,3,4(x)
P1,2,3,4(x)
...
...
...
...
...
xk
f (xk)
Pk−1,k(x)
Pk−2,k−1,k(x)
Pk−3,k−2,k−1,k(x)
. . .
P1,2,3,...,k
Any entry in this table is obtained as a linear combination of the entries to the left and
diagonally above in the preceding column.
Note that it is easy to add a new interpolation point in this scheme. To proceed only
the last row needs to be retained. This is convenient in applications where the function
values are generated sequentially and it is not known in advance how many values are to
be generated.
Neville’s algorithm is used, for example, in repeated Richardson extrapolation (see
Sec. 3.4.6), where polynomial extrapolation to x = 0 is to be carried out. Another use
of Neville’s algorithm is in iterative inverse interpolation; see Isaacson and Keller [208,
Chapter 6.2].
If it is known in advance that a ﬁxed number k of points are to be used, then one can
instead generate the table column by column. When one column has been evaluated the
preceding may be discarded.
Aitken’s algorithm uses another sequence of interpolants, as indicated in the table
below.
x1
f (x1)
x2
f (x2)
P1,2(x)
x3
f (x3)
P1,3(x)
P1,2,3(x)
x4
f (x4)
P1,4(x)
P1,2,4(x)
P1,2,3,4(x)
...
...
...
...
xk
f (xk)
P1,k(x)
P1,2,k(x)
P1,2,3,k(x)
. . .
P1,2,3,...,k

4.2. Interpolation Formulas and Algorithms
373
For a ﬁxed number k of points this table can be generated column by column. To add a new
point the upper diagonal f (x1), P1,2(x), P1,2,3(x), . . . , P1,2,...,k(x) needs to be saved. The
basic difference between these two procedures is that inAitken’s the interpolants in any row
use points with subscripts near 1, whereas Neville’s algorithm uses rows with subscripts
nearest n.
Neville’s and Aitken’s algorithms can easily be used in the case of multiple interpo-
lation points also. The modiﬁcation is similar to that in Newton’s interpolation method.
4.2.5
Fast Algorithms for Vandermonde Systems
Given distinct scalars x1, x2, . . . , xn, the coefﬁcients for the interpolating polynomial in the
power basis p = n
i=1 aixi−1 are given by the solution to the linear system
V T a = f,
(4.2.36)
where V is the Vandermonde matrix
V = V (x1, x2, . . . , xn) =


1
1
· · ·
1
x1
x2
· · ·
xn
...
...
· · ·
...
xn−1
1
xn−1
2
· · ·
xn−1
n

.
(4.2.37)
Vandermonde systems are appropriate to use for polynomial interpolation when the result
is to be expressed in the monomial basis. Often the Newton basis is a better choice. As
we shall see, even for solving the Vandermonde system (4.2.36) an algorithm based on
Newton’s interpolation formula is a much better choice than Gaussian elimination.
Newton’s interpolation formula gives the interpolation polynomial in the form
p(x) = c1p1(x) + c2p2(x) + · · · + cnpn(x),
where the basis polynomials are
p1(x) = 1,
pk(x) = (x −x1) · · · (x −xk−1),
k = 2 : n.
Here cj = [x1, . . . , xj−1]f and the divided differences can be recursively computed as
described in Sec. 4.2.1. Then the coefﬁcient vector a of p(x) in the power basis
p(x) = a1 + a2x + · · · + anxn−1
can be computed by Horner’s rule. Note that the matrix V T is never formed and only storage
for a few vectors is needed. It is easily veriﬁed that the operation count is 5
2n(n + 1) ﬂops.
The related primal Vandermonde system
Vy = b
(4.2.38)
arises in related problems of determining approximation of linear functionals.

374
Chapter 4. Interpolation and Approximation
Example 4.2.6.
We shall ﬁnd a formula for integrals of the form I(f ) =
 1
0 x−1/2f (x) dx that is
exact for f ∈Pn and uses values f (xi), i = 1 : n. Such integrals need a special treatment
because of the singularity at x = 0. Set µj =
 1
0 x−1/2xj−1 dx and introduce the row vector
µT = (µ1, µ2, . . . , µn). We have that f (x) ≈p(x) = n
i=1 cjxj−1, where V T c = f , and
I(f ) ≈
 1
0
x−1/2p(x) dx =
n

i=1
cjµj = µT V −T
n
f,
(4.2.39)
where Vn is the Vandermonde basis. This can be written I(f ) = aT f , where the coefﬁcient
vector is a = V −1
n µ, i.e., a is the solution to the primal Vandermonde system V a = µ.
Clearly the approach in Example 4.2.6 can be used for any linear functional. We
would also like to have a stable and efﬁcient method for solving the primal system. One
possibility would be to use the algorithm given in Sec. 4.2.3 which computes the inverse
V −1 in about 6n2 operations, and then form the product V −1b = y.
We shall now derive a more efﬁcient and accurate algorithm for solving primalVander-
monde systems. We start by expressing the solution of the dual problem in terms of a matrix
factorization. Using the power basis the unique polynomial satisfying the interpolation
conditions p(xi) = fi, i = 1 : n, is
p(x) = (1, x, . . . , xn−1)a,
where the coefﬁcient vector a satisﬁes the linear system V T a = f .
To derive a corresponding algorithm for solving primal Vandermonde systems the
above algorithm can be interpreted as a factorization of the matrix (V T )−1 into a product
of diagonal and lower bidiagonal matrices. Let
Dk = diag (1, . . . , 1, (xk+1 −x1), . . . , (xn −xn−k))
(4.2.40)
and deﬁne the matrices
Lk(x) =

Ik−1
0
0
Bn−k+1(x)

,
k = 1 : n −1,
(4.2.41)
where
Bp(x) =


1
−x
1
...
...
−x
1

∈Rp×p.
(4.2.42)
Then the dualVandermonde algorithm can be written in matrix terms as c = U T f , a = LT c,
where
U T = D−1
n−1Ln−1(1) · · · D−1
1 L1(1),
(4.2.43)
LT = LT
1 (x1)LT
2 (x2) · · · LT
n−1(xn−1).
(4.2.44)
Since a = V −T f = LT U T f , we have V −T = LT U T .

4.2. Interpolation Formulas and Algorithms
375
Algorithm 4.1. Fast Dual Vandermonde Solver.
function a = dvand(x,f);
n = length(x);
a = f;
% Compute divided differences of f(1:n).
for k = 1:n-1
for j = n:(-1):k+1
a(j) = (a(j) - a(j-1))/(x(j) - x(j-k));
end
end
% Compute coefficients using Horner’s scheme.
for k = n-1:(-1):1
for j = k:n-1
a(j) = a(j) - x(k)*a(j+1);
end
end
We can now obtain a fast algorithm for solving a primal Vandermonde system Vy = b
as follows. Transposing the matrix factorization of V −T gives V −1 = UL. Hence y =
V −1b = U(Lb) and the solution to the primal system can be computed from d = Lb,
y = Ud. Transposing (4.2.43)–(4.2.44) gives
L = Ln−1(xn−1) · · · L2(x2)L1(x1),
U = LT
1 (1)D−1
1
· · · LT
n−1(1)D−1
n−1.
This leads to an algorithm for solving primal Vandermonde systems. The operation count
and storage requirement of this are the same as for Algorithm 4.1.
Algorithm 4.2. Fast Primal Vandermonde Solver.
function y = pvand(x,b);
n = length(x);
y = b;
for k = 1:n-1
for j = n:(-1):k+1
y(j) = y(j) - x(k)*y(j-1);
end
end
for k = n-1:(-1):1
for j = k+1:n
y(j) = y(j)/(x(j) - x(j-k));
end
for j = k:n -1
y(j) = y(j) - y(j+1);
end
end

376
Chapter 4. Interpolation and Approximation
The given algorithms can be generalized to conﬂuent Vandermonde matrices (see
Example 4.3.1).
The above two algorithms are not only fast. They also give almost full relative
accuracy in the solution of some Vandermonde systems which are so ill-conditioned that
Gaussian elimination with complete pivoting fails to produce a single correct digit. This
was ﬁrst observed by Björck and Pereyra [31], from which the following example is taken.
Example 4.2.7.
Consider a primal Vandermonde system Vny = b, with
xi = 1/(i + 2),
bi = 1/2i−1,
i = 1 : n.
The exact solution can be shown to be
yi = (−1)i−1
n
i

(1 + i/2)n−1.
Let ¯yi be the solution computed by the primal Vandermonde algorithm and take as a measure
of the relative error en = max1≤i≤n |yi −¯yi|/|yi|. Using a hexadecimal ﬂoating-point
arithmetic with u = 16−13 = 2.22 · 10−16, the following results were obtained.
n
5
10
15
20
25
en/u
4
5
10
54
81
The computed solution has small componentwise relative error which is remarkable since,
for example, κ(V10) = 9 · 1013.
A forward error analysis given by Higham [198] explains the surprisingly favorable
results. If the points are positive and monotonically ordered,
0 < x1 < x2 < · · · < xn,
(4.2.45)
then the error in the solution ¯a of a Vandermonde system Vy = b computed by the primal
algorithm can be bounded as
|¯a −a| ≤5u|V −1| |b| + O(u2).
(4.2.46)
If the components of the right-hand side satisfy (−1)nbi ≥0, then |V −1| |b| = |V −1b|, and
this bound reduces to
|¯a −a| ≤5u|a| + O(u2);
(4.2.47)
i.e., the solution is computed with small relative error independent of the conditioning of
V . A similar result holds for the dual algorithm. These good results can be shown to be
related to the fact that when (4.2.45) holds, the matrix V (x1, x2, . . . , xn) is totally positive,
i.e., the determinant of every square’s submatrix of V is positive; see [135, 42, 95].
Fast Björck–Pereyra-type algorithms for Vandermonde-like matrices of the form
P = (pi(αj))n
i,j=1,

4.2. Interpolation Formulas and Algorithms
377
where pi(x), i = 1 : n, are basis polynomials in Pn that satisfy a three-term recurrence
relation, have also been developed; see Higham [199, Sec. 22.2]. Cauchy linear systems
Cx = b, where the elements of C have the special form
cij = 1/(xi + yj),
1 ≤i, j ≤n,
can also be solved with an O(n2) Björck–Pereyra-type algorithm; see Boros, Kailath, and
Olshevsky [42].
4.2.6
The Runge Phenomenon
The remainder term in interpolation is, according to Theorem 4.2.3, equal to
1
n!
n
&
i=1
(x −xi)f (n)(ξx).
Here ξx depends on x, but one can say that the error curve behaves for the most part like
a polynomial curve y = c 2n
i=1(x −xi). A similar curve is also typical for error curves
arising from least squares approximation. This contrasts sharply with the error curve for
Taylor approximation, whose behavior is described approximatively by y = c(x −x0)n.
It is natural to ask what the optimal placing of the interpolation points x1, . . . , xn
should be in order to minimize the maximum magnitude of Mn(x) = 2n
i=1(x −xi) in the
interval in which the formula is to be used. For the interval [−1, 1] the answer is given
directly by the minimax property (Lemma 3.2.4) of the Chebyshev polynomials—choose
Mn(x) = Tn(x)/2n−1.
Thus the interpolation points should be taken as the zeros of Tn(x). (In the case of an interval
[a, b] one makes the linear substitution x = 1
2(a + b) + 1
2(b −a)t.)
Example 4.2.8.
Use the same notations as before. For f (x) = xn the interpolation error becomes
f (x) −p(x) = Mn(x), because f (n)(x)/n! ≡1. Figure 4.2.1 shows the interpolation error
with n equidistant points on [−1, 1] and with n Chebyshev points on the same interval, i.e.,
xi = −1 + 2 i −1
n −1,
xi = cos
2i −1
n
π
2

,
respectively, for n = 6 and n = 12. The behavior of the error curves as shown in Figure 4.2.1
is rather typical for functions where f (n)(x) is slowly varying. Also note that the error
increases rapidly when x leaves the interval int(x1, x2, . . . , xn). In the equidistant case, the
error is also quite large in the outer parts of the interval.
Equidistant interpolation can give rise to convergence difﬁculties when the number
of interpolation points becomes large. This difﬁculty is often referred to as Runge’s phe-
nomenon, and we illustrate it in the following example. A more advanced discussion is
given in Sec. 4.3.5, by means of complex analysis.

378
Chapter 4. Interpolation and Approximation
−1
−0.5
0
0.5
1
−6
−5
−4
−3
−2
−1
0
1 x 10
−3
Figure 4.2.1. Error of interpolation in Pn for f (x) = xn, using n = 12: Cheby-
shev points (solid line) and equidistant points (dashed line).
Example 4.2.9.
The graph of the function
f =
1
1 + 25x2 = i
2

1
i + 5x +
1
i −5x

,
where i =
√
−1, is the continuous curve shown in Figure 4.2.2, and is approximated in
two different ways by a polynomial of degree 10 in [−1, 1]. The dashed curve has been
−1
−0.5
0
0.5
1
−0.5
0
0.5
1
1.5
2
Figure 4.2.2. Polynomial interpolation of 1/(1 + 25x2) in two ways using 11
points: equidistant points (dashed curve), Chebyshev abscissae (dashed-dotted curve).

4.2. Interpolation Formulas and Algorithms
379
determined by interpolation on the equidistant grid with m = 11 points
xi = −1 + 2(i −1)/(m −1),
i = 1 : m.
(4.2.48)
The dash-dot curve has been determined by interpolation at the Chebyshev points
xi = cos
2i −1
m
π
2

,
i = 1 : m.
(4.2.49)
The graph of the polynomial obtained from the equidistant grid has—unlike the graph of
f —a disturbing course between the grid points. The agreement with f near the ends of
the interval is especially bad, while near the center of the interval [−1
5, 1
5] the agreement is
fairly good. Such behavior is typical of equidistant interpolation of fairly high degree, and
can be explained theoretically.
The polynomial obtained from interpolation at Chebyshev points agrees much better
with f , but still is not good. The function f is not at all suited for approximation by one
polynomial over the entire interval. One would get a much better result using approximation
with piecewise polynomials; see Sec. 4.4.
Notice that the difference between the values of the two polynomials is much smaller
at the grid points of the equidistant grid than in certain points between the grid points,
especially in the outer parts of the interval. This intimates that the values which one gets
by equidistant interpolation with a polynomial of high degree can be very sensitive to
disturbances in the given values of the function. Put another way, equidistant interpolation
using polynomials of high degree is in some cases an ill-conditioned problem, especially in
the outer parts of the interval [x1, xm]. The effect is even worse if one extrapolates—i.e.,
if one computes values of the polynomial outside the grid. But equidistant interpolation
works well near the center of the interval.
Even with equidistant data one can often get a more well behaved curve by—instead
of interpolating—ﬁtting a polynomial of lower degree using the method of least squares.
Generally, if one chooses n < 2√m, then the polynomial ﬁt is quite well-conditioned, but
higher values of n should be avoided.131
If one intends to approximate a function in [−1, 1] and one can choose the points
at which the function is computed or measured, then one should choose the Chebyshev
points. Using these points, interpolation is a fairly well-conditioned problem in the entire
interval. The risk of disturbing surprises between the grid points is insigniﬁcant. One can
also conveniently ﬁt a polynomial of lower degree than n−1, if one wishes to smooth errors
in measurement; see Sec. 4.5.5.
Example 4.2.9 shows how important it is to study the course of the approximating
curve p∗(x) between the points which are used in the calculation before one accepts the
approximation. When one uses procedures for approximation for which one does not have a
complete theoretical analysis, one should make an experimental perturbational calculation.
In the above case such a calculation would very probably reveal that the interpolation
polynomial reacts quite strongly if the values of the function are disturbed by small amounts,
say ±10−3. This would give a basis for rejecting the unpleasant dashed curve in the example,
evenifoneknewnothingmoreaboutthefunctionthanitsvaluesattheequidistantgridpoints.
131This fact is related to the shape of the so-called Gram polynomials; see Sec. 4.5.5.

380
Chapter 4. Interpolation and Approximation
Review Questions
4.2.1 Prove the theorem which says that the interpolation problem for polynomials has a
unique solution.
4.2.2 When is linear interpolation sufﬁcient?
4.2.3 Derive Newton’s interpolation formula.
4.2.4 Derive Newton’s interpolation formula for the equidistant case, starting from New-
ton’s general interpolation formula. How is this formula easily remembered?
4.2.5 Derive the Lagrange interpolation formula. Show how it can be rewritten in barycen-
tric form. When is the latter form more efﬁcient to use?
4.2.6 Neville’s and Aitken’s interpolation algorithms both perform successive linear inter-
polation. What is the difference between these?
4.2.7 The fast algorithm in Sec. 4.2.5 for solving primal Vandermonde systems can give
surprisingly accurate results provided that the points x1, x2, . . . , xn satisfy certain
conditions. Which?
4.2.8 (a) Why is it important to study the course of the approximating curve p∗(x) between
the points which are used in the calculation before one accepts the approximation?
(b) What is a good choice of interpolation points in the interval [a, b], if one wants
to get a small error?
Problems and Computer Exercises
4.2.1 (a) Compute f (3) by quadratic interpolation in the following table:
x
1
2
4
5
f (x)
0
2
12
21
Use the points 1, 2, and 4, and the points 2, 4, and 5, and compare the results.
(b) Compute f (3) by cubic interpolation.
4.2.2 Compute f (0) using one of the interpolation formulas treated above on the following
table:
x
0.1
0.2
0.4
0.8
f (x)
0.64987
0.62055
0.56074
0.43609
The interpolation formula is here used for extrapolation. Use also Richardson ex-
trapolation and compare the results.
4.2.3 Work out the details of Example 4.2.3 (about divided differences for 1/(z −x)).
4.2.4 (a) Consider the two polynomials p(x) and q(x), both in Pn, which interpolate f (x)
at the points x1, . . . , xn, and x2, . . . , xn+1, respectively. Assume that {xi}n+1
i=1 is an
increasing sequence, and that f (n)(x) has constant sign in the interval [x1, xn+1].

4.3. Generalizations and Applications
381
Show that f (x) is contained between p(x) and q(x) for all x ∈[x1, xn+1].
(b) Suppose that f (x) = f1(x) −f2(x), where both f (n)
1 (x) and f (n)
2 (x) have the
same constant sign in [x1, xn+1]. Formulate and prove a kind of generalization of the
result in (a).
4.2.5 Using the barycentric formula (4.2.29) the interpolation polynomial can be written as
p(x) =
n

i=1
wif (xi)
m
&
j=1
j̸=i
(x −xj).
Show by taking f (x) ≡1 and equating the coefﬁcients for xn−1 on both sides that
the support coefﬁcients satisfy n
i=1 wi = 0.
4.3
Generalizations and Applications
4.3.1
Hermite Interpolation
Newton’s interpolation formula can also be used in generalized interpolation problems,
where one or more derivatives are matched at the interpolation points. This problem is
known as Hermite interpolation.132
Theorem 4.3.1.
Let {zi}m
i=1 be m distinct real or complex points. Let f (z) be a given real- or complex-
valued function that is deﬁned and has derivatives up to order ri−1 (ri ≥1) at zi. The
Hermite interpolation problem, to ﬁnd a polynomial p(z) of degree ≤n −1, where
n = m
i=1 ri such that p(z) and its ﬁrst ri −1 derivatives agree with those of f (z) at zi,
i.e.,
p(j)(zi) = f (j)(zi),
j = 0 : ri −1,
i = 1 : m,
(4.3.1)
is uniquely solvable. We use here the notation f (0)(z) for f (z).
Proof. Note that (4.3.1) are precisely n conditions on p(z). The conditions can be expressed
by a system of n linear equations for the coefﬁcients p, with respect to some basis. This has
a unique solution for any right-hand side, unless the corresponding homogeneous problem
has a nontrivial solution. Suppose that a polynomial p ∈Pn comes from such a solution of
the homogeneous problem, that is,
p(j)(zi) = 0,
i = 1 : m,
j = 0 : ri −1.
Then zi must be a zero of multiplicity ri of p(x), hence p(z) must have at least  ri = n
zeros (counting the multiplicities). But this is impossible, because the degree of p is less
than n. This contradiction proves the theorem.
Hermite interpolation can be viewed as the result of passages to the limit in interpo-
lation at n points, where for i = 1 : m, ri interpolation points coalesce into the point zi. We
132Charles Hermite (1822–1901), a French mathematician, made important contributions to number theory,
orthogonal polynomials, and elliptic functions.

382
Chapter 4. Interpolation and Approximation
say that the point zi has multiplicity ri. For example, the Taylor polynomial in Pn,
p(z) =
n−1

j=0
f (j)(z1)
j!
(z −z1)j,
(4.3.2)
interpolates f (z) at the point z1 with multiplicity n (or z1 is repeated n times).
Example 4.3.1.
Consider the problem of ﬁnding a polynomial p(x) ∈P4 that interpolates the function
f and its ﬁrst derivative f ′ at the two points z1 and z2, and also its second derivative at z1. In
the notations of Sec. 4.1.1 the linear system for the coefﬁcient vector c becomes V T c = f ,
where f = (f (z1), f ′(z1), f ′′(z1), f (z2), f ′(z2))T , and
V =


1
0
0
1
0
z1
1
0
z2
1
z2
1
2z1
2
z2
2
2z2
z3
1
3z2
1
6z1
z3
2
3z2
2
z4
1
4z3
1
12z2
1
z4
2
4z3
2


(4.3.3)
is a conﬂuent Vandermonde matrix. Note that the second, third, and ﬁfth column of V is
obtained by “differentiating” the previous column. From Theorem 4.3.1 we conclude that
such conﬂuent Vandermonde matrices are nonsingular. We remark that the fast algorithms
in Sec. 4.2.5 can be generalized to conﬂuent Vandermone systems; see [29].
For the determinant of the general conﬂuent Vandermonde matrix one can show that
det(V ) =
m
&
j=1
rj−1
&
n=0
n!
&
i<j
(zi −zj)rirj .
(4.3.4)
When there are gaps in the sequence of derivatives that are known at a point the
interpolation problem is called Birkhoff interpolation or lacunary interpolation. Such a
problem may not have a solution, as is illustrated by the following example.
Example 4.3.2.
Find a polynomial p ∈P3 that interpolates the data f = (f (−1), f ′(0), f (1))T . The
new feature is that f (0) is missing. If we use the power basis, then we obtain the linear
system
Mp =
 1
−1
1
0
1
0
1
1
1

.
The determinant is evidently zero, so there is no solution for most data. An explanation is
that hf ′ = µδf for all f ∈P3.
Newton’s interpolation formula can be generalized to the conﬂuent case quite easily.
We will require some continuity and differentiability properties for divided differences.

4.3. Generalizations and Applications
383
These can be obtained through an alternative expression for divided differences that we
now give.
Deﬁnition 4.3.2.
Aset S of points in C is called convex if for any z, u ∈S, the straight line {tz+(1−t)u |
t ∈(0, 1)} is also contained in S. The convex hull of a set S in Rd is the smallest convex
subset of C which contains S.
Let D be the convex hull of the set of points z, u1, . . . , un in C. Assume that f is
deﬁned in D and has that its nth derivative exists and is continuous on D. Set u0(z) = f (z)
and consider the functions
wk(z) =
 1
0
 t1
0
. . .
 tk−1
0
f (k),
u1 + (u2 −u1)t1 + · · · + (z −uk)tk
-
dtk · · · dt1, (4.3.5)
k = 1 : n. The argument of the integrand lies in the convex hull D, since
ζk = u1 + (z −u1)t1 + · · · + (z −uk)tk
= (1 −t1)u1 + (t1 −t2)u2 + · · · + (tk−1 −tk)uk + tkz
= λ1u1 + λ2u2 + · · · + λkuk + λk+1z,
where 1 ≤k ≤n. From 1 ≥t1 ≥t2 ≥· · · ≥tn ≥0, it follows that
k+1

i=1
λi = 1,
λi ≥0,
i = 1 : k + 1.
If in (4.3.5) we carry out the integration with respect to tk and express the right-hand
side using uk−1 we ﬁnd that the functions uk(z) can be deﬁned through the recursion
wk(z) = wk−1(z) −wk−1(xk)
z −uk
,
k = 1 : n,
(4.3.6)
with w0(z) = f (z). But this is the same recursion (4.3.5) that was used before to deﬁne the
divided differences and thus
[z, u1, . . . , uk]f =
 1
0
 t1
0
. . .
 tk−1
0
f (k),
u1 + (u2 −u1)t1 + · · · + (z −uk)tk
-
dtk · · · dt1,
(4.3.7)
holds for arbitrary distinct points z, u1, . . . , un.
Notice that the integrand on the right-hand side of (4.3.7) is a continuous function of
the variables z, u1, . . . , un and hence the right-hand side is a continuous function of these
variables. Thus, when the nth derivative of f exists and is continuous on D, (4.3.7) deﬁnes
the continuous extension of [z, u1, . . . , uk]f to the conﬂuent case.
From the continuity of the divided differences it follows that the remainder term for
interpolation given in Theorem 4.2.3 remains valid for Hermitian interpolation. Provided
the points x, z1, . . . , zm are real we have
f (x) −p(x) = f (n)(ξx)
n!
Mn(x),
Mn(x) =
m
&
i=1
(x −zi)ri,
(4.3.8)

384
Chapter 4. Interpolation and Approximation
with ξx ∈int (x, z1, . . . , zm). From (4.3.7) follows
(([z, x1, . . . , xk]f
(( ≤max
z∈D
((f (n)(z)
((
 1
0
 t1
0
. . .
 tn−1
0
dtn · · · dt1
(4.3.9)
= 1
n! max
z∈D
((f (n)(z)
((,
(4.3.10)
which can be used to give an upper bound for the remainder in polynomial interpolation for
arbitrary interpolation points.
From (4.3.6) it follows that the divided difference [u1, . . . , uk+s, x]f is equal to the
divided difference of wk(x) at [uk+1, . . . , uk+s, x]f , so that by (4.3.7) we can write
wk+s(z) =
 1
0
. . .
 ts−1
0
u(s)
k
,
uk+1 + (uk+2 −uk+1)t1 + · · · + (z −uk+s)tk
-
dts · · · dt1.
If all points uk+1, . . . , uk+s all tend to the limit z and wk(z) has a continuous sth derivative
at z, then it holds that
wk+s(z) = w(s)
k (z)
 1
0
. . .
 ts−1
0
dts · · · dt1 = w(s)
k (z)
s!
.
It can be shown that if f ∈Ck, the divided differences belong to Ck+1−max ri, and that the
interpolation polynomial has this kind of differentiability with respect to the ui (nota bene,
if the “groups” do not coalesce further).
Example 4.3.3.
As established above, the usual recurrence formula for divided differences can still
be used for the construction of the divided-difference table in case of multiple points. The
limit process is just applied to the divided differences, for example,
[x0, x0]f = lim
x1→x0
f (x1) −f (x0)
x1 −x0
= f ′(x0),
[x0, x0, x1]f = [x0, x0]f −[x0, x1]f
x0 −x1
= f ′(x0) −[x0, x1]f
x0 −x1
.
For the interpolation problem considered in Example 4.3.1 we construct the generalized
divided-difference table, where x1 ̸= x0.
x0
f0
f ′
0
x0
f0
1
2f ′′
0
f ′
0
[x0, x0, x0, x1]f
x0
f0
[x0, x0, x1]f
[x0, x0, x0, x1, x1]f
[x0, x1]f
[x0, x0, x1, x1]f
x1
f1
[x0, x1, x1]f
f ′
1
x1
f1

4.3. Generalizations and Applications
385
The interpolating polynomial is
p(x) = f0 + (x −x0)f ′
0 + (x −x0)2 1
2f ′′
0 + (x −x0)3[x0, x0, x0, x1]f,
+ (x −x0)3(x −x1)[x0, x0, x0, x1, x1]f,
and the remainder is
f (x) −p(x) = [x0, x0, x0, x1, x1, x]f (x −x0)3(x −x1)2
= f (5)(ξx)(x −x0)3(x −x1)2/5.
An important case of the Hermite interpolation problem is when the given data are
fi = f (xi), f ′
i = f ′(xi), i = 0, 1. We can then write the interpolation polynomial as
p(x) = f0 + (x −x0)[x0, x1]f + (x −x0)(x −x1)[x0, x0, x1]f
+ (x −x0)2(x −x1)[x0, x0, x1, x1]f.
Set x1 = x0 + h and x = x0 + θh, and denote the remainder f (x) −p(x) by RT . Then one
can show (Problem 4.3.1) that
p(x) = f0 + θ4f0 + θ(1 −θ)(hf ′
0 −4f0)
−θ2(1 −θ)
*
(hf ′
0 −4f0) + (hf ′
1 −4f0)
+
(4.3.11)
= (1 −θ)f0 + θf1 + θ(1 −θ)
*
(1 −θ)(hf ′
0 −4f0) −θ(hf ′
1 −4f0)
+
.
For x ∈[x0, x1] we get the error bound
|f (x) −p(x)| ≤
1
384h4 max
x∈[x0,x1] |f (4)(x)|.
(4.3.12)
Setting t = 1/2, we get the useful approximation formula
f
1
2(x0 + x1)

≈1
2(f0 + f1) + 1
8h(f ′
0 −f ′
1).
(4.3.13)
4.3.2
Complex Analysis in Polynomial Interpolation
We shall encounter multivalued functions: the logarithm and the square root. For each of
these we choose that branch which is positive for large positive values of the argument z.
They will appear in such contexts that we can then keep them nonambiguous by forbidding
z to cross the interval [−1, 1]. (We can, however, allow z to approach that interval.)
We ﬁrst consider the general problem of interpolation of an analytic function, at an
arbitrary sequence of nodes u1, u2, . . . , un in C. Multiple nodes are allowed. Set
Mn(z) = (z −u1)(z −u2) · · · (z −un),
z, uj ∈C.
Let D be a simply connected open domain in C that contains the point u and the nodes.
The interpolation problem is to ﬁnd the polynomial p∗∈Pn, that is determined by the

386
Chapter 4. Interpolation and Approximation
conditions p∗(uj) = f (uj), j = 1 : n, or the appropriate Hermite interpolation problem in
the case of multiple nodes. We know that p∗depends linearly on f . In other words, there
exists a linear mapping Ln from some appropriate function space so that p∗= Lnf .
Assume that f is an analytic function in the closure of D, perhaps except for a ﬁnite
number of poles p. A pole must not be a node. Recall the elementary identity (4.2.20) in
Example 4.2.3,
1
z −u =
n

j=1
Mj−1(u)
Mj(z)
+
Mn(u)
Mn(z)(z −u) ,
(4.3.14)
which is valid also for multiple nodes. Introduce the linear operator Kn,
(Knf )(u) =
1
2πi

∂D
Mn(u)f (z)
Mn(z)(z −u) dz,
(4.3.15)
multiply the above identity by f (z)/(2πi), and integrate along the boundary of D to get
1
2πi

∂D
f (z)
z −u dz =
n

j=1
Mj−1(u) 1
2πi

∂D
f (z)
Mj(z) dz + (Knf )(u) .
(4.3.16)
First, assume that f has no poles in D. By applying the residue theorem to the ﬁrst two
integrals, we note that the equation has the same structure as Newton’s unique interpolation
formula with exact remainder (4.2.6),
f (u) =
n

j=1
Mj−1(u)[u1, . . . , uj]f + Mn(u)[u1, u2, . . . , un, u]f.
Matching terms in the last two formulas we obtain
(f −Lnf )(u) = (Knf )(u),
and, if f is analytic in D, also the formula for the divided-difference,
[u1, u2, . . . , uj]f =
1
2πi

∂D
f (z) dz
(z −u1) · · · (z −uj).
(4.3.17)
From this we obtain an upper bound for the divided-difference
(([u1, u2, . . . , uj]f
(( = L
2π
maxz∈∂D |f (z)|
minz∈∂D |(z −u1) · · · (z −uj)|,
(4.3.18)
where L =

∂D |dz|.
If there are poles p ∈D with residues resf (p), we must add 
p resf (p)/(z −p) to
the left-hand side of (4.3.17) and 
p resf (p) 
j Mj−1(u)/Mj(p) to the right-hand side.
By (4.3.14) this is, however, equivalent to subtracting

p
resf (p)Mn(u)/Mn(p)(p −u)
from the right-hand side. This yields the following theorem.

4.3. Generalizations and Applications
387
Theorem 4.3.3.
Assume that f is analytic in the closure of the open region D, perhaps except for a
ﬁnite number of poles, p ∈D, with residues resf (p). D also contains the interpolation
points u1, u2, . . . , un, as well as the point u. Multiple nodes are allowed. The point u and
the poles p must not be nodes, and p ̸= u.
Then the interpolation error can be expressed as a complex integral,
(f −Lnf )(u) = (Knf )(u) −Mn(u)

p
resf (p)
Mn(p)(p −u),
(4.3.19)
where Kn is deﬁned by (4.3.15) and the sum (which may be void) is extended over the poles
of f in D.
This theorem is valid when the interpolation points uj are in the complex plane,
although we shall here mainly apply it to the case when they are located in the interval
[−1, 1]. An important feature is that this expression for the interpolation error requires no
knowledge of f (n)(z).
For the case of distinct nodes we have, by the residue theorem,
(Knf )(u) =
n

j=1
Mn(u)
(uj −u)M′n(uj)f (uj) + f (u),
(4.3.20)
where the sum, with reversed sign, is Lagrange’s form of the interpolation polynomial
Lnf (u).
Example 4.3.4 (Chebyshev Interpolation).
In Chebyshev interpolation on the interval [−1, 1], the nodes are the zeros of the
Chebyshev polynomials Tn(u), and
Mn(u) = 21−nTn(u).
Recall the notation and results in the discussion of Chebyshev expansions in Sec. 3.2.3.
In this example we assume that f (u) is analytic in D. We shall, by means of the
integral Knf , show that it yields almost as accurate an approximation as the ﬁrst n terms of
the Chebyshev expansion of the same function.
Let D = ER, where ER is the ellipse with foci at ±1, and where R is the sum of
the semiaxis. Let z ∈∂ER and u ∈[−1, 1]. Then |Tn(u)| ≤1, and it can be shown
(Problem 4.3.13) that
|Tn(z)| ≥1
2(Rn −R−n),
|z −u| ≥aR −1,

∂ER
|dz| ≤2πaR.
If we assume that |f (z)| ≤MR for z ∈∂ER, a straightforward estimation of the line integral
(4.3.20) gives
|f (u) −(Lnf )(u)| = |(Knf )(u)| ≤1
2π
21−nMR2πaR
21−n 1
2(Rn −R−n)(aR −1)
≤
2MRaRR−n
(1 −R−2n)(aR −1).
(4.3.21)

388
Chapter 4. Interpolation and Approximation
We see that the interpolation error converges at the same exponential rate as the truncation
error of the Chebyshev expansion (see Sec. 3.2.3). If f (z) has a singularity arbitrarily close
to the interval [−1, 1], then R ≈1, and the exponential convergence rate will be very poor.
This above analysis can be extended to the case of multiple poles by including higher
derivatives of f .
This yields the general Lagrange formula valid also when there are
interpolation points of multiplicity greater than one:
p(u) =
m

i=1
3 ri−1

k=0
1
k!
dk
duk
f (u)
m
&
q=1
q̸=i
(u −uq)rq
(((((
u=ui
(u −ui)k
4 m
&
j=1
j̸=i
(u −uj)rj .
(4.3.22)
Clearly, when ri = 1 for all i = 1 : m, this formula equals the usual Lagrange interpolation
formula. It can be written in the simpler form
p(u) =
n

i=1
ri−1

k=0
f (k)(ui)Lik(u),
(4.3.23)
where Lik(u) are generalized Lagrange polynomials. These can be deﬁned starting from
the auxiliary polynomials
lik(u) = (u −ui)k
k!
n
&
j=1
j̸=i
 u −uj
ui −uj
rj
,
i = 1 : m,
k = 0 : ri −1.
Set Li,ri−1 = li,ri−1, i = 1 : m, and form recursively
Lik(u) = lik(u) −
ri−1

ν=k+1
l(ν)
ik (ui)Li,ν(u),
k = ri −2 : −1 : 0.
It can be shown by induction that
L(σ)
ik (uj) =
! 1
if i = j and k = σ,
0
otherwise.
Hence the Lik are indeed the appropriate polynomials.
Example 4.3.5.
When ri = 2, i = 1 : n, the Hermite interpolating polynomial is the polynomial of
degree less than n = 2m, which agrees with f (u) and f ′(u) at u = ui, i = 1 : m. We have
p(u) =
n

i=1
(f (ui)Li0(u) + f ′(ui)Li1(u)),
where Lik(u) can be written in the form
Li1(u) = (u −ui)li(u)2,
Li0(u) = (1 −2l′
i(ui)(u −ui))li(u)2,

4.3. Generalizations and Applications
389
and li(u), i = 1 : m, are the elementary Lagrange polynomials:
lik(u) = (u −ui)k
k!
n
&
j=1
j̸=i
 u −uj
ui −uj
rj
,
i = 1 : n,
k = 0 : ri −1.
4.3.3
Rational Interpolation
The rational interpolation problem is to determine a rational function
rm,n(z) = Pm(z)
Qn(z) ≡
m
j=0 pjzj
n
j=0 qjzj ,
(4.3.24)
withnumeratorofdegreemanddenominatorofdegreensothatatdistinctpointsx0, x1, . . . , xn
agrees with a function f
rm,n(xi) = fi,
i = 0 : N,
N = m + n.
(4.3.25)
Rational approximation is often superior to polynomial approximation in the neighborhood
of a point at which the function has a singularity. Since the coefﬁcients can be determined
only up to a common nonzero factor, the number of unknown constants in (4.3.24) equals
the number of interpolation points N + 1 = m + n + 1.
A necessary condition for (4.3.25) to hold clearly is that the linearized condition
Pm(xi) −fiQn(xi) = 0,
i = 0 : N,
(4.3.26)
is satisﬁed, i.e., for i = 0 : m + n,
p0xi + p1xi + · · · + pmxm
i −fi(q0xi + q1xi + · · · + qnxn
i ) = 0.
(4.3.27)
This is a homogeneous linear system of (m+n+1) equations for the (m+n+2) coefﬁcients
in Pm and Qn. If we introduce the Vandermonde matrices
A =


1
x0
. . .
xm
0
1
x1
. . .
xm
1
...
...
...
...
1
xN
. . .
xm
N

,
B =


1
x0
. . .
xn
0
1
x1
. . .
xn
1
...
...
...
...
1
xN
. . .
xn
N

,
this system can be written in matrix form as
( A
FB )

p
q

= 0,
F = diag (f0, f1, . . . , fN),
(4.3.28)
where p = (p0, p1, . . . , pm)T , q = (q0, q1, . . . , qn)T . This is a homogeneous linear system
of N + 1 equations in N + 2 unknowns. Such a system always has a nontrivial solution.
Moreover, since A has full column rank, we must have q ̸= 0 for such a solution.

390
Chapter 4. Interpolation and Approximation
We note that the rational interpolant is fully determined by the denominator polyno-
mial. By (4.3.26) Pm(x) is the unique polynomial determined by the interpolation conditions
P(xi) = fiQ(xi),
i = 1 : m.
While for polynomial interpolation there is always a unique solution to the interpola-
tion problem, this cannot be guaranteed for rational interpolation, as shown by the example
below. A further drawback is that the denominator polynomial Q(x) may turn out to have
zeros in the interval of interpolation.
Example 4.3.6.
Assume that we want to interpolate the four points
x
0
1
2
3
y
2
3/2
4/5
1/2
by a rational function
r(x) = p0 + p1x + p2x2
q0 + q1x
.
Then we must solve the homogeneous linear system


1
0
0
1
1
1
1
2
4
1
3
9


 p0
p1
p2

−


2
0
3/2
3/2
4/5
8/5
1/2
3/2



q0
q1

= 0.
Setting p2 = 1 we ﬁnd the solution p0 = 8, p1 = −6, q0 = 4, q1 = −2. The corresponding
rational function
r(x) = 8 −6x + x2
4 −2x
= (4 −x)(2 −x)
2(2 −x)
has the common factor (2 −x) and is reducible to f2,1 = (4 −x)/2. The original form is
indeterminate 0/0 at x = 2, while the reduced form does not take on the prescribed value
at x = 2.
As shown in the above example, for given data (x0, f0), . . . , (xn, fn) there can be
certain points xj where the given function value fj cannot be attained. Such a point xj is
called unattainable. This can occur only if xj is a zero of the denominator Qn(x). From
(4.3.26) it also follows that Pm(xj) = 0. Hence the polynomials P(x) and Q(x) have a
common factor (x −xj)d, where d is chosen maximal. The polynomials pair
P ∗(x) =
P(x)
(x −xj)d ,
Q∗(x) =
Q(x)
(x −xj)d
(4.3.29)
then satisﬁes (4.3.26) for all points xk ̸= xj. Since d was chosen maximal it holds that
Q∗(xj) ̸= 0, and the value P ∗(xj)/Q∗(xj) must be ﬁnite. But since when Qn(xj) = 0,
(4.3.26) is satisﬁed for any choice of fj, one cannot expect the rational function to interpolate
a particular value at xj.

4.3. Generalizations and Applications
391
If there are unattainable points, then the polynomials deﬁned in (4.3.29) only solve
the linearized equations in the attainable points. It can also happen that the polynomials
given by the linearized equations have a common factor (x −x∗)d, d ≥1, with x∗̸= xi,
i = 1 : n. In this case all polynomials of the form
P ∗(x) =
P(x)
(x −xj)ν ,
Q∗(x) =
Q(x)
(x −xj)ν ,
ν = 0 : d,
satisfy the linearized system and the matrix ( A
FB ) in (4.3.28) has at most rank
N + 1 −d. Conversely we have the following result.
Theorem 4.3.4.
If the rank of the matrix ( A
FB ) equals N + 1 −d, then there exists a unique
solution p, q corresponding to polynomials P ∗and Q∗with degrees at most m −d and
n −d. Further, all solutions have the form
r(x) = s(x)P ∗(x)
s(x)Q∗(x),
where s(x) is a polynomial of degree at most d. A point xj is unattainable if and only if
Q∗(xj) = 0.
Proof. See Schaback and Werner [313, Theorem 12.1.8].
Another complication of rational interpolation is that zeros may occur in Qn(x), which
are not common to Pm(x). These zeros correspond to poles in r(x), which if they lie inside
the interval [min xk, max xk] may cause trouble. In general it is not possible to determine a
priori if the given data (xk, fk) will give rise to such poles. Neither do the coefﬁcients in
the representation of r(x) give any hint of such occurrences.
An algorithm similar to Newton’s algorithm can be used for ﬁnding rational inter-
polants in continued fraction form. Set v0(x) = f (x), and use a sequence of substitutions:
vk(x) = vk(xk) + x −xk
vk+1(x),
k = 0, 1, 2, . . . .
(4.3.30)
The ﬁrst two substitutions give
f (x) = v0(x) = v0(x0) + x −x0
v1(x) = v0(x0) +
x −x0
v1(x1) + x −x1
v2(x)
.
In general this gives a continued fraction
f (x) = a0 +
x −x0
a1 +
x −x1
a2 + x −x2
a3+
. . . = a0 + x −x1
a1+
x −x2
a2+
x −x3
a3+ . . . ,
(4.3.31)
where ak = vk(xk), and we have used the compact notation introduced in Sec. 3.5.1. This
becomes an identity if the expansion is terminated by replacing an in the last denominator

392
Chapter 4. Interpolation and Approximation
by an + (x −xn)/vn+1(x). If we set x = xk, k ≤n, then the fraction terminates before
the residual (x −xn)/vn+1(x) is introduced. This means that setting 1/vk+1 = 0 will give
a rational function which agrees with f (x) at the points xi, i = 0 : k ≤n, assuming
that the constants a0, . . . , ak exist. These continued fractions give a sequence of rational
approximations fk,k, fk+1,k, k = 0, 1, 2, . . . .
Introducing the notation
vk(x) = [x0, x1, . . . , xk−1, x]φ
(4.3.32)
we have ak = [x0, x1, . . . , xk−1, xk]φ. Then by (4.3.30) we have
[x]φ = f (x),
[x0, x]φ =
x −x0
[x]φ −[x0]φ =
x −x0
f (x) −f (x0),
[x0, x1, x]φ =
x −x1
[x0, x]φ −[x0, x1]φ ,
and in general
[x0, x1, . . . , xk−1, x]φ =
x −xk−1
[x0, . . . , xk−2, x]φ −[x0, . . . , xk−2, xk−1]φ .
(4.3.33)
Therefore, we also have
ak =
xk −xk−1
[x0, . . . , xk−2, xk]φ −[x0, . . . , xk−2, xk−1]φ .
(4.3.34)
We call the quantity deﬁned by (4.3.34) the kth inverse divided difference of f (x). Note
that certain inverse differences can become inﬁnite if the denominator vanishes. They are,
in general, symmetrical only in their last two arguments.
The inverse divided differences of a function f (x) can conveniently be computed
recursively and arranged in a table similar to the divided-difference table.
x1
f (x1)
[x1]φ
[x1, x2]φ
x2
f (x2)
[x2]φ
[x1, x2, x3]φ
[x2, x3]φ
[x1, x2, x3, x4]φ
x3
f (x3)
[x3]φ
[x2, x3, x4]φ
[x3, x4]φ
x4
f (x4)
[x4]φ
Here the upper diagonal elements are the desired coefﬁcients in the expansion (4.3.31).

4.3. Generalizations and Applications
393
Example 4.3.7.
Assume that we want to interpolate the four points given in Example 4.3.6 and the
additional points (4, 6/17). Forming the inverse differences we get the following table.
xi
fi
φ1
φ2
φ3
φ4
0
2
−2
1
3/2
3
5/3
0
2
4/5
∞
−5
−2
−1/5
3
1/2
−7
−17/7
4
6/17
This gives a sequence of rational approximations. If we terminate the expansion
f2,2 = 2 +
x
−2+
x −1
3+
x −2
0+
x −3
−5
after a3 we recover the solution of the previous example. Note that the degeneracy of the
approximation is shown by the entry a3 = 0. Adding the last fraction gives the (degenerate)
approximation
f2,2 = 2 + x
1 + x2 .
It is veriﬁed directly that this rational function interpolates all the given points.
Because the inverse differences are not symmetric in all their arguments the reciprocal
differences are often preferred. These are recursively deﬁned by
[xi]ρ = fi,
[xi, xi+1]ρ = xi −xi+1
fi −fi+1
,
(4.3.35)
[xi, xi+1, . . . , xi+k]ρ =
xi −xi+k
[xi, . . . , xi+k−1]ρ −[xi+1, . . . , xi+k]ρ
+ [xi+1, . . . , xi+k−1]ρ.
(4.3.36)
See Hildebrand [201, p. 406]. While this formula is less simple than (4.3.34), the reciprocal
differences are symmetric functions of all their arguments. The symmetry permits the
calculation of the kth reciprocal difference from any two (k −1)th reciprocal differences
having k−1 arguments in common, together with the (k−2)th reciprocal difference formed
with this argument. Using (4.3.36) a reciprocal difference table may be constructed.
The coefﬁcients in the continued fraction (4.3.31) can then be determined by an
interpolation formula due to Thiele [350]:
a0 = f0,
a1 = [x0, x1]ρ,
a2 = [x0, x1, x2]ρ −f0,
a3 = [x0, x1, x2, x3]ρ −[x0, x1]ρ, . . . .
The formulas using inverse or reciprocal differences are useful if one wants to deter-
mine the coefﬁcients of the rational approximation, and use it to compute approximations

394
Chapter 4. Interpolation and Approximation
for several arguments. If one only wants the value of the rational interpolating function for
a single argument, then it is more convenient to use an alternative algorithm of Neville type.
This is the case in the ρ-algorithm, which is a convergence acceleration procedure using
rational interpolation to extrapolate to inﬁnity with the same degree in the numerator and
denominator.
If we consider the sequence of rational approximations of degrees (m, n)
(0, 0), (0, 1), (1, 1), (1, 2), (2, 2),
the following recursive algorithm results (Stoer and Bulirsch [338, Sec. 2.2]):
For i = 0, 1, 2, . . ., set Ti,−1 = 0, Ti,0 = fi, and
Tik = Ti,k−1 +
Ti,k−1 −Ti−1,k−1
x −xi−k
x −xi
0
1 −Ti,k−1 −Ti−1,k−1
Ti,k−1 −Ti−1,k−2
1
−1
,
1 ≤k ≤i.
(4.3.37)
As in Neville interpolation the calculations can be arranged in a table of the following form.
(m, n)
(0, 0)
(0, 1)
(1, 1)
(1, 2)
· · ·
f1 = T1,0
0
T2,1
f2 = T2,0
T3,2
0
T3,1
T4,3
f2 = T2,0
T4,2
...
...
0
T4,1
...
f4 = T4,0
...
...
...
Here any entry is determined by a rhombus rule from three entries in the preceding two
columns. Note that it is easy to add a new interpolation point in this scheme.
As shown by Berrut and Mittelmann [23], every rational interpolant can be written in
barycentric form
r(x) =
N

k=0
uk
x −xk
fk
5
N

k=0
uk
x −xk
.
(4.3.38)
Let qk = Qn(xk), k = 1 : N, be the values of the denominator at the nodes. Then the
barycentric representation of the denominator Qn(x) is
Qn(x) =
N
&
i=0
(x −xi)
N

k=0
wk
x −xk
qk,
wk = 1
5 N
&
i=0
(xk −xi).
Hence r(x) can be written as in (4.3.38), where uk = wkqk is the weight corresponding
to the node xk. Since wk ̸= 0 for all k, it follows that qk = 0 at a node if and only if the
corresponding weight equals zero.

4.3. Generalizations and Applications
395
The barycentric form has the advantage that the barycentric weights give information
about possible unattainable points. However, the determination of the parameter vector
u = (u0, u1, . . . , uN)T is more complicated. An elimination method for the computation
of u in O(n3) operations is given by Berrut and Mittelmann [23].
4.3.4
Multidimensional Interpolation
Polynomial interpolation for functions of several independent variables are generally more
difﬁcult than the one-dimensional case. There is in general a lack of uniqueness. In par-
ticular, it may not sufﬁce to require that the interpolation points are distinct; see Prob-
lem 4.3.9 (b).
As a simple example, consider the problem of interpolating a function given at three
distinct points pi = (xi, yi), i = 1 : 3, by a linear function in two variables,
p(x, y) = c1 + c2x + c3y.
This leads to the linear system V c = f , where
V =
 1
x1
y1
1
x2
y2
1
x3
y3

,
c =
 c1
c2
c3

,
f =
 f1
f2
f3

.
The interpolation problem has exactly one solution if V is nonsingular, i.e., when det(V ) ̸=
0. But 1
2 det(V ) is just the area of the triangle with vertices (xi, yi), i = 1 : 3. If this area is
zero, then the three points lie on a line and the problem has either inﬁnitely many solutions
or no solution.
Much of the theory for univariate interpolation can be generalized to multidimensional
interpolation problems provided that the function is speciﬁed on a Cartesian (tensor) product
grid. For simplicity, we ﬁrst concentrate on functions f (x, y) of two variables, but the
extension to more dimensions is not difﬁcult. Assume that we are given function values
fij = f (xi, yj),
i = 1 : n,
j = 1 : m,
(4.3.39)
where xi, i = 1 : n, and yj, j = 1 : m, are distinct points. We seek a polynomial p(x, y)
of degree at most n −1 in x and at most m −1 in y that interpolates these values. Such a
polynomial has the form
p(x, y) =
n−1

i=0
m−1

j=0
cijxiyj,
(4.3.40)
where the mn coefﬁcients cij are to be determined. Since the number of coefﬁcients equal
the number of interpolatory conditions we can expect the polynomial p(x, y) to be uniquely
determined. To show this it sufﬁces to show that any polynomial q(x, y) of degree n −1
in x and m −1 in y that vanishes at the mn distinct points (xi, yj), i = 1 : n, and yj,
j = 1 : m, must vanish identically.
If we want to compute p(x, y) for given values of x and y, then we can proceed as
follows. For each j = 1 : m, use univariate interpolation to determine the values p(x, yj),
where p(x, y) is the interpolation polynomial in (4.3.40). Next, the m values p(x, yj),

396
Chapter 4. Interpolation and Approximation
j = 1 : m, are interpolated using univariate interpolation, which determines p(x, y).
Note that since the points xi and yj are distinct all univariate interpolation polynomials are
uniquely determined. It is clear that we will obtain the same result, whether we interpolate
ﬁrst for x and then for y or vice versa. Clearly this approach can also be used in more than
two dimensions.
In many cases we are not satisﬁed with obtaining p(x, y) for speciﬁc values of x
and y, but want to determine p(x, y) as a polynomial in x and y. We can then use the
above procedure algebraically to derive a Newton formula for a tensor product interpolation
problem in two variables. We set [xi; yj]f = f (xi, yj), and deﬁne bivariate divided
differences [x1, . . . , xν; y1, . . . , yµ]f , by recurrences, separately for each variable. We
start by forming, for each yj, j = 1 : m, divided differences with respect to the x variable:
[x1; y1]f
[x1, x2; y1]f
· · ·
[x1, x2, . . . , xn; y1]f ,
[x1; y2]f
[x1, x2; y2]f
· · ·
[x1, x2, . . . , xn; y2]f ,
...
...
...
[x1; ym]f
[x1, x2; ym]f
· · ·
[x1, x2, . . . , xn; ym]f .
These are used to form the Newton polynomials
p(x, yj) =
n

i=1
[x1, . . . , xi; yj]f
i−1
&
ν=1
(x −xν),
j = 1 : m,
which give, for any x, the values of the interpolation polynomial p(x, y) for y1, . . . , ym.
Next we form in each column above the divided differences with respect to y:
[x1; y1]f
[x1, x2; y1]f
· · ·
[x1, . . . , xn; y1]f ,
[x1; y1, y2]f
[x1, x2; y1, y2]f
· · ·
[x1, . . . , xn; y1, y2]f ,
...
...
...
[x1; y1, . . . , ym]f
[x1, x2; y1, . . . , ym]f
· · ·
[x1, . . . , xn; y1, . . . , ym]f .
If these nm divided differences are used for Newton interpolation in the y variable, we
obtain Newton’s interpolation formula in two variables,
p(x, y) =
n

i=1
m

j=1
[x1, . . . , xi; y1, . . . , yj]f
i−1
&
ν=1
(x −xν)
j−1
&
µ=1
(y −yµ),
(4.3.41)
where empty products have the value 1. Note that it is indifferent in which order the divided
differences are formed. We could equally well have started to form divided differences with
respect to y.
Remainder formulas can be derived from the corresponding one-dimensional error
formulas; see Isaacson and Keller [208, Sec. 6.6]. For f sufﬁciently smooth there exist

4.3. Generalizations and Applications
397
values ξ, ξ ′, η, η′ such that
R(x, y) = ∂nf (ξ, y)
∂xn
2n
ν=1(x −xν)
n!
+ ∂mf (x, η)
∂ym
2m
µ=1(y −yµ)
m!
(4.3.42)
−∂n+mf (ξ ′, η′)
∂xn∂ym
2n
ν=1(x −xν)
n!
2m
µ=1(y −yµ)
m!
.
(4.3.43)
Lagrange’s interpolation formula can also be generalized for the tensor product case.
We have
p(x, y) =
n

i=1
m

j=1
f (xi, yj)
n
&
ν=1
ν̸=i
(x −xν)
(xi −xν)
n
&
µ=1
µ̸=j
(y −yµ)
(yj −yµ).
(4.3.44)
Clearly p(x, y) assumes the values f (xi, yj) for i = 1 : n, j = 1 : m. As the polynomial
is of degree n −1 in x and m −1 in y, it must equal the unique interpolating polynomial.
Therefore, the remainder must also be the same as for the Newton formula. The Lagrange
formula (4.3.44) is easily extended to three and more variables.
The interpolation problem we have treated so far speciﬁes the maximum degree of
p(x, y) in x and y separately. Instead we could specify the total degree to be at most n −1.
Then the interpolation polynomial must have the form
p(x, y) =
n−1

i=0
n−i−1

j=0
bijxiyj.
(4.3.45)
There are 1
2n(n + 1) coefﬁcients to determine in (4.3.45). We shall show that with the
“triangular” array of interpolation points (xi, yj), i+j = 1 : n, the interpolation polynomial
is uniquely determined.
The Newton formula (4.3.41) can be generalized to the case when for each i, i = 1 : n,
the interpolation given points are (xi, yj), j = 1 : mi with 1 ≤mi ≤m, with a slightly more
complicated remainder formula. A particularly interesting case is when mi = n −i + 1,
i.e., the interpolation points form a triangle. This gives rise to the interpolating polynomial
p(x, y) =

2≤i+j≤n+1
[x1, . . . , xi; y1, . . . , yj]f
i−1
&
ν=1
(x −xν)
j−1
&
µ=1
(y −yµ),
(4.3.46)
with remainder formula
R(x, y) =
n+1

i=1
∂nf (ξi, ηi)
∂xi∂yn−i
2i−1
ν=1(x −xν)
(i −1)!
2n−i
µ=1(y −yµ)
(n −i)!
.
(4.3.47)
This formula is due to Biermann [26].
Interpolation formulas for equidistant points xi = x0 + ih and yj = y0 + jk can
readily be obtained from Newton’s formula (4.3.41). Using the points (xi, yj), i = 0 : n,
i = 0 : m, we get
p(x0 + ph, y + qk) =
n

i=0
m

j=0
p
i
q
j

4i
x4j
yf (x0, y0).
(4.3.48)

398
Chapter 4. Interpolation and Approximation
Example 4.3.8.
Formulas for equidistant points can also be obtained by using the operator formulation
of Taylor’s expansion:
f (x0 + h, y0 + k) = exp

hDx + kDy

f (x0, y0)
(4.3.49)
= f0,0 +

hDx + kDy

f0,0
+

h2D2
x + 2hkDxDy + k2D2
y

f0,0 + O(h3 + k3).
An interpolation formula, exact for all functions p(x, y) in (4.3.40) with m = n = 3, can
be obtained by replacing in Taylor’s formula the derivatives by difference approximations
valid for quadratic polynomials,
f (x0 + ph, y0 + qh) ≈f0,0 + 1
2p(f1,0 −f−1,0) + 1
2q(f0,1 −f0,−1)
+ 1
2p2(f1,0 −2f0,0 + f−1,0)
(4.3.50)
+ 1
4pq(f1,1 −f1,−1 −f−1,1 + f−1,−1)
+ 1
2q2(f0,1 −2f0,0 + f0,−1).
This formula uses function values in nine points. (The proof of the expression for approxi-
mating the mixed derivative DxDyf0,0 is left as an exercise; see Problem 4.3.11.)
An important case, to be treated in Sec. 5.4.4, is interpolation formulas in two or more
dimensions with function values speciﬁed on the vertices and sides of a simplex. These
play a fundamental role in the ﬁnite element method.
4.3.5
Analysis of a Generalized Runge Phenomenon
In this section we make a more detailed theoretical and experimental study of the Runge
phenomenon (see Sec. 4.2.6). We then study interpolation at an inﬁnite equidistant point
set from the point of view of complex analysis. This interpolation problem, which was
studied by Whittaker and others in the beginning of the twentieth century, became revived
and modiﬁed in the middle of the same century under the name of the Shannon sampling
theorem, with important applications to communication theory.
It is well known that the Taylor series of an analytic function converges at an expo-
nential rate inside its circle of convergence, while it diverges at an exponential rate outside.
The circle of convergence passes through the nearest singularity. We shall see that similar
results hold for certain interpolation processes. In general, the domains of convergence are
not disks but bounded by level curves of a logarithmic potential, related to the asymptotic
distribution of the interpolation points.
For the sake of simplicity, we now conﬁne the discussion to the case when the points
of interpolation are located in the standard interval [−1, 1], but we are still interested in the
evaluation of the polynomials in the complex domain. Part of the discussion can, however,
be generalized to a case when the interpolation points are on an arc in the complex plane.

4.3. Generalizations and Applications
399
We shall study interpolation processes which are regular in the following sense.
Let the nodes tn,j, j = 1 : n, n = 1, 2, 3, . . . , be such that there exists an increasing
continuously differentiable function q : [a, b] 9→[−1, 1] such that
q(a + (b −a)(j −1)/n) < tn,j ≤q(a + (b −a)j/n),
i.e., one node in each of n subintervals of [−1, 1]. More precisely, we assume that q′(τ) > 0,
τ ∈(0, 1), while q′(0), q′(1) may be zero. Suppose that z /∈[−1, 1], but z may be close to
this interval. Then
1
n ln Mn(z) = 1
n
n

j=1
ln (z −tn,j)
→ψ(z) :=
1
b −a
 b
a
ln (z −q(τ)) dτ,
(n →∞).
(4.3.51)
The crucial factors of the interpolation error are Mn(u)/Mn(p) and Mn(u)/Mn(z). We now
obtain a fundamental approximation formula:
Mn(u)/Mn(z) = exp

ln Mn(u) −ln Mn(z)

= exp n

ψ(u) −ψ(z) + o(1)

,
(n →∞).
(4.3.52)
If u and z are bounded away from the nodes, the o(1)-term is of marginal importance; it is
basically O(1/n). For u ∈[−1, 1] Mn(u) and ln |Mn(u)| are oscillatory, and this formula
is there to be considered as a one-sided approximate error bound; see Figure 4.3.2 and the
more detailed preparatory discussion leading up to Proposition 4.3.6.
We now make a variable transformation in order to be able to utilize results of classical
potential theory. Put q(τ) = t ∈[−1, 1], dt = q′(τ)dτ. Then we can deﬁne an asymptotic
node density for the process,
w(t) =
1
q′τ(τ(t))(b −a),
w(t)dt =
dτ
b −a ,
(4.3.53)
where w is the derivative of the inverse function of q divided by b −a. Then
ψ(z) =
 1
−1
ln(z−t)w(t) dt,
w ∈C(−1, 1),
w(t) > 0,
 1
−1
w(t) dt = 1, (4.3.54)
and ψ(z) is analytic in the whole plane outside the interval [−∞, 1]. Its real part is the
logarithmic potential of the density w with reversed sign,
1
n ln |Mn(z)| ≈ℜψ(z) =
 1
−1
ln |z −t|w(t) dt.
(4.3.55)
ℜψ(z) is a harmonic function for all z = x+iy /∈[−1, 1], i.e., it satisﬁes Laplace’s equation
∂2ℜψ/∂x2 + ∂2ℜψ/∂y2 = 0.
The function 1
n ln |Mn(z)| is itself the logarithmic potential of a discrete distribution
of equal weights 1/n at the nodes tj,n, but it is less pleasant to deal with than ℜψ(z). For
example, it becomes −∞at the nodes while, according to classical results of potential
heory, ℜψ(z) is continuous and single-valued everywhere, also on the interval [−1, 1] (see
Figure 4.3.1). The imaginary part, however, is not single-valued. It becomes single-valued

400
Chapter 4. Interpolation and Approximation
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−1
−0.9
−0.8
−0.7
−0.6
−0.5
−0.4
−0.3
Figure 4.3.1. ℜψ(u), u ∈[−1, 1], for the processes based on equidistant nodes
and on Chebyshev nodes. For Chebyshev nodes, the convergence properties are the same
over the whole interval [−1, 1], because ℜψ(u) = −ln 2 = −0.693 for all u ∈[−1, 1].
For equidistant nodes, the curve, which is based on (4.3.61), partly explains why there are
functions f such that the interpolation process diverges fast in the outer parts of [−1, 1]
and converges fast in the central parts (often faster than Chebyshev interpolation).
if we cut the complex plane along the real axis from −∞to 1, but it tends to +π or −π,
depending on whether the cut is approached from above or from below.
Another result from classical potential theory that will be useful later reads
ψ′(x −0i) −ψ′(x + 0i) = 2πiw(x),
x ∈[−1, 1].
(4.3.56)
For |z| ≫1, ψ(z) depends only weakly on the node distribution, and the level curves
approach circles since, by (4.3.55),
ψ(z) =
 1
−1

ln z + ln(1 −z−1t)

w(t) dt
= ln z −z−1
 1
−1
tw(t) dt −1
2z−2
 1
−1
t2w(t) dt . . . .
(4.3.57)
Note that the coefﬁcient of z−1 vanishes for a symmetric node distribution.
We have
∂ℜψ
∂y
= ℜ∂ψ
∂y
= ℜ
 1
−1
i
x −t + iy w(t) dt =
 1
−1
y
(x −t)2 + y2)w(t) dt > 0
if y > 0. Then ℜψ(x + iy) is an increasing function of y for y > 0.
∂ℜψ
∂x
= ℜ∂ψ
∂x
= ℜ
 1
−1
1
x −t + iy w(t) dt =
 1
−1
x −t
(x −t)2 + y2)w(t) dt > 0
if x > 1. Then ℜψ(x + iy) is an increasing function of x for x > 1. Similarly ℜψ(x + iy)
is a decreasing function of y for y < 0, and a decreasing function of x for x < −1.

4.3. Generalizations and Applications
401
We deﬁne the region
D(v) =
6
z ∈C : ℜψ(z) < ℜψ(v)
7
.
(4.3.58)
Set P ∗= maxx∈[−1,1] ℜψ(x), and suppose that ℜψ(v) > P ∗. It then follows that D(v) is
a simply connected domain, and the level curve
∂D(v) =
6
z : ℜψ(z) = ℜψ(v)
7
encloses [−1, 1].
Suppose that a′
>
a and a′
>
P ∗.
Then the level curve
{z : ℜψ(z) = a} is strictly inside the level curve {z : ℜψ(z) = a′}. (The proof of
these statements essentially utilizes the minimum principle for harmonic functions and the
fact that ℜψ(z) is a regular harmonic function outside [−1, 1] that grows to ∞with |z|.)
Suppose that f (z) is analytic for |z| = R, where R is so large that we can set
ψ(z) ≈ln z; see (4.3.57). We then obtain, by Theorem 4.3.3, (4.3.20), and (4.3.52),
ln
(((f −Lnf )(u)
(( ≈n

ℜψ(u) −ln R + o(1)

+ ln M(R).
(4.3.59)
Example 4.3.9 (An Entire Function).
For f (z) = exp zα, α > 0, ln M(R) = Rα. For a ﬁxed n large enough so that
the o(1)-term can be neglected, the bound in (4.3.59) has a minimum for R = (n/α)1/α.
Inserting this into (4.3.59) we obtain
ln
(((f −Lnf )(u)
(( ≤n

ℜψ(u) −1
α (ln n + 1 + ln α) + o(1)

,
which shows that, in this example, the convergence is faster than exponential, and depends
rather weakly on the node distribution.
The following estimate comes as a consequence,
(((f −Lnf )(u)
(( = Mn(u)n−ne−n+o(1)n,
but this is no surprise. If u is real and α = 1, it follows directly from the remainder term
(4.2.10) in interpolation
(f −Lnf )(u) = Mn(u)eξu/n!,
together with Stirling’s formula (3.2.36).
The technique used in this example is, however, very general. It can be used for
complex u, and for more complicated entire functions.
Example 4.3.10 (Functions with Poles).
We choose D = D(v) = {z ∈C : ℜψ(z) < ℜψ(v)} and assume that f (z) has
two conjugate poles, p, ¯p, ℜψ(p) < ℜψ(v). (There may be other poles outside D(v).)
Consider the error formula ofTheorem 4.3.3. For n ≫1, the ratio of Knf to the contribution
from the poles is exp(−n(ℜψ(v)−ℜψ(p)+o(1))); we can thus neglect Knf (u). It follows
that
(((f −Lnf )(u)
(( ≈
(((((Mn(u)

p
resf(p)
Mn(p)(p −u)
(((((
= exp n

ℜψ(u) −ℜψ(p) + o(1)

·
((((
2resf(p)
(p −u)
(((( .
(4.3.60)

402
Chapter 4. Interpolation and Approximation
An important conclusion is that the level curve of the logarithmic potential through the pole
p separates the points u /∈[−1, 1], where the interpolation process converges from the
points where it diverges. This separation statement holds under more general conditions
than we have in this example.
For u ∈[−1, 1] there are, however, interpolation processes that may converge in an
enumerable point set that is everywhere dense in [−1, 1], even though ℜψ(u) > ℜψ(p) in
large parts of this interval. It is doubtful if such a process can be regular in the sense deﬁned
above, but it can be a subprocess of a regular process.133 Figure 4.3.3 may give hints how
the above separation statement is to be modiﬁed in order to make sense also in such a case.
The smooth curves of Figure 4.3.3 have been computed by means of (4.3.60) for Runge’s
example f (z) = 1/(1 + 25z2) with equidistant nodes; see Example 4.3.11.
We now consider two node distributions.
Example 4.3.11 (Equidistant Interpolation).
In this case we may take q(τ) = τ, τ ∈[−1, 1], t ∈[−1, 1], hence w(t) ≡1/2. For
this equidistant case we have, if z /∈[−∞, 1],
ψ(z) = 1
2
 1
−1
ln(z −t) dt = 1
2

(1 −z) ln(z −1) + (1 + z) ln(z + 1)

−1.
The real part ℜψ(z) is, however, single-valued and continuous everywhere, as mentioned
in the comment after (4.3.55). Some level curves are shown in Figure 4.3.2. Note that the
tangent of a curve is discontinuous at the intersection with [−1.1]. On the imaginary axis,
ℜψ(iy) = 1
2 ln(1 + y2) + y

sign (y) π
2 −arctan y

−1.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
−0.129
−0.228
−0.335
−0.450
−0.573
−0.706
−0.848
−0.307
−1.000
Figure 4.3.2. Some level curves of the logarithmic potential for w(t) ≡1
2, t ∈
[−1, 1]. Due to the symmetry only one quarter of each curve is shown. The value of ℜψ(z)
on a curve is seen to the left close to the curve. It is explained in Example 4.3.11 how the
curves have been computed.
133For example, a process generated by successive bisections of the interval [−1, 1] can be a subprocess of one
of the equidistant processes studied in next example.

4.3. Generalizations and Applications
403
When z →x ∈[−1, 1], from any direction, ℜψ(z) tends to
ℜψ(x) = 1
2

(1 −x) ln(1 −x) + (1 + x) ln(1 + x)

−1.
(4.3.61)
The level curve of ℜψ(z) that passes through the points ±1 intersects the imaginary
axis at the points ±iy, determined by the equation ℜψ(iy) = ℜψ(1) = ln 2 −1, with
the root y = 0.5255. Theorem 4.3.5 (below) will tell us that Lnf (x) →f (x) for all
x ∈(−1, 1), if f (z) is analytic inside and on this contour.
In the classical example of Runge, f (z) = 1/(1 + 25z2) has poles inside this contour
at z = ±0.2i. The separation statement in Example 4.3.10 told us that the level curve
of ℜψ(z) which passes through these poles will separate between the points, where the
interpolation process converges and diverges. Its intersection with the real axis is determined
by the equation ℜψ(x) = ℜψ(0.2i) = −0.70571. The roots are x = ±0.72668; see also
Figure 4.3.2.
The theory based on the logarithmic potential is of asymptotic nature, and one may ask
how relevant it is when the number of nodes is of a reasonable size for practical computation.
After all, the behavior of the interpolation error between the nodes is quite different from
the smooth logarithmic potential. Figure 4.3.3 indicates, however, that the prediction of
this theory can be rather realistic when we ask for local maxima of the modulus of the
interpolation error on the real axis.
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−5
−4
−3
−2
−1
0
1
2
3
4
5
x
Log10(Interpolation Error)
Figure 4.3.3.
log10 |(f −Lnf )(u)| for Runge’s classical example f (u) =
1/(1 + 25u2) with 30 equidistant nodes in [−1, 1]. The oscillating curves are the em-
pirical interpolation errors (observed at 300 equidistant points), for u = x in the lower
curve and for u = x + 0.02i in the upper curve; in both cases x ∈[−1, 1]. The smooth
curves are the estimates of these quantities obtained by the logarithmic potential model;
see Examples 4.3.10 and 4.3.11.

404
Chapter 4. Interpolation and Approximation
Example 4.3.12 (Chebyshev Interpolation Revisited).
In this example we have q(t) = cos(πτ), τ ∈[−1, 0]. By (4.3.53)
w(t) =
1
−π sin πτ = 1
π (1 −t2)−1
2 .
Moreover,134
Mn(z) = 21−nTn(z) = 2−n(sn + s−n),
where z = 1
2(s + s−1), s = z +
√
z2 −1. According to our convention about the choice of
branch for the square root |s| ≥1. Hence,
ℜψ(z) = lim 1
n ln |Mn(z)| −ln 2 = ln |s|
2 = ln |z +

z2 −1| −ln 2.
As in the previous example, ℜψ(z) is single-valued and continuous everywhere,
while ψ(z) is unique for z /∈[−∞, 1] only. Therefore, the family of confocal ellipses
∂ER with foci at ±1 are the level curves of ℜψ(z). In fact, the interior of ER equals
D(ln R −ln 2). The family includes, as a limit case (R →1), the interval [−1, 1], on which
ℜψ(z) = ln R −ln 2 →−ln 2 = −0.693. Note that as z →cos φ ∈[−1, 1],
|z +

(z2 −1)| = | cos φ + i sin φ| = 1,
∀φ.
Some level curves are shown in Figure 4.3.4.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
6 +0.556
+0.445
+0.323
+0.188
+0.040
−0.124
−0.303
−0.494
−0.693
Figure4.3.4. SomelevelcurvesofthelogarithmicpotentialassociatedwithCheby-
shev interpolation. They are ellipses with foci at ±1. Due to the symmetry only a quarter is
shown of each curve. The value of ℜψ(z) for a curve is seen to the left, close to the curve.
Note that ℜψ(z) is smaller in [−1, 1] than anywhere else; this conﬁrms the conclusion
in Sec. 4.3.2 that, for any function analytic in a region D that encloses [−1, 1], Chebyshev
interpolation converges at the same exponential rate in (almost) all points u close to [−1, 1].
134The complex variable denoted w in Sec. 3.2.3 is now denoted s, in order to avoid a collision with the density
w(t).

4.3. Generalizations and Applications
405
For the classical Runge case, we have
|(f −Lnf )(u)| ≈en(ψ(0)−ψ(0.02i)) = en(−0.693+0.494) = e−0.199n.
This is very different from the equidistant case which diverges if |u| > 0.72668 but at the
central subinterval has, by Figure 4.3.2,
|(f −Lnf )(u)| ≈en(−1+0.706) = e−0.294n,
which is better than Chebyshev interpolation.
Also note that if z /∈[−1, 1], we can, by the deﬁnition of ℜψ(z) as a Riemann sum
(see (4.3.51)), ﬁnd a sequence {ϵn} that decreases monotonically to zero such that
1
n
(( ln Mn(z) −ψ(z)
(( < ϵn,
z /∈[−1, 1].
(4.3.62)
It is conceivable that the same sequence can be used for all z on a curve that does not touch
the interval [−1, 1]. (This can be proved, but the proof is omitted.)
We can only claim a one-sided inequality if we allow that u ∈[−1, 1]
1
nℜ

ln Mn(u) −ψ(u)

< ϵn,
u ∈C.
(4.3.63)
(Recall that ℜln Mn(u) = −∞at the nodes.) We can use the same sequence for z and u.
We can also say that |Mn(u)| behaves like exp((ℜψ(u)±δ)n) outside the immediate vicinity
of the interpolation points.
Theorem 4.3.5.
Assume that the nodes are in [−1, 1], and that [−1, 1] is strictly inside a simply
connected domain D ⊇D(v).
If f (ζ) is analytic in the closure of D, then the interpolation error (Lnf )(u) −f (u)
converges like an exponential to 0 for any u ∈D(v).
Proof. By Theorem 4.3.3, f (u) −(Lnf )(u) = In(u), where
In(u) =
1
2πi

∂D
Mn(u)f (z)
Mn(z)(z −u) dz.
(4.3.64)
Note that ℜψ(z) ≥ℜψ(v), because D ⊇D(v). Then, by (4.3.62) and (4.3.63),
((Mn(u)/Mn(z)
(( < exp n

ℜψ(u) −ℜψ(v) + 2ϵn

.
Let |f (z)| ≤M. For any u ∈D(v), we can choose δ > 0 such that ℜψ(u) < ℜψ(v) −3δ,
|z −u| > δ. Next, choose n large enough so that ϵn < δ. Then
|f (u) −(Lnf )(u)| < 1
2π M exp n(−3δ + 2δ)

∂D(v)
|dz|
δ
≤K exp(−nδ)
δ
,
where K
= K(v) does not depend on n, δ, and u.
Hence, the convergence is
exponential.

406
Chapter 4. Interpolation and Approximation
We shall now state without proof a complement and a kind of converse to Theo-
rem 4.3.5, for functions f (z) that have simple poles in D(v).
Proposition 4.3.6.
Assume that [−1, 1] is strictly inside a domain D ⊃D(v), and that f (ζ) is analytic
in the closure of D, except for a ﬁnite number of simple poles p in the interior, all with the
same value of P (p).
Outside the interval [−1, 1], the curve ∂D(p) then separates the points, where the
sequence {(Lnf )(u)} converges, from the points, where it diverges.
The behavior of
|(Lnf )(u) −f (u)|, when u ∈D(v), n ≫1, is roughly described by the formula
|(f −Lnf )(u)| ≈K|Mn(u)|e(−P(p)±δ)n/ max
p (1/|p −u|).
(4.3.65)
There are several interpolation processes with interpolation points in [−1, 1] that con-
verge for all u ∈[−1, 1], when the condition of analyticity is replaced by a more modest
smoothness assumption, for example, f ∈Cp. This is the case when the sequence of inter-
polation points are the zeros of the orthogonal polynomials which belong to a density func-
tion that is continuous and strictly positive in [−1, 1]. We shall prove the following result.
Proposition 4.3.7.
Consider an interpolation process where the interpolation points have a (perhaps
unknown) asymptotic density function w(x), x ∈[−1, 1]. Assume that, for some k ≥1,
(Lnf −f )(x) →0
∀x ∈[−1, 1],
∀f ∈Ck[−1, 1]
as n →∞. Then the logarithmic potential ℜψ(x) must be constant in [−1, 1], and the
density function must be the same as for Chebyshev interpolation, i.e., w(x) =
1
π (1 −
x2)−1/2.
Proof. Let f (z) be analytic in some neighborhood of [−1, 1], for example, any function
with a pole at a point p (arbitrarily) close to this interval. A fortiori, for such a function our
interpolation process must converge at all points u in some neighborhood of the interval
[−1, 1].
Suppose that ℜψ(x) is not constant, and let x1, x2 be points such that ℜψ(x1) <
ℜψ(x2). We can then choose the pole p so that ℜψ(x1) + δ < ℜψ(p) < ℜψ(x2) −δ. By
Proposition 4.3.6, the process would then diverge at some point u arbitrarily close to x2.
This contradiction shows that ℜψ(x) must be constant in [−1, 1], ℜψ(x) = a (say).
This gives a Dirichlet problem for the harmonic function ℜψ(z), z /∈[−1, 1], which
has a unique solution, and one can verify that the harmonic function ℜψ(z) = a +ℜln(z +
√
z2 −1) satisﬁes the boundary condition. We must also determine a. This is done by
means of the behavior as z →∞. We ﬁnd that
ℜψ(z) = a + ℜln

z + z(1 −z−2)1/2
= a + ℜln

2z −O(z−1)

= a + ℜln z + ln 2 −O(z−2).
This is to be matched with the result of the discussion of the general logarithmic potential
in the beginning of Sec. 4.3.5. In our case, where we have a symmetric distribution and

Problems and Computer Exercises
407
 1
1 w(x) dx = 1, we obtain ℜψ(z) = ℜln z + O(z−2). The matching yields a = −ln 2.
Finally, by (4.3.56), we obtain after some calculation w(x) = (1 −x2)−1/2.
Our problem is related to a more conventional application of potential theory, namely
the problem of ﬁnding the electrical charge distribution of a long insulated charged metallic
strip through [−1, 1], perpendicular to the (x, y)-plane. Such a plate will be equipotential.
Suppose that such a distribution is uniquely determined by the total charge.135 The charge
density must then be proportional to the density in our example. This density w corresponds
to q(τ) = cos πt, i.e., w(x) = 1
π (1 −x2)−1/2. A fascinating relationship of electricity to
approximation!
The above discussion is related to the derivations and results concerning the asymp-
totic distribution of the zeros of orthogonal polynomials, given in the standard monograph
Szegö [347].
Review Questions
4.3.1 What is meant by Lagrange–Hermite (osculatory) interpolation? Prove the unique-
ness result for the Lagrange–Hermite interpolation problem.
4.3.2 (a) Write down the conﬂuent Vandermonde matrix for the Lagrange–Hermite cubic
interpolation problem.
(b) Express the divided difference [x0, x0, x1, x1]f in terms of f0, f ′
0, and f1,f ′
1.
4.3.3 What are the inverse divided differences of a function f (x) used for? How are they
deﬁned? Are they symmetric in all their arguments?
4.3.4 Give the complex integral formula for the interpolation error of a function that is
analytic in a domain. Give the assumptions, and explain your notations. Give the
interpolation error for the case with poles in the domain also.
4.3.5 How is bilinear interpolation performed? What is the order of accuracy?
4.3.6 What is Chebyshev interpolation, and what can be said about its convergence for
analytic functions?
Problems and Computer Exercises
4.3.1 (a) Construct the divided-difference scheme for the simplest Lagrange–Hermite
interpolation problem, where the given data are f (xi), f ′(xi), i = 0, 1; x1 = x0 +h.
Prove all the formulas concerning this problem that are stated at the end of Sec. 4.3.1.
(b) For f (x) = (1+x)−1, x0 = 1, x1 = 1.5, compute f (1.25) by Lagrange–Hermite
interpolation. Compare the error bound and the actual error.
135It is unique, but we have not proved this.

408
Chapter 4. Interpolation and Approximation
(c) Show that for Lagrange–Hermite interpolation
|f ′(x) −p′(x)| ≤
h3
72
√
3

max
x∈[x0,x1] |f (iv)(x)| + O(h|f (v)(x)|)

.
Hint:
d
dx [x0, x0, x1, x1, x]f = [x0, x0, x1, x1, x, x]f ≤· · ·.
4.3.2 Let p ∈P6 be the Lagrange–Hermite interpolation polynomial to the data xi, y(xi),
y′(xi), xi = x0 + ih, i = 1, 2, 3.
(a) Find the remainder term, and show that the interpolation error for x ∈[x1, x3]
does not exceed h6 max |f (6)(x)|/4860 in magnitude.
(b) Write a program that computes p(x1 + 2jh/k), j = 0 : k.
Comment: This is one of several possible procedures for starting a multistep method
for an ordinary differential equation y′ = f (x, y). Two steps with an accurate one-
step method provide values of y, y′, and this program then produces starting values
(y only) for the multistep method.
4.3.3 Give a short and complete proof of the uniqueness of the interpolation polynomial
for distinct points, by the use of the ideas in the proof of Theorem 4.3.1.
4.3.4 Derive an approximate formula for f ′(x0) when the values f (x−1), f (x0), f (x1) are
given at three nonequidistant points. Give an approximate remainder term. Check
the formula and the error estimate on an example of your own choice.
4.3.5 Consider the problem of ﬁnding a polynomial p ∈P4 that interpolates the data f (1),
f (−1), f ′′(1), f ′′(−1). The new feature is that there are no ﬁrst derivatives. Show
that this problem is uniquely solvable.
Hint: Show that using the power basis one obtains a linear system of the form
Mc = f , where f = (f1, f−1, f ′′
1 , f ′′
−1)T , and
M =


1
1
1
1
1
−1
1
−1
0
0
2
6
0
0
2
−6


with det(M) = 48.
4.3.6 (a) Given a sequence of function values f1, f2, f3, . . . at equidistant points xj =
x0 + jh, assume that min fj = fn, and let p(x) be the quadratic interpolation
polynomial determined by fn−1, fn, fn+1. Show that
min p(x) = fn −(µδfn)2
2δ2fn
at
x = xn −hµδfn
δ2fn
,
and that the error of the minimum value can be bounded by max |43fj|/
√
243,
where j is in some neighborhood of n. Why and how is the estimate of x less
accurate?
(b) Write a handy program that includes the search for all local maxima and minima.
Sketch or work out improvements of this algorithm, perhaps with ideas of inverse
interpolation and with cubic interpolation, and perhaps for nonequidistant data.

Problems and Computer Exercises
409
4.3.7 We use the notations and assumptions of Theorem 4.3.3.
Using the representation of the interpolation operator as an integral operator, show
that
(Lnf )(x) =
1
2πi

∂D
K(x, z) f (z)
M(z) dz,
K(x, z) = M(x) −M(z)
(x −z)
,
also if x /∈D. Note that K(x, z) is a polynomial, symmetric in the two variables x,
z.
4.3.8 If f ∈Pn, then f −Lnf is clearly zero. How would you deduce this obvious fact
from the integral (Knf )(u)?
Hint: Let ∂D be the circle |z| = R. Make the standard estimation, and let R →∞.
4.3.9 (a) Show the bilinear interpolation formula
p(x0 + ph, y0 + qk) = (1 −p)(1 −q)f0,0 + p(1 −q)f1,0 + (1 −p)qf0,1 + pqf1,1
(4.3.66)
with error bound
1
2

p(1 −p)h2|fxx| + q(1 −q)k2|fyy|

+ O(k2h2),
where fxx and fyy denote partial derivatives.
(b) Compute by bilinear interpolation f (0.5, 0.25) when
f (0, 0) = 1,
f (1, 0) = 2,
f (0, 1) = 3,
f (1, 1) = 5.
4.3.10 (a) Consider the interpolation problem: Given xi, yi, fi, i = 1 : 6, ﬁnd c =
(c1, c2, c3, c4, c5, c6)T so that p(xi, yi; c) = fi, i = 1 : 6, where
p(x, y; c) = c1 + c2x + c3y + c4x2 + c5xy + c6y2.
Choose xi, yi, fi, i = 1 : 6, by 18 independent random numbers, solve the linear
system p(xi, yi; c) = fi, i = 1 : 6, and look at max |ci|. Repeat this (say) 25 times.
You have a fair chance to avoid singular cases, or cases where max |ci| is very large.
(b) Now choose (xi, yi) as six distinct points on some circle in R2, and choose fi at
random. This should theoretically lead to a singular matrix. Explain why, and ﬁnd
experimentally the rank (if your software has convenient commands or routines for
that). Find a general geometric characterization of the sextuples of points (xi, yi),
i = 1 : 6, that lead to singular interpolation problems.
Hint: Brush up on your knowledge of conic sections.
4.3.11 Derive a formula for f ′′
xy(0, 0) using fij, |i| ≤1, |j| ≤1, which is exact for all
quadratic functions.
4.3.12 (Bulirsch and Rutishauser (1968))
(a) The function cot x has a singularity at x = 0. Use values of cot x for x = 1◦,
2◦, . . . , 5◦, and rational interpolation of order (2,2) to determine an approximate
value of cot x for x = 2.5◦, and its error.
(b) Use polynomial interpolation for the same problem. Compare the result with
that in (a).

410
Chapter 4. Interpolation and Approximation
4.3.13 Check the omitted details of the derivations in Example 4.3.4. Compare the bounds
for Chebyshev interpolation and Chebyshev expansion for R = 1 + k/n.
4.3.14 Check the validity of (4.3.56) on the Chebyshev and the equidistant cases. Also
show that
 1
−1 w(x) dx = 1, and check the statements about the behavior of P(z)
for |z| ≫1.
4.3.15 A generalization of Runge’s example. Let f be an analytic function for which the
poles nearest to [−1, 1] are a pair of complex conjugate poles at an arbitrary place
on the imaginary axis. Consider interpolation with nodes in [−1, 1].
(a) Suppose that equidistant interpolation converges at u = 1. Is it true that Cheby-
shev interpolation converges faster at u = 1?
(b) Is it true that equidistant interpolation converges faster than Chebyshev interpo-
lation in an environment of u = 0?
4.3.16 (after Meray (1884) and Cheney [66, p. 65])
(a) Let Lnf be the polynomial of degree less than n which interpolates to the function
f (z) = 1/z at the nth roots of unity. Show that (Lnf )(z) = zn−1, and that
lim
n→∞max
|u|=1 |(Lnf −f )(u)| > 0.
Hint: Solve this directly, without the use of the previous theory.
(b) Modify the theory of Sec. 4.3.2 to the case in (a) with equidistant interpolation
points on the unit circle, and make an application to f (z) = 1/(z −a), a > 0,
a ̸= 1. Here, Mn(z) = zn −1. What is ψ(z), P(z)? The density function? Check
your result by thinking like Faraday. Find out for which values of a, u, (|u| ̸= 1,
|u| ̸= a), (Lnf −f )(u) →0, and estimate the speed of convergence (divergence).
Hint: The integral for ψ(z) is a little tricky, but you may ﬁnd it in a table. There
are, however, simpler alternatives to the integral; see the end of Sec. 4.3.2.
(c) What can be said about the cases excluded above, i.e., |u| = 1, |u| = a? Also
look at the case when |a| = 1, (a ̸= 1).
(d) Is the equidistant interpolation on the unit circle identical to the Cauchy–FFT
method (with a = 0, R = 1) for the approximate computation of the coefﬁcients in
a power series?
4.4
Piecewise Polynomial Interpolation
Interpolating a given function by a single polynomial over its entire range can be an ill-
conditioned problem, as illustrated by Runge’s phenomenon. On the other hand, polynomi-
als of low degree can give good approximations locally in a small interval. In this section
we will consider approximation schemes for piecewise polynomial approximation with
different degrees of global continuity.
With the use of piecewise polynomials, there is no reason to fear equidistant data,
as opposed to the situation with higher-degree polynomials. Moreover, if the function to
be approximated is badly behaved in a subregion the effect of this can be conﬁned locally,
allowing good approximation elsewhere.

4.4. Piecewise Polynomial Interpolation
411
In computer graphics and computer aided design (CAD) curves and surfaces have to
be represented mathematically, so that they can be manipulated and visualized easily. In
1962 Bézier and de Casteljau, working for the French car companies Renault and Citroën,
independently developed Bézier curves for ﬁtting curves and surfaces. Similar work, using
bicubic splines, was done in USA at General Motors by Garret Birkhoff and Henry Garabe-
dian [28]. Subsequently, W. J. Gordon of General Motors Research developed the technique
of spline blending for ﬁtting smooth surfaces to a rectangular smooth mesh of curves.
Today Bézier curves and spline functions are used extensively in all aircraft and auto-
motive industries. Spline functions can also be used in the numerical treatment of boundary
value problems for differential equations. Bézier curves have found use in computer graph-
ics and typography. For example, scalable fonts like PostScript® are stored as piecewise
Bézier curves.
4.4.1
Bernštein Polynomials and Bézier Curves
Inthefollowingwerestrictourselvestoconsideringpolynomialcurves, i.e., one-dimensional
geometric objects. Parametric curves are often used to ﬁnd the functional form of a curve
given geometrically by a set of points pi ∈Rd, i = 0 : n.
Let c(t) ∈Rd, t ∈[0, 1], be a parametric curve. In the simplest case, n = 1, we
take c(t) to be linear,
c(t) = (1 −t)p0 + tp1,
and connecting the two points p0 and p1 so that p0 = c(0) and p1 = c(1). For n > 1 this
will not give a smooth curve and is therefore of limited interest.
We now generalize this approach and take c(t) to be a polynomial of degree n,
c(t) =
n

i=0
piBi(t),
t ∈[0, 1],
where Bi(t), i = 0 : n, are the Bernštein polynomials136 deﬁned by
Bn
i (t) =
n
i

ti(1 −t)n−i,
i = 0 : n.
(4.4.1)
Using the binomial theorem we have
1 = ((1 −t) + t)n =
n

i=0
n
i

ti(1 −t)n−i =
n

i=0
Bn
i (t).
Thus the Bernštein polynomials of degree n are nonnegative on [0, 1] and give a partition
of unity.
For n = 3 the four cubic Bernštein polynomials are
B3
0 = (1 −t)3,
B3
1 = 3t(1 −t)2,
B3
2 = 3t2(1 −t),
B3
3 = t3.
(4.4.2)
They are plotted in Figure 4.4.1.
136Bernštein introduced the polynomials named after him in 1911.

412
Chapter 4. Interpolation and Approximation
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 4.4.1. The four cubic Bernštein polynomials.
Some important properties of the Bernštein polynomials are given in the following
theorem.
Theorem 4.4.1.
The Bernštein polynomials Bn
i (t) have the following properties:
1. Bn
i (t) > 0, t ∈(0, 1)
(nonnegativity);
2. Bn
i (t) = Bn
n−i(1 −t)
(symmetry);
3. Bn
i (t) = 0 has a root t = 0 of multiplicity i and a root t = 1 of multiplicity n −i;
4. The Bernštein polynomials Bn
i (t) have a unique maximum value at t = i/n on [0, 1];
5. The Bernštein polynomials satisfy the following recursion formula:
Bn
i (t) = (1 −t)Bn−1
i
(t) + tBn−1
i−1 (t),
i = 0 : n;
(4.4.3)
6. The Bernštein polynomials of degree n form a basis for the space of polynomials of
degree ≤n.
Proof. The ﬁrst four properties follow directly from the deﬁnition (4.4.1). The recursion
formula is a consequence of the relation
n
i

=
n −1
i

+
n −1
i −1

between the binomial coefﬁcients.
To show the linear independence we observe that if
n

i=0
aiBn
i (t) = 0,

4.4. Piecewise Polynomial Interpolation
413
then according to property 3,
n

i=0
aiBn
i (1) = anBn
i (1) = an = 0.
By repeatedly dividing by (1 −t) and using the same argument we ﬁnd that a0 = · · · =
an−1 = an = 0.
The unique parametric Bézier curve corresponding to a given set of n + 1 control
points pi, i = 0 : n, equals
c(t) =
n

i=0
piBn
i (t),
t ∈[0, 1],
(4.4.4)
where Bn
i (t) are Bernštein polynomials of degree n. By property 3 in Theorem 4.4.1 the
Bézier curve interpolates the ﬁrst and last control points p0 and pn. Often a curve is
constructed by smoothly patching together several Bézier curves of low order.
Starting with B0
0(t) = 1 and setting Bn
−1(t) = Bn
n+1(t) = 0, the recursion in Theo-
rem 4.4.1 can be used to evaluate the Bernštein polynomials at a given point t.
It follows directly from the form of (4.4.4) that applying an afﬁne transformation to
c(t) can be performed simply by applying the same transformation to the control points.
Hence the Bézier curve has the desirable property that it is invariant under translations and
rotations.
Example 4.4.1.
A quadratic Bézier curve is given by
c(t) = (1 −t)2p0 + 2t(1 −t)p1 + t2p2,
t ∈[0, 1].
Clearly c(0) = p0 and c(1) = p2. For t = 1/2 we get
c
1
2

= 1
2
p0 + p2
2
+ p1

.
Hence we can construct the point c(1/2) geometrically as the intersection between the
midpoint of the line between p0 and p2 and the point p1; see Figure 4.4.2.
The Bézier polygon is the closed piecewise linear curve connecting the control points
pi and pi+1, i = 0 : n −1, and ﬁnally pn and back to p0. In Figures 4.4.2 and 4.4.3 this is
the polygon formed by the dashed lines. This polygon provides a rough idea of the shape
of the Bézier curve.
From the deﬁnition (4.4.4) of the Bézier curve it follows that for all t ∈[0, 1], the
curve c(t) is a convex combination of the control points. Therefore, c(t) lies within the
convex hull (see Deﬁnition 4.3.2) of the control points.
The variation of a function in an interval [a, b] is the least upper bound on the sum of
the oscillations in the closed subintervals [a, x1], [x1, x2], . . . , [xn, b] for all possible such

414
Chapter 4. Interpolation and Approximation
p0
p1
p2
Figure 4.4.2. Quadratic Bézier curve with control points.
p0
p1
p2
p3
Figure 4.4.3. Cubic Bézier curve with control points p0, . . . , p3.
subdivisions. The Bézier curve is variation diminishing. In particular, if the control points
pi are monotonic, so is c(t).
Usually, not all control points are known in advance. The shape of the curve is
controlled by moving the control points until the curve has the desired shape. For example, in
the quadratic case moving p1 has a direct and intuitive effect on the curve c(t). An advantage
of the Bernštein basis for representing polynomials is that the coefﬁcients (control points)
are closely related to the shape of the curve. This is not the case when using a monomial or
Chebyshev basis.
Theorem 4.4.2.
The Bézier curve c(t) is tangent to p1 −p0 and pn −pn−1 for t = 0 and t = 1,
respectively.
Proof. To show this we compute the derivative of the Bernštein polynomial (4.4.1):
d
dt Bn
i (t) =



−nBn−1
0
(t)
if i = 0,
n

Bn−1
i−1 (t) −Bn−1
i
(t)

if 0 < i < n,
nBn−1
n−1(t)
if i = n.

4.4. Piecewise Polynomial Interpolation
415
This follows from
d
dt Bn
i (t) =
n
i
 
iti−1(1 −t)n−i −(n −i)ti(1 −t)n−i−1
,
and using the deﬁnition of the Bernštein polynomials. Setting t = 0 we ﬁnd that d
dt Bn
i (0) =
0, i > 1, and therefore from (4.4.4)
d
dt c(t) = n(p1 −p0).
The result for t = 1 follows from symmetry.
More generally, at a boundary point the kth derivative of the Bézier curve depends
only on the k closest control points. This fact is useful for smoothly joining together several
pieces of Bézier curves.
De Casteljau’s Algorithm
To evaluate the Bézier curve at t ∈[0, 1] we use the recursion formula (4.4.3) to obtain
c(t) =
n

i=0
piBn
i (t) = (1 −t)
n−1

i=0
piBn−1
i
(t) + t
n

i=1
piBn−1
i−1 (t)
=
n−1

i=0

(1 −t)pi + tpi+1

Bn−1
i
(t) =
n−1

i=0
p(1)
i (t)Bn−1
i
(t).
Here we have introduced the new auxiliary control points
p(1)
i (t) = (1 −t)pi + tpi+1,
i = 0 : n −1,
as convex combinations (depending on t) of the original control points. Using this result
we can successively lower the grade of the Bernštein polynomial until we arrive at B0
0 = 1.
This gives de Casteljau’s algorithm, a recursion scheme for the auxiliary control points
p(0)
i (t) = pi,
i = 0 : n,
p(r)
i (t) = (1 −t)p(r−1)
i
(t) + tp(r−1)
i+1 (t),
i = 0 : n −r.
(4.4.5)
It follows that
c(t) =
n−r

i=0
p(r)
i (t)Bn−r
i
(t),
r = 0 : n,
(4.4.6)
and in particular c(t) = p(n)
0 .

416
Chapter 4. Interpolation and Approximation
De Casteljau’s algorithm works by building convex combinations (4.4.5) and is there-
fore numerically very stable. It can conveniently be arranged in a triangular array.
p0 = p(0)
0
p(1)
0
p1 = p(0)
1
p(2)
0
p(1)
1
p2 = p(0)
2
...
p(2)
1
...
...
...
...
...
p(n)
0
...
p(1)
n−2
...
pn−1 = p(0)
n−1
p(2)
n−2
p(1)
n−1
pn = p(0)
n
(4.4.7)
The algorithm uses about n2 operations and so is less efﬁcient than Horner’s algorithm for
evaluating a polynomial in the monomial basis.
The kth derivative of c(t) is also available from the de Casteljau scheme. It holds that
c′(t) = n

p(n−1)
1
−p(n−1)
0

,
c′′(t) = n(n −1)

p(n−2)
2
−2p(n−2)
1
+ p(n−2)
0

, . . . ,
and in general
c(k)(t) =
n!
(n −k)!4kp(n−k)
0
,
0 ≤k ≤n,
(4.4.8)
where the difference operates on the lower index i.
De Casteljau’s algorithm is illustrated for the quadratic case in Figure 4.4.4, where the
following geometric interpretation can be observed. In the interval [0, t] the Bézier curve is
represented by a quadratic spline with control points p0, p(1)
0 , p(2)
0 . In the remaining interval
[t, 1] it is represented by a quadratic spline with control points p(2)
0 , p(1)
1 , p2. Note that these
p0
p1
p2
p0
(1)
p1
(1)
p0
(2)
Figure 4.4.4. De Casteljau’s algorithm for n = 2, t = 1
2.

4.4. Piecewise Polynomial Interpolation
417
two sets of control points lie closer to the curve c(t). After a few more subdivisions it will
be hard to distinguish the polygon joining the control points from the curve.
De Casteljau’s algorithm can also be used to subdivide a Bézier curve into two seg-
ments. By repeating this partitioning the Bézier polygons converge fast to the curve. This
construction is very well suited to control, for example, a milling machine which can only
remove materiel.
4.4.2
Spline Functions
The name spline comes from a very old technique in drawing smooth curves in which a thin
strip of wood, called a drafting spline, is bent so that it passes through a given set of points;
see Figure 4.4.5. The points of interpolation are called knots and the spline is secured at the
knots by means of lead weights called ducks. Before the computer age splines were used
in all kinds of geometric design to produce sufﬁciently smooth proﬁles. This process was
slow and expensive.
Figure 4.4.5. A drafting spline.
The mathematical model of a spline is a special case of the elastic line treated in
the theory of elasticity. By Hamilton’s principle the spline assumes a shape y(x) which
minimizes the strain energy. This is proportional to the line integral of the square of the
curvature
κ(x) =
y′′(x)
(1 + (y′(x))2)3/2 .
(4.4.9)
Since ds =

1 + (y′(x))2dx, the energy becomes
E(y) =
 b
a
y′′(x)2
(1 + (y′(x))2)5/2 dx.
(4.4.10)
A mathematical model of the nonlinear spline was given by Daniel Bernoulli (1742) and
Euler (1744).137 A modern description of the development of Bernoulli and Euler is given
by Malcolm in [256], where algorithms for computing discrete approximations of nonlinear
splines are also discussed and compared. In general these are very costly.
137Euler derived the differential equation satisﬁed by an elastic line using techniques now known as calculus of
variation and Lagrange multipliers. When Euler did this work Lagrange was still a small child!

418
Chapter 4. Interpolation and Approximation
For slowly varying deﬂections, i.e., when (y′(x))2 is approximately constant, we have
the approximation
E(y) ≈const ·
 b
a
y′′(x)2 dx.
Under this assumption, y(x) is built up of piecewise cubic polynomials in such a way that
y(x) and its two ﬁrst derivatives are everywhere continuous. Let xi, i = 0 : m, be the points
the spline is forced to interpolate. Then the third derivative can have discontinuities at the
points xi.
The mathematical concept of spline functions was introduced in 1946 by Schoenberg
in the seminal paper [314]. The importance of the B-spline basis for spline approximation
(see Sec. 4.4.3) was also ﬁrst appreciated by Schoenberg. These were not used in practical
calculations for general knot sequences until the early seventies, when a stable recurrence
relation was established independently by de Boor [36] and Cox [84].
In the following we restrict ourself to consider curves in the plane. Today B-splines
enable the mathematical representation of surfaces far beyond hand techniques. In aircraft
design computations they may involve more than 50,000 data points.
Linear and Cubic Splines
We start by formally deﬁning a spline function of order k ≥1.
Deﬁnition 4.4.3.
A spline function s(x) of order k ≥1 (degree k −1 ≥0), on a grid
4 = {a = x0 < x1 < · · · < xm = b}
of distinct knots is a real function s with the following properties:
(a) For x ∈[xi, xi+1], i = 0 : m −1, s(x) is a polynomial of degree < k.
(b) For k = 1, s(x) is a piecewise constant function. For k ≥2, s(x) and its ﬁrst k −2
derivatives are continuous on [a, b], i.e., s(x) ∈Ck−2[a, b].
The space of all spline functions of order k on 4 is denoted by S4,k. From the
deﬁnition it follows that if s1(x) and s2(x) are spline functions of the same degree, so is
c1s1(x) + c2s2(x). Clearly S4,k is a linear vector space. The space Pk of polynomials of
degree less than k is a linear subspace of S4,k. The truncated power functions
(x −xj)k−1
+
= max{(x −xj)k−1, 0},
j = 1 : m −1,
introduced in Sec. 3.3.3 in connection with the Peano kernel are also elements of S4,k.
Theorem 4.4.4.
The monomials and truncated power functions
{1, x, . . . , xk−1, (x −x1)k−1
+ , . . . , (x −xm−1)k−1
+ }
(4.4.11)
form a basis for the spline space S4,k. In particular, the dimension of this space is k+m−1.

4.4. Piecewise Polynomial Interpolation
419
Proof. All we need for the ﬁrst subinterval is a basis of Pk, for example, the power basis
{1, x, . . . , xk−1}. For each additional subinterval [xj, xj+1), j = 1 : m −1, we need only
add the new basis function (x −xj)k−1
+ . It is easy to show that these k + m −1 functions
are linearly independent.
The truncated power basis (4.4.35) has several disadvantages and is not well suited
for numerical computations. The basis functions are not local; i.e., they are nonzero on the
whole interval [a, b]. Further, they (4.4.35) are almost linearly dependent when the knots
are close. Therefore, this basis yields dense ill-conditioned linear systems for various tasks.
A more suitable basis will be introduced in Sec. 4.4.3.
The simplest case k = 1 is the linear spline interpolating given values yi = f (xi),
xi ∈[a, b], i = 0 : m. This is the broken line
s(x) = qi(x) = yi−1 + di(x −xi−1),
x ∈[xi−1, xi),
i = 1 : m.
(4.4.12)
Here
di = [xi−1, xi]f (x) = (yi −yi−1)/hi,
hi = xi −xi−1,
(4.4.13)
are the divided differences of f at [xi−1, xi]. By (4.2.16) the error satisﬁes
|f (x) −s(x)| ≤1
8 max
i

h2
i
max
x∈[xi−1,xi] |f ′′(x)|

,
(4.4.14)
if f ∈C2[a, b]. Hence, we can make the error arbitrarily small by decreasing maxi hi.
An important property of interpolation with a linear spline function is that it preserves
monotonicity and convexity of the interpolated function.
The broken line interpolating function has a discontinuous ﬁrst derivative at the knots
which makes it unsuitable for many applications.
To get better smoothness piecewise
polynomials of higher degree need to be used. Cubic spline functions, which interpolate a
given function f (x) on the grid 4 and have continuous ﬁrst and second derivatives, are by
far the most important; see Figure 4.4.6.
0
1
2
3
4
5
6
7
0
0.5
1
1.5
2
2.5
3
3.5
x0
x1
x2
x3
x4
Figure 4.4.6. Broken line and cubic spline interpolation.

420
Chapter 4. Interpolation and Approximation
With piecewise Lagrange–Hermite interpolation we can interpolate given function
values and ﬁrst derivatives of a function on the grid 4. First recall that in cubic Lagrange–
Hermite interpolation (see Theorem 4.3.1 and Problem 4.3.1) a cubic polynomial qi(x) is ﬁt-
ted to values of a function and its ﬁrst derivative at the endpoints of the interval [xi−1, xi). Let
θ = x −xi−1
hi
∈[0, 1),
x ∈[xi−1, xi),
(4.4.15)
be a local variable. Then, by (4.3.11) translated to the notation in (4.4.13), the cubic qi(x)
can be written in the form
qi(x) = θyi + (1 −θ)yi−1 + hiθ(1 −θ) [(ki−1 −di)(1 −θ) −(ki −di)θ] ,
(4.4.16)
where hi, di, i = 1 : m, are as deﬁned in (4.4.13), and
ki = q′
i(xi)
(4.4.17)
is the derivative at xi.
The second derivative (and the curvature) of this piecewise polynomial will in general
be discontinuous at the grid points. We now show how to choose the parameters ki, i = 0 :
m, to also get a continuous second derivative. This will yield an interpolating cubic spline.
In contrast to piecewise Lagrange–Hermite interpolation, each piece of the cubic spline will
depend on all data points.
Theorem 4.4.5.
Every cubic spline function, with knots a = x0 < x1 < · · · < xm = b, which
interpolates the function y = f (x),
s(xi) = f (xi) = yi,
i = 0 : m,
equals for x ∈[xi−1, xi), i = 1 : m, a third degree polynomial of the form (4.4.16). The
m + 1 parameters ki, i = 0 : m, satisfy m −1 linear equations
hi+1ki−1 + 2(hi + hi+1)ki + hiki+1 = 3(hidi+1 + hi+1di),
i = 1 : m −1,
(4.4.18)
where hi = xi −xi−1, di = (yi −yi−1)/hi.
Proof. We require the second derivative of the spline s(x) to be continuous at xi, i = 1 :
m −1. We have
s(x) =
% qi(x),
x ∈[xi−1, xi),
qi+1(x),
x ∈[xi, xi+1),
where qi(x) is given by (4.4.22)–(4.4.23). Differentiating qi(x) twice we get 1
2q′′
i (x) =
a2,i + 3a3,i(x −xi−1), and putting x = xi we obtain
1
2q′′
i (xi) = a2,i + 3a3,ihi = (ki−1 + 2ki −3di)
hi
.
(4.4.19)
Replacing i by i + 1 we get 1
2q′′
i+1(x) = a2,i+1 + 3a3,i+1(x −xi), and hence
1
2q′′
i+1(xi) = a2,i+1 = (3di+1 −2ki −ki+1)
hi+1
.
(4.4.20)

4.4. Piecewise Polynomial Interpolation
421
These last two expressions must be equal, which gives the conditions
1
hi
(ki−1 + 2ki −3di) =
1
hi+1
(3di+1 −2ki −ki+1),
i = 1 : m −1.
(4.4.21)
Multiplying both sides by hihi+1 we get (4.4.18).
If the cubic spline s(x) is to be evaluated at many points, then it is more efﬁcient to
ﬁrst convert it from the form (4.4.16) to piecewise polynomial form (pp-form):
qi(x) = yi−1 + a1i(x −xi−1) + a2i(x −xi−1)2 + a3i(x −xi−1)3,
i = 1 : m. (4.4.22)
From (4.4.16) we obtain after some calculation
a1i = q′
i(xi−1) = ki−1,
a2i = 1
2q′′
i (xi−1) = (3di −2ki−1 −ki)
hi
,
(4.4.23)
a3i = 1
6q′′′
i (xi−1) = (ki−1 + ki −2di)
h2
i
.
Using Horner’s scheme qi(x) can be evaluated from (4.4.22) using only four multiplications.
Algorithms for performing conversion to pp-form are given in [37, Chapter X].
The conditions (4.4.18) are (m −1) linearly independent equations for the (m + 1)
unknowns ki, i = 0 : m. Two additional conditions are therefore needed to uniquely
determine the interpolating spline. The most important choices are discussed below.
• If the derivatives at the endpoints are known we can take
k0 = f ′(a),
km = f ′(b).
(4.4.24)
The corresponding spline function s(x) is called the complete cubic spline inter-
polant.
• If k0 and km are determined by numerical differentiation with a truncation error O(h4),
we call the spline interpolant almost complete. For example, k0 and km may be the
sum of (at least) four terms of the expansions
Df (x0) = 1
h ln(1 + 4)y0,
Df (xm) = −1
h ln(1 −∇)ym,
into powers of the operators 4 and ∇, respectively.138
• A physical spline is straight outside the interval [a, b]. The corresponding boundary
conditions are q′′
1(x0) = q′′
m(xm) = 0. From (4.4.20) and (4.4.19) it follows that
1
2q′′
i (xi−1) = (3di −2ki−1 −ki)
hi
,
1
2q′′
i (xi) = −(3di −ki−1 −2ki)
hi
.
138Two terms of the central difference expansion in (3.3.46).

422
Chapter 4. Interpolation and Approximation
Setting i = 1 in the ﬁrst equation and i = m in the second gives the two conditions
2k0 + k1 = 3d1,
(4.4.25)
km−1 + 2km = 3dm.
The spline function corresponding to these boundary conditions is called the natural
spline interpolant. It should be stressed that when a cubic spline is used for the
approximation of a smooth function, these boundary conditions are not natural! Al-
though the natural spline interpolant in general converges only with order h2, it has
been shown that on any compact subinterval of the open interval (a, b) the order of
convergence is O(h4).
• If the endpoint derivatives are not known, a convenient boundary condition is to
require that s′′′(x) be continuous across the ﬁrst and last interior knots x1 and xm−1.
Hence
q′′′
1 (x) = q′′′
2 (x),
q′′′
m−1(x) = q′′′
m(x).
Then x1 and xm−1 are no longer knots, and the corresponding spline interpolant is
called the not-a-knot spline interpolant.
From (4.4.23) we obtain,
1
6q′′′
i (x) = a3i = (ki−1 + ki −2di)
h2
i
,
x ∈[xi−1, xi),
i = 1 : m.
Hence the condition q′′′
1 = q′′′
2 gives (k0 + k1 −2d1)/h2
1 = (k1 + k2 −2d2)/h2
2, or
h2
2k0 + (h2
2 −h2
1)k1 −h2
1k2 = 2(h2
2d1 −h2
1d2).
Since this equation would destroy the tridiagonal form of the system, we use (4.4.18),
with i = 1 to eliminate k2. This gives the equation
h2k0 + (h2 + h1)k1 = 2h2d1 + h1(h2d1 + h1d2)
h2 + h1
.
(4.4.26)
If the right boundary condition is treated similarly we get
(hm−1 + hm)km−1 + hm−1km = 2hm−1dm + hm(hm−1dm + hmdm−1)
hm−1 + hm
.
(4.4.27)
Theerrorofthenot-a-knotsplineinterpolantisofthesameorderasthatofthecomplete
spline. For functions f ∈C4[a, b] one has an error estimate of the same form as
() for r = 0 : 2; see Beatson [22]. Indeed, the error analysis below shows that the
Lagrange–Hermite interpolation error is, in the whole interval [a, b], asymptotically,
the dominant source of error for both complete and not-a-knot spline approximation.
• If the spline is used to represent a periodic function, then y0 = ym and the boundary
conditions
s′(a) = s′(b),
s′′(a) = s′′(b)
(4.4.28)
sufﬁce to determine the spline uniquely. From the ﬁrst condition it follows that
k0 = km, which can be used to eliminate k0 in (4.4.18) for k = 1. Using (4.4.21) the

4.4. Piecewise Polynomial Interpolation
423
second condition in (4.4.28) gives
(k0 + 2k1 −3d1)/h1 = −(2km−1 + km −3dm)/hm
or, after eliminating k0,
2hmk1 + 2h1km−1 + (h1 + hm)km = 3(hmd1 + h1dm).
• The natural spline interpolant has the following best approximation property.
Theorem 4.4.6.
Among all functions g that are twice continuously differentiable on [a, b] and which
interpolate f at the points a = x0 < x1 < · · · < xm = b, the natural spline function
minimizes
 b
a

s′′(t)
2dt.
The same minimum property holds for the complete spline interpolant if the functions g
satisfy g′(a) = f ′(a) and g′(b) = f ′(b).
Proof. See de Boor [37, Chapter 5].
Due to this property spline functions yield smooth interpolation curves, except for
rather thin oscillatory layers near the boundaries if the “natural” boundary conditions
s′′(a) = s′′(b) = 0 are far from being satisﬁed. For the complete or almost complete
cubic spline and for cubic splines determined by the not-a-knot conditions, these oscil-
lations are much smaller as we shall see below. Hence, when a spline is to be used for
the approximate representation of a smooth function, the natural spline is not a natural
choice!
Equations (4.4.18) together with any of these boundary conditions gives a linear sys-
tem for determining the derivatives ki. It can be shown that this system is well-conditioned
using the following result, which will be proved in Volume II.
Lemma 4.4.7.
Assume that the matrix A ∈Rn×n is strictly row diagonally dominant, i.e.,
αi := |aii| −

j̸=i
|aij| > 0,
i = 1 : n.
(4.4.29)
ThenAisnonsingular, andforrowdiagonallydominantlinearsystemsGaussianelimination
without pivoting is stable.
For the ﬁrst three boundary conditions the system is tridiagonal and, which is easily
veriﬁed, also strictly row diagonally dominant. In Example 1.3.2, and algorithm was given
for solving a tridiagonal linear systems of order m by Gaussian elimination in O(m) ﬂops.
The linear system resulting from the not-a-knot boundary condition is not diago-
nally dominant in the ﬁrst and last row. However, it can be shown that also in this case
the system is well-conditioned and can be solved stably by Gaussian elimination without
pivoting.

424
Chapter 4. Interpolation and Approximation
Example 4.4.2.
In the case of spline interpolation with constant step size hi = h, (4.4.18) becomes
ki−1 + 4ki + ki+1 = 3(di + di+1),
i = 1 : m −1.
(4.4.30)
The not-a-knot boundary conditions (4.4.26)–(4.4.27) become
k0 + 2k1 = 1
2(5d1 + d2),
2km−1 + km = 1
2(dm−1 + 5dm).
(4.4.31)
We obtain for (k0, k1, . . . , km) a tridiagonal system T k = g, where
T =


1
2
1
4
1
...
...
...
1
4
1
2
1


,
g = 3


(5d1 + d2)/6
d1 + d2
...
dm−1 + dm
(dm−1 + 5dm)/6


.
Except for the ﬁrst and last row, the elements of T are constant along the diagonals. The
condition number of T increases very slowly with m; for example, κ(T ) < 16 for m = 100.
Consider now the periodic boundary conditions in (4.4.28). Setting km = k0 in the
last equation we obtain a linear system of equations T k = g for k1, . . . , km−1 where
T =


b1
c1
am
a1
b2
c2
0
...
...
...
...
am−3
bm−2
cm−2
0
am−2
bm−1
cm−1
cm
0
· · ·
0
am−1
bm


.
(4.4.32)
Here T is tridiagonal except for its last row and last column, where an extra nonzero element
occurs. Such systems, called arrowhead systems, can be solved with about twice the work
of a tridiagonal system; see [30].
In some applications one wants to smoothly interpolate given points (xj, yj), j =
0 : m, where a representation of the form y = f (x) is not possible. Then we can use a
parametric spline representation x = x(t), y = y(t), where the parameter values 0 = t0 ≤
t1 ≤· · · ≤tm correspond to the given points. Using the approach described previously, two
spline functions sx(t) and sy(t) can then be determined that interpolate the points (ti, xi)
and (ti, yi), i = 0 : m, respectively. The parametrization is usually chosen as
ti = di/d,
i = 1 : m,
where d0 = 0,
di = di−1 +

(xi −xi−1)2 + (yi −yi−1)2,
i = 1 : m,
are the cumulative distance, and d = dm.

4.4. Piecewise Polynomial Interpolation
425
For boundary conditions we have the same choices as mentioned previously.
In
particular, using periodic boundary conditions for sx(t) and sy(t) allows the representation
of closed curves (see Problem 4.4.8 (b)). Parametric splines can also be used to approximate
curves in higher dimensions.
Error in Cubic Spline Interpolation
The following standard error estimate for the complete cubic spline interpolant is due to
Beatson [22].
Theorem 4.4.8.
Let the function f be four times continuously differentiable in [a, b] and let s be the
complete cubic spline interpolant on the grid a = x0 < x1 < · · · < xm = b. Then
max
x∈[a,b] |f (r)(x) −s(r)(x)| ≤cr(β)h4−r max
x∈[a,b] |f (iv)(x)|,
r = 0 : 3,
(4.4.33)
where h = maxi hi, β = h/ mini hi, and
c0 =
5
384,
c1 =
1
216(9 +
√
3),
c2(β) = 1
12(1 + 3β),
c3(β) = 1
2(1 + β2).
We comment below on the above result for r = 0 and how it is inﬂuenced by the
choice of other boundary conditions. Let x ∈Ii = [xi−1, xi], i = 1 : m, and set
t = (x −xi−1)/hi,
yi = f (xi),
y′
i = f ′(xi).
The error in cubic spline interpolation can be expressed as the sum of two components:
i. TheerrorEH(x)duetoLagrange–Hermiteinterpolationwithcorrectvaluesoff ′(xi−1),
f ′(xi). From (4.3.12) we obtain
max
x∈Ii |EH(x)| ≤
1
384 max
x∈Ii |h4
i f (iv)(x)|.
ii. The error ES(x) due to the errors of the slopes ei = ki −y′
i, i = 0 : m. In particular,
e0 and em are the errors in the boundary derivatives.
The bound in Theorem 4.4.8 for r = 0 is only a factor of ﬁve larger than that for piecewise
Lagrange–Hermite interpolation with correct derivatives of f at all points xi, i = 0 : m,
not just at x0 = a and xm = b. Indeed, typically the error EH(x) is the dominant part. By
(4.4.16) the second part of the error is
ES(x) = hit(1 −t)
,
ei−1(1 −t) −eit
-
,
x = xi−1 + thi,
t ∈[0, 1).
Since |1 −t| + |t| = 1, and the maxima of t(1 −t) on [0, 1] equals 1/4, it follows that
|ES(x)| ≤1
4 max
1≤i≤m |hiej|,
j = i −1, i,
(4.4.34)
where hi = xi −xi−1.

426
Chapter 4. Interpolation and Approximation
We shall consider the case of constant step size hi = h. For complete splines e0 =
em = 0 and for almost complete splines e0 = O(h4), em = O(h4). It can be shown that
then ei = O(h4), and its contribution to ES is O(h5). Thus if h is sufﬁciently small, the
Lagrange–Hermite interpolation error is asymptotically the dominant source of error in the
whole interval [a, b].
Finally, we discuss the effect of the boundary slope errors for other boundary condi-
tions. Figure 4.4.7 shows (for m = 20, e0 = em = −1) how rapidly the error component
from the boundary dies out. At the midpoint x = 0.5 the error is 0.3816 · 10−5. If m ≫1,
e0 ̸= 0, and em ̸= 0, the error is negligible outside thin oscillatory boundary layers near
x = x0 and x = xm. The height and thickness of the layers depend on e0 and em.
0
0.2
0.4
0.6
0.8
1
−0.3
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
Figure 4.4.7. Boundary slope errors eB,i for a cubic spline, e0 = em = −1; m = 20.
Consider the left boundary; the right one is analogous. For the natural splines, there is
a peak error near x = x0 of approximately 0.049h2|y′′|, i.e., 40% of the linear interpolation
error (instead of cubic). This is often clearly visible in a graph of s(x).
For the not-a-knot splines we obtain approximately e0 ∼0.180h3y(4), and the peak
near x0 becomes 0.031h4y(4), typically very much smaller than we found for natural splines.
Still it is about 11.5 times as large as the Lagrange–Hermite interpolation error, but since
the oscillations quickly die out, we conclude that the Lagrange–Hermite interpolation is
the dominant error source in cubic not-a-knot spline interpolation in (say) the interval
[a + 3h, b −3h] .
Similar conclusions seem to hold in the case of variable step size also, under the
reasonable assumption that hn+1 −hn = O(h2
n). (The use of variable step size in the
context of ordinary differential equations will be treated in a later volume.)
4.4.3
The B-Spline Basis
In the following we will introduce a more convenient B-spline basis for the linear space
S4,k of spline functions of degree k. The term B-spline was coined by I. J. Schoenberg [314]
and is short for basis spline.

4.4. Piecewise Polynomial Interpolation
427
It was shown in Sec. 4.4.2 that the set of spline functions of order k, S4,k, on the grid
4 = {a = x0 < x1 < · · · < xm = b},
is a linear space of dimension k + m −1. One basis was shown to be the truncated power
basis:
6
1, x, . . . , xk−17
∪
6
(x −x1)k−1
+ , (x −x2)k−1
+ , . . . , (x −xm−1)k−1
+
7
.
(4.4.35)
In particular, a basis for S4,2 consists of continuous piecewise linear functions
{1, x} ∪{l1(x), . . . , lm−1(x)},
li(x) = (x −xi)+.
Another basis for S4,2 is obtained by introducing an artiﬁcial exterior knot x−1 ≤x0. Then
it is easy to see that using the functions li(x), i = −1 : m−1, every linear spline on [x0, xm]
can also be written as
s(x) =
m−1

i=−1
cili(x).
In anticipation of the fact that it may be desirable to interpolate at other points than
the knots, we consider from now on the sequence of knots
4 = {τ0 ≤τ1 ≤· · · ≤τm},
(4.4.36)
where τi < τi+k, i = 0 : m −k, i.e., at most k successive knots are allowed to coincide.
We start by considering the space S4,1. This consists of piecewise constant functions and a
basis is
Ni,1(x) =
%
1
x ∈[τi, τi+1),
0
otherwise,
i = 0 : m −1.
(4.4.37)
The basis functions are arbitrarily chosen to be continuous from the right, i.e., Ni,1(τi) =
Ni,1(τi + 0).
For k = 2 we deﬁne the hat functions139 by
Ni,2(x) =
 (x −τi)/(τi+1 −τi),
x ∈[τi, τi+1],
(τi+2 −x)/(τi+2 −τi+1),
x ∈[τi+1, τi+2),
0,
x ̸∈(τi, τi+2),
i = −1 : m −1.
(4.4.38)
We have here introduced two exterior knots τ−1 ≤τ0 and τm+1 ≥τm at the boundaries.
(In the following we refer to the knots τ0, . . . , τm as interior knots.)
At a distinct knot τi just one hat function is nonzero, Ni+1,2(τi) = 1. If all knots are
distinct it follows that the spline function of order k = 2 interpolating the points (τi, yi),
i = 0 : m, can be written uniquely as
s(x) =
m−1

i=−1
ciNi,2(x),
(4.4.39)
139The generalization of hat functions to several dimensions plays a very important role in ﬁnite element methods;
see Sec. 5.4.4.

428
Chapter 4. Interpolation and Approximation
where ci = yi+1. This shows that the restriction of the functions Ni,2(x), i = −1 : m −1,
to the interval [τ0, τm] are (m + 1) linearly independent functions in S4,2 and form a basis
for S4,2.
If we allow two interior knots to coalesce, τi = τi+1, 0 < i < m −1, then Ni−1,2(x)
and Ni,2(x) will have a discontinuity at τi. This generalizes the concept of a B-spline of
order 2 given in Deﬁnition 4.4.3 and allows us to model functions with discontinuities at
certain knots. Figure 4.4.8 illustrates the formation of a double knot for a linear spline.
0
2
4
6
8
10
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
Figure 4.4.8. Formation of a double knot for a linear spline.
By deﬁnition the basis function Ni,2(x) is nonzero only on the interval (τi, τi+2). It
follows that for x ∈(τi, τi+1) we have Nj,2(x) = 0, j ̸= i −1, i. Hence, for any given
value of x at most two hat functions will be nonzero and
s(x) = ci−1Ni−1,2(x) + ciNi,2(x),
x ∈(τi, τi+1).
The exterior knots are usually taken to coincide with the boundary so that τ−1 = τ0
and τm+1 = τm. In this case N−1,1 and Nm−1,1 become “half-hats” with a discontinuity at
τ0 and τm, respectively.
It is easily veriﬁed that the functions Ni,2(x) can be written as a linear combination
of the basis function li(x) = (x −τi)+, i = 1 : m + 1. We have
Ni,2(x) =

(x −τi+2)+ −(x −τi+1)+

/(τi+2 −τi+1)
−

(x −τi+1)+ −(x −τi)+

/(τi+1 −τi)
= [τi+1, τi+2]t(t −x)+ −[τi, τi+1]t(t −x)+
(4.4.40)
= (τi+2 −τi)[τi, τi+1, τi+2]t(t −x)+,
i = 1 : m.
Here [τi, τi+1, τi+2]t means the second order divided-difference functional140 operating on
a function of t; i.e., the values τi are to be substituted for t, not for x. Recall that divided
differences are deﬁned also for coincident values of the argument; see Sec. 4.3.1.
The Peano kernel and its basic properties were given in Sec. 3.3.3. The last expression
in(4.4.40)tellsusthatNi,2 isthePeanokernelofasecondorderdivided-differencefunctional
140The notation is deﬁned in Sec. 4.2.1.

4.4. Piecewise Polynomial Interpolation
429
multiplied by the constant τi+2 −τi. This observation suggests a deﬁnition of B-splines of
arbitrary order k and a B-spline basis for the space S4,k.
Deﬁnition 4.4.9.
Let 4 = {τ0 ≤τ1 ≤· · · ≤τm} be an arbitrary sequence of knots such that τi < τi+k,
i = 0 : m −k. Then a B-spline of order k (apart from a stepsize factor) equals the Peano
kernel of a kth order divided-difference functional; more precisely, we deﬁne (with the
notations used in this chapter)
Ni,k(x) = (τi+k −τi)[τi, τi+1, . . . , τi+k]t(t −x)k−1
+ ,
(4.4.41)
where [τi, τi+1, . . . , τi+k]lk−1
x
denotes the kth divided difference of the function lk−1
x
(·) with
respect to the set of points τi, τi+1, . . . , τi+k. This deﬁnition remains valid for knots that are
not distinct.
It can be shown that Ni,k(x) is deﬁned for all x. If the knots are distinct, then by
Problem 4.2.7
Ni,k(x) = (τi+k −τi)
i+k

j=i
(τj −x)k−1
+
M′
i,k(τj)
,
Mi,k(x) =
i+k
&
j=i
(x −τj),
(4.4.42)
which shows that Ni,k is a linear combination of functions (τj −x)k−1
+ , j = i : i +k. Hence
it is a spline of order k (as anticipated in the terminology).
For equidistant knots the B-spline is related to the probability density of the sum of
k uniformly distributed random variables on [−1
2, 1
2]. This was known already to Laplace.
B-splines of order k = 1, 2, and 3 are shown in Figure 4.4.9.
0
2
4
6
8
10
12
0
0.5
1
1.5
tj
tj+1
tj
tj+1
tj+2
tj
tj+1
tj+2
tj+3
Figure 4.4.9. B-splines of order k = 1, 2, 3.
Theorem 4.4.10. The B-splines of order k have the following properties:
(i) Positivity:
Ni,k(x) > 0,
x ∈(τi, τi+k).
(ii) Compact support:
Ni,k(x) = 0,
x ̸∈[τi, τi+k].
(iii) Summation property:

i Ni,k(x) = 1 for all x ∈[τ0, τm].

430
Chapter 4. Interpolation and Approximation
Proof. A proof can be based on the general facts concerning Peano kernels found in
Sec. 3.3.3, where also an expression for the B-spline (k = 3) is calculated for the equidistant
case. (Unfortunately the symbol x means different things here and in Sec. 3.3.3.)
(i) By (4.2.11), Rf = [τi, τi+1, . . . , τi+k]f = f (k)(ξ)/k!, ξ ∈(τi, τi+k), and Rp = 0,
for p ∈Pk. It then follows from the corollary of Peano’s remainder theorem that the
Peano kernel does not change sign in [τi, τi+k]. It must then have the same sign as

K(u) du = R(x −a)k/k! = 1. This proves a somewhat weaker statement than (i)
(Ni,k(x) ≥0 instead of Ni,k(x) > 0).
(ii) This property follows since a Peano kernel always vanishes outside its interval of
support of the functional, in this case [τi, τi+k]. (A more general result concerning
the number of zeros is found, e.g., in Powell [292, Theorem 19.1]. Among other
things this theorem implies that the jth derivative of a B-spline, j ≤k −2, changes
sign exactly j times. This explains the “bell shape” of B-splines.)
(iii) For a sketch of a proof of the summation property,141 see Problem 4.4.11.
To get a basis of B-splines for the space S4,k, 4 = {τ0 ≤τ1 ≤· · · ≤τm}, (m+k −1)
B-splines of order k are needed. We therefore choose 2(k −1) additional knots τ−k+1 ≤
· · · ≤τ−1 ≤τ0, τm+k−1 ≥· · · ≥τm+1 ≥τm, and B-splines Ni,k(x), i = −k + 1 : m −1.
As for k = 2 it is convenient to let the exterior knots coincide with the endpoints,
τ−k+1 = · · · = τ−1 = τ0,
τm = τm+1 = · · · = τm+k−1.
It can be shown that this choice tends to optimize the conditioning of the B-spline ba-
sis. Figure 4.4.10 shows the ﬁrst four cubic B-splines for k = 4 (the four last B-splines are
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
t−3=t−2=t−1=t0
t1
t2
t3
t4
N−3,4N−2,4
N−1,4
N0,4
Figure 4.4.10. The four cubic B-splines nonzero for x ∈(t0, t1) with coalescing
exterior knots t−3 = t−2 = t−1 = t0.
141The B-splines Mi,k originally introduced by Schoenberg in 1946 were normalized so that
 ∞
−∞Mi,k dx = 1.

4.4. Piecewise Polynomial Interpolation
431
a mirror image of these). We note that N−3,4 is discontinuous, N−2,4 has a nonzero ﬁrst
derivative, and N−2,4 has a nonzero second derivative at the left boundary.
Interior knots of multiplicity r > 1 are useful when we want to model a function
which has less than k −2 continuous derivatives at a particular knot. If r ≤k interior knots
coalesce, then the spline will only have k −1 −r continuous derivatives at this knot.
Lemma 4.4.11.
Let τi be a knot of multiplicity r ≤k, i.e.,
τi−1 < τi = · · · = τi+r−1 < τi+r.
Then Ni,k is at least (k −r −1) times continuously differentiable at τi. For r = k, the
B-spline becomes discontinuous.
Proof.
The truncated power (t −τi)k−1
+
is (k −2) times continuously differentiable
and [τi, . . . , τi+k]g contains at most the (r −1)st derivative of g.
Hence the lemma
follows.
Consider the spline function
s(x) =
m−1

i=−k+1
ciNi,k(x).
(4.4.43)
If s(x) = 0, x ∈[τ0, τm], then s(τ0) = s′(τ0) = · · · = s(k−1)(τ0) = 0, and s(τi) = 0,
i = 1 : m −1. From this it can be determined by induction that in (4.4.43) ci = 0,
i = −k +1 : m−1. This shows that the (m+k −1) B-splines Ni,k(x), i = −k +1 : m−1,
are linearly independent and form a basis for the space S4,k. (Amore general result is given
in de Boor [37, Theorem IX.1].) Thus any spline function s(x) of order k (degree k −1)
on 4 can be uniquely written in the form (4.4.43). Note that from the compact support
property it follows that for any ﬁxed value of x ∈[τ0, τm] at most k terms will be nonzero
in the sum in (4.4.43), and thus we have
s(x) =
j

i=j−k+1
ciNi,k(x),
x ∈[τj, τj+1).
(4.4.44)
We will now develop a very important stable recurrence relation for computing B-
splines. For this we need the following difference analogue of Leibniz’ formula.142
Theorem 4.4.12 (Leibniz’ Formula).
Let f (x) = g(x)h(x), and xi ≤xi+1 ≤· · · ≤xi+k. Then
[xi, . . . , xi+k]f =
i+k

r=i
[xi, . . . , xr]g · [xr, . . . , xi+k]h,
(4.4.45)
142Gottfried Wilhelm von Leibniz (1646–1716), a German mathematician who developed his version of calculus
at the same time as Newton. Many of the notations he introduced are still used today.

432
Chapter 4. Interpolation and Approximation
provided that g(x) and f (x) are sufﬁciently many times differentiable so that the divided
differences on the right-hand side are deﬁned for any coinciding points xj.
Proof. Note that the product polynomial
P(x) =
i+k

r=i
(x −xi) · · · (x −xr−1)[xi, . . . , xr]g
·
i+k

s=i
(x −xs+1) · · · (x −xi+k)[xs, . . . , xi+k]h
agrees with f (x) at xi, . . . , xi+k since by Newton’s interpolation formula the ﬁrst factor
agrees with g(x) and the second with h(x) there. If we multiply out we can write P(x) as
a sum of two polynomials:
P(x) =
i+k

r,s=i
. . . =

r≤s
. . . +

r>s
. . . = P1(x) + P2(x).
Since in P2(x) each term in the sum has 2i+k
j=i(x −xj) as a factor, it follows that P1(x)
will also interpolate f (x) at xi, . . . , xi+k. The theorem now follows since the leading
coefﬁcient of P1(x), which equals i+k
r=i[xi, . . . , xr]g · · · [xr, . . . , xi+k]h, must equal the
leading coefﬁcient [xi, . . . , xi+k]f of the unique interpolation polynomial of degree
k.
Theorem 4.4.13.
The B-splines satisfy the recurrence relation
Ni,k(x) =
x −τi
τi+k−1 −τi
Ni,k−1(x) +
τi+k −x
τi+k −τi+1
Ni+1,k−1(x).
(4.4.46)
Proof. (de Boor [37, pp. 130–131]) The recurrence is derived by applying Leibniz’formula
for the kth divided difference to the product
(t −x)k−1
+
= (t −x)(t −x)k−2
+ .
This gives
[τi, . . . , τi+k]t(t −x)k−1
+
= (τi −x)[τi, . . . , τi+k]t(t −x)k−2
+
+ 1 · [τi+1, . . . , τi+k]t(t −x)k−2
+ ,
(4.4.47)
since [τi]t(t −x) = (τi −x), [τi, τi+1]t(t −x) = 1, and [τi, . . . , τj]t(t −x) = 0 for
j > i + 1. By the deﬁnition of a divided difference
(τi −x)[τi, . . . , τi+k]t =
τi −x
τi+k −τi
([τi+1, . . . , τi+k]t −[τi, . . . , τi+k−1]t) .
Substitute this in (4.4.47), simplify, and apply the deﬁnition of B-splines. This yields
(4.4.46).

4.4. Piecewise Polynomial Interpolation
433
Note that with k multiple knots at the boundaries the denominators in (4.4.46) can
become zero. In this case the corresponding numerator is also zero and the term should be
set equal to zero.
From Property (ii) in Theorem 4.4.10 we conclude that only k B-splines of order k
may be nonzero on a particular interval [τj, τj+1]. Starting from Ni,1(x) = 1, x ∈[τi, τi+1)
and 0 otherwise (cf. (4.4.37)), these B-splines of order k can be simultaneously evaluated
using this recurrence by forming successively their values for order 1 : k in only about 3
2k2
ﬂops. This recurrence is very stable, since it consists of taking a convex combination of
two lower-order splines to get the next one.
Suppose that x ∈[τi, τi+1], and τi ̸= τi+1. Then the B-splines of order k = 1, 2, 3, . . .
nonzero at x can be simultaneously evaluated by computing the following triangular array.
0
0
. . .
0
Ni−3,4
0
Ni−2,3
. . .
Ni−1,2
Ni−2,4
Ni,1
Ni−1,3
. . .
Ni,2
Ni−1,4
0
Ni,3
. . .
0
Ni,4
0
. . .
0
(4.4.48)
The boundary of zeros in the array is due to the fact that all other B-splines not mentioned
explicitly vanish at x. This array can be generated column by column. The ﬁrst column is
known from (4.4.37), and each entry in a subsequent column can be computed as a linear
combination with nonnegative coefﬁcients of its two neighbors using (4.4.46). Note that
if this is arranged in a suitable order the elements in the new column can overwrite the
elements in the old column.
To evaluate s(x), we ﬁrst determine the index i such that x ∈[τi, τi+1) using, for
example, a linear search or bisection (see Sec. 6.1.2). The recurrence above is then used to
generate the triangular array (4.4.48) which provides Nj,k(x), j = i −k + 1 : i, in the sum
(4.4.44).
Assume now that τ0 < τ1 < · · · < τm are distinct knots. Using the B-spline basis we
can formulate a more general interpolation problem, where the n = m+k −1 interpolation
points (knots) xj do not necessarily coincide with the knots τi. We consider determining a
spline function s(x) ∈S4,k such that
s(xj) = fj,
j = 1 : m + k −1.
Since any spline s(x) ∈S4,k can be written as a linear combination of B-splines, the
interpolation problem can equivalently be written
m−1

i=−k+1
ciNi,k(xj) = fj,
j = 1 : m + k −1.
(4.4.49)

434
Chapter 4. Interpolation and Approximation
These equations form a linear system Ac = f for the coefﬁcients, where
aij = Ni−k,k(xj),
i, j = 1 : m + k −1,
(4.4.50)
and
c = (c−k+1, . . . , cm−1)T ,
f = (f1, . . . , fm+k−1)T .
The elements aij = Ni−k,k(xj) of the matrix A can be evaluated by the recurrence (4.4.46).
The matrix A will have a banded structure since aij = 0 unless xj ∈[τi, τi+k]. Hence at
most k elements are nonzero in each row of A. (Note that if xj = τi for some i only k −1
elements will be nonzero. This explains why tridiagonal systems were encountered in cubic
spline interpolation in earlier sections.)
Schoenberg and Whitney [315] showed that the matrix A is nonsingular if and only
if its diagonal elements are nonzero,
ajj = Nj−k,k(xj) ̸= 0,
j = 1 : n,
or, equivalently, if the knots xj satisfy
τj−k < xj < τj,
j = 1 : n.
(4.4.51)
Further, the matrix can be shown to be totally nonnegative, i.e., the determinant of every
submatrix is nonnegative. For such systems, if Gaussian elimination is carried out without
pivoting, the error bound is particularly favorable; see [40]. This will also preserve the
banded structure of A during the elimination.
When the B-spline representation (4.4.43) of the interpolant has been determined it can
be evaluated at a given point using the recursion formula (4.4.46). If it has to be evaluated
more than two or three times per polynomial piece it is more efﬁcient to ﬁrst convert the
B-spline to pp-form (4.4.22). For hints on how to do that, see Problem 4.4.12 (b) and (c).
A detailed discussion is found in de Boor [37, Chapter X].
Unless the Schoenberg–Whitney condition (4.4.51) is well-satisﬁed the system may
become ill-conditioned. For splines of even order k the interior knots
τ0 = x0,
τj+1 = xj+k/2,
j = 0 : n −k −1,
τm = xn,
is a good choice in this respect. In the important case of cubic splines this means that
knots are positioned at each data point except the second- and next-last (cf. the not-a-knot
condition in Sec. 4.4.2).
4.4.4
Least Squares Splines Approximation
In some application we are given function values fj = f (xj), j = 1 : n, that we want
to approximate with a spline functions with much fewer knots so that m + k −1 ≤n.
Then (4.4.49) is an overdetermined linear system and the interpolation conditions cannot
be satisﬁed exactly. We therefore consider the linear least squares spline approximation
problem:
min
n

j=1

m−1

i=−k+1
ciNi,k(xj) −fj
2
.
(4.4.52)

4.4. Piecewise Polynomial Interpolation
435
Using the same notation as above this can be written in matrix form as
min
c
∥Ac −f ∥2
2.
(4.4.53)
The matrix A will have full column rank equal to m + k −1 if and only if there is a subset
of points τj satisfying the Schoenberg–Whitney conditions (4.4.51).
If A has full column rank, then the least squares solution c is uniquely determined by
the normal equations AT Ac = AT f . Since A has at most k nonzero elements in each row
the matrix AT A will have symmetric banded form with at most 2k + 1 nonzero elements in
each row; see Figure 4.4.11.
0
10
20
0
5
10
15
20
25
30
35
40
45
50
nz = 191
0
10
20
0
5
10
15
20
nz = 121
Figure 4.4.11. Banded structure of the matrices A and AT A arising in cubic spline
approximation with B-splines (nonzero elements shown).
Example 4.4.3 (de Boor [37]).
Consider experimental data describing a property of titanium as a function of tem-
perature. Experimental values for ti = 585 + 10i, i = 1 : 49, are given. We want to ﬁt
these data using a least squares cubic spline. Figure 4.4.12 shows results from using a least
squares ﬁtted cubic spline with 17 knots. The spline with 9 knots shows oscillations near the
points where the curve ﬂattens out and the top of the peak is not well matched. Increasing
the number of knots to 17 we get a very good ﬁt.
We have in the treatment above assumed that the set of (interior) knots {τ0 ≤τ1 ≤
· · · ≤τm} is given. In many spline approximation problems it is more realistic to consider
the location of knots to be free and try to determine a small set of knots such that the given
data can be approximated to a some preassigned accuracy. Several schemes have been
developed to treat this problem.
One class of algorithms start with only a few knots and iteratively add more knots
guided by some measure of the error; see de Boor [36, Chapter XII]. The placement of the

436
Chapter 4. Interpolation and Approximation
500
600
700
800
900
1000
1100
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
Figure 4.4.12. Least squares cubic spline approximation of titanium data using
17 knots marked on the axes by an “o.”
knots is chosen so that the Schoenberg–Whitney conditions are always satisﬁed. The itera-
tions are stopped when the approximation is deemed satisfactory. If a knot ¯τ ∈[τj, τj+1) is
inserted, then the B-spline series with respect to the enlarged set of knots can cheaply and
stably be computed from the old one (see Dierckx [99]).
Other algorithms start with many knots and successively remove knots which are not
contributing much to the quality of the approximation. In these two classes of algorithms
one does not seek an optimal knot placement at each step. This is done in more recent
algorithms; see Schwetlick and Schütze [319].
Review Questions
4.4.1 What is meant by a cubic spline function? Give an example where such a function is
better suited than a polynomial for approximation over the whole interval.
4.4.2 (a) What is the dimension of the space S4,k of spline functions of order k on a grid
4 = {x0, x1, . . . , xm}? Give a basis for this space.
(b) Set up the linear system for cubic spline interpolation in the equidistant case for
some common boundary conditions. What do the unknown quantities mean, and what
conditions are expressed by the equations? About how many operations are required
to interpolate a cubic spline function to m + 1, m ≫1, given values of a function?
4.4.3 What error sources have inﬂuence on the results of cubic spline interpolation? How
fast do the boundary errors die out? How do the results in the interior of the interval
depend on the step size (asymptotically)? One of the common types of boundary
conditions yields much larger error than the others. Which one? Compare it quanti-
tatively with one of the others.

Problems and Computer Exercises
437
4.4.4 Approximately how many arithmetic operations are required to evaluate the function
values of all cubic B-splines that are nonzero at a given point?
4.4.5 Express the restrictions of f (x) = 1 and f (x) = x to the interval [x0, xm] as linear
combinations of the hat functions deﬁned by (4.4.38).
4.4.6 The Schoenberg–Whitney conditions give necessary and sufﬁcient conditions for a
certain interpolation problem with B-splines of order k. What is the interpolation
problem and what are the conditions?
Problems and Computer Exercises
4.4.1 Consider a cubic Bézier curve deﬁned by the four control points p0, p1, p2, and p3.
Show that at t = 1/2
c
1
2

= 1
4
p0 + p3
2
+ 3
4
p1 + p2
2
and interpret this formula geometrically.
4.4.2 (G. Eriksson)Approximate the function y = cos x on [−π/2, π/2] by a cubic Bézier
curve. Determine the four control points in such a way that it interpolates cos x and
its derivative at −π/2, 0, and π/2.
Hint: Use symmetry and the result of Problem 4.4.1 to ﬁnd the y-coordinate of p1
and p2.
4.4.3 Suppose that f (x) and the grid 4 are symmetric around the midpoint of the interval
[a, b]. You can then considerably reduce the amount of computation needed for the
construction of the cubic spline interpolant by replacing the boundary condition at
x = b by an adequate condition at the midpoint. Which?
(a) Set up the matrix and right-hand side for this in the case of constant step size h.
(b) Do the same for a general case of variable step size.
4.4.4 (a) Write a program for solving a tridiagonal linear system by Gaussian elimination
without pivoting. Assume that the nonzero diagonals are stored in three vectors.
Adapt it to cubic spline interpolation with equidistant knots with several types of
boundary conditions.
(b) Consider the tridiagonal system resulting from the not-a-knot boundary condi-
tions. Show that after eliminating k0 between the ﬁrst two equations and km between
the last two equations the remaining tridiagonal system for k1, . . . , km−1 is diago-
nally dominant.
(c) Interpolate a cubic spline s(x) through the points (xi, f (xi)), where
f (x) = (1 + 25x2)−1,
xi = −1 + 2
10(i −1),
i = 1 : 11.
Compute a natural spline, a complete spline (here f ′(x1) and f ′(x11) are needed) and
a not-a-knot spline. Compute and compare error curves (natural and logarithmic).
(d) Conduct similar runs as in (b), though for f (x) = 1/x, 1 ≤x ≤2, with h = 0.1

438
Chapter 4. Interpolation and Approximation
and h = 0.05. Compare the “almost complete,” as described in the text, with the
complete and the natural boundary condition.
4.4.5 If f ′′ is known at the boundary points, then the boundary conditions can be chosen
so that f ′′ = s′′ at the boundary points. Show that this leads to the conditions
2k0 + k1 = 3d1 −h1f ′′(x0),
km−1 + 2km = 3dm + hmf ′′(xm).
4.4.6 Show that the formula
 xm
x0
s(x)dx =
m

i=1
1
2hi(yi−1 + yi) + 1
12(ki−1 −ki)h2
i

is exact for all cubic spline functions s(x). How does the formula simplify if all
hi = h?
Hint: Integrate (4.4.16) from xi−1 to xi.
4.4.7 In (4.4.16) the cubic spline qi(x) on the interval [xi−1, xi) is expressed in terms of
function values yi−1, yi, and the ﬁrst derivatives ki−1, ki.
(a) Show that if Mi = s′′(xi), i = 0 : m, are the second derivatives (also called
moments) of the spline function, then
ki −di = hi
6 (2Mi + Mi−1),
ki−1 −di = −hi
6 (Mi + 2Mi−1).
Hence qi(x) can also be uniquely expressed in terms of yi−1, yi and Mi−1, Mi.
(b) Show that, using the parametrization in (a), the continuity of the ﬁrst derivative
of the spline function at an interior point xi gives the equation
hiMi−1 + 2(hi + hi+1)Mi + hi+1Mi+1 = 6(di+1 −di).
4.4.8 (a) Develop an algorithm for solving the arrowhead linear system T k = g (4.4.32),
using Gaussian elimination without pivoting. Show that about twice the number of
arithmetic operations are needed compared to a tridiagonal system.
(b) At the end of Sec. 4.4.2 parametric spline interpolation to given points (xi, yi),
i = 0 : m, is brieﬂy mentioned. Work out the details on how to use this to represent
a closed curve. Try it out on a boomerang, an elephant, or what have you.
4.4.9 (a) Compute and plot a B-spline basis of order k = 3 (locally quadratic) and m = 6
subintervals of equal length.
Hint: In the equidistant case there is some translation invariance and symmetry, so
you do not really need more than essentially three different B-splines. You need one
spline with triple knot at x0 and a single knot at x1 (very easy to construct), and two
more splines.
(b) Set up a scheme to determine a locally quadratic B-spline which interpolates
given values at the midpoints
xi = (τi+1 + τi)/2,
(τi+1 ̸= τi),
i = 0 : m −1,

4.5. Approximation and Function Spaces
439
and the boundary points τ0, τm. Show that the spline is uniquely determined by these
interpolation conditions.
4.4.10 Derive the usual formula of Leibniz for the kth derivative from (4.4.45) by a passage
to the limit.
4.4.11 Use the recurrence (4.4.46)
Ni,k(x) =
x −τi
τi+k−1 −τi
Ni,k−1(x) +
τi+k −x
τi+k −τi+1
Ni+1,k−1(x)
to show that

i
Ni,k(x) =

i
Ni,k−1(x),
τ0 ≤x ≤τm,
where the sum is taken over all nonzero values. Use this to give an induction proof
of the summation property in Theorem 4.4.10.
4.4.12 (a) Using the result
d
dx (t −x)k−1
+
= −(k −1)(t −k)k−2
+ , k ≥1, show the formula
for differentiating a B-spline,
d
dx Ni,k(x) = (k −1)
 Ni,k−1(x)
τi+k−1 −τi
−Ni+1,k−1(x)
τi+k −τi+1

.
Then use the relation (4.4.46) to show
d
dx
s

i=r
ciNi,k(x) = (k −1)
s+1

i=r
ci −ci−1
τi+k−1 −τi
Ni,k−1(x),
where cr−1 := cs+1 := 0.
(b) Given the B-spline representation of a cubic spline function s(x). Show how
to ﬁnd its polynomial representation (4.4.22) by computing the function values and
ﬁrst derivatives s(τi), s′(τi), i = 0 : m.
(c) Apply the idea in (a) recursively to show how to compute all derivatives of s(x)
up to order k −1. Use this to develop a method for computing the polynomial
representation of a spline of arbitrary order k from its B-spline representation.
4.4.13 Three different bases for the space of cubic polynomials of degree ≤3 on the interval
[0, 1] are the monomial basis {1, t, t2, t3}, the Bernštein basis {B3
0(t), B3
1(t), B3
2(t),
B3
3(t)}, and the Hermite basis. Determine the matrices for these basis changes.
4.5
Approximation and Function Spaces
Function space concepts have been introduced successively in this book. Recall, for exam-
ple, the discussion of operators and functionals in Sec. 3.3.2, where the linear space Pn, the
n-dimensional space of polynomials of degree less than n, was also introduced. This termi-
nology was used and extended in Sec. 4.1, in the discussion of various bases and coordinate
transformations in Pn.
Forforthcomingapplicationsoffunctionalanalysistointerpolationandapproximation
it is now time for a digression about

440
Chapter 4. Interpolation and Approximation
• distances and norms in function spaces;
• a general error bound that we call the norm and distance formula;
• inner-product spaces and orthogonal systems.
4.5.1
Distance and Norm
For the study of accuracy and convergence of methods of interpolation and approximation
we now introduce the concept of a metric space. By this we understand a set of points S
and a real-valued function d, a distance deﬁned for pairs of points in S in such a way that
the following axioms are satisﬁed for all x, y, z in S. (Draw a triangle with vertices at the
points x, y, z.)
1.
d(x, x) = 0
(reﬂexivity),
2.
d(x, y) > 0 if x ̸= y
(positivity),
3.
d(x, y) = d(y, x)
(symmetry),
4.
d(x, y) ≤d(x, z) + d(z, y)
(triangle inequality).
The axioms reﬂect familiar features of distance concepts used in mathematics and
everyday life, such as the absolute value of complex numbers, the shortest distance along a
geodesic on the surface of the Earth, or the shortest distance along a given road system.143
Many other natural and useful relations can be derived from these axioms, such as
d(x, y) ≥|d(x, z) −d(y, z)|,
d(x1, xn) ≤
n−1

i=1
d(xi, xi+1),
(4.5.1)
where x1, x2, . . . , xn is a sequence of points in S.
Deﬁnition 4.5.1.
A sequence of points {xn} in a metric space S is said to converge to a limit x∗∈S
if d(xn, x∗) →0. As n →∞, we write xn →x∗or limn→∞xn = x∗. A sequence {xn}
in S is called a Cauchy sequence if for every ϵ > 0 there is an integer N(ϵ) such that
d(xm, xn) < ϵ for all m, n ≥N(ϵ). Every convergent sequence is a Cauchy sequence, but
the converse is not necessarily true. S is called a complete space if every Cauchy sequence
in S converges to a limit in S.
It is well known that R satisﬁes the characterization of a complete space, but the
set of rational numbers is not complete.
For example, the Newton iteration x1 = 1,
xn+1 = 1
2(xn + 2/xn), deﬁnes a sequence of rational numbers that converges to
√
2, which
is not a rational number.
Many important problems in pure and applied mathematics can be formulated as
minimization problems. The function space terminology often makes proofs and algorithms
less abstract.
143If S is a functions space, the points of S are functions with operands in some other space, for example, in R
or Rn.

4.5. Approximation and Function Spaces
441
Most spaces that we shall encounter in this book are linear spaces. Their elements
are called vectors, which is why these spaces are called vector spaces. Two operations are
deﬁned in these spaces, namely the addition of vectors and the multiplication of a vector by
a scalar. They obey the usual rules of algebra.144 The set of scalars can be either R or C;
the vector space is then called real or complex, respectively.
Deﬁnition 4.5.2.
Let f be in a metric space B with a distance function d(x, y), and let S be a subset
or linear subspace of B. We deﬁne the distance of f to S by
dist (f, S) = inf
g∈S d(f, g).
(4.5.2)
Much of our discussion also applies to linear spaces of inﬁnite dimension, i.e., func-
tion spaces. The elements (vectors) then are functions of one or several real variables on
a compact set, i.e., a closed bounded region. The idea of such a functions space is now
illustrated in an example.
Example 4.5.1.
Consider the set of functions representable by a convergent power series on the interval
[−1, 1],
f (t) = c0 + c1t + c2t2 + · · · .
This is an inﬁnite-dimensional linear space. The functions 1, t, t2, . . . can be considered as
a standard basis of this space. The coordinates of f (t) then are the vector c0, c1, c2, . . . .
We shall be concerned with the problem of linear approximation, i.e., a function f
is to be approximated using a function f ∗that can be expressed as a linear combination of n
given linearly independent functions φ1(x), φ2(x), . . . , φn(x),
f ∗= c1φ1(x) + c2φ2(x) + · · · + cnφn(x),
(4.5.3)
where c1, c2, . . . , cn are parameters to be determined.145 They may be considered as coor-
dinates of f ∗in the functions space spanned by φ1(x), φ2(x), . . . , φn(x).
In a vector space the distance of the point f from the origin is called the norm of f
and denoted by ∥f ∥, typically with some subscript that more or less cryptically indicates
the relevant space. The deﬁnition of the norm depends on the space. The following axioms
must be satisﬁed.
Deﬁnition 4.5.3.
Areal-valued function ∥f ∥is called a norm on a vector space S if it satisﬁes conditions
1–3 below for all f, g ∈S, and for all scalars λ.
144See Sec.A.1 in OnlineAppendixAfor a summary about vector spaces. In more detailed texts on linear algebra
or functional analysis you ﬁnd a collection of eight axioms (commutativity, associativity, etc.) required by a linear
vector space.
145The functions φj, however, are typically not linear. The term “linear interpolation” is, from our present point
of view, rather misleading.

442
Chapter 4. Interpolation and Approximation
1.
∥f ∥> 0, unless f = 0
(positivity),
2.
∥λf ∥= |λ|∥f ∥
(homogeneity),
3.
∥f + g∥≤∥f ∥+ ∥g∥
(triangle inequality).
A normed vector space is a metric space with the distance
d(x, y) = ∥x −y∥.
If it is also a complete space, it is called a Banach space.146
The most common norms in spaces of (real and complex) inﬁnite sequences x =
(ξ1, ξ2, ξ3, . . .)T or spaces of functions on a bounded and closed interval [a, b] are
∥x∥∞= max
j
|ξj|,
∥f ∥∞= max
x∈[a,b] |f (x)|,
∥x∥2 =
 ∞

j=1
|ξj|2
1/2
,
∥f ∥2 = ∥f ∥2,[a,b] =
 b
a
|f (x)|2 dx
1/2
,
∥x∥1 =
∞

j=1
|ξj|,
∥f ∥1 = ∥f ∥1,[a,b] =
 b
a
|f (x)| dx.
These norms are called
• the ℓ∞or max(imum) norm (or uniform norm);
• the Euclidean norm (or the l2-norm for coordinate sequences and L2 norm for inte-
grals);
• the ℓ1-norm.
It is possible to introduce weight function w(x), which is continuous and strictly
positive on the open interval (a, b), into these norms. For example,
∥x∥2,w =
 ∞

j=1
wj|ξj|2
1/2
,
∥f ∥2,w =
 b
a
|f (x)|2w(x) dx
1/2
,
is the weighted Euclidean norm. We assume that the integrals
 b
a
|x|kw(x) dx
exist for all k. Integrable singularities at the endpoints are permitted; an important example
is w(x) = (1 −x2)−1/2 on the interval [−1, 1].
146Stefan Banach (1892–1945), a Polish mathematician at the University in Lvov. Banach founded modern
functional analysis and made major contributions to the theory of topological vector spaces, measure theory, and
related topics. In 1939 he was elected President of the Polish Mathematical Society.

4.5. Approximation and Function Spaces
443
The above norms are special cases or limiting cases (p →∞gives the max norm)
of the lp- or Lp-norms and weighted variants of these. They are deﬁned for p ≥1 as
follows:147
∥x∥p =
 ∞

j=1
|ξj|p
1/p
,
∥f ∥p =
 b
a
|f (x)|p dx
1/p
.
(4.5.4)
(The sum in the lp-norm has a ﬁnite number of terms, if the space is ﬁnite dimensional.)
Convergence in a space, equipped with the max norm, means uniform convergence.
Therefore, the completeness of the space C[a, b] follows from a classical theorem of analysis
that tells us that the limit of a uniformly convergent sequence is a continuous function. The
generalization of this theorem to several dimensions implies the completeness of the space
of continuous functions, equipped with the max norm on a closed bounded region in Rn.
Other classes of functions can be normed with the max norm maxx∈[a,b] |f (x)|, for
example, C1[a, b]. This space is not complete, but one can often live well with incomplete-
ness.
The notation L2-norm comes from the function space L2[a, b], which is the class
of functions for which the integral
 b
a |f (x)|2 dx exists, in the sense of Lebesgue. The
Lebesgue integral is needed in order to make the space complete and greatly extends the
scope of Fourier analysis. No knowledge of Lebesgue integration is needed for the study
of this book, but this particular fact can be interesting as background. One can apply this
norm also to the (smaller) class of continuous functions on [a, b]. In this case the Riemann
integral is equivalent. This also yields a normed linear space but it is not complete.148
Although C[0, 1] is an inﬁnite-dimensional space, the restriction of the continuous
functions f to the equidistant grid deﬁned by xi = ih, h = 1/n, i = 0 : n, constitutes
an (n + 1)-dimensional space, with the function values on the grid as coordinates. If we
choose the norm
∥f ∥2,Gh =

h
1/h

i=0
|f (xi)|2
1/2
,
then
lim
h→0 ∥f ∥2,Gh =
 1
0
|f (x)|2 dx
1/2
= ∥f ∥2,[0,1].
Limit processes of this type are common in numerical analysis.
Notice that even if n + 1 functions φ1(x), φ1(x), . . . , φn+1(x) are linearly indepen-
dent on the interval [0, 1] (say), their restrictions to a grid with n points must be linearly
dependent; but if a number of functions are linearly independent on a set M (a discrete set
or continuum), any extensions of these functions to a set M′ ⊃M will also be linearly
independent.
147The triangle inequality for ∥x∥p is derived from two classical inequalities due to Hölder and Minkowski.
Elegant proofs of these are found in Hairer and Wanner [178, p. 327].
148A modiﬁcation of the L2-norm that also includes higher derivatives of f is used in the Sobolev spaces, which
are used as a theoretical framework for the study of the practically very important ﬁnite element methods (FEMs),
used in particular for the numerical treatment of partial differential equations.

444
Chapter 4. Interpolation and Approximation
The class of functions, analytic in a simply connected domain D ⊂C, normed with
∥f ∥D = maxz∈∂D |f (z)|, is a Banach space denoted by Hol(D). (The explanation of this
term is that analytic functions are also called holomorphic.) By the maximum principle for
analytic functions |f (z)| ≤∥f ∥D for z ∈D.
4.5.2
Operator Norms and the Distance Formula
The concepts of linear operator and linear functional were introduced in Sec. 3.3.2. Here
we extend to a general vector space B some deﬁnitions for a ﬁnite-dimensional vector space
given in Online Appendix A.
Next we shall generalize the concept operator norm that is used for matrices; see Sec.
A.4.3 in Online Appendix A. Consider an arbitrary bounded linear operator A : S1 9→S2 in
a normed vector space S:
∥A∥= sup
∥f ∥S1
∥Af ∥S2.
(4.5.5)
Note that ∥A∥depends on the vector norm in both S1 and S2. It follows that ∥Af ∥≤
∥A∥∥f ∥. Moreover, whenever the ranges of the operators A1, A2 are such that A1 + A2
and A1A2 are deﬁned,
∥λA∥≤|λ|∥A∥,
∥A1 + A2∥≤∥A1∥+ ∥A2∥,
∥A1 · A2∥≤∥A1∥· ∥A2∥.
(4.5.6)
Sums with an arbitrary number of terms and integrals are deﬁned similarly. It follows that
∥An∥≤∥A∥n, n = 2, 3, . . . .
Example 4.5.2.
Let f ∈C[0, 1], ∥f ∥= ∥f ∥∞,
Af =
m

i=1
aif (xi) ⇒∥A∥=
m

i=1
|ai|,
Af =
 1
0
e−xf (x) dx ⇒∥A∥=
 1
0
e−x dx = 1 −e−1,
B =
m

i=1
aiAi ⇒∥B∥≤
m

i=1
|ai|∥A∥i,
(ai ∈C),
Kf =
 1
0
k(x, t)f (t) dt ⇒∥K∥∞≤sup
x∈[0,1]
 1
0
|k(x, t)| dt.
The proofs of these results are left as a problem. In the last example, approximate the
unit square by a uniform grid (xi, tj)m
i,j=1, h = 1/m, and approximate the integrals by
Riemann sums. Then approximate ∥K∥∞by the max norm for the matrix with the elements
ki,j = hk(xi, tj); see Sec. A.4.3 in Online Appendix A.
Example 4.5.3.
For the forward difference operator 4 we obtain ∥4∥∞= 2, hence ∥4k∥∞≤2k.
In fact ∥4k∥∞= 2k, because the upper bound 2k is attained (for every integer k) by the
sequence {(−1)n}∞
0 .

4.5. Approximation and Function Spaces
445
Example 4.5.4.
Let D be a domain in C, the interior of which contains the closed interval [a, b].
Deﬁne the mapping Dk: Hol D ⇒C[a, b] (with max norm), by
Dkf (x) = ∂k
∂xk
1
2πi

∂D
f (z)
1
(z −x) dz =
1
2πi

∂D
k!f (z)
(z −x)k+1 dz.
Then
supx∈[a,b]∥Dkf (x)∥≤max
z∈∂D |f (z)| · sup
x∈[a,b]
k!
2π

∂D
|dz|
|z −x|k+1 < ∞.
Note that Dk is in this setting a bounded operator, while if we had considered Dk to be a
mapping from Ck[a, b] to C[a, b], where both spaces are normed with the max norm in
[a, b], Dk would have been an unbounded operator.
Many of the procedures for the approximate computation of linear functionals that we
encounter in this book may be characterized as follows. Let A be a linear functional such
that Af cannot be easily computed for an arbitrary function f , but it can be approximated by
another linear functional ˜Ak, more easily computable, such that ˜Akf = Af for all f ∈Pk.
A general error bound to such procedures was given in Sec. 3.3.3 by the Peano remainder
theorem, in terms of an integral

f (k)(u)K(u) du,
where the Peano kernel K(u) is determined by the functional R = ˜A −A.
Now we shall give a different type of error bound for more general approximation
problems, where other classes of functions and operators may be involved. Furthermore,
no estimate of f (k)(u) is required. It is based on the following almost trivial theorem. It
yields, however, often less sharp bounds than the Peano formula, in situations when the
latter can be applied.
Theorem 4.5.4 (The Norm and Distance Formula).
Let A, ˜A be two linear operators bounded in a Banach space B such that for any
vector s in a certain linear subspace S, ˜As = As. Then
∥˜Af −Af ∥≤∥˜A −A∥dist (f, S)
∀f ∈B.
Proof. Set R = ˜A −A. For any positive ϵ, there exists a vector sϵ ∈S such that
∥f −sϵ∥= dist (f, sϵ) < inf
s∈S dist (f, s) + ϵ = dist (f, S) + ϵ.
Then ∥Rf ∥= ∥Rf −Rsϵ∥= ∥R(f −sϵ)∥≤∥R∥∥f −sϵ∥< (dist (f, S) + ϵ)∥R∥.
The theorem follows, since this holds for every ϵ > 0.
The following is a common particular case of the theorem.
If A, Ak are linear
functionals such that Akp = Ap for all p ∈Pk, then
|Akf −Af | ≤(∥Ak∥+ ∥A∥)dist (f, Pk).
(4.5.7)

446
Chapter 4. Interpolation and Approximation
Another important particular case of the theorem concerns projections P from a function
space to a ﬁnite-dimensional subspace Sk, for example, interpolation and series truncation
operators, A = I, ˜A = P; see the beginning of Sec. 4.5.2. Then
∥Pf −f ∥≤∥(P −I)f ∥≤(∥P∥+ 1)dist (f, Sk).
(4.5.8)
The norm and distance formula requires bounds for ∥˜A∥, ∥A∥and dist (f, S). We have
seen examples above of how to obtain bounds for operator norms. Now we shall exemplify
how to obtain bounds for the distance of f from some relevant subspace S, in particular
spaces of polynomials or trigonometric polynomials restricted to some real interval [a, b].
For the efﬁcient estimation of dist (f, S) it may be important to take into account that f is
analytic in a larger domain than [a, b].
Theorem 4.5.5 (Estimation of dist ∞(f, Pk) in Terms of ∥f (k)∥∞).
Let f ∈Ck[a, b] ⊂R, and deﬁne the norm
∥g∥∞,[a,b] = max
t∈[a,b] |g(t)|
∀g ∈C[a, b].
Then
dist ∞,[a,b](f, Pk) ≤2
k!
b −a
4
k
∥f (k)∥∞,[a,b].
Proof. Letp(t)bethepolynomialwhichinterpolatesf (t)atthepointstj ∈[a, b], j = 1 : k.
By the remainder term (4.2.10) in interpolation,
|f (t) −p(t)| ≤max
ξ∈[a,b]
|f (k)(ξ)|
k!
k&
j=1
|t −tj|.
Set t = 1
2(b+a)+ 1
2(b−a)x, and choose tj = 1
2(b+a)+ 1
2(b−a)xj, where xj are the zeros
of the Chebyshev polynomial Tk(x). Then p is the Chebyshev interpolation polynomial for
f on the interval [a, b], and with M = maxt∈[a,b] |f (k)|/k!,
|f (t) −p(t)| ≤M
k&
j=1
(b −a)|x −xj|
2
,
x ∈[−1, 1].
Since the leading coefﬁcient of Tk(x) is 2k−1 and |Tk(x)| ≤1, we have, for t ∈[a, b],
|f (t) −p(t)| ≤M
b −a
2
k
1
2k−1 |Tk(x)| ≤M
b −a
2
k
1
2k−1 .
The bound stated for dist ∞(f, Pk) is thus satisﬁed.
Example 4.5.5.
By the above theorem, the function et can be approximated on the interval [0, 1] by
a polynomial in P6 with the error bound 2e·4−6/6! ≈2· 10−6. According to the proof this
accuracy is attained by Chebyshev interpolation on [0, 1].
If one instead uses the Maclaurin series, truncated to P6, then the remainder is
eθ/(6!) ≥1.3 · 10−3. Similarly, with the truncated Taylor series about t = 1
2 the remainder

4.5. Approximation and Function Spaces
447
is eθ/(266!) ≥2 · 10−5, still signiﬁcantly less accurate than the Chebyshev interpolation.
Economization of power series (see Problem 3.2.5), yields approximately the same accuracy
as Chebyshev interpolation.
If we do these things on an interval of length h (instead of the interval [0, 1]) all the
bounds are to multiplied by h6.
Deﬁnition 4.5.6.
Let f ∈C[a, b] and Pn be the space of polynomials of degree less than n. Then we
set
En(f ) = dist ∞(f, Pn).
(4.5.9)
Example 4.5.6.
The use of analyticity in estimates for dist ∞(f, Pn): Denote by ER an ellipse in C with
foci at −1 and 1; R is equal to the sum of the semiaxes. Bernštein’s approximation theorem,
Theorem 3.2.5, gives the following truncation error bound for the Chebyshev expansion for
a function f ∈Hol(ER), real-valued on [−1, 1] and with ∥f ∥ER ≤M:
(((f (x) −
n−1

j=0
cjTj(x)
((( ≤2MR−n
1 −R−1 ,
x ∈[−1, 1].
This implies that, on the same assumptions concerning f ,
dist ∞,[−1,1](f, Pn) ≤2MR−n
1 −R−1 .
By the application of a theorem of Landau concerning bounded power series one can
obtain the following bound that can be sharper when R ≈1:
dist ∞,[−1,1](f, Pn) ≤2MR−(n+1)Gn,
(4.5.10)
where
Gn = 2 +
1
2
2
+
1 · 3
2 · 4
2
+ · · · +
1 · 3 · · · (2n −1)
2 · 4 · · · 2n
2
∼log n
π
.
Suppose that f ∈Hol(D), where D ⊇1
2(b + a) + 1
2(b −a)ER. Then transforming
from [−1, 1] to a general interval [a, b] ⊂R, we have
dist ∞,[a,b](f, Pn) ≤2∥f ∥∞,D
b −a
2R
n
2R
2R −(b −a).
Example 4.5.7.
As a ﬁrst simple example we shall derive an error bound for one step with the trape-
zoidal rule. Set
Af =
 h
0
f (x) dx,
A2f = h
2 (f (0) + f (h)) .
We know that A2p = Ap if p ∈P2. By Theorem 4.5.5, dist ∞(f, P2) ≤∥f ′′∥∞h2/16.

448
Chapter 4. Interpolation and Approximation
Furthermore, ∥A∥∞=
 h
0 dx = h, ∥A2∥∞= h, hence by (4.5.7) the requested error
bound becomes
∥A2f −Af ∥∞≤2h · ∥f ′′∥∞h2/16 = ∥f ′′∥∞h3/8.
This general method does not always give the best possible bounds but, typically, it gives no
gross overestimate. For the trapezoidal rule we know by Peano’s method (Example 3.3.7)
that ∥f ′′∥h3/12 is the best possible estimate, and thus we now obtained a 50% overestimate
of the error.
The norm and distance formula can also be written in the form
dist (f, S) ≥|Af −˜Af |/∥A −˜A∥.
(4.5.11)
This can be used for ﬁnding a simple lower bound for dist (f, Pk) in terms of an easily
computed functional that vanishes on Pk.
Example 4.5.8.
Let ˜A = 0. The functional Af = f (1) −2f (0) + f (−1) vanishes for f ∈P2. If the
maximum norm is used on [-1,1], then ∥A∥= 1 + 2 + 1 = 4. Thus
dist (f, P2)∞,[−1,1] ≥|Af |
∥A∥= 1
4|f (1) −2f (0) + f (−1)|.
It follows, for example, that the curve y = ex cannot be approximated by a straight line in
[−1, 1] with an error less than (e −2 + e−1)/4 ≈0.271. (This can also be seen without the
use of the norm and distance formula.)
It is harder to derive the following generalization without the norm and distance
formula. By Example 4.5.3, ∥4k∥= 2k, 4kp = 0, if p ∈Pk; hence
dist (f, Pk)∞,[x0,xk] ≥2−k|4kf (x0)|.
(4.5.12)
There is another inequality that is usually sharper but less convenient to use (it follows from
the discrete orthogonality property of the Chebyshev polynomials; see Sec. 4.6):
dist (f, Pk)∞,[x0,xk] ≥1
k
((((((
k

j=0
(−1)jajf

cos jπ
k
((((((
,
(4.5.13)
where aj = 1
2, j = 0, k, and aj = 1 otherwise. Inequalities of this type can reveal when
one had better use piecewise polynomial approximation of a function on an interval instead
of using a single polynomial over the whole interval. See also Sec. 4.4.
Denote by C[a, b] the space of continuous functions on a closed bounded interval
[a, b]. We shall study the approximation of a continuous function f ∈C[a, b] by polyno-
mials. We introduce a metric by
∥f ∥= max
x∈[a,b] |f (x)|.

4.5. Approximation and Function Spaces
449
We deﬁne the modulus of continuity of f by
ω(δ; f ) =
sup
|x−y|≤δ
|f (x) −f (y)|.
(4.5.14)
Then ω(δ; f ) ↓0 as δ ↓0, because a continuous function is uniformly continuous on a
closed bounded interval.
One of the fundamental theorems in approximation theory is Weierstrass’149 approxi-
mation theorem from 1885. It is concerned with the uniform approximation of a continuous
function f on a closed bounded interval by polynomials. It is of basic importance for many
convergence proofs, which we shall see examples of in this book.
Theorem 4.5.7 (Weierstrass’Approximation Theorem).
Suppose that f is continuous on a closed, bounded interval [a, b]. Let Pn denote the
space of polynomials of degree less than n. Then we can, for any ϵ > 0, ﬁnd a polynomial
pn ∈Pn such that
max
x∈[a,b] |f (x) −pn(x)| < ϵ.
Two equivalent formulations are
• dist (f, Pn) ↓0 as n →∞;
• the set of polynomials are dense in C[a, b].
Proof. Our proof is based on ideas of Lebesgue [241]. For simplicity we assume that
the interval is [0, 1]. First we interpolate f (x) by a piecewise afﬁne functions on the grid
xk = k/r, k = 0 : r, i.e., by a linear spline; see Sec. 4.4.2. A basis for the space of linear
splines is given by
{1, (x −x0)+, (x −x1)+, . . . , (x −xr−1)+};
see (4.4.11), where we recall the notation (x −a)+ = max(x −a, 0). Hence we can write
the interpolating linear spline in the form
g(x) = f (0) +
r−1

k=0
ck(x −xk)+.
(4.5.15)
Set Mk = max(f (xk), f (xk+1)) and mk = min(f (xk), f (xk+1)). For x ∈[xk, xk+1] we
have
Mk −ω(1/r) ≤f (x) ≤mk + ω(1/r),
−Mk ≤−g(x) ≤−mk,
where ω is the modulus of continuity of f . Adding the last two inequalities we obtain
−ω(1/r) ≤f (x) −g(x) ≤ω(1/r).
149Kurt Theodor Wilhelm Weierstrass (1815–1897), German mathematician, whose lectures at Berlin University
attracted students from all over the world. He set high standards of rigor in his work and is considered the father
of modern analysis. For a more detailed biography and a survey of Weierstrass’work in approximation theory, see
Pinkus [291].

450
Chapter 4. Interpolation and Approximation
This holds for x ∈[xk, xk+1], k = 0 : r −1, hence ∥f −g∥≤ω(1/r). We now choose r so
that ω(1/r) < ϵ/2 so that ∥f −l∥< ϵ/2. Now that r is ﬁxed we see that the coefﬁcients
are bounded, (say) |ck| ≤L. (In fact we may choose any L ≥rϵ.)
If we can ﬁnd any polynomial p such that ∥p −g∥< ϵ/2, we are through. In order
to achieve this we look at each term in the sum (4.5.15), and set t = x −xk. We shall next
ﬁnd a polynomial P such that
|P(t) −t+| <
ϵ
2rL,
t ∈[−1, 1].
(4.5.16)
Note that
t+ = 1
2(t + |t|) = 1
2

t +

1 −(1 −t2)

,
t ∈[−1, 1].
Set z = 1 −t2. We know that the binomial expansion of
√
1 −z is valid for |z| < 1. By
a classical theorem of Abel, see, e.g., Titchmarsh [351, Sec. 1.22], the sequence of partial
sums also converges uniformly to
√
1 −z for z ∈[0, 1]. After return to the variable t, we
thus have a sequence of polynomials that converge uniformly to |t|, t+ ∈[−1, 1], and this
trivially yields a polynomial p(t) that satisﬁes (4.5.16).
If we now set
P(x) = f (0) +
r−1

k=0
ckp(x −xk),
then
|P(x) −g(x)| =
(((
r−1

k=0
lk(p(x −xk) −(x −xk)+)
((( < 2ϵ
2 = ϵ.
This ﬁnishes our proof.
Several other proofs have been given, e.g., one by S. Bernštein, using Bernštein
polynomials; see Davis [92, Sec. 6.2].
The smoother f is, the quicker dist(f, Pn) decreases, and the narrower the interval
is, the less dist(f, Pn) becomes. In many cases dist(f, Pn) decreases so slowly toward zero
(as n grows) that it is impractical to attempt to approximate f with only one polynomial in
the entire interval [a, b].
In inﬁnite-dimensional spaces, certain operators may not be deﬁned everywhere, but
only in a set that is everywhere dense in the space. For example, in the space C[a, b]
of continuous functions on a bounded interval (with the maximum norm), the operator
A = d/dx is not deﬁned everywhere, since there are continuous functions which are not
differentiable. By Weierstrass’approximation theorem the set of polynomials is everywhere
dense in C, and hence the set of differentiable functions is so too. Moreover, even if Au
exists, A2u may not exist. That A−1 may not exist is no novel feature of inﬁnite-dimensional
spaces. In C[a, b] the norm of A = d/dx is inﬁnite. This operator is said to be unbounded.
4.5.3
Inner Product Spaces and Orthogonal Systems
An abstract foundation for least squares approximation is furnished by the theory of inner
product spaces which we now introduce.

4.5. Approximation and Function Spaces
451
Deﬁnition 4.5.8.
A normed linear space S will be called an inner product space if for each two elements
f, g in S there is a scalar designated by (f, g) with the following properties:
1.
(f + g, h) = (f, h) + (g, h)
(linearity),
2.
(f, g) = (g, f )
(symmetry),
3.
(f, αg) = α(f, g),
α scalar
(homogeneity),
4.
(f, f ) ≥0,
(f, f ) = 0
(positivity).
The inner product (f, g) is scalar, i.e., real in a real space and complex in a complex space.
We set ∥f ∥= (f, f )1/2. If ∥f ∥= 0 implies that f = 0 this is a norm on S; otherwise it
is a seminorm on S.
We shall show below that the triangle inequality is satisﬁed. (The other axioms for a
norm are obvious.) The standard vector inner products introduced in Sec. A.1.2 in Online
Appendix A are particular cases, if we set (x, y) = yT x in Rn and (x, y) = yHx in Cn. A
complete inner-product space is called a Hilbert space and is often denoted H in this book.
One can make computations using the more general deﬁnition of (f, g) given above
in the same way that one does with scalar products in linear algebra. Note, however, the
conjugations necessary in a complex space,
(αf, g) = ¯α(f, g),
(4.5.17)
because, by the axioms,
(αf, g) = (g, αf ) = α(g, f ) = ¯α(g, f ) = ¯α(f, g).
By the axioms it follows by induction that

φk,
n

j=0
cjφj

=
n

j=0
(φk, cjφj) =
n

j=0
cj(φk, φj).
(4.5.18)
Theorem 4.5.9 (Cauchy–Schwarz inequality).
The Cauchy–Schwarz inequality in a complex space is
|(f, g)| ≤∥f ∥∥g∥.
Proof. Let f , g be two arbitrary elements in an inner-product space. Then,150 for every
real number λ,
0 ≤(f + λ(f, g)g, f + λ(f, g)g) = (f, f ) + 2λ|(f, g)|2 + λ2|(f, g)|2(g, g).
This polynomial in λ with real coefﬁcients cannot have two distinct zeros, hence the dis-
criminant cannot be positive, i.e.,
|(f, g)|4 −(f, f )|(f, g)|2(g, g) ≤0.
So, even if (f, g) = 0, |(f, g)|2 ≤(f, f )(g, g).
150We found this proof in [304, sec. 83]. The application of the same idea in a real space can be made simpler.

452
Chapter 4. Interpolation and Approximation
By Deﬁnition 4.5.8 and the Cauchy–Schwarz inequality,
∥f + g∥2 = (f + g, f + g) = (f, f ) + (f, g) + (g, f ) + (g, g)
≤∥f ∥2 + ∥f ∥∥g∥+ ∥g∥∥f ∥+ ∥g∥2 = (∥f ∥+ ∥g∥)2.
This shows that the triangle inequality is satisﬁed by the norm deﬁned above.
Example 4.5.9.
The set of all complex inﬁnite sequences {xi} for which ∞
i=1 |xi|2 < ∞and which
is equipped with the inner product
(x, y) =
∞

i=1
xiyi
constitutes a Hilbert space.
Deﬁnition 4.5.10.
Two functions f and g are said to be orthogonal if (f, g) = 0. A ﬁnite or inﬁnite
sequence of functions φ0, φ1, . . . , φn constitutes an orthogonal system if
(φi, φj) = 0,
i ̸= j, and ∥φi∦= 0
∀i.
(4.5.19)
If, in addition, ∥φi∥= 1, for all i ≥0, then the sequence is called an orthonormal system.
Theorem 4.5.11 (Pythagoras’ Theorem).
Let {φ1, φ2, . . . , φn} be an orthogonal system in an inner-product space. Then

n

j=0
cjφj

2
=
n

j=0
|cj|2∥φj∥2.
The elements of an orthogonal system are linearly independent.
Proof. We start as in the proof of the triangle inequality:
∥f + g∥2 = (f, f ) + (f, g) + (g, f ) + (g, g) = (f, f ) + (g, g) = ∥f ∥2 + ∥g∥2.
Using this result and induction the ﬁrst statement follows. The second statement then
follows because  cjφj = 0 ⇒|cj| = 0 for all j.
Theorem 4.5.12.
A linear operator P is called idempotent if P = P 2. Let V be the range of P. Then
P is a projection (or projector) onto V if and only if P is idempotent and Pv = v for each
v ∈V.
Proof. If P is a projection, then v = Px for some x ∈B, hence Pv = P 2x = Px = v.
Conversely, if Q is a linear operator such that Qx ∈V for all x ∈B, and v = Qv for all
v ∈V , then Q is a projection; in fact Q = P.

4.5. Approximation and Function Spaces
453
Note that I −P is also a projection, because
(I −P)(I −P) = I −2P + P 2 = I −P.
Any vector x ∈B can be written uniquely in the form
x = u + w,
u = Px,
w = (I −P)x.
(4.5.20)
Important examples of projections in function spaces are interpolation operators, for
example, the mapping of C[a, b] into Pk by Newton or Lagrange interpolation, because
each polynomial is mapped to itself. The two types of interpolation are the same projection,
although they use different bases in Pk. Another example is the mapping of a linear space of
functions, analytic on the unit circle, into Pk so that each function is mapped to its Maclaurin
expansion truncated to Pk. There are analogous projections where periodic functions and
trigonometric polynomials are involved
In an inner-product space, the adjoint operator A∗of a linear operator A is deﬁned
by the requirement that
(A∗u, v) = (u, Av)
∀u, v.
(4.5.21)
An operator A is called self-adjoint if A = A∗. In Rn, we deﬁne (u, v) = uT v, i.e.,
the standard scalar product. Then (A∗u)T v = uT Av, i.e., uT ((A∗)T v = uT Av, hence
A∗= AT . It follows that symmetric matrices are self-adjoint in Rn.
In Cn, with the inner product (u, v) = uHv, it follows that A∗= AH, i.e., Hermitian
matrices are self-adjoint. An operator B is positive deﬁnite if (u, Bu) > 0 for all u ̸= 0.
Example 4.5.10.
An important example of an orthogonal system is the sequence of trigonometric func-
tions φj(x) = cos jx, j = 0 : N −1. These form an orthogonal system, with either of the
two inner products
(f, g) =



 π
0
f (x)g(x) dx
(continuous case),
N−1

i=0
f (xi)g(xi),
xi = 2i + 1
N
π
2
(discrete case).
(4.5.22)
Moreover, it holds that
∥φj∥2 =



1
2π,
j ≥1,
∥φ0∥2 = π
(continuous case),
1
2N,
j = 1 : N −1,
∥φ0∥2 = N
(discrete case).
These results are closely related to the orthogonality of the Chebyshev polynomials; see
Theorem 4.5.20. Trigonometric interpolation and Fourier analysis will be treated in Sec. 4.6.
There are many other examples of orthogonal systems. Orthogonal systems of poly-
nomials play an important role in approximation and numerical integration. Orthogonal
systems also occur in a natural way in connection with eigenvalue problems for differential
equations, which are quite common in mathematical physics.

454
Chapter 4. Interpolation and Approximation
4.5.4
Solution of the Approximation Problem
Orthogonal systems give rise to extraordinary formal simpliﬁcations in many situations.
We now consider the least squares approximation problem.
Theorem 4.5.13.
If φ0, φ1, . . . , φn are linearly independent, then the least squares approximation prob-
lem of minimizing the norm of the error function ∥f ∗−f ∥over all functions f ∗=
n
j=0 cjφj has the unique solution
f ∗=
n

j=0
c∗
jφj.
(4.5.23)
The solution is characterized by the orthogonality property that f ∗−f is orthogonal to
all φj, j = 0 : n. The coefﬁcients c∗
j are called orthogonal coefﬁcients (or Fourier
coefﬁcients), and satisfy the linear system of equations
n

j=0
(φj, φk)c∗
j = (f, φk),
(4.5.24)
called normal equations. In the important special case when φ0, φ1, . . . , φn form an or-
thogonal system, the coefﬁcients are computed more simply by
c∗
j = (f, φj)/(φj, φj),
j = 0 : n.
(4.5.25)
Proof. Let (c0, . . . , cn) be a sequence of coefﬁcients with cj ̸= c∗
j for at least one j. Then
n

j=0
cjφj −f =
n

j=0
(cj −c∗
j)φj + (f ∗−f ).
If f ∗−f is orthogonal to all the φj, then it is also orthogonal to the linear combination
n
j=0(cj −c∗
j)φj and, according to the Pythagorean theorem,

n

j=0
cjφj −f

2
=

n

j=0
(cj −c∗
j)φj

2
+ ∥(f ∗−f )∥2 > ∥(f ∗−f )∥2.
Thus if f ∗−f is orthogonal to all φk, then f ∗is a solution to the approximation problem.
It remains to show that the orthogonality conditions

n

j=0
c∗
jφj −f, φk

= 0,
k = 0 : n,
can be fulﬁlled. The above conditions are equivalent to the normal equations in (4.5.24). If
{φj}n
j=0 constitutes an orthogonal system, then the system can be solved immediately, since
in each equation all the terms with j ̸= k are zero. The formula in (4.5.25) then follows
immediately.

4.5. Approximation and Function Spaces
455
Suppose now that we know only that {φj}n
j=0 are linearly independent. The solution
to the normal equations exists and is unique, unless the homogeneous system,
n

j=0
(φj, φk)c∗
j = 0,
k = 0 : n,
has a solution c0, c1, . . . , cn with at least one ci ̸= 0. But this would imply

n

j=0
cjφj

2
=

n

j=0
cjφj,
n

k=0
ckφk

=
n

k=0
n

j=0
(φj, φk)cjck =
n

k=0
0 · ck = 0,
which contradicts that the φj were linearly independent.
In the case where {φj}n
j=0 form an orthogonal system, the Fourier coefﬁcients c∗
j
are independent of n (see formula (4.5.25)). This has the important advantage that one can
increase the total number of parameters without recalculating any previous ones. Orthogonal
systems are advantageous not only because they greatly simplify calculations; using them,
one can often avoid numerical difﬁculties with roundoff error which may occur when one
solves the normal equations for a nonorthogonal set of basis functions.
With every continuous function f one can associate an inﬁnite series,
f ∼
∞

j=0
c∗
jφj,
c∗
j = (f, φj)
(φj, φj).
Such a series is called an orthogonal expansion. For certain orthogonal systems this series
converges with very mild restrictions on the function f .
Theorem 4.5.14.
If f ∗is deﬁned by formulas (4.5.23) and (4.5.25), then
∥f ∗−f ∥2 = ∥f ∥2 −∥f ∗∥2 = ∥f ∥2 −
n

j=0
(c∗
j)2∥φj∥2.
Proof. Since f ∗−f is, according to Theorem 4.5.13, orthogonal to all φj, 0 ≤j ≤n,
then f ∗−f is orthogonal to f ∗. The theorem then follows directly from Pythagoras’
theorem.
We obtain as corollary Bessel’s inequality:
n

j=0
(c∗
j)2∥φj∥2 ≤∥f ∥2.
(4.5.26)
The series ∞
j=0(c∗
j)2∥φj∥2 is convergent. If ∥f ∗−f ∥→0 as n →∞, then the sum of
the latter series is equal to ∥f ∥2, which is Parseval’s identity.

456
Chapter 4. Interpolation and Approximation
Theorem 4.5.15.
If {φj}m
j=0 are linearly independent on the grid G = {xi}m
i=0, then the interpolation
problem of determining the coefﬁcients {cj}m
j=0 such that
m

j=0
cjφj(xi) = f (xi),
i = 0 : m,
(4.5.27)
has exactly one solution. Interpolation is a special case (n = m) of the method of least
squares. If {φj}m
j=0 is an orthogonal system, then the coefﬁcients cj are equal to the orthog-
onal coefﬁcients in (4.5.25).
Proof. The system of equations (4.5.27) has a unique solution, since its column vectors
are the vectors φj(G), j = 0 : n, which are linearly independent. For the solution of the
interpolation problem it holds that ∥cjφj −f ∥= 0; i.e., the error function has the least
possible seminorm. The remainder of the theorem follows from Theorem 4.5.13.
The following collection of important and equivalent properties is named the fun-
damental theorem of orthonormal expansions by Davis [92, Theorem 8.9.1], whom we
follow closely at this point.
Theorem 4.5.16.
Let φ0, φ1, φ2, · · · be a sequence of orthonormal elements in a complete inner product
space H. The following six statements are equivalent:151
(A) The φj is a complete orthonormal system in H.
(B) The orthonormal expansion of any element y ∈H converges in norm to y; i.e.,
lim
n→∞
y −
n

j=0
(y, φj)φj
.
(4.5.28)
(C) Parseval’s identity holds for every y ∈H, i.e.,
∥y∥2 =
∞

j=0
|(y, φj)|2.
(4.5.29)
(D) There is no strictly larger orthonormal system containing φ1, φ2, . . . .
(E) If y ∈H and (y, φj) = 0, j = 0, 2, . . . , then y = 0.
(F) An element of H is determined uniquely by its Fourier coefﬁcients, i.e., if (w, φj) =
(y, φj), j = 0, 2, . . . , then w = y.
151We assume that H is not ﬁnite-dimensional, in order to simplify the formulations. Only minor changes are
needed in order to cover the ﬁnite-dimensional case.

4.5. Approximation and Function Spaces
457
Proof. The proof that A ⇔B is formulated with respect to the previous statements. By the
conjugations necessary in the handling of complex scalars in inner products (see (4.5.17)
and (4.5.18)),

x −
∞

j=0
(x, φj)φj, y −
n

j=0
(y, φj)φj

= (x, y) −
n

j=0
(x, φj)(φj, y).
By the Schwarz inequality,
(((((x, y) −
n

j=0
(x, φj)(φj, y)
(((( ≤
x −
∞

j=0
(x, φj)φj
 ·
y −
n

j=0
(y, φj)φj
.
For the rest of the proof, see Davis [92, pp. 192 ff.].
Theorem 4.5.17.
The converse of statement (F) holds, i.e., let H be a complete inner product space,
and let aj be constants such that ∞
j=0 |aj|2 < ∞. Then there exists an y ∈H such that
y = ∞
j=0 ajφj and (y, φj) = aj for all j.
Proof. See Davis [92, Sec. 8.9].
4.5.5
Mathematical Properties of Orthogonal Polynomials
By a family of orthogonal polynomials we mean a triangle family of polynomials (see
(4.1.8)), which (in the continuous case) is an orthogonal system with respect to a given
inner product. The theory of orthogonal polynomials is also of fundamental importance for
many problems which at ﬁrst sight seem to have little connection with approximation (e.g.,
numerical integration, continued fractions, and the algebraic eigenvalue problem).
We assume in the following that in the continuous case the inner product is
(f, g) =
 b
a
f (x)g(x)w(x) dx,
w(x) ≥0,
(4.5.30)
where −∞≤a < b ≤∞. We assume that the weight function w(x) ≥0 is such that the
moments
µk = (xk, 1) =
 b
a
xkw(x) dx
(4.5.31)
are deﬁned for all k ≥0, and µ0 > 0. In the discrete case, we deﬁne the weighted discrete
inner product of two real-valued functions f and g on the grid {xi}m
j=0 of distinct points by
(f, g) =
m

i=0
wif (xi)g(xi),
wi > 0.
(4.5.32)
Note that both these inner products have the property that
(xf, g) = (f, xg).
(4.5.33)

458
Chapter 4. Interpolation and Approximation
The continuous and discrete case are both special cases of the more general inner product
(f, g) =
 b
a
f (x)g(x) dα(x),
(4.5.34)
where the integral is a Stieltjes integral (see Deﬁnition 3.4.4) and α(x) is allowed to be
discontinuous. However, in the interest of clarity, we will in the following treat the two
cases separately.
The weight function w(x) determines the orthogonal polynomials φn(x) up to a con-
stant factor in each polynomial. The speciﬁcation of those factors is referred to as stan-
dardization. These polynomials satisfy a number of relationships of the same general
form. In the case of a continuously differentiable weight function w(x) we have an explicit
expression
φn(x) =
1
anw(x)
dn
dxn {w(x)(g(x))n},
n = 0, 1, 2, . . . ,
(4.5.35)
where g(x) is a polynomial in x independent of n. This is Rodrigues’ formula. The
orthogonal polynomials also satisfy a second order differential equation,
g2(x)φ′′
n + g1(x)φ′
n + anφn = 0,
(4.5.36)
where g2(x) and g1(x) are independent of n and an is a constant only dependent on n.
Let pn(x) = knxn +· · · , n = 0, 1, 2, . . . , be a family of real orthogonal polynomials.
The symmetric function
Kn(x, y) =
n

k=0
pk(x)pk(y)
(4.5.37)
is called the kernel polynomial of order n for the orthogonal system. It can be shown that
the kernel polynomial has the reproducing property that for every polynomial p of degree
at most n
(p(x), Kn(x, y)x) = p(y).
(4.5.38)
Here the subscript x indicates that the inner product is taken with respect to x. Conversely,
if K(x, y) is a polynomial of degree at most n in x and y and if (p(x), K(x, y)x) = p(y),
for all polynomials p of degree at most n, then K(x, y) = Kn(x, y).
An alternative expression, the Christoffel–Darboux formula, can be given for the
kernel polynomial.
Theorem 4.5.18.
Let pn(x) = knxn + · · ·, n = 0, 1, 2, . . . , be real orthonormal polynomials. Then
Kn(x, y) =
kn
kn+1
pn+1(x)pn(y) −pn(x)pn+1(y)
x −y
.
(4.5.39)
Proof. See Davis [92, Theorem 10.1.6].
Given a linearly independent sequence of vectors, an orthogonal system can be derived
by a process analogous to Gram–Schmidt orthogonalization.

4.5. Approximation and Function Spaces
459
Theorem 4.5.19.
For every weight function in an inner product space there is a triangle family of
orthogonal polynomials φk(x), k = 0, 1, 2, . . . , such that φk(x) has exact degree k, and
is orthogonal to all polynomials of degree less than k. The family is uniquely determined
apart from the fact that the leading coefﬁcients can be given arbitrary positive values.
The monic orthogonal polynomials satisfy the three-term recurrence formula,
φk+1(x) = (x −βk)φk(x) −γ 2
k−1φk−1(x),
k ≥1,
(4.5.40)
with initial values φ−1(x) = 0, φ0(x) = 1.
The recurrence coefﬁcients are given by
Darboux’s formulas
βk = (xφk, φk)
∥φk∥2
,
γ 2
k−1 =
∥φk∥2
∥φk−1∥2 .
(4.5.41)
Proof. The proof is by induction. We have φ−1 = 0, φ0 = 1. Suppose that φj ̸= 0 have
been constructed for 0 ≤j ≤k, k ≥0. We now seek a polynomial φk+1 of degree k + 1
with leading coefﬁcient equal to 1 which is orthogonal to all polynomials of degree ≤k.
Since {φj}k
j=0 is a triangle family, every polynomial of degree k can be expressed as a linear
combination of these polynomials. Therefore, we can write
φk+1 = xφk −
k

i=0
ck,iφi,
(4.5.42)
where φk+1 has leading coefﬁcient one. The orthogonality condition is fulﬁlled if and only
if
(xφk, φj) −
k

i=0
ck,i(φi, φj) = 0,
j = 0 : k.
But (φi, φj) = 0 for i ̸= j, and thus ck,j∥φj∥2 = (xφk, φj). This determines the coefﬁcients
uniquely. From the deﬁnition of inner product (4.5.30), it follows that
(xφk, φj) = (φk, xφj).
But xφj is a polynomial of degree j + 1. Thus if j < k, then it is orthogonal to φk. So
ckj = 0 for j < k −1. From (4.5.42) it then follows that
φk+1 = xφk −ck,kφk −ck,k−1φk−1,
(4.5.43)
with ck,k−1 = 0 if k = 0. This has the same form as the original assertion of the theorem if
we set
βk = ck,k = (xφk, φk)
∥φk∥2
,
(4.5.44)
γ 2
k−1 = ck,k−1 = (φk, xφk−1)
∥φk−1∥2
,
k ≥1.
In the discrete case the division in (4.5.44) can always be performed, as long as k ≤m. In
the continuous case, no reservation need be made.

460
Chapter 4. Interpolation and Approximation
The expression for γ 2
k−1 can be written in another way. If we take the inner product
of (4.5.42) and φk+1 we get
(φk+1, φk+1) = (φk+1, xφk) −
k

i=0
ck,i(φk+1, φi) = (φk+1, xφk).
Thus (φk+1, xφk) = ∥φk+1∥2, or if we decrease all indices by one, (φk, xφk−1) = ∥φk∥2.
Substituting this in the expression for γ 2
k−1 gives the second equation of (4.5.41).
If the weight distribution w(x) is symmetric about β, i.e., (in the continuous case)
w(β −x) = w(x + β), then βk = β for all k ≥0. Further,
φk(β −x) = (−1)kφk(x + β),
k ≥0;
(4.5.45)
that is, φk is symmetric about β for k even and antisymmetric for k odd. The proof is by
induction. We have φ0 = 1 and φ1(x) = x −β0. Clearly (φ1, φ0) = 0 implies that φ1 is
antisymmetric about β and therefore β0 = β. Thus the hypothesis is true for k ≤1. Now
assume that (4.5.45) holds for k ≤n. Then
βn = (xφn, φn)
∥φn∥2
= ((x −β)φn, φn)
∥φn∥2
+ β.
Here the ﬁrst term is zero since it is an integral of an antisymmetric function. It follows that
φn+1(x) = (x −β)φn(x) −γ 2
n−1φn−1(x),
which shows that (4.5.45) holds for k = n + 1. An analog result holds for a symmetric
discrete inner product.
Often it is more convenient to consider corresponding orthonormal polynomials
ˆφk(x), which satisfy ∥ˆφk∥= 1. We set ˆφ0 = 1/√µ0, µ0 = ∥φ0∥2
2, and scale the monic
orthogonal polynomials according to
φk = (γ1 · · · γk−1) ˆφk,
k ≥1;
(4.5.46)
then we ﬁnd using (4.5.41) that
∥ˆφk∥
∥ˆφk−1∥
= γ1 · · · γk−2
γ1 · · · γk−1
∥φk∥
∥φk−1∥= 1.
Substituting (4.5.46) in (4.5.40) we obtain the recurrence relation for the orthonormal poly-
nomials
γk ˆφk+1(x) = (x −βk) ˆφk(x) −γk−1 ˆφk−1(x),
k ≥1,
(4.5.47)
where γk is determined by the condition ∥ˆφk+1∥= 1.
Perhaps the most important example of a family of orthogonal polynomials is the
Chebyshev polynomials Tn(x) = cos(n arccos(x)) introduced in Sec. 3.2.3. These are
orthogonal on [−1, 1] with respect to the weight function (1−x2)−1/2 and also with respect
to a discrete inner product. Their properties can be derived by rather simple methods.

4.5. Approximation and Function Spaces
461
Theorem 4.5.20.
The Chebyshev polynomials have the following two orthogonality properties. Set
(f, g) =
 1
−1
f (x)g(x)(1 −x2)−1/2 dx
(4.5.48)
(the continuous case). Then (T0, T0) = π, and
(Tj, Tk) =
% 0
if j ̸= k,
π/2
if j = k ̸= 0.
(4.5.49)
Let xk be the zeros of Tm+1(x) and set
(f, g) =
m

k=0
f (xk)g(xk),
xk = cos
2k + 1
m + 1
π
2

(4.5.50)
(the discrete case). Then (T0, T0) = m + 1, and
(Tj, Tk) =
% 0
if j ̸= k,
(m + 1)/2
if j = k ̸= 0.
(4.5.51)
Proof. In the continuous case, let j ̸= k, j ≥0, k ≥0. From x = cosφ it follows that
dx = sin φdφ = (1 −x2)1/2dφ. Hence
(Tj, Tk) =
 π
0
cos jx cos kx dx =
 π
0
1
2

cos(j −k)x + cos(j + k)x

dx
= 1
2
sin(j −k)π
j −k
+ sin(j + k)π
j + k

= 0,
whereby orthogonality is proved.
In the discrete case, set h = π/(m + 1), xµ = h/2 + µh,
(Tj, Tk) =
m

µ=0
cos jxµ cos kxµ = 1
2
m

µ=0

cos(j −k)xµ + cos(j + k)xµ

.
Using notation from complex numbers (i =
√
−1) we have
(Tj, Tk) = 1
2Re

m

µ=0
ei(j−k)h(1/2+µ) +
m

µ=0
ei(j+k)h(1/2+µ)

.
(4.5.52)
The sums in (4.5.52) are geometric series with ratios ei(j−k)h and ei(j+k)h, respectively. If
j ̸= k, 0 ≤j ≤m, 0 ≤k ≤m, then the ratios are never equal to one, since
0 < | (j ± k)h| ≤
2m
m + 1π < π.

462
Chapter 4. Interpolation and Approximation
Using the formula for the sum of a geometric series the ﬁrst sum in (4.5.52) is
ei(j−k)(h/2) ei(j−k)(m+1)h −1
ei(j−k)h −1
=
ei(j−k)π −1
ei(j−k)(h/2) −e−i(j−k)(h/2) −1
=
(−1)j−k −1
2i sin(j −k)h/2.
The real part of the last expression is clearly zero. An analogous computation shows that
the real part of the other sum in (4.5.52) is also zero. Thus the orthogonality property holds
in the discrete case also. It is left to the reader to show that the expressions when j = k
given in the theorem are correct.
For the uniform weight distribution w(x) = 1 on [−1, 1] the relevant orthogonal poly-
nomials are the Legendre polynomials152 Pn(x). The Legendre polynomials are deﬁned
by Rodrigues’ formula
Pn(x) = (−1)n 1
2nn!
dn
dxn {(1 −x2)n},
n = 0, 1, 2, . . . .
(4.5.53)
Since (1 −x2)n is a polynomial of degree 2n, Pn(x) is a polynomial of degree n. The
Legendre polynomials Pn = Anxn + · · · have leading coefﬁcient and norm
An =
(2n)!
2n(n!)2 ,
∥Pn∥=
2
2n + 1.
(4.5.54)
This standardization corresponds to setting Pn(1) = 1 for all n ≥0. The extreme values
are
|Pn(x)| ≤1,
x ∈[−1, 1].
There seems to be no easy proof for the last result; see [193, p. 219].
Since the weight distribution is symmetric about the origin, these polynomials have
the symmetry property
Pn(−x) = (−1)nPn(x).
The Legendre polynomials satisfy the three-term recurrence formula P0(x) = 1, P1(x) = x,
Pn+1(x) = 2n + 1
n + 1 xPn(x) −
n
n + 1Pn−1(x),
n ≥1.
(4.5.55)
The ﬁrst few Legendre polynomials are
P2(x) = 1
2(3x2 −1),
P3(x) = 1
2(5x3 −3x),
P4(x) = 1
8(35x4 −30x2 + 3),
P5(x) = 1
8(63x5 −70x3 + 15), . . . .
A graph of the Legendre polynomial P21(x) is shown in Figure 4.5.1.
152Legendre had obtained these polynomials in 1784–1789 in connection with his investigation concerning the
attraction of spheroids and the shape of planets.

4.5. Approximation and Function Spaces
463
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Figure 4.5.1. The Legendre polynomial P21.
Often a more convenient standardization is to consider monic Legendre polynomials,
with leading coefﬁcient equal to one.
These satisfy P0(x) = 1, P1(x) = x, and the
recurrence formula
Pn+1(x) = xPn(x) −
n2
4n2 −1Pn−1(x),
n ≥1;
(4.5.56)
note that we have kept the same notation for the polynomials. It can be shown that
Pn = xn + cnxn−2 + · · · ,
cn = −n(n −1)
2(2n −1).
The Jacobi polynomials153 Jn(x; α, β) arise from the weight function
w(x) = (1 −x)α(1 + x)β,
x ∈[−1, 1],
α, β > −1.
They are special cases of Gauss’ hypergeometric function F(a, b, c : x),
F(−n, α + 1 + β + n, α + 1; x),
(see (3.1.16)). The Jacobi polynomials are usually standardized so that the coefﬁcient An
of xn in Jn(x; α, β) is given by
An =
1
2nn!
T(2n + α + β + 1)
T(n + α + β + 1) .
The Legendre polynomials are obtained as the special case when α = β = 0. The case
α = β = −1/2, which corresponds to the weight function w(x) = 1/
√
1 −x2, gives the
Chebyshev polynomials.
153Carl Gustav Jacob Jacobi (1805–1851) was a German mathematician. Jacobi joined the faculty of Berlin
University in 1825. Like Euler, he was a proﬁcient calculator who drew a great deal of insight from immense
algorithmic work.

464
Chapter 4. Interpolation and Approximation
The generalized Laguerre polynomials L(α)
n (x) are orthogonal with respect to the
weight function
w(x) = xαe−x,
x ∈[0, ∞],
α > −1.
Setting α = 0, we get the Laguerre polynomials L(0)
n (x) = Ln(x). Standardizing these so
that Ln(0) = 1, they satisfy the three-term recurrence relation
(n + 1)Ln+1(x) = (2n + 1 −x)Ln(x) −nLn−1(x),
(4.5.57)
Rodrigues’ formula becomes
Lα
n(x) =
ex
n!x(α)
dn
dxn (xn−αe−x).
The Hermite polynomials are orthogonal with respect to the weight function
w(x) = e−x2,
−∞< x < ∞.
With the classic standardization they satisfy the recurrence relation H0(x) = 1, H1(x) = 2x,
Hn+1(x) = 2xHn(x) −2nHn−1(x),
and
Hn(0) =
! (−1)m(2m)!/m!
if n = 2m,
0
if n = 2m + 1.
The Hermite polynomials can also be deﬁned by Rodrigues’ formula:
Hn(x) = (−1)nex2 dn
dxn {e−x2}.
It can be veriﬁed that these polynomials are identical to those deﬁned by the recurrence
relation.
The properties of some important families of orthogonal polynomials are summarized
in Table 4.5.1. Note that here the coefﬁcients in the three-term recurrence relation are given
for the monic orthogonal polynomials; cf. (4.5.40).
For equidistant data, the Gram polynomials {Pn,m}m
n=0 are of interest.154
These
polynomials are orthogonal with respect to the discrete inner product
(f, g) = (1/m)
m

i=1
f (xi)g(xi),
xi = −1 + (2i −1)/m.
The weight distribution is symmetric around the origin αk = 0. For the monic Gram
polynomials the recursion formula is (see [16])
P−1,m(x) = 0,
P0,m = 1,
Pn+1,m(x) = xPn,m(x) −βn,mPn−1,m(x),
n = 0 : m −1,
154Jørgen Pedersen Gram (1850–1916), a Danish mathematician, graduated from Copenhagen University and
worked as a director for a life insurance company. He introduced the Gram determinant in connection with his
study of linear independence, and his name is also associated with Gram–Schmidt orthogonalization.

4.5. Approximation and Function Spaces
465
Table 4.5.1. Weight functions and recurrence coefﬁcients for some classical monic
orthogonal polynomials.
[a, b]
w(x)
Orthog.
pol.
µ0
βk
γ 2
k
[−1, 1]
1
Pn(x) Legendre
2
0
k2
4k2 −1
[−1, 1]
(1 −x2)−1/2
Tn(x) Cheb. 1st
π
0
1
2 (k = 1)
1
4 (k > 1)
[−1, 1]
(1 −x2)1/2
Un(x) Cheb. 2nd
π/2
0
1
4
[−1, 1]
(1 −x)α(1 + x)β
Jn(x; α, β) Jacobi
[0, ∞]
xαe−x, α > −1
L(α)
n (x) Laguerre
T(1 + α)
2k + α + 1
k(k + α)
[−∞, ∞]
e−x2
Hn(x) Hermite
√π
0
1
2k
where (n < m) and
βn,m =
n2
4n2 −1

1 −n2
m2

.
When n ≪m1/2, these polynomials are well behaved. But when n ≥m1/2, the Gram
polynomials have very large oscillations between the grid points, and a large maximum
norm in [−1, 1]. This fact is related to the recommendation that when ﬁtting a polynomial
to equidistant data, one should never choose n larger than about 2m1/2.
Complex Orthogonal Polynomials
So far we have considered the inner products (4.5.30) deﬁned by an integral over the real
interval [a, b]. Now let T be a rectiﬁable curve (i.e., a curve of ﬁnite length) in the complex
plane. Consider the linear space of all polynomials with complex coefﬁcients and z = x+iy
on T. Let α(s) be a function on T with inﬁnitely many points of increase and deﬁne an
inner product by the line integral
(p, q) =

T
p(z)q(z)w(s) d(s).
(4.5.58)
The complex monomials 1, z, z2, . . . are independent functions, since if a0 + a1z + a2z2 +
· · · + anzn ≡0 on T it would follow from the fundamental theorem of algebra that a0 =
a1 = a2 = · · · = an = 0. There is a unique inﬁnite sequence of polynomials φj =
zj + c1zj−1 + · · · + cj, j = 0, 1, 2, . . . , which are orthonormal with respect to (4.5.58).
They can be constructed by Gram–Schmidt orthogonalization as in the real case.

466
Chapter 4. Interpolation and Approximation
An important case is when T is the unit circle in the complex plane. We then write
(p, q) = 1
2π
 π
−π
p(z)q(z) dα(t),
z = eit,
(4.5.59)
where the integral is to be interpreted as a Stieltjes integral. The corresponding orthog-
onal polynomials are known as Szeg˝o polynomials and have applications, e.g., in signal
processing.
Properties of Szeg˝o polynomials are discussed in [174]. Together with the reverse
polynomials ˜φj(z) = zjφj(1/z) they satisfy special recurrence relations. A linear combi-
nation n
j=0 cjφj(z) can be evaluated by an analogue to the Clenshaw algorithm; see [5].
4.5.6
Expansions in Orthogonal Polynomials
Expansions of functions in terms of orthogonal polynomials are very useful. They are easy
to manipulate, have good convergence properties in the continuous case, and usually give
a well-conditioned representation.
Let ˆpn denote the polynomial of degree less than n for which
∥f −ˆpn∥∞= En(f ) = min
p∈Pn ∥f −p∥∞,
and set
pn =
n−1

j=0
cjφj,
where cj is the jth Fourier coefﬁcient of f and {φj} are the orthogonal polynomials with
respect to the inner product (4.5.30). If we use the weighted Euclidean norm, ˆpn is of course
not a better approximation than pn. In fact,
∥f −pn∥2
w =
 b
a
|f (x) −pn(x)|2w(x) dx
≤
 b
a
|f (x) −ˆpn(x)|2w(x) dx ≤En(f )2
 b
a
w(x) dx.
(4.5.60)
This can be interpreted as saying that a kind of weighted mean of |f (x) −pn(x)| is less
than or equal to En(f ), which is about as good a result as one could demand. The error
curve has an oscillatory behavior. In small subintervals, |f (x)−pn(x)| can be signiﬁcantly
greater than En(f ). This is usually near the ends of the intervals or in subintervals where
w(x) is relatively small. Note that from (4.5.60) and Weierstrass’ approximation theorem
it follows that
lim
n→∞∥f −p∥2
2,w = 0
for every continuous function f . From (4.5.60) one gets after some calculations
∞

j=n
c2
j∥φj∥2 = ∥f −pn∥2
2,w ≤En(f )2
 b
a
w(x) dx,
which gives one an idea of how quickly the terms in the orthogonal expansion decrease.

4.5. Approximation and Function Spaces
467
Example 4.5.11 (Chebyshev Interpolation).
Let p(x) denote the interpolation polynomial in the Chebyshev points xk
=
cos( 2k+1
m+1
π
2 )k = 0 : m. For many reasons it is practical to write this interpolation poly-
nomial in the form
p(x) =
m

i=0
ciTi(x).
(4.5.61)
Then using the discrete orthogonality property (4.5.51)
ci = (f, Ti)
∥Ti∥2 =
1
∥Ti∥2
m

k=0
f (xk)Ti(xk)
(4.5.62)
where
∥T0∥2 = m + 1,
∥Ti∥2 = 1
2(m + 1),
i > 0.
The recursion formula (3.2.20) can be used for calculating the orthogonal coefﬁcients ac-
cording to (4.5.62).
For computing p(x) with (4.5.61), Clenshaw’s algorithm [71] can be used. Clen-
shaw’s algorithm holds for any sum of the form S = n
k=1 ckφk, where {φk} satisﬁes
a three-term recurrence relation. It can also be applied to series of Legendre functions,
Bessel functions, Coulomb wave functions, etc., because they satisfy recurrence relations
of this type, where the αk, γk depend on x; see the Handbook [1] or any text on special
functions.
Theorem 4.5.21 (Clenshaw’s Algorithm).
Suppose that a sequence {pk} satisﬁes the three-term recurrence relation
pk+1 = γkpk −βkpk−1,
k = 0 : n −1,
(4.5.63)
where p−1 = 0. Then
S =
n

k=0
ckpk = y0p0,
where yn+1 = 0, yn = cn, and y0 is obtained by the recursion
yk = ck + γk−1yk+1 −βkyk+2,
k = n −1 : −1 : 0.
(4.5.64)
Proof. Write the recursion (4.5.63) in matrix form as Lnp = p0e1, where
Ln =


1
−γ0
1
β1
−γ1
1
...
...
...
βn−1
−γn−1
1



468
Chapter 4. Interpolation and Approximation
is unit lower triangular and e1 is the ﬁrst column of the unit matrix. Then
S = cT p = cT L−1
n g = gT (LT
n )−1c = gT y,
where y is the solution to the upper triangular system LT
n y = c. Solving this by back-
substitution we get the recursion (4.5.64).
It can be proved that Clenshaw’s algorithm is componentwise backward stable with
respect to the data γk and βk; see Smoktunowicz [329].
Occasionally one is interested in the partial sums of (4.5.61). For example, if the
values of f (x) are afﬂicted with statistically independent errors with standard deviation σ,
then the series can be broken off when for the ﬁrst time
f −
p

i=0
ciTi(x)
 < σm1/2.
The proof of Theorem 4.5.19 is constructive and leads to a unique construction of the
sequence of orthogonal polynomials φk, n ≥1, with leading coefﬁcients equal to one. The
coefﬁcients βk and γk and the polynomials φk can be computed in the order
β0, φ1(x), β1, γ0, φ2(x), . . . .
(Recall that φ−1 = 0 and φ0 = 1.) This procedure is called the Stieltjes procedure. This
assumes that the inner product (xφk, φk) and norm ∥φk∥2 can be computed. This clearly
can be done for any discrete n-point weight.
For a continuous weight function w(x) this is more difﬁcult. One possible way to
proceed is then to approximate the integrals using some appropriate quadrature rule. For a
discussion of such methods and examples we refer to Gautschi [148].
There are many computational advantages of using the Stieltjes procedure when com-
puting the discrete least squares approximation
f (x) ≈pm(x) =
m

k=0
ckφk(x),
(4.5.65)
where φ0(x), . . . , φm(x) are the orthogonal polynomials with respect to the discrete weight
function (4.5.32). Recall the family ends with φm(x), since
φm+1(x) = (x −x0)(x −x1) · · · (x −xm)
is zero at each grid point and ∥φm+1∥= 0. The construction cannot proceed, which is
natural since there cannot be more than m + 1 orthogonal (or even linearly independent)
functions on a grid with m + 1 points.
By Theorem 4.5.13 the coefﬁcients are given by
ck = (f, φk)/∥φk∥2,
k = 0 : m.
(4.5.66)
Approximations of increasing degree can be recursively generated as follows. Suppose that
φj, j = 0 : k, and the least squares approximation pk of degree k, have been computed. In

4.5. Approximation and Function Spaces
469
the next step the coefﬁcients βk, γk−1, and φk+1 are computed and the next approximation
is obtained by
pk+1 = pk + ck+1φk+1,
ck+1 = (f, φk+1)/∥φk+1∥2.
(4.5.67)
To compute numerical values of p(x) Clenshaw’s algorithm is used; see Theorem 4.5.21.
For unit weights and a symmetric grid the Stieltjes procedure requires about 4mn ﬂops
to compute the orthogonal functions φk at the grid points. If there are differing weights, then
about mn additional operations are needed. Similarly, mn additional operations are required
if the grid is not symmetric. If the orthogonal coefﬁcients are determined simultaneously for
several functions on the same grid, then only about mn additional operations per function
are required. (In the above, we assume m ≫1, n ≫1.) Hence the procedure is much more
economical than the general methods based on normal equations, which require O(mn2)
ﬂops.
Since pk is a linear combination of φj, j = 0 : k, φk+1 is orthogonal to pk. Therefore,
an alternative expression for the new coefﬁcient is
ck+1 = (rk, φk+1)/∥φk+1∥2,
rk = f −pk,
(4.5.68)
where rk is the residual. Mathematically the two formulas (4.5.67) and (4.5.68) for ck+1
are equivalent. In ﬁnite precision, as higher-degree polynomials pk+1 are computed, they
will gradually lose orthogonality to previously computed pj, j ≤k. In practice there is
an advantage in using (4.5.68) since cancellation will then take place mostly in computing
the residual rk = f −pk, and the inner product (rk, pk+1) is computed more accurately.
Theoretically the error ∥pk −f ∥must be a nonincreasing function of k. Often the error
decreases rapidly with k and then pk provides a good representation of f already for small
values of k. Note that for n = m we obtain the (unique) interpolation polynomial for the
given points.
When using the ﬁrst formula one sometimes ﬁnds that the residual norm increases
when the degree of the approximation is increased! With the modiﬁed formula (4.5.68) this
is very unlikely to happen; see Problem 4.5.17.155
One of the motivations for the method of least squares is that it effectively reduces the
inﬂuence of random errors in measurements. We now consider some statistical aspects of
the method of least squares. First, we need a slightly more general form of Gauss–Markov’s
theorem.
Corollary 4.5.22.
Assume that in the linear model (1.4.2), ϵ has zero mean and positive deﬁnite covari-
ance matrix V . Then the normal equations for estimating c are
(AT V −1A)ˆc = AT V −1y.
(4.5.69)
In particular, if V = diag (σ 2
1 , σ 2
2 , . . . , σ 2
n) this corresponds to a row scaling of the linear
model y = Ac + ϵ.
155The difference between the two variants discussed here is similar to the difference between the so-called
classical and modiﬁed Gram–Schmidt orthogonalization methods.

470
Chapter 4. Interpolation and Approximation
Suppose that the values of a function have been measured in the points x1, x2, . . . , xm.
Let f (xp) be the measured value, and let ¯f (xp) be the “true” (unknown) function value
which is assumed to be the same as the expected value of the measured value. Thus no
systematic errors are assumed to be present. Suppose further that the errors in measurement
at the various points are statistically independent. Then we have a linear model f (xp) =
¯f (xp) + ϵ, where
E(ϵ) = 0,
V (ϵ) = diag (σ 2
1 , . . . , σ 2
n).
(4.5.70)
The problem is to use the measured data to estimate the coefﬁcients in the series
f (x) =
n

j=1
cjφ(x),
n ≤m,
where φ1, φ2, . . . , φn are known functions. According to the Gauss–Markov theorem and
its corollary the estimates c∗
j, which one gets by minimizing the sum
m

p=1
wp

f (xp) −
n

j=1
cjφj(xp)
2
,
wp = σ −2
p ,
have a smaller variance than the values one gets by any other linear unbiased estimation
method. This minimum property holds not only for the estimates of the coefﬁcients cj, but
also for every linear functional of the coefﬁcients, for example, the estimate
f ∗
n (α) =
n

j=1
c∗
jφj(α)
of the value f (α) at an arbitrary point α.
Suppose now that σp = σ for all p and that the functions {φj}n
j=1 form an orthonormal
system with respect to the discrete inner product
(f, g) =
m

p=1
f (xp)g(xp).
Then the least squares estimates are c∗
j = (f, φj), j = 1 : n. According to (1.4.4) the
estimates c∗
j and c∗
k are uncorrelated if j ̸= k and
E{(c∗
j −¯cj)(c∗
k −¯ck)} =
%
0
if j ̸= k,
σ 2
if j = k.
From this it follows that
var{f ∗
n (α)} = var
%
n

j=1
c∗
jφj(α)
)
=
n

j=1
var{c∗
j}|φj(α)|2 = σ 2
n

j=1
|φj(α)|2.
As an average, taken over the grid of measurement points, the variance of the smoothed
function values is
1
m
n

j=1
var{f ∗
n (xi)} = σ 2
m
n

j=1
m

i=1
|φj(xi)|2 = σ 2 n
m.

4.5. Approximation and Function Spaces
471
Between the grid points, however, the variance can in many cases be signiﬁcantly larger.
For example, when ﬁtting a polynomial to measurements in equidistant points, the Gram
polynomial Pn,m can be much larger between the grid points when n > 2m1/2. Set
σ 2
I = σ 2
n

j=1
1
2
 1
−1
|φ(x)|2 dx.
Thus σ 2
I is an average variance for f ∗
n (x) taken over the entire interval [−1, 1].
The
following values for the ratio ρ between σ 2
I and σ 2(n + 1)/(m + 1) when m = 42 were
obtained by H. Björk [32]. Related results are found in Reichel [298].
n + 1
5
10
15
20
25
30
35
ρ
1.0
1.1
1.7
26
7 · 103
1.7 · 107
8 · 1011
These results are related to the recommendation that one should choose n < 2m1/2 when
ﬁtting a polynomial to equidistant data. This recommendation seems to contradict the
Gauss–Markov theorem, but in fact it just means that one gives up the requirement that the
estimates are unbiased. Still, it is remarkable that this can lead to such a drastic reduction
of the variance of the estimates f ∗
n .
If the measurement points are the Chebyshev abscissae, then no difﬁculties arise in
ﬁtting polynomials to data. The Chebyshev polynomials have a magnitude between grid
points not much larger than their magnitude at the grid points. The average variance for f ∗
n
becomes the same on the interval [−1, 1] as on the net of measurements, σ2(n+1)/(m+1).
The choice of n when m is given is a question of compromising between taking into
account the truncation error (which decreases as n increases) and the random errors (which
grow when n increases). If f is a sufﬁciently smooth function, then in the Chebyshev
case |cj| decreases quickly with j. In contrast, the part of cj which comes from errors in
measurements varies randomly with a magnitude of about σ(2/(m + 1)1/2), using (4.5.50)
and ∥Tj∥2 = (m+1)/2. The expansion should be broken off when the coefﬁcients begin to
“behave randomly.” An expansion in terms of Chebyshev polynomials can hence be used
for ﬁltering away the “noise” from the signal, even when σ is initially unknown.
Example 4.5.12.
Fifty-one equidistant values of a certain analytic function were rounded to four dec-
imals. In Figure 4.5.2, a semilog diagram is given which shows how |ci| varies in an
expansion in terms of the Chebyshev polynomials for these data. For i > 20 (approxi-
mately) the contribution due to noise dominates the contribution due to signal. Thus it is
sufﬁcient to break off the series at n = 20.
4.5.7
Approximation in the Maximum Norm
In some applications it is more natural to seek an approximation that minimizes the maximum
deviation instead of the Euclidean norm. Let U be an (n + 1)-dimensional vector space of
continuous (real or complex) functions uk(x), k = 0 : n, deﬁned on an interval I = [a, b].

472
Chapter 4. Interpolation and Approximation
Figure 4.5.2. Magnitude of coefﬁcients ci in a Chebyshev expansion of an analytic
function contaminated with roundoff noise.
Given a function f ∈C[a, b], we consider the approximation problem of ﬁnding a function
˜u =
n

k=0
ckuk ∈U,
(4.5.71)
which minimizes the maximum norm of the error,
η(f, I) := inf
u∈U ∥f −u∥∞,I = inf
u∈U max
x∈I |f (x) −u(x)|.
Approximation in the maximum norm is often called Chebyshev approximation. We
avoidthisterminology, sinceChebyshev’snameisalsonaturallyassociatedwithtwoentirely
different approximation methods: interpolation in the Chebyshev abscissae, and expansion
in a series of Chebyshev polynomials.
The related discrete approximating problem is that of minimizing
inf
u∈U ∥f −u∥∞,X = inf
u∈U max
0≤i≤m |f (xi) −u(xi)|,
(4.5.72)
where X = {x0, . . . , xm} is a ﬁnite set of points. We note that the discrete approximation
can be posed as a linear programming problem with 2(m + 1) linear inequalities:
Minimize η
subject to −η ≤f (xi) −
n

k=0
ckuk(xi) ≤η,
i = 0 : m.
In principle this problem can be solved by the simplex method. Algorithms for solving this
problem are given in [17].

4.5. Approximation and Function Spaces
473
The discrete approximation problem in l1-norm corresponds to the linear program-
ming problem
Minimize
m

i=0
ηi
subject to −ηi ≤f (xi) −
n

k=0
ckuk(xi) ≤ηi,
i = 0 : m.
Algorithms for solving this problem are discussed in [18].
As a simple example, consider the problem of best approximation in the maximum
norm to the function f (x) = xn on [−1, 1] by a polynomial of lower degree. From the
minimax property of the Chebyshev polynomials it follows that the solution is given by the
polynomial
f ∗
n (x) = xn −21−nTn(x),
which has in fact degree n −2. The error function −21−nTn(x) assumes its extrema 21−n
on a sequence of n + 1 points, xk = cos(kπ/n). The sign of the error alternates at these
points, and thus for f (x) = xn
En−1(f ) = En−2(f ) = 21−n.
The above property can be generalized. We shall now give a theorem which is the
basis of many algorithms.
Theorem 4.5.23 (Chebyshev’s Equioscillation Theorem).
Let f be a continuous function on [a, b] and let ˜p be an nth degree polynomial which
best approximates f in the maximum norm. Then ˜p is characterized by the fact that there
exists at least n + 2 points,
a ≤x0 < x1 < · · · < xn+1 ≤b,
where the error (f −˜p) takes on its maximal magnitude η = ∥˜f −p∥∞with alternating
signs, that is,
f (xk) −˜p(xk) = −(f (xk+1) −˜p(xk+1)),
k = 0 : n.
(4.5.73)
This characterization constitutes both a necessary and a sufﬁcient condition. If f (n+1) has
constant sign in [a,b], then x0 = a and xn+1 = b.
Proof. We prove here the sufﬁciency of the condition (4.5.73). For the rest of the proof,
see, e.g., Cheney [66]. We ﬁrst derive another characterization of a best approximation.
Given a polynomial ˜p of degree n, consider the set of points
M = {x ∈[a, b] | |f (x) −˜p(x)| = ∥f −˜p∥∞},
where the difference ˜d = f −˜p takes on the extreme values ±∥f −˜p∥∞. If ˜p is not a best
approximation, then there is a best approximation that can be written p∗= ˜p + p, where
p ̸= 0 is a polynomial of degree at most n. Then for all x ∈M, we have
|f (x) −( ˜p(x) + p(x))| < |f (x) −˜p(x)|.

474
Chapter 4. Interpolation and Approximation
This can only happen if the sign of p(x) and the sign of (f (x) −˜p(x)) are the same, that is,
(f (x) −˜p(x))p(x) > 0,
x ∈M.
Reversing this implication, it follows that ˜p is a best approximation whenever there is no
polynomial satisfying this condition.
Suppose now that ˜p satisﬁes the conditions in the theorem. Then we claim that the
conditions (f (xk) −˜p(xk))p(xk) > 0, k = 0 : n + 2, cannot hold for any polynomial
p ̸= 0 of degree n. For if they did hold, then p would have at least n + 1 sign changes in
[a, b] and hence also n + 1 zeros there. But by the fundamental theorem of algebra this is
impossible.
Notice that Theorem 4.5.23 states that the best solution in maximum norm has at least
n + 2 points where the error alternates. In general, there can be more points where the
maximal deviation is achieved.
Suppose that the function f ∈C[a, b] is known at m > n + 1 distinct points X =
{ξi}m+1
i=0 in [a, b], and let ˜p be a polynomial of degree n. Then ˜p is a best approximation of
f in the discrete maximum norm if there are n+2 points xk ∈X such that (f −˜p) assumes
its maximal absolute value d = ∥˜f −p∥∞at these points with alternating sign. The proof
of this is identical to the proof above.
Example 4.5.13.
In certain simple cases the alternating property can be used to construct the best
uniform approximation. Suppose we want to ﬁnd the best linear approximation to a convex
function f on [a, b]. In this case the maximum error occurs with alternating signs at the
three points a = x0, x1, x2 = b, where x1 is chosen so that
f ′(x1) = [a, b]f = (f (b) −f (a))/(b −a)
gives the solution. The linear polynomial equals the mean value of the tangent of f at x1
and the linear interpolant through the endpoints of [a, b], i.e.,
˜p(x) = (f (a) + f (b))/2 + (x −(a + x1)/2)[a, b]f,
is illustrated in Figure 4.5.3. Notice that linear interpolation (dotted line) using f (a) and
f (b) gives a maximum error which is exactly twice as large.
The only property that was used in the proof of Chebyshev’s equioscillation theorem
was the fundamental theorem of algebra. This property also holds for other subspaces of
C[a, b], which we now deﬁne.
Deﬁnition 4.5.24.
LetU bean(n+1)-dimensionalvectorspaceofcontinuous(realorcomplex)functions
ui, i = 0 : n, deﬁned on a domain D containing at least n + 1 distinct points. The set
of functions {u0, . . . , un} are said to satisfy the Haar condition156 (or form a Chebyshev
system) if one of the three following equivalent conditions is satisﬁed:
156This concept is named after the Austrian-Hungarian mathematician Alfred Haar (1885–1933) who, together
with Friedrich Riesz established a mathematical center in Szeged, Hungary. Many important contributions to
modern functional analysis originated in this center.

4.5. Approximation and Function Spaces
475
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
0
0.5
1
1.5
2
2.5
x0=a
x1
x2 = b
η
η
Figure 4.5.3. Linear uniform approximation.
1. For any n + 1 distinct points xi, i = 0 : n, it holds that
det

x0, x1, . . . , xn
u0, u1, . . . , un

:= det


u0(x0)
u1(x0)
· · ·
un(x0)
u0(x1)
u1(x1)
· · ·
un(x1)
...
...
...
...
u0(xn)
u1(xn)
· · ·
un(xn)

̸= 0.
2. The interpolation problem to determine u ∈U such that u(xi) = fi, i = 0 : n, is
uniquely solvable.
3. Each u ∈U, not identically zero, has at most n distinct zeros on D.
We remark that continuous functions of more than one variable cannot form a Cheby-
shev system. A large part of the theory of polynomial interpolation can be generalized to
Chebyshev systems. Theorem 4.5.23 holds for any Chebyshev system, since only the Haar
condition is needed in the proof.
Example 4.5.14.
The classical example of a Chebyshev system is the set of monomials uk(x) = xk,
k = 0 : n. The Vandermonde determinant
det(V ) =
&
0≤i,j≤n
(xi −xj)
is nonzero if the points xi are pairwise distinct. Some other important examples are as
follows:
1. If {u0, u1, . . . , un} is a Chebyshev system on [a, b] and w(x) ̸= 0, then
w(x)u0, w(x)u1, . . . , w(x)un is a Chebyshev system on [a, b].

476
Chapter 4. Interpolation and Approximation
2. If α0 < α1 < · · · αn is a strictly increasing sequence of real numbers, then the power
functions uk(x) = xαk, k = 0 : n, form a Chebyshev system on any closed subinterval
of [0, ∞].
3. 1, ex, e2x, . . . , enx, x ∈[a, b].
4. 1, sin x, sin 2x, . . . , sin mx, cos x, cos x, . . . , cos mx, x ∈[0, 2π].
5. Let s0, s1, . . . , sn be distinct values not in [a, b]. Then the functions
ui(x) =
1
x −si
,
i = 0 : n,
form a Chebyshev system on [a, b].
The last example follows from Cauchy’s formula for the determinant
det

x0, x1, . . . , xn
u0, u1, . . . , un

=
n
&
i>j
(xi −xj)(si −sj)
5
n
&
i,j=0
(xi + sj);
see Davis [92, Lemma 11.3.1].
As shown by Haar, the Haar condition is necessary and sufﬁcient for the best approx-
imation problem in maximum norm to have a unique solution. (Note that because the ∥·∥∞
is not a strict norm for C[a, b], this does not directly follow from general principles.)
Theorem 4.5.25.
Let U = span (u0, u1, . . . , un) be a Haar subspace of C[a, b]. Then for any f ∈
C[a, b] there is a unique best uniform approximation u ∈U.
Even when the alternating property is only approximately satisﬁed, the following the-
orem by de la Vallée-Poussin still gives a practical estimate for how good an approximation
it is.
Theorem 4.5.26.
Let f (x) and uk(x), k = 0 : n, be real-valued continuous functions deﬁned on the
interval I = [a, b] which satisfy the Haar condition on I. Assume that an approximation
˜u(x) ∈U has the alternating property
(f (xj) −˜u(xj))(−1)jσ = µj > 0
(4.5.74)
at (n + 2) distinct points x0, . . . , xn+1 ∈I, where σ = ±1. Let η(f, D) = inf u∈U ∥f −
u∥∞,D denote the error of the best approximation on the domain D. Then it holds that
µ :=
min
0≤j≤n+1 µj ≤η(f, X) ≤η(f, I) ≤∥f −˜u∥∞,I,
(4.5.75)
min
0≤j≤n+1 µj ≤η(f, X) ≤
max
0≤j≤n+1 µj = ∥f −˜u∥∞,X.
(4.5.76)

4.5. Approximation and Function Spaces
477
Proof. Since X ⊂I we have η(f, X) ≤η(f, I). It then sufﬁces to show that for any
u ∈U, the inequality µ ≤∥f −u∥∞,X holds. This follows from
µ ≤µj = (f (xj) −˜u(xj))(−1)jσ
= (f (xj) −u(xj))(−1)jσ + (u(xj) −˜u(xj))(−1)jσ
≤(f (xj) −u(xj))(−1)jσ ≤∥f −u∥∞,X
for those xj ∈X for which (u(xj) −˜u(xj))(−1)jσ ≤0. Such an xj must exist since
otherwise u −˜u has at least n + 1 zeros and does not vanish.
It follows that an approximation ˜u with equality in (4.5.76) is the best approximation
on X. If X is chosen so that equality also holds in (4.5.75), then ˜u is the best approximation
on the whole interval I = [a, b].
The alternating property of the best approximation plays a key role in the solution
of the best approximation problem. The idea is to solve a sequence of discrete uniform
approximation problems each using (n + 2) reference points:
X = {x0, . . . , xn+1},
a ≤x0 ≤x1 · · · ≤xn+1 ≤b.
We therefore now consider how we can determine ˜u = u∗so that equality in (4.5.76) is
attained. For a given set of points X = x0, . . . , xn+1 assume that u∗has the alternating
property
(f (xj) −u∗(xj))(−1)n+1−jη,
j = 0 : n + 1,
(4.5.77)
where η is to be determined. We extend the function u0, . . . , un with the function un+1(xj) =
(−1)j deﬁned on X. Then we obtain a linear system ˜Un+1c = f , or


u0(x0)
· · ·
un(x0)
1
...
...
...
...
u0(xn)
· · ·
un(xn)
(−1)n
u0(xn+1)
· · ·
un(xn+1)
(−1)n+1




c0
...
cn
η

=


f0
...
fn
fn+1

.
(4.5.78)
The ﬁrst leading submatrix of order n + 1 of ˜Un+1 is nonsingular since u0, . . . , un
form a Chebyshev system. Hence its ﬁrst n + 1 columns are linearly independent. Further,
there cannot exist a linear combination of the ﬁrst n + 1 columns that is parallel to the last
column since this would imply that some function u ∈U has n+1 zeros in [a, b]. It follows
that the system (4.5.78) is nonsingular and has a unique solution. Note that once d has been
determined, then the remaining coefﬁcients ci, i = 0 : n, can be computed by solving an
interpolating problem.
In the special case of polynomial approximation the system (4.5.78) can be solved in
O(n2) ﬂops. It can be shown that η can be determined as the quotient between two divided
differences
η = [x0, x1, . . . , xn+1]f/[x0, x1, . . . , xn+1]s,
(4.5.79)

478
Chapter 4. Interpolation and Approximation
where s(x) is a function such that s(xi) = (−1)i. The Newton form of the solution p(x) is
then given by
p(x) =
n

j=0
[x0, . . . , xj](f −ηs)
j−1
&
k=0
(x −xk).
(4.5.80)
The Remez exchange algorithm, proposed by Remez [300] in 1934, is an iterative
method for computing the best uniform approximation on an interval [a, b]. (Sometimes
the name is transcribed as Remes.) In this method one proceeds as follows:
1. Choose an initial reference set X consisting of (n + 2) points a ≤x0 < x1 < · · · <
xn+1 ≤b.
2. Compute the (unique) discrete approximation ˜uX to f (x) on the reference set.
3. Adjust the points in the reference set to be the extreme points of the error curve
f (x) −˜uX(x).
4. Repeat steps 2 and 3 until convergence.
Usually the initial reference set is chosen as Chebyshev points transformed to the
interval [a, b]. The Remez algorithm can also be applied to the discrete case where f is
only deﬁned on a grid ξ0, ξ1, . . . , ξm, m ≫n. In this case the Remez algorithm converges
in a ﬁnite number of steps.
In the continuous case, step 3, one can use Newton’s method on the equation f (x) −
˜uX(x) = 0, with starting values equal to xi for all interior points. If derivatives are not
available, then a combination of inverse parabolic interpolation and golden section search
given by Brent [44, Chap. 5] is a good alternative; see Sec. 6.4.3.
A discussion of the exchange algorithm and its convergence can be found in Meinar-
dus [264]. A careful implementation of the Remez algorithm, where the delicate points to
consider in an implementation are discussed, is given by Golub and Smith [168]. Assuming
that f (x) is sufﬁciently smooth quadratic, convergence is achieved.
Review Questions
4.5.1 State the axioms that any norm must satisfy. Deﬁne the maximum norm and the
Euclidean norm for a continuous function f on a closed interval.
4.5.2 Deﬁne dist(f, Pn), and state Weierstrass’ approximation theorem.
4.5.3 Prove the Pythagorean theorem in an inner product space.
4.5.4 Deﬁne and give examples of orthogonal systems of functions.
4.5.5 Formulate and prove Bessel’s inequality and Parseval’s identity, and interpret them
geometrically.
4.5.6 (a) Give some reasons for using orthogonal polynomials in polynomial approxima-
tion with the method of least squares.
(b) Give some argument against the assertion that orthogonal polynomials are difﬁ-
cult to work with.

Problems and Computer Exercises
479
4.5.7 In Chebyshev interpolation we seek the coefﬁcients cj such that
p(xk) =
m

j=0
cjTj(xk),
k = 0 : m,
where xk are the zeros of Tm+1(x). How would you compute cj?
4.5.8 The Gram polynomials are examples of orthogonal polynomials. With respect to
what inner product are they orthogonal?
4.5.9 Let φk(x), k = 1, 2, 3, . . . , be a triangle family of orthogonal polynomials in an inner
product space. What property can be used to generate this sequence of polynomials?
4.5.10 What is the characteristic property of the nth degree polynomial which best approx-
imates a given continuous function in the maximum norm over a closed interval?
4.5.11 Give at least three examples of a Chebyshev system of functions.
Problems and Computer Exercises
4.5.1 Compute ∥f ∥∞and ∥f ∥2 for the function f (x) = (1 + x)−1 on the interval [0, 1].
4.5.2 Determine straight lines which approximate the curve y = ex such that
(a) the discrete Euclidean norm of the error function on the grid (−1, −0.5, 0, 0.5, 1)
is as small as possible;
(b) the Euclidean norm of the error function on the interval [−1, 1] is as small as
possible;
(c) the line is tangent to y = ex at the point (0, 1), i.e., the Taylor approximation at
the midpoint of the interval.
4.5.3 Determine, for f (x) = π2 −x2, the “cosine polynomial” f ∗= n
j=0 cj cos jx
which makes ∥f ∗−f ∥2 on the interval [0, π] as small as possible.
4.5.4 (a) Show that on any interval containing the points −1, −1/3, 1/3, 1,
E2(f ) ≥1
8
((((f (1) −3f
1
3

+ 3f

−1
3

−f (−1)
(((( .
(b) Compute the above bound and the actual value of E2(f ) for f (x) = x3.
4.5.5 (a) Let a scalar product be deﬁned by (f, g) =
 b
a f (x)g(x) dx. Calculate the matrix
of normal equations, when φj(x) = xj, j = 0 : n, when a = 0, b = 1.
(b) Do the same when a = −1, b = 1. Show how in this case the normal equations
can be easily decomposed into two systems, with approximately (n+1)/2 equations
in each.
4.5.6 Verify the formulas for ∥φj∥2 given in Example 4.5.10.
4.5.7 (a) Show that ∥f −g∥≥∥f ∥−∥g∥for all norms. (Use the axioms mentioned in
Sec. 4.5.1.)
(b) Show that if {cj}n
0 is a set of real numbers and if {fj}n
0 is a set of vectors, then
∥ cjfj∥≤ |cj|∥fj∥.

480
Chapter 4. Interpolation and Approximation
4.5.8 Let G ∈Rn×n be a symmetric positive deﬁnite matrix. Show that an inner product
is deﬁned by the formula (u, v) = uT Gv. Show that A∗= G−1AT G.
4.5.9 In a space of complex-valued twice differentiable functions of t which vanish at
t = 0 and t = 1, let the inner product be
(u, v) =
 1
0
u(t)¯v(t) dt.
What is the adjoint of the operator A = d/dt? Is it true that the operator iA is
self-adjoint, and that −A2 is self-adjoint and positive deﬁnite?
4.5.10 (a) Show that, in a real inner-product space,
4(u, v) = ∥u + v∥2 −∥u −v∥2.
In a complex space this gives only the real part of the inner product. Show that one
has to add ∥u −iv∥2 −∥u + iv∥2.
(b)This can be used to reduce many questions concerning inner products to questions
concerningnorms. Forexample, inageneralinner-productspaceaunitaryoperator
is deﬁned by the requirement that ∥Au∥= ∥u∥for all u. Show that (Au, Av) =
(u, v) for all u, v.
Note, however, that the relation (u, Au) = (Au, u) for all u which, in a real space,
holds for every operator A, does not imply that (u, Av) = (Au, v) for all u, v. The
latter holds only if A is self-adjoint.
4.5.11 Show that (AB)∗= B∗A∗. Also show that if C is self-adjoint and positive deﬁnite,
then A∗CA is so too. (A is not assumed to be self-adjoint.)
4.5.12 Show that
(A−1)∗= (A∗)−1,
(Ap)∗= (A∗)p,
for all integers p, provided that the operators mentioned exist. Is it true that Cp is
self-adjoint and positive deﬁnite if C is so?
4.5.13 Show the following minimum property of orthogonal polynomials: Among all
nth-degree polynomials pn with leading coefﬁcient 1, the smallest value of
∥pn∥2 =
 b
a
p2
n(x)w(x)dx,
w(x) ≥0,
is obtained for pn = φn/An, where φn is the orthogonal polynomial with leading
coefﬁcient An associated with the weight distribution w(x).
Hint: Determine the best approximation to xn in the above norm or consider the
expansion pn = φn/An + n−1
j=0 cjφj.
4.5.14 Verify the formulas for ∥Tj∥2 given in Theorem 4.5.20.
4.5.15 Modify Clenshaw’s algorithm to a formula for the derivative of an orthogonal ex-
pansion.

Problems and Computer Exercises
481
4.5.16 (a) Let αj, j = 1 : n, be the zeros of the Chebyshev polynomial Tn(x), n ≥1.
(There are, of course, simple trigonometric expressions for them.) Apply Clenshaw’s
algorithm to compute
n−1

m=0
Tm(α1)Tm(x), x = αj, j = 1 : n.
It turns out that the results are remarkably simple.
(b) Show that S = n−1
k=0 ckφk can be computed by a forward version of Clenshaw’s
algorithm that reads
y−2 = 0;
y−1 = 0;
for k = 0 : n −1,
yk = (−yk−2 + αkyk−1 + ck)/γk+1;
end
S = cnφn + γnyn−1φn−1 −yn−2φn.
Add this version as an option to your program, and study [294, Sec. 5.4], from which
this formula is quoted (with adaptation to our notation). Make some test example
of your own choice.
4.5.17 (a) Write a MATLAB function c = stieltjes(f,x,w,m,n) that computes the
orthogonal coefﬁcients in a least squares polynomial ﬁt to the data (fi, xi) and
weights wi, i = 0 : m. Compute the orthogonal polynomials φk using the Stieltjes
procedure. For computing ck, k = 0 : n, use either (4.5.67) or (4.5.68).
(b) Apply the function in (a) to the case fi = x7
i , wi = 1/(fi)2, and m = 20.
Compute and print the error ∥pk −f ∥for k = 0 : 10, using the expression (4.5.67)
for ck+1. Note that for k > 7 the ﬁt should be exact.
(c) Repeat the calculations in (b) using the modiﬁed formula (4.5.68). Compare the
error with the results in (b).
4.5.18 (a) What ﬁrst-degree polynomial does one get with Chebyshev interpolation to
f (x) = 1/(3 + x), [−1, 1]? What is the maximum norm of the error?
Answer: p(x) = (−2x + 6)/17; 0.0294 attained at x = −1.
(b) Determine the ﬁrst-degree polynomial which best approximates the function
f (x) = 1/(3 + x), [−1, 1], in the maximum norm. Determine E1(f ) to at least
three decimals.
Hint: The error has maximum magnitude at the two endpoints of the interval and at
one inner point.
Answer: p(x) = (−x + 2
√
2)/8; E1(f ) = 0.0214.
4.5.19 (a) Show that the three conditions in Deﬁnition 4.5.24 are equivalent.
(b) Show the formula for eta given in (4.5.79).
4.5.20 Determine, using the Remez algorithm, the second-degree polynomial p(x) = c0 +
c1x + c2x24 that best approximates f (x) = sin
 π
2 x

on I = [0, 1].
Hint: Since the second derivative f ′′ has constant sign in I, we can take x0 = a,
x3 = b, and only the two interior points x1 and x2 are unknown.

482
Chapter 4. Interpolation and Approximation
4.6
Fourier Methods
Many natural phenomena, for example, acoustical and optical, are of a periodic character.
For instance, it was known already by Pythagoras (600 B.C.) that a musical sound is com-
posed of regular oscillations, partly a fundamental tone with a certain frequency f , and
partly overtones with frequencies 2f , 3f , 4f, . . . . The ratio of the strength of the funda-
mental tone to that of the overtones is decisive for our impression of the sound. Sounds
which are free from overtones occur, for instance, in electronic music, where they are called
pure sine tones.
In an electronic oscillator, a current is generated whose strength at time t varies
according to the formula r sin(ωt + v), where r is called the amplitude of the oscillation; ω
is called the angular frequency and is equal to 2π times the frequency; and v is a constant
which deﬁnes the state at the time t = 0. In a loudspeaker, variations of current are
converted into variations in air pressure which, under ideal conditions, are described by the
same function. In practice, however, there is always a certain distortion; overtones occur.
The variations in air pressure which reach the ear can, from this viewpoint, be described as
a sum of the form
∞

k=0
rk sin(kωt + vk).
(4.6.1)
An expansion of this form is called a Fourier series.
The separation of a periodic phenomenon into a fundamental tone and overtones
permeates not only acoustics, but also many other areas. It is related to an important, purely
mathematical theorem ﬁrst given by Fourier.157 According to this theorem, every function
f (t) with period 2π/ω can, under certain very general conditions, be expanded in a Fourier
series of the form (4.6.1). (A function has period p if f (t + p) = f (t) for all t.) A more
precise formulation will be given later in Theorem 4.6.2.
Fourier series are valuable aids in the study of phenomena which are periodic in time
(such as vibrations, sound, light, alternating currents) or in space (waves, crystal structure,
etc.). One very important area of application is in digital signal and image processing
which is used in interpreting radar and sonar signals. Another is statistical time series,
which are used in communications theory, control theory, and the study of turbulence. For
the numerical analyst, Fourier analysis is partly a very common computational task and
partly an important aid in the analysis of properties of numerical methods.
Modiﬁcations of pure Fourier methods are used as a means of analyzing nonperiodic
phenomena; see, e.g., Sec. 4.6.3 (periodic continuation of functions) and Sec. 4.6.5 (Fourier
transforms). The approximation of Fourier expansions using sampled data and discrete
Fourier analysis is treated in Sec. 4.6.2. The fast Fourier transform (FFT) for discrete
Fourier analysis and synthesis is treated Sec. 4.7.1. It has caused a complete change of
attitude toward what can be done using discrete Fourier methods.
157Jean Baptist Joseph Fourier (1768–1830), French mathematician. In 1807 Fourier completed and read to the
Paris Institute his important memoir Théorie Analytique de la Chaleur [Analytical Theory of Heat], in which he
used what is now called Fourier series. It won a prize competition set by the Institute in 1811, but was not published
until 1822.

4.6. Fourier Methods
483
4.6.1
Basic Formulas and Theorems
The basic formulas and theorems derived in this section rely to a great extent on the theory
in Sec. 4.5. An expansion of the form of (4.6.1) can be expressed in many equivalent ways.
If we set ak = rk sin vk, bk = rk cos vk, then using the addition theorem for the sine function
we can write
f (t) =
∞

k=0
(ak cos kωt + bk sin kωt),
(4.6.2)
where ak, bk are real constants. Another form, which is often the most convenient, can be
found with the help of Euler’s formulas,
cos x = 1
2(eix + e−ix),
sin x = 1
2i (eix −e−ix),
(i =
√
−1).
Here and in what follows i denotes the imaginary unit. Then one gets
f (t) =
∞

k=−∞
ckeikωt,
(4.6.3)
where
c0 = a0,
ck = 1
2(ak −i bk),
c−k = 1
2(ak + i bk),
k ≥1, . . . .
(4.6.4)
In the rest of this chapter we shall use the term Fourier series to denote an expansion of
the form of (4.6.1) or (4.6.3). We shall call the partial sums of the form of these series
trigonometric polynomials. Sometimes the term spectral analysis is used to describe the
above methods.
We shall study functions with period 2π. These are fully deﬁned by their values on
the fundamental interval [−π, π]. If a function of t has period L, then the substitution
x = 2πt/L transforms the function to a function of x with period 2π. We assume that the
function can have complex values, since the complex exponential function is convenient
for manipulations.
In the continuous case the inner product of two complex-valued functions f and g of
period 2π is deﬁned in the following way (the bar over g indicates complex conjugation):
(f, g) =
 π
−π
f (x) ¯g(x)dx.
(4.6.5)
(It makes no difference what interval one uses, as long as it has length 2π—the value
of the inner product is unchanged.) As usual the norm of the function f is deﬁned by
∥f ∥= (f, f )1/2. Notice that (g, f ) = (f, g).
Theorem 4.6.1.
The following orthogonality relations hold for the functions
φj(x) = eijx,
j = 0, ±1, ±2, . . . ,
where
(φj, φk) =
% 2π
if j = k,
0
if j ̸= k.
(4.6.6)

484
Chapter 4. Interpolation and Approximation
Proof. In the continuous case, if j ̸= k, it holds that
(φj, φk) =
 π
−π
eijxe−ikxdx =
(((
π
−π
ei(j−k)x
i(j −k) = (−1)j−k −(−1)j−k
i(j −k)
= 0,
whereby orthogonality is proved. For j = k
(φk, φk) =
 π
−π
eikxe−ikxdx =
 π
−π
1 dx = 2π.
If one knows that the function f (x) has an expansion of the form
f =
∞

j=−∞
cjφj,
then from Theorem 4.6.1 it follows formally that
(f, φk) =
b

j=a
cj(φj, φk) = ck(φk, φk),
a ≤k ≤b,
since (φj, φk) = 0 for j ̸= k. Thus, changing k to j, we have
cj = (f, φj)
(φj, φj) = 1
2π
 π
−π
f (x)e−ijxdx.
(4.6.7)
These coefﬁcients are called Fourier coefﬁcients; see the more general case in Theo-
rem 4.5.13. In accordance with (4.6.4) set
aj = cj + c−j,
bj = i(cj −c−j).
Then
N

j=−N
cjeijx = c0 +
N

j=1

cj(cos jx + i sin jx) + c−j(cos jx −i sin jx)

= 1
2a0 +
N

j=1
(aj cos jx + bj sin jx),
where
aj = 1
π
 π
−π
f (x) cos jx dx,
j ≥0,
bj = 1
π
 π
−π
f (x) sin jx dx,
j ≥1.
(4.6.8)
(Notice that the factors preceding the integral are different in the expressions for cj and for
aj, bj, respectively.)
From a generalization of Theorem 4.5.13, we also know that the error
f −
n

j=−n
kjφj
,
n < ∞,

4.6. Fourier Methods
485
becomes as small as possible if we choose kj = cj, −n ≤j ≤n. Theorem 4.5.14 and its
corollary, Parseval’s identity,
2π
∞

j=−∞
|cj|2 = ∥f ∥2 =
 π
−π
|f (x)|2 dx,
(4.6.9)
are of great importance in many applications of Fourier analysis. The integral in (4.6.9) can
be interpreted as the “energy” of the function f (x).
Theorem 4.6.2 (Fourier Analysis, Continuous Case).
Assume that the function f is deﬁned at every point in the interval [−π, π] and that
f (x) is ﬁnite and piecewise continuous. Associate with f a Fourier series in the following
two ways:
1
2a0 +
∞

j=1
(aj cos jx + bj sin jx)
and
∞

j=−∞
cjeijx,
where the coefﬁcients aj, bj, and cj are deﬁned by (4.6.8) in the ﬁrst case and (4.6.7)
in the second case. Then the partial sums of the above expansions give the best possible
approximations to f (x) by trigonometric polynomials, in the least squares sense.
If f is of bounded variation and has at most a ﬁnite number of discontinuities, then
the series is everywhere convergent to f (x). At a point x = a of discontinuity f (a) equals
the mean f (a) = 1
2(f (a+) + f (a−)).
Proof. The proof of the convergence results is outside the scope of this book (see, e.g.,
Courant and Hilbert [83]). The rest of the assertions follow from previously made cal-
culations in Theorem 4.6.1 and the comments following; see also the proof of Theorem
4.5.13.
The more regular a function is, the faster its Fourier series converges. The following
useful result is relatively easy to prove using (4.6.7) and integrating by parts k + 1 times
(cf. (3.2.8)).
Theorem 4.6.3.
If f and its derivatives up to and including order k are periodic and everywhere
continuous, and if f (k+1) is piecewise continuous, then
|cj| ≤
1
j (k+1) ∥f (k+1)∥∞.
(4.6.10)
Sometimes it is convenient to separate a function f deﬁned on [−π, π] into an even
and an odd part. We set f (x) = g(x) + h(x), where
g(x) = 1
2(f (x) + f (−x)),
h(x) = 1
2(f (x) −f (−x))
∀x.
(4.6.11)
Then g(x) = g(−x) and h(x) = −h(−x). For both g(x) and h(x) it sufﬁces to give the
function only on [0, π]. For the even function g(x) the sine part of the Fourier series drops

486
Chapter 4. Interpolation and Approximation
and we have
g(x) = 1
2a0 +
∞

j=1
aj cos jx,
aj = 2
π
 π
0
g(x) cos jx dx.
(4.6.12)
For h(x) the cosine part drops out and the Fourier series becomes a sine series:
h(x) =
∞

j=1
bj sin jx,
bj = 2
π
 π
0
h(x) sin jx dx.
(4.6.13)
The proof is left as an exercise to the reader (use the formulas for the coefﬁcients
given in (4.6.8)).
Example 4.6.1.
Consider the rectangular wave function obtained by periodic continuation outside the
interval (−π, π) of
f (x) =
% −1/2,
−π < x < 0,
1/2,
0 < x < π;
see Figure 4.6.1. The function is odd, so aj = 0 for all j, and
bj = 1
π
 π
0
sin jx dx = 1
jπ (1 −cos jπ) =
%
0
if j even,
2/(jπ)
if j odd.
Hence
f (x) = 2
π

sin x + sin 3x
3
+ sin 5x
5
+ · · ·

.
(4.6.14)
Notice that the coefﬁcients cj decay as j −1 in agreement with Theorem 4.6.3.
−π
0
π
Figure 4.6.1. A rectangular wave.
The sum of the series is zero at the points where f has a jump discontinuity; this
agrees with the fact that the sum should equal the average of the limiting values to the left
and to the right of the discontinuity.
Figure 4.6.2 shows the approximations to the square wave using one, two, ﬁve, and
ten terms of the series (4.6.14). As can be seen, there is a ringing effect near the discon-
tinuities. The width and energy of this error are reduced when the number of terms in the
approximation is increased. However, the height of the overshoot and undershoot near the

4.6. Fourier Methods
487
−4
−3
−2
−1
0
1
2
3
4
−1.5
−1
−0.5
0
0.5
1
Figure 4.6.2. Illustration of Gibbs’phenomenon.
discontinuity converges to a ﬁxed height, which is equal to about 0.179 times the jump in
the function value. This artifact is known as Gibbs’ phenomenon.158
The Gibbs’ oscillations can be smoothed by multiplying the terms by factors which
depend on m, the order of the partial sum. Let us consider the ﬁnite expansion
fm(x) =
m−1

k=−(m−1)
ckeikx.
Then in the smoothed expansion each term in the sum is multiplied by the Lanczos σ-factors,
fm(x) =
m−1

k=−(m−1)
σkckeikx,
σk = sin πk/m
π/m
(4.6.15)
(see Lanczos [235, Chap. IV, Secs. 6 and 9]). Since the coefﬁcients in the real form of the
Fourier series are
ak = ck + c−k,
bk = i(ck −c−k),
the same σ-factor applies to them.
The Gibbs’ oscillations can also be suppressed by using the epsilon algorithm. For
this purpose one adds the conjugate Fourier series, applies the epsilon algorithm, and keeps
only the real part of the result.
4.6.2
Discrete Fourier Analysis
Although the data to be treated in Fourier analysis are often continuous in the time or space
domain, for computational purposes these data must usually be represented in terms of a
158Named after the American physicist J. William Gibbs, who in 1899 proved that this ringing effect will always
occur when approximating a discontinuous function with a Fourier series.

488
Chapter 4. Interpolation and Approximation
ﬁnite discrete sequence. For example, a function f (t) of time is recorded at evenly spaced
intervals 4t in time. Assume that the function f is known at equidistant arguments in the
interval [0, 2π],
xk = 2πk/N,
k = 0 : N −1.
Such data can be analyzed by discrete Fourier analysis. Deﬁne the inner product
(f, g) =
N−1

k=0
f (xk) ¯g(xk),
xk = 2πk/N.
(4.6.16)
Then, with φj(x) = eijx we have
(φj, φk) =
N−1

k=0
eijxke−ikxk =
N−1

k=0
ei(j−k)hk.
From Lemma 3.2.2 it now follows that
(φj, φk) =
! N
if (j −k)/N is an integer,
0
otherwise.
(4.6.17)
Theorem 4.6.4 (Trigonometric Interpolation).
Every function, deﬁned on the equidistant grid xk = 2πk/N, k = 0 : N −1, can be
interpolated by the trigonometric polynomial
f (x) =



k+θ

j=−k
cjeijx,
1
2a0 +
k

j=1
(aj cos jx + bj sin jx) + 1
2θak+1 cos(k + 1)x.
(4.6.18)
Here
θ =
% 1
if N even,
0
if N odd,
k =
% N/2 −1
if N even,
(N −1)/2
if N odd,
(4.6.19)
and
cj = 1
N
N−1

k=0
f (xk)e−ijxk,
(4.6.20)
aj = 2
N
N−1

k=0
f (xk) cos jxk,
bj = 2
N
N−1

k=0
f (xk) sin jxk.
(4.6.21)
If the sums in (4.6.18) are terminated when |j| < k + θ, then one obtains the trigono-
metric polynomial which is the best least squares approximation, among all trigonometric
polynomials with the same number of terms, to f on the grid.
Proof. The expression for cj is justiﬁed by (4.6.17). Further, by (4.6.20)–(4.6.21) it follows
that
aj = cj + c−j,
bj = i(cj −c−j),
ck+1 = 1
2ak+1.

4.6. Fourier Methods
489
The two expressions for f (x) are equivalent, because
k+θ

j=−k
cjeijx = c0 +
k

j=1

cj(cos jx + i sin jx) + c−j(cos jx −i sin jx)

+ θck+1 cos(k + 1)x
= c0 +
k

j=1
(aj cos jx + bj sin jx) + 1
2θak+1 cos(k + 1)x.
The function f (x) coincides on the grid x0, x1, . . . , xN−1 with the function
f ∗(x) =
N−1

j=0
cjeijx
(4.6.22)
because
e−i(N−j)xk = eijxk,
c−j = cN−j.
However, the functions f and f ∗are not identical between the grid points. If we set ω = eix
and ωk = eixk, then
f ∗(x) = P(ω) =
N−1

j=0
cjωj,
where P (ω) is a polynomial of degree less than N. It becomes clear that trigonometric
interpolation is equivalent to polynomial interpolation at the grid points ωk. The mapping
CN →CN
(f0, f1, . . . , fN−1) 9→(c0, c1, . . . , cN−1)
is called the discrete Fourier transform (DFT).
The calculations required to compute the coefﬁcients cj according to (4.6.20), Fourier
analysis, are of essentially the same type as the calculations needed to compute f ∗(x) at
the grid points
xk = 2πk/N,
k = 0 : N −1,
when the expansion in (4.6.22) is known, so-called Fourier synthesis. Both calculations
can be performed very efﬁciently using FFT algorithms; see Sec. 4.7.1.
Functions of several variables are treated analogously. Quite simply, one takes one
variable at a time. In the discrete case with two variables we set
xk = 2πk/N,
yℓ= 2πℓ/N,
and assume that f (xk, yℓ) is known for k = 0 : N −1, ℓ= 0 : N −1. Set
cj(yℓ) = 1
N
N−1

k=0
f (xk, yℓ)e−ijxk,
cj,k = 1
N
N−1

ℓ=0
cj(yℓ)e−ikyℓ.

490
Chapter 4. Interpolation and Approximation
From Theorem 4.6.4, then (with obvious changes in notations)
cj(yℓ) =
N−1

k=0
cj,keikyℓ,
f (xk, yℓ) =
N−1

j=0
cj(yℓ)eijxk =
N−1

j=0
N−1

k=0
cj,ke(ijxk+ikyℓ).
The above expansion is of considerable importance in, e.g., crystallography.
The Fourier coefﬁcients
cj(f ) = 1
2π
 π
−π
f (x)eijx dx,
j = 0, ±1, ±2, . . . ,
(4.6.23)
of a function f with period 2π are often difﬁcult to compute. On the other hand the
coefﬁcients of the DFT
ˆcj(f ) = 1
N
N−1

k=0
f (xk)e−ijxk,
xk = 2πk
N ,
j = 0 : N −1,
(4.6.24)
can be computed very efﬁciently by the FFT. Now, since f (x0) = f (xN), the sum in
(4.6.24) can be thought of as a trapezoidal sum
ˆcj(f ) = 1
N
01
2f (x0) + f (x1)e−ijx1 + · · · + f (xN−1)e−ijxN−1 + 1
2f (xN)
1
approximating the integral (4.6.23). Therefore, one might think of using ˆcj as an approxima-
tion to cj for all j = 0, ±1, ±2, . . . . But ˆcj(f ) are periodic in j with period N, whereas by
Theorem 4.6.3 the true Fourier coefﬁcients cj(f ) decay as some power j −(k+1) as j →∞.
We now show a way to remove this deﬁciency. Let fk, k = 0, ±1, ±2, . . . , be an
inﬁnite N-periodic sequence with fk = f (xk). Let ϕ = Pf be a continuous function such
that ϕ(xk) = fk, k = 0, ±1, ±2, . . . , and approximate cj(f ) by cj(ϕ). Then cj(ϕ) will
decay to zero as j →∞. It is a remarkable fact that if the approximation scheme P is
linear and translation invariant, we have
cj(ϕ) = τj ˆcj(f ),
(4.6.25)
where the attenuation factors τj depend only on the approximation scheme P and not
the function f . This implies that τj can be determined from (4.6.25) by evaluating cj(ϕ)
and ˆcj(f ) for some suitable sequence fk, k = 0, ±1, ±2, . . . . This allows the FFT to be
used. For a proof of the above result and a more detailed exposition of attenuation factors
in Fourier analysis, we refer to Gautschi [141].
Example 4.6.2.
For a given N-periodic sequence fk, k = 0, ±1, ±2, . . . , take ϕ(x) = Pf to be
deﬁned as the piecewise linear function such that ϕ(xk) = fk, k = 0, ±1, ±2, . . . . Clearly
this approximation scheme is linear and translation invariant. Further, the function ϕ is
continuous and has period 2π.

4.6. Fourier Methods
491
Consider in particular the sequence fk = 1, k = 0 mod N, and fk = 0 otherwise. We
have
ˆcj(f ) = 1
N
N−1

k=0
f (xk)e−ijxk = 1
N .
Further, setting h = 2π/N and using symmetry, we get
cj(Pf ) = 1
2π
 h
−h

1 −|x|
h

eijx dx
= 1
π
 h
0

1 −x
h

cos jx dx =
2
j 2πh sin2 jh
2

.
This gives the attenuation factors
τj =
sin(jπ/N)
jπ/N
2
,
j = 0, ±1, ±2, . . . .
Note that the coefﬁcients cj(Pf ) decay as j −2, reﬂecting the fact that the ﬁrst derivative of
Pf is discontinuous. If instead we use a cubic spline approximation, the ﬁrst and second
derivatives are continuous and the attenuation factors decay as j −4.
4.6.3
Periodic Continuation of a Function
Assume now that the function f is from the beginning deﬁned only on the interval [0, π]. In
that case we can extend the function to [−π, 0] as either an even or an odd function. Hence
the same function can be expanded as either a sine series or a cosine series. Both expansions
will converge to the given f (x) in the interval [0, π] provided f satisﬁes the conditions in
Theorem 4.6.2. However, the rate of convergence of the two Fourier expansions may be
vastly different depending on continuity properties at the points x = 0 and x = π of the
original interval.
If the function f deﬁned in [0, π] satisﬁes f (0) = f (π) = 0, it can be continued as
an odd function (see Figure 4.6.3). Its Fourier expansion then becomes a sine series. In the
continuous case,
∞

j=1
bj sin jx,
bj = 2
π
 π
0
f (x) sin jx dx.
(4.6.26)
−π
0
π
2π
Figure 4.6.3. Periodic continuation of a function f outside [0, π] as an odd function.

492
Chapter 4. Interpolation and Approximation
In the discrete case (xk = πk/N),
N−1

j=1
bj sin jx,
bj = 2
N
N−1

k=1
f (xk) sin jxk.
(4.6.27)
The reﬂection as an odd function preserves the continuity in function values and the ﬁrst
derivative in the extended function. According to Theorem 4.6.3 the Fourier coefﬁcients bj
will therefore decrease as j −3.
If f (0) ̸= 0, one can still use such an expansion but the function will cause a discon-
tinuity at x = 0 and x = π. The discontinuity at x = 0 follows from the reﬂection; the
discontinuity at x = π follows from the periodicity condition f (x + 2π) = f (x) which
gives f (π) = f (−π). This will cause slow convergence (as j −1) of the Fourier coefﬁcients
and cause undesirable oscillations due to Gibbs’ phenomenon. However, by subtracting a
linear function a + bx from f we can always ensure that the condition f (0) = f (π) = 0
is satisﬁed.
If f (0) ̸= 0 and one makes a continuation of f into an even function on [−π, π], the
extended function will be continuous at x = 0 and x = π. The Fourier series then becomes
a pure cosine series In the continuous case,
1
2a0 +
∞

j=1
aj cos jx,
aj = 2
π
 π
0
f (x) cos jx dx.
(4.6.28)
In the discrete case (xk = πk/N),
1
2a0 +
N−1

j=1
aj cos jx,
aj = 2
N
N−1

k=0
f (xk) cos jxk.
(4.6.29)
The coefﬁcients aj will decrease as j −2. If f ′(0) = f ′(π) = 0, then the ﬁrst derivative will
also be continuous. One can still use such an expansion even if f ′(0) ̸= 0 or f ′(π) ̸= 0,
but then a discontinuity appears in the ﬁrst derivative.
4.6.4
Convergence Acceleration of Fourier Series
The generalized Euler transformation described in Sec. 3.4.3 can be used for accelerating the
convergence of Fourier series, except in the immediate vicinity of singular points. Consider
a complex power series
S(z) =
∞

n=1
unzn−1,
z = eiφ.
(4.6.30)
A Fourier series that is originally of the form ∞
n=−∞cneinφ, or in trigonometric form, can
easily be brought to this form; see Problem 4.6.7.
We consider the case
S(z) = −1
z log(1 −z) =
∞

n=1
1
nzn−1,
(4.6.31)

4.6. Fourier Methods
493
which is typical for a power series with completely monotonic terms. (The rates of conver-
gence are the same for almost all series of this class.) Numerical computation, essentially
by the above algorithm, gave the following results. The coefﬁcients uj are computed in
IEEE double precision arithmetic. We make the rounding errors during the computations
less important by subtracting the ﬁrst row of partial sums by its last element; it is, of course,
added again to the ﬁnal result.159 The ﬁrst table shows, for various φ, the most accurate
result that can be obtained without thinning. These limits are due to the rounding errors;
we can make the pure truncation error arbitrarily small by choosing N large enough.
φ
π
2π/3
π/2
π/3
π/4
π/8
π/12
π/180
|error|
2·10−16 8·10−16 10−14 6·10−12
10−9 5·10−7 3·10−5 2·10−1
N
30
33
36
36
36
40
40
100
kk
21
22
20
21
20
13
10
(3)
τ
80
120
90
15
τ ·φ
π
2π/3
π/2
π/12
|error|
2·10−14
10−14
3·10−13
3·10−5
N
28
31
33
41
kk
20
22
18
10
no. terms
5040
3720
2970
615
Note that a rather good accuracy is also obtained for φ = π/8 and φ = π/12, where
the algorithm is “unstable,” since |
z
1−z| > 1. In this kind of computation “instability” does
not mean that the algorithm is hopeless, but it shows the importance of a good termination
criterion. The question is to navigate safely between Scylla and Charybdis. For a small
value such as φ = π/180, the sum is approximately 4.1+1.5i. The smallest error with 100
terms (or less) is 0.02; it is obtained for k = 3. Also note that kk/N increases with φ.
By the application of thinning the results can often be improved considerably for
φ ≪π, in particular for φ = π/180. Let τ be a positive integer. The thinned form of S(z)
reads
S(z) =
∞

p=1
u∗
pzτ·(p−1),
u∗
p =
τ

j=1
uj+τ·(p−1) zj−1.
The series (4.6.31) has “essentially positive” terms originally that can become “essentially
alternating” by thinning. For example, if z = eiπ/3 and τ = 3, the series becomes an
alternating series, perhaps with complex coefﬁcients. It does not matter in the numerical
work that u∗
p depends on z.
We present the errors obtained for four values of the parameter τ, with different
amounts of work. Compare |error|, kk, etc. with appropriate values in the table above. We
see that, by thinning, it is possible to calculate the Fourier series very accurately for small
values of φ also.
159Tricks like this can often be applied in linear computations with a slowly varying sequence of numbers. See,
for example, the discussion of rounding errors in Richardson extrapolation in Sec. 3.4.6.

494
Chapter 4. Interpolation and Approximation
Roughly speaking, the optimal rate of convergence of the Euler transformation de-
pends on z in the same way for all power series with completely monotonic coefﬁcients,
independently of the rate of convergence of the original series. The above tables from a
particular example can therefore—with some safety margin—be used as a guide for the
application of the Euler transformation with thinning to any series of this class.
Say that you want the sum of a series  unzn for z = eiφ, φ = π/12, with relative
|error| < 10−10. You see in the ﬁrst table that |error| = 6·10−12 for φ = π/3 = 4π/12
without thinning. The safety margin is, we hope, large enough. Therefore, try τ = 4. We
make two tests with completely monotonic terms: un = n−1 and un = exp(−√n). We
hope that tol = 10−10 is large enough to make the irregular errors relatively negligible. In
both tests the actual magnitude of the error turns out to be 4 · 10−11, and the total number
of terms is 4·32 = 128. The values of errest are 6 ·10−11 and 7·10−11; both slightly
overestimate the actual errors and are still smaller than tol.
4.6.5
The Fourier Integral Theorem
We have seen how Fourier methods can be used on functions deﬁned on a ﬁnite interval
usually taken to be [−π, π]. Fourier found that expansion of an arbitrary function in a
Fourier series remains possible even if the function is deﬁned on an interval that extends
on both sides to inﬁnity. In this case the fundamental frequency converges to zero and the
summation process changes into one of integration.
Suppose that the function f (x) is deﬁned on the entire real axis, and that it satisﬁes
the regularity properties which we required in Theorem 4.6.2. Set
ϕ(ξ) = f (x),
ξ = 2πx/L ∈[−π, π],
and continue ϕ(ξ) outside [−π, π] so that it has period 2π. By Theorem 4.6.2, if
cj = 1
2π
 π
−π
ϕ(ξ)e−ijξ dξ = 1
L
 L/2
−L/2
f (x)e−2πixj/L dx,
(4.6.32)
then ϕ(ξ) = ∞
j=−∞cjeijξ, ξ ∈(−π, π), and hence
f (x) =
∞

j=−∞
cje2πixj/L,
x ∈

−L
2 , L
2

.
If we set
gL(ω) =
 L/2
−L/2
f (x)e−2πixω dx,
ω = j
L,
(4.6.33)
then by (4.6.32) we have cj = (1/L)gL(ω), and hence
f (x) = 1
L
∞

j=−∞
gL(ω)e2πixω,
x ∈
−L
2 , L
2

.
(4.6.34)
Now by passing to the limit L →∞, one avoids making an artiﬁcial periodic continuation
outside a ﬁnite interval. The sum in (4.6.34) is a “sum of rectangles” similar to the sum
which appears in the deﬁnition of a deﬁnite integral. But here the argument varies from

4.6. Fourier Methods
495
−∞to +∞, and the function gL(t) depends on L. By a somewhat dubious passage to the
limit, then, the pair of formulas (4.6.33) and (4.6.34) becomes the pair
g(ω) =
 ∞
−∞
f (x)e−2πixω dx ⇐⇒f (x) =
 ∞
−∞
g(ω)e2πixω dω.
(4.6.35)
One can, in fact, after a rather complicated analysis, show that the above result is correct; see,
e.g., Courant–Hilbert [83]. The proof requires, besides the previously mentioned “local”
regularity conditions on f , the “global” assumption that
 ∞
−∞
|f (x)| dx
is convergent. The beautiful, almost symmetric relation of (4.6.35) is called the Fourier
integral theorem. This theorem, and other versions of it, with varying assumptions under
which they are valid, is one of the most important aids in both pure and applied mathematics.
The function g is called the Fourier transform160 of f . The Fourier transform is one of the
most important tools of applied analysis. It plays a fundamental role in problems relating
to input–output relations, e.g., in electrical networks.
Clearly the Fourier transform is a linear operator. Another elementary property that
can easily be veriﬁed is
f (ax)
⇐⇒
1
|a|g
ω
a

.
Whether the function f (x) has even or odd symmetry and is real or purely imaginary
leads to relations between g(ω) and g(−ω) that can be used to increase computational
efﬁciency. Some of these properties are summarized in Table 4.6.1.
Table 4.6.1. Useful symmetry properties of the continuous Fourier transform.
Function
Fourier transform
f (x) real
g(−ω) = g(ω)
f (x) imaginary
g(−ω) = −g(ω)
f (x) even
g(−ω) = g(ω)
f (x) odd
g(−ω) = −g(ω)
f (x) real even
g(ω) real even
f (x) imaginary odd
g(ω) real odd
Example 4.6.3.
The function f (x) = e−|x| has Fourier transform
g(ω) =
 ∞
−∞
e−|x|e−2πixω dx =
 ∞
0

e−(1+2πiω)x + e−(1−2πiω)x
dx
=
1
1 + 2πiω +
1
1 −2πiω =
2
1 + 4π2ω2 .
160The terminology in the literature varies somewhat as to the placement of the factor 2π; it can be taken out of
the exponent by a simple change of variable.

496
Chapter 4. Interpolation and Approximation
Here f (x) is real and an even function. In agreement with Table 4.6.1, the Fourier transform
is also real and even.
From (4.6.35) it follows that
e−|x| =
 ∞
−∞
2
1 + 4π2ω2 e2πixω dω = 2
π
 ∞
0
1
1 + x2 cos πx dx,
(2πω = x).
It is not so easy to prove this formula directly.
Many applications of the Fourier transform involve the use of convolutions.
Deﬁnition 4.6.5.
The convolution of f1 and f2 is the function
h(ξ) = conv (f1, f2) =
 ∞
−∞
f1(x)f2(ξ −x) dx.
(4.6.36)
It is not difﬁcult to verify that conv (f1, f2) = conv (f2, f1). The following theorem
states that the convolution of f1 and f2 can be computed as the inverse Fourier transform
of the product g1(ω)g2(ω). This fact is of great importance in the application of Fourier
analysis to differential equations and probability theory.
Theorem 4.6.6.
Let f1 and f2 have Fourier transforms g1 and g2, respectively. Then the Fourier
transform g of the convolution of f1 and f2 is the product g(ω) = g1(ω)g2(ω).
Proof. By deﬁnition the Fourier transform of the convolution is
g(ω) =
 ∞
−∞
e−2πiξω
 ∞
−∞
f1(x)f2(ξ −x) dx

dξ
=
 ∞
−∞
 ∞
−∞
e−2πi(x+ξ−x)ωf1(x)f2(ξ −x) dx dξ
=
 ∞
−∞
e−2πixωf1(x) dx
 ∞
−∞
e−2πi(ξ−x)ωf2(ξ −x) dξ
=
 ∞
−∞
e−2πixωf1(ξ) dx
 ∞
−∞
e−2πixωf2(x) dx = g1(ω)g2(ω).
The legitimacy of changing the order of integration is here taken for granted.
In many physical applications, the following relation, analogous to Parseval’s identity
(corollary to Theorem 4.5.14), is of great importance. If g is the Fourier transform of f ,
then
 ∞
−∞
|g(ω)|2 dω =
 ∞
−∞
|f (ξ)|2 dξ.
(4.6.37)
In signal processing this can be interpreted to mean that the total power in a signal is the
same whether computed in the time domain or the frequency domain.

4.6. Fourier Methods
497
4.6.6
Sampled Data and Aliasing
The ideas of Sec. 4.3.5 can be applied to the derivation of the celebrated sampling theo-
rem. This is an interpolation formula that expresses a function that is band-limited to the
frequency interval [−W, W]. Such a function has a Fourier representation of the form (see
also Strang [339, p. 325])
f (z) = 1
2π
 W
−W
ˆf (k)eikz dk,
| ˆf (k)| ≤M,
(4.6.38)
in terms of its values at all integer points.
Theorem 4.6.7 (Shannon’s Sampling Theorem).
Let the function be band-limited to the frequency interval [−W, W]. Then
f (z) =
∞

j=−∞
f
jπ
W
 sin(Wz −jπ)
(Wz −jπ) .
(4.6.39)
Proof. We shall sketch a derivation of this for W = π. (Strang [339] gives an entirely
different derivation, based on Fourier analysis.) We ﬁrst note that (4.6.38) shows that f (z)
is analytic for all z. Then we consider the same Cauchy integral as many times before,
In(u) =
1
2πi

∂Dn
M(u)f (z)
M(z)(z −u) dz,
u ∈Dn.
Here M(z) = sin πz, which vanishes at all integer points, and Dn is the open rectangle with
vertices at ±(n + 1/2) ± bi. By the residue theorem, we obtain after a short calculation
In(u) = f (u) +
n

j=−n
M(u)f (j)
M′(j)(j −u) = f (u) −
n

j=−n
f (j) sin π(j −u)
π(j −u)
.
Set z = x + iy. Note that
|f (z)| ≤1
2π
 π
−π
Me−kydk ≤M(e|πy| −e−|πy|)
|2πy|
,
|M(z)| ≥e|πy|.
These inequalities, applied for y = b, allow us to let b →∞(2b is the height of the
symmetric rectangular contour). Then it can be shown that In(u) →0 as n →∞, which
establishes the sampling theorem for W = π. The general result is then obtained by “regula
de tri,”161 but it is sometimes hard to get it right.
Note the similarity of (4.6.39) to Lagrange’s interpolation formula. Like Lagrange’s,
it is a so-called cardinal interpolation formula. As Wz/π tends to an integer m, all terms
except one on the right-hand side become zero; for j = m the term becomes f (mπ/W).
161This rule from Euclid’s ﬁfth book of Elementa tells us how, given three out of four proportional quantities
a/b = c/d, one determines the fourth. This name is used in elementary mathematics, e.g., in Germany and
Sweden, but does not seem to be known under the same name in English-speaking countries.

498
Chapter 4. Interpolation and Approximation
Let f be a function which is zero outside the interval [0, L]. Its Fourier transform is
then given by
g(ω) =
 L
0
f (x)e−2πiωx dx.
(4.6.40)
We want to approximate g(ω) using values of f (x) sampled at intervals 4x,
fj = f (j4x),
0 < j < N −1,
L = N4x.
The integral (4.6.40) can be approximated by
g(ω) ≈L
N
N−1

j=0
fje−2πiωj4x.
(4.6.41)
Since only N values of fj are used as input and we want the computed values to be linearly
independent, we cannot approximate g(ω) at more than N points. The wave of lowest
frequency associated with the interval [0, L] is ω = 1/L = 1/(N4x), since then [0, L]
corresponds to one full period of the wave. We therefore choose points ωk = k4ω, k = 0 :
N in the frequency space such that the following reciprocity relations hold:
LW = N,
4x4ω = 1/N.
(4.6.42)
With this choice it holds that
W = N4ω = 1/4x,
L = N4x = 1/4ω.
(4.6.43)
Noting that (j4x)(k4ω) = jk/N we get from the trapezoidal approximation
g(ωk) ≈L
N
N−1

j=0
fje−2πikj/N = Lck,
k = 0 : N −1,
where ck is the coefﬁcient of the DFT.
The frequency ωc = 1/(24x) = W/2 is the so-called Nyquist’s critical frequency.
Sampling the wave sin(2πωcx) with sampling interval 4x will sample exactly two points
per cycle. It is a remarkable fact that if a function f (x), deﬁned on [−∞, ∞], is band-width
limited to frequencies smaller than or equal to ωc, then f (x) is completely determined by
its sample values j4x, ∞≤j ≤∞; see Shannon’s sampling theorem, our Theorem 4.6.7.
If the function is not band-width limited the spectral density outside the critical fre-
quency is moved into that range. This is called aliasing. The relationship between the
Fourier transform g(ω) and the DFT of a ﬁnite sampled representation can be character-
ized as follows. Assuming that the reciprocity relations (4.6.42) are satisﬁed, the DFT of
fj = f (j4x), 0 ≤j < N, will approximate the periodic aliased function
˜gk = ˜g(k4ω),
0 ≤j < N,
(4.6.44)
where
˜g(ω) = g(ω) +
∞

k=1
(g(ω + kW) + g(ω −kW)) ,
ω ∈[0, W].
(4.6.45)

4.6. Fourier Methods
499
Since by (4.6.43) W = 1/4x, we can increase the frequency range [0, W] covered by
decreasing 4x.
Example 4.6.4.
The function f (x) = e−x, x > 0, f (x) = 0, x < 0, has Fourier transform
g(ω) =
1
1 + 2πiω = 1 −i2πω
1 + 4π2ω2
(cf. Example 4.6.3). Set f (0) = 1/2, the average of f (−0) and f (+0), which is the value
given by the inverse Fourier transform at a discontinuity.
Set N = 32, T = 8, and sample the f in the interval [0, T ] at equidistant points j4t,
j = 0 : N −1. Note that T is so large that the aliased function (4.6.44) is nearly equal to
f . This sampling rate corresponds to 4t = 8/32 = 1/4 and W = 4.
The effect of aliasing in the frequency domain is evident. The error is signiﬁcant
for frequencies larger than the critical frequency W/2. To increase the accuracy W can be
increased by decreasing the sampling interval 4x; see Figure 4.6.4.
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
0.5
1
1.5
2
2.5
3
3.5
4
−0.5
0
0.5
Figure 4.6.4. The real (top) and imaginary (bottom) parts of the Fourier transform
(solid line) of e−x and the corresponding DFT (dots) with N = 32, T = 8.

500
Chapter 4. Interpolation and Approximation
Review Questions
4.6.1 Derive the orthogonality properties and coefﬁcient formulas which are fundamental
to Fourier analysis, for both the continuous and the discrete case.
4.6.2 Give sufﬁcient conditions for the Fourier series of the function f to be everywhere
convergent to f . To what value will the Fourier series converge at a point x = a of
discontinuity of f ?
4.6.3 How can the Fourier expansion be generalized to a function f (x, y) of two variables?
4.6.4 (a) Give two ways in which a real function f deﬁned on the interval [0, π] can be
extended to a periodic function.
(b) What disadvantage (for Fourier analysis) is incurred if the periodic continuation
has a discontinuity—for example, in its derivative at the end points of 80, π9?
4.6.5 Describe how the behavior of the coefﬁcients of the discrete Fourier expansion can
be modiﬁed to improve the approximation of the corresponding continuous Fourier
expansion.
4.6.6 Formulate the Fourier integral theorem.
4.6.7 What is meant by aliasing, when approximating the Fourier transform by a discrete
transform?
Problems and Computer Exercises
4.6.1 Give a simple characterization of the functions which have a sine expansion con-
taining odd terms only.
4.6.2 Let f be an even function, with period 2π, such that
f (x) = π −x,
0 ≤x ≤π.
(a) Plot the function y = f (x) for −3π ≤x ≤3π. Expand f in a Fourier series.
(b) Use this series to show that 1 + 3−2 + 5−2 + 7−2 + · · · = π2/8.
(c) Compute the sum 1 + 2−2 + 3−2 + 4−2 + 5−2 + · · · .
(d) Compute, using (4.6.9), the sum 1 + 3−4 + 5−4 + 7−4 + · · · .
(e) Differentiate the Fourier series term by term, and compare with the result for the
rectangular wave in Sec. 4.6.1.
4.6.3 (a) Show that the function G1(t) = t −1/2, 0 < t < 1, has the expansion
G1(t) = −
∞

n=1
sin 2nπt
nπ
.
(b) Let G1(t) be as in (a) and consider a sequence of functions such that G′
p+1(t) =
Gp(t), p = 1, 2, . . . . Derive by termwise integration the expansion for the functions

Problems and Computer Exercises
501
Gp(t), and show that cp −Gp(t) has the same sign as cp. Show also that for p even
∞

n=1
n−p = 1
2|cp|(2π)p.
4.6.4 (a) Using that sin x is the imaginary part of eix, prove that
N−1

k=1
sin πk
N = cot π
2N .
(b) Determine a sine polynomial n−1
j=1 bj sin jx, which takes on the value 1 at the
points xk = πk/n, k = 1 : n −1.
Hint: Use (4.6.26) or recall that the sine polynomial is an odd function.
(c) Compare the limiting value for bj as n →∞with the result in Example 4.6.1.
4.6.5 (a) Prove the inequality in (4.6.10).
(b) Show, under the assumptions on f which hold in (4.6.10), that for k ≥1, f can
be approximated by a trigonometric polynomial such that
f −
n

j=−n
cjeijx
∞<
2
knk ∥f (k+1)∥∞.
4.6.6 (a) Fourier approximations to the rectangular wave, f (x) = 1, 0 < x < π, f (x) =
−1, −π < x < 0, are shown in Figure 4.6.2 for one, two, ﬁve, and ten terms.
Plot and compare the corresponding smoothed approximations when the σ-factors
in (4.6.15) have been applied.
(b) The delta function δ(x) is deﬁned as being zero everywhere except between the
limits ±ϵ, where ϵ tends to zero. At x = 0 the function goes to inﬁnity in such a
way that the area under the function equals one. The formal Fourier expansion of
δ(x) yields
f (x) = 1
π
1
2 + cos x + cos 2x + cos 3x + · · ·

,
which does not converge anywhere. If the σ-factors in (4.6.15) are applied, we
obtain the expansion
ym(x) = 1
π
1
2 + σ1 cos x + σ2 cos 2x + · · · + σm−1 cos(m −1)x

,
(4.6.46)
which can be considered the trigonometric representation of the delta function. Plot
ym(x), −6 ≤x ≤6, for several values of m.
4.6.7 (a) Consider a real function with the Fourier expansion
F(φ) =
∞

n=−∞
cneinφ.

502
Chapter 4. Interpolation and Approximation
Show that this rewritten for convergence acceleration with the generalized Euler’s
method is
F(φ) = c0 + 2ℜ
∞

n=1
cnzn,
z = eiφ.
Hint: Show that c−n = ¯cn.
(b) Set cn = an −ibn, where an, bn are real. Show that
∞

n=0
(an cos nφ + bn sin nφ) = ℜ
∞

n=0
cnzn.
(c) How would you rewrite the Chebyshev series ∞
n=0 Tn(x)/(1 + n2)?
(d) Consider also how to handle a complex function F(φ).
In the following problems, we do not require any investigation of whether it is
permissible to change the order of summations, integrations, differentiations; it is
sufﬁcient to treat the problems in a purely formal way.
4.6.8 The partial differential equation ∂u
∂t = ∂2u
∂x2 is called the heat equation. Show that the
function
u(x, t) = 4
π
∞

k=0
sin(2k + 1)x
2k + 1
e−(2k+1)2t
satisﬁes the differential equation for t > 0, 0 < x < π, with boundary conditions
u(0, t) = u(π, t) = 0 for t > 0, and initial condition u(x, 0) = 1 for 0 < x < π
(see Example 4.6.1).
4.6.9 Show that if g(t) is the Fourier transform of f (x), then
(a) e2πktg(t) is the Fourier transform of f (x + k).
(b) (2πit)kg(t) is the Fourier transform of f (k)(x), assuming that f (x) and its
derivatives up to the kth order tend to zero as x →∞.
4.6.10 The correlation of f1(x) and f2(x) is deﬁned by
c(ξ) =
 ∞
−∞
f1(x + ξ)f2(x) dx.
(4.6.47)
Show that if f1(x) and f2(x) have Fourier transforms g1(t) and g2(t), respectively,
then the Fourier transform of c(ξ) is h(t) = g1(t)g2(−t).
Hint: Compare Theorem 4.6.6.
4.6.11 (a) Work out the details of the proof of the sampling theorem.
(b) The formulation of the sampling theorem with a general W in Strang [339] does
not agree with ours in (4.6.39). Who is right?
4.6.12 Suppose that f (z) satisﬁes the assumptions of our treatment of the sampling theorem
for W = π/h. Show by integration term by term that
lim
R→∞
 R
−R
f (u) du = h lim
n→∞
n

j=−n
f (jh).

4.7. The Fast Fourier Transform
503
Hint: Use the classical formula
 ∞
−∞
sin x
x
dx = π.
Full rigor is not necessary.
4.7
The Fast Fourier Transform
4.7.1
The FFT Algorithm
Consider the discrete Fourier transform (DFT)
f (x) =
N−1

j=0
cjeijx
of a function, whose values f (xk) are known at the grid points xk = 2πk/N, k = 0 : N −1.
According to Theorem 4.6.4 the coefﬁcients are given by
cj = 1
N
N−1

k=0
f (xk)e−ijxk,
j = 0 : N −1.
(4.7.1)
The evaluation of expressions of the form (4.7.1) occur also in discrete approximations to
the Fourier transform.
Setting ωN = e−2πi/N this becomes
cj = 1
N
N−1

k=0
ωjk
N f (xk),
j = 0 : N −1,
(4.7.2)
where ωN is an Nth root of unity, (ωN)N = 1. It seems from (4.7.2) that computing the
discrete Fourier coefﬁcients would require N2 complex multiplications and additions. As
we shall see, only about N log2 N complex multiplications and additions are required using
an algorithm called the fast Fourier transform (FFT).
The modern usage of the FFT started in 1965 with the publication of [78] by James W.
Cooley of IBM Research and John W. Tukey, Princeton University.162
In many areas
of application (digital signal processing, image processing, time-series analysis, to name
a few), the FFT has caused a complete change of attitude toward what can be done using
discrete Fourier methods. Without the FFTmany modern devices such as cell phones, digital
cameras, CT scanners, and DVDs would not be possible. Some applications considered in
astronomy require FFTs of several gigapoints.
162Tukey came up with the basic algorithm at a meeting of President Kennedy’s Science Advisory Committee.
One problem discussed at this meeting was that the ratiﬁcation of a US–Soviet nuclear test ban depended on a fast
method to detect nuclear tests by analyzing seismological time-series data.

504
Chapter 4. Interpolation and Approximation
In the following we will use the common convention not to scale the sum in (4.7.2)
by 1/N.
Deﬁnition 4.7.1.
The DFT of the vector f ∈CN is
y = FNf,
(4.7.3)
where Fn ∈CN×N is the DFT matrix with elements
(FN)jk = ωjk
N ,
j, k = 0 : N −1,
(4.7.4)
where ωN = e−2πi/N.163
From the deﬁnition it follows that the DFT matrix FN is a complex Vandermonde
matrix. Since ωjk
N = ωkj
N , FN is symmetric. By Theorem 4.6.4
1
N F H
N FN = I,
where F H
N is the complex conjugate transpose of FN. Hence the matrix
1
√
N FN is a unitary
matrix and the inverse transform can be written as
f = 1
N F H
N y.
We now describe the central idea of the FFT algorithm, which is based on the divide
and conquer strategy (see Sec. 1.2.3). Assume that N = 2p and set
k =
%
2k1
if k is even,
2k1 + 1
if k is odd,
0 ≤k1 ≤m −1,
where m = N/2 = 2p−1. Split the DFT sum into an even and an odd part:
yj =
m−1

k1=0
(ω2
N)jk1f2k1 + ωj
N
m−1

k1=0
(ω2
N)jk1f2k1+1,
j = 0 : N −1.
Let β be the quotient and j1 the remainder when j is divided by m, i.e., j = βm+j1. Then,
since ωN
N = 1,
(ω2
N)jk1 = (ω2
N)βmk1(ω2
N)j1k1 = (ωN
N)βk1(ω2
N)j1k1 = ωj1k1
m .
Thus if, for j1 = 0 : m −1, we set
φj1 =
m−1

k1=0
f2k1ωj1k1
m ,
ψj1 =
m−1

k1=0
f2k1+1ωj1k1
m ,
(4.7.5)
163Some authors set ωN = e2πi/N. Which convention is used does not much affect the development.

4.7. The Fast Fourier Transform
505
then yj = φj1 + ωj
Nψj1. The two sums on the right are elements of the DFTs of length
N/2 applied to the parts of f with odd and even subscripts. The entire DFT of length N is
obtained by combining these two DFTs! Since ωm
N = −1, we have
yj1 = φj1 + ωj1
Nψj1,
(4.7.6)
yj1+N/2 = φj1 −ωj1
Nψj1,
j1 = 0 : N/2 −1.
(4.7.7)
These expressions, noted already by Danielson and Lanczos [90], are often called butterﬂy
relations because of the data ﬂow pattern. Note that these can be performed in place, i.e.,
no extra vector storage is needed.
The computation of φj1 and ψj1 means that one does two Fourier transforms with
m = N/2 terms instead of one with N terms. If N/2 is even the same idea can be applied
to these two Fourier transforms. One then gets four Fourier transforms, each of which has
N/4 terms. If N = 2p, this reduction can be continued recursively until we get N DFTs
with one term. But F1 = I, the identity. A recursive MATLAB implementation of the FFT
algorithm is given in Problem 4.7.2.
Example 4.7.1.
For n = 22 = 4, we have ω4 = e−πi/2 = −i, and the DFT matrix is
F4 =


1
1
1
1
1
−i
(−i)2
(−i)3
1
(−i)2
(−i)4
(−i)6
1
(−i)3
(−i)6
(−i)9

=


1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i

.
(4.7.8)
It is symmetric and its inverse is
F −1
4
= 1
4


1
1
1
1
1
i
−1
−i
1
−1
1
−1
1
−i
−1
i

.
The number of complex operations (one multiplication and one addition) required to
compute {yj} from the butterﬂy relations when {φj1} and {ψj1} have been computed is 2p,
assuming that the powers of ω are precomputed and stored. Thus, if we denote by qp the
total number of operations needed to compute the DFT when N = 2p, we have
qp ≤2qp−1 + 2p,
p ≥1.
Since q0 = 0, it follows by induction that qp ≤p · 2p = N · log2 N. Hence, when N is a
power of 2, the FFT solves the problem with at most N · log2 N operations.
For example, when N = 220 = 1,048,576 the FFT algorithm is theoretically a factor
of 84,000 faster than the “conventional” O(N2) algorithm. On a 3 GHz laptop, a real FFT
of this size takes about 0.1 second using MATLAB 6, whereas more than two hours would
be required by the conventional algorithm! The FFT not only uses fewer operations to
evaluate the DFT, it also is more accurate. Whereas when using the conventional method
the roundoff error is proportional to N, for the FFT algorithm it is proportional to log2 N.

506
Chapter 4. Interpolation and Approximation
Example 4.7.2.
Let N = 24 = 16. Then the 16-point DFT (0:1:15) can be split into two 8-point
DFTs (0:2:14) and (1:2:15), which can each be split in two 4-point DFTs. Repeating these
splittings we ﬁnally get 16 one-point DFTs which are the identity F1 = 1. The structure of
this FFT is illustrated below.
[ 0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15 ]
↙↘
[ 0
2
4
6
8
10
12
14 ]
↙↘
[ 1
3
5
7
9
11
13 ]
↙↘
[ 0
4
8
12 ]
↙↘
[ 2
6
10
14 ]
↙↘
[ 1
5
9
13 ]
↙↘
[ 3
7
11
15 ]
↙↘
[ 0
8 ]
↙↘
[ 4
12 ]
↙↘
[ 2
10 ]
↙↘
[ 6
14 ]
↙↘
[ 1
9 ]
↙↘
[ 5
13 ]
↙↘
[ 3
11 ]
↙↘
[ 7
15 ]
↙↘
[0] [8] [4] [12]
[2] [10] [6] [14] [1] [9] [5] [13] [3] [11] [7] [15]
In most implementations the explicit recursion is avoided. Instead the FFT algorithm
is implemented in two stages:
• a reordering stage in which the data vector f is permuted;
• a second stage in which ﬁrst N/2 FFTtransforms of length 2 are computed on adjacent
elements, followed by N/4 transforms of length 4, etc., until the ﬁnal result is obtained
by merging two FFTs of length N/2.
We now consider each stage in turn.
Each step of the recursion involves an even–odd permutation. In the ﬁrst step the
points with last binary digit equal to 0 are ordered ﬁrst and those with last digit equal to 1
are ordered last. In the next step the two resulting subsequences of length N/2 are reordered
according to the second binary digit, etc. It is not difﬁcult to see that the combined effect of
the reordering in stage 1 is a bit-reversal permutation of the data points. For i = 0 : N −1,
let the index i have the binary expansion
i = b0 + b1 · 2 + · · · + bt−1 · 2t−1
and set
r(i) = bt−1 + · · · + b1 · 2t−2 + b0 · 2t−1.
That is, r(i) is the index obtained by reversing the order of the binary digits. If i < r(i),
then exchange fi and fr(i). This reordering is illustrated for N = 16 below.

4.7. The Fast Fourier Transform
507
Decimal
Binary
Decimal
Binary
0
0000
0
0000
1
0001
8
1000
2
0010
4
0100
3
0011
12
1100
4
0100
2
0010
5
0101
10
1010
6
0110
6
0110
7
0111
=⇒
14
1110
8
1000
1
0001
9
1001
9
1001
10
1010
5
0101
11
1011
13
1101
12
1100
3
0011
13
1101
11
1011
14
1110
7
0111
15
1111
15
1111
We denote the permutation matrix performing the bit-reversal ordering by PN. Note
that if an index is reversed twice we end up with the original index. This means that
P −1
N
= P T
N = PN,
i.e., PN is symmetric. The permutation can be carried out “in place” by a sequence of
pairwise interchanges or transpositions of the data points. For example, for N = 16 the pairs
(1,8), (2,4), (3,12), (5,10), (7,14), and(11,13)areinterchanged. Thebit-reversalpermutation
can take a substantial fraction of the total time to do the FFT. Which implementation is best
depends strongly on the computer architecture.
We now consider the second stage of the FFT. The key observation to develop a
matrix-oriented description of this stage is to note that the Fourier matrices FN after an
odd–even permutation of the columns can be expressed as a 2×2 block matrix, where each
block is either FN/2 or a diagonal scaling of FN/2.
Theorem 4.7.2 (Van Loan [366, Theorem 1.2.1]).
Let ZT
N be the permutation matrix which applied to a vector groups the even-indexed
components ﬁrst and the odd-indexed last.164 If N = 2m, then
FNZN =
 Fm
<mFm
Fm
−<mFm

=
 Im
<m
Im
−<m
 
Fm
0
0
Fm

,
<m = diag (1, ωN, . . . , ωm−1
N
),
ωN = e−2πi/N.
(4.7.9)
Proof. The proof essentially follows from the derivation of the butterﬂy relations (4.7.6)–
(4.7.7).
164Note that ZT
N = Z−1
N is the so-called perfect shufﬂe permutation, which in the permuted vector ZT
Nf is
obtained by splitting f in half and then “shufﬂing” the top and bottom halves.

508
Chapter 4. Interpolation and Approximation
Example 4.7.3.
We illustrate Theorem 4.7.2 for N = 22 = 4. The DFT matrix F4 is given in
Example 4.7.1. After a permutation of the columns F4 can be written as a 2 × 2 block-
matrix
F4ZT
4 =


1
1
1
1
1
−1
−i
i
1
1
−1
−1
1
−1
i
−i

=
 F2
<2F2
F2
−<2F2

,
where
F2 =

1
1
1
−1

,
<2 = diag (1, −i).
When N = 2p the FFT algorithm can be interpreted as a sparse factorization of the
DFT matrix
FN = Ak · · · A2A1PN,
(4.7.10)
wherePN isthebit-reversalpermutationmatrixandA1, . . . , Ak areblock-diagonalmatrices,
Aq = diag (BL, . . . , BL
8
9:
;
r
),
L = 2q,
r = N/L.
(4.7.11)
Here the matrix Bk ∈CL×L is the radix-2 butterﬂy matrix deﬁned by
BL =

IL/2
<L/2
IL/2
−<L/2

,
(4.7.12)
<L/2 = diag (1, ωL, . . . , ωL/2−1
L
),
ωL = e−2πi/L.
(4.7.13)
The FFT algorithm described above is usually referred to as the Cooley–Tukey FFT
algorithm. Using the fact that both the bit-reversal matrix PN and the DFT matrix Fn are
symmetric, we obtain by transposing (4.7.10) the factorization
FN = F T
N = PNAT
1 AT
2 · · · AT
k .
(4.7.14)
This gives rise to a “dual” FFT algorithm, referred to as the Gentleman–Sande algo-
rithm [155]. In this the bit-reversal permutation comes after the other computations. In
many important applications, such as convolution and the solution of discretized Poisson
equations (see Sec. 1.1.4), this permits the design of in-place FFT solutions that avoid bit-
reversal altogether; see Van Loan [366, Secs. 4.1, 4.5].
In the operation count for the FFT above we assumed that the weights ωj
L, j = 1 :
L −1, ωL = e−2πi/L, are precomputed. To do this one could use that
ωj
L = cos(jθ) −i sin(jθ),
θ = 2π/L,
for L = 2q, q = 2 : k. This is accurate, but expensive, since it involves L−1 trigonometric
functions calls. An alternative is to compute ω = cos(θ) −i sin(θ) and use repeated
multiplication,
ωj = ωωj−1,
j = 2 : L −1.
This replaces one sine/cosine call with a single complex multiplication, but has the drawback
that accumulation of roundoff errors will give an error in ωj
L of order ju.

4.7. The Fast Fourier Transform
509
4.7.2
Discrete Convolution by FFT
The most important operation in signal processing is computing the discrete version of the
convolution operator. This awkward operation in the time domain becomes very simple in
the frequency domain.
Deﬁnition 4.7.3.
The convolution of two sequences fi and gi, i = 0 : N −1, is conv (f, g) =
(h0, h1, . . . , hN−1)T , where
hk =
N−1

i=0
figk−i,
k = 0 : N −1,
(4.7.15)
where the sequences are extended to have period N by setting fi = fi+jN, gi = gi+jN for
all integers i, j.
The discrete convolution can be used to approximate the convolution deﬁned for
continuous functions in Deﬁnition 4.6.5 in a similar way as the Fourier transform was
approximated using sampled values in Sec. 4.6.6.
We can write the sum in (4.7.15) as a matrix-vector multiplication h = Gf , where G
is a Toeplitz matrix. Writing out components we have


h0
h1
h2
...
hN−1


=


g0
gN−1
gN−2
· · ·
g1
g1
g0
gN−1
· · ·
g2
g2
g1
g0
· · ·
g3
...
...
...
· · ·
gN−1
gN−2
gN−3
· · ·
g0




f0
f1
f2
...
fN−1


.
Note that each column in G is a cyclic down-shifted version of the previous column. Such
a matrix is called a circulant matrix. We have
G = [ g
CNg
· · ·
CN−1
N
g ] = g0I + g1CN + · · · + gN−1CN−1
N
,
(4.7.16)
and CN is the circulant permutation matrix
CN =


0
· · ·
1
1
1
...
...
...
0
· · ·
1
0


.
(4.7.17)
The following result is easily veriﬁed.
Lemma 4.7.4.
The eigenvalues of the circulant matrix CN in (4.7.17) are
ωj = e−2πj/N,
j = 0 : N −1,

510
Chapter 4. Interpolation and Approximation
where ω is an Nth root of unity, i.e., ωN = 1. The columns of the DFT matrix FN,
xj = (1, ωj, . . . , ωN−1
j
)T ,
j = 0 : N −1,
are eigenvectors.
Since the matrix G in (4.7.16) is a polynomial in CN it has the same set of eigenvectors,
and thus G is diagonalized by the DFT matrix FN,
G = FNXF −1
N ,
X = diag (λ1, . . . , λn),
(4.7.18)
where the eigenvalues of G are
λi = (1, ωj, . . . , ωN−1
j
)g,
j = 0 : N −1,
which is the FFT of the ﬁrst column in G. Hence X = diag (FNg), where diag (x) denotes
a diagonal matrix with diagonal elements equal to the elements in the vector x.
Theorem 4.7.5.
Let fi and gi, i = 0 : N −1, be two sequences with DFTs equal to FNf and FNg.
Then the DFT of the convolution of f and g is FNf. ∗FNg (.∗denotes the elementwise
product).
Proof. From G = F −1
N diag (FNg)FN it follows that
h = Gf = F −1
N diag (FNg)FNf = F −1
N ((FNg). ∗(FNf )).
(4.7.19)
This shows that using the FFT algorithm the discrete convolution can be computed
in O(N log2 N) operations as follows: First the two FFTs of f and g are computed and
multiplied (pointwise) together. Then the inverse DFT of this product is computed. This is
one of the most useful properties of the FFT!
Using the Gentleman–Sande algorithm FN = PNAT for the forward DFT and the
Cooley–Tukey algorithm FN = APN for the inverse DFT,
F −1
N
= 1
N F H
N = 1
N
¯APN,
we get from (4.7.19)
h = 1
N
¯APN((PNAT f ). ∗(PNAT g)) = 1
N
¯A((AT f ). ∗(AT g)).
(4.7.20)
This shows that h can be computed without the bit-reversal permutation PN which typically
can save 10–30 percent of the overall computation time.
4.7.3
FFTs of Real Data
Frequently the FFT of a real data vector is required. The complex FFT algorithm can still
be used, but is inefﬁcient both in terms of storage and operations. By using symmetries in

4.7. The Fast Fourier Transform
511
the DFT, which correspond to the symmetries noted in the Fourier transform in Table 4.6.1,
better alternatives can be found.
Consider the DFT matrix for N = 4 in (4.7.8). Note that the fourth row is the
conjugate of the second row. This is not a coincidence; the conjugate transpose of the DFT
matrix FN can be obtained by reversing the order of the last N −1 rows. Let TN be the
N × N permutation matrix obtained by reversing the last N −1 columns in the unit matrix
IN. For example,
T4 =


1
0
0
0
0
0
0
1
0
0
1
0
0
1
0
0

.
Then it holds that
F H
N = F N = TNFN = FNTN.
(4.7.21)
We ﬁrst verify that F N = TNFN, by observing that
[TNFN]jk = ω(N−j)k
N
= ω−jk
N
= ωjk
N = [F N]jk,
1 ≤j ≤N −1.
Since FN and TN are both symmetric, we also have F H
N = (TNFN)T = FNTN.
We say that a vector y ∈CN is conjugate even if y = TNy, and conjugate odd if
y = −TNy. Suppose now that f is real and u = FNf . Then it follows that
u = F Nf = TNFNf = TNu,
i.e., u is conjugate even. If a vector u of even length N is conjugate even, this implies that
uj = uN−j,
j = 1 : N/2.
In particular, uj is real for j = 0, N/2.
For purely imaginary data g and v = FNg, we have
v = F H
N g = −F H
N g = −TNFNg = −TNv,
i.e., v is conjugate odd. Some other useful symmetry properties are given in Table 4.7.1.
We have proved the ﬁrst two properties; the others are established similarly and we leave
the proofs to the reader; see Problem 4.7.4.
Table 4.7.1. Useful symmetry properties of the DFT.
Data f
Deﬁnition
DFT FNf
real
conjugate even
imaginary
conjugate odd
real even
f = TNf
real
real odd
f = −TNf
imaginary
conjugate even
f = TNf
real
conjugate odd
f = −TNf
imaginary

512
Chapter 4. Interpolation and Approximation
We now outline how symmetries can be used to compute the DFTs u = FNf and
v = FNg of two real functions f and g simultaneously. First form the complex function
f + ig and compute its DFT
w = FN(f + ig) = u + iv
by any complex FFT algorithm. Multiplying by TN we have
TNw = TNFN(f + ig) = TN(u + iv) = u + iv,
where we have used that u and v are conjugate even. Adding and subtracting these two
equations we obtain
w + TNw = (u + u) + i(v + v),
w −TNw = (u −u) + i(v −v).
We can now retrieve the two DFTs from
u = FNf = 1
2
,
Re(w + TNw) + i Im(w −TNw)
-
,
(4.7.22)
v = FNg = 1
2
,
Im(w + TNw) −i Re(w −TNw)
-
.
(4.7.23)
Note that because of the conjugate even property of u and v there is no need to save the
entire transforms.
The above scheme is convenient when, as for convolutions, two real transforms are
involved. It can also be used to efﬁciently compute the DFT of a single real function of
length N = 2p. First express this DFT as a combination of the two real FFTs of length
N/2 corresponding to even and odd numbered data points (as in (4.7.5)). Then apply the
procedure above to simultaneously compute these two real FFTs.
4.7.4
Fast Trigonometric Transforms
Two real transforms, the discrete sine transform (DST) and discrete cosine transform
(DCT), are of interest. There are several variations of these. We deﬁne the ﬁrst two as
follows:
• Given real data fj, j = 1 : N −1, compute
yk =
N−1

j=1
sin

kj π
N

fj
(DST-1).
(4.7.24)
• Given real data fj, j = 0 : N, compute
yk =
N
′′
j=0
cos

kj π
N

fj
(DCT-1).
(4.7.25)
(The double prime on the sum means that the ﬁrst and last term are to be halved.)

4.7. The Fast Fourier Transform
513
The real and imaginary parts of the DFT matrix FN consist of cosines and sines,
FN = CN −iSN, where
(CN)kj = cos

kj 2π
N

,
(SN)kj = sin

kj 2π
N

,
k, j = 0 : N −1.
(4.7.26)
The DST and DCT transforms can be expressed in matrix form as
y = S2N(1 : N −1, 1 : N −1)f,
y = C2N(0 : N, 0 : N) ˜f ,
respectively, where ¯f = ( 1
2f0, f1, . . . , fN−1, 1
2fN)T .
These two transforms can be computed by applying the FFT algorithm (for real data)
to an auxiliary vector ˜f formed by extending the given data vector f either into an odd or
even sequence.
For DST-1 the data fj, j = 1 : N −1, are extended to an odd sequence of length 2N
by setting
f0 = fN = 0,
f2N−j ≡−fj,
j = 1 : N −1.
For example, the data f = {f1, f2, f3}, (N = 4) are extended to
˜f = {0, f1, f2, f3, 0, −f3, −f2, −f1}.
The extended vector satisﬁes ˜f = −T2N ˜f , and thus by Table 4.7.1 the DFT of ˜f will be
pure imaginary.
For DCT-1 the data fj, j = 0 : N, are extended to an even sequence of length 2N by
setting
f2N−j ≡fj,
j = 1 : N.
For example, the data f = {f0, f1, f2, f3, f4}, (N = 4) are extended to
˜f = {f0, f1, f2, f3, f4, f3, f2, f1}
so that ˜f = T2N ˜f . By Table 4.7.1 the DFT of ˜f will then be real.
Theorem 4.7.6 (Van Loan [366, Sec. 4.4]).
Let fj, j = 1 : N −1, form a real data vector f and extend it to a vector ˜f with
˜f0 = ˜fN = 0 so that ˜f = −T2N ˜f . Then y(1 : N −1) is the DST of ˜f , where
y = y(0 : 2N −1) = i
2F2N ˜f .
Let fj, j = 0 : N, form a real data vector f and extend it to a vector ˜f so that
˜f = T2N ˜f . Then y(0 : N) is the DCT of f , where
y = y(0 : 2N −1) = 1
2F2N ˜f .
There is an inefﬁciency factor of two in the above procedure. This can be eliminated
by using a different auxiliary vector. For details we refer to [294, pp. 420–421]; [366,
Sec. 4.4].

514
Chapter 4. Interpolation and Approximation
In some application areas variants of the above transforms called DST-2 and DCT-2
turn out to be more useful. We deﬁne them as follows:
• Given real data fj, j = 1 : N, compute
yk =
N

j=1
sin

k(2j −1) π
2N

fj
(DST-2).
(4.7.27)
• Given real data fj, j = 0 : N −1, compute
yk =
N−1

j=0
cos

k(2j + 1) π
2N

fj
(DCT-2).
(4.7.28)
The DST-2 and DCT-2 transforms can also be obtained by extending the data vector f .
For DST-2 the data vector fj, j = 0 : N −1, is extended to an odd sequence of length
2N. For example, the data {f1, f2, f3, f4}, (N = 4) are extended to
˜f = {f1, f2, f3, f4, −f4, −f3, −f2, −f1}.
The extended vector satisﬁes f = −T2Nf , and thus by Table 4.7.1 the DFT of f will be
imaginary.
For DCT-2 the data fj, j = 0 : N, are extended to an even sequence of length 2N.
For example, the data {f0, f1, f2, f3}, (N = 4) are extended to
˜f = {f0, f1, f2, f3, f3, f2, f1, f0}.
so that f = T2Nf . By Table 4.7.1 the DFT of f will then be real.
We give without proof the following result, which allows the computation of DST-2
and DCT-2 from the FFT.
Theorem 4.7.7.
Let fj, j = 1 : N, form a real data vector f and extend it to a vector ˜f so that
˜f = −T2N ˜f . Then y(1 : N) is the DST-2 of f , where
y = y(0 : 2N −1) = i
2<2NF2N ˜f ,
where
<2N = diag (1, ω4N, . . . , ω2N
4N).
Let fj, j = 0 : N −1, form a real data vector f and extend it to a vector ˜f so that
˜f = T2N ˜f . Then y(0 : N −1) is the DCT-2 of f , where
y = y(0 : 2N −1) = 1
2<2NF2N ˜f .
The two-dimensional DCT-2 transform has the property that, for a visual image, most
of the information is concentrated in the ﬁrst few coefﬁcients of the DCT. For this reason
the DCT-2 transform is often used in image compression algorithms.165
165This transform is used in the JPEG (Joint Photographic Experts Group) compression algorithm for image
processing. Each 8×8 block in the image is transformed by a two-dimensional DCT-2 transform; see Strang [340].

4.7. The Fast Fourier Transform
515
4.7.5
The General Case FFT
It can be argued [294, p. 409] that one should always choose N = 2k when using the FFT.
If necessary the data can be padded with zeros to achieve this. To introduce an odd factor
s, let N = sr, where r is a power of 2. Then one can combine the power of two algorithm
for the r-point subseries with a special algorithm for the s-point subseries. If s is a small
number, then one could generate the DFT matrix Fs and use matrix-vector multiplication;
(cf. the code given in Problem 4.7.2). But the general case when N is not a power of two
is at least of theoretical interest.
Suppose that N = r1r2 · · · rp. We will describe an FFT algorithm which requires
N(r1 + r2 + · · · + rp) complex operations. Set
Nν =
p
&
i=ν+1
ri,
ν = 0 : p −1,
Np = 1.
Thus
N = r1r2 · · · rνNν,
N0 = N.
The algorithm is based on two representations of integers which are generalizations of the
position principle (see Sec. 2.2.1).
I. Every integer j, 0 ≤j ≤N −1, has a unique representation of the form
j = α1N1 + α2N2 + · · · + αp−1Np−1 + αp,
0 ≤αi ≤ri −1.
(4.7.29)
II. For every integer β, 0 ≤β ≤N −1, β/N has a unique representation of the form
β
N = k1
N0
+ k2
N1
+ · · · +
kp
Np−1
,
0 ≤ki ≤ri −1.
(4.7.30)
Set
jν =
p

i=ν+1
αiNi,
αν
Nν
=
p−1

i=ν
ki+1
Ni
,
(jν < Nν).
(4.7.31)
As an exercise, the reader can verify that the coefﬁcients in the above representations can
be recursively determined from the following algorithms:166
j0 = j,
ji−1/Ni = αi + ji/Ni,
i = 1 : p,
β0 = β,
βi−1/ri = βi + ki/ri,
i = 1 : p.
From (4.7.29)–(4.7.31), it follows that, since Ni is divisible by Nν for i ≤ν,
jβ
N = integer +
p−1

ν=0
kν+1
Nν

p

i=ν+1
αiNi

=
p−1

ν=0
kν+1jν
Nν
+ integer.
166These algorithms can, in the special case that ri = B for all i, be used for converting integers or fractions to
the number system whose base is B; see Algorithm 2.1.

516
Chapter 4. Interpolation and Approximation
From this, it follows that
ωjβ = e2πijβ/N =
p−1
&
ν=0
ekν+1jν2πi/Nν =
p−1
&
ν=0
ωjνkν+1
ν
,
(4.7.32)
where ων = e2πi/Nν, ω0 = ω.
A split-radix approach for the general case FFT, which has a lower operation count,
is described in Van Loan [366, Sec. 2.1.4].
We now give an illustration of how the factorization in (4.7.32) can be utilized in FFT
for the case p = 3. Set, in accordance with (4.7.30),
fβ = c(0)(k1, k2, k3).
We have then
cj =
N−1

β=0
fβωjβ =
r1−1

k1=0
r2−1

k2=0
r3−1

k3=0
c(0)(k1, k2, k3)ωj2k3
2
ωj1k2
1
ωjk1.
One can thus compute successively (see (4.7.31))
c(1)(k1, k2, α3) =
r3−1

k3=0
c(0)(k1, k2, k3)ωj2k3
2
(j2 depends only on α3),
c(2)(k1, α2, α3) =
r2−1

k2=0
c(1)(k1, k2, α3)ωj1k2
1
(j1 depends only on α2, α3),
cj = c(3)(α1, α2, α3) =
r1−1

k1=0
c(2)(k1, α2, α3)ωjk1
(j depends on α1, α2, α3).
The quantities c(i) are computed for all r1r2r3 = N combinations of the values of the
arguments. Thus the total number of operations for the entire Fourier analysis becomes at
most N(r3 + r2 + r1). The generalization to arbitrary p is obvious.
Review Questions
4.7.1 Suppose we want to compute the DFT for N = 210. Roughly how much faster is the
FFT algorithm compared to the straightforward O(N2) algorithm?
4.7.2 Show that the matrix U =
1
√
N FN is unitary, i.e., U HU = I, where U H = (U)T .
4.7.3 Show that the DFT matrix F4 can be written as a 2 × 2 block matrix where each
block is related to F2. Give a generalization of this for FN, N = 2m that holds for
arbitrary m.
4.7.4 Work out, on your own, the bit-reversal permutation of the vector [0 : N −1] for the
case N = 24 = 16. How many exchanges need to be performed?

Problems and Computer Exercises
517
Problems and Computer Exercises
4.7.1 The following MATLAB script uses an algorithm due to Cooley et al. to permute the
vector x(1 : 2m), in bit-reversal order:
n = 2ˆm;
nv2 = n/2; nm1 = n - 1;
j = 1;
for i = 1:nm1
if i < j
t = x(j); x(j) = x(i); x(i) = t;
end
k = nv2;
while k < j
j = j - k; k = k/2;
end
j = j + k;
end
Plot the time taken by this algorithm on your computer for m = 5 : 10. Does the
execution time depend linearly on N = 2m?
4.7.2 The following MATLAB program (Moler and Eddins [269]) demonstrates how the
FFT idea can be implemented in a simple but efﬁcient recursive MATLAB program.
The program uses the fast recursion as long as n is a power of two. When it reaches
an odd length it sets up the Fourier matrix and uses matrix-vector multiplication.
Algorithm 4.3. Fast Fourier Transform.
function y = fftx(x);
% FFT computes the Fast Fourier Transform of x(1:n)
x = x(:);
n = length(x);
omega = exp(-2*pi*i/n);
if rem(n,2) == 0
% Recursive divide and conquer
k = (0:n/2-1)
w = omega.ˆk;
u = fftx(x(1:2:n-1));
v = w.*fftx(x(2:2:n));
y = [u+v; u-v];
else
% Generate the Fourier matrix
j = 0:n-1;
k = j’;
F = omega.ˆ(k*j);
y = F*x;
end
Apply this program to compute DFT of the rectangular wave in Sec. 4.6.1, sampled
at the points 2πα/N, α = 0 : N −1. Choose, for instance, N = 32, 64, 128.

518
Chapter 4. Interpolation and Approximation
4.7.3 Write an efﬁcient MATLAB program for computing the DFT of a real data vector
of length N = 2m. As outlined in Sec. 4.7.3, ﬁrst split the data in odd and even
data points. Compute the corresponding DFTs using one call of the function fftx in
Problem 4.7.2 with complex data of length N/2.
4.7.4 Verify the last four symmetry properties of DFTs in Table 4.7.1.
Notes and References
A modern treatment of divided differences considered as linear functionals on some vector
space of functions is given by de Boor [38]. Our notation [x1, . . . , xk]f for the divided
difference agrees with that used in de Boor [37]. Ageneral treatment of complex polynomial
interpolation on complex nodes is given by Gelfond [151, Sec. 1.4.3]; see also Horn and
Johnson [203, Sec. 6.1].
The barycentric form of Lagrange’s interpolation formula found in German literature
does not seem to have caught on elsewhere, until recently.
Berrut and Trefethen [24]
argue convincingly that this should be the standard method. The problem of choosing a
good ordering of points in Newton and barycentric Lagrange interpolations is discussed in
Werner [371]. Newton interpolation using the Leja ordering of points has been analyzed by
Reichel [299].
Uniform methods of solving linear problems and computing the inverse of a Vander-
monde matrix have been studied byTraub [355]. The algorithm for the inverseVandermonde
matrix given in the text is due to Higham [199, Sec. 22.1]. The O(n2) Björck–Pereyra al-
gorithm for solving Vandermonde systems appeared in [31].
An introduction to rational interpolation is given in Stoer and Bulirsch [338, Sec. 2.2].
The reciprocal differences of Thiele are also treated by Milne-Thomson [265], with ex-
pressions for the truncation error. The ρ-algorithm for convergence acceleration is due to
Wynn [385].
A good reference for multidimensional polynomial interpolation is Steffensen [330,
Sec. 19]; see also Isaacson and Keller [208, Sec. 6.6]. Methods for multidimensional inter-
polation of scattered data points, including radial basis functions, are surveyed by Powell
in [293]. For a more practical approach the survey by Hayes [189] is still very good. The
history of multidimensional interpolation is surveyed in [134].
The survey paper [27] gives a good summary of the pioneering work done by Garret
Birkhoff on interpolation and approximation of univariate and bivariate data. The book by
de Boor [37] has done much to further the use of B-splines in geometric constructions. It
contains a clear outline of the theory and also a library of Fortran programs for computations
with spline functions. Several packages are available for computing with splines, e.g., the
spline toolbox in MATLAB and FITPACK by Dierckx [97, 98]. A more geometric view of
splines is taken in Farin [116].
The computational advantage of the Stieltjes approach for discrete least squares ﬁtting
was pointed out by Forsythe [119]. Shampine [321] established the advantage of using the
alternative formula involving the residual rk.
The theory and application of Chebyshev systems in approximation and statistics
is surveyed in Karlin and Studden [222]. An application of uniform approximation with
polynomials and the Remez algorithm is the determination of nonrecursive digital ﬁlters
with minimal ripple; see Parks and McClellan [284]. The discrete ℓ1 approximation problem

Notes and References
519
is related to “sparse approximation” ideas that are currently attracting much attention in the
signal processing community.
The series named after Fourier was well established by the work of Bernoulli, Euler,
and Lagrange, before the time of Fourier. The Fourier integral is, however, the undis-
puted discovery of Fourier. A very readable introduction to harmonic analysis is given in
Lanczos [235, Chapter IV]. For a more exhaustive treatment of Gibbs’ phenomenon, see
Tadmor [348].
The rediscovery of the FFT algorithm is surveyed in [76]. Applications are surveyed
in [77, 58, 57]. The matrix-oriented framework for the FFT used in this book is due to Van
Loan [366]. An early implementation of the FFT is given by Singleton [324, 325]. For
a roundoff error analysis of the FFT see [8]. Algorithms for bit-reversal permutations are
reviewed in [223].
The FFT algorithm was discovered independently by several mathematicians. Similar
ideas were used already by Gauss [137]. The doubling algorithm is contained in the textbook
by Runge and König [309] from 1924 and in the paper by Danielson and Lanczos [90].
Some signiﬁcant developments in approximation theory are not covered in this book,
such as wavelets (see Daubechies [91]), radial basis functions (see Buhmann [59]), and
box splines (see de Boor, Höllig, and Riemenschneider [39]). The last two constructs are
different multidimensional generalizations of univariate spline functions.


Chapter 5
Numerical Integration
Commit your blunders on a small scale and
make your proﬁts on a large scale.
—Leo Hendrik Baekeland
5.1
Interpolatory Quadrature Rules
5.1.1
Introduction
In this chapter we study the approximate calculation of a deﬁnite integral
I[f ] =
 b
a
f (x) dx,
(5.1.1)
where f (x) is a given function and [a, b] a ﬁnite interval. This problem is often called
numerical quadrature, since it relates to the ancient problem of the quadrature of the
circle, i.e., constructing a square with equal area to that of a circle. The computation of
(5.1.1) is equivalent to solving the initial value problem
y′(x) = f (x),
y(a) = 0,
x ∈[a, b]
(5.1.2)
for y(b) = I[f ]; cf. Sec. 1.5.
As is well known, even many relatively simple integrals cannot be expressed in ﬁnite
terms of elementary functions, and thus must be evaluated by numerical methods. (For a
table of integrals that have closed analytical solutions, see [171].) Even when a closed form
analytical solution exists it may be preferable to use a numerical quadrature formula.
Since I[f ] is a linear functional, numerical integration is a special case of the problem
of approximating a linear functional studied in Sec. 3.3.4. The quadrature rules considered
will be of the form
I[f ] ≈
n

i=1
wif (xi),
(5.1.3)
521

522
Chapter 5. Numerical Integration
where x1 < x2 < · · · < xn are distinct nodes and w1, w2, . . . , wn the corresponding
weights. Often (but not always) all nodes lie in [a, b].
The weights wi are usually determined so that the formula (5.1.3) is exact for poly-
nomials of as high degree as possible. The accuracy therefore depends on how well the
integrand f (x) can be approximated by a polynomial in [a, b]. If the integrand has a sin-
gularity, for example, it becomes inﬁnite at some point in or near the interval of integration,
some modiﬁcation is necessary. Another complication arises when the interval of integra-
tion is inﬁnite. In both cases it may be advantageous to consider a weighted quadrature
rule:
 b
a
f (x)w(x) dx ≈
n

i=1
wif (xi).
(5.1.4)
Here w(x) ≥0 is a weight function (or density function) that incorporates the singularity
so that f (x) can be well approximated by a polynomial. The limits (a, b) of integration are
now allowed to be inﬁnite.
To ensure that the integral (5.1.4) is well deﬁned when f (x) is a polynomial, we
assume in the following that the integrals
µk =
 b
a
xkw(x) dx,
k = 1, 2, . . . ,
(5.1.5)
are deﬁned for all k ≥0, and µ0 > 0. The quantity µk is called the kth moment with respect
to the weight function w(x). Note that for the formula (5.1.4) to be exact for f (x) = 1 it
must hold that
µ0 =
 b
a
1 · w(x) dx =
n

i=1
wi.
(5.1.6)
In the special case that w(x) = 1, we have µ0 = b −a.
Deﬁnition 5.1.1.
A quadrature rule (5.1.3) has order of accuracy (or degree of exactness) equal to d
if it is exact for all polynomials of degree ≤d, i.e., for all p ∈Pd+1.
In a weighted interpolatory quadrature formula the integral is approximated by
 b
a
p(x)w(x) dx,
where p(x) is the unique polynomial of degree n−1 interpolating f (x) at the distinct points
x1, x2, . . . , xn. By Lagrange’s interpolation formula (Theorem 4.1.1)
p(x) =
n

i=1
f (xi)ℓi(x),
ℓi(x) =
n
&
j=1
j̸=i
(x −xj)
(xi −xj),
whereℓi(x)aretheelementaryLagrangepolynomialsassociatedwiththenodesx1, x2, . . . , xn.
It follows that for an interpolatory quadrature formula the weights are given by
wi =
 b
a
ℓi(x)w(x) dx.
(5.1.7)

5.1. Interpolatory Quadrature Rules
523
In practice, the coefﬁcients are often more easily computed using the method of undeter-
mined coefﬁcients rather than by integrating ℓi(x).
An expression for the truncation error is obtained by integrating the remainder (see
Theorems 4.2.3 and 4.2.4):
Rn(f ) =
 b
a
[x1, . . . , xn, x]f
n
&
i=1
(x −xi)w(x) dx
= 1
n!
 b
a
f (n)(ξx)
n
&
i=1
(x −xi)w(x) dx,
ξx ∈[a, b],
(5.1.8)
where the second expression holds if f (n) is continuous in [a, b].
Theorem 5.1.2.
For any given set of nodes x1, x2, . . . , xn an interpolatory quadrature formula with
weights given by (5.1.7) has order of exactness equal to at least n −1. Conversely, if the
formula has degree of exactness n −1, then the formula must be interpolatory.
Proof. For any f ∈Pn we have p(x) = f (x), and hence (5.1.7) has degree of exactness
at least equal to n −1. On the other hand, if the degree of exactness of (5.1.7) is n −1,
then putting f = ℓi(x) shows that the weights wi satisfy (5.1.7); that is, the formula is
interpolatory.
In general the function values f (xi) cannot be evaluated exactly. Assume that the
error in f (xi) is ei, where |ei| ≤ϵ, i = 1 : n. Then, if wi ≥0, the related error in the
quadrature formula satisﬁes
(((
n

i=1
wiei
((( ≤ϵ
n

i=1
|wi| ≤ϵµ0.
(5.1.9)
The last upper bound holds only if all weights in the quadrature rules are positive.
So far we have assumed that all the nodes xi of the quadrature formula are ﬁxed.
A natural question is whether we can do better by a judicious choice of the nodes. This
question is answered positively in the following theorem. Indeed, by a careful choice of
nodes the order of accuracy of the quadrature rule can be substantially improved.
Theorem 5.1.3.
Let k be an integer such that 0 ≤k ≤n. Consider the integral
I[f ] =
 b
a
f (x)w(x) dx,
and an interpolatory quadrature rule
In(f ) =
n

i=1
wif (xi),

524
Chapter 5. Numerical Integration
using n nodes. Let
γ (x) =
n
&
i=1
(x −xi)
(5.1.10)
be the corresponding node polynomial. Then the quadrature rule I[f ] ≈In(f ) has degree
of exactness equal to d = n + k −1 if and only if, for all polynomials p ∈Pk, the node
polynomial satisﬁes
 b
a
p(x)γ (x)w(x) dx = 0.
(5.1.11)
Proof. We ﬁrst prove the necessity of the condition (5.1.11). For any p ∈Pk the product
p(x)γ (x) is in Pn+k. Then since γ (xi) = 0, i = 1 : n,
 b
a
p(x)γ (x)w(x) dx =
n

i=1
wif (xi)γ (xi) = 0,
and thus (5.1.11) holds.
To prove the sufﬁciency, let p(x) be any polynomial of degree n + k −1. Let q(x)
and r(x) be the quotient and remainder, respectively, in the division
p(x) = q(x)γ (x) + r(x).
Then q(x) and r(x) are polynomials of degree k −1 and n −1, respectively. It holds that
 b
a
p(x)w(x) dx =
 b
a
q(x)γ (x)w(x) dx +
 b
a
r(x)w(x) dx,
where the ﬁrst integral on the right-hand side is zero because of the orthogonality property
of γ (x). For the second integral we have
 b
a
r(x)w(x) dx =
n

i=1
wir(xi),
since the weights were chosen such that the formula was interpolatory and therefore exact
for all polynomials of degree n −1. Further, since
p(xi) = q(xi)γ (xi) + r(xi) = r(xi),
i = 1 : n,
it follows that
 b
a
p(x)w(x) dx =
 b
a
r(x)w(x) dx =
n

i=1
wir(xi) =
n

i=1
wip(xi),
which shows that the quadrature rule is exact for p(x).
How to determine quadrature rules of optimal order will be the topic of Sec. 5.3.

5.1. Interpolatory Quadrature Rules
525
5.1.2
Treating Singularities
When the integrand or some of its low-order derivative is inﬁnite at some point in or near
the interval of integration, standard quadrature rules will not work well. It is not uncommon
that a single step taken close to such a singular point will give a larger error than all other
steps combined. In some cases a singularity can be completely missed by the quadrature
rule.
If the singular points are known, then the integral should ﬁrst be broken up into several
pieces so that all the singularities are located at one (or both) ends of the interval [a, b].
Many integrals can then be treated by weighted quadrature rules, i.e., the singularity is
incorporated into the weight function. Romberg’s method can be modiﬁed to treat integrals
where the integrand has an algebraic endpoint singularity; see Sec. 5.2.2.
It is often proﬁtable to investigate whether one can transform or modify the given
problem analytically to make it more suitable for numerical integration. Some difﬁculties
and possibilities in numerical integration are illustrated below in a series of simple examples.
Example 5.1.1.
In the integral
I =
 1
0
1
√x ex dx
the integrand is inﬁnite at the origin. By the substitution x = t2 we get
I = 2
 1
0
et2 dt,
which can be treated without difﬁculty.
Another possibility is to use integration by parts:
I =
 1
0
x−1/2ex dx = 2x1/2ex((1
0 −2
 1
0
x1/2ex dx
= 2e −22
3x3/2ex((1
0 + 4
3
 1
0
x3/2ex dx = 2
3e + 4
3
 1
0
x3/2ex dx.
The last integral has a mild singularity at the origin. If one wants high accuracy, then it is
advisable to integrate by parts a few more times before the numerical treatment.
Example 5.1.2.
Sometimes a simple comparison problem can be used. In
I =
 1
0.1
x−3ex dx
the integrand is inﬁnite near the left endpoint. If we write
I =
 1
0.1
x−3
1 + x + x2
2

dx +
 1
0.1
x−3
ex −1 −x −x2
2

dx,

526
Chapter 5. Numerical Integration
the ﬁrst integral can be computed analytically. The second integrand can be treated numeri-
cally. The integrand and its derivatives are of moderate size. Note, however, the cancellation
in the evaluation of the integrand.
For integrals over an inﬁnite interval one can try some substitution which maps the
interval (0, ∞) to (0, 1), for example, t = e−x of t = 1/(1 + x). But in such cases one
must be careful not to introduce an unpleasant singularity into the integrand instead.
Example 5.1.3.
More general integrals of the form
 2h
0
x−1/2f (x) dx
need a special treatment, due to the integrable singularity at x = 0. Aformula which is exact
for any second-degree polynomial f (x) can be found using the method of undetermined
coefﬁcients. We set
1
√
2h
 2h
0
x−1/2f (x) dx ≈w0f (0) + w1f (h) + w2f (2h),
and equate the left- and right-hand sides for f (x) = 1, x, x2. This gives
w0 + w1 + w2 = 2,
1
2w1 + w2 = 2
3,
1
4w1 + w2 = 2
5.
This linear system is easily solved, giving w0 = 12/15, w1 = 16/15, w2 = 2/15.
Example 5.1.4.
Consider the integral
I =
 ∞
0
(1 + x2)−4/3 dx.
If one wants ﬁve decimal digits in the result, then
 ∞
R is not negligible until R ≈103. But
one can expand the integrand in powers of x−1 and integrate termwise:
 ∞
R
(1 + x2)−4/3 dx =
 ∞
R
x−8/3(1 + x−2)−4/3 dx
=
 ∞
R

x−8/3 −4
3x−14/3 + 14
9 x−20/3 −· · ·

= R−5/33
5 −4
11R−2 + 14
51R−4 −· · ·

.
If this expansion is used, then one need only apply numerical integration to the interval
[0, 8].
With the substitution t = 1/(1 + x) the integral becomes
I =
 1
0
(t2 + (1 −t)2)−4/3t2/3 dt.

5.1. Interpolatory Quadrature Rules
527
The integrand now has an inﬁnite derivative at the origin. This can be eliminated by making
the substitution t = u3 to get
I =
 1
0
(u6 + (1 −u3)2)−4/33u4 du,
which can be computed with, for example, a Newton–Cotes’ method.
5.1.3
Some Classical Formulas
Interpolatory quadrature formulas, where the nodes are constrained to be equally spaced, are
called Newton–Cotes167 formulas. These are especially suited for integrating a tabulated
function, a task that was more common before the computer age. The midpoint, trapezoidal,
and Simpson’s rules, to be described here, are all special cases of (unweighted) Newton–
Cotes’ formulas.
The trapezoidal rule (cf. Figure 1.1.5) is based on linear interpolation of f (x) at
x1 = a and x2 = b; i.e., f (x) is approximated by
p(x) = f (a) + (x −a)[a, b]f = f (a) + (x −a)f (b) −f (a)
b −a
.
The integral of p(x) equals the area of a trapezoid with base (b −a) times the average
height 1
2(f (a) + f (b)). Hence
 b
a
f (x) dx ≈(b −a)
2
(f (a) + f (b)).
To increase the accuracy we subdivide the interval [a, b] and assume that fi = f (xi)
is known on a grid of equidistant points
x0 = a,
xi = x0 + ih,
xn = b,
(5.1.12)
where h = (b −a)/n is the step length. The trapezoidal approximation for the ith subin-
terval is
 xi+1
xi
f (x) dx = T (h) + Ri,
T (h) = h
2(fi + fi+1).
(5.1.13)
Assuming that f ′′(x) is continuous in [a, b] and using the exact remainder in Newton’s
interpolation formula (see Theorem 4.2.1) we get
Ri =
 xi+1
xi
(f (x) −p2(x)) dx =
 xi+1
xi
(x −xi)(x −xi+1) [xi, xi+1, x]f dx.
(5.1.14)
Since [xi, xi+1, x]f is a continuous function of x and (x −xi)(x −xi+1) has constant
(negative) sign for x ∈[xi, xi+1], the mean value theorem of integral calculus gives
Ri = [xi, xi+1, ξi]f
 xi+1
xi
(x −xi)(x −xi+1) dx,
ξi ∈[xi, xi+1].
167Roger Cotes (1682–1716) was a highly appreciated young colleague of Isaac Newton. He was entrusted with
the preparation of the second edition of Newton’s Principia. He worked out and published the coefﬁcients for
Newton’s formulas for numerical integration for n ≤11.

528
Chapter 5. Numerical Integration
Setting x = xi + ht and using the Theorem 4.2.3, we get
Ri = −1
2f ′′(ζi)
 1
0
h2t(t −1)h dt = −1
12h3f ′′(ζi),
ζi ∈[xi, xi+1].
(5.1.15)
For another proof of this result using the Peano kernel see Example 3.3.16.
Summing the contributions for each subinterval [xi, xi+1], i = 0 : n, gives
 b
a
f (x) dx = T (h) + RT ,
T (h) = h
2(f0 + fn) + h
n−1

i=2
fi,
(5.1.16)
which is the composite trapezoidal rule. The global truncation error is
RT = −h3
12
n−1

i=0
f ′′(ζi) = −1
12(b −a)h2f ′′(ξ),
ξ ∈[a, b].
(5.1.17)
(The last equality follows since f ′′ was assumed to be continuous on the interval [a, b].)
This shows that by choosing h small enough we can make the truncation error arbitrarily
small. In other words, we have asymptotic convergence when h →0.
In the midpoint rule f (x) is approximated on [xi, xi+1] by its value
fi+1/2 = f (xi+1/2),
xi+1/2 = 1
2(xi + xi+1),
at the midpoint of the interval. This leads to the approximation
 xi+1
xi
f (x) dx = M(h) + Ri,
M(h) = hfi+1/2.
(5.1.18)
The midpoint rule approximation can be interpreted as the area of the trapezium deﬁned by
the tangent of f at the midpoint xi+1/2.
The remainder term in Taylor’s formula gives
f (x) −(fi+1/2 + (x −xi+1/2)f ′
i+1/2) = 1
2(x −xi+1/2)2f ′′(ζx),
ζx ∈[xi, xi+1/2].
By symmetry the integral over [xi, xi+1] of the linear term vanishes. We can use the mean
value theorem to show that
Ri =
 xi+1
xi
1
2f ′′(ζx)(x −xi+1/2)2 dx = 1
2f ′′(ζi)
 1/2
−1/2
h3t2 dt = h3
24f ′′(ζi).
Although it uses just one function value, the midpoint rule, like the trapezoidal rule, is exact
when f (x) is a linear function. Summing the contributions for each subinterval, we obtain
the composite midpoint rule:
 b
a
f (x) dx = M(h) + RM,
M(h) = h
n−1

i=0
fi+1/2.
(5.1.19)

5.1. Interpolatory Quadrature Rules
529
(Compare the above approximation with the Riemann sum in the deﬁnition of a deﬁnite
integral.) For the global error we have
RM = (b −a)h2
24
f ′′(ζ),
ζ ∈[a, b].
(5.1.20)
The trapezoidal rule is called a closed rule because values of f at both endpoints are
used. It is not uncommon that f has an integrable singularity at an endpoint. In that case
an open rule, like the midpoint rule, can still be applied.
If f ′′(x) has constant sign in each subinterval, then the error in the midpoint rule
is approximately half as large as that for the trapezoidal rule and has the opposite sign.
But the trapezoidal rule is more economical to use when a sequence of approximations
for h, h/2, h/4, . . . is to be computed, since about half of the values needed for h/2 were
already computed and used for h. Indeed, it is easy to verify the following useful relation
between the trapezoidal and midpoint rules:
T
h
2

= 1
2(T (h) + M(h)).
(5.1.21)
If the magnitude of the error in the function values does not exceed 1
2U, then the
magnitude of the propagated error in the approximation for the trapezoidal and midpoint
rules is bounded by
RA = 1
2(b −a)U,
(5.1.22)
independent of h. By (5.1.9) this holds for any quadrature formula of the form (5.1.3),
provided that all weights wi are positive.
If the roundoff error is negligible and h sufﬁciently small, then it follows from (5.1.17)
that the error in T (h/2) is about one-quarter of that in T (h). Hence the magnitude of the
error in T (h/2) can be estimated by (1/3)|T (h/2) −T (h)|, or more conservatively by
|T (h/2)−T (h)|. (Amore systematic use of Richardson extrapolation is made in Romberg’s
method; see Sec. 5.2.2.)
Example 5.1.5.
Use (5.1.21) to compute the sine integral Si(x) =
 x
0
sin t
t
dt for x = 0.8. Midpoint
and trapezoidal sums (correct to eight decimals) are given below.
h
M(h)
T (h)
0.8
0.77883 668
0.75867 805
0.4
0.77376 698
0.76875 736
0.2
0.77251 272
0.77126 217
0.1
0.77188 744
The correct value, to ten decimals, is 0.77209 57855 (see [1, Table 5.2]). We verify that in
this example the error is approximately proportional to h2 for both M(h) and T (h), and we
estimate the error in T (0.1) to be 1
36.26 · 10−4 ≤2.1 · 10−4.

530
Chapter 5. Numerical Integration
From the error analysis above we note that the error in the midpoint rule is roughly
half the size of the error in the trapezoidal rule and of opposite sign. Hence it seems that
the linear combination
S(h) = 1
3(T (h) + 2M(h))
(5.1.23)
should be a better approximation. This is indeed the case and (5.1.23) is equivalent to
Simpson’s rule.168
Another way to derive Simpson’s rule is to approximate f (x) by a piecewise polyno-
mial of third degree. It is convenient to shift the origin to the midpoint of the interval and
consider the integral over the interval [xi −h, xi + h]. From Taylor’s formula we have
f (x) = fi + (x −xi)f ′
i + (x −xi)2
2
f ′′
i + (x −xi)3
3!
f ′′′
i + O(h4),
where the remainder is zero for all polynomials of degree three or less. Integrating term by
term, the integrals of the second and fourth term vanish by symmetry, giving
 xi+h
xi−h
f (x) dx = 2hfi + 0 + 1
3h3f ′′
i + 0 + O(h5).
Using the difference approximation h2f ′′
i = (fi−1 −2fi + fi+1) + O(h4) (see (4.7.5)) we
obtain
 xi+h
xi−h
f (x) dx = 2hfi + 1
3h(fi−1 −2fi + fi+1) + O(h5)
(5.1.24)
= 1
3h(fi−1 + 4fi + fi+1) + O(h5),
where the remainder term is zero for all third-degree polynomials. We now determine the
error term for f (x) = (x −xi)4, which is
RT = 1
3h(h4 + 0 + h4) −
 xi+h
xi−h
x4 dx =
2
3 −2
5

h5 = 4
15h5.
It follows that an asymptotic error estimate for Simpson’s rule is
RT = h5 4
15
f (4)(xi)
4!
+ O(h6) = h5
90f (4)(xi) + O(h6).
(5.1.25)
A strict error estimate for Simpson’s rule is more difﬁcult to obtain. As for the
midpoint formula, the midpoint xi can be considered as a double point of interpolation; see
Problem 5.1.3. The general error formula (5.1.8) then gives
Ri(f ) = 1
4!
 xi+1
xi−1
f (4)(ξx)(x −xi−1)(x −xi)2(x −xi+1) dx,
168Thomas Simpson (1710–1761), English mathematician best remembered for his work on interpolation and
numerical methods of integration. He taught mathematics privately in the London coffee houses and from 1737
began to write texts on mathematics.

5.1. Interpolatory Quadrature Rules
531
where (x −xi−1)(x −xi)2(x −xi+1) has constant negative sign on [xi−1, xi+1]. Using the
mean value theorem gives the error
RT (f ) = 1
90f (4)(ξ)h5,
ξ ∈[xi −h, xi + h].
(5.1.26)
The remainder can also be obtained from Peano’s error representation. It can be shown
(see [338, p. 152 ff]) that for Simpson’s rule
Rf =

R
f (4)(u)K(u) du,
where the kernel equals
K(u) = −1
72(h −u)3(3u + h)2,
0 ≤u ≤h,
and K(u) = K(|u|) for u < 0, K(u) = 0 for |u| > h. This again gives (5.1.26).
In the composite Simpson’s rule one divides the interval [a, b] into an even number
n = 2m steps of length h and uses the formula (5.1.24) on each of m double steps, giving
 b
a
f (x) dx = h
3(f0 + 4S1 + 2S2 + fn) + RT ,
(5.1.27)
where
S1 = f1 + f3 + · · · + fn−1,
S2 = f2 + f4 + · · · + fn−2
are sums over odd and even indices, respectively. The remainder is
RT (f ) =
m−1

i=0
h5
90f (4)(ξi) = (b −a)
180
h4f (4)(ξ),
ξ ∈[a, b].
(5.1.28)
This shows that we have gained two orders of accuracy compared to the trapezoidal rule,
without using more function evaluations. This is why Simpson’s rule is such a popular
general-purpose quadrature rule.
5.1.4
Superconvergence of the Trapezoidal Rule
In general the composite trapezoidal rule integrates exactly polynomials of degree 1 only.
It does much better with trigonometric polynomials.
Theorem 5.1.4.
Consider the integral
 2π
0
tm(x) dx, where
tm(x) = a0 + a1 cos x + a2 cos 2x + · · · + am cos mx
+ b1 sin x + b2 sin 2x + · · · + bm sin mx
is any trigonometric polynomial of degree m. Then the composite trapezoidal rule with step
length h = 2π/n, n ≥m + 1, integrates this exactly.

532
Chapter 5. Numerical Integration
Proof. See Problem 5.1.16.
Suppose that the function f is inﬁnitely differentiable for x ∈R, and that f has [a, b]
as an interval of periodicity, i.e., f (x + (b −a)) = f (x) for all x ∈R. Then
f (k)(b) = f (k)(a),
k = 0, 1, 2, . . . ,
hence every term in the Euler–Maclaurin expansion is zero for the integral over the whole
period [a, b]. One could be led to believe that the trapezoidal rule gives the exact value of the
integral, but this is usually not the case. For most periodic functions f , limr→∞R2r+2f ̸= 0;
the expansion converges, of course, though not necessarily to the correct result.
On the other hand, the convergence as h →0 for a ﬁxed (though arbitrary) r is a
different story; the error bound (5.2.10) shows that
|R2r+2(a, h, b)f | = O(h2r+2).
Since r is arbitrary, this means that for this class of functions, the trapezoidal error tends
to zero faster than any power of h, as h →0 . We may call this superconvergence. The
application of the trapezoidal rule to an integral over [0, ∞) of a function f ∈C∞(0, ∞)
often yields similar features, sometimes even more striking.
Suppose that the periodic function f (z), z = x + iy, is analytic in a strip, |y| < c,
around the real axis. It can then be shown that the error of the trapezoidal rule is
O(e−η/h),
h ↓0,
where η is related to the width of the strip. Asimilar result (3.2.19) was obtained in Sec. 3.2.2,
for an annulus instead of a strip. There the trapezoidal rule was used in the calculation of
the integral of a periodic analytic function over a full period [0, 2π] that deﬁned its Taylor
coefﬁcients. The error was shown to tend to zero faster than any power of the step length 4θ.
As a rule, this discussion does not apply to periodic functions which are deﬁned
by periodic continuation of a function originally deﬁned on [a, b] (such as the Bernoulli
functions). They usually become nonanalytic at a and b, and at all points a + (b −a)n,
n = 0, ±1, ±2, . . . .
The Poisson summation formula is even better than the Euler–Maclaurin formula
for the quantitative study of the trapezoidal truncation error on an inﬁnite interval. For
convenient reference we now formulate the following surprising result.
Theorem 5.1.5.
Suppose that the trapezoidal rule (or, equivalently, the rectangle rule) is applied with
constant step size h to
 ∞
−∞f (x) dx. The Fourier transform of f reads
ˆf (ω) =
 ∞
−∞
f (x)e−iωt dt.
Then the integration error decreases like 2 ˆf (2π/h) as h ↓0.
Example 5.1.6.
For the normal probability density, we have
f (x) =
1
σ
√
2π
e−1
2( t
σ )2,
ˆf (ω) = e−1
2 (ωσ)2.

5.1. Interpolatory Quadrature Rules
533
The integration error is thus approximately 2 exp(−2(πσ/h)2). Roughly speaking, the
number of correct digits is doubled if h is divided by
√
2; for example, the error is approx-
imately 5.4 · 10−9 for h = σ, and 1.4 · 10−17 for h = σ/
√
2.
The application of the trapezoidal rule to an integral over [0, ∞) of a function f ∈
C∞(0, ∞) often yields similar features, sometimes even more striking. Suppose that, for
k = 1, 2, 3, . . . ,
f (2k−1)(0) = 0 and f (2k−1)(x) →0,
x →∞,
and
 ∞
0
|f (2k)(x)| dx < ∞.
(Note that for any function g ∈C∞(−∞, ∞) the function f (x) = g(x) + g(−x) satisﬁes
such conditions at the origin.) Then all terms of the Euler–Maclaurin expansion are zero,
and one can be misled to believe that the trapezoidal sum gives
 ∞
0 f (x) dx exactly for any
step size h! The explanation is that the remainder R2r+2(a, h, ∞) will typically not tend to
zero, as r →∞for ﬁxed h. On the other hand, if we consider the behavior of the truncation
error as h →0 for given r, we ﬁnd that it is o(h2r) for any r, just like the case of a periodic
function.
For a ﬁnite subinterval of [0, ∞), however, the remainder is still typically O(h2), and
for each step the remainder is typically O(h3). So, there is an enormous cancellation of the
local truncation errors, when a C∞-function with vanishing odd-order derivatives at the
origin is integrated by the trapezoidal rule over [0, ∞).
Example 5.1.7.
For integrals of the form
 ∞
−∞f (x) dx, the trapezoidal rule (or the midpoint rule)
often gives good accuracy if one integrates over the interval [−R1, R2], assuming that f (x)
and its lower derivatives are small for x ≤−R1 and x ≥R2.
The correct value to six decimal digits of the integral
 ∞
−∞e−x2 dx is π1/2 = 1.772454.
For x ± 4, the integrand is less than 0.5 · 10−6. Using the trapezoidal rule with h = 1/2
for the integral over [−4, 4] we get the estimate 1.772453, an amazingly good result. (The
function values have been taken from a six-place table.) The truncation error in the value
of the integral is here less than 1/10,000 of the truncation error in the largest term of
the trapezoidal sum—a superb example of “cancellation of truncation error.” The error
committed when we replace ∞by 4 can be estimated in the following way:
|R| = 2
 ∞
4
e−x2 dx =
 ∞
16
e−t 1
√t dt <
 ∞
16
e−t
1
√
16
dt = 1
4e−16 < 10−7.
5.1.5
Higher-Order Newton–Cotes’ Formulas
The classical Newton–Cotes’quadrature rules are interpolatory rules obtained for w(x) = 1
and equidistant points in [0, 1]. There are two classes: closed formulas, where the endpoints
of the interval belong to the nodes, and open formulas, where all nodes lie strictly in the
interior of the interval (cf. the trapezoidal and midpoint rules).

534
Chapter 5. Numerical Integration
The closed Newton–Cotes’ formulas are usually written as
 nh
0
f (x) dx = h
n

j=0
wjf (jh) + Rn(f ).
(5.1.29)
The weights satisfy wj = wn−j and can, in principle, be determined from (5.1.7). Further,
by (5.1.6) it holds that
n

j=0
hwj = nh.
(5.1.30)
(Note that here we sum over n + 1 points in contrast to our previous notation.)
It can be shown that the closed Newton–Cotes’formulas have order of accuracy d = n
for n odd and d = n + 1 for n even. The extra accuracy for n even is, as in Simpson’s
rule, due to symmetry. For n ≤7 all coefﬁcients wi are positive, but for n = 8 and n ≥10
negative coefﬁcients appear. Such formulas may still be useful, but since n
j=0 h|wj| > nh,
they are less robust with respect to errors in the function values fi.
The closed Newton–Cotes’ formulas for n = 1 and n = 2 are equivalent to the
trapezoidal rule and Simpson’s rule, respectively. The formula for n = 3 is called the 3/8th
rule, for n = 4 Milne’s rule, and for n = 6 Weddle’s rule. The weights wi = Aci and error
coefﬁcient cn,d of Newton–Cotes’ closed formulas are given for n ≤6 in Table 5.1.1.
Table 5.1.1. The coefﬁcients wi = Aci in the n-point closed Newton–Cotes’formulas.
n
d
A
c0
c1
c2
c3
c4
c5
c6
cn,d
1
1
1/2
1
1
−1/12
2
3
1/3
1
4
1
−1/90
3
3
3/8
1
3
3
1
−3/80
4
5
2/45
7
32
12
32
7
−8/945
5
5
5/288
19
75
50
50
75
19
−275/12,096
6
7
1/140
41
236
27
272
27
236
41
−9/1400
The open Newton–Cotes’ formulas are usually written as
 nh
0
f (x) dx = h
n−1

i=1
wif (ih) + Rn−1,n(h)
and use n −1 nodes. The weights satisfy w−j = wn−j. The simplest open Newton–Cotes’
formula for n = 2 is the midpoint rule with step size 2h. The open formulas have order of
accuracy d = n −1 for n even and d = n −2 for n odd. For the open formulas negative
coefﬁcients occur already for n = 4 and n = 6.
The weights and error coefﬁcients of open formulas for n ≤5 are given in Table 5.1.2.
We recognize the midpoint rule for n = 2. Note that the sign of the error coefﬁcients in the
open rules are opposite the sign in the closed rules.

5.1. Interpolatory Quadrature Rules
535
Table 5.1.2. The coefﬁcients wi = Aci in the n-point open Newton–Cotes’formulas.
n
d
A
c1
c2
c3
c4
c5
cn,d
2
1
2
1
1/24
3
1
3/2
1
1
1/4
4
3
4/3
2
−1
2
14/45
5
3
5/24
11
1
1
11
95/144
6
5
3/10
11
−14
26
−14
11
41/140
7
5
7/1440
611
−453
562
562
−453
611
5257/8640
The Peano kernels for both the open and the closed formulas can be shown to have
constant sign (Steffensen [330]). Thus the local truncation error can be written as
Rn(h) = cn,dhd+1f (d)(ζ),
ζ ∈[0, nh].
(5.1.31)
It is easily shown that the Peano kernels for the corresponding composite formulas also
have constant sign.
Higher-order Newton–Cotes’ formulas can be found in [1, pp. 886–887]. We now
show how they can be derived using the operator methods developed in Sec. 3.3.4. Let m,
n be given integers and let h be a positive step size. In order to utilize the symmetry of the
problem more easily, we move the origin to the midpoint of the interval of integration. If
we set
xj = jh,
fj = f (jh),
j = −n/2 : 1 : n/2,
the Newton–Cotes’ formula now reads
 mh/2
−mh/2
f (x) dx = h
n/2

j=−n/2
wjfj + Rm,n(h),
w−j = wj.
(5.1.32)
Note that j, n/2, and m/2 are not necessarily integers. For a Newton–Cotes’ formula,
n/2 −j and m/2 −j are evidently integers and hence (m −n)/2 is an integer too. There
may, however, be other formulas, perhaps almost as good, where this is not the case. The
coefﬁcients wj = wj;m,n are to be determined so that the remainder Rm,n vanishes if f ∈Pq,
with q as large as possible for given m, n.
The left-hand side of (5.1.32), divided by h, reads in operator form
(emhD/2 −e−mhD/2)(hD)−1f (x0),
which is an even function of hD. By (3.3.42), hD is an odd function of δ. It follows that
the left-hand side is an even function of δ; hence we can, for every m, write
(ehDm/2 −e−hDm/2)(hD)−1
9→Am(δ2) = a1m + a2mδ2 + · · · + ak+1,mδ2k · · · . (5.1.33)

536
Chapter 5. Numerical Integration
We truncate after (say) δ2k; the ﬁrst neglected term is then ak+2,mδ2k+2. We saw in Sec. 3.3.4
how to bring a truncated δ2-expansion to B(E)-form,
b1 + b2(E + E−1) + b3(E2 + E−2) + · · · + bk(Ek + E−k),
by matrix multiplication with a matrix M of the form given in (3.3.49). By comparison
with (5.1.32), we conclude that n/2 = k, that the indices j are integers, and that wj = bj+1
(if j ≥0). If m is even, this becomes a Newton–Cotes’ formula. If m is odd, it may
still be a useful formula, but it does not belong to the Newton–Cotes’ family, because
(m −n)/2 = m/2 −k is no integer.
If n = m, a formula is of the closed type. Its remainder term is the ﬁrst neglected
term of the operator series, truncated after δ2k, 2k = n = m (and multiplied by h). Hence
the remainder of (5.1.32) can be estimated by a2+m/2δm+2f0 or (better)
Rm,m ∼(am/2+2/m)H(hD)m+2f0,
where we call H = mh the “bigstep.”
If the integral is computed over [a, b] by means of a sequence of bigsteps, each of
length H, an estimate of the global error has the same form, except that H is replaced by
b −a and f0 is replaced by maxx∈[a,b] |f (x)|. The exponent of hD in an error estimate that
contains H or b −a is known as the global order of accuracy of the method.
If n < m, a formula of the open type is obtained. Among the open formulas we shall
only consider the case that n = m −2, which gives the open Newton–Cotes’formulas. The
operator expansion is truncated after δm−2, and we obtain
Rm−2,m ∼(am/2+1/m)H(hD)mf0.
Formulas with n > m are rarely mentioned in the literature (except for m = 1). We
do not understand why; it is rather common that an integrand has a smooth continuation
outside the interval of integration.
We next consider the effect of a linear transformation of the independent variable.
Suppose that a formula
N

j=1
ajf (tj) −
 1
0
f (t) dt ≈cNf (N)
has been derived for the standard interval [0, 1]. Setting x = xk + th, dx = hdt we ﬁnd
that the corresponding formula and error constant for the interval [xk, xk + h] reads
N

j=1
ajg(xk + htj) −
 xk+h
xk
g(x) dx ≈cNhN+1g(N)(xk).
(5.1.34)
This error estimate is valid asymptotically as h →0. The local order of accuracy, i.e.,
over one step of length h, is N +1; the global order of accuracy, i.e., over (b −a)/h steps
of length h, becomes N.
For example, the trapezoidal rule is exact for polynomials of degree 1 and hence
N = 2. For the interval [0, 1], L(t2) = 1
3, ˜L(t2) = 1
2, and thus c2 = 1
2( 1
2 −1
3) =
1
12. On

5.1. Interpolatory Quadrature Rules
537
an interval of length h the asymptotic error becomes h3g′′/12. The local order of accuracy
is N + 1 = 3; the global order of accuracy is N = 2.
If the “standard interval” is [−1, 1] instead, the transformation becomes x = 1
2ht,
and h is to be replaced by 1
2h everywhere in (5.1.34). Be careful about the exact meaning
of a remainder term for a formula of this type provided by a table.
We shall illustrate the use of the Cauchy–FFT method for computing the coefﬁcients
aim in the expansion (5.1.33). In this way extensive algebraic calculations are avoided.169
It can be shown that the exact coefﬁcients are rational numbers, though it is sometimes hard
to estimate in advance the order of magnitude of the denominators. The algorithm must be
used with judgment. Figure 5.1.1 was obtained for N = 32, r = 2; the absolute errors of the
coefﬁcients are then less than 10−13. The smoothness of the curves for j ≥14 indicates that
the relative accuracy of the values of am,j are still good there; in fact, other computations
show that it is still good when the coefﬁcients are as small as 10−20.
0
2
4
6
8
10
12
14
16
18
20
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
amj, for m=2:2:14,  j=0:20
m=2:2:14
j
m=2
8
14
Figure 5.1.1. The coefﬁcients |am,j| of the δ2-expansion for m = 2 : 2 : 14,
j = 0 : 20. The circles are the coefﬁcients for the closed Cotes’formulas, i.e., j = 1+m/2.
The coefﬁcients are ﬁrst obtained in ﬂoating-point representation. The transformation
to rational form is obtained by a continued fraction algorithm, described in Example 3.5.3.
For the case m = 8 the result reads
A8(δ2) = 8 + 64
3 δ2 + 688
45 δ4 + 736
189δ6 + 3956
14,175δ8 −
2368
467,775δ10 + · · · .
(5.1.35)
169These could, however, be carried out using a system such as Maple.

538
Chapter 5. Numerical Integration
The closed integration formula becomes
 x4
−x4
f (x)dx =
4h
14,175

−4540f0 + 10,496(f1 + f−1) −928(f2 + f−2)
+ 5888(f3 + f−3) + 989(f4 + f−4)

+ R,
(5.1.36)
R ∼
296
467,775Hh10f (10)(x0).
(5.1.37)
It goes without saying that this is not how Newton and Cotes found their methods. Our
method may seem complicated, but the MATLAB programs for this are rather short, and to
a large extent useful for other purposes. The computation of about 150 Cotes coefﬁcients
and 25 remainders (m = 2 : 14) took less than two seconds on a PC. This includes the
calculation of several alternatives for rational approximations to the ﬂoating-point results.
For a small number of the 150 coefﬁcients the judicious choice among the alternatives took,
however, much more than two (human) seconds; this detail is both science and art.
It was mentioned that if m is odd, (5.1.33) does not provide formulas of the Newton–
Cotes’ family, since (m −n)/2 is no integer, nor are the indices j in (5.1.32) integers.
Therefore, the operator associated with the right-hand side of (5.1.32) is of the form
c1(E1/2 + E−1/2) + c2(E3/2 + E−3/2) + c3(E5/2 + E−5/2) + · · · .
If it is divided algebraically by µ = 1
2(E1/2 + E−1/2), however, it becomes the B(E)-form
(say)
b′
1 + b′
2(E + E−1) + b′
3(E2 + E−2) + · · · + bk(Ek + E−k).
If m is odd, we therefore expand
(ehDm/2 −e−hDm/2)(hD)−1/µ,
µ =

1 + δ2/4,
intoaδ2-series, withcoefﬁcientsa′
j. Again, thiscanbedonenumericallybytheCauchy–FFT
method. Foreachm, twotruncatedδ2-series(onefortheclosedandonefortheopencase)are
then transformed into B(E)-expressions numerically by means of the matrix M, as described
above. The expressions are then multiplied algebraically by µ = ( 1
2)(E1/2 + E−1/2). We
then have the coefﬁcients of a Newton–Cotes’ formula with m odd.
The asymptotic error is
a′
m/2+1H(hD)m+1
and
a′
m/2−1H(hD)m−1
for the closed type and open type, respectively (2k = m−1). The global orders of accuracy
for Newton–Cotes’ methods with odd m are thus the same as for the methods where m is
one less.
5.1.6
Fejér and Clenshaw–Curtis Rules
Equally spaced interpolation points as used in the Newton–Cotes’ formulas are useful for
low degrees but can diverge as fast as 2n as n →∞. Quadrature rules which use a set of
points which cluster near the endpoints of the interval have better properties for large n.

5.1. Interpolatory Quadrature Rules
539
Fejér [117] suggested using the zeros of a Chebyshev polynomial of ﬁrst or second
kind as interpolation points for quadrature rules of the form
 1
−1
f (x) dx =
n

k=0
wkf (xk).
(5.1.38)
Fejér’s ﬁrst rule uses the zeros of the Chebyshev polynomial Tn(x) = cos(n arccos x)
of the ﬁrst kind in (−1, 1), which are
xk = cos θk,
θk = (2k −1)π
2n
,
k = 1 : n.
(5.1.39)
The following explicit formula for the weights is known (see [93]):
wf 1
k
= 2
n

1 −2
⌊n/2⌋

j=1
cos(2jθk)
4j 2 −1

,
k = 1 : n.
(5.1.40)
Fejér’s second rule uses the zeros of the Chebyshev polynomial Un−1(x) of the second
kind, which are the extreme points of Tn(x) in (−1, 1) (see Sec. 3.2.3):
xk = cos θk,
θk = kπ
n ,
k = 1 : n −1.
(5.1.41)
An explicit formula for the weights is (see [93])
wf 2
k
= 4 sin θk
n
⌊n/2⌋

j=1
sin(2j −1)θk
2j −1
,
k = 1 : n −1.
(5.1.42)
Both Fejér’s rules are open quadrature rules, i.e., they do not use the endpoints of the interval
[−1, 1]. Fejér’s second rule is the more practical, because going from n+1 to 2n+1 points,
only n new function values need to be evaluated; cf. the trapezoidal rule.
The quadrature rule of Clenshaw and Curtis [72] is a closed version of Fejér’s second
rule; i.e., the nodes are the n+1 extreme points of Tn(x), in [−1, 1], including the endpoints
x0 = 1, xn = −1. An explicit formula for the Clenshaw–Curtis weights is
wcc
k = ck
n

1 −
⌊n/2⌋

j=1
bj
4j 2 −1 cos(2jθk)

,
k = 0 : n,
(5.1.43)
where
bj =
%
1
if j = n/2,
2
if j < n/2,
ck =
% 1
if k = 0, n,
2
otherwise.
(5.1.44)
In particular the weights at the two boundary points are
wcc
0 = wcc
n =
1
n2 −1 + mod (n, 2).
(5.1.45)
For both the Fejér and Clenshaw–Curtis rules the weights can be shown to be positive;
see Imhof [207]. Therefore the convergence of In(f ) as n →∞for all f ∈C[−1, 1] is

540
Chapter 5. Numerical Integration
assured for these rules by the following theorem, which is a consequence of Weierstrass’
theorem.
Theorem 5.1.6.
Let xnj and anj, where j = 1 : n, n = 1, 2, 3, . . . , be triangular arrays of nodes and
weights, respectively, and suppose that anj > 0 for all n, j ≥1. Consider the sequence of
quadrature rules
Inf =
n

j=1
anjf (xnj)
for the integral If =
 b
a f (x)w(x) dx, where [a, b] is a closed, bounded interval, and w(x)
is an integrable weight function. Suppose that Inp = Ip for all p ∈Pkn, where {kn}∞
n=1 is
a strictly increasing sequence. Then
Inf →If
∀f ∈C[a, b].
Note that this theorem is not applicable to Cotes’ formulas, where some weights are
negative.
Convergence will be fast for the Fejér and Clenshaw–Curtis rules provided the in-
tegrand is k times continuously differentiable—a property that the user can often check.
However, if the integrand is discontinuous, the interval of integration should be partitioned
at the discontinuities and the subintervals treated separately.
Despite its advantages these quadrature rules did not receive much use early on,
because computing the weights using the explicit formulas given above is costly (O(n2)
ﬂops) and not numerically stable for large values of n. However, it is not necessary to
compute the weights explicitly. Gentleman [154, 153] showed how the Clenshaw–Curtis
rule can be implemented by means of a discrete cosine transform (DCT; see Sec. 4.7.4). We
recall that the FFT is not only fast, but also very resistant to roundoff errors.
The interpolation polynomial Ln(x) can be represented in terms of Chebyshev poly-
nomials
Ln(x) =
n
′′
k=0
ckTk(x),
ck = 2
n
n
′′
j=0
f (xj) cos
kjπ
n

,
where xj = cos(jπ/n). This is the real part of an FFT. (The double prime on the sum
means that the ﬁrst and last terms are to be halved.) Then we have
In(f ) =
 1
−1
Ln(x) dx =
n
′′
k=0
ckµk,
µk =
 1
−1
Tk(x) dx,
where µk are the moments of the Chebyshev polynomials. It can be shown (Problem 5.1.7)
that
µk =
 1
−1
Tk(x) dx =
%
0
if k odd,
2/(1 −k2)
if k even.
The following MATLAB program, due to Trefethen [359], is a compact implementation of
this version of the Clenshaw–Curtis quadrature.

5.1. Interpolatory Quadrature Rules
541
Algorithm 5.1. Clenshaw–Curtis Quadrature.
function I = clenshaw_curtis(f,n);
% Computes the integral I of f over [-1,1] by the
% Clenshaw-Curtis quadrature rule with n+1 nodes.
x =
cos(pi*(0:n)’/n);
%Chebyshev extreme points
fx = feval(f,x)/(2*n);
%Fast Fourier transform
g = real(fft(fx([1:n+1 n:-1:2])));
%Chebyshev coefficients
a = [g(1); g(2:n)+g(2*n:-1:n+2); g(n+1)];
w = 0*a’; w(1:2:end) = 2./(1-(0:2:n).ˆ2);
I = w*a;
A fast and accurate algorithm for computing the weights in the Fejér and Clenshaw–
CurtisrulesinO(n log n)ﬂopshasbeengivenbyWaldvogel[368]. Theweightsareobtained
as the inverse FFT of certain vectors given by explicit rational expressions. On an average
laptop this takes just about ﬁve seconds for n = 220 + 1 = 1,048,577 nodes!
For Fejér’s second rule the weights are the inverse discrete FFT of the vector v with
components vk given by the expressions
vk =
2
1 −4k2 ,
k = 0 : ⌊n/2⌋−1,
v⌊n/2⌋=
n −3
2⌊n/2⌋−1 −1,
(5.1.46)
vn−k = vk,
k = 1 : ⌊(n −1)/2⌋.
(Note that this will give zero weights for k = 0, n corresponding to the endpoint nodes
x0 = −1 and xn = 1.)
For the Clenshaw–Curtis rule the weights are the inverse FFT of the vector v + g,
where
gk = −wcc
0 ,
k = 0 : ⌊n/2⌋−1,
g⌊n/2⌋= w0 [(2 −mod (n, 2)) n −1] ,
(5.1.47)
gn−k = gk,
k = 1 : ⌊(n −1)/2⌋,
and wcc
0 is given by (5.1.45). For the weights Fejér’s ﬁrst rule and MATLAB ﬁles imple-
menting the algorithm, we refer to [368].
Since the complexity of the inverse FFT is O(n log n), this approach allows fast and
accurate calculation of the weights for rules of high order, in particular when n is a power
of 2. For example, using the MATLAB routine IFFT the weights for n = 1024 only takes
a few milliseconds on a PC.

542
Chapter 5. Numerical Integration
Review Questions
5.1.1 Name three classical quadrature methods and give their order of accuracy.
5.1.2 What is meant by a composite quadrature rule? What is the difference between local
and global error?
5.1.3 What is the advantage of including a weight function w(x) > 0 in some quadrature
rules?
5.1.4 Describe some possibilities for treating integrals where the integrand has a singularity
or is “almost singular.”
5.1.5 For some classes of functions the composite trapezoidal rule exhibits so-called super-
convergence. What is meant by this term? Give an example of a class of functions
for which this is true.
5.1.6 Give an account of the theoretical background of the classical Newton–Cotes’ rules.
Problems and Computer Exercises
5.1.1 (a) Derive the closed Newton–Cotes’ rule for m = 3,
I = 3h
8 (f0 + 3f1 + 3f2 + f3) + RT ,
h = (b −a)
3
,
also known as Simpson’s (3/8)-rule.
(b) Derive the open Newton–Cotes’ rule for m = 4,
I = 4h
3 (2f1 −f2 + 2f3) + RT ,
h = (b −a)
4
.
(c) Find asymptotic error estimates for the formulas in (a) and (b) by applying them
to suitable polynomials.
5.1.2 (a) Show that Simpson’s rule is the unique quadrature formula of the form
 h
−h
f (x) dx ≈h(a−1f (−h) + a0f (0) + a1f (h))
that is exact whenever f ∈P4. Try to ﬁnd several derivations of Simpson’s rule,
with or without the use of difference operators.
(b) Find the Peano kernel K2(u) such that Rf =

R f ′′(u)K2(u) du, and ﬁnd the
best constants c, p such that
|Rf | ≤chp max |f ′′(u)|
∀f ∈C2[−h, h].
If you are going to deal with functions that are not in C3, would you still prefer
Simpson’s rule to the trapezoidal rule?

Problems and Computer Exercises
543
5.1.3 The quadrature formula
 xi+1
xi−1
f (x) dx ≈h

af (xi−1) + bf (xi) + cf (xi+1)

+ h2df ′(xi)
can be interpreted as a Hermite interpolatory formula with a double point at xi. Show
that d = 0 and that this formula is identical to Simpson’s rule. Then show that the
error can be written as
R(f ) = 1
4!
 xi+1
xi−1
f (4)(ξx)(x −xi−1)(x −xi)2(x −xi+1) dx,
where f (4)(ξx) is a continuous function of x. Deduce the error formula for Simpson’s
rule. Setting x = xi + ht, we get
R(f ) = h4
24f (4)(ξi)
 1
−1
(t + 1)t2(t −1)h dt = h5
90f (4)(ξi).
5.1.4 A second kind of Newton–Cotes’ open quadrature rule uses the midpoints of the
equidistant grid xi = ih, i = 1 : n, i.e.,
 xn
x0
f (x) dx =
n

i=1
wifi−1/2,
xi−1/2 = 1
2(xi−1 + xi).
(a) For n = 1 we get the midpoint rule. Determine the weights in this formula for
n = 3 and n = 5. (Use symmetry!)
(b) What is the order of accuracy of these two rules?
5.1.5 (a) Simpson’s rule with end corrections is a quadrature formula of the form
 h
−h
f (x) dx ≈h

αf (−h) + βf (0) + αf (h)

+ h2γ (f ′(−h) −f ′(h)),
which is exact for polynomials of degree ﬁve. Determine the weights α, β, γ by
using the test functions f (x) = 1, x2, x4. Use f (x) = x6 to determine the error
term.
(b) Show that in the corresponding composite formula for the interval [a, b] with
b −a = 2nh, only the endpoint derivatives f ′(a) and f ′(b) are needed.
5.1.6 (Lyness) Consider the integral
I(f, g) =
 nh
0
f (x)g′(x) dx.
(5.1.48)
An approximation related to the trapezoidal rule is
Sm = 1
2
n−1

j=0
,
f (jh) + f ((j + 1)h)
-,
g((j + 1)h) −g(jh)
-
,

544
Chapter 5. Numerical Integration
which requires 2(m+1) function evaluations. Similarly, an analogue to the midpoint
rule is
Rm = 1
2
n−1

j=0
′′f (jh)
,
g((j + 1)h) −g((j −1)h)
-
,
where the double prime on the summation indicates that the extreme values j = 0
and j = m are assigned a weighting factor 1
2. This rule requires 2(m + 2) function
evaluations, two of which lie outside the interval of integration. Show that the
difference Sm −Rm is of order O(h2).
5.1.7 Show the relations
 x
−1
Tn(t) dt =



Tn+1(x)
2(n + 1) −Tn−1(x)
2(n −1) + (−1)n+1
n2 −1
if n ≥2,
(T2(x) −1)
4
if n = 1,
T1(x) + 1
if n = 0.
Then deduce that
 1
−1
Tn(x) dx =
%
0
if n odd,
2/(1 −n2)
if n even.
Hint: Make a change of variable in the integral and use the trigonometric identity
2 cos nφ sin φ = sin(n + 1)φ −sin(n −1)φ.
5.1.8 Compute the integral
 ∞
0 (1 + x2)−4/3 dx with ﬁve correct decimals. Expand the
integrand in powers of x−1 and integrate termwise over the interval [R, ∞] for a
suitable value of R. Then use a Newton–Cotes’rule on the remaining interval [0, R].
5.1.9 Write a program for the derivation of a formula for integrals of the form I =
 1
0 x−1/2f (x) dx that is exact for f ∈Pn and uses the values f (xi), i = 1 : n, by
means of the power basis.
(a) Compute the coefﬁcients bi for n
=
6
:
8 with equidistant points,
xi = (i −1)/(n −1), i = 1 : n. Apply the formulas to the integrals
 1
0
x−1/2e−x dx,
 1
0
dx
sin √x ,
 1
0
(1 −t3)−1/2 dt.
In the ﬁrst of the integrals compare with the result obtained by series expansion in
Problem 3.1.1. A substitution is needed for bringing the last integral to the right
form.
(b) Do the same for the case where the step size xi+1 −xi grows proportionally to
i; x1 = 0; xn = 1. Is the accuracy signiﬁcantly different compared to (a), for the
same number of points?
(c) Make some very small random perturbations of the xi, i = 1 : n in (a), (say) of
the order of 10−13. Of which order of magnitude are the changes in the coefﬁcients
bi, and the changes in the results for the ﬁrst of the integrals?

Problems and Computer Exercises
545
5.1.10 Propose a suitable plan (using a computer) for computing the following integrals,
for s = 0.5, 0.6, 0.7, . . . , 3.0.
(a)
 ∞
0 (x3 + sx)−1/2 dx;
(b)
 ∞
0 (x2 + 1)−1/2e−sx dx, error < 10−6;
(c)
 ∞
π (s + x)−1/3 sin x dx.
5.1.11 It is not true that any degree of accuracy can be obtained by using a Newton–Cotes’
formula of sufﬁciently high order. To show this, compute approximations to the
integral
 4
−4
dx
1 + x2 = 2 tan−1 4 ≈2.6516353 . . .
using the closed Newton–Cotes’ formula with n = 2, 4, 6, 8. Which formula gives
the smallest error?
5.1.12 For expressing integrals appearing in the solution of certain integral equations, the
following modiﬁcation of the midpoint rule is often used:
 xn
x0
K(xj, x)y(x) dx =
n−1

i=0
mijyi+1/2,
where yi+1/2 = y( 1
2(xi + xi+1)) and mij is the moment integral
mij =
 xi+1
xi
K(xj, x) dx.
Derive an error estimate for this formula.
5.1.13 (a) Suppose that you have found a truncated δ2-expansion, (say) A(δ2) ≡a1 +
a2δ2 +· · ·+ak+1δ2k. Then an equivalent symmetric expression of the form B(E) ≡
b1 + b2(E + E−1) + · · · + bk+1(Ek + E−k) can be obtained as b = Mk+1a, where
a, b are column vectors for the coefﬁcients, and Mk+1 is the (k + 1) × (k + 1)
submatrix of the matrix M given in (3.3.49).
Use this for deriving (5.1.36) from (5.1.35). How do you obtain the remainder term?
If you obtain the coefﬁcients as decimal fractions, multiply them by 14,175/4 in
order to check that they agree with (5.1.36).
(b) Use Cauchy–FFT for deriving (5.1.35), and the open formula and the remainder
for the same interval.
(c) Set zn = ∇−1yn −4−1y0. We have, in the literature, seen the interpretation
that zn = n
j=0 yj if n ≥0. It seems to require some extra conditions to be true.
Investigate if the conditions z−1 = y−1 = 0 are necessary and sufﬁcient. Can you
suggest better conditions? (The equations 44−1 = ∇∇−1 = 1 mentioned earlier
are assumed to be true.)
5.1.14 (a) Write a program for the derivation of quadrature formulas and error estimates
using the Cauchy–FFT method in Sec. 5.1.5 for m = n −1, n, n + 1. Test the
formulas and the error estimates for some m, n on some simple (though not too
simple) examples. Some of these formulas are listed in the Handbook [1, Sec. 25.4].

546
Chapter 5. Numerical Integration
In particular, check the closed Newton–Cotes’ 9-point formula (n = 8).
(b) Sketch a program for the case that h = 1/(2n + 1), with the computation of f
at 2m symmetrical points.
(c) [1, Sec. 25.4] gives several Newton–Cotes’ formulas of closed and open types,
with remainders. Try to reproduce and extend their tables with techniques related
to Sec. 5.3.1.
5.1.15 Compute the integral
1
2π
 2π
0
e
1
√
2 sin xdx
by the trapezoidal rule, using h = π/2k k = 0, 1, 2, . . . , until the error is on the
level of the roundoff errors. Observe how the number of correct digits vary with h.
Notice that Romberg is of no use in this problem.
Hint: First estimate how well the function g(x) = ex/
√
2 can be approximated by a
polynomial in P8 for x ∈[−1, 1]. The estimate found by the truncated Maclaurin
expansion is not quite good enough. Theorem 3.1.5 provides a sharper estimate with
an appropriate choice of R; remember Scylla and Charybdis.
5.1.16 (a) Show that the trapezoidal rule, with h = 2π/(n+1), is exact for all trigonometric
polynomials of period 2π, i.e., for functions of the type
n

k=−n
ckeikt,
i2 = −1,
when it is used for integration over a whole period.
(b) Show that if f (x) can be approximated by a trigonometric polynomial of degree
n so that the magnitude of the error is less than ϵ, in the interval (0, 2π), then the
error with the use of the trapezoidal rule with h = 2π/(n + 1) on the integral
1
2π
 2π
0
f (x) dx
is less than 2ϵ.
(c) Use the above to explain the sensationally good result in Problem 5.1.15 above,
when h = π/4.
5.2
Integration by Extrapolation
5.2.1
The Euler–Maclaurin Formula
Newton–Cotes’ rules have the drawback that they do not provide a convenient way of
estimating the error. Also, for high-order rules negative weights appear. In this section we
will derive formulas of high order, based on the Euler–Maclaurin formula (see Sec. 3.4.5),
which do not share these drawbacks.

5.2. Integration by Extrapolation
547
Let xi = a + ih, xn = b, and let T (a : h : b)f denote the trapezoidal sum
T (a : h : b)f =
n

i=1
h
2

f (xi−1) + f (xi)

.
(5.2.1)
According to Theorem 3.4.10, if f ∈C2r+2[a, b], then
T (a : h : b)f −
 b
a
f (x) dx = h2
12

f ′(b) −f ′(a)

−h4
720

f ′′′(b) −f ′′′(a)

+ · · · + B2rh2r
(2r)!

f (2r−1)(b) −f (2r−1)(a)

+ R2r+2(a, h, b)f.
By (3.4.37) the remainder R2r+2(a, h, b)f is O(h2r+2) and represented by an integral with
a kernel of constant sign in [a, b]. The estimation of the remainder is very simple in
certain important particular cases. Note that although the expansion contains derivatives at
the boundary points only, the remainder requires that |f (2r+2)| is integrable on the whole
interval [a, b].
We recall the following simple and useful relation between the trapezoidal sum and
the midpoint sum (cf. (5.1.21)):
M(a : h : b)f =
n

i=1
hf (xi−1/2) = 2T

a : 1
2h : b

f −T (a : h : b)f.
(5.2.2)
From this one easily derives the expansion
M(a : h : b)f =
 b
a
f (x) dx −h2
24

f ′(b) −f ′(a)

+ 7h4
5760

f ′′′(b) −f ′′′(a)

+ · · · +

1
22r−1 −1
B2rh2r
(2r)!

f (2r−1)(b) −f (2r−1)(a)

+ · · · ,
which has the same relation to the midpoint sum as the Euler–Maclaurin formula has to the
trapezoidal sum.
The Euler–Maclaurin formula can be used for highly accurate numerical integration
when the values of derivatives of f are known at x = a and x = b. It is also possible to use
difference approximations to estimate the derivatives needed. A variant with uncentered
differences is Gregory’s170 quadrature formula:
 b
a
f (x) dx = hEn −1
hD
f0 = h

fn
−ln(1 −∇) −
f0
ln(1 + 4)

= T (a; h; b) + h
∞

j=1
aj+1(∇jfn + (−4)jf0),
170James Gregory (1638–1675), a Scotch mathematician, discovered this formula long before the Euler–
Maclaurin formula. It seems to have been used primarily for numerical quadrature. It can be used also for
summation, but the variants with central differences are typically more efﬁcient.

548
Chapter 5. Numerical Integration
where T (a : h : b) is the trapezoidal sum. The operator expansion must be truncated at
∇kfn and 4lf0, where k ≤n, l ≤n. (Explain why the coefﬁcients aj+1, j ≥1, occur in
the implicit Adams formula too; see Problem 3.3.10 (a).)
5.2.2
Romberg’s Method
The Euler–Maclaurin formula is the theoretical basis for the application of repeated Richard-
son extrapolation (see Sec. 3.4.6) to the results of the trapezoidal rule. This method is known
as Romberg’s method.171 It is one of the most widely used methods, because it allows a
simple strategy for the automatic determination of a suitable step size and order. Romberg’s
method was made widely known through Stiefel [336]. A thorough analysis of the method
was carried out by Bauer, Rutishauser, and Stiefel in [20], which we shall refer to for proof
details.
Let f ∈C2m+2[a, b] be a real function to be integrated over [a, b] and denote the
trapezoidal sum by T (h) = T (a : h : b)f . By the Euler–Maclaurin formula it follows that
T (h) −
 b
a
f (x) dx = c2h2 + c4h4 + · · · + cmh2m + τm+1(h)h2m+2,
(5.2.3)
where ck = 0 if f ∈Pk. This suggests the use of repeated Richardson extrapolation applied
to the trapezoidal sums computed with step lengths
h1 = b −a
n1
,
h2 = h1
n2
,
. . . ,
hq = h1
nq
,
(5.2.4)
wheren1, n2, . . . , nq arestrictlyincreasingpositiveintegers. IfwesetTm,1 = T (a, hm, b)f ,
m = 1 : q, then using Neville’s interpolation scheme the extrapolated values can be
computed from the recursion:
Tm,k+1 = Tm,k +
Tm,k −Tm−1,k
(hm−k/hm)2 −1,
1 ≤k < m.
(5.2.5)
Romberg used step sizes in a geometric progression, hm/hm−1 = q = 2. In this case the
denominators in (5.2.5) become 22k −1. This choice has the advantage that successive
trapezoidal sums can be computed using the relation
T
h
2

= 1
2(T (h) + M(h)),
M(h) =
n

i=1
hf (xi−1/2),
(5.2.6)
where M(h) is the midpoint sum. This makes it possible to reuse the function values that
have been computed earlier.
We remark that, usually, a composite form of Romberg’s method is used; the method
is applied to a sequence of interval [a + iH, a + (i + 1)H] for some bigstep H. The
171Werner Romberg (1909–2003) was a German mathematician. For political reasons he ﬂed Germany in 1937,
ﬁrst to Ukraine and then to Norway, where in 1938 he joined the University of Oslo. He spent the war years in
Sweden and then returned to Norway. In 1949 he joined the Norwegian Institute of Technology in Trondheim. He
was called back to Germany in 1968 to take up a position at the University of Heidelberg.

5.2. Integration by Extrapolation
549
applications of repeated Richardson extrapolation and the Neville algorithms to differential
equations belong to the most important.
Rational extrapolation can also be used. This gives rise to a recursion of a form similar
to (5.2.5):
Tm,k+1 = Tm,k +
Tm,k −Tm−1,k
hm−k
hm
2*
1 −Tm,k −Tm−1,k
Tm,k −Tm−1,k−1
+
−1
,
1 ≤k ≤m;
(5.2.7)
see Sec. 4.3.3.
For practical numerical calculations the values of the coefﬁcients ck in (5.2.3) are
not needed, but they are used, for example, in the derivation of an error bound; see Theo-
rem 5.2.1. It is also important to remember that the coefﬁcients depend on derivatives of
increasing order; the success of repeated Richardson extrapolations is thus related to the
behavior in [a, b] of the higher derivatives of the integrand.
Theorem 5.2.1 (Error Bound for Romberg’s Method).
The items Tm,k in Romberg’s method are estimates of the integral
 b
a f (x) dx that can
be expressed as a linear functional,
Tm,k = (b −a)
n

j=0
α(k)
m,jf (a + jh),
(5.2.8)
where n = 2m−1, h = (b −a)/n, and
n

j=0
α(k)
m,j = 1,
α(k)
m,j > 0.
(5.2.9)
The remainder functional for Tm,k is zero for f ∈P2k, and its Peano kernel is positive in
the interval (a, b). The truncation error of Tm,k reads
Tm,k −
 b
a
f (x)dx = rkh2k(b −a)f (2k)
1
2(a + b)

+ O(h2k+2(b −a)f (2k+2))
= rkh2k(b −a)f (2k)(ξ),
ξ ∈(a, b),
(5.2.10)
where
rk = 2k(k−1)|B2k|/(2k)!,
h = 21−m(b −a).
Proof. Sketch: Equation (5.2.8) follows directly from the construction of the Romberg
scheme. (It is for theoretical use only; the recursion formulas are better for practical use.)
The ﬁrst formula in (5.2.9) holds because Tm,k is exact if f = 1. The second formula is easily
proved for low values of k. The general proof is more complicated; see [20, Theorem 4].
The Peano kernel for m = k = 1 (the trapezoidal rule) was constructed in Exam-
ple 3.3.7. For m = k = 2 (Simpson’s rule), see Sec. 5.1.3. The general case is more compli-
cated. Recall that, by Corollary 3.3.9 of Peano’s remainder theorem, a remainder formula
with a mean value ξ ∈(a, b) exists if and only if the Peano kernel does not change sign.

550
Chapter 5. Numerical Integration
Bauer, Rutishauser, and Stiefel [20, pp. 207–210] constructed a recursion formula
for the kernels, and succeeded in proving that they are all positive, by an ingenious use
of the recursion. The expression for rk is also derived there, although with a different
notation.
From (5.2.9) it follows that if the magnitude of the irregular error in f (a + jh) is at
most ϵ, then the magnitude of the inherited irregular error in Tm,k is at most ϵ(b −a). There
is another way of ﬁnding rk. Note that for each value of k, the error of Tk,k for f (x) = x2k
can be determined numerically. Then rk can be obtained from (5.2.10). Tm,k is the same
formula as Tk,k, although with a different h.
According to the discussion of repeated Richardson extrapolation in Sec. 3.4.6, one
continues the process until two values in the same row agree to the desired accuracy. If no
other error estimate is available, mink |Tm,k −Tm,k−1| is usually chosen as an estimate of the
truncation error, even though it is usually a strong overestimate. A feature of the Romberg
algorithm is that it also contains exits with lower accuracy at a lower cost.
Example 5.2.1 (A Numerical Illustration to Romberg’s Method).
Use Romberg’s method to compute the integral (cf. Example 5.1.5)
 0.8
0
sin x
x
dx.
The midpoint and trapezoidal sums are with ten correct decimals equal to
h
M(h)f
T (h)f
0.8
0.77883 66846
0.75867 80454
0.4
0.77376 69771
0.76875 73650
0.2
0.77251 27161
0.77126 21711
0.1
0.77188 74436
.
It can be veriﬁed that in this example the error is approximately proportional to h2 for both
M(h) and T (h). We estimate the error in T (0.1) to be 1
36.26 · 10−4 ≤2.1 · 10−4.
The trapezoidal sums are then copied to the ﬁrst column of the Romberg scheme.
Repeated Richardson extrapolation is performed giving the following table.
m
Tm1
Tm2
Tm3
Tm4
1
0.75867 80454
2
0.76875 73650
0.77211 71382
3
0.77126 21711
0.77209 71065
0.77209 57710
4
0.77188 74437
0.77209 58678
0.77209 57853
0.77209 57855
5
0.77204 37039
0.77209 57906
0.77209 57855
0.77209 57855
.
We ﬁnd that |T44 −T43| = 2 · 10−10, and the irregular errors are less than 10−10. Indeed, all
ten digits in T44 are correct, and I = 0.77209 57854 82 . . . . Note that the rate of convergence
in successive columns is h2, h4, h6, h8, . . . .
The following MATLAB program implements Romberg’s method. In each major
step a new row in the Romberg table is computed.

5.2. Integration by Extrapolation
551
Algorithm 5.2. Romberg’s Method.
function [I, md, T] = romberg(f,a,b,tol,q);
% Romberg’s method for computing the integral of f over [a,b]
% using at most q extrapolations. Stop when two adjacent values
% in the same column differ by less than tol or when q
% extrapolations have been performed. Output is an estimate
% I of the integral with error bound md and the active part
% of the Romberg table.
%
T = zeros(q+2,q+1);
h = b - a;
m = 1; P = 1;
T(1,1) = h*(feval(f,a) + feval(f,b))/2;
for m = 2:q+1
h = h/2; m = 2*m;
M = 0;
% Compute midpoint sum
for k = 1:2:m
M = M + feval(f, a+k*h);
end
T(m,1) = T(m-1,1)/2 + h*M;
kmax = min(m-1,q);
for k = 1:kmax
% Repeated Richardson extrapolation
T(m,k+1) = T(m,k) + (T(m,k) - T(m-1,k))/(2ˆ(2*k) - 1);
end
[md, kb] = min(abs(T(m,1:kmax) - T(m-1,1:kmax)));
I = T(m,kb);
if md <= tol
% Check accuracy
T = T(1:m,1:kmax+1); % Active part of T
return
end
end
In the above algorithm the value Tm,k is accepted when |Tm,k −Tm−1,k| ≤tol, where
tol is the permissible error. Thus one extrapolates until two values in the same column agree
to the desired accuracy. In most situations, this gives, if h is sufﬁciently small, with a large
margin a bound for the truncation error in the lower of the two values. Often instead the
subdiagonal error criterion |Tm,m−1 −Tm,m| < δ is used, and Tmm taken as the numerical
result.
If the use of the basic asymptotic expansion is doubtful, then the uppermost diagonal
of the extrapolation scheme should be ignored. Such a case can be detected by inspection
of the difference quotients in a column. If for some k, where Tk+2,k has been computed and
the modulus of the relative irregular error of Tk+2,k −Tk+1,k is less than (say) 20%, and,
most important, the difference quotient
(Tk+1,k −Tk,k)/(Tk+2,k −Tk+1,k)
is very different from its theoretical value qpk, then the uppermost diagonal is to be ignored
(except for its ﬁrst element).
Sometimes several of the uppermost diagonals are to be ignored. For the integration
of a class of periodic functions the trapezoidal rule is superconvergent; see Sec. 5.1.4. In

552
Chapter 5. Numerical Integration
this case all the difference quotients in the ﬁrst column are much larger than qp1 = q2.
According to the rule just formulated, every element of the Romberg scheme outside the
ﬁrst column should be ignored. This is correct; in superconvergent cases Romberg’s method
is of no use; it destroys the excellent results that the trapezoidal rule has produced.
Example 5.2.2.
The remainder for Tk,k in Romberg’s method reads
Tk,k −
 b
a
f (x) dx = rkh2k(b −a)f 2k(ξ).
For k = 1, T11 is the trapezoidal rule with remainder r1h2(b −a)f (2)(ξ). By working
algebraically in the Romberg scheme, we see that T22 is the same as Simpson’s rule. It can
also be shown that T33 is the same as Milne’s formula, i.e., the ﬁve-point closed Newton–
Cotes’ formula. It follows that for k = {1, 2, 3} both methods give, with k′ = {2, 3, 5}
function values, exact results for f ∈Pk′.
This equivalence can also be proved by the following argument. By Corollary 3.3.5,
there is only one linear combination of the values of the function f at n + 1 given points
that can yield
 b
a f (x) dx exactly for all polynomials f ∈Pn+1. It follows that the methods
of Cotes and Romberg for Tk,k are identical for k = 1, 2, 3.
For k > 3 the methods are not identical. For k = 4 (9 function values), Cotes is
exact in P10, while T44 is exact in P8. For k = 5 (17 function values), Cotes is exact in
P18, while T55 is exact in P10. This sounds like an advantage for Cotes, but one has to be
sceptical about formulas that use equidistant points in polynomial approximation of very
high degree; see the discussion of Runge’s phenomena in Chapter 4.
Note that the remainder of T44 is
r4h8(b −a)f (8)(ξ) ≈r4(b −a)48f (a),
r4 = 16/4725,
where 48f (a) uses the same function values as T44 and C8. So we can use r4(b−a)48f (a)
as an asymptotically correct error estimate for T44.
We have assumed so far that the integrand is a real function f ∈C2m+2[a, b]. For
example, if the integrand f (x) has an algebraic endpoint singularity,
f (x) = xβh(x),
−1 < β ≤0,
where h(x) ∈Cp+1[a, b], this assumption is not valid. In this case an asymptotic error
expansion of the form
T (h) −I =
n

q=1
aqhq+β +
q

q=2
bqhq + O(hq+1)
(5.2.11)
can be shown to hold for a trapezoidal sum. Similar but more complicated expansions can
be obtained for other classes of singularities. If p = −1/2, then T (h) has an error expansion
in h1/2:
T (h) −I = a1h3/2 + b2h2 + a2h5/2 + b3h3 + a3h5/2 + · · · .

5.2. Integration by Extrapolation
553
Richardson extrapolation can then be used with the denominators
2pj −1,
pj = 1.5, 2, 2.5, 3, . . . .
Clearly the convergence acceleration will be much less effective than in the standard
Romberg case.
In Richardson extrapolation schemes the exponents in the asymptotic error expansions
have to be known explicitly. In cases when the exponents are unknown a nonlinear extrapola-
tion scheme like the ϵ algorithm should be used. In this a two-dimensional array of numbers
ϵ(p)
k , initialized with the trapezoidal approximations Tm = T (hm), hm = (b −a)/2m, is
computed by the recurrence relation
ϵ(m)
−1 = 0,
m = 1 : n −1, . . . ,
ϵ(m)
0
= Tm,
m = 0 : n,
ϵ(m)
k+1 = ϵ(m+1)
k−1
+
1
ϵ(m+1)
k
−ϵ(m)
k
,
k = 0 : n −2,
m = 0 : n −k −1.
Example 5.2.3.
Accelerating the sequence of trapezoidal sums using the epsilon algorithm may work
when Romberg’s method fails. In the integral
 1
0
√x dx = 2/3,
the integrand has a singularity at the left endpoint.
Using the trapezoidal rule with 2k + 1 points, k = 0 : 9, the error is divided roughly
by 2
√
2 ≈2.828 when the step size is halved.
For k = 9 we get he approximation
I ≈0.66664 88815 with an error 0.18 · 10−4.
Applying the ϵ algorithm to these trapezoidal sums, we obtained the accelerated values
displayed in the table below. (Recall that the quantities in odd-numbered columns are only
intermediate quantities.) The magnitude of the error in ϵ(1)
8
is close to full IEEE double
precision. Note that we did not use any a priori knowledge of the error expansion.
k
ϵ(9−2k)
2k
Error
0
0.66664 88815 4995
−0.1779 · 10−4
1
0.66666 67335 1817
0.6685 · 10−7
2
0.66666 66666 6037
−0.6292 · 10−11
3
0.66666 66666 6669
0.268 · 10−13
4
0.66666 66666 6666
−0.044 · 10−13
An application of the epsilon algorithm to computing the integral of an oscillating
integrand to high precision is given in Example 5.2.5.

554
Chapter 5. Numerical Integration
5.2.3
Oscillating Integrands
Highly oscillating integrals of the form
I[f ] =
 b
a
f (x)eiωg(x) dx,
(5.2.12)
where f (x) is a slowly varying function and eiωg(x) is oscillating, frequently occur in
applications from electromagnetics, chemistry, ﬂuid mechanics, etc. Such integrals are
allegedlydifﬁculttocompute. Whenastandardnumericalquadratureruleisusedtocompute
(5.2.12), using a step size h such that ωh ≪1 is required. For large values of ω this means
an exceedingly small step size and a large number of function evaluations.
Some previously mentioned techniques such as using a simple comparison problem,
or a special integration formula, can be effective also for an oscillating integrand. Consider
the case of a Fourier integral, where g(x) = x, in (5.2.12). The trapezoidal rule gives the
approximation
I[f ] ≈1
2h

f0eiωa + fNeiωb
+ h
N−1

j=1
fjeiωxj ,
(5.2.13)
where h = (b −a)/N, xj = a + jh, fj = f (xj). This formula cannot be used unless
ωh ≪1, since its validity is based on the assumption that the whole integrand varies linearly
over an interval of length h.
Abetter method is obtained by approximating just f (x) by a piecewise linear function,
pj(x) = fj + x −xj
h
(fj+1 −fj),
x ∈[xj, xj+1],
j = 0 : N −1.
The integral over [xj, xj+1] can then be approximated by
 xj+1
xj
pj(x)eiωx dx = heiωxj

fj
 1
0
eiωht dt + (fj+1 −fj)
 1
0
teiωht dt

,
where we have made the change of variables x −xj = th. Let θ = hω be the characteristic
frequency. Then
 1
0
eiθt dt = 1
iθ (eiθ −1) = α,
and using integration by parts
 1
0
teiθt dt = 1
iθ teiθt
((((
1
0
−1
iθ
 1
0
eiθt dt = 1
iθ eiθ + 1
θ2 (eiθ −1) = β.
Here α and β depend on θ but not on j. Summing the contributions from all intervals we
obtain
h(α −β)
N−1

j=0
fjeiωxj + hβ
N−1

j=0
fj+1eiωxj
= h(α −β)
N−1

j=0
fjeiωxj + hβe−iθ
N

j=1
fjeiωxj .

5.2. Integration by Extrapolation
555
The resulting quadrature formula has the same form as (5.2.13),
I[f ] ≈hw(θ)
N−1

j=0
fjeiωxj + hwN(θ)

fNeiωxN −f0eiωx0
,
(5.2.14)
with the weights w0(θ) = α −β, wN(θ) = βe−iθ, and w(θ) = w0 + wN. Then
w0(θ) = wN(−θ) = 1 −iθ −e−iθ
θ2
,
w(θ) = (sin 1
2θ)2
( 1
2θ)2
.
(5.2.15)
Note that the same trigonometric sum is involved, now multiplied with the real factor w(θ).
The sum in (5.2.14) can be computed using the FFT; see Sec. 4.7.3.
The weights tend to the trapezoidal weights when ωh →0 (check this!). For small
values of |θ| there will be cancellation in these expressions for the coefﬁcients and the Taylor
expansions should be used instead; see Problem 5.2.8.
A similar approach for computing trigonometric integrals of one of the forms
 b
a
f (x) cos(ωx) dx,
 b
a
f (x) sin(ωx) dx
(5.2.16)
was advanced by Filon [118] already in 1928. In this the interval [a, b] is divided into an
even number of 2N subintervals of equal length h = (b −a)/(2N). The function f (x)
is approximated over each double interval [x2i, x2(i+1)] by the quadratic polynomial pi(x)
interpolating f (x) at x2i, x2i+1, and x2(i+1). Filon’s formula is thus related to Simpson’s
rule. (The formula (5.2.14) is often called the Filon-trapezoidal rule.) For ω = 0, Filon’s
formula reduces to the composite Simpson’s formula, but it is not exact for cubic functions
f (x) when ω ̸= 0.
The integrals
 x2(i+1)
x2i
pi(x) cos(ωx) dx,
 x2(i+1)
x2i
pi(x) sin(ωx) dx
can be computed analytically using integration by parts. This leads to Filon’s integration
formula; see the Handbook [1, Sec. 25.4.47].
Similar formulas can be developed by using different polynomial approximations of
f (x). Einarsson [105] uses a cubic spline approximation of f (x) and assumes that the ﬁrst
and second derivatives of f at the boundary are available. The resulting quadrature formula
has an error which usually is about four times smaller than that for Filon’s rule.
Using the Euler–Maclaurin formula on the function it can be shown (see Einars-
son [106, 107]) that the expansion of the error for the Filon-trapezoidal rule, the Filon–
Simpson method, and the cubic spline method contain only even powers of h. Thus the
accuracy can be improved by repeated Richardson extrapolation. For example, if the Filon-
trapezoidal rule is used with a sequence of step sizes h, h/2, h/4, . . . , then one can proceed
as in Romberg’s method. Note that the result after one extrapolation is not exactly equal to
the Filon–Simpson rule, but gives a marginally better result when ωh = O(1).

556
Chapter 5. Numerical Integration
Example 5.2.4 (Einarsson [106]).
Using the standard trapezoidal rule to compute the Fourier integral
I =
 ∞
0
e−x cos ωx dx =
1
1 + ω2
gives the result
IT = h
1
2 + ℜ
∞

j=1
e−jheihωj

= h
2
sinh h
cosh h −cosh hω,
where h is the step length. Assuming that hω is sufﬁciently small we can expand the
right-hand side in powers of h, obtaining
IT = I

1 + h2
12(1 + ω2) + h4
720(1 + ω2)(3ω2 −1) + O(h6)

.
For the Filon-trapezoidal rule the corresponding result is
IFT =

sin 1
2ωh
1
2ωh
2
IT = I

1 + h2
12 −h4
720(3ω2 + 1) + O(h6)

.
For small values of ω the two formulas are seen to be equivalent. However, for larger values
of ω, the error in the standard trapezoidal rule increases rapidly.
The expansions only have even powers of h. After one step of extrapolation the
Filon-trapezoidal rule gives a relative error equal to h4(3ω2 + 1)/180, which can be shown
to be slightly better than for the Filon–Simpson rule.
More general Filon-type methods can be developed as follows. Suppose we wish to
approximate the integral
I[f ] =
 h
0
f (x)eiωx dx = h
 1
0
f (ht)eihωt dt,
(5.2.17)
where f is itself sufﬁciently smooth. We choose distinct nodes 0 ≤c1 < c2 < · · · < cν ≤1
andconsiderthequadratureformulainterpolatoryweights b1, b2, . . . , bν. Lets bethelargest
integer j so that
 1
0
tj−1γ (t) dt = 0,
γ (t) =
ν&
i=1
(t −ci).
(5.2.18)
Then by Theorem 5.1.3, s ≤ν, and the order of the corresponding quadrature formula
is p = ν + s. A Filon-type quadrature rule is now obtained by interpolating f by the
polynomial
p(x) =
ν

k=1
ℓk(x/h)f (ckh),

5.2. Integration by Extrapolation
557
where ℓk is the kth cardinal polynomial of Lagrange interpolation. Replacing f by p in
(5.2.17), we obtain
Qh[f ] =
ν

k=1
βk(θ)f (ckh),
βk(θ) =
 1
0
ℓk(t)eihωt dt.
(5.2.19)
The coefﬁcients βk(θ) can be computed also from the moments
µk(θ) =
 1
0
tkeiθt dt,
k = 0 : ν −1,
by solving the Vandermonde system
ν

j=1
βj(θ)ck
j = µk(θ),
k = 0 : ν −1.
The derivation of the Filon-type quadrature rule is analogous to considering eiθt as
a complex-valued weight function. However, any attempt to choose the nodes cj so that
the order of the integration rule is increased over ν is likely to lead to complex nodes and
useless formulas.
The general behavior of Filon-type quadrature rules is that for 0 < θ ≪1 they show
similar accuracy to the corresponding standard interpolatory rule. For θ = O(1) they are
also very effective, although having order ν ≤p. The common wisdom is that if used in the
region where θ is large they can give large errors. However, Einarsson [106] observed that
the cubic spline method gives surprisingly good results also for large values of θ, seemingly
in contradiction to the condition in the sampling theorem that at least two nodes per full
period are needed.
Iserles [209] shows that once appropriate Filon-type methods are used the problem
of highly oscillatory quadrature becomes relatively simple. Indeed, the precision of the
calculation actually increases as the oscillation grows. This is quantiﬁed in the following
theorem.
Theorem 5.2.2 (Iserles [209, Theorem 2]).
Let θ = hω be the characteristic frequency. Then the error Eh[f ] in the Filon-type
quadrature formula (5.2.19) is
Eh[f ] ∼O(hν+1θ−p),
(5.2.20)
where p = 2 if c1 = 0 and cν = 0; p = 1 otherwise.
To get the best error decay the quadrature formula should include the points c1 = 0
and cν = 1. This is the case both for the Filon-trapezoidal method and the Filon–Simpson
rule. Figure 5.2.1 shows the absolute value of the integral
I =
 h
0
exeiωx dx = (e(1+iω)h −1)/(1 + iω),

558
Chapter 5. Numerical Integration
0
200
400
600
800
1000
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
Figure 5.2.1. The Filon-trapezoidal rule applied to the Fourier integral with
f (x) = ex, for h = 1/10, and ω = 1 : 1000; solid line: exact integral; dashed line:
absolute value of the error.
and the absolute value of the error in the Filon-trapezoidal approximation for h = 0.1 and
ω = 1 : 1000. Clearly the error is small and becomes smaller as the characteristic frequency
grows!
Sometimes convergence acceleration of a related series can be successfully employed
for the evaluation of an integral with an oscillating integrand. Assume that the integral has
the form
I[f ] =
 ∞
0
f (x) sin(g(x)) dx,
where g(x) is an increasing function and both f (x) and g(x) can be approximated by a
polynomial. Set
I[f ] =
∞

n=0
(−1)Nun,
un =
 xn+1
xn
f (x) |sin(g(x))| dx,
where x0, x1, x2, . . . are the successive zeros of sin(g(x)). The convergence of this al-
ternating series can then be improved with the help of repeated averaging; see Sec. 3.4.3.
Alternatively a sequence of partial sums can be computed, which then is accelerated by the
epsilon algorithm. Sidi [323] has developed a useful extrapolation method for oscillatory
integrals over an inﬁnite interval.
Example 5.2.5 (Gautschi [149]).
The ﬁrst problem in “The 100-digit Challenge”172 is to compute the integral
I = lim
ϵ→0
 1
ϵ
t−1 cos(t−1 ln t) dt
(5.2.21)
172See [41] and www.siam.org/books/100digitchallenge.

5.2. Integration by Extrapolation
559
to ten decimal places. Since the integrand is densely oscillating as t ↓0 and at the same time
the oscillations tend to inﬁnity (see Figure 5.2.2), this is a challenging integral to compute
numerically. (Even so the problem has been solved to an accuracy of 10,000 digits!)
0
0.1
0.2
0.3
0.4
0.5
−400
−300
−200
−100
0
100
200
x
y
Figure 5.2.2. The oscillating function x−1 cos(x−1 ln x).
With the change of variables u = t−1, du = −t−2dt, we get
I =
 ∞
1
u−1 cos(u ln u) du.
(5.2.22)
Making the further change of variables x(u) = u ln u, we have dx = (1 + ln u)du =
(u + x)u−1du, and the integral becomes
I =
 ∞
0
cos x
x + u(x) dx.
(5.2.23)
The inverse function u(x) is smooth and relatively slowly varying, with u(0) = 1, u′(0) = 1.
For x > 0, u′(x) is positive and decreasing, while u′′(x) is negative and decreasing in
absolute value. The function u(x) is related to Lambert’s W-function, which is the inverse
of the function x = wew (see Problem 3.1.12). Clearly u(x) = ew(x).
The zeros of the integrand in (5.2.23) are at odd multiples of π/2. We split the interval
of integration into intervals of constant sign for the integrand
I =
 π/2
0
cos x
x + u(x) dx +
∞

k=1
Ik,
Ik =
 (2k+1)π/2
(2k−1)π/2
cos x
x + u(x) dx.
Changing variables x = t + kπ in the integrals Ik,
Ik = (−1)k
 π/2
−π/2
cos t
t + kπ + u(t + kπ) dt.
(5.2.24)

560
Chapter 5. Numerical Integration
The terms form an alternating series with terms decreasing in absolute values. It is, however,
slowly converging and for an error bound of 1
210−5 about 116,000 terms would be needed.
Accelerating the convergence using the epsilon algorithm, Gautschi found that using only
21 terms in the series sufﬁces to give an accuracy of about 15 decimal digits:
I = 0.32336 74316 77779.
The integrand in the integrals (5.2.24) is regular and smooth. For computing these,
for example, a Clenshaw–Curtis quadrature rule can be used after shifting the interval of
integration to [−1, 1]; see also Problem 5.3.11.
5.2.4
Adaptive Quadrature
Suppose the integrand f (x) (or some of its low-order derivatives) has strongly varying
orders of magnitude in different parts of the interval of integration [a, b]. Clearly, one
should then use different step sizes in different parts of the integration interval. If we write
 b
a
=
 c1
a
+
 c2
c1
+ · · · +
 b
c1
,
then the integrals on the right-hand side can be treated as independent subproblems. In adap-
tive quadrature methods step sizes are automatically adjusted so that the approximation
satisﬁes a prescribed error tolerance:
(((I −
 b
a
f (x) dx
((( ≤ϵ.
(5.2.25)
A common difﬁculty is when the integrand exhibits one or several sharp peaks as
exempliﬁed in Figure 5.2.3. It should be realized that without further information about the
location of the peaks all quadrature algorithms can fail if the peaks are sharp enough.
We consider ﬁrst a ﬁxed-order adaptive method based on Simpson’s rule. For a
subinterval [a, b], set h = (b −a) and compute the trapezoidal approximations
T00 = T (h),
T10 = T (h/2),
T20 = T (h/4).
The extrapolated values
T11 = (4T10 −T00)/3,
T21 = (4T20 −T10)/3
are equivalent to (the composite) Simpson’s rule with step length h/2 and h/4, respectively.
We can also calculate
T22 = (16T21 −T11)/15,
which is Milne’s method with step length h/4 with remainder equal to
(2/945)(h/4)6(b −a)f (6)(ξ).

5.2. Integration by Extrapolation
561
−1
−0.5
0
0.5
1
0
1000
2000
3000
4000
5000
6000
7000
8000
x
y
Figure 5.2.3. A needle-shaped function.
For T22 we can estimate the truncation error by |T22 −T21|, which is usually a strong
overestimate. We accept the approximation if
|T22 −T21| <
hjϵ
b −a ,
(5.2.26)
i.e., we require the error to be less than ϵ/(b −a) per unit step. Otherwise we reject the
approximation, and subdivide the interval in two intervals, [aj, 1
2(aj +bj)], [ 1
2(aj +bj), bj].
The same rule is now applied to these two subintervals.
Note that if the function values computed previously are saved, these can be reused
for the new intervals. We start with one interval [a, b] and carry on subdivisions until the
error criterion in (5.2.26) is satisﬁed for all intervals. Since the total error is the sum of
errors for all subintervals, we then have the required error estimate:
RT <

j
hjϵ
b −a = ϵ.
The possibility that a user might try to integrate a nonintegrable function (e.g., f (x) =
x−1 on [0, 1]) cannot be neglected. In principle it is not possible to decide whether a
function f (x) is integrable on the basis of a ﬁnite sample f (x1), . . . , f (xn) of function
values. Therefore, it is necessary to impose
1. an upper limit on the number of function evaluation,
2. a lower limit on the size of the subregions.
This means that premature termination may occur even when the function is only close to
being nonintegrable, for example, f (x) = x−0.99.
Many different adaptive quadrature schemes exist. Here we shall illustrate one simple
scheme based on a ﬁve-point closed Newton–Cotes’ rule, which applies bisection in a

562
Chapter 5. Numerical Integration
locally adaptive strategy. All function evaluations contribute to the ﬁnal estimate. In many
situations it might be preferable to specify a relative error tolerance:
tol = η
(((
 b
a
f (x) dx
(((.
A more complete discussion of the choice of termination criteria in adaptive algorithms is
found in Gander and Gautschi [131].
Algorithm 5.3. Adaptive Simpson.
Let f be a given function to be integrated over [a, b]. The function adaptsimp uses
a recursive algorithm to compute an approximation with an error less than a speciﬁed
tolerance τ > 0.
function [I,nf] = adaptsimp(f,a,b,tol);
% ADAPTSIMP calls the recursive function ADAPTREC to compute
% the integral of the vector-valued function f over [a,b];
% tol is the desired absolute accuracy; nf is the number of
% function evaluations.
%
ff = feval(f,[a, (a+b)/2, b]);
nf = 3; % Initial Simpson approximation
I1 = (b - a)*[1, 4, 1]*ff’/6;
% Recursive computation
[I,nf] =
adaptrec(f,a,b,ff,I1,tol,nf);
function [I,nf] = adaptrec(f,a,b,ff,I1,tol,nf);
h = (b - a)/2;
fm = feval(f, [a + h/2, b - h/2]);
nf = nf + 2;
% Simpson approximations for left and right subinterval
fR = [ff(2); fm(2); ff(3)];
fL = [ff(1); fm(1); ff(2)];
IL = h*[1, 4, 1]*fL/6;
IR = h*[1, 4, 1]*fR/6;
I2 = IL + IR;
I
= I2 + (I2 - I1)/15;
% Extrapolated approximation
if
abs(I - I2) > tol
% Refine both subintervals
[IL,nf] = adaptrec(f,a,a+h,fL,IL,tol/2,nf);
[IR,nf] = adaptrec(f,b-h,b,fR,IR,tol/2,nf);
I
= IL + IR;
end
Note that in a locally adaptive algorithm using a recursive partitioning scheme, the
subintervals are processed from left to right until the integral over each subinterval satisﬁes
some error requirement. This means that an a priori initial estimate of the whole integral,
needed for use in a relative local error estimate cannot be updated until all subintervals are
processed and the computation is ﬁnished. Hence, if a relative tolerance is speciﬁed, then

5.2. Integration by Extrapolation
563
an estimate of the integral is needed before the recursion starts. This is complicated by the
fact that the initial estimate might be zero, for example, if a periodic integrand is sampled
at equidistant intervals. Hence a combination of relative and absolute criteria might be
preferable.
Example 5.2.6.
This algorithm was used to compute the integral
 4
−4
dx
1 + x2 = 2.65163532733607
with an absolute tolerance 10−p, p = 4, 5, 6. The following approximations were obtained.
I
tol
n
Error
2.65162 50211
10−4
41
1.0 10−5
2.65163 52064
10−5
81
1.2 10−7
2.65163 5327353
10−6
153
−1.7 10−11
Note that the actual error is much smaller than the required tolerance.
So far we have considered adaptive routines, which use ﬁxed quadrature rules on
each subinterval but where the partition of the interval depends on the integrand. Such
an algorithm is said to be partition adaptive. We can also consider doubly adaptive
integration algorithms. These can choose from a sequence of increasingly higher-order
rules to be applied to the current subinterval. Such algorithms use a selection criterion to
decide at each stage whether to subdivide the current subinterval or to apply a higher-order
rule. Doubly adaptive routines cope more efﬁciently with smooth integrands.
Many variations on the simple scheme outlined above are possible. For example,
we could base the method on a higher-order Romberg scheme, or even try to choose an
optimal order for each subinterval. Adaptive methods work even when the integrand f (x)
is badly behaved. But if f has singularities or unbounded derivatives, the error criterion
may never be satisﬁed. To guard against such cases it is necessary to include some bound
of the number of recursion levels that are allowed. It should be kept in mind that although
adaptive quadrature algorithms are convenient to use they are in general less efﬁcient than
methods which have been specially adapted for a particular problem.
We ﬁnally warn the reader that no automatic quadrature routine can always be guar-
anteed to work. Indeed, any estimate of
 b
a f (x) dx based solely on the value of f (x) on
ﬁnitely many points can fail. The integrand f (x) may, for example, be nonzero only on a
small subset of [a, b]. An adaptive quadrature rule based only on samples f (x) in a ﬁnite
number of points theoretically may return the value zero in such a case!
We recall the remark that evaluation of the integral
 b
a f (x) dx is equivalent to solving
an initial value problem y′ = f (x), y(a) = 0, for an ordinary differential equation. For
such problems sophisticated techniques for adaptively choosing step size and order in the
integration have been developed. These may be a good alternative choice for handling
difﬁcult cases.

564
Chapter 5. Numerical Integration
Review Questions
5.2.1 (a) Give an account of the theoretical background of Romberg’s method.
(b) For which values of k are the elements Tkk in the Romberg scheme identical to
closed Newton–Cotes’ formulas?
5.2.2 Romberg’s method uses extrapolation of a sequence of trapezoidal approximations
computed for a sequence of step sizes h0, h1, h2, . . . . What sequences have been
suggested and what are their relative merits?
5.2.3 Whentheintegrandhasasingularityatoneoftheendpoints, manyquadraturemethods
converge very slowly. Name a few possible ways to resolve this problem.
5.2.4 Romberg’s method works only when the error of the trapezoidal rule has an expansion
in even powers of h. If this is not the case, what other extrapolations methods should
be tried?
5.2.5 Describe at least two methods for treating an integral with an oscillating integrand.
5.2.6 In partition adaptive quadrature methods the step sizes are locally adopted. Discuss
how the division into subintervals can be controlled.
Problems and Computer Exercises
5.2.1 Is it true that (the short version of) Simpson’s formula is a particular case of Gregory’s
formula?
5.2.2 Use Romberg’s method to compute the integral
 4
0 f (x) dx, using the following (cor-
rectly rounded) values of f (x). Need all the values be used?
x
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
f (x)
−4271
−2522
−499
1795
4358
7187
10,279
13,633
17,247
5.2.3 (a) Suppose that the form of the error of Romberg’s method is known, but not the
error constant rk. Determine rk numerically for k = 3 and k = 4, by computing the
Romberg scheme for f (x) = x2k.
(b) Prove the formula for the error constant of Romberg’s method.
5.2.4 Compute by the Euler–Maclaurin formula, or rather the composite trapezoidal rule,
(a)
 ∞
0
e−x2/2dx,
(b)
 ∞
0
dx
cosh(πx)
as accurately as you can with the normal precision of your computer (or software).
Then ﬁnd out empirically how the error depends on h. Make semilogarithmic plots
on the same screen. How long a range of integration do you need?
5.2.5 (a) Use Romberg’s method and Aitken acceleration to compute the integral
I[f ] =
 ∞
1
1
1 + x2 dx =
 2
1
+
 4
2
+
 8
4
+ · · · .

5.3. Quadrature Rules with Free Nodes
565
Determine where to terminate the expansion, and then use Aitken acceleration to ﬁnd
I[f ]. Compare with the exact result. Think of an error estimate that can be used if
the exact result is not known.
(b) Treat in the same way
 ∞
1
1
√
x + x3 .
Compare the computational effort for the computation of the tail
 ∞
R by acceleration
and by series expansion with the same accuracy.
5.2.6 Modify the MATLAB function romberg so that it uses rational extrapolation accord-
ing to the recursion (5.2.7) instead of polynomial extrapolation. Use the modiﬁed
program to compute the integral in Example 5.2.2. Compare the results for the two
different extrapolation methods.
5.2.7 Apply the MATLAB program romberg in Sec. 5.2.2 and repeated averages on the
integral
 1000
0
x cos(x3) dx.
Try to obtain the results with 10 decimal places.
5.2.8 (a) Show the following series expansions for the coefﬁcients in the Filon-trapezoidal
formula:
w0(θ) = wN(−θ) = 1
2 −θ2
24 + θ4
720 −· · · + i
θ
6 −θ3
120 +
θ5
5040 −· · ·

,
w(θ) = w0(θ) + wN(−θ) = 1 −θ2
12 + θ4
360 −· · · .
(b) For what value of θ should you switch to using the series expansions above, if
you want to minimize an upper bound for the error in the coefﬁcients?
5.3
Quadrature Rules with Free Nodes
5.3.1
Method of Undetermined Coefﬁcients
We have previously seen how to derive quadrature rules using Lagrange interpolation or
operator series. We now outline another general technique, the method of undetermined
coefﬁcients, for determining quadrature formulas of maximum order with both free and
prescribed nodes.
Let L be a linear functional and consider approximation formulas of the form
Lf ≈˜Lf =
p

i=1
aif (xi) +
q

j=1
bjf (zj),
(5.3.1)
where the xi are p given nodes, while the zj are q free nodes. The latter are to be determined
together with the weight factors ai, bj. The altogether p + 2q parameters in the formula

566
Chapter 5. Numerical Integration
are to be determined, if possible, so that the formula becomes exact for all polynomials of
degree less than N = p + 2q. We introduce the two node polynomials
r(x) = (x −x1) · · · (x −xp),
s(x) = (x −z1) · · · (x −zq)
(5.3.2)
of degree p and q, respectively.
Let φ1, φ2, . . . , φN be a basis of the space of polynomials of degree less than N. We
assume that the quantities Lφk, k = 1 : p + 2q are known. Then we obtain the nonlinear
system
p

i=1
φk(xi)ai +
q

j=1
φk(zj)bj = Lφk,
k = 1, 2, . . . , p + 2q,
(5.3.3)
for the p + 2q parameters. This system is nonlinear in zj, but of a very special type.
Note that the free nodes zj appear in a symmetric fashion; the system (5.3.3) is invariant
with respect to permutations of the free nodes together with their weights. We therefore
ﬁrst ask for their elementary symmetric functions, i.e., for the coefﬁcients gj of the node
polynomial
s(x) = φq+1(x) −
q

j=1
gjφj(x)
(5.3.4)
that has the free nodes z1, . . . , zq as zeros. We change the basis to the set
φ1(x), . . . , φq(x), s(x)φ1(x), . . . , s(x)φp+q(x).
In the system (5.3.3), the equations for k = 1 : q will not be changed, but the equations for
k = 1 + q : p + 2q become
p

i=1
φk′(xi)s(xi)ai +
q

j=1
φk′(zj)s(zj)bj = L(sφk′),
1 ≤k′ ≤p + q.
(5.3.5)
Here the second sum disappears since s(zj) = 0 for all j. (This is the nice feature of this
treatment.) Further, by (5.3.4),
L(sφk′) = L(φk′φq+1) −
q

j=1
L(φk′φj)gj,
1 ≤k′ ≤p + q.
(5.3.6)
We thus obtain the following linear system for the computation of the p + q quantities, gj,
and Ai = s(xi)ai:
q

j=1
L(φk′φj)gj +
p

i=1
φk′(xi)Ai = L(φk′φq+1),
k′ = 1 : p + q.
(5.3.7)
The weights of the ﬁxed nodes are ai = Ai/s(xi). The free nodes zj are then determined by
ﬁnding the q roots of the polynomial s(x). Methods for computing roots of a polynomial
are given in Sec. 6.5. Finally, with ai and zj known, the weights bj are obtained by the
solution of the ﬁrst q equations of the system (5.3.3) which are linear in bj.

5.3. Quadrature Rules with Free Nodes
567
The remainder term Rf = (Lf −˜Lf ) of the method, exact for all polynomials of
degree less than N = p + 2q, is of the form
Rf = R(f −PN) ≈cNf (N)(ξ),
cN = R(xN)/N!,
where cN is called the error constant. Note that R(xN) = R(φN+1), where φ(N+1) is any
monic polynomial of degree N, since xN −φ(N+1) is a polynomial of degree less than N.
Hence, for the determination of the error constant we compute the difference between the
right-hand and the left-hand sides of
p

i=1
φk(xi) ai +
q

j=1
φk(zj) bj + N!cN = LφN+1,
N = p + 2q,
(5.3.8)
and divide by (N)!. If, for example, a certain kind of symmetry is present, then it can
happen that cp+2q = 0. The formula is then more accurate than expected, and we take
N = p + 2q + 1 instead. The case that also cp+2q+1 = 0 may usually be ignored. It can
occur if several of the given nodes are located, where free nodes would have been placed.
From a pure mathematical point of view all bases are equivalent, but equation (5.3.3)
may be better conditioned with some bases than with others, and this turns out to be an
important issue when p + 2q is large. We mention three different situations.
(i) The most straightforward choice is to set [a, b] = [0, 1] and use the monomial basis
φk(x) = xk−1, x ∈(0, b) (b may be inﬁnite). For this choice the condition number of
(5.3.3) increases exponentially with p + 2q. Then the free nodes and corresponding
weights may become rather inaccurate when p + 2q is large. It is usually found,
however, that unless the condition number is so big that the solution breaks down
completely, the computed solution will satisfy equation (5.3.3) with a small residual.
This is what really matters for the application of formula (5.3.1).
(ii) Take [a, b] = [−1, 1], and assume that the weight function w(x) and the given nodes
xi are symmetrical with respect to the origin. Then the weights ai and bi, and the free
nodes zj will also be symmetrically located, and with the monomial basis it holds that
L(φk(x)) = 0, when k is even. If p = 2p′ is even, the number of parameters will be
reduced to p′ + q by the transformation x = √ξ, ξ ∈[0, b2]. Note that w(x) will be
replaced by w(√ξ)/√ξ. If p is odd, one node is at the origin, and one can proceed
in an analogous way. This should also reduce the condition number approximately to
its square root, and it is possible to derive in a numerically stable way formulas with
about twice as high an order of accuracy as in the unsymmetric case.
(iii) Taking φk to be the orthogonal polynomials for the given weight function will give
a much better conditioned system for determining the weights. This case will be
considered in detail in Sec. 5.3.5.
Example 5.3.1.
Consider the linear functional L(f ) =
 1
0 f (x) dx. Set p = 0, q = 3 and choose the
monomial basis φi(x) = xi−1. Introducing the node polynomial
s(x) = (x −z1)(x −z2)(x −z3) = x3 −s3x2 −s2x −s1,

568
Chapter 5. Numerical Integration
the linear system (5.3.6) becomes
 1
1/2
1/3
1/2
1/3
1/4
1/3
1/4
1/5
  s1
s2
s3

=
 1/4
1/5
1/6

.
The exact solution is s1 = 1/20, s2 = −3/5, and s3 = 3/2. The free nodes thus are the
zeros of s(x) = x3 −3x2/2 + 3x/5 −1/20, which are z2 = 1/2 and z1,3 = 1/2 ± √3/20.
The weights b1, b2, b3 are then found by solving (5.3.3) for k = 1 : 3.
The matrix of the above system is a Hankel matrix. The reader should verify that when
p > 0 the matrix becomes a kind of combination of a Hankel matrix and a Vandermonde
matrix.
5.3.2
Gauss–Christoffel Quadrature Rules
Assume that the n nodes in a quadrature formula are chosen so that
(f, s) =
 b
a
p(x)s(x)w(x) dx = 0
∀p(x) ∈Pn,
(5.3.9)
where s(x) = (x−x1)(x−x2) · · · (x−xn) is the node polynomial. Then, by Theorem 5.1.3,
the corresponding interpolatory quadrature rule will have the maximum possible order
2n −1.
We deﬁne an inner product with respect to a weight function w(x) ≥0 by
(f, g) =
 b
a
f (x)g(x)w(x) dx,
(5.3.10)
and assume that the moments
µk = (xk, 1) =
 b
a
xkw(x) dx
(5.3.11)
are deﬁned for all k ≥0, and µ0 > 0. This inner product has the important property that
(xf, g) = (f, xg). The condition (5.3.9) on the node polynomial can then be interpreted to
mean that s(x) is orthogonal to all polynomials in Pn.
For the weight function w(x) ≡1 the corresponding quadrature rules were derived in
1814 by Gauss [136]. Formulas for more general weight functions were given by Christof-
fel [69] in 1858,173 which is why these are referred to as Gauss–Christoffel quadrature
rules.
The construction of Gauss–Christoffel quadrature rules is closely related to the the-
ory of orthogonal polynomials. In Sec. 4.5.5 we showed how the orthogonal polynomials
corresponding to the inner product (5.3.10) could be generated by a three-term recurrence
formula. The zeros of these polynomials are the nodes in a Gauss–Christoffel quadra-
ture formula. As for all interpolatory quadrature rules the weights can be determined by
173Elwin Bruno Christoffel (1829–1900) worked mostly in Strasbourg. He is best known for his work in geometry
and tensor analysis, which Einstein later used in his theory of relativity.

5.3. Quadrature Rules with Free Nodes
569
integrating the elementary Lagrange polynomials (5.1.7)
wi =
 b
a
ℓi(x)w(x) dx,
ℓi(x) =
n
&
j=1
j̸=i
(x −xj)
(xi −xj),
i = 1 : n.
InSec. 5.3.5wewilloutlineamorestablealgorithmthatdeterminesthenodesandweightsby
solvingtheeigenvalueproblemforasymmetrictridiagonalmatrixdeﬁnedbythecoefﬁcients
in the recurrence relation.
We shall now prove some important properties of Gauss–Christoffel quadrature rules
using the general theory of orthogonal polynomials.
Theorem 5.3.1.
The zeros xi, i = 1 : n, of the orthogonal polynomial polynomial ϕn+1(x) of degree
n, associated with the weight function w(x) ≥0 on [a, b], are real, distinct, and contained
in the open interval (a, b).
Proof. Let a < x1 < x2 < · · · < xm < b be the roots of ϕn+1(x) of odd multiplicity, which
lie in (a, b). At these roots ϕn+1(x) changes sign and therefore the polynomial q(x)ϕn+1(x),
where
q(x) = (x −x1)(x −x2) · · · (x −xm),
has constant sign in [a, b]. Hence,
 b
a
ϕn+1q(x)w(x) dx > 0.
But this is possible only if the degree of q(x) is equal to n. Thus m = n and the theorem
follows.
Corollary 5.3.2.
If x1, x2, . . . , xn are chosen as the n distinct zeros of the orthogonal polynomial ϕn+1
of degree n in the family of orthogonal polynomials associated with w(x), then the formula
 b
a
f (x)w(x) dx ≈
n

i=1
wif (xi),
wi =
 b
a
ℓi(x)w(x) dx,
(5.3.12)
is exact for polynomials of degree 2n −1.
Apart from having optimal degree of exactness equal to 2n −1, Gaussian quadrature
rules have several important properties, which we now outline.
Theorem 5.3.3.
All weights in a Gaussian quadrature rule are real, distinct, and positive.
Proof. Let
ℓi(x) =
n
&
j=1
j̸=i
(x −xj)
(xi −xj),
i = 1 : n,

570
Chapter 5. Numerical Integration
be the Lagrange polynomials. Then the quadrature formula (5.3.12) is exact for p(x) =
(ℓi(x))2, which is of degree 2(n −1). Further, ℓi(xj) = 0, j ̸= i, and therefore
 b
a
(ℓi(x))2w(x) dx = wi(ℓi(xi))2 = wi.
Since w(x) > 0 it follows that wi > 0.
Gaussian quadrature formulas can also be derived by Hermite interpolation on the
nodes xk, each counted as a double node and requiring that coefﬁcients of the derivative
terms should be zero. This interpretation gives a convenient expression for the error term
in Gaussian quadrature.
Theorem 5.3.4.
The remainder term in Gauss’ quadrature rule (5.3.12) with n nodes is given by the
formula
I[f ] −In(f ) = f (2n)(ξ)
(2n)!
 b
a
0
n
&
i=1
(x −xi)
12
w(x) dx = cnf (2n)(ξ),
a < ξ < b.
(5.3.13)
The constant cn can be determined by applying the formula to some polynomial of degree
2n.
Proof. Denote by q(x) the polynomial of degree 2n −1 which solves the Hermite interpo-
lation problem (see Sec. 4.3.1)
q(xi) = f (xi),
q′(xi) = f ′(xi),
i = 1 : n.
The Gauss quadrature formula is exact for q(x), and hence
 b
a
q(x)w(x) dx =
n

i=1
wiq(xi) =
n

i=1
wif (xi).
Thus
n

i=1
wif (xi) −
 b
a
f (x)w(x) dx =
 b
a
(q(x) −f (x))w(x) dx.
Using the remainder term (4.3.4) in Hermite interpolation gives
f (x) −q(x) = f (2n)(ξ)
(2n)! (ϕn(x))2,
ϕn(x) =
n
&
i=1
(x −xi),
and the theorem now follows.
Using Bernštein’s approximation theorem (Theorem 3.2.5) we get the following
corollary.

5.3. Quadrature Rules with Free Nodes
571
Corollary 5.3.5.
Let f real-valued for z ∈[−1, 1], and analytic and single-valued |f (z)| ≤M in the
region z ∈ER, R > 1, where
ER = {z : |z −1| + |z + 1| ≤R + R−1},
be an ellipse with foci at 1 and −1. Then the remainder term in a Gauss quadrature rule
with n nodes for the interval [−1, 1] satisﬁes
|I[f ] −In(f )| ≤
2Mµ0
1 −1/R R−2n.
(5.3.14)
This shows the rapid convergence of Gauss’quadrature rules for functions analytic in
a region ER, with R ≫1.
We now mention some classical Gauss–Christoffel quadrature rules, which are related
to the orthogonal polynomials surveyed in Sec. 4.5.5. For an integral
 1
−1 f (x) dx, with
uniformweightdistributionw(x) = 1, therelevantorthogonalpolynomialsaretheLegendre
polynomials Pn(x).
As a historical aside, Gauss derived his quadrature formula by considering the con-
tinued fraction
1
2
 1
−1
dx
z −x = 1
2 ln
z + 1
z −1

= 1
z−
1/3
z−
4/(3 · 5)
z−
9/(5 · 7)
z−
· · · ,
(5.3.15)
which he had derived in an earlier paper. The nth convergent of this continued fraction is a
rational function with a numerator of degree n −1 in z and denominator of degree n which
is the (n −1, n) Padé approximant to the function. Decomposing this fraction in partial
fractions the residues and the poles can be taken as nodes of a quadrature formula. Using
the accuracy properties of the Padé approximants Gauss showed that the quadrature formula
will have order 2n −1.
The reciprocal of the denominators’ polynomials Pn(z) = znQn(1/z) are precisely
the Legendre polynomials; see Example 3.5.6. Recall that the monic Legendre polynomials
satisfy the recurrence formula P0 = 1, P1 = x,
Pn+1(x) = xPn(x) −
n2
4n2 −1Pn−1(x),
n ≥1.
The ﬁrst few monic Legendre polynomials are
P2(x) = 1
3(3x2 −1),
P3(x) = 1
5(5x3 −3x),
P4(x) = 1
35(35x4 −30x2 + 3),
P5(x) = 1
63(63x5 −70x3 + 15x), . . . .
Example 5.3.2.
For a two-point Gauss–Legendre quadrature rule the two abscissae are the zeros of
P2(x) = 1
3(3x2 −1), i.e., ±3−1/2. Note that they are symmetric with respect to the origin.
The weights can be determined by application of the formula to f (x) = 1 and f (x) =
x, respectively. This gives
w0 + w1 = 2,
−3−1/2w0 + 3−1/2w1 = 0,

572
Chapter 5. Numerical Integration
with solution w0 = w1 = 1. Hence the formula
 1
−1
f (x) dx ≈f (−3−1/2) + f (3−1/2)
is exact for polynomials of degree ≤3. For a three-point Gauss formula, see Problem 5.3.1.
Abscissae and weights for Gauss formulas using n = m + 1 points, for n = 2 : 10,
with 15 decimal digits and n = 12, 16, 20, 24, 32, 40, 48, 64, 80, and 96 with 20 digits are
tabulated in [1, Table 25.4]; see Table 5.3.1 for a sample. Instead of storing these constants,
it might be preferable to use a program that generates abscissae and weights as needed.
Table 5.3.1. Abscissae and weight factors for some Gauss–Legendre quadrature
from [1, Table 25.4].
xi
wi
n = 3
0.00000 00000 00000
0.88888 88888 88889
±0.77459 66692 41483
0.55555 55555 55556
n = 4
±0.33998 10435 84856
0.65214 51548 62546
±0.86113 63115 94053
0.34785 48451 37454
n = 5
0.00000 00000 00000
0.56888 88888 88889
±0.53846 93101 05683
0.47862 86704 99366
±0.90617 98459 38664
0.23692 68850 56189
For the weight function
w(x) = (1 −x)α(1 + x)β,
x ∈[−1, 1],
α, β > −1,
the nodes are obtained from the zeros of the Jacobi polynomials Jn(x; α, β). In the special
case when α = β = 0 these equal the Legendre polynomials. The case α = β = −1/2,
which corresponds to the weight function w(x) = 1/
√
1 −x2, gives the Chebyshev poly-
nomials Tn(x) of the ﬁrst kind. Similarly, α = β = 1/2 gives the Chebyshev polynomials
Un(x) of the second kind.
If a quadrature rule is given for the standard interval [−1, 1], the corresponding
formula for an integral over the interval [a, b] is obtained by the change of variable t =
1
2((b −a)x + (a + b)), which maps the interval [a, b] onto the standard interval [−1, 1]:
 b
a
f (t)dt = b −a
2
 1
−1
g(x) dx,
g(x) = f
1
2

(b −a)x + (a + b)

.
If f (t) is a polynomial, then g(x) will be a polynomial of the same degree, since the
transformation is linear. Hence the order of accuracy of the formula is not affected.

5.3. Quadrature Rules with Free Nodes
573
Two other important cases of Gauss quadrature rules deal with inﬁnite intervals of
integration. The generalized Laguerre polynomials L(α)
n (x) are orthogonal with respect to
the weight function
w(x) = xαe−x,
x ∈[0, ∞],
α > −1.
Setting α = 0, we get the Laguerre polynomials L(0)
n (x) = Ln(x).
The Hermite polynomials are orthogonal with respect to the weight function
w(x) = e−x2,
−∞< x < ∞.
Recall that weight functions and recurrence coefﬁcients for the above monic orthogonal
polynomials are given in Table 4.5.1.
Rather little is found in the literature on numerical analysis about densities on inﬁ-
nite intervals, except the classical cases above. It follows from two classical theorems of
Hamburger in 1919 and M. Riesz in 1923 that the system of orthogonal polynomials for the
density w over the inﬁnite interval [−∞, ∞] is complete if, for some β > 0,
 ∞
−∞
eβ|x|w(x) dx < ∞;
see Freud [126, Sec. II.4–5]. For densities on [0, ∞], x is to be replaced by √x in the above
result. (Note that a density function on the positive real x-axis can be mapped into an even
density function on the whole real t-axis by the substitution x = t2.
5.3.3
Gauss Quadrature with Preassigned Nodes
In many applications it is desirable to use Gauss-type quadrature where some nodes are
preassigned and the rest chosen to maximize the order of accuracy. In the most common
cases the preassigned nodes are at the endpoints of the interval. Consider a quadrature rule
of the form
 b
a
f (x)w(x) dx =
n

i=1
wif (xi) +
m

j=1
bjf (zj) + R(f ),
(5.3.16)
where zj, j = 1 : m, are ﬁxed nodes in [a, b] and the xi are determined so that the
interpolatory rule is exact for polynomials of order 2n + m −1. By a generalization of
Theorem 5.3.4 the remainder term is given by the formula
R(f ) = f (2n+m)(ξ)
(2n)!
 b
a
m
&
i=1
(x −zi)
*
n
&
i=1
(x −xi)
+2
w(x) dx,
a < ξ < b.
(5.3.17)
In Gauss–Lobatto quadrature both endpoints are used as abscissae, z1 = a, z2 = b,
and m = 2. For the standard interval [a, b] = [−1, 1] and the weight function w(x) = 1,
the quadrature formula has the form
 1
−1
f (x) dx = w0f (−1) + wn+1f (1) +
n

i=1
wif (xi) + EL.
(5.3.18)

574
Chapter 5. Numerical Integration
The abscissae a < xi < b are the zeros of the orthogonal polynomial φn corresponding
to the weight function ˜w(x) = (1 −x2), i.e., up to a constant factor equal to the Jacobi
polynomial Jn(x, 1, 1) = P ′
n+1(x). The nodes lie symmetric with respect to the origin. The
corresponding weights satisfy wi = wn+1−i, and are given by
w0 = wn+1 =
2
(n + 2)(n + 1),
wi =
w0
(Pn+1(xi))2 ,
i = 1 : n.
(5.3.19)
TheLobattorule(5.3.18)isexactforpolynomialsoforder2n+1, andforf (x) ∈C2m[−1, 1]
the error term is given by
R(f ) = −(n + 2)(n + 1)322n+3(n!)4
(2n + 3)[(2n + 2)!]3
f (2n+2)(ξ),
ξ ∈(−1, 1).
(5.3.20)
Nodes and weights for Lobatto quadrature are found in [1, Table 25.6].
In Gauss–Radau quadrature rules m = 1 and one of the endpoints is taken as the
abscissa, z1 = a or z1 = b. The remainder term (5.3.17) becomes
R(f ) = f (2n+1)(ξ)
(2n)!
 b
a
(x −z1)
0
n
&
i=1
(x −xi)
12
w(x) dx,
a < ξ < b.
(5.3.21)
Therefore, if the derivative f (n+1)(x) has constant sign in [a, b], then the error in the Gauss–
Radau rule with z1 = b will have opposite sign to the Gauss–Radau rule with z1 = a. Thus,
by evaluating both rules we obtain lower and upper bounds for the true integral. This has
many applications; see Golub [163].
For the standard interval [−1, 1] the Gauss–Radau quadrature formula with z1 = 1
has the form
 1
−1
f (x) dx = w0f (−1) +
n

i=1
wif (xi) + ER1.
(5.3.22)
The n free abscissae are the zeros of
Pn(x) + Pn+1(x)
x −1
,
where Pm(x) are the Legendre polynomials. The corresponding weights are given by
w0 =
2
(n + 1)2 ,
wi =
1
(n + 1)2
1 −xi
(Pn(xi))2 ,
i = 1 : n.
(5.3.23)
The Gauss–Radau quadrature rule is exact for polynomials of order 2n. Assuming that
f (x) ∈C2m−1[−1, 1], then the error term is given by
ER1(f ) = (n + 1)22n+1
[(2n + 1)!]3 (n!)4f (2n+1)(ξ1),
ξ1 ∈(−1, 1).
(5.3.24)
Asimilar formula can be obtained with the ﬁxed point +1 by making the substitution t = −x.
By modifying the proof of Theorem 5.3.3 it can be shown that the weights in Gauss–
Radau and Gauss–Lobatto quadrature rules are positive if the weight function w(x) is
nonnegative.

5.3. Quadrature Rules with Free Nodes
575
Example 5.3.3.
The simplest Gauss–Lobatto rule is Simpson’s rule with n = 1 interior node. Taking
n = 2 the interior nodes are the zeros of φ2(x), where
 1
−1
(1 −x2)φ2(x)p(x) dx = 0
∀p ∈P2.
Thus, φ2 is, up to a constant factor, the Jacobi polynomial J2(x, 1, 1) = (x2 −1/5). Hence
the interior nodes are ±1/
√
5 and by symmetry the quadrature formula is
 1
−1
f (x) dx = w0(f (−1) + f (1)) + w1(f (−1/
√
5) + f (1/
√
5)) + R(f ),
(5.3.25)
where R(f ) = 0 for f ∈P6. The weights are determined by exactness for f (x) = 1 and
f (x) = x2. This gives 2w0 + 2w1 = 2, 2w0 + (2/5)w1 = 2
3, i.e., w0 = 1
6, w1 = 5
6.
Aserious drawback with Gaussian rules is that as we increase the order of the formula,
all interior abscissae change, except that at the origin. Thus function values computed for
the lower-order formula are not used in the new formula. This is in contrast to Romberg’s
method and Clenshaw–Curtis quadrature rules, where all old function values are used also
in the new rule when the number of points is doubled.
Let Gn be an n-point Gaussian quadrature rule
 b
a
f (x)w(x) dx ≈
n−1

i=0
aif (xi),
where xi, i = 0 : n −1, are the zeros of the nth degree orthogonal polynomial πn(x).
Kronrod [232, 233] considered extending Gn by ﬁnding a new quadrature rule
K2n+1 =
n−1

i=0
aif (xi) +
n

i=0
bif (yi),
(5.3.26)
where the new n + 1 abscissae yi are chosen such that the degree of the rule K2n+1 is equal
to 3n + 1. The new nodes yi should then be selected as the zeros of a polynomial pn+1(x)
of degree n + 1, satisfying the orthogonality conditions
 b
a
πn(x)pn+1(x)w(x) dx = 0.
(5.3.27)
If the zeros are real and contained in the closed interval of integration [a, b] such a rule is
called a Kronrod extension of the Gaussian rule. The two rules (Gn, K2n+1) are called a
Gauss–Kronrod pair. Note that the number of new function evaluations are the same as
for the Gauss rule Gn+1.
It has been proved that a Kronrod extension exists for the weight function w(x) =
(1 −x2)λ−1/2, λ ∈[0, 2], and [a, b] = [−1, 1]. For this weight function the new nodes
interlace the original Gaussian nodes, i.e.,
−1 ≤y0 < x0 < y1 < x1 < y2 < · · · < xn−1 < yn < 1.

576
Chapter 5. Numerical Integration
This interlacing property can be shown to imply that all weights are positive. Kronrod
considered extensions of Gauss–Legendre rules, i.e., w(x) = 1, and gives nodes and weights
in [233] for n ≤40.
It is not always the case that all weights are positive.
For example, it has been
shown that Kronrod extensions of Gauss–Laguerre and Gauss–Hermite quadrature rules
with positive weights do not exist when n > 0 in the Laguerre case and n = 3 and n > 4 in
the Hermite case. On the other hand, the Kronrod extensions of Gauss–Legendre rules can
be shown to exist and have positive weights.
Gauss–Kronrod rules are one of most effective methods for calculating integrals.
Often one takes n = 7 and uses the Gauss–Kronrod pair (G7, K15), together with the
realistic but still conservative error estimate (200|Gn −K2n+1|)1.5; see Kahaner, Moler, and
Nash [220, Sec. 5.5].
Kronrod extension of Gauss–Radau and Gauss–Lobatto rules can also be constructed.
Kronrod extension of the Lobatto rule (5.3.25) is given by Gander and Gautschi [131] and
used in an adaptive Lobatto quadrature algorithm. The simplest extension is the four-point
Lobatto–Kronrod rule
 1
−1
f (x) dx = 11
210(f (−1) + f (1)) + 72
245

f

−
$
2
3

+ f
$
2
3

+ 125
294

f

−1
√
5

+ f
 1
√
5

+ 16
35f (0) + R(f ).
(5.3.28)
This rule is exact for all f ∈P10. Note that the Kronrod points ±√2/3 and 0 interlace the
previous nodes.
5.3.4
Matrices, Moments, and Gauss Quadrature
We ﬁrst collect some classical results of Gauss, Christoffel, Chebyshev, Stieltjes, and others,
with a few modern aspects and notations appropriate for our purpose.
Let {p1, p2, . . . , pn}, where pj is of exact degree j −1, be a basis for the space Pn
of polynomials of degree n −1. We introduce the row vector
π(x) = [p1(x), p1(x), . . . , pn(x)]
(5.3.29)
containing these basis functions. The modiﬁed moments with respect to the basis π(x) are
νk = (pk, 1) =
 b
a
pk(x)w(x) dx,
k = 1 : n.
(5.3.30)
We deﬁne the two symmetric matrices
G =

π(x)T π(x)w(x) dx,
ˆG =

xπ(x)T π(x)w(x) dx
(5.3.31)
associated with the basis deﬁned by π. These have elements
gij = (pi, pj) = (pj, pi),
ˆgij = (xpi, pj) = (xpj, pi),

5.3. Quadrature Rules with Free Nodes
577
respectively. Here
G =


(p1, p1)
(p1, p2)
. . .
(p1, pn)
(p2, p1)
(p2, p2)
. . .
(p2, pn)
...
...
...
...
(pn, p1)
(pn, p2)
. . .
(pn, pn)


(5.3.32)
is called the Gram matrix.
In particular, for the power basis
θ(x)(1, x, x2, . . . , xn−1)
(5.3.33)
we have gij = (xi−1, xj−1) = µi+j−2, where µk = (xk, 1) =
 b
a xkw(x) dx are the
ordinary moments. In this case the matrices G and ˆG are the Hankel matrices,
G =


µ0
µ1
· · ·
µn−1
µ1
µ2
· · ·
µn
...
...
· · ·
...
µn−1
µn
· · ·
µ2n−2

,
ˆG =


µ1
µ2
· · ·
µn
µ2
µ3
· · ·
µn+1
...
...
· · ·
...
µn
µn+1
· · ·
µ2n−1

.
In particular, for w(x) ≡1 and [a, b] = [0, 1] we have µk =
 1
0 xk−1 dx = 1/k, and G is
the notoriously ill-conditioned Hilbert matrix.
Let u and v be two polynomials in Pn and set
u(x) = π(x)uπ,
v(x) = π(x)vπ,
where uπ, vπ are column vectors with the coefﬁcients in the representation of u and v with
respect to the basis deﬁned by π(x). Then
(u, v) =
 b
a
uT
ππ(x)T π(x)vπw(x) dx = uT
πGvπ.
For u = v ̸= 0 we ﬁnd that uT
πGuπ = (u, u) > 0, i.e., the Gram matrix G is positive
deﬁnite. (The matrix ˆG is, however, usually indeﬁnite.)
A polynomial of degree n that is orthogonal to all polynomials of degree less than n
can be written in the form
φn+1(x) = xpn(x) −π(x)cn,
cn ∈Rn.
(5.3.34)
Here cn is determined by the linear equations
0 = (π(x)T , φn+1(x)) = (π(x)T , xpn(x)) −(π(x)T , π(x))cn,
or in matrix form
Gcn = ˆgn,
(5.3.35)
where ˆgn = ˆGen is the last column of the matrix ˆG. Further, there are coefﬁcients ck,j
depending on the basis only such that
xpj(x) =
j+1

k=1
ck,jpk(x),
j = 1 : n −1.

578
Chapter 5. Numerical Integration
Together with (5.3.34) this can be summarized in the (row) vector equation
xπ(x) = π(x)C + φn+1(x)eT
n ,
C = (C, cn).
(5.3.36)
Here eT
n = (0, 0, . . . , 1) and C = (ck,j) ∈Rn×(n−1] is an upper Hessenberg matrix. Note
that C depends on the basis only, while cn also depends on the weight function.
For the power basis pj(x) = xj−1, the matrix C is a shift matrix; the only nonzero
elements are ones in the ﬁrst main subdiagonal. If the basis is some family of orthogonal
polynomials (possibly with respect to weight function other than w) C is a tridiagonal
matrix, obtained by means of the three-term recurrence relation for this family.
After multiplication of (5.3.36) by π(x)T w(x) and integration we obtain by (5.3.31)
GC = ˆG,
(5.3.37)
where the last column of this matrix equation is the same as (5.3.35). Let G∗, C∗be deﬁned
like G, C, with n increased by one. Note that G and C are principal submatrices of G∗and
C∗. Then ˆG equals the n ﬁrst rows of the product G∗C∗. Thus, no integrations are needed
for gn, except for the Gram matrix G.
Theorem 5.3.6.
Denote by R the matrix of coefﬁcients of the expansions of the general basis functions
π(x) = [p1(x), p1(x), . . . , pn(x)] into the orthonormal basis polynomials with respect to
the weight function w, i.e.,
π(x) = ϕ(x)R,
ϕ(x) = (φ1(x), φ2(x), . . . , φn(x)).
(5.3.38)
(Conversely, the coefﬁcients of the expansions of the orthogonal polynomials into the orig-
inal basis functions are found in the columns of R−1.) Then G = RTR, i.e., R is the upper
triangular Cholesky factor of the Gram matrix G. Note that up to the mth row this factor-
ization is the same for all n ≥m. Further, ˆG = RT JR, where J is a symmetric tridiagonal
matrix.
Proof. R is evidently an upper triangular matrix. Further, we have
G =

π(x)T π(x)w(x) dx =

RT ϕ(x)T ϕ(x)Rw(x) dx
= RT IR = RTR,
since the elements of ϕ(x) are an orthonormal system. This shows that R is the Cholesky
factor of G. We similarly ﬁnd that
ˆG = RT JR,
J =

xϕ(x)T ϕ(x)w(x) dx,
and thus J clearly is a symmetrical matrix. J is a particular case of ˆG and from (5.3.37)
and G = I it follows that J = C, a Hessenberg matrix. Hence J is a symmetric tridiagonal
matrix.

5.3. Quadrature Rules with Free Nodes
579
From (5.3.37) and Theorem 5.3.6 it follows that
ˆG = RT JR = GC = RTRC.
Since R is nonsingular we have RC = JR, or
J = RCR−1.
(5.3.39)
This shows that the spectrum of C equals the spectrum of J, for every choice of basis. We
shall see that it is equal to the set of zeros of the orthogonal polynomial φn+1. For the power
basis pj(x) = xj−1 (5.3.34) reads
φn+1(x) = xn −
n

k=1
cn,kxk−1,
and hence
C =


0
cn,1
1
0
cn,2
1
...
...
...
0
cn,n−1
1
cn,n


∈Rn×n.
This is the companion matrix of φn+1(x), and it can be shown that (see Sec. 6.5.2)
det(zI −C) = φn+1(x).
(5.3.40)
Thus the eigenvalues λj, j = 1 : n, of C are the zeros of φn+1(x), and hence the nodes for
the Gauss–Christoffel quadrature formula.
It can be veriﬁed that the row eigenvector of G corresponding to λj is
θ(λj) = (1, λj, λ2
j, . . . , λn−1
j
);
(5.3.41)
i.e., it holds that
θ(λj)C = λjθ(λj),
j = 1 : n.
(5.3.42)
This yields a diagonalization of C, since, by the general theory of orthogonal polynomials
(see Theorem 5.3.3), the roots are simple roots, located in the interior of the smallest interval
that contains the weight distribution.
To summarize, we have shown that if C and the Gram matrix G are known, then cn
can be computed by performing the Cholesky decomposition G = RTR and then solving
RTRcn = ˆgn for cn. The zeros of φn+1(x) are then equal to the eigenvalues of C = (C, cn)
or, equivalently, the eigenvalues of the symmetric tridiagonal matrix J = RCR−1. This
is true for any basis π(x). Note that J can be computed by solving the matrix equation
JR = RC or
RT J = (RC)T .
(5.3.43)
Here RT is a lower triangular matrix and the right-hand side a lower Hessenberg matrix.
This and the tridiagonal structure of J considerably simpliﬁes the calculation of J. In the
next section we show how the theory developed here leads to a stable and efﬁcient algorithm
for computing Gauss quadrature rules.

580
Chapter 5. Numerical Integration
5.3.5
Jacobi Matrices and Gauss Quadrature
The computations are most straightforward for the power basis, θ(x), using the moments
of the weight function as the initial data. But the condition number of the Gram matrix G,
which in this case is a Hankel matrix, increases rapidly with n. This is related to the by now
familiar fact that, when n is large, xn can be accurately approximated by a polynomial of
lower degree. Thus the moments for the power basis are not generally a good starting point
for the numerical computation of the matrix J.
For the orthonormal basis ϕ(x), we have G = I, and
C = ˆG = J =


β1
γ1
0
γ1
β2
γ2
γ2
...
...
...
γn−1
0
γn−1
βn


(5.3.44)
is a symmetric tridiagonal matrix with nonzero off-diagonal elements. Such a tridiagonal
matrix is called a Jacobi matrix and has n real distinct eigenvalues λj. The row eigenvectors
ϕ(λj) satisfy
ϕ(λj)J = λjϕ(λj),
j = 1 : n,
(5.3.45)
and are mutually orthogonal. Setting
M = (ϕ(λ1)T , . . . , ϕ(λn)T ),
X = diag (λ1, . . . , λn),
we obtain by (5.3.45) and the symmetry of J the important matrix formula
JM = MX.
(5.3.46)
It also follows from (5.3.36) that for all x
xϕ(x) = Jϕ(x) + γnφn+1(x)eT
n ,
(5.3.47)
where γn is to be chosen so that ∥φn+1∥= 1. The last column of this equation gives
(x −βn)φn(x) = γn−1φn−1(x) + γnφn+1(x),
(5.3.48)
which is the three-term recurrence relation (4.5.36) for orthogonal polynomials.
Let V be an orthogonal matrix that diagonalizes J, i.e.,
JV = V X,
V T V = V V T = I,
where X is the diagonal in (5.3.46). It follows that V = MD for some diagonal matrix
D = diag (di), and
V V T = MD2MT = I,
i.e.,
n

k=1
d2
k φi(λk)φj(λk) = δij = (φi, φj),
i, j = 1 : n.

5.3. Quadrature Rules with Free Nodes
581
This equality holds also for i = n + 1, because φn+1(λk) = 0, for all k, and (φn+1, φj) = 0,
j = 1 : k.
Since every polynomial p of degree less than 2n can be expressed as a linear combi-
nation of polynomials of the form φiφj (in inﬁnitely many ways) it follows that
n

k=1
d2
k p(λk) =

p(x)w(x) dx,
(5.3.49)
for any polynomial p of degree less than 2n. This yields the Gauss–Christoffel quadrature
rule:

f (x)w(x) dx =
n

k=1
d2
k f (λk) + R,
(5.3.50)
where
R =

(f (x) −p(x))w(x) dx,
for any polynomial p of degree less than 2n such that p(λk) = f (λk), k = 1 : n.
The familiar form for the remainder term
R = knf (2n)(ξ)/(2n)!
(5.3.51)
is obtained by choosing a Hermite interpolation polynomial for p and then applying the
mean value theorem. The constant kn is independent of f . The choice f (x) = A2
nx2n + · · ·
gives kn = A−2
n . Arecurrence relation for the leading coefﬁcient Aj is obtained by (5.3.48).
We obtain
A0 = µ−1/2
0
,
Ak+1 = Ak/γk.
(5.3.52)
The mean value form for R may be inappropriate when the interval is inﬁnite. Some other
estimate of the above integral for R may then be more adequate, for example, in terms of
the best approximation of f by a polynomial in some weighted Lp-norm.
Asimple formula for the weights d2
k , due to Golub andWelsch, is obtained by matching
the ﬁrst rows of the equality V = MD. Since the elements in the ﬁrst row of M are all equal
to the constant φ1 = µ−1/2
0
, we obtain
eT
1 V = µ−1/2
0
dT ,
d2
k = µ0v2
1,k,
k = 1 : n.
(5.3.53)
The well-known fact that the weights are positive and their sum equals µ0 follows immedi-
ately from this simple formula for the weights. We summarize these results in the following
theorem.
When the three-term recurrence relation for the orthonormal polynomials associated
with the weight function w(x) is known, or can be computed by the Stieltjes procedure in
Sec. 4.5.5, the Gauss–Christoffel rule can be obtained elegantly as follows. The nodes of
the Gauss–Christoffel rule are the eigenvalues of the tridiagonal matrix J, and by (5.3.53)
the weights equal the square of the ﬁrst components of the corresponding eigenvectors.
These quantities can be computed in a stable and efﬁcient way by the QR algorithm; see
Volume II. In [162] this scheme is extended to the computation of nodes and weights for
Gauss–Radau and Gauss–Lobatto quadrature rules.

582
Chapter 5. Numerical Integration
When the coefﬁcients in the three-term relation cannot be obtained by theoretical
analysis or numerical computation, we consider the matrices C and G = RTR as given data
about the basis and weight function. Then J can be computed by means of (5.3.39) and the
nodes and weights are computed according to the previous case. Note that R and J can be
determined simultaneously for all k ≤n; just take the submatrices of the largest ones.
The following concise and applicable result was found independently by Golub and
Meurant (see [165, Theorem 3.4]) and the ﬁrst-named author (see [88, Theorem 2.2]).
Theorem 5.3.7.
Let J be the symmetric tridiagonal n × n matrix that contains the coefﬁcients in the
three-term recurrence relation for the orthogonal polynomials associated with a positive
weight function w(x) (with any sequence of leading coefﬁcients). Let e1 = (1, 0, 0, . . . , 0)T
and f be an analytic function in a domain that contains the spectrum of J.
Then the formula
1
µ0

f (x)w(x) dx ≈eT
1 f (J)e1
(5.3.54)
is exact when f is a polynomial of degree less than 2n.
Proof. If J = V XV T is the spectral decomposition of J, then we have
f (J) = V T diag (f (λ1, . . . , f (λn)))V.
Let p be a polynomial of degree less than 2n. We obtain using (5.3.53)
eT
1 V XV T eT
1 = µ−1/2
0
dT p(X)µ−1/2
0
d = µ−1
0
n

j=1
p(λj)d2
j = µ−1
0

p(x)w(x) dx,
since Gauss–Christoffel quadrature is exact for p.
If f (J) is evaluated by means of the diagonalization of J, (5.3.54) becomes exactly
the Gauss–Christoffel rule, but it is noteworthy that eT
1 V T f (X)V e1 can sometimes be
evaluated without a diagonalization of J. The accuracy of the estimate of the integral still
depends on how well f (z) can be approximated by a polynomial of degree less than twice
the size of J in the weighted L1-norm with weight function w(x).
In many important cases the weight function w(x) is symmetric about the origin.
Then the moments of odd order are zero, and the orthogonal polynomials of odd (even)
degree are odd (even) functions. By Theorem 4.5.19 the coefﬁcients βk = 0 for all k, i.e.,
the matrix J will have a zero diagonal. The eigenvalues of J will then appear in pairs, ±λk.
If n is odd, there is also a simple zero eigenvalue. The weights are symmetric so that the
weights corresponding to the two eigenvalues ±λi are the same.
We shall see that in the symmetric case the eigenvalue problem for the tridiagonal
matrix J ∈Rn×n can be reduced to a singular value problem for a smaller bidiagonal matrix
B, where
B ∈
%
Rn/2×n/2
if n even,
R(n+1)/2×(n−1)/2
if n odd.

5.3. Quadrature Rules with Free Nodes
583
We permute rows and columns in J, by an odd-even permutation; for example, if n = 7,
then (1, 2, 3, 4, 5, 6, 7) 9→(1, 3, 5, 7, 2, 4, 6), and
˜J = T −1JT =

0
B
BT
0

,
B =


γ1
0
0
γ2
γ3
0
0
γ4
γ5
0
0
γ6

,
where T is the permutation matrix effecting the permutation. Then, J and ˜J have the same
eigenvalues. If the orthogonal matrix V diagonalizes J, i.e., J = V XV T , then ˜V = T −1V
diagonalizes ˜J = T T JT , i.e., ˜J = T −1JT = T −1V λV T T . Note that the ﬁrst row of V is
just a permutation of ˜V . We can therefore substitute ˜V for V in equation (5.3.53), which
gives the weights in the Gauss–Christoffel formula.
The following relationship between the singular value decomposition (SVD) and a
Hermitian eigenvalue problem, exploited by Lanczos [236, Chap. 3], can easily be veriﬁed.
Theorem 5.3.8.
Let the SVD of B ∈Rm×n (m ≥n) be B = PHQT , where
H = diag (H1, 0),
H1 = diag (σ1, σ2, . . . , σn),
and
P = (P1, P2) ∈Cm×m,
P1 ∈Cm×n,
Q ∈Cn×n.
Then the symmetric matrix C ∈R(m+n)×(m+n) has the eigendecomposition
C =

0
B
BT
0

= V
 H1
0
0
0
0
0
0
0
−H1

V T ,
(5.3.55)
where V ∈is orthogonal:
V =
1
√
2

P1
√
2 P2
P1
Q
0
−Q
T
.
(5.3.56)
Hence the eigenvalues of C are ±σ1, ±σ2, . . . , ±σr, and zero repeated (m −n) times.
The QR algorithm for symmetric tridiagonal matrices can be adopted to compute the
singular values σi and the ﬁrst components of the matrix P = (P1, P2) of left singular
vectors of the bidiagonal matrix B; see Volume II.
Example 5.3.4.
The monic Legendre polynomials are symmetric around the origin, and thus βn = 0
for all n and µ0 = 2. According to (4.5.56) we have
γn =
n
√
4n2 −1
=
1
√
4 −n−2 .

584
Chapter 5. Numerical Integration
Algorithm 5.4. Gauss–Legendre Quadrature.
The following MATLAB function computes the nodes and weights of the Gauss–Legendre
rule with n points by generating the bidiagonal matrix B and its SVD.
function [x,w] = legendre(n);
% LEGENDRE(n) computes the nodes and weights in the
% Gauss-Legendre quadrature rule with n+1 nodes (n > 1).
%
gamma = 1./sqrt(4 - [1:n].ˆ(-2));
gamma(n+1) = 0;
b0(1) = gamma(1:2:n+1);
b1(k) = gamma(2:2:n);
B = diag(b0,0) + diag(b1,1);
[P,S,Q] = svd(B);
x = diag(S); [x,i] = sort(x);
w = P(1,i).ˆ2;
if rem(n,2) == 0 w(1) = 2*w(1); end
For n = 6 the upper bidiagonal matrix becomes
B =


1/
√
3
2/
√
15
3/
√
35
4/
√
63
5/
√
99
6/
√
143
0

∈R4×4,
and we obtain the nonnegative nodes (cf. Table 5.3.1) x1 = 0,
x2 = 0.40584515137740,
x3 = 0.74153118559939,
x4 = 0.94910791234276.
The ﬁrst row of P = ( P1
P2 ) is
−0.45714285714286, −0.61792398440675, 0.52887181007242, −0.35984019532130,
where the ﬁrst entry corresponds to node x1 = 0. Dividing the last three components by
√
2, squaring, and multiplying with µ0 = 2, gives the weights
w1 = 0.41795918367347, w2 = 0.38183005050512, w3 = 0.27970539148928,
w4 = 0.12948496616887.
We remark that the given program is inefﬁcient in that the full matrices of left and
right singular vectors are computed. Unless n is very large the execution time is negligible
anyway.
In the computation of harmonic transforms used in spectral weather analysis, Gauss–
Legendre quadrature rules with values of n in excess of 1000 are required. Methods for
computing points and weights accurate to double precision for such high values of n are
discussed by Swarztrauber in [345].

Problems and Computer Exercises
585
Review Questions
5.3.1 What increase in order of accuracy can normally be achieved by a judicious choice
of the nodes in a quadrature formula?
5.3.2 What are orthogonal polynomials? Give a few examples of families of orthogo-
nal polynomials together with the three-term recursion formula, which its members
satisfy.
5.3.3 Formulate and prove a theorem concerning the location of zeros of orthogonal poly-
nomials.
5.3.4 Give an account of Gauss quadrature formulas, including accuracy and how the nodes
and weights are determined. What important properties are satisﬁed by the weights?
5.3.5 What is the orthogonality property of the Legendre polynomials?
Problems and Computer Exercises
5.3.1 Prove that the three-point quadrature formula
 1
−1
f (x) dx ≈1
9

5f

−
$
3
5

+ 8f (0) + 5f
$
3
5

is exact for polynomials of degree ﬁve. Apply it to the computation of
 1
0
sin x
1 + x dx,
and estimate the error in the result.
5.3.2 (a) Calculate the Hermite polynomials Hn for n ≤4 using the recurrence relation.
(b) Express, conversely, 1, x, x2, x3, x4 in terms of the Hermite polynomials.
5.3.3 (a) Determine the orthogonal polynomials φn(x), n = 1, 2, 3, with leading coefﬁ-
cient 1, for the weight function w(x) = 1 + x2, x ∈[−1, 1].
(b) Give a two-point Gaussian quadrature formula for integrals of the form
 1
−1
f (x)(1 + x2) dx
which is exact when f (x) is a polynomial of degree three.
Hint: Use either the method of undetermined coefﬁcients taking advantage of sym-
metry, or the three-term recurrence relation in Theorem 5.3.1.
5.3.4 (Gautschi)
(a) Construct the quadratic polynomial φ2 orthogonal on [0, ∞] with respect to the
weight function w(x) = e−x. Hint: Use
 ∞
0 tme−t dt = m!.

586
Chapter 5. Numerical Integration
(b) Obtain the two-point Gauss–Laguerre quadrature formula
 ∞
0
f (x)e−x dx = w1f (x1) + w2f (x2) + E2(f ),
including a representation for the remainder E2(f ).
(c) Apply the formula in (b) to approximate
I =
 ∞
0
(x + 1)−1e−x dx.
Use the remainder term to estimate the error, and compare your estimate with the
true error (I = 0.596347361 . . .).
5.3.5 Show that the formula
 1
−1
f (x)(1 −x2)−1/2 dx = π
n
n

k=1
f

cos 2k −1
2n
π

is exact when f (x) is a polynomial of degree at most 2n −1.
5.3.6 (a) Use the MATLAB program in Example 5.3.4 to compute nodes and weights for
the Gauss–Hermite quadrature rule. Use it to compute a 10-point rule; check the
result using a table.
(b) Write a program for computing nodes and weights for Gauss quadrature rules
when w(x) is not symmetric. In MATLAB use the function [v,d] = eig(J) to
solve the eigenvalue problems. Use the program to compute some Gauss–Laguerre
quadrature rules.
5.3.7 Derive the Gauss–Lobatto quadrature rule in Example 5.3.3, with two interior points
by using the Ansatz
 1
−1
f (x) dx = w1(f (−1) + f (1)) + w2(f (−x1) + f (x1)),
and requiring that it be exact for f (x) = 1, x2, x4.
5.3.8 Compute an approximate value of
 1
−1
x4 sin2 πx dx = 2
 1
0
x4 sin2 πx dx,
using a ﬁve-point Gauss–Legendre quadrature rule on [0, 1] for the weight func-
tion w(x) = 1.
For nodes and weights see Table 5.3.1 or use the MATLAB
function legendre(n) given in Example 5.3.4. (The true value of the integral
is 0.11407 77897 39689.)
5.3.9 (a) Determine exactly the Lobatto formulas with given nodes at −1 and 1 (and the
remaining nodes free), for the weight functions
w(x) = (1 −x2)−1/2,
x ∈[−1, 1].

5.4. Multidimensional Integration
587
Determine for this weight function also the nodes and weights for the Gauss quadra-
ture formula (i.e., when all nodes are free).
Hint: Set x = cos φ, and formulate equivalent problems on the unit circle. Note that
you obtain (at least) two different discrete orthogonality properties of the Chebyshev
polynomials this way.
(b) Lobatto–Kronrod pairs are useful when a long interval has been divided into
several shorter intervals (cf. Example 5.3.3). Determine Lobatto–Kronrod pairs
(exactly) for w(x) = (1 −x2)−1/2.
5.3.10 Apply the formulas in Problem 5.3.9 to the case w(x) = 1, x ∈[−1, 1], and some
of the following functions.
(a) f (x) = ekx, k = 1, 2, 4, 8, . . . ;
(b) f (x) = 1/(k + x), k = 1, 2, 1.1, 1.01;
(c) f (x) = k/(1 + k2x2), k = 1, 4, 16, 64.
Compare the actual errors with the error estimates.
5.3.11 For k = 1 the integral (5.2.24) in Example 5.2.5 is
 π/2
−π/2
cos t
t + π + u(t + π) dt.
Compute this integral with at least ten digits of accuracy, using a Gauss–Legendre
rule of sufﬁciently high order. Use the MATLAB function legendre(n) given in
Example 5.3.4 to generate the nodes and weights.
5.3.12 Write a MATLAB function for the evaluation of the Sievert174 integral,
S(x, θ) =
 θ
0
e−x/ cos φ dφ,
for any x ≥0, x ≤θ ≤90◦, with at least six decimals relative accuracy. There may
be useful hints in [1, Sec. 27.4].
5.4
Multidimensional Integration
Numerical integration formulas in several dimensions, sometimes called numerical cuba-
ture, are required in many applications. Several new difﬁculties are encountered in deriving
and applying such rules.
In one dimension any ﬁnite interval of integration [a, b] can be mapped by an afﬁne
transformation onto [−1, 1] (say). Quadrature rules need therefore only be derived for this
standard interval. The order of accuracy of the rule is preserved since afﬁne transformations
preserve the degree of the polynomial. In d dimensions the boundary of the region of
integration has dimension d −1, and can be complicated manifold. For any dimension
d ≥2 there are inﬁnitely many connected regions in Rd which cannot be mapped onto each
other using afﬁne transformations. Quadrature rules with a certain polynomial accuracy
designed for any of these regions are fundamentally different than for any other region.
174Sievert was a Swedish radiophysicist who was so great that doses of radiation are measured in millisieverts,
or even microsieverts, all over the world.

588
Chapter 5. Numerical Integration
The number of function values needed to obtain an acceptable approximation tends
to increase exponentially in the number of dimensions d. That is, if n points are required
for an integral in one dimension, then nd points are required in d dimensions. Thus, even
for a modest number of dimensions, achieving an adequate accuracy may be an intractable
problem. This is often referred to as the curse of dimensionality, a phrase coined by
Richard Bellman.175
5.4.1
Analytic Techniques
It is advisable to try, if possible, to reduce the number of dimensions by applying analytic
techniques to parts of the task.
Example 5.4.1.
The following triple integral can be reduced to a single integral:
 ∞
0
 ∞
0
 ∞
0
e−(x+y+z) sin(xz) sin(yx) dxdydz
=
 ∞
0
e−x dx
 ∞
0
e−y sin(yx)dy
 ∞
0
e−z sin(zx) dz =
 ∞
0

x
1 + x2
2
e−x dx.
This follows because
 ∞
0
e−z sin(zx)dz =
 ∞
0
e−y sin(yx)dz =
x
1 + x2 .
The remaining single integral is simply evaluated by the techniques previously studied.
Often a transformation of variable is needed for such a reduction. Given a region
D in the (x, y)-plane, this is mapped onto a region D′ in the (u, v)-plane by the variable
transformation
x = φ(u, v),
y = ψ(u, v).
(5.4.1)
If φ and ψ have continuous partial derivatives and the Jacobian
J(u, v) =
((((
∂φ/∂u
∂φ∂u
∂ψ∂u
∂ψ∂u
((((
(5.4.2)
does not vanish in D′, then
 
D
f (x, y) dx dy =
 
D′ f (φ(x, y), ψ(x, y))|J(u, v)| du dv.
(5.4.3)
It is important to take into account any symmetries that the integrand can have. For example,
the integration of a spherically symmetric function over a spherical region reduces in polar
coordinates to a one-dimensional integral.
175Richard Ernest Bellman (1920–1984) was an American mathematician. From 1949 to 1965 he worked at the
Rand Corporation and made important contributions to operations research and dynamic programming.

5.4. Multidimensional Integration
589
Example 5.4.2.
To evaluate the integral
I =
 
D
y sin(ky)
x2 + y2 dx dy,
where D is the unit disk x2 + y2 ≤1, we introduce polar coordinates (r, ϕ), x = r cos ϕ,
y = r sin ϕ, dx dy = r dr dϕ. Then, after integrating in the r variable, this integral is
reduced to the single integral
I = 1
k
 2π
0
[1 −cos (k sin ϕ)] dϕ.
This integral is not expressible in ﬁnite terms of elementary functions. Its value is in fact
(1 −J0(k))2π/k, where J0 is a Bessel function. Note that the integrand is a periodic
function of ϕ, in which the trapezoidal rule is very efﬁcient (see Sec. 5.1.4). This is a useful
device for Bessel functions and many other transcendental functions which have integral
representations.
If the integral cannot be reduced, then several approaches are possible:
(a) Tensor products of one-dimensional quadrature rules can be used. These are particu-
larly suitable if the boundary of the region is composed of straight lines. Otherwise
numerical integration in one direction at a time can be used; see Sec. 5.4.3.
(b) For more general boundaries an irregular triangular grid can be used; see Sec. 5.4.4.
(c) Monte Carlo or quasi–Monte Carlo methods can be used, mainly for problems with
complicated boundaries and/or a large number of dimensions; see Sec. 5.4.5.
5.4.2
Repeated One-Dimensional Integration
Consider a double integral
I =
 
D
u(x, y) dxdy,
(5.4.4)
over a region D in the (x, y)-plane such that lines parallel with the x-axis have at most one
segment in common with D (see Figure 5.4.1). Then I can be written in the form
I =
 b
a
 d(x)
c(x)
f (x, y)dy

dx,
or
I =
 b
a
ϕ(x) dx,
ϕ(x) =
 d(x)
c(x)
f (x, y)dy.
(5.4.5)
The one-dimensional integral ϕ(x) can evaluated for the sequence of abscissae xi, i =
1, . . . , n, used in another one-dimensional quadrature rule for J. Note that if D is a more

590
Chapter 5. Numerical Integration
−1
0
1
2
3
4
5
−1.5
−1
−0.5
0
0.5
1
1.5
x
y
Figure 5.4.1. Region D of integration.
general domain, it might be possible to decompose D into the union of simpler domains on
which these methods can be used.
Example 5.4.3.
Compute
I =
 
D
sin2 y sin2 x(1 + x2 + y2)−1/2 dx dy,
where
D = {(x, y) | x2 + y2 ≤1} ∪{(x, y) | 0 ≤x ≤3, |y| ≤0.5}
is a composite region (see Figure 5.4.1). Then
I =
 3
−1
sin2 x ϕ(x) dx,
(5.4.6)
ϕ(x) =
 c(x)
−c(x)
sin2 y (1 + x2 + y2)−1/2dy,
(5.4.7)
where
c(x) =

(1 −x2)1/2,
x < 1
2
√
3,
1
2,
x ≥1
2
√
3.
Values of ϕ(x) were obtained by the application of Romberg’s method to (5.4.7) and nu-
merical integration applied to the integral (5.4.6) yielded the value of I = 0.13202 ± 10−5.
Ninety-six values of x were needed, and for each value of x, 20 function evaluations used,
on the average. The grid is chosen so that x = 1
2
√
3, where ϕ′(x) is discontinuous, is a grid
point.
5.4.3
Product Rules
In d = 2 dimensions, common boundaries are a rectangle, circle, or triangle, or a combi-
nation of these. Consider a double integral over a rectangular region D = {(x, y) | a ≤

5.4. Multidimensional Integration
591
x ≤b, c ≤y ≤d}. Introduce an equidistant rectangular grid in the (x, y)-plane, with
grid spacings h and k in the x and y directions,
xi = a + ih,
yj = c + jk,
h = (b −a)/n,
k = (d −c)/m,
and set uij = u(xi, yj). Then the following product rule for the double integral generalizes
the compound midpoint rule:
I ≈hk
m

i=1
n

j=1
ui−1/2,j−1/2.
(5.4.8)
The product trapezoidal rule is
I ≈hk
m

i=1
n

j=1
1
4(ui−1,j−1 + ui−1,j + ui,j−1 + ui,j)
= hk
m

i=0
n

j=0
wijuij.
(5.4.9)
Here wij = 1 for the interior grid points, i.e., when 0 < i < m, and 0 < j < n. For the
trapezoidal rule wij = 1
4 for the four corner points, while wij = 1
2 for the other boundary
points. Both formulas are exact for all bilinear functions xiyj, 0 ≤i, j ≤1. The error
can be expanded in even powers of h and k so that Romberg’s method can be used to
get more accurate results. The generalization to integrals over the hypercube [0, 1]d is
straightforward.
It is not necessary to use the same quadrature rule in both dimensions. Suppose we
have the two one-dimensional quadrature rules
 b
a
f (x) dx ≈
n

i=1
wif (xi) + (b −a)R1,
 d
c
g(y) dy ≈
m

j=1
vjg(yj) + (d −c)R2.
(5.4.10)
Combining these two rules over the rectangular region D gives the product rule
 b
a
 d
c
u(x, y) dx dy ≈
 b
a

m

j=1
vju(x, yj) + (d −c)R2

dx
=
m

j=1
vj
 b
a
u(x, yj) dx +
 b
a
(d −c)R2 dx ≈
n

i=1
m

j=1
wivju(xi, yj) + R,
where
R = (d −c)
 b
a
R2 dx + (b −a)
m

j=1
vjR1 ≈(b −a)(d −c)(R1 + R2).
The following property of product rules follows easily.

592
Chapter 5. Numerical Integration
Theorem 5.4.1.
If the two one-dimensional rules (5.4.10) integrate f (x) exactly over [a, b] and g(y)
exactly over [c, d], then the product rule (5.4.11) integrates u(x, y) = f (x)g(y) exactly
over the region [a, b] × [c, d].
If the one-dimensional rules are exact for polynomials of degree d1 and d2, respec-
tively, then the product rule will be exact for all bivariate polynomials xpyq, where p ≤d1
and q ≤d2.
Example 5.4.4.
The product Simpson’s rule for the square |x| ≤h, |y| ≤h has the form
 h
−h
 h
−h
u(x, y) dxdy = 4h2
1

j=−1
1

i=−1
wi,ju(xi, yj).
It uses 32 = 9 function values, with abscissae and weights given by
(xi, yj)
(0,0)
(±h, ±h)
(±h, 0)
(0, ±h)
wi,j
4/9
1/36
1/9
1/9
.
Of similar accuracy is the product rule obtained from a two-point Gauss–Legendre rule,
which uses the four points
(xi, yi) =

± h
√
3
, ± h
√
3

wi = 1
4.
For both rules the error is O(h4). Note that for the corresponding composite rules, the
functions values at corner points and midpoints in the product Simpson’s rule are shared
with other subsquares. Effectively this rule also uses four function values per subsquare.
Higher-accuracy formulas can also be derived by operator techniques, based on an
operator formulation of Taylor’s expansion (see Example 4.3.8),
u(x0 + h, y0 + k) = e(hDx+kDy)u(x0, y0).
(5.4.11)
For regions D, such as a square, cube, cylinder, etc., which are the Cartesian product
of lower-dimensional regions, product integration rules can be developed by multiplying
together the lower-dimensional rules. Product rules can be used on nonrectangular regions,
if these can be mapped into a rectangle. This can be done, for example, for a triangle, but
product rules derived in this way are often not very efﬁcient and are seldom used.
For nonrectangular regions, the rectangular grid may also be bordered by triangles or
“triangles” with one curved side, which may be treated with the techniques outlined in the
next section.
So far we have restricted ourselves to the two-dimensional case. But the ideas are
more general. Let (x1, . . . , xr) ∈C, where C is a region in Rr and (y1, . . . , ys) ∈D, where
D is a region in Rr. Let C × D denote the Cartesian product of C and D, i.e., the region in
Rr+s consisting of points (using vector notations) (x, y) such that x ∈C and y ∈D.

5.4. Multidimensional Integration
593
Suppose we have two quadrature rules for the regions C and D

C
f (x) dx ≈
n

i=1
wif (xi),

D
g(y) dy ≈
m

j=1
vig(yi).
(5.4.12)
We can combine these two rules to give a product rule for the region C × D:

C×D
u(x, y) dx dy ≈
n

i=1
n

i=1
wivju(xi, yj).
(5.4.13)
Product rules are not necessarily the most economical rules. More efﬁcient quadrature
rules exist, which are not the result of applying one-dimensional rules to several dimensions.
We could try to determine such rules by selecting n nodes and weights so that the rule
integrates bivariate polynomials of as high a degree as possible. This is much more difﬁcult
in several dimensions than in one dimension, where this approach led to Gaussian rules.
The solution is in general not unique; there may be several rules with different nodes and
weights. For most regions it is not known what the best rules are. Some progress has been
made in developing nonproduct quadrature rules of optimal order for triangles.
Some simple quadrature rules for circles, triangles, hexagons, spheres, and cubes are
given in [1, pp. 891–895], for example, the following quadrature rule for a double integral
over a disk C = {(x, y) ∈C | x2 + y2 ≤h2}:
 
C
f (x, y) dxdy = πh2
4

i=1
wif (xi, yi) + O(h4),
where
(xi, yi) = (±h/2, ±h/2),
wi = 1/4,
i = 1 : 4.
This four-point rule has the same order of accuracy as the four-point Gaussian product for
the square given in Example 5.4.4. A seven-point O(h6) rule uses the points (see Figure
5.4.2)
(x1, y1) = (0, 0),
(xi, yi) = (±h

2/3, 0),
i = 2, 3
(xi, yi) = (±h/
√
6, ±h/
√
2),
i = 4 : 7,
with weights w1 = 1/4, and wi = 1/8, i = 2 : 7.
Figure 5.4.2. A seven-point O(h6) rule for a circle.

594
Chapter 5. Numerical Integration
Example 5.4.5.
We seek a quadrature rule
I =
 
T
f (x, y) dxdy = A
n

i=1
wif (xi, yi) + R,
(5.4.14)
where T is an equilateral triangle with sides of length h and area A = h2√
3/4. We use
function values at the “center of mass” (x1, y1) = (0, h/(2
√
3)) of the triangle and at the
corner nodes
(xi, yi) = (±h/2, 0),
i = 2, 3,
and
(x4, y4) = (0, h
√
3/2).
Then, taking w1 = 3/4, and wi = 1/12, i = 2 : 4, we get a four-point rule with error
R = O(h3).
Adding nodes at the midpoint of the sides
(x5, y5) = (0, 0)
and
(xi, yi) = (±h/4, h
√
3/4),
i = 6, 7,
and using weights w1 = 9/20, and wi = 1/20, i = 2 : 4, wi = 2/15, i = 5 : 7, gives a
seven-point rule for which R = O(h4) in (5.4.14).
5.4.4
Irregular Triangular Grids
A grid of triangles of arbitrary form is a convenient means for approximating a complicated
plane region. It is fairly easy to program a computer to reﬁne a coarse triangular grid
automatically; see Figure 5.4.3. It is also easy to adapt the density of points to the behavior
of the function.
Figure 5.4.3. Reﬁnement of a triangular grid.
Triangular grids are thus more ﬂexible than rectangular ones. On the other hand, the
administration of a rectangular grid requires less storage and a simpler program. Some-
times the approximation formulas are also a little simpler. Triangular grids are used, for
example, in the ﬁnite element method (FEM) for problems in continuum mechanics and
other applications of partial differential equations; see [113].

5.4. Multidimensional Integration
595
Let the points Pj, j = 1, 2, 3, with coordinates pj = (xj, yj), be the vertices of
a triangle T with area Y > 0. Then any point p = (x, y) in the plane can be uniquely
expressed by the vector equation
p = θ1p1 + θ2p2 + θ3p3,
θ1 + θ2 + θ3 = 1.
(5.4.15)
The θi, which are called homogeneous barycentric coordinates of P, are determined from
the following nonsingular set of equations:
θ1x1 + θ2x2 + θ3x3 = x,
(5.4.16)
θ1y1 + θ2y2 + θ3y3 = y,
θ1 + θ2 + θ3 = 1.
Barycentric coordinates were discovered by Möbius176 in 1827; see Coxeter [85, Sec. 13.7].
In engineering literature the barycentric coordinates for a triangle are often called area
coordinates since they are proportional to the area of the three subtriangles induced by P;
see Figure 5.4.4.
P1
P2
P3
P
θ2
θ3
θ1
Figure 5.4.4. Barycentric coordinates of a triangle.
The interior of the triangle is characterized by the inequalities θi > 0, i = 1, 2, 3.
In this case P is the center of mass (centroid) of the three masses θ1, θ2, θ3 located at the
vertices of the triangle. This explains the term “barycentric coordinates.” The equation for
the side P2P3 is θ1 = 0; similarly θ2 = 0 and θ3 = 0 describe the other two sides. Note that
if θ and θ′ (i = 1, 2, 3) are the barycentric coordinates of the points Pi and Pj, respectively,
then the barycentric coordinates of αP + (1 −α)P ′ are αθ + (1 −α)θ′.
Barycentric coordinates are useful also for d > 2 dimensions. By a simplex in Rd we
mean the convex hull of (d + 1) points pj = (p1j, p2j, . . . , pdj)T ∈Rd, which are called
the vertices of the simplex. We assume that the vertices are not contained in a hyper-plane.
This is the case if and only if the (d + 1) × (d + 1) matrix
A =

p1
p2
· · ·
pd+1
1
1
· · ·
1

(5.4.17)
is nonsingular. For d = 2 the simplex is a triangle and for d = 3 a tetrahedron.
176August Ferdinand Möbius (1790–1868) was a German astronomer and mathematician, and a professor at the
University of Leipzig. His 1827 work Barycentric Calculus became a classic and played an important role in the
development of projective geometry.

596
Chapter 5. Numerical Integration
The barycentric coordinates of a point p are the components of the unique vector
θ ∈Rd+1 such that
(p1. . . . , pd+1)θ = p,
eT θ = 1
(5.4.18)
or, equivalently, θ = A−1p
1

.
The center of gravity of the simplex is the point with
coordinates θi = 1/(d + 1), i = 1 : d + 1.
If u is a nonhomogeneous linear function of p, i.e., if
u(p) = aT p + b = (aT , b)

p
1

,
then the reader can verify that
u(p) =
d+1

j=1
θju(pj),
u(pj) = aT pj + b.
(5.4.19)
This is a form of linear interpolation and shows that a linear function is uniquely determined
by its values at the vertices.
Using also the midpoints of the edges pij = 1
2(pi + pj) a quadratic interpolation
formula can be obtained.
Theorem 5.4.2.
Deﬁne
4′′
ij = u(pi) + u(pj) −2u
1
2(pi + pj)

,
i < j.
(5.4.20)
Then the interpolation formula
u(p) =

j
θju(pj) −2

i<j
θiθj4′′
ij,
(5.4.21)
where the summation indices i, j are assumed to take all values 1 : d + 1 unless otherwise
speciﬁed, is exact for all quadratic functions.
Proof. The right-hand is a quadratic function of p, since it follows from (5.4.16) that the θi
are (nonhomogeneous) linear functions of the coordinates of p. It remains to show that the
right-hand side is equal to u(p) for p = pj, and p = (pi + pj)/2, i, j = 1 : d + 1.
For p = pj, θj = 1, θi = 0, i ̸= j, hence the right-hand side equals ui. For
p = (pi + pj)/2, θi = θj = 1/2, θk = 0, k ̸= i, j, and hence the right-hand side becomes
1
2(ui + uj) −2 · 1
2

ui + uj −2u
1
2(pi + pj)

= u
1
2(pi + pj)

.
The following theorem for triangles (d = 2) is equivalent to a rule which has been
used in mechanics for the computation of moments of inertia since the nineteenth century.

5.4. Multidimensional Integration
597
Theorem 5.4.3.
Let T be a triangle with vertices p1, p2, p3 and area Y. Then the integration formula

T
u(x, y) dxdy = Y
3

u
1
2(p1 + p2)

+ u
1
2(p1 + p3)

+ u
1
2(p2 + p3)

(5.4.22)
is exact for all quadratic functions.
Proof. Using the interpolation formula (5.4.21), the integral equals

T
u(x, y) dxdy =

j
u(pj)

T
θjdxdy −2

i<j
4′′
ij

T
θiθjdxdy.
By symmetry,

T θi dxdy is the same for i = 1, 2, 3. Similarly,

T θiθj dxdy is the same
for all i < j. Hence using (5.4.20)

T
u(x, y) dxdy = a(u1 + u2 + u3) −2b(4′′
23 + 4′′
13 + 4′′
12)
= (a −4b)(u1 + u2 + u3)
(5.4.23)
+ 4b

u
1
2(p1 + p2)

+ u
1
2(p2 + p3)

+ u
1
2(p3 + p1)

,
where
a =

T
θ1 dxdy,
b =

T
θ1θ2 dxdy.
Using θ1, θ2 as new variables of integration, we get by (5.4.16) and the relation θ3 =
1 −θ1 −θ2
x = θ1(x1 −x3) + θ1(x1 −x3) + x3,
y = θ1(y1 −y3) + θ1(y1 −y3) + y3.
The functional determinant is equal to
((((
x1 −x3
x2 −x3
y1 −y3
y2 −y3
(((( = 2Y,
and (check the limits of integration!)
a =
 1
θ1=0
 1−θ1
θ2=0
2θ1dθ1dθ2 = 2Y
 1
0
θ1(1 −θ1)dθ1 = Y
3 ,
b =
 1
θ1=0
 1−θ1
θ2=0
2θ1θ2dθ1dθ2 = 2A
 1
0
θ1
(1 −θ1)2
2
dθ1 = Y
12.
The results now follow by insertion of this into (5.4.23).

598
Chapter 5. Numerical Integration
A numerical method for a two-dimensional region can be based on Theorem 5.4.2, by
covering the domain D by triangles. For each curved boundary segment (Figure 5.4.5) the
correction
4
3f (S)A(PRQ)
(5.4.24)
is to be added, where A(PRQ) is the area of the triangle with vertices P, R, Q. The
error of the correction can be shown to be O(∥Q −P∥5) for each segment, if R is close
to the midpoint of the arc PQ. If the boundary is given in parametric form, x = x(x),
y = y(x), where x and y are twice differentiable on the arc PQ, then one should choose
tR = 1
2(tP + tQ). Richardson extrapolation can be used to increase the accuracy; see the
examples below.
P
Q
S
R
Figure 5.4.5. Correction for curved boundary segment.
Example 5.4.6.
Consider the integral
I =
 
D
(x2 + y2)k dxdy,
where the region D and the grids for I4 and I16 are shown in Figure 5.4.6 and In denotes the
result obtained with n triangles. Because of symmetry the error has an expansion in even
powers of h. Therefore, we can use repeated Richardson extrapolation and put
R′
n = I4n + 1
15(I4n −In),
R′′
n = R′
4n + 1
63(R′
4n −R′
n).
The results are shown in the table below. In this case the work could be reduced by a factor
of four, because of symmetry.
k
I4
I16
I64
R′
4
R′
16
R′′
4
Correct
2
0.250000
0.307291
0.310872
0.311111
0.311111
0.311111
28/90
3
0.104167
0.161784
0.170741
0.165625
0.171338
0.171429
0.171429
4
0.046875
0.090678
0.104094
0.093598
0.104988
0.105169
0.105397
It is seen that R′-values have full accuracy for k = 2 and that the R′′-values have
high accuracy even for k = 4. In fact, it can be shown that R′-values are exact for any

5.4. Multidimensional Integration
599
Figure 5.4.6. The grids for I4 and I16.
fourth-degree polynomial and R′′-values are exact for any sixth-degree polynomial, when
the region is covered exactly by the triangles.
Example 5.4.7.
The integral
a
 
(a2 −y2)−1/2 dxdy,
over a quarter of the unit circle x2+y2 ≤1, is computed with the grids shown in Figure 5.4.6,
and with boundary corrections according to (5.4.15). The following results, using the
notation of the previous example, were obtained and compared with the exact values.
a
I8
I32
R′
8
Correct
2
0.351995
0.352077
0.352082
0.352082
4
0.337492
0.337608
0.337615
0.337616
6
0.335084
0.335200
0.335207
0.335208
8
0.334259
0.334374
0.334382
0.334382
Note, however, that Richardson extrapolation may not always give improvement, for
example, when the rate of convergence of the basic method is more rapid than usual.
5.4.5
Monte Carlo Methods
Multidimensional integrals arise frequently in physics, chemistry, computational eco-
nomics,177 and other branches of science. If a product rule is used to evaluate a multi-
dimensional integral in d dimensions the work will increase exponentially with the number
of dimensions d. For example, the product rule of an 8-point one-dimensional rule will
require (8)8 = 224 ≈1.6 · 107 function evaluations in eight dimensions. This means that
the problem may quickly become intractable when d increases.
One important application of the Monte Carlo method described in Section 1.4.2 is
the numerical calculation of integrals of high dimension. For the Monte Carlo method the
177The valuation of ﬁnancial derivatives can require computation of integrals in 360 dimensions!

600
Chapter 5. Numerical Integration
accuracy achieved is always proportional to 1/√n, where n is the number of function eval-
uations independent of the dimension d. Thus, if approached randomly multidimensional
integration becomes tractable! The Monte Carlo method can be said to break “the curse of
dimension” inherent in other methods. (For smooth integrands the Monte Carlo method is,
however, not of optimal complexity.)
We shall brieﬂy describe some ideas used in integration by the Monte Carlo method.
For simplicity, we ﬁrst consider integrals in one dimension, even though the Monte Carlo
method cannot really compete with traditional numerical methods for this problem.
Let Ri, i = 1 : N, be a sequence of random numbers rectangularly distributed on
[0, 1], and set
I =
 1
0
f (x) dx ≈I1,
I1 = 1
N
N

i=1
f (Ri).
Then the expectation of the variable I1 is I and its standard deviation decreases as N−1/2.
I1 can be interpreted as a stochastic estimate of the mean value of f (x) in the interval
[0, 1]. This generalizes directly to multidimensional integrals over the unit hypercube. Let
Ri ∈Rd, i = 1 : N, be a sequence of random points uniformly distributed on [0, 1]d. Then
I =

[0,1]d f (x) dx ≈I1,
I1 = 1
N
N

i=1
f (Ri).
(5.4.25)
If the integral is to be taken over a subregion D ⊂[0, 1]d, we can simply set f (x) = 0,
x /∈D. In contrast to interpolatory quadrature methods smooth functions are not integrated
more efﬁciently than discontinuous functions. According to the law of large numbers, the
convergence
IN(f ) →vol(D)µ(f )
as
N →∞,
where µ(f ) is the mean value of f (X), where X is a continuous random variable uniformly
distributed in D ⊂[0, 1]d.
A probabilistic error estimate can be obtained by estimating the standard deviation of
µ(f ) by the empirical standard deviation sN(f ), where
sN(f )2 =
1
N −1
N

i=1
(f (Ri) −IN(f ))2 .
(5.4.26)
If the integral is over a subregion D ⊂[0, 1]d, we should use the mean value over D, that
is, neglect all points Ri /∈D.
The standard deviation of the Monte Carlo estimate in (5.4.25) decreases as N−1/2.
This is very slow even compared to the trapezoidal rule, for which the error decreases as
N−2. To get one extra decimal place of accuracy we must increase the number of points by
a factor of 100. To get three-digit accuracy the order of one million points may be required!

5.4. Multidimensional Integration
601
But if we consider, for example, a six-dimensional integral this is not exorbitant. Using a
product rule with ten subdivisions in each dimension would also require 106 points.
The above Monte Carlo estimate is a special case of a more general one. Suppose Xi,
i = 1 : N, has density function g(x). Then
I2 = 1
N
N

i=1
f (Xi)
g(Xi)
has expected value I, since
E
f (Xi)
g(Xi)

=
 1
0
f (x)
g(x) f (x) dx =
 1
0
f (x) dx = I.
If one can ﬁnd a frequency function g(x) such that f (x)/g(x) ﬂuctuates less than f (x),
then I2 will have smaller variance than I1. This procedure is called importance sampling;
it has proved very useful in particle physics problems, where important phenomena (for
example, dangerous radiation which penetrates a shield) are associated with certain events
of low probability.
We have previously mentioned the method of using a simple comparison problem.
The Monte Carlo variant of this method is called the control variate method. Suppose
that ϕ(x) is a function whose integral has a known value K, and suppose that f (x) −ϕ(x)
ﬂuctuates much less than f (x). Then
I = K +
 1
0
(f (x) −ϕ(x)) dx,
where the integral to the right can be estimated by
I3 = 1
N
N

i=1
(f (Ri) −ϕ(Ri)),
which has less variance than I1.
5.4.6
Quasi–Monte Carlo and Lattice Methods
In Monte Carlo methods the integrand is evaluated at a sequence of points which are sup-
posed to be a sample of independent random variables. In quasi–Monte Carlo methods
the accuracy is enhanced by using specially chosen deterministic points not necessarily
satisfying the statistical tests discussed in Sec. 1.6.2. These points are constructed to be
approximately equidistributed over the region of integration.
If the region of integration D is a subset of the d-dimensional unit cube Cn = [0, 1]d
wesetf (x) ≡0, forx /∈D. Wecanthenalwaysformulatetheproblemastheapproximation
of an integral over the d-dimensional unit cube Cn = [0, 1]d:
I[f ] =

Cn
f (x1, . . . , xd) dx1 · · · dxd.
(5.4.27)

602
Chapter 5. Numerical Integration
An inﬁnite sequence of vectors x1, x2, x3, . . . in Rd is said to be equidistributed in the cube
[0, 1]d if
I[f ] = lim
N→∞QN(f ),
QN(f ) = 1
N
N

i=1
f (xi),
(5.4.28)
for all Riemann integrable functions f (x). The quadrature rules QN are similar to those
used in Monte Carlo methods; this explains the name quasi–Monte Carlo methods.
In the average case setting the requirement that the worst case error is smaller than
ϵ is replaced by the weaker guarantee that the expected error is at most ϵ. This means that
we make some assumptions about the distribution of the functions to be integrated. In this
setting the complexity of multidimensional integration has been shown to be proportional
to 1/ϵ, compared to (1/ϵ)2 for the Monte Carlo method. Hence the Monte Carlo method
is not optimal.
The convergence of the quadrature rules QN in (5.4.28) depends on the variation of
f and the distribution of the sequence of points x1, . . . , xN. The discrepancy of a ﬁnite
sequence of points x1, x2, . . . , xN is a measure of how much the distribution of the sequence
deviates from an equidistributed sequence. The deterministic set of points used in quasi–
Monte Carlo methods are constructed from low discrepancy sequences, which are, roughly
speaking, uniformly spread as N →∞; see Niederreiter [275].
Let 0 < ai ≤1, i = 1 : d, and restrict f (x), x ∈Rn, to the class of functions
f (x) =
! 1
if 0 ≤xi ≤ai,
0
otherwise.
We require the points to be such that every QN(f ) gives a good approximation of the integral
of f (x) over the hypercube [0, 1]d for all functions in this class.
Low discrepancy sequences are usually generated by algorithms from number theory,
a branch of mathematics seemingly far removed from analysis. Recall from Sec. 2.2.1 that
each integer i has a unique representation dk · · · d2d1d0 with respect to a integer basis b ≥2.
The radical inverse function ϕb maps an integer i onto the real number
ϕb(i) = 0.d0d1d2 · · · dk ∈[0, 1).
The Van der Corput sequence (see [365]) with respect to the base b is the inﬁnite sequence
deﬁned by
xi = ϕb(i),
i = 1, 2, 3, . . . .
These sequences can be shown to have an asymptotic optimal discrepancy. The ﬁrst few
elements in the sequence for b = 2 are shown in the table below.
i
ϕ2(i)
1
1
.1
0.5
2
10
.01
0.25
3
11
.11
0.75
4
100
.001
0.125
5
101
.101
0.625
6
110
.011
0.375
7
111
.111
0.875

5.4. Multidimensional Integration
603
Halton sequences (see [180]) are multidimensional extensions of Van der Corput
sequences.
Deﬁnition 5.4.4.
Let the bases b1, b2, . . . , bd be pairwise relative prime. Then the Halton sequence
xi ∈[0, 1]d with respect to these bases is deﬁned by
xi = (ϕb1(i), ϕb2(i), . . . , ϕbd(i))T ,
i = 0, 1, 2, . . . ,
(5.4.29)
where ϕbk(i) is the radical inverse function with respect to the basis bk, k = 1 : d.
The Hammersley sequences [183] are similar to Halton sequences but are ﬁnite and
differ in the deﬁnition of the ﬁrst component. The N-point Hammersley sequence with
respect to the bases b1, b2, . . . , bd−1 is the sequence of points xi ∈[0, 1]d deﬁned by
xi =

i/N, ϕb1(i), ϕb2(i), . . . , ϕbd−1(i)
T
,
i = 0 : N −1.
(5.4.30)
The Hammersley points in [0, 1]2 for N = 512 and b1 = 2 are shown in Figure 5.4.7.
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5.4.7. Hammersley points in [0, 1]2.
Wozniakowski [383] showed that the Hammersley points are optimal sampling points
for multidimensional integration. With n function evaluations a worst case error of quasi–
Monte Carlo methods is bounded by a multiple of (log n)d/n, which can be compared to
n−1/2 for Monte Carlo methods.

604
Chapter 5. Numerical Integration
Algorithm 5.5. Hammersley Points.
The following MATLAB program generates N Hammersley points in the two-dimensional
square [0, 1]2 for b1 = 2.
[x,y] = Hammersley(N);
n = ceil(log2(N));
for i = 1:N
x(i) = (i-1)/N;
j = i-1;
for p = 1:n
j = j/2; d(p) = 0;
if j > floor(j)
d(p) = 1; j = floor(j);
end;
end;
phi = d(n)/2;
for p = n-1:-1:1
phi = (phi + d(p))/2;
end;
y(i) = phi;
end;
Using harmonic analysis it was shown in Sec. 5.1.4 that the composite trapezoidal
rule can be very accurate for periodic integrands. These results can be extended to multi-
dimensional integration of periodic integrands. A lattice rule for the numerical integration
over the d-dimensional unit cube Cn = [0, 1]d is an equal weight rule,
QN(f ) = 1
N
N−1

j=0
f (xi),
(5.4.31)
where the sampling points xi, i = 0 : N −1, are points of an integration lattice in the
cube [0, 1]d. A multidimensional extension of the compound trapezoidal rule is obtained
by taking
xi = fraction
 i
Np

,
where fraction(x) returns the fractional part of x. Lattice rules can be studied by expanding
the integrand in a Fourier series. The “curse of dimension” can be lifted by using a class of
randomly shifted lattice rules introduced by Ian H. Sloane.
Review Questions
5.4.1 What is meant by a product integration rule for computing a multidimensional inte-
gral? What is the drawback with such rules for high dimensions?

Problems and Computer Exercises
605
5.4.2 Give the generalization of the composite trapezoidal and midpoint rules for a two-
dimensional rectangular grid.
5.4.3 Deﬁne barycentric coordinates in two dimensions. Give a formula for linear interpo-
lation on a triangular grid.
5.4.4 For high-dimensional integrals and difﬁcult boundaries Monte Carlo methods are
often used. How does the accuracy of such methods depend on the number n of
evaluations of the integrand?
5.4.5 How do quasi–Monte Carlo methods differ from Monte Carlo methods?
Problems and Computer Exercises
5.4.1 Let E be the ellipse {(x, y) | (x/a)2 + (y/b)2 ≤1}. Transform
I =
 
E
f (x, y) dxdy
into an integral over a rectangle in the (r, t)-plane with the transformation x =
a r cos t, y = b r sin t.
5.4.2 Consider the integral I of u(x, y, z) over the cube |x| ≤h, |y| ≤h, |z| ≤h. Show
that the rule
I ≈4
3h3
6

i=1
f (xi, yi, zi),
where (xi, yi, zi) = (±h, 0, 0), (0, ±h, 0), (0, 0, ±h), is exact for all monomials
xiyj, 0 ≤i, j ≤1.
5.4.3 (a) In one dimension Simpson’s rule can be obtained by taking the linear combination
S(h) = (T (h)+2M(h))/3 of the trapezoidal and midpoint rule. Derive a quadrature
rule
 h
−h
 h
−h
f (x, y) dxdy = 4h2
6 (f1,0 + f0,1 + f−1,0 + f0,−1 + 2f0,0)
for the square [−h, h]2 by taking the same linear combination of the product trape-
zoidalandmidpointrules. NotethatthisruleisnotequivalenttotheproductSimpson’s
rule.
(b) Show that the rule in (a) is exact for all cubic polynomials. Compare its error term
with that of the product Simpson’s rule.
(c) Generalize the midpoint and trapezoidal rules to the cube [−h, h]3. Then derive
a higher-order quadrature rule using the idea in (a).
5.4.4 Is a quadratic polynomial uniquely determined, given six function values at the ver-
tices and midpoints of the sides of a triangle?
5.4.5 Show that the boundary correction of (5.4.15) is exact if f ≡1, and if the arc is a
parabola where the tangent at R is parallel to PQ.

606
Chapter 5. Numerical Integration
5.4.6 Formulate generalizations to several dimensions of the integral formula of Theo-
rem 5.4.2, and convince yourself of their validity.
Hint: The formula is most simply expressed in terms of the values in the vertices and
in the centroid of a simplex.
5.4.7 (a) Write a program which uses the Monte Carlo method to compute
 1
0 ex dx. Take
25, 100, 225, 400, and 635 points. Plot the error on a log-log scale. How does the
error depend (approximately) on the number of points?
(b) Compute the integral in (a) using the control variate method.
Take ϕ(x) =
1 + x + x2/2. Use the same number of points as in (a).
5.4.8 Use the Monte Carlo method to estimate the multiple integral
I =

[0,1]n
&
|xk −1/3|1/2 ≈0.49n,
for n = 6. What accuracy is attained using N = 103 random points uniformly
distributed in six dimensions?
5.4.9 Write a program to generate Halton points in two dimensions for b1 = 2 and b2 = 5.
Then plot the ﬁrst 200 points in the unit square.
Hint: For extracting the digits to form xi, see Algorithm 2.1. You can also use TOMS
Algorithm 247; see [181].
Notes and References
Numerical integration is a mature and well-understood subject. There are several com-
prehensive monographs devoted to this area; see in particular Davis and Rabinowitz [93]
and Engels [112]. Examples of integrals arising in practice and their solution are found
in [93, Appendix 1]. Newton–Cotes’ and other quadrature rules can also be derived using
computer algebra systems; see [132]. A collection of numerical quadrature rules is given in
the Handbook [1, Sec. 25].
The idea of adaptive Simpson quadrature is old and treated fully by Lyness [251].
Further schemes, computer programs, and examples are given in [93]. A recent discussion
of error estimates and reliability of different codes is given by Espelid [115].
The literature on Gauss–Christoffel quadrature and its computational aspects is exten-
sive. Gauss–Legendre quadrature was derived by Gauss in 1814 using a continued fraction
expansion. In 1826 Jacobi showed that the nodes were the zeros of the Legendre poly-
nomials and that they were real, simple, and in [−1, 1]. The convergence of Gaussian
quadrature methods was ﬁrst studied by Stieltjes in 1884. More on the history can be found
in Gautschi [143]. Recent results by Trefethen [359] suggest that the Clenshaw–Curtis rule
may be as accurate as Gauss–Legendre quadrature with an equal number of nodes.
The importance of the eigenvalues and eigenvectors of the Jacobi matrices for com-
puting Gauss’quadrature rules was ﬁrst elaborated by Golub andWelsch [170]. The general-
ization to Radau and Lobatto quadrature was outlined in Golub [162] and further generalized
by Golub and Kautsky [164].
The presentation in Sec. 5.3.4 was developed in Dahlquist [88]. Related ideas can be
foundinGautschi[140,146]andMysovskih[273]. TheencyclopedicbookbyGautschi[148]

Notes and References
607
describes the current state-of-the-art of orthogonal polynomials and Gauss–Christoffel
quadrature computation; see also the survey by Laurie [238].
There is an abundance of tables giving abscissae and weights for various quadra-
ture rules. Abramowitz and Stegun [1, Sec. 25] give tables for Newton–Cotes’ and several
Gauss–Christoffel rules. Many Gaussian quadrature formulas with various weight func-
tions are tabulated in Stroud and Secrest [343]. A survey of other tables is given in [93,
Appendix 4].
Manyalgorithmsandcodesforgeneratingintegrationruleshaveappearedinthepublic
domain. In [93, Appendices 2, 3] several useful Fortran programs are listed and a bibliogra-
phy ofAlgol and Fortran programs published before 1984 is given. Kautsky and Elhay [224]
have developed algorithms and a collection of Fortran subroutines called IQPACK [110]
for computing weights of interpolatory quadratures. QUADPACK is a collection of Fortran
77 and 90 subroutines for integration of functions available at www.netlib.org. It is
described in the book by R. Piessens et al. [290].
A software package in the public domain by Gautschi [145] includes routines for gen-
erating Gauss–Christoffel rules with arbitrary weight functions. A package QPQ consisting
of MATLAB programs for generating orthogonal polynomials as well as dealing with appli-
cations is available at www.cs.purdue.edu/archives/2002/wxg/codes. Part of these
programs are described in Gautschi [150]. Maple programs for Gauss quadrature rules are
given by von Matt [262]. An overview of results related to Gauss–Kronrod rules is given
by Monegato [270]. The calculation of Gauss–Kronrod rules is dealt with in [237, 62].
Gaussian product rules for integration over the n-dimensional cube, sphere, surface
of a sphere, and tetrahedron are derived in Stroud and Secrest [343, Ch. 3]. Some simple
formulas of various accuracy are tabulated in [1, Sec. 25]. The derivation of such formulas
are treated by Engels [112]. Nonproduct rules for multidimensional integration are found
in Stroud [342].
Agood introduction to multidimensional integration formulas and Monte Carlo meth-
ods is given by Ueberhuber [364, Chap. 12]. Construction of fully symmetric numerical
multidimensional integration formulas over the hypercube [−h, h]d using a rectangular
grid is treated by McNamee and Stenger [263]. For a survey of recent results on sparse
grids and breaking “the curse of dimensionality,” see Bungartz and Griebel [60]. The ef-
ﬁciency of quasi–Monte Carlo methods are discussed in [328]. Lattice rules are treated in
the monograph by Sloan and Joe [327]. For an introduction see also [364, Sec. 12.4.5].


Chapter 6
Solving Scalar Nonlinear
Equations
If we start with an approximation to a zero that is
appreciably more correct than the limiting accuracy,
a single (Newton) iteration will usually spoil this very
good approximation and produce one with an error
which is typically of the limiting accuracy.
—J. H. Wilkinson
6.1
Some Basic Concepts and Methods
6.1.1
Introduction
In this chapter we study numerical methods for computing accurate approximations to the
roots of a scalar nonlinear equation
f (x) = 0,
(6.1.1)
where f (x) is a real-valued function of one variable. This problem has occupied mathemati-
cians for many centuries and several of the basic methods date back a long time. In general
the roots of (6.1.1) cannot be expressed in closed form. Even when an explicit solution is
available (as, for example, for the reduced cubic equation), this is often so complicated that
using Newton’s method is much more practical; see Problem 2.3.8.
Numerical methods are iterative in nature. Starting from one or more initial ap-
proximations, they produce a sequence of approximations, which presumably converges to
the desired root. Note that it sufﬁces that the function f (x), and preferably some of its
derivatives, can be evaluated for given numerical values of x for numerical methods to be
applicable.
It is not uncommon in applications that each function value is obtained by a compli-
cated computation, for example, by the numerical solution of a differential equation. The
object is then to use as few function evaluations as possible in order to approximate the root
with a prescribed accuracy.
609

610
Chapter 6. Solving Scalar Nonlinear Equations
Iterative methods have to be truncated after a ﬁnite number of steps and therefore can
yield only approximations to the desired roots. Further, the roundoff errors that occur in the
evaluation of f (x) will limit the accuracy limiting by any numerical method. The effect of
such rounding errors depends on the conditioning of the roots and is discussed in Sec. 6.1.3.
With certain methods it is sufﬁcient for convergence to know an initial interval [a, b],
which contains the desired root (and no other root). An important example is the bisection
method described in Sec. 6.1.2. It is often suitable to use a hybrid method in which the
bisection method is used to roughly locate the root. A more rapidly convergent method is
then used to reﬁne this approximation. These latter methods make use of more regularity
assumptions about f (x), and usually also require an initial approximation close to the
desired root.
The theory of ﬁxed-point iteration methods is treated in Sec. 6.1.4 and the concepts
of convergence order and efﬁciency introduced in Sec. 6.1.5. The secant method and other
methods based on interpolation are described in Sec. 6.2. In Sec. 6.4 we brieﬂy consider
methods for solving the related problem of ﬁnding the minimum or maximum of a real-
valued function g(x). Newton’s method and other methods of higher order are analyzed in
Sec. 6.3. A classical problem is that of determining all real or complex roots of an algebraic
equation. Special features and methods for this problem are taken up in Sec. 6.5.
Many of the methods for a single equation, such as Newton’s method, are easily
generalized for systems of nonlinear equations. But unless good approximations to the
roots are known, several modiﬁcations of the basic methods are required; see Volume II,
Chapter 11.
6.1.2
The Bisection Method
It is often advisable to start by collecting some qualitative information about the roots to
be computed. One should try to determine how many roots there are and their approximate
location. Such information can often be obtained by graphing the function f (x). This can
be a useful tool for determining the number of roots and intervals containing each root.
Example 6.1.1.
Consider the equation
f (x) = (x/2)2 −sin x = 0.
InFigure6.1.1thegraphsofy = (x/2)2 andy = sin x areshown. Observingtheintersection
of these we ﬁnd that the unique positive root lies in the interval (1.8, 2), probably close to
α ≈x0 = 1.9.
The following intermediate value theorem can be used to infer that an interval [a, b]
contains at least one root of f (x) = 0.
Theorem 6.1.1 (Intermediate Value Theorem).
Assume that the function f (x) is continuous for x ∈[a, b], f (a) ̸= f (b), and k is
between f (a) and f (b). Then there is a point ξ ∈(a, b) such that f (ξ) = k. In particular,
if f (a)f (b) < 0, then the equation f (x) = 0 has at least one root in the interval (a, b).

6.1. Some Basic Concepts and Methods
611
0
0.5
1
1.5
2
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
y
x
y = sin(x)
y = (x/2)2
Figure 6.1.1. Graph of curves y = (x/2)2 and sin x.
A systematic use of the intermediate value theorem is made in the bisection method.
Assume that f (x) is continuous in the interval (a0, b0) and that f (a0)f (b0) < 0. We shall
determine a nested sequence of intervals Ik = (ak, bk), k = 1, 2, 3, . . . , such that
(a0, b0) ⊃(a1, b1) ⊃(a2, b2) ⊃· · ·
and which all contain a root of the equation f (x) = 0. The intervals are determined
recursively as follows. Given Ik = (ak, bk) compute the midpoint
mk = 1
2(ak + bk) = ak + 1
2(bk −ak) . . .
(6.1.2)
and f (mk). The latter expression has the advantage that using this to compute the midpoint,
no rounding error occurs in the subtraction (see Theorem 2.2.2 and Example 6.1.3).
We can assume that f (mk) ̸= 0, since otherwise we have found a root. The new
interval is then determined by
Ik+1 = (ak+1, bk+1) =
%
(mk, bk)
if f (mk)f (ak) > 0,
(ak, mk)
if f (mk)f (ak) < 0.
(6.1.3)
From the construction it follows immediately that f (ak+1)f (bk+1) < 0 (see also Fig-
ure 6.1.1) and therefore the interval Ik+1 also contains a root of f (x) = 0.
If the sign of f (mk) has been correctly evaluated for k = 1 : n, then after n bisection
steps we have contained a root in the interval (an, bn) of length 2−n(b0 −a0). If we take
mn as an estimate of the root α, we have the error estimate
|α −mn| < 2−(n+1)(b0 −a0).
(6.1.4)
At each step we gain one binary digit in accuracy or, since 10−1 ≈2−3.3, on average
one decimal digit per 3.3 steps. To ﬁnd an interval of length δ which includes a root will

612
Chapter 6. Solving Scalar Nonlinear Equations
require about log2((b −a)/δ) evaluations of f . Note that the bisection algorithm makes no
quantitative use of the magnitude of computed function values.
Example 6.1.2.
The bisection method applied to the equation (x/2)2 −sin x = 0, with I0 = (1.8, 2)
gives the sequence of intervals [an, bn], where
k
ak
bk
mk
f (mk)
1
1.8
2
1.9
< 0
2
1.9
2
1.95
> 0
3
1.9
1.95
1.925
< 0
4
1.925
1.95
1.9375
> 0
5
1.925
1.9375
1.93125
< 0
6
1.93125
1.9375
1.934375
> 0
.
Here after six function evaluations we have α ∈(1.93125, 1.934375), an interval of length
0.2 · 2−6 = 0.003125 (see Figure 6.1.2).
1.75
1.8
1.85
1.9
1.95
2
2.05
2.1
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
x
y
a1
b1
m1
m2
m3
Figure 6.1.2. The bisection method.
Example 6.1.3.
The inequalities a ≤f l( 1
2(a + b)) ≤b, where a and b are ﬂoating-point numbers
with a ≤b, can be violated in ﬂoating-point arithmetic. For example, assume that base
10 arithmetic with six digits is used. Taking a = 0.742531 and b = 0.742533 we obtain
f l(a + b) = 1.48506 (rounded) and 1
2(a + b) = 0.742530 < a.
On the other hand the inequalities a ≤f l(a + 1
2(b −a)) ≤b hold in ﬂoating-point
arithmetic, for any base β. (Recall that by Lemma 2.3.2 f l(b −a) is evaluated exactly
in binary arithmetic if b/2 ≤a ≤2b.) With a and b as given, we get the correct value
0.742532.

6.1. Some Basic Concepts and Methods
613
An algorithmic description of the bisection method is given below. Let f be a given
function and I = [a, b] an interval such that b > a and f (a)f (b) ≤0. The bisection
algorithm attempts to compute an approximation to a root m ∈I of f (x) = 0, with an
error less than a speciﬁed tolerance τ > 0. Note that the given tolerance τ is increased
by the amount u max(|a|, |b|), where u is the machine precision. This is to guard against
the possibility that δ has been chosen too small (e.g., smaller than the spacing between the
ﬂoating-point numbers between a and b).
Algorithm 6.1. Bisection.
The function bisect computes an approximation r to a local root of a given function in the
interval I = (a, b) with an error less than a speciﬁed tolerance τ + u max(|a|, |a|), where
u is the unit roundoff.
function root = bisect(fname,a,b,tau);
%
% [a,b] is an initial interval such that
% f(a)*f(b) < 0.
fa = feval(fname,a);
fb = feval(fname,b);
while abs(b-a) > tau + eps*max(abs(a),abs(b))
mid = a + (b - a)/2;
fmid = feval(fname,mid);
if fa*fmid <= 0
% Keep left endpoint a
b = mid;
else
% Keep right endpoint b
a = mid; fa = fmid;
end;
root = a
+ (b - a)/2;
end;
The time required by the bisection algorithm is typically proportional to the number of
functionevaluations, otherarithmeticoperationsbeinginsigniﬁcant. Thecorrectsubinterval
will be chosen in the algorithm as long as the sign of the computed function value f (m) is
correctly determined. If the tolerance τ is taken too “small” or the root is ill-conditioned,
this may fail to be true in the later steps. Even then the computed midpoints will stay within
a certain domain of uncertainty. Due to rounding errors there is a limiting accuracy with
which a root can be determined from approximate function values; see Sec. 6.1.3.
The bisection method is optimal for the class of functions that changes sign on [a, b]
in the sense that it minimizes the maximum number of steps over all such functions. The
convergence is rather slow, but independent of the regularity of f (x). For other classes of
functions, for example, functions that are continuously differentiable on [a, b], methods like
Newton’s method which assume some regularity of f (x) can achieve signiﬁcantly faster
convergence.

614
Chapter 6. Solving Scalar Nonlinear Equations
If f (a)f (b) < 0, then by the intermediate value theorem the interval (a, b) contains
at least one root of f (x) = 0. If the interval (a, b) contains several roots of f (x) = 0,
then the bisection method will converge to just one of these. (Note that there may be one
or several roots in (a, b), and in the case f (a)f (b) > 0.)
If we only know (say) a lower bound a < α for the root to be determined we can
proceed as follows. We choose an initial step length h and in the ﬁrst hunting phase
compute successively function values f (a +h), f (a +2h), f (a +4h), . . . , i.e., we double
the step, until a function value is found such that f (a)f (a + 2kh) < 0. At this point we
have bracketed a root and can initiate the bisection algorithm.
In the bisection method the interval of interest is in each step split into two subintervals.
An obvious generalization is to partition instead into k subintervals, for p ≥2. In such
a multisection method of order p the interval I = [a, b] is divided into k subintervals
Ii = [xi, xi+1], where
xi = a + i [(b −a)/p],
i = 0 : p.
If there exists only one root in the interval I and we wish to compute it with an absolute
error ϵ, then it is necessary to perform
nk = log2
b −a
2ϵ
#
log2(p)
multisections of order p. Thus, the efﬁciency of multisection of order p compared to
bisection (p = 2) is
n2/(pnp) = log2(p)/p.
Hence if there is a single root in the interval bisection is always preferable. If there are
several roots in the interval multisection may perform better if the subintervals can be
processed in parallel.
There are several other applications of the bisection algorithm.
For example, in
Sec. 4.4.3 we considered evaluating the nonzero B-splines for a given argument x. Then
we ﬁrst have to search an ordered sequence of knots τ0, . . . , τm to ﬁnd the interval such that
τj ≤x < τj+1. This can be achieved by a slight modiﬁcation of the bisection method.
Asimilar problem, important in computer science, is searching in an ordered register,
forexample, aregisterofemployeesorderedaccordingtoincreasingSocialSecuritynumber.
If the nth number in the register is denoted by f (n), then searching for a certain number
a means that an equation f (n) = a is to be solved (here f is an increasing, discontinuous
function). The bisection method can also be used in searching an alphabetically ordered
register.
In later sections we will study methods for solving a nonlinear equation which make
more efﬁcient use of computed function values than the bisection method and possibly also
use values of derivatives of f (x). If f (x) is sufﬁciently regular such methods can achieve
signiﬁcantly faster convergence.
6.1.3
Limiting Accuracy and Termination Criteria
In the following we denote by f (x) the limited-precision approximation obtained when f (x)
is evaluated in ﬂoating-point arithmetic. When a monotone function f (x) is evaluated in

6.1. Some Basic Concepts and Methods
615
ﬂoating-point arithmetic the resulting approximation f (x) is not, in general, monotone.
The effect of rounding errors, in evaluating a certain polynomial of ﬁfth degree with a
simple zero at x = 1, is illustrated in Figure 6.1.3. Note the loss of monotonicity caused
by rounding errors. This ﬁgure also shows that even if f (a)f (b) < 0, the true equation
f (x) = 0 may not have a zero in [a, b]!
0.9985
0.999
0.9995
1
1.0005
1.001
1.0015
−1
−0.5
0
0.5
1
1.5 x 10
−14
Figure 6.1.3. Limited-precision approximation of a continuous function.
Even if the true function value |f (xn)| is “small” one cannot deduce that xn is close
to a zero of f (x) without some assumption about the size of the derivative of f . We recall
some basic results from analysis; for proofs see, for example, Ostrowski [279, Chapter 2].
Theorem 6.1.2.
Let f (x) be continuous and differentiable in the interval J = [xn −η, xn + η] for
some η > 0. If |f ′(x)| ≥m1 for all x ∈J and |f (xn)| ≤ηm1, then f (x) has exactly one
zero in J.
A root α of f (x) = 0 is said to be a simple root if f ′(α) ̸= 0. We now derive an
error estimate for a simple root α of f (x), which takes into account errors in the computed
values of f (x). Assume that
f (x) = f (x) + δ(x),
|δ(x)| ≤δ,
x ∈J,
(6.1.5)
where δ is an upper bound for rounding and other errors in computed function values of
f (x). Using Theorem 6.1.2 we obtain
|xn −α| ≤η,
η = (|f (xn)| + δ)/m1,
|f ′(x)| ≥m1,
x ∈J.
(6.1.6)
Obviously the best we can hope for is to ﬁnd an approximation xn such that the computed
function value f (xn) = 0. It follows that for any numerical method, δ/m1 is an approximate

616
Chapter 6. Solving Scalar Nonlinear Equations
limit for the accuracy with which a simple zero α can be determined. If f ′(x) does not vary
much near xn = α, then we have the approximate error bound
|xn −α| ≤δ/m1 ≈ϵα,
ϵα = δ/|f ′(α)|.
(6.1.7)
Since this is the best error bound for any method, we call ϵα the limiting accuracy for the
simple root α, and the interval [α −ϵα, α +ϵα] the domain of uncertainty for the root α. If
|f ′(α)| is small, then ϵα is large and the problem of computing the root α is ill-conditioned
(see again Figure 6.1.3).
Example 6.1.4.
Suppose we have computed the approximation x = 1.93375 to the positive root to
the equation f (x) = sin x −(x/2)2. Now f ′(x) = cos x −x/2 and it is easily veriﬁed that
|f ′(x)| > 1.31 = m1,
x ∈[1.93, 1.94].
Further, using six decimals we get sin 1.93375 = 0.934852 ± 0.5 · 10−6, and (x/2)2 =
0.9668752 = 0.934847 ± 0.5 · 10−6. Then (6.1.6) gives the strict error estimate
|x −α| < 6 · 10−6/1.31 < 5.6 · 10−6.
Using the following theorem, an analogous result can be shown for zeros of a complex
function f (z) of a complex variable z.
Theorem 6.1.3.
Let f (z) be analytic in the disk K = {z | |z −z0| ≤η} for some η > 0.
If |f ′(z)| ≥m1 in K and |f (z0)| ≤ηm1, then f (z) has a zero inside K.
The multiplicity of a root is deﬁned as follows.
Deﬁnition 6.1.4.
Suppose that f (x) is q times continuously differentiable in a neighborhood of a root
α to the equation f (x) = 0. Then α is said to have multiplicity q if
0 ̸= lim
x→α
((f (x)/(x −α)q(( < ∞.
(6.1.8)
If a root α has multiplicity q, then by (6.1.8) f (j)(α) = 0, j < q, and from Taylor’s
formula
f (x) = 1
q!(x −α)qf (q)(ξ),
ξ ∈int(x, α).
(6.1.9)
Assuming that |f (q)(x)| ≥mq, x ∈J, and proceeding as before, we ﬁnd that the limiting
accuracy for a root of multiplicity q is given by
|xn −α| ≤(q! δ/mq)1/q ≈ϵα,
ϵα = (q! δ/|f (q)(α)|)1/q.
(6.1.10)
Comparing this with (6.1.7), we see that because of the exponent 1/q multiple roots are in
general very ill-conditioned. Asimilar behavior can also be expected when there are several

6.1. Some Basic Concepts and Methods
617
distinct but “close” roots. An instructive example is the Wilkinson polynomial, studied in
Sec. 6.5.2.
Example 6.1.5.
The equation f (x) = (x −2)x + 1 = 0 has a double root x = 1. The (exact) value
of the function at x = 1 + ϵ is
f (1 + ϵ) = (ϵ −1)(1 + ϵ) + 1 = −(1 −ϵ2) + 1 = ϵ2.
Now, suppose that we use a ﬂoating-point arithmetic with eight decimal digits in the man-
tissa. Then
f l(1 −ϵ2) = 1,
|ϵ| < 1
2
√
2· 10−4,
and for 0.99992929 ≤x ≤1.0000707, the computed value of f (x) will be zero when f (x)
is evaluated using Horner’s rule. Hence the root can only be computed with about four
correct digits, i.e., with a relative error equal to the square root of the machine precision.
An important practical question concerning iterative methods is how to stop them.
There are a few different termination criteria in practical use. The simplest is to stop after
a preset number of iterations. This is, in general, too crude, but it is advisable always to
specify the maximum number of iterations allowed, to guard against unforeseen problems.
For example, programming errors could otherwise lead to an inﬁnite loop being executed.
If the iteration method produces a sequence of bracketing intervals [ak, bk] the itera-
tions can be terminated when
|bk −ak| ≤tol,
tol = τ + 2u|xn|,
(6.1.11)
where τ > 0 is a user speciﬁed absolute tolerance and u is the rounding unit (see Sec. 2.2.2).
The second term assures that the iterations are terminated when the roundoff level of the
ﬂoating-point arithmetic is reached.
If for a simple root a lower bound for |f ′(α)| is known, the iterations can be terminated
on the basis of the error estimate (6.1.7). But it is usually more effective to iterate a few
extra times, rather than make the effort to use a special formula for error estimation.
The termination criteria must be able to deal with the possibility that the user speciﬁed
toleranceistoosmall(ortheroottooill-conditioned)andcannotbeattained. Inthiscasefrom
some step onward rounding errors will dominate in the evaluation of f (xn) and the computed
values of f (x) may vary quasi-randomly in the interval (−δ, δ) of limiting accuracy. If we
are using a bracketing method like the bisection method, the iterations will continue until
the criterion (6.1.11) is satisﬁed. This, of course, does not ensure that the root actually has
been determined to this precision.
For iterative methods with fast convergence (like Newton’s method) the following
termination criterion can often be used: Stop when for the ﬁrst time the approximation xn
satisﬁes the two conditions
|xn+1 −xn| ≥|xn −xn−1|,
|xn −xn−1| ≤tol.
(6.1.12)
Here tol is a coarse tolerance, used only to prevent the iterations from being terminated
before xn even has come close to α. When (6.1.12) is satisﬁed the limiting accuracy has

618
Chapter 6. Solving Scalar Nonlinear Equations
been reached and the quantity |xn+1 −xn| usually is a good estimate of the error |xn −α|.
Using this criterion the risk of never terminating the iterations for an ill-conditioned root is
quite small. Note also that iteration methods of superlinear convergence ultimately converge
so fast that the cost of always iterating until the limiting accuracy is obtained may be small,
even if the user speciﬁed tolerance is much larger than ϵα.
6.1.4
Fixed-Point Iteration
We now introduce a very general class of iteration methods, which includes many important
root ﬁnding methods as special cases.
Let φ be a continuous function and {xn} the sequence generated by
xn+1 = φ(xn),
n = 0, 1, 2, . . .
(6.1.13)
for some initial value x0. Assuming that limn→∞xn = α, it follows that
α = lim
n→∞xn = lim
n→∞φ(xn) = φ(α),
(6.1.14)
i.e., the limiting value α is a root of the equation x = φ(x). We call α a ﬁxed point of the
mapping x →φ(x) and the iteration (6.1.13) a ﬁxed-point iteration.
An iterative method for solving an equation f (x) = 0 can be constructed by rewriting
it in the equivalent form x = φ(x), which then deﬁnes a ﬁxed-point iteration (6.1.13).
Clearly this can be done in many ways. For example, let g(x) be any function such that
g(α) ̸= 0 and set
φ(x) = x −f (x)g(x).
(6.1.15)
Then α is a solution to f (x) = 0 if and only if α is a ﬁxed point of φ.
Example 6.1.6.
The equation x + ln x = 0 can, for example, be written as
(i) x = −ln x;
(ii) x = e−x;
(iii) x = (x + e−x)/2.
Each of these give rise to a different ﬁxed-point iteration. Results from the ﬁrst eight
iterations,
xn+1 = e−xn,
x0 = 0.3,
are pictured in Figure 6.1.4. The convergence is slow and we get x9 = 0.5641 (correct
value 0.567143).
As was shown already in Sec. 1.1.1 the iteration (6.1.13) may not converge even if
the initial value x0 is chosen arbitrarily close to a root. If limn→∞xn = α for all x0 in a
sufﬁciently close neighborhood of α, then α is called a point of attraction; otherwise α
is a point of repulsion. The case |φ′(α)| = 1 is indeterminate; the ﬁxed-point iteration
xn+1 = φ(xn) can either converge or diverge (see Problem 6.1.10).
Weshallseethatundercertainconditionstheﬁxed-pointproblemhasauniquesolution
and that the iteration deﬁned by (6.1.13) converges to this solution. The following important
theorem not only provides a solid basis for iterative numerical techniques, but also is an

6.1. Some Basic Concepts and Methods
619
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x0
x1
x2
x3
x4
Figure 6.1.4. The ﬁxed-point iteration xk+1 = e−xk, x0 = 0.3.
important tool in theoretical analysis. Note that the existence of a ﬁxed point is not assumed
a priori.
Theorem 6.1.5 (Contraction Mapping Theorem).
Let x0 be a starting point, and consider the iteration xn+1 = φ(xn), n = 1, 2, . . . .
Let
Jρ = {x | |x −x0| < ρ}
be an open interval of length ρ around x0 and assume that x →φ(x) is a contraction
mapping, i.e., for arbitrary points s and t in Jρ
|φ(s) −φ(t)| ≤L|s −t|,
0 ≤L < 1.
(6.1.16)
Then if
|x0 −x1| ≤(1 −L)ρ,
(6.1.17)
the equation x = φ(x) has a unique solution α in the closed interval |x −x0| ≤ρ. This
solution can be obtained by the convergent iteration process xk+1 = φ(xk), k = 0, 1, . . . .
We have the error estimate
|xk −α| ≤|xk −xk−1|
L
1 −L ≤|x1 −x0| Lk
1 −L.
(6.1.18)
Proof. We ﬁrst prove the uniqueness. If there were two solutions x′ and x′′, we would get
x′ −x′′ = φ(x′) −φ(x′′) so that
|x′ −x′′| = |φ(x′) −φ(x′′)| ≤L|x′ −x′′|.
Since L < 1, it follows that |x′ −x′′| = 0, i.e., x′ = x′′.

620
Chapter 6. Solving Scalar Nonlinear Equations
By (6.1.17) we have |x1 −x0| = |φ(x0) −x0| ≤(1 −L)ρ, and hence x1 ∈Jρ. We
now use induction to prove that xn ∈Jρ for n < j, and that
|xj −xj−1| ≤Lj−1(1 −L)ρ,
|xj −x0| ≤(1 −Lj)ρ.
We already know that these estimates are true for j = 1. Using the triangle inequality and
(6.1.16) we get
|xj+1 −xj| = |φ(xj) −φ(xj−1)| ≤L|xj −xj−1| ≤Lj(1 −L)ρ,
|xj+1 −x0| ≤|xj+1 −xj| + |xj −x0| ≤Lj(1 −L)ρ + (1 −Lj)ρ
= (1 −Lj+1)ρ.
This proves the induction step, and it follows that the sequence {xk}∞
k=0 stays in Jρ. We also
have for p > 0
|xj+p −xj| ≤|xj+p −xj+p−1| + · · · + |xj+1 −xj|
≤(Lj+p−1 + · · · + Lj)(1 −L)ρ ≤Lj(1 −Lp)ρ ≤Ljρ,
and hence limj→∞|xj+p −xj| = 0. The sequence {xk}∞
k=0 therefore is a Cauchy sequence,
and since Rn is complete has a limit α. Since xj ∈Jρ for all j it follows that α ∈{x |
|x −x0| ≤ρ}.
Finally, by (6.1.16) φ is continuous, and it follows that limk→∞φ(xk) = φ(α) = α.
The demonstration of the error estimates (6.1.18) is left as an exercise for the reader.
There is an analogue of Theorem 6.1.5 for complex functions φ(z) analytic in a circle
K = {z | |z −α| ≤ρ} containing the initial approximation z0. Indeed, Theorem 6.1.5
holds in a much more general setting, where φ : Sr →B, and B is a Banach space.178 The
proof goes through with simple modiﬁcations. In this form the theorem can be used to prove
existence and uniqueness for initial value problems for ordinary differential equations.
An estimate of the error in xn, which depends only on x0, x1 and the Lipschitz constant
L, may be derived as follows. For arbitrary positive integers m and n we have
xm+n −xn = (xm+n −xm+n−1) + · · · + (xn+2 −xn+1) + (xn+1 −xn).
From the Lipschitz condition we conclude that |xi+1 −xi| ≤Ci|x1 −x0|, and hence
|xm+n −xn| ≤(Cm−1 + · · · + C + 1)|xn+1 −xn|.
Summing the geometric series and letting m →∞we obtain
|α −xn| ≤
1
1 −C |xn+1 −xn| ≤
Cn
1 −C |x1 −x0|.
(6.1.19)
Note that if C is close to unity, then the error in xn can be much larger than |xn+1 −xn|.
Clearly it is not always safe to terminate the iterations when |xn+1 −xn| is less than the
required tolerance!
178Recall that a Banach space is a normed vector space which is complete, i.e., every Cauchy sequence converges
to a point in B; see Dieudonné [100].

6.1. Some Basic Concepts and Methods
621
For a linearly convergent ﬁxed-point iteration the sequence {xj −α} approximately
forms a geometric series. Then, as seen in Sec. 3.4.2, a more rapidly convergent sequence
{x′
j} can be obtained by Aitken extrapolation,
x′
j = xj −(∇xj)2/∇2xj.
(6.1.20)
(Note that if the convergence is not linear, then the sequence {x′
n} will usually converge
slower than {xn}.)
Example 6.1.7.
Let φ(x) = 1 −1
2x2, and consider the ﬁxed-point iteration xn+1 = φ(xn), with
x0 = 0.8, n = 0, 1, 2, . . . . In this example the theoretical criterion (3.4.2) is satisﬁed, since
one can show that xn →a =
√
3 −1 ≈0.73205,
∇xn/∇xn−1 →φ′(a) = −a,
n →∞.
(See the discussion of iteration in Sec. 1.1.1.)
We obtain x1 = 0.68, x2 = 0.7688, and x′
2 = 0.7688−(0.0888)2/0.2088 = 0.73103.
The error in x′
2 is only about 3% of the error in x2. Iterated Aitken yields, with u = 2−53 =
1.1·10−16, the results x′
2 = 0.73103 . . . , x′′
4 = 0.73211, etc., and the errors
e′
2 = −10 · 10−3,
e′′
4 = 5.6 · 10−5,
e′′′
6 = 7.8 · 10−7,
e(4)
8
= −2.8 · 10−9,
after 2, 4, 6, 8 evaluations of the function φ, respectively. We shall see (Problem 6.1.13) that
the accelerated sequence may converge, even if the basic iteration xn+1 = φ(xn) diverges.
When the basic sequence {xn}, as in the above example, is produced by a convergent
iterative process, one can apply Aitken acceleration in a different, usually more efﬁcient
way. This can be called active Aitken acceleration, since the result of an acceleration is
actively used in the basic iterative process. We start as before by computing x1 = φ(x0),
x2 = φ(x1) and apply (6.1.20) to compute x′
2. Next we continue the iterations from x′
2, i.e.,
compute x3 = φ(x′
2), x4 = φ(x3). We can now extrapolate from x′
2, x3, and x4 to get x′
4,
etc. It is easily veriﬁed that the sequence zn = x′
2n is generated by the ﬁxed-point iteration
zn+1 = ψ(zn),
ψ(z) = z −
(φ(z) −z)2
(φ(φ(z)) −φ(z)) −(φ(z) −z).
(6.1.21)
This is repeated until some termination criterion is satisﬁed.
It can be shown that active Aitken extrapolation applied to the ﬁxed point iteration
xn+1 = φ(xn) is equivalent to Steffensen’s method applied to the equation f (x) = x −
φ(x) = 0; see (6.2.10).
6.1.5
Convergence Order and Efﬁciency
In general we will be given an equation f (x) = 0 to solve and want to construct a rapidly
converging ﬁxed-point iteration. Basic concepts to quantify the rate of convergence will
now be introduced.

622
Chapter 6. Solving Scalar Nonlinear Equations
Deﬁnition 6.1.6.
Consider a sequence {xn}∞
0 with limn→∞xn = α, and xn ̸= α for n < ∞. The
sequenceissaidtohaveconvergenceorderequaltoq ≥1ifforsomeconstant0 < C < ∞,
lim
n→∞
|xn+1 −α|
|xn −α|q = C,
(6.1.22)
where q need not be an integer. C is called the asymptotic error constant.
If q = 1, then we require that C < 1 and then {xn} is said to converge linearly and
C is the rate of linear convergence. For q = 2, 3 the convergence is called quadratic, and
cubic, respectively.
More precisely, the order q in Deﬁnition 6.1.6 is called the Q-order of convergence,
where Q stands for quotient. The same deﬁnitions can be used also for vector-valued
sequences. Then absolute values in (6.1.22) are replaced by a vector norm.
There are types of convergence that are not covered by the above deﬁnition of order.
A sequence may converge more slowly than linear so that (6.1.22) holds with q = 1 and
C = 1. Then convergence is called sublinear. If (6.1.22) holds with q = 1 and C = 0,
then convergence is called superlinear.
Example 6.1.8.
Examples of sublinear, linear, and superlinear convergence are
xn = 1/n,
xn = 2−n,
and
xn = n−n,
respectively.
Alternative deﬁnitions of convergence order are considered by Ortega and Rhein-
boldt [278, Chap. 9] and Brent [44, Sec. 3.2]. For example, if
lim
n→∞inf(−log |xn −α|)1/n = q,
(6.1.23)
then q is called the weak order of convergence for xn, since (6.1.22) implies (6.1.23), but
not vice versa. For example, the sequence xn = exp(−pn)(2 + (−1)n) converges to 0 with
weak order p. But the limit in (6.1.22) does not exist if q = p, is zero if q < p, and is
inﬁnite if q > p.
Consider a ﬁxed-point iteration xn+1 = φ(xn). Assume that φ′(x) exists and is
continuous in a neighborhood of α. It then follows from the proof of Theorem 6.1.5 that if
0 < |φ′(α)| < 1 and x0 is chosen sufﬁciently close to α, then the sequence xn generated by
xn+1 = φ(xn) satisﬁes (6.1.22) with q = 1 and C = |φ′(α)|.
The number of accurate decimal places in the approximation xn is δn = −log10 |xn −
α|. Equation (6.1.22) implies that
δn+1 ≈qδn −log10 |C|.
Hence, for linear convergence (q = 1) as n →∞each iteration gives a ﬁxed (fractional)
number of additional decimal places. For a method with convergence of order q > 1 each
iteration increases the number of correct decimal places q-fold as n →∞. This shows that
eventually a method with larger order of convergence will converge faster.

6.1. Some Basic Concepts and Methods
623
Example 6.1.9.
Consider a sequence xn with quadratic convergence with C = 1. Set ϵn = |xn −α|
and assume that ϵ0 = 0.9. From ϵn+1 ≤Cϵ2
n, it follows that ϵn, for n = 2, 3, . . . , is
bounded by
0.81, 0.66, 0.43, 0.19, 0.034, 0.0012, 1.4 · 10−6, 1.9 · 10−12, . . . ,
respectively. For n ≥6 the number of signiﬁcant digits is approximately doubled at each
iteration!
Deﬁnition 6.1.7.
Consider an iteration method with convergence order q ≥1. If each iteration requires
m units of work (usually the work involved in computing a function value or a value of one
of its derivatives) then
E = q1/m
(6.1.24)
is the efﬁciency index of the iteration.
The efﬁciency index gives a basis for comparing the efﬁciency of iterative methods of
different order of superlinear convergence. Assuming that the cost of evaluating f (xn) and
f ′(xn) is two units the efﬁciency index for Newton’s method is E = 21/2 =
√
2. (Methods
that converge linearly all have E = 1.)
The order of the ﬁxed-point iteration xn+1 = φ(xn) can be determined if φ(x) is
sufﬁciently many times continuously differentiable in a neighborhood of α.
Theorem 6.1.8.
Assume that φ(x) is p times continuously differentiable. Then the iteration method
xn+1 = φ(xn) is of order p for the root α if and only if
φ(j)(α) = 0,
j = 1 : p −1,
φ(p)(α) ̸= 0.
(6.1.25)
Proof. If (6.1.25) holds, then according to Taylor’s theorem we have
xn+1 = φ(xn) = α + 1
p!φ(p)(ζn)(xn −α)p,
ζn ∈int(xn, α).
Hence for a convergent sequence xn the error ϵn = xn −α satisﬁes
lim
n→∞|ϵn+1|/|ϵn|p = |φ(p)(α)|/p! ̸= 0,
and the order of convergence equals p. It also follows that if φ(j)(α) ̸= 0 for some j,
1 ≤j < p, or if φ(p)(α) = 0, then the iteration cannot be of order p.
Example 6.1.10.
We remarked before that to compute a root α of f (x) = 0, we can use a ﬁxed-point
iteration with φ(x) = x−f (x)g(x), where g(x) is an arbitrary function such that g(α) ̸= 0.
We evaluate the derivative
φ′(x) = 1 −f ′(x)g(x) −f (x)g′(x).

624
Chapter 6. Solving Scalar Nonlinear Equations
To achieve quadratic convergence we take g(x) = 1/f ′(x). Assuming that f ′(α) ̸= 0 we
ﬁnd, using f (α) = 0, that φ′(α) = 1 −f ′(α)g(α) = 0. Hence the iteration
xn+1 = xn −f (xn)/f ′(xn)
(6.1.26)
achieves quadratic convergence. This is Newton’s method, which will be treated at length
in Sec. 6.3.
Review Questions
6.1.1 What limits the ﬁnal accuracy of a root computed by the bisection algorithm?
6.1.2 (a) Given a nonlinear scalar equation f (x) = 0 with a simple root α, how can a
ﬁxed-point iteration xn+1 = φ(xn) that converges to α be constructed?
(b) Assume that a ﬁxed point α exists for the mapping x = φ(x). Give sufﬁcient
conditions for convergence of the sequence generated by xn+1 = φ(xn).
(c) How can the conditions in (b) be modiﬁed so that the existence of a ﬁxed point
can be proved?
6.1.3 (a) Deﬁne the concepts “order of convergence” and “asymptotic error constant” for
a convergent sequence {xn} with limn→∞xn = α.
(b) What is meant by sublinear and superlinear convergence? Give examples of
sequences with sublinear and superlinear convergence.
6.1.4 (a) Deﬁne the efﬁciency index of a given iterative method of order p and asymptotic
error constant C ̸= 0.
(b) Determine the order of a new iterative method consisting of m consecutive steps
of the method in (a). What is the order and error constant of this new method? Show
that it has the same efﬁciency index as the ﬁrst method.
6.1.5 (a) When can (passive) Aitken extrapolation be applied to speed up the convergence
of a sequence?
(b) Describe the difference between active and passive Aitken extrapolation.
6.1.6 What two quantities determine the limiting accuracy of a simple root α to the equation
f (x) = 0? Give an example of an ill-conditioned root.
6.1.7 Discuss the choice of termination criteria for iterative methods.
Problems and Computer Exercises
6.1.1 Use graphic representation to determine the zeros of the following functions to one
correct decimal.
(a) 4 sin x + 1 −x;
(b) 1 −x −e−2x;
(c) (x + 1)ex−1 −1;
(d) x4 −4x3 + 2x2 −8;
(e) ex + x2 + x;
(f) ex −x2 −2x −2;
(g) 3x2 + tan x.

Problems and Computer Exercises
625
6.1.2 Show analytically that the equation xe−x = γ has exactly two real roots when
γ < e−1.
6.1.3 Plot the functions f (x) = cosh x and g(x) = 1/ cos x and deduce that the equation
cosh x cos x = 1 has its smallest positive root in the interval (3π/2, 2π). Determine
this root using the bisection method.
6.1.4 The following equations all have a root in the interval (0, 1.6) Determine these with
an error less than 10−8 using the bisection method.
(a) x cos x = ln x;
(b) 2x = e−x;
(c) e−2x = 1 −x.
6.1.5 Locate the real root of the equation
ex(x −1) = e−x(x + 1)
by graphing both sides. Then compute the root with an error less than 10−8 using
bisection. How many bisection steps are needed?
6.1.6 Let k be a given nonnegative number and consider the equation sin x = −k cos x.
This equation has inﬁnitely many roots. Separate the roots, i.e., partition the real
axis into intervals which contain exactly one root.
6.1.7 The choice of mk as the arithmetic mean of ak−1 and bk−1 in the bisection method
minimizes the worst case maximum absolute error. If in the case that ab > 0 we
take instead the geometric mean
mk =

akbk,
then the worst case relative error is minimized. Do Example 6.1.2 using this variation
of the bisection method.
6.1.8 In Example 6.1.6 three different ﬁxed-point iterations were suggested for solving
the equation x + ln x = 0.
(a) Which of the formulas can be used?
(b) Which of the formulas should be used?
(c) Give an even better formula.
6.1.9 Investigate if and to what limit the iteration xn+1 = 2xn−1 sequence converges for
various choices of x0.
6.1.10 (Wittmeyer-Koch)
(a) Show that the iteration xn+1 = φ(xn), where φ(x) = x + (x −1)2, has a ﬁxed
point α = 1, and |φ′(α)| = 1. Then verify that it converges by graphing the iteration
for x0 = 0.6.
(b) Show that for the iteration in (a) if xn = 1 −ϵ, then
|xn+1 −1|
|xn −1|
= 1 −ϵ,
i.e., the asymptotic rate of convergence is sublinear.

626
Chapter 6. Solving Scalar Nonlinear Equations
6.1.11 (a) Consider the ﬁxed-point iteration xn+1 = φ(xn), where φ(x) = x + (x −1)2.
Show that this has a ﬁxed point for α = 1 and that φ′(α) = 1.
(b) Show that the iteration in (a) is convergent for x0 < 1.
6.1.12 Consider the iteration xn+1 = 1 −λx2
n. Illustrate graphically how the iteration
behaves for λ = 0.7, 0.9, 2. (For λ = 2 the iteration is chaotic.)
6.1.13 Apply active Aitken acceleration to the iteration formula sn+1 = φ(sn), where
φ(s) = 1 −1
2s2, until either you have 10 correct decimals, or there are clear indica-
tions that the process is divergent.
(a) with s0 = 0.8;
(b) with s0 = −2.7.
6.2
Methods Based on Interpolation
6.2.1
Method of False Position
Given two initial approximations a0 = a and b0 = b such that f (a)f (b) < 0, a nested
sequence of intervals (a0, b0) ⊃(a1, b1) ⊃(a2, b2) ⊃· · · such that f (an)f (bn) < 0,
n = 0, 1, 2, . . . , can be generated as follows. Given (an, bn), we take xn+1 to be the
intersection of the x-axis and the secant through the point (an, f (an)) and (bn, f (bn)).
Then by Newton’s interpolation formula xn+1 satisﬁes
0 = f (an) + (xn+1 −an)f (an) −f (bn)
an −bn
,
giving
xn+1 = an −f (an)
an −bn
f (an) −f (bn), n = 0, 1, 2, . . . .
(6.2.1)
If f (xn+1)f (an) > 0, set an+1 = xn+1 and bn+1 = bn; otherwise set bn+1 = xn+1 and
an+1 = an. As for bisection, convergence to a root is guaranteed (in exact arithmetic) for a
continuous function f (x). This is the false-position method or, in Latin, regula falsi.179
Note that if f (x) is linear we obtain the root in just one step, but sometimes the rate
of convergence can be much slower than for bisection.
Suppose now that f (x) is convex on [a, b], f (a) < 0, and f (b) > 0, as in Fig-
ure 6.2.1. Then the secant through x = a and x = b will lie above the curve and hence
intersect the x-axis to the left of α. The same is true for all subsequent secants and therefore
the right endpoint b will be kept. The approximations x1, x2, x3, . . . will all lie on the con-
vex side of the curve and cannot go beyond the root α. A similar behavior, with monotone
convergence and one of the points a or b ﬁxed, will occur whenever f ′′(x) exists and has
constant sign on [a, b].
179Regula falsi is a very old method that originated in ﬁfth century Indian texts and was used in medieval Arabic
mathematics. It got its current name from the Italian mathematician Leonardo Pisano, also known as Leonardo
Fibonacci (ca 1170–1250). He is considered to be one of the most talented mathematicians of the Middle Ages.

6.2. Methods Based on Interpolation
627
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
y
x
x1
x0
x2
x3
x4
Figure 6.2.1. The false-position method.
Example 6.2.1.
We apply the method of false position to the f (x) = (x/2)2 −sin x = 0 from Exam-
ple 6.1.2 with initial approximations a0 = 1.5, b1 = 2. We have f (1.5) = −0.434995 < 0
and f (2.0) = +0.090703 > 0, and successive iterates are as follows.
n
xn
f (xn)
hn
1
1.913731 221035
−0.026180060742
−0.019322989205
2
1.933054 210240
−0.000924399645
−0.000675397892
3
1.933729 608132
−0.000031930094
−0.000023321005
4
1.933752 929137
−0.000001102069
−0.000000804916
5
1.933753 734053
Note that f (xn) < 0 for all n ≥0 and consequently bn = 2 is ﬁxed. In the limit convergence
is linear with rate approximately equal to C ≈0.034.
If f is twice continuously differentiable and f ′′(α) ̸= 0, then eventually an interval
will be reached on which f ′′(x) does not change sign. Then, as in the example above, one
of the endpoints (say b) will be retained and an = xn in all future steps. By (6.2.1) the
successive iterations are
xn+1 = xn −f (xn)
xn −b
f (xn) −f (b).
To determine the speed of convergence subtract α and divide by ϵn = xn −α to get
ϵn+1
ϵn
= 1 −f (xn)
xn −α
xn −b
f (xn) −f (b).

628
Chapter 6. Solving Scalar Nonlinear Equations
Since limn→∞xn = α and f (α) = 0, it follows that
lim
n→∞
ϵn+1
ϵn
= C = 1 −(b −α)f ′(α)
f (b) ,
(6.2.2)
which shows that convergence is linear. Convergence will be very slow if f (x) is very ﬂat
near the root α, f (b) is large, and α near b, since then (b −α)f ′(α) ≪f (b) and C ≈1.
6.2.2
The Secant Method
A serious drawback to the method of false position is that ultimately one endpoint of the
sequence of bracketing intervals will become ﬁxed and therefore the length (bn −an) will
not tend to zero. This can be avoided and convergence substantially improved by always
using the secant through the last two points, (xn−1, f (xn−1)) and (xn, f (xn)). The next
approximation xn+1 is determined as the abscissa of the point of intersection between this
secant and the x-axis; see Figure 6.2.2.
1.2
1.4
1.6
1.8
2
2.2
2.4
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
y
x
x0
x1
x2
x3
Figure 6.2.2. The secant method.
Given initial approximations x−1 = a and x0 = b, approximations x1, x2, , x3, . . . are
computed by
xn+1 = xn + hn,
hn = −f (xn)
xn −xn−1
f (xn) −f (xn−1),
n ≥1,
(6.2.3)
assuming that f (xn) ̸= f (xn−1). This is the secant method, which historically predates
Newton’s method.
Notice that although regula falsi and the secant method require two initial approxi-
mations to the root, only one function evaluation per step is needed. The iteration, which
is of the form xn+1 = φ(xn; xn−1), is not a ﬁxed-point iteration as deﬁned in Sec. 6.1.4.
Sometimes methods of this form, which use old information at points xn−k, k ≥1, are called
ﬁxed-point iterations with memory.

6.2. Methods Based on Interpolation
629
When the secant method converges |xn −xn−1| will eventually become small. The
quotient (xn−xn−1)/(f (xn)−f (xn−1)) will then be determined with poor relative accuracy.
If xn and xn−1 are both very close to the root α and not bracketing α, then the resulting
rounding error in xn+1 can become very large. Fortunately, from the error analysis below
it follows that the approximations in general are such that |xn −xn−1| ≫|xn −α| and the
dominant contribution to the roundoff error in xn+1 comes from the error in f (xn). Note
that (6.2.3) should not be “simpliﬁed” to the form
xn+1 = xn−1f (xn) −xnf (xn−1)
f (xn) −f (xn−1)
,
since this formula can give rise to severe difﬁculties with cancellation when xn ≈xn−1 and
f (xn)f (xn−1) > 0. Even (6.2.3) is not always safe to use. We must take care to avoid
overﬂow or division by zero. Without restriction we can assume that |f (xn−1)| ≥|f (xn)| >
0 (otherwise renumber the two points). Then, sn can be computed without risk of overﬂow
from
xn+1 = xn +
sn
1 −sn
(xn −xn−1),
sn =
f (xn)
f (xn−1),
(6.2.4)
where the division with 1 −sn is only carried out if 1 −sn is large enough.
Example 6.2.2.
To illustrate the improved convergence of the secant method we consider once again
the equation f (x) = (x/2)2 −sin x = 0 with initial approximations x0 = 1.5, x1 = 2. The
result is shown in the following table.
n
xn
f (xn)
hn
−1
1.5
−0.434994 986604
0
2.0
+0.090702 573174
−0.086268 778965
1
1.913731 221035
−0.026180 060742
+0.019322 989205
2
1.933054 210240
−0.000924 399645
+0.000707 253882
3
1.933761 464122
+0.000010 180519
−0.000007 704220
4
1.933753 759902
−0.000000 003867
+0.000000 002925
5
1.933753 762827
Note that the approximations x1 and x2 are the same as for the false-position method, but
here x4 is correct to eight decimals and x5 to twelve decimals. The rapid convergence is
partly because x1 = 2 is quite a good initial approximation. But note that although the root
is bracketed by the initial intervals [x0, x1] and [x1, x2] it lies outside the interval [x2, x3].
Assume that f is twice continuously differentiable. Then according to Newton’s
interpolation formula with error term (Theorem 4.3.1) we have
f (x) = f (xn) + (x −xn)[xn−1, xn]f + (x −xn−1)(x −xn)f ′′(ζn)
2
,
(6.2.5)

630
Chapter 6. Solving Scalar Nonlinear Equations
where ζn ∈int(x, xn−1, xn) and
[xn−1, xn]f = f (xn) −f (xn−1)
xn −xn−1
.
To derive an asymptotic formula for the secant method we put x = α in (6.2.5) and subtract
the secant equation 0 = f (xn) + (xn+1 −xn)[xn−1, xn]f . Since f (α) = 0 we get
(α −xn+1)[xn−1, xn]f + (α −xn−1)(α −xn)f ′′(ζn)/2 = 0,
where ζn ∈int(α, xn−1, xn). According to the mean value theorem, we have
[xn−1, xn]f = f ′(ζ ′
n),
ζ ′
n ∈int(xn−1, xn),
and it follows that
ϵn+1 = −1
2
f ′′(ζn)
f ′(ζ ′n) ϵnϵn−1.
(6.2.6)
Example 6.2.3.
The ratios ϵn+1/(ϵnϵn−1) in Example 6.2.2 are equal to 0.697, 0.527, 0.550, n = 1 : 3,
which compares well with the limiting value 0.543 of 1
2f ′′(α)/f ′(α).
From (6.2.6) it can be deduced that the secant method always converges from starting
values x0, x1 sufﬁciently close to α. For this to be true it sufﬁces that the ﬁrst derivative
f ′(x) is continuous, since then
ϵn+1 =

1 −f ′(ξn)
f ′(ζn)

ϵn,
ξn ∈int(xn−1, α),
ζn ∈int(xn, xn−1).
But in the secant method there is no guarantee that the computed sequence of approximations
stays in the initial interval [x0, x1]. Unlike the steady convergence of the bisection method
things can go seriously wrong using the secant method! A remedy will be discussed in
Sec. 6.2.4.
The following theorem gives the order of convergence for the secant method.
Theorem 6.2.1.
Suppose that f (x) is twice continuously differentiable and that in a neighborhood I
of the root α, containing x0, x1, x2, . . . , xn, we have
1
2
((((
f ′′(y)
f ′(x)
(((( ≤M,
x, y ∈I.
Denote by ϵn the error in the nth iterate of the secant iteration Let q be the unique positive
root of the equation µ2 −µ −1 = 0 and set
K = max

M|ϵ0|, (M|ϵ1|)1/q
.
(6.2.7)
Then it holds that
|ϵn| ≤1
M Kqn,
n = 0, 1, 2, . . . ;
(6.2.8)
i.e., the secant iteration has convergence order equal to q = (1 +
√
5)/2 = 1.618 . . . .

6.2. Methods Based on Interpolation
631
Proof. The proof is by induction. From the choice of K it follows that (6.2.8) is trivially
true for n = 0, 1. Suppose that (6.2.8) holds for n and n −1. Then since q2 = q + 1 it
follows using the assumption and (6.2.6) that
|ϵn+1| ≤M|ϵn| |ϵn−1| ≤1
M KqnKqn−1 = 1
M Kqn+qn−1 = 1
M Kqn+1.
(6.2.9)
To compare the efﬁciency of the secant method and Newton’s method, which is
quadratically convergent, we use the efﬁciency index introduced in Sec. 6.1.5. Assume that
the work to compute f ′(x) is θ times the amount of work required to compute f (x). Then,
with the same amount of work we can perform k(1 + θ) iterations with the secant method
and k iterations with Newton’s method. Equating the errors we get (mϵ0)2k = (mϵ0)pk(1+θ),
where q = 1.618 . . . . Hence the errors are the same for both methods when pk(1+θ) = 2k or
(1 + θ) log
1
2(1 +
√
5)

= log 2,
which gives θ = 0.4404 . . . . Thus, from this analysis we conclude that if θ > 0.44, then
the secant method is asymptotically more efﬁcient than Newton’s method.
In Example 6.2.2 we can observe that the error ϵn = xn −αn changes sign at every
third step. Hence, in this example,
α ∈int(xn+1 −xn),
n = 0, 1, 3, 4, . . . ,
i.e., the root α is bracketed by xn and xn+1 except for every third step. We shall show that
this is no coincidence. Assume that xn ∈(a, b), n = 0, 1, 2, . . . , and that f ′(x) ̸= 0 and
f ′′(x) does not change sign in (a, b). Then from (6.2.6) it follows that the ratio
ϵn+1
ϵnϵn−1
will have constant sign for all n. Then if α ∈int(x0, x1) and ϵ0ϵ1 < 0, it follows that
the sign of ϵn must change every third step according to one of the following two schemes
(verify this!):
· · · + −+ + −+ + −+ + · · · ;
· · · + −−+ −−+ −−+ · · · .
Hence convergence for the secant method, if it occurs, will take place in a waltz rhythm!
This means that at every third step the last two iterates xn−1 and xn will not always bracket
the root.
6.2.3
Higher-Order Interpolation Methods
The secant method does not achieve quadratic convergence. Indeed, it can be shown that
under very weak restrictions no iterative method using only one function evaluation per step
can achieve this. In Steffensen’s method
xn+1 = xn −f (xn)
g(xn) ,
g(xn) = f (xn + f (xn)) −f (xn)
f (xn)
,
(6.2.10)

632
Chapter 6. Solving Scalar Nonlinear Equations
two function evaluations are used. This method can be viewed as a variant of the secant
method, where the step size used to approximate the ﬁrst derivative goes to zero as f (x).
To show quadratic convergence we put βn = f (xn) and expand g(xn) in a Taylor
series about xn; we get
g(xn) = f ′(xn)

1 −1
2hnf ′′(xn) + O(β2
n)

,
where hn = −f (xn)/f ′(xn) is the Newton correction. Thus,
xn+1 = xn + hn

1 + 1
2hnf ′′(xn) + O(β2
n)

.
Using the error equation for Newton’s method we get
ϵn+1
ϵ2n
→1
2
f ′′(α)
f ′(α)

1 + f ′(α)

,
where ϵn = xn −α. This shows that Steffenson’s method is of second order.
Steffensen’s method is of particular interest for solving systems of nonlinear equations.
For a generalization to nonlinear operator equations on a Banach space, see [213].
In the secant method linear interpolation through (xn−1,fn−1) and (xn, fn) is used
to determine the next approximation to the root. A natural generalization is to use an
interpolating method of higher order. Let xn−r, . . . , xn−1, xn be r+1 distinct approximations
and determine the (unique) polynomial p(x) of degree r interpolating (xn−j, f (xn−j)),
j = 0 : r. By Newton’s interpolation formula (Sec. 4.2.1) the interpolating polynomial is
p(x) = fn + [xn, xn−1]f · (x −xn) +
r

j=2
[xn, xn−1, . . . , xn−j]f Mj(x),
where
Mj(x) = (x −xn)(x −xn−1) · · · (x −xn−j).
The next approximation xn+1 is taken as the real root to the equation p(x) = 0 closest to
xn and xn−r is deleted. Suppose the interpolation points lie in an interval J, which contains
the root α and in which f ′(x) ̸= 0. It can be shown that if there is at least one interpolation
point on each side of α, then p(x) = 0 has a real root in J. Further, the following formula
for the error holds (Traub [354, pp. 67–75]):
ϵn+1 = −
f (r+1)(ζn)
(r + 1)!p′(ηn)
r&
i=0
ϵn−i,
(6.2.11)
where ζn ∈int (α, xn−1, xn) and ηn ∈int (α, xn+1). (Recall that by int (a, b, . . . , w) we
denote the smallest interval that contains the points a, b, . . . , w.) In the special case r = 2
we get the quadratic equation
p(x) = fn + (x −xn)[xn, xn−1]f + (x −xn)(x −xn−1)[xn, xn−1, xn−2]f.
(6.2.12)

6.2. Methods Based on Interpolation
633
We assume that [xn, xn−1, xn−2]f ̸= 0 since otherwise the method degenerates into the
secant method. Setting hn = (x −xn) and writing (x −xn−1) = hn + (xn −xn−1), this
equation becomes
h2
n[xn, xn−1, xn−2]f + ωhn + fn = 0,
(6.2.13)
where
ω = [xn, xn−1]f + (xn −xn−1)[xn, xn−1, xn−2]f.
(6.2.14)
The root closest to xn corresponds to the root hn of smallest absolute value to (6.2.13).
To express this root in a numerically stable way the standard formula for the roots of a
quadratic equation should be multiplied by its conjugate quantity (see Example 2.3.3).
Using this formula we get
xn+1 = xn + hn,
hn = −
2fn
ω ±

ω2 −4fn [xn, xn−1, xn−2]f
,
(6.2.15)
where the sign in the denominator should be chosen so as to minimize |hn|. This is the
Muller–Traub method.
A drawback is that the equation (6.2.13) may not have a real root even if a real zero is
being sought. On the other hand, this means that the Muller–Traub method has the useful
property that complex roots may be found from real starting approximations.
By (6.2.11) it follows that
ϵn+1 = −f ′′′(ζn)
3! p′(ηn)ϵnϵn−1ϵn.
(6.2.16)
It can be shown that the convergence order for the Muller–Traub method is at least q =
1.839 . . . , which equals the largest root of the equation µ3 −µ2 −µ −1 = 0 (cf. Theo-
rem 6.2.1). Hence this method does not quite achieve quadratic convergence.
For r > 2 there are no useful explicit formulas for determining the zeros of the
interpolating polynomial p(x). Then we can proceed as follows. We write the equation
p(x) = 0 in the form x = xn + F(x), where
F(x) ≡
−fn −r
j=2[xn, xn−1, . . . , xn−j]f Mj(x)
[xn, xn−1]f
(cf. Sec. 4.2.2). Then a ﬁxed-point iteration can be used to solve for x. To get the ﬁrst
guess x0 we ignore the sum (this means using the secant method) and then iterate, xi =
xn + F(xi−1), i = 1, 2, . . . , until xi and xi−1 are close enough.
Suppose that xn−j −xn = O(h), j = 1 : r, where h is some small parameter in the
context (usually some step size). Then Mj(x) = O(hj), M′
j(x) = O(hj−1). The divided
differences are O(1), and we assume that [xn, xn−1]f is bounded away from zero. Then
the terms of the sum decrease like hj. The convergence ratio F ′(x) is here approximately
M′
2(x)[xn, xn−1, xn−2]f
[xn, xn−1]f
= O(h).
Thus, if h is small enough, the iterations converge rapidly.

634
Chapter 6. Solving Scalar Nonlinear Equations
A different way to extend the secant method is to use inverse interpolation. Assume
that yn, yn−1, . . . , yn−r are distinct and let q(y) be the unique polynomial in y interpo-
lating the values xn, xn−1, . . . , xn−r. Reversing the rule of x and y and using Newton’s
interpolation formula, this interpolating polynomial is
q(y) = xn + [yn, yn−1]g · (y −yn) +
r

j=2
[yn, yn−1, . . . , yn−j]f [j(y),
where g(yn−j) = xn−j, j = 0 : r,
[j(y) = (y −yn)(y −yn−1) · · · (y −yn−j).
The next approximation is then taken to be xn+1 = q(0), i.e.,
xn+1 = xn −yn [yn, yn−1]g +
r

j=2
[yn, yn−1, . . . , yn−j]g [j(0).
For r = 1 there is no difference between direct and inverse interpolation and we
recover the secant method. For r > 1 inverse interpolation as a rule gives different re-
sults. Inverse interpolation has the advantage of not requiring the solution of a polynomial
equation. (For other ways of avoiding this see Problems 6.2.3 and 6.2.4.) The case r = 2
corresponds to inverse quadratic interpolation:
xn+1 = xn −yn [yn, yn−1]g + ynyn−1 [yn, yn−1, yn−2]g.
(6.2.17)
Note that it has to be required that yn, yn−1, and yn−2 are distinct. This method has the same
order of convergence as the Muller–Traub method.
Even if this is the case it is not always safe to compute xn+1 from (6.2.17). Care has
to be taken in order to avoid overﬂow and possibly division by zero. If we assume that
0 ̸= |yn| ≤|yn−1| ≤|yn−2|, then it is safe to compute
sn = yn/yn−1,
sn−1 = yn−1/yn−2,
rn = yn/yn−2 = snsn−1.
We can rewrite (6.2.17) in the form xn+1 = xn + pn/qn, where
pn = sn[(1 −rn)(xn −xn−1) −sn−1(sn−1 −rn)(xn −xn−2)],
qn = (1 −sn)(1 −sn−1)(1 −rn).
The ﬁnal division pn/qn is only carried out if the correction is sufﬁciently small.
6.2.4
A Robust Hybrid Method
Efﬁcient and robust root ﬁnders can be constructed by combining the secant method (or some
higher-order interpolation method) with bisection. A particularly elegant combination of
bisection and the secant method was developed in the 1960s by van Wijngaarden, Dekker,
and others at the Mathematical Center in Amsterdam; see [61]. It uses a rapidly convergent
method for smooth functions and the slow but sure bisection method when necessary. This

Review Questions
635
algorithm was taken up and improved by Brent [44]; see also [123, Section 7.2]. In contrast
to Dekker’s algorithm, Brent’s new algorithm zeroin never converges much more slowly
than bisection.
In each of the iterations of zeroin three approximations a, b, and c to the root are
present. Initially we are given an interval [a, b] such that f (a) and f (b) have opposite sign,
and set c = a. In the following steps a, b, and c are arranged so that
• f (a) and f (b) have opposite sign, and |f (b)| ≤|f (a)|.
• b is the latest iteration.
• c is the value of b in the previous step.
At all times a and b bracket the zero and the length |b −a| of the interval shrinks in each
iteration.
If c = a a secant step is taken; otherwise inverse quadratic interpolation is used. If the
computed step gives an approximation in [a, b] (and not too close to one of the endpoints)
take it; otherwise a bisection step is taken. The iterations are terminated if f (b) = 0, or
|b −a| ≤tol + 4u|b|,
where tol is a user tolerance and u the unit roundoff. The midpoint r = b + 0.5(a −b) is
then returned as the root.
Brent claims that his version of zeroin will always converge, even in ﬂoating-point
arithmetic. Further, if f has a continuous second derivative the method will eventually
converge at least as fast as the secant method. In this case typically about ten function eval-
uations are needed. He never found a function requiring more than three times the number
of evaluations needed for bisection. On average, using inverse quadratic interpolation when
possible saves 0.5 function evaluations over using only the secant method.
The MATLAB function fzero, which ﬁnds a zero near a given approximation x0,
is based on zeroin. A discussion of a slightly simpliﬁed version of fzero is given in
Moler [268, Chap. 4.7].
Review Questions
6.2.1 Sketch a function f (x) with a root in (a, b) such that regula falsi converges very
slowly.
6.2.2 Outline how the secant method can be safeguarded by combining it with the bisection
method.
6.2.3 What property should the function f (x) have to be unimodal on the interval [a, b]?
6.2.4 Discuss root ﬁnding methods based on quadratic interpolation (the Muller–Traub
method) and inverse quadratic interpolation. What are the merits of these two ap-
proaches?

636
Chapter 6. Solving Scalar Nonlinear Equations
Problems and Computer Exercises
6.2.1 Use the secant method to determine the roots of the following equations to six correct
decimals.
(a) 2x = e−x;
(b) tan x + cosh x = 0.
6.2.2 Assume that we have fnfn−1 < 0, and have computed xn+1. If fn+1fn < 0, then
in the next step we compute xn+2 by a secant step; otherwise we use a line through
(xn+1, fn+1) and (xn−1, θfn−1), where 0 < θ < 1. Clearly, θ = 1 corresponds to a
step with the method of false position and will usually give fn+2fn+1 > 0. On the
other hand, θ = 0 gives xn+1 = xn, and thus fn+1fn < 0. Hence a suitable choice of
θ will always give fn+2fn+1 < 0.
Show that in a modiﬁed step ϵn+1 ≈−ϵn when θ = 0.5. Deduce that the resulting
algorithm gives cubic convergence with three function evaluations and hence has
efﬁciency index E = 31/3 = 1.4422 . . . .180
6.2.3 Another modiﬁcation of the secant method can be derived by estimating f ′(xn) in
Newton’s method by quadratic interpolation through the points xn, xn−1, xn−2. Show
that the resulting method can be written xn+1 = xn −f (xn)/ω, where
ω = f [xn, xn−1] + (xn −xn−1)f [xn, xn−1, xn−2].
6.2.4 The Muller–Traub method uses three points to determine the coefﬁcient of an inter-
polating parabola. The same points can also be interpolated by a rational function of
the form
g(x) = x −a
bx + c.
An iterative method is derived by taking xn+1 equal to the root a of g(x) = 0.
(a)Showthatthisisequivalenttocalculatingxn+1 fromthe“modiﬁedsecantformula”:
xn+1 = xn −fn
xn −xn−2
fn −˜fn−2
,
˜fn−2 = fn−2
f [xn, xn−1]
f [xn−1, xn−2].
Hint: Use a theorem in projective geometry, according to which the cross ratio of any
four values of x is equal to the cross ratio of the corresponding values of g(x) (see
Householder [204, p. 159]). Hence
(0 −fn)/(0 −fn−2)
(yn−1 −fn)/(yn−1 −fn−2) = (xn+1 −xn)/(xn+1 −xn−2)
(xn−1 −xn)/(xn−1 −xn−2).
(b) Use the result in (a) to show that xn−1 ∈int(xn−2, xn) if
sign(yn) = −sign(yn−2),
sign(y[xn, xn−1]) = sign(y[xn−1, xn−2]).
180The resulting modiﬁed rule of false position is often called, after its origin, the Illinois method. It is due
originally to the staff of the computer center at the University of Illinois in the early 1950s.

6.3. Methods Using Derivatives
637
6.2.5 The result in Problem 6.2.4 suggests that the Illinois method in Problem 6.2.2 should
be modiﬁed by taking
β = f [xn+1, xn]/f [xn, xn−1],
θ =
% β
if β > 0,
1
2
if β ≤0.
Implement this modiﬁed method. Compare it with the unmodiﬁed Illinois method
and with the safeguarded secant algorithm. As test equations use the following:
(a) A curve with one inﬂection point on [0, 1]:
f (x) = x2 −(1 −x)n, a = 25, b = 1, n = 2, 5, 10.
(b) A family of curves which lie increasingly close to the x-axis for large n:
f (x) = e−nx(x −1) + xn, a = 0.25, b = 1, n = 5, 10, 15.
(c) A family of curves with the y-axis asymptotic:
f (x) = (nx −1)/((n −1)x), a = 0.01, b = 1, n = 2, 5, 10.
6.3
Methods Using Derivatives
6.3.1
Newton’s Method
In Newton’s method for solving an equation f (x) = 0 the curve y = f (x) is approximated
by its tangent at the point (xn, f (xn)), where xn is the current approximation to the root.
Assuming that f ′(xn) ̸= 0, the next approximation xn+1 is then determined as the abscissa
of the point of intersection of the tangent with the x-axis (see Figure 1.1.3).
When f ′(x) is available Newton’s method is usually the method of choice. It is
equivalent to replacing the equation f (x) = 0 by
f (xn) + (x −xn)f ′(xn) = 0,
(6.3.1)
which is obtained by taking the linear part of the Taylor expansion of f (x) at xn. Hence if
f ′(xn) ̸= 0, then
xn+1 = xn + hn,
hn = −f (xn)/f ′(xn).
(6.3.2)
Clearly Newton’s method can also be viewed as the limit of the secant method when the
two interpolation points coalesce.
Example 6.3.1.
We want to compute the unique positive root of the equation f (x) = (x/2)2−sin x =
0(cf.Example6.2.2)forwhichf ′(x) = x/2−cos x. ThefollowingNewtoniterates, starting
from x0 = 1.8, are given in the table below (correct digits in xn shown in bold).
n
xn
f (xn)
f ′(xn)
hn
0
1.8
−0.163847 630878
1.127202 094693
−0.145357 812631
1
1.945357 812631
0.015436 106659
1.338543 359427
0.011532 018406
2
1.933825 794225
0.000095 223283
1.322020 778469
0.000072 028582
3
1.933753 765643
0.000000 003722
1.3219174 29113
0.000000 002816
4
1.933753 762827

638
Chapter 6. Solving Scalar Nonlinear Equations
The number of correct digits approximately doubles in each iteration until the limiting
precision is reached. Although the initial approximation is not very good, already x4 is
correct to 12 decimals!
If the iterations are broken off when |hn| < δ it can be shown (see the error analysis
below) that the truncation error is less than δ, provided that |Khn| ≤1/2, where K is
an upper bound for |f ′′/f ′| in the neighborhood of the root. This restriction is seldom of
practical importance. But rounding errors made in computing hn must also be taken into
account.
Note that when the root is approached, the relative precision in the computed values
of f (xn) usually decreases. Since f ′(xn) is only used for computing hn it need not be
computed to much greater relative precision than f (xn). In the above example we could
have used f ′(x2) instead of f ′(xn) for n > 2 without much effect on the accuracy. Such a
simpliﬁcation is of great importance when Newton’s method is used on systems of nonlinear
equations.
We now consider the local convergence of Newton’s method, i.e., the convergence in
a neighborhood of a simple root α.
Theorem 6.3.1.
Assume that α is a simple root of the equation f (x) = 0, i.e., f ′(α) ̸= 0. If f ′ exists
and is continuous in a neighborhood of α, then the convergence order of Newton’s method
is at least equal to two.
Proof. A Taylor expansion of f yields
0 = f (α) = f (xn) + (α −xn)f ′(xn) + 1
2(α −xn)2f ′′(ζn),
ζn ∈int(xn, α).
Subtracting f (xn) + (xn+1 −xn)f ′(xn) = 0 and solving for ϵn+1 = xn+1 −α we get
ϵn+1 = 1
2ϵ2
n
f ′′(ζn)
f ′(xn) ,
ζn ∈int(xn, α).
(6.3.3)
Provided that f ′(α) ̸= 0, it follows that (6.1.22) is satisﬁed with p = 2 and the asymptotic
error constant is
C = 1
2
((((
f ′′(α)
f ′(α)
(((( .
(6.3.4)
If f ′′(α) ̸= 0, then C > 0 and the rate of convergence is quadratic.
The following classical theorem gives rigorous conditions for the quadratic conver-
gence of Newton’s method. In particular, it is not necessary to assume the existence of a
solution.
Theorem 6.3.2 (Ostrowski [279, Theorem 7.1]).
Let f (x) be a real function, f (x0)f ′(x0) ̸= 0, and put h0 = f (x0)/f ′(x0). Assume
that f ′′(x) exists in J0 = int [x0, x0 + 2h0], and that
2M|h0| ≤|f ′(x0)|,
M = sup
x∈J0
|f ′′(x)|.
(6.3.5)

6.3. Methods Using Derivatives
639
Let the sequence xk, k = 1, 2, . . . , be generated by Newton’s method:
xk+1 = xk −f (xk)/f ′(xk),
k = 0, 1, . . . .
(6.3.6)
Then xk ∈J0, and we have limk→∞xk = α, where α is the only zero of f (x) in J0. Unless
α = x0 + 2h0, α is a simple zero.181 Further, it holds that
|xk+1 −xk| ≤
M
2|f ′(xk)||xk −xk−1|2,
k = 1, 2, . . . ,
(6.3.7)
|α −xk+1| ≤
M
2|f ′(xk)||xk −xk−1|2,
k = 1, 2, . . . .
(6.3.8)
Proof. We have by (6.3.5) that |f ′(x) −f ′(x0)| ≤|x −x0|M, and
|f ′(x1) −f ′(x0)| ≤|h0|M ≤1
2|f ′(x0)|.
It follows that
|f ′(x1)| ≥|f ′(x0)| −|f ′(x1) −f ′(x0)| ≥1
2|f ′(x0)|.
(6.3.9)
Integrating by parts we have
 x1
x0
(x1 −x)f ′′(x) dx = −(x1 −x0)f ′(x0) + f (x1) −f (x0) = f (x1).
Introducing a new variable by x = x0 + th, this becomes
f (x1) = h2
0
 1
0
(1 −t)f ′′(x0 + th) dt,
and it now follows that
|f (x1)| = |h0|2
 1
0
(1 −t)|f ′′(x0 + th)| dt ≤M|h0|2
 1
0
(1 −t) dt = 1
2M|h0|2. (6.3.10)
Using (6.3.9) and (6.3.10) gives
|h1| ≤M|h0|2
|f ′(x0)|,
(6.3.11)
and applying (6.3.10) gives
2M|h1|
|f ′(x1)| ≤
2(M|h0|)2
|f ′(x0)||f ′(x1)| ≤
(2M|h0|)
|f ′(x0)|
2
.
By (6.3.5) the expression in parenthesis is ≤1 and hence
2M|h1| ≤|f ′(x1)|,
|h1| ≤1
2|h0|.
(6.3.12)
181It can be proved that if α = x0 + 2h0, then f (x) is a quadratic polynomial with the double root α; see [279,
Chapter 40].

640
Chapter 6. Solving Scalar Nonlinear Equations
This shows that the point x2 will not get beyond the distance 1
2|h0| from x1 and will remain
in J0, and int [x1, x1 + 2h1] ∈J0.
We can now use induction to prove that the intervals Jk = int [xk, xk + 2hk], k =
1, 2, . . . , lie in J0, that Jk+1 ∈Jk, and that the length of Jk+1 is at most half of the length of
Jk. Since J0 is closed it follows that
lim
k→∞xk = α ∈J0.
To show that α is a root of f (x) we note that from (6.3.6) it follows that f ′(xk)(xk −xk+1) =
f (xk). Taking the limit it follows that limk→∞f (xk) = f (α) = 0.
By (6.3.5) we have for all x ∈J0
|f ′(x) −f ′(x0)| ≤|x −x0|M ≤2M|h0|.
Assume that |x −x0| < |h0|, i.e., x0 lies in the interior of J0. Then
|f ′(x) −f ′(x0)| < 2M|h0| ≤|f ′(x0)|,
so that f ′(x) ̸= 0 for all x in the interior of J0. Therefore, α must be a simple root if it lies
in the interior of J0. Further, since f ′(x) does not vanish in J0, f (x) is strictly monotonic
in J0 and thus has only one root.
The assertion (6.3.8) is equivalent to
|hk| ≤M|hk−1|2
2|f ′(xk)| .
Since our starting assumption (6.3.5) is true for all k ≥0, we have |f (xk)| ≤1
2M|hk−1|2,
and hence the assertion follows using (6.3.6). To prove (6.3.8) we notice that α lies in an
interval with center xk+1 and radius |hk|, and use (6.3.8).
Note that the relation (6.3.8) between the errors holds only as long as the roundoff
errors in the calculations can be ignored. As pointed out in Sec. 6.1.3, the accuracy which
can be achieved in calculating the root is always limited by the accuracy in the computed
values of f (x).
So far we have assumed that α is a simple root. Suppose now that α is a root of
multiplicity q > 1. Then by Taylor’s formula we have (cf. (6.1.9))
f ′(x) =
1
(q −1)!(x −α)q−1f (q)(ξ ′),
ξ ′ ∈int(x, α).
It follows that if xn is close to α, then the Newton correction will satisfy
hn = f (xn)
f ′(xn) ≈1
q (xn −α) = 1
q ϵn.
For the corresponding errors we have
ϵn+1 = ϵn −ϵn/q = (1 −1/q)ϵn,

6.3. Methods Using Derivatives
641
which shows that for a root of multiplicity q > 1 Newton’s method only converges linearly
with rate C = 1 −1/q. (The same is true of other methods which have quadratic or higher
rates of convergence for simple roots.) For q > 2 this is much slower even than for the
bisection method! For a root of multiplicity q > 1 convergence is linear with rate 1 −1/q.
For q = 20 it will take 45 iterations to gain one more decimal digit.
Note also that when xn →α, f (xn) →0 and f ′(xn) →0. Therefore, rounding
errors may seriously affect the Newton correction when evaluated close to α, This clearly
is related to the lower limiting accuracy for multiple roots as given by (6.1.10).
When the multiplicity q of a root is known a priori the modiﬁed Newton’s method
xn+1 = xn −q f (xn)
f ′(xn)
(6.3.13)
is easily shown to have quadratic convergence. For a root of unknown multiplicity we can
use the following observation. From (6.1.9) it follows that if the equation f (x) = 0 has a
multiple root at x = α, then α is a simple root of the equation
u(x) = 0,
u(x) = f (x)/f ′(x).
(6.3.14)
Hence, if Newton’s method is applied to this equation it will have a quadratic rate of
convergence independent of the multiplicity of α as a root to f (x) = 0. The Newton
iteration for u(x) = 0 becomes
xn+1 = xn −
f (xn)
f ′(xn) −f (xn)f ′′(xn)/f ′(xn),
(6.3.15)
and thus requires the evaluation also of f ′′(xn).
Under certain regularity assumptions Newton’s method is locally convergent from
a sufﬁciently good initial approximation. But in general Newton’s method is not globally
convergent, i.e., it does not converge from an arbitrary starting point. This is not surprising
since it is based on a series of local approximations.
It is easy to construct examples where Newton’s method converges very slowly or
not at all.
Example 6.3.2.
The equation f (x) = sin x = 0 has exactly one root α = 0 in the interval |x| < π/2.
Newton’s method becomes
xn+1 = xn −tan xn,
n = 0, 1, 2, . . . .
If we choose the initial value x0 = z such that tan z = 2z, then x1 = −x0, x2 = −x1 = x0.
Hence the successive approximations show a cyclic behavior.
Newton’s method will converge for any starting value such that |x0| < z. The critical
value can be shown to be x0 = 1.16556 . . . .
Example 6.3.3.
In Example 5.2.5 we encountered the function u(x) deﬁned for x ≥0 by
x = u ln u.
(6.3.16)

642
Chapter 6. Solving Scalar Nonlinear Equations
The function u(x) is related to Lambert’s W-function w(x) (see Problem 3.1.12 (d) and
[370]), which is the inverse of the function x(w) = wew. Clearly u(x) = ew(x). It can be
shown that u(x) is a smooth and slowly varying function x.
In Figure 6.3.1 we show a plot of the function x = u ln u. For the inverse function
u(x) we have upr(x) = 1/(1 + lnu(x)). Clearly u(0) = 1, u′(0) = 1, and for x > 0 u′(x),
is positive and decreasing, while u′′(x) is negative and decreasing in absolute value.
0
0.5
1
1.5
2
2.5
3
3.5
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
u
x
Figure 6.3.1. The function x = u ln u.
To compute u(x) to high accuracy for a given value of x > 0, we can use Newton’s
method to solve the equation
f (u) = u ln u −x = 0.
This takes the form
uk+1 =
uk + x
1 + ln uk
,
k = 0, 1, 2, . . . .
For x > 10, we use as starting value u0 the asymptotic approximation u(x) ∼x/ ln x = u0,
x →∞. For 0 < x ≤10 the approximation
u0 = 1.0125 + 0.8577x −0.129013x2 + 0.208645x3 −0.00176148x4 + 0.000057941x5,
derived by Gautschi from a truncated Chebyshev expansion of u(x), has a maximum error
of about 1%. It is left to Problem 6.3.6 to investigate how efﬁcient the outlined Newton
method is for computing u(x).
Global Convergence
In some simple cases the global convergence of Newton’s method may be easy to verify.
Two examples are given in the following theorems.

6.3. Methods Using Derivatives
643
Theorem 6.3.3.
Suppose that f ′(x)f ′′(x) ̸= 0 in an interval [a, b], where f ′′(x) is continuous and
f (a)f (b) < 0. Then if
((((
f (a)
f ′(a)
(((( < b −a,
((((
f (b)
f ′(b)
(((( < b −a,
Newton’s method converges from an arbitrary x0 ∈[a, b].
Proof. The theorem follows easily by inspecting Figure 6.3.2.
1.4
1.6
1.8
2
2.2
2.4
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
y
x
a
b
f(a)/f′(a)
f(b)/f′(b)
Figure 6.3.2. A situation where Newton’s method converges from any x0 ∈[a, b].
Lemma 6.3.4.
Let [a, b] be an interval such that f (a)f (b) < 0. Assume that the so-called Fourier
conditions are satisﬁed, i.e.,
f ′(x)f ′′(x) ̸= 0,
x ∈[a, b],
(6.3.17)
withf ′′(x)continuousandf (x0)f ′′(x0) > 0, forx0 ∈[a, b]. Thenthesequence{x0, x1, x2, . . .}
generated by Newton’s method converges monotonically to a root α ∈[a, b].
Proof. We can assume that f ′′(x) > 0; otherwise consider the equation −f (x) = 0.
Assume ﬁrst that f ′(x) < 0 in the interval. Since by assumption f (x0) ≥0, this corresponds
to the situation in Figure 6.3.2, with b = x0 > α. Clearly, it holds that x1 > x0 and since
the curve lies to the left of the tangent in x0 we also have x1 > α. The case when f ′(x) < 0
can be treated similarly. The theorem now follows by induction.
Newton’s method can be safeguarded by taking a bisection step whenever a Newton
step “fails” in some sense. Assume that initially a < b, f (a)f (b) < 0, and x is equal to

644
Chapter 6. Solving Scalar Nonlinear Equations
a or b. At each step a new approximation x′ is computed and a, b are updated to a′, b′ as
follows:
• If the Newton iterate x′ = x −f (x)/f ′(x) lies in (a, b), then accept x′; otherwise
take a bisection step, i.e., set x′ = (a + b)/2.
• Set either a′ = x, b′ = b or a′ = a, b′ = x, where the choice is made so that
f (a′)f (b′) ≤0.
This ensures that at each step the interval [a′, b′] contains a root.
When checking if z ∈(a, b), it is important to avoid division by f ′(x), since this may
cause overﬂow or division by zero. Hence, we note z ∈(a, b) if and only if
b −z = b −x + f (x)/f ′(x) > 0
and
z −a = x −a −f (x)/f ′(x) > 0.
If f ′(x) > 0 these two inequalities are equivalent to
(x −b)f ′(x) < f (x) < (x −a)f ′(x).
The case when f ′(x) < 0 is analyzed similarly, giving
(x −a)f ′(x) < f (x) < (x −b)f ′(x).
In either case only one of the inequalities will be nontrivial depending on whether f (x) > 0.
6.3.2
Newton’s Method for Complex Roots
Newton’s method is based on approximating f with the linear part of its Taylor expansion.
Taylor’s theorem is valid for a complex function f (z) around a point of analyticity (see
Sec. 3.1.2). Thus Newton’s method applies also to an equation f (z) = 0, where f (z) is a
complex function, analytic in a neighborhood of a root α. An important example is when
f is a polynomial; see Sec. 6.5.
The geometry of the complex Newton iteration has been studied by Yao and Ben-
Israel [387]. Let z = x + iy, f (z) = u(x, y) + iv(x, y), and consider the absolute value of
f (z):
φ(x, y) = |f (x + iy)| =

u(x, y)2 + v(x, y)2.
This is a differentiable function as a function of (x, y), except where f (z) = 0. The gradient
of φ(x, y) is
grad φ = (φx, φy) = 1
φ

uux + vvx, uuy + vvy

,
(6.3.18)
where ux = ∂u/∂x, uy = ∂u/∂y, etc. Using the Cauchy–Riemann equations ux = vy,
uy = −vx, we calculate (see Henrici [196, Sec. 6.1.4])
f (z)
f ′(z) =
u + iv
ux + ivx
= (uux + vvx) + i(uuy + vvy)
u2x + v2x
.
A comparison with (6.3.18) shows that the Newton step
zk+1 = zk −f (zk)/f ′(zk),
k = 0, 1, . . . ,
(6.3.19)

6.3. Methods Using Derivatives
645
is in the direction of the negative gradient of |f (zk)|, i.e., in the direction of strongest
decrease of |f (z)|.
Theorem 6.3.5.
Let the function f (z) = f (x + iy) be analytic and zk = xk + iyk be a point such
that f (zk)f ′(zk) ̸= 0. Let zk+1 be the next iterate of Newton’s method (6.3.19). Then
zk+1 −zk is in the direction of the negative gradient of φ(x, y) = |f (x + iy)| and therefore
orthogonal to the level set of |f | at (xk, yk). If Tk is the tangent plane of φ(x, y) at (xk, yk)
and Lk is the line of intersection of Tk with the (x, y)-plane, then (xk+1, yk+1) is the point
on Lk closest to (xk, yk).
Proof. See [387].
Newton’s method is very efﬁcient if started from an initial approximation sufﬁciently
close to a simple zero. If this is not the case Newton’s method may converge slowly or
even diverge. In general, there is no guarantee that zn+1 is closer to the root than zn, and if
f ′(zn) = 0 the next iterate is not even deﬁned.
It is straightforward to generalize the results in Theorem 6.3.2 to the complex case. In
the case of a complex function f (x) of a complex variable the interval I0 = int [x0, x0+2h0]
can be replaced by a disk K0 : |z −z1| ≤|h0|.
Theorem 6.3.6.
Let f (z) be a complex function of a complex variable. Let f (z0)f ′(z0) ̸= 0 and set
h0 = −f (z0)/f ′(z0), x1 = x0 + h0. Assume that f (z) is twice continuously differentiable
in the disk K0 : |z −z1| ≤|h0|, and that
2 |h0|M2 ≤|f ′(z0)|,
M2 = max
z∈K0 |f ′′(x)|.
(6.3.20)
Let zk be generated by Newton’s method:
zk+1 = zk −f (zk)
f ′(zk),
k = 1, 2, . . . .
Then zk ∈K0 and we have limk→∞zk = ζ, where ζ is the only zero of f (z) in K0. Unless
ζ lies on the boundary of K0, ζ is a simple zero. Further, we have the relations
|ζ −zk+1| ≤
M2
2|f ′(zk)||zk −zk−1|2,
k = 1, 2, . . . .
(6.3.21)
A generalization of this theorem to systems of nonlinear equations is the famous
Newton–Kantorovich theorem; see Volume II, Sec. 11.1.
Since the Newton step is in the direction of the negative gradient of |f (z)| at z = zk, it
will necessarily give a decrease in |f (zk)| if a short enough step in this direction is taken. A
modiﬁed Newton method based on the descent property and switching to standard Newton
when the condition (6.3.20) is satisﬁed will be described in Sec. 6.5.5.

646
Chapter 6. Solving Scalar Nonlinear Equations
6.3.3
An Interval Newton Method
Sometimes it is desired to compute a tiny interval that is guaranteed to enclose a real simple
root x∗of f (x), even when rounding errors are taken into account. This can be done using
an adaptation of Newton’s method to interval arithmetic method due to Moore [271].
Suppose that the function f (x) is continuously differentiable. Using the notation
in Sec. 2.5.3, let f ′([x0]) denote an interval containing f ′(x) for all x in a ﬁnite interval
[x0] := [a, b]. Deﬁne the Newton operator on N[x] by
N([x]) := m −f (m)
f ′([x]),
(6.3.22)
where m = mid ([x]) = 1
2(a + b).
Theorem 6.3.7.
If α ∈[x] is a zero of f (x), then α ∈N([x]). If N([x]) ⊆[x], then f (x) has one
and only one zero in N([x]).
Proof. Suppose α is a zero of f (x) in [x]. If 0 ∈f ′([x]), then N([x]) = [−∞, ∞].
Otherwise, by the mean value theorem
0 = f (α) = f (m) + f ′(ξ)(α −m),
ξ ∈int [α, m] ⊆[x].
This implies that α = m −f (m)/f ′(ξ) ⊆N([x]), which proves the ﬁrst statement.
If N([x]) ⊆[x], then f ′([x]) ̸= 0 on [x]. Then by the mean value theory there are ξ1
and ξ2 in [x] such that
(m −f (m)/f ′(ξ1)) −a = −f (a)/f ′(ξ1),
b −(m −f (m)/f ′(ξ2)) = f (b)/f ′(ξ2).
Because N([x]) ⊆[a, b], the product of the left sides is positive. But since f ′(ξ1) and
f ′(ξ2) have the same sign this means that f (a)f (b) < 0, and f has therefore a zero in [x].
Finally, there cannot be two or more zeros in [x], because then we would have f ′(c) =
0 for some c ∈[x].
In the interval Newton method, a starting interval [x0] is chosen, and we compute for
k = 0, 1, 2, . . . a sequence of intervals [xk+1] given by
N([xk]) = mid ([xk]) −f (mid [xk])
f ′([xk])
.
If N([xk]) ⊂[xk] we set [xk+1] = N([xk]) ∩[xk]. Otherwise, if N([xk]) ∩[xk] is empty,
we know that [xk] does not contain a root and stop. In neither condition holds we stop,
subdivide the initial interval, and start again. It can be shown that if [x0] does not contain a
root, then after a ﬁnite number of steps the iteration will stop with an empty interval.
If we are close enough to a zero, then the length of the intervals [xk] will converge
quadratically to zero, just as with the standard Newton method.

6.3. Methods Using Derivatives
647
Example 6.3.4.
Take f (x) = x2 −2 and [x0] = [1, 2]. Using the interval Newton method
N([xk]) = mid ([xk]) −(mid [xk])2 −2
2 [xk]
,
[xk+1] = N([xk]) ∩[xk],
we obtain the sequence of intervals
[x1] = N([x0]) = 1.5 −2.25 −2
2[1, 2]
=
022
16, 23
16
1
= [1.375, 1.4375] ,
[x2] = N([x1]) = 45
32 −
(45/32)2 −2
2[22/16, 23/16] = 45
32 −(45)2 −2(32)2
128 [22, 23]
⊂[1.41406, 1.41442].
The quadratic convergence of the radius of the intervals is evident:
0.5, 0.03125, 0.00036, . . . .
The interval Newton method is well suited to determine all zeros in a given interval.
Divide the given interval into subintervals and for each subinterval [x] check whether the
condition N([x]) ⊆[x] in Theorem 6.3.7 holds. If this is the case, we continue the interval
Newton iterations, and if we are close enough the iterations converge toward a root. If the
condition is not satisﬁed but N([x]) ∩[x] is empty, then there is no zero in the subinterval
and this can be discarded. If the condition fails but N([x])∩[x] is not empty, then subdivide
the interval and try again. The calculations can be organized so that we have a queue of
intervals waiting to be processed. Intervals may be added or removed from the queue. When
the queue is empty we are done.
The above procedure may not always work. Its performance will depend on, among
other things, the sharpness of the inclusion of the derivative f ′([x]). The algorithm will
fail, for example, in the case of multiple roots where N([x]) = [−∞, ∞].
6.3.4
Higher-Order Methods
Newton’s method has quadratic convergence, which means that the number of signiﬁcant
digits approximately doubles in each iteration. Although there is rarely any practical need
for methods of higher order of convergence such methods may be useful in special applica-
tions, for example, when higher-order derivatives are easily computed. For the following
discussion we assume that α is a simple zero of f and that f has a sufﬁcient number of
continuous derivatives in a neighborhood of α.
We ﬁrst brieﬂy derive some famous methods with cubic convergence for simple roots
of f (x) = 0. Newton’s method was derived by approximating the function f (x) with
its linear Taylor approximation.
Higher-order iteration methods can be constructed by
including more terms from the Taylor expansion. The quadratic Taylor approximation of
the equation f (xn + h) = 0 is
f (xn) + hf ′(xn) + h2
2 f ′′(xn) = 0,
h = x −xn.
(6.3.23)

648
Chapter 6. Solving Scalar Nonlinear Equations
Assuming that f ′(xn)2 ≥2f (xn)f ′′(xn), the solutions of this quadratic equation are real
and equal to
hn = −f ′(xn)
f ′′(xn)

1 ±
<
1 −2f (xn)f ′′(xn)
(f ′(xn))2

.
Rearranging and taking the solution of smallest absolute value we get
xn+1 = xn −u(xn) ·
2
1 + √1 −2t(xn),
(6.3.24)
where we have introduced the notations
u(x) = f (x)
f ′(x),
t(x) = f (xn)f ′′(xn)
(f ′(xn))2
= u(x)f ′′(x)
f ′(x) .
(6.3.25)
The iteration (6.3.24) is Euler’s iteration method.
Assuming that |t(xn)| ≪1 and using the approximation
2
1 + √1 −2t(xn) ≈1 + 1
2t(xn),
(6.3.26)
valid for |t| ≪1, we obtain another third order iteration method usually also attributed to
Euler:
xn+1 = xn −u(xn)

1 + 1
2t(xn)

.
(6.3.27)
A different method of cubic convergence is obtained by using a rational approximation of
(6.3.26):
xn+1 = xn −·
u(xn)
1 −1
2t(xn).
(6.3.28)
ThisisHalley’s182 iterationmethod[179], whichaccordingtoTraub[354]hasthedistinction
of being the most frequently rediscovered iteration method. Halley’s method has a simple
geometric interpretation. Consider a hyperbola
h(x) = b + a/(x −c),
where a, b, c are determined so that h(x) is tangent to the curve f (x) at the point x = xn,
and has the same curvature there. Then xn+1 is the intersection of this hyperbola with the
x-axis.
The methods (6.3.27) and (6.3.28) correspond to the (1,0) and (0,1) Padé approxi-
mations of Euler’s method. We now show that both are of third order for simple zeros.
Following Gander [129] we consider an iteration function of the general form
φ(x) = x −u(x)H(t(x)),
(6.3.29)
where u(x) and t(x) are deﬁned by (6.3.25). Differentiating (6.3.29) and using u′(x) =
1 −t(x) we get
φ′(x) = 1 −(1 −t(x))H(t) −u(x)H ′(t)t′(x).
182Edmond Halley (1656–1742), English astronomer, who predicted the periodic reappearance (ca 75 years) of
a comet, which is now named after him.

6.3. Methods Using Derivatives
649
Since u(α) = t(α) = 0 it follows that φ′(α) = 1 −H(0). Hence if H(0) = 1, then
φ′(α) = 0 and the iteration function (6.3.29) is at least of second order. Differentiating
once more and putting x = α we get
φ′′(α) = t′(α)H(0) −2u′(α)H ′(0) t′(α) = t′(α)(H(0) −2H ′(0)).
Hence φ′(α) = φ′′(α) = 0 and the method (6.3.29) yields convergence of at least third
order if
H(0) = 1,
H ′(0) = 1/2.
(6.3.30)
For Euler’s and Halley’s method we have
H(t) = 2

1 +
√
1 −2t
−1
,
H(t) =

1 −1
2t
−1
,
respectively, and both these methods satisfy (6.3.30). (Verify this!)
It is not very difﬁcult to construct iteration methods of arbitrarily high order for solving
f (x) = 0. It can be shown [354, Theorem 5.3] that any iteration function of order p must
depend explicitly on the ﬁrst p −1 derivatives of f . Consider the Taylor expansion at xn:
0 = f (xn + h) = f (xn) + hf ′(xn) +
p−1

k=2
hk
k! f (k)(xn) + O(hp).
(6.3.31)
Neglecting the O(hp)-term this is a polynomial equation of degree p −1 in h. Assuming
that f ′(xn) ̸= 0 we could solve this by the ﬁxed-point iteration hi = F(hi−1), where
F(h) = −f (xn) + p−1
k=2 hkf (k)(xn)/k!
f ′(xn)
,
taking h0 to be the Newton correction.
To get an explicit method of order p we set u = f (xn)/f ′(xn), and write
u = −h −
p−1

k=2
akhk,
ak = f (k)(xn)
k!f ′(xn),
k = 2 : p −1.
(6.3.32)
This can be interpreted as a formal power series in h (cf. Sec. 3.1.5). Reversing this series
we can express h as a formal power series in u:
h = −u −
p−1

k=2
ckuk + · · · ,
(6.3.33)
where the ﬁrst few coefﬁcients are
c2 = a2,
c3 = 2a2
2 −a3,
c4 = 5a3
2 −5a2a3 + a4,
(6.3.34)
c5 = 14a4
2 −21a2
2a3 + 6a2a4 + 3a2
3 −a5, . . . .

650
Chapter 6. Solving Scalar Nonlinear Equations
More coefﬁcients can easily be determined; see Problem 3.1.12. This leads to the family of
Schröder methods [316]. If f is analytic it can be shown that the method
xn+1 = xn −u(xn) −
p−1

k=2
ck(xn)uk(xn),
p ≥2,
(6.3.35)
yields convergence order p for simple roots (see Henrici [196, p. 529]). Setting p = 2 gives
Newton’s method and for p = 3 we get the third-order method of Euler (6.3.27).
The Schröder methods make use of polynomials in u. As for the case p = 3, there
are methods which instead use rational expressions in u. These can be derived from Padé
approximants of the Schröder polynomials; see Traub [354, Sec. 5.2]. There are indications
that methods which use rational approximations with about equal degree of numerator and
denominator are best. For example, for p = 4, the rational approximation
1 + c2u + c3u2 = c2 + (c2
2 −c3)u
c2 −c3u
+ O(u3)
can be used to derive an alternative method of order 4.
Example 6.3.5.
A kth-order iteration method (k odd) for the square root of c is
xn+1 = xn
xk−1
n
+
k
2

xk−3
n
c + · · · + kc(n−1)/2
k
1

xk−1
n
+
k
3

xk−3
n
c + · · · + c(n−1)/2 ,
(6.3.36)
where the sums end in a natural way with zero binomial coefﬁcients. For k = 5 we obtain
the iteration
xn+1 = x2
n + 10c + 5(c/xn)2
5x2n + 10c + (c/xn)2 xn,
n = 0, 1, 2, . . . ,
(6.3.37)
with order of convergence equal to ﬁve. For k = 3 we obtain Halley’s method
xn+1 = xn + 3c/xn
3xn + c/xn
xn,
n = 0, 1, 2, . . . ,
(6.3.38)
which has cubic rate of convergence.
Using Halley’s method with c = 3/4 and the initial approximation x0 = (
√
2 + 2)/4
(obtained by linear interpolation at 1/2 and 1) we obtain the following result (correct digits
in boldface):
x0 = 0.85355339059327,
x1 = 0.86602474293290,
x2 = 0.86602540378444.
Already two iterations give a result correct to 14 digits. Compared to Newton’s method (see
Example 1.1.1) we have gained one iteration. Since each iteration is more costly there is
no improvement in efﬁciency.
We now introduce a rational family of iteration methods of arbitrary order, which
is very convenient to use. First note that alternative derivation of Halley’s method is as
follows. Starting from (6.3.23) we get
hn = −f (xn)
#
f ′(xn) + hn
2 f ′′(xn)

.

6.3. Methods Using Derivatives
651
Replacing hn in the denominator by the Newton correction −f (xn)/f ′(xn) does not change
the order of the method and leads to (6.3.28). We note that this can be written
xn+1 = xn + B2(xn),
where
B2(x) = −f (x)
f ′(x)
det

f ′(x)
f ′′(x)
2!
f (x)
f ′(x)
.
This method belongs to a rational family of iteration functions of arbitrary order which we
now present. Set Dp(x) = det(Fp), where
Fp(x) =


f ′(x)
f ′′(x)
2!
. . .
f (p−1)(x)
(p−1)!
f (p)(x)
(p)!
f (x)
f ′(x)
...
...
f (p−1)(x)
(p−1)!
0
f (x)
...
...
...
...
...
...
...
f ′′(x)
2!
0
0
. . .
f (x)
f ′(x)


∈Rp×p
(6.3.39)
is aToeplitz matrix of upper Hessenberg form whose elements are the normalized derivatives
of f (x). (Recall that a square matrix is called Toeplitz if its elements are identical along
each diagonal.) The iteration
xn+1 = xn + Bp(xn),
Bp(x) = −f (x)det(Fp−2(x))
det(Fp−1(x))
(6.3.40)
can be shown to be of order p for simple roots. Notice that for f (x) = x2 −c the matrix
Fp becomes tridiagonal, which gives a simple iteration method of arbitrarily high order for
the square root.
The determinant formula (6.3.40) is attractive since it leads to a simple implementa-
tion. Use Gaussian elimination without pivoting to compute the LU factorization Fp(xn) =
LpUp, where
diag (Up) = (u11, u22, . . . , upp).
Since Lp is unit lower triangular and Up upper triangular (see Sec. 1.3.2) we have det(Fp) =
u11u22 . . . upp. Hence the ratio
det(Fp(x))/ det(Fp−1(x))
simply equals the last diagonal element upp in Up. It follows that
x(k)
n+1 = xn −f (xn)/ukk(xn),
k = 1 : p,
(6.3.41)
gives the result of one iteration step using a sequence of iteration formulas of order k = 2 :
p + 1.
Note that the Gaussian elimination is simpliﬁed by the fact that Fp(xn) is a Hessenberg
matrix. Only the p −1 subdiagonal elements need to be eliminated, which requires 1
2p2

652
Chapter 6. Solving Scalar Nonlinear Equations
ﬂops. Further, it is possible to implement the computation of diag (Up) using only two row
vectors; see Problem 6.3.16.
Methods using high-order derivatives are useful, especially when seeking zeros of
a function satisfying a differential equation (usually of second order). Then higher-order
derivatives can be calculated by differentiating the differential equation.
Example 6.3.6.
The Bessel function of the ﬁrst kind Jν(x) satisﬁes the differential equation
x2y′′ + xy′ + (x2 −ν2)y = 0.
The smallest zero ξ of J0(x) is close to x0 = 2.40, and
J0(x0) = 0.00250 76832 9724,
J ′
0(x0) = −J1(x0) = −0.52018 52681 8193.
Differentiating the differential equation for J0(x) we get
xy(k+1) + ky(k) + xy(k−1) + (k −1)y = 0,
k ≥1,
which gives a recursion for computing higher derivatives. Taking p = 5 we compute
y′′(x0)/2! = 0.10711 80892 2261,
y′′′(x0)/3! = 0.05676 83752 3951,
yiv(x0)/4! = −0.00860 46362 1903,
and form the Toeplitz matrix F4. Computing the diagonal elements of U in its LU factor-
ization, we obtain using (6.3.41) the following sequence of approximations to ξ:
2.40482 07503,
2.40482 55406,
2.40482 55576 7553,
2.40482 55576 9573.
Correct digits are shown in boldface. Hence the ﬁfth-order method gives close to full IEEE
double precision accuracy!
Review Questions
6.3.1 (a) Under what assumptions is convergence of Newton’s method quadratic?
(b) Construct an example where Newton’s method diverges, even though the equation
has real roots.
6.3.2 Describe an iteration for the division-free computation of the reciprocal of a positive
number c. Determine the largest set of starting values x0 such that the iterates converge
to 1/c.
6.3.3 The equation f (x) = sin x = 0 has one trivial root x = 0 in the interval (−π/2, π/2).
Show that for an initial approximation x0 chosen so that tan x0 = 2x0 Newton’s
method cycles, and x2k = x0 for all k ≥0!
6.3.4 (a) Assume that f is continuously differentiable in a neighborhood of a double root
α of the equation f (x) = 0. Describe how the equation can be converted to one with
a simple root α.
(b) Discuss the case when f (x) = 0 has two distinct roots which nearly coincide.

Problems and Computer Exercises
653
Problems and Computer Exercises
6.3.1 (a) Compute ϵn+1/ϵ2
n for n = 0, 1, 2, and the limit, as n →∞, in Example 6.3.1.
(b) Treat the equation in Example 6.3.1 using f ′(x2) as a ﬁxed approximation to
f ′(xn) for n > 2. Compare the convergence of this simpliﬁed method with the true
Newton method.
6.3.2 The equation x3 −2x −5 = 0 is of historical interest because it was the one used by
Wallis183 to present Newton’s method to the French Academy. Determine the roots
of this equation.
Hint: It has one real and two complex roots.
6.3.3 Use Newton’s method to determine the positive root of the equation to six correct
decimals:
(a) x = 1 −e−2x;
(b) x ln x −1 = 0.
6.3.4 Determine the unique positive real root of the equation xq −x −1 = 0 for q = 2 : 8.
6.3.5 (a) Consider the Newton iteration used in Example 6.3.5 for computing square root.
Show that the iterations satisfy
xn+1 −√c =
1
2xn
(xn −√c)2.
Use this relation to show that, for all x0 > 0, convergence is monotone x1 ≥x2 ≥
x3 ≥· · · ≥√c and limn→∞= √c (compare Figure 1.1.2).
(b) In Example 6.3.5 Newton’s method was used to compute √c for 1/2 ≤c ≤1.
Determine the maximum error of the linear initial approximation used there. Then
use the expression for the error in (a) to determine the number of iterations that
sufﬁces to give √c with an error less than 10−14 for all c in [1/2, 1] using this initial
approximation. Show that the inﬂuence of rounding errors is negligible.
6.3.6 (a) Investigate how many Newton iterations are required to compute the inverse
function u(x) of x = u ln u, to 12 digits of accuracy in the interval 0 < x ≤20.
Use the starting approximations given in Example 6.3.3.
(b) Lambert’s W-function equals w(x) = ln u(x), where u(x) is the inverse function
in (a). Compute and plot the function w(x) for 0 < x ≤20.
6.3.7 Determine p, q, and r so that the order of the iterative method
xn+1 = pxn + qc/x2
n + rc2/x5
n
for computing
3√c becomes as high as possible. For this choice of p, q, and r, give
a relation between the error in xn+1 and the error in xn.
6.3.8 (Ben-Israel) The function f (x) = xe−x has a unique zero α = 0. Show that for any
x0 > 1 the Newton iterates move away from the zero.
6.3.9 The Cartesian coordinates of a planet in elliptic orbit at time t are equal to ea(sin(x),
cos(x)), where a is the semimajor axis, and e the eccentricity of the ellipse. Using
183John Wallis (1616–1703), the most inﬂuential English mathematician before Newton.

654
Chapter 6. Solving Scalar Nonlinear Equations
Kepler’s laws of planetary motion it can be shown that the angle x, called the
eccentric anomaly, satisﬁes Kepler’s equation
x −e sin x = M,
0 < |e| < 1,
where M = 2πt/T is the mean anomaly and T the orbital period.
(a) Newton used his method to solve Kepler’s equation. Show that for each e, M
there is one unique real solution x = α such that M −|e| ≤α < M + |e|.
(b) Show that the simple ﬁxed-point iteration method
xn+1 = e sin xn + M,
x0 = 0,
is convergent.
(c) Study the convergence of Newton’s method:
xn+1 = xn + e sin xn −xn + M
1 −e cos xn
.
6.3.10 Determine the multiple root α = 1 of the equation p(x) = (1 −x)5 = 0, when the
function is evaluated using Horner’s scheme:
p(x) = ((((x −5)x + 10)x −10)x + 5)x −1 = 0.
(a) Use bisection (cf. Algorithm 6.1) with initial interval (0.9, 1.1) and tolerance
τ = 10−8. What ﬁnal accuracy is achieved?
(b) Use Newton’s method, starting from x0 = 1.1 and evaluating p′(x) using
Horner’s scheme. Terminate the iterations when for the ﬁrst time |xn+1 −1| >
|xn −1|. How many iterations are performed before termination? Repeat with a
couple of other starting values.
(c) Repeat (b), but perform one step of the modiﬁed Newton’s method (6.3.13) with
x0 = 1.1 and q = 5. How do you explain that the achieved accuracy is much better
than predicted by (6.1.10)?
6.3.11 Show that if Newton’s method applied to the equation u(x) = 0, where u(x) =
f (x)/f ′(x), then
xn+1 = xn −
u(xn)
1 −t(xn),
t(xn) = f (xn)f ′′(xn)
(f ′(xn))2
.
(6.3.42)
This transformation is most useful if an analytical simpliﬁcation can be done such
that u(x) can be evaluated accurately also in a neighborhood of α.
6.3.12 (a) Show that Halley’s method can also be derived by applying Newton’s method to
the equation f (x)(f ′(x))−1/2 = 0.
(b) What are the efﬁciency indexes of Newton’s and Halley’s methods, respectively,
if it is assumed that evaluating each of f , f ′, and f ′′ takes one unit of work?
(c) Show that Halley’s method applied to f (x) = x2 −c = 0, c > 0, gives rise to
the iteration
xn+1 = xn
x2
n + 3c
3x2n + c = xn −2xn(x2
n −c)
3x2n + c
.
Apply Halley’s method to f (x) = xk −c = 0, c > 0.

Problems and Computer Exercises
655
6.3.13 (Ben-Israel) Consider the quasi-Halley method
xn+1 = xn −
f (xk)
f ′(xk) −f ′(xk) −f ′(xk−1)
2(xk −xk−1)f ′(xk)f (xk)
,
where the second derivative f ′′(xk) has been approximated by a divided difference.
Show that if f ′′ is Lipschitz continuous near a root α, then
|α −xk+1| = O(|α −xk|γ ),
where γ satisﬁes the quadratic equation γ 2 −2γ −1 = 0. Conclude that the order
of this method is approximately 2.41 as compared to 3 for Halley’s method.
6.3.14 (Bailey et al. [11]) In 1976 Brent and Salamin independently discovered the follow-
ing iteration, which generates a sequence {pk} converging quadratically to π.
Set a0 = 1, b0 = 1/
√
2, and s0 = 1/2. For k = 1, 2, 3, . . . compute
ak = (ak−1 + bk−1)/2,
bk =

ak−1bk−1,
ck = a2
k −b2
k,
sk = sk−1 −2kck,
pk = 2a2
k/sk.
Perform this iteration in IEEE 754 double precision. Verify the quadratic conver-
gence by listing the errors in |pk −π| in successive iterations. How many iterations
can you do before the error starts to grow? What is the best accuracy achieved?
6.3.15 In Example 6.3.4 the ﬁrst two steps in the interval Newton method for solving the
equation x2 −2 = 0 are shown. Implement this method and carry out the iterations
until convergence.
6.3.16 (a) Compute det(Fp(x)) in (6.3.39) for p = 3 and write down the corresponding
rational fourth-order iteration method in terms of u, a2, a3 in (6.3.34).
(b) Implement in MATLAB the iteration method (6.3.40) for arbitrary order p.
Input should be an approximation xn to the root f (xn) and the row vector of scaled
derivatives,

f ′(x), f ′′(x)
2!
, . . . , f (p−1)(x)
(p −1)! , f (p)(x)
p!

,
evaluated at x = xn. Output should be the diagonal elements of Up in the LU factor-
ization of Fp(xn) and the sequence of approximations xn+1,k = xn +f (xn)/ukk(xn),
k = 1 : p. Try to economize on memory requirement.
6.3.17 Write a program that computes the inverse of the error function erf(x) by solving
the equation erf(x) −y = 0, 0 ≤y < 1. Use Newton’s method and the series
expansion given in Example 1.3.4 to compute values of erf(x) and its derivative.
Note that erf(x) ≈1 −1/(√πx) for large values of x.
6.3.18 (a) Given σ1 ≥σ2 ≥· · · ≥σn > 0, c1, c2, . . . , cn, and a parameter γ > 0, consider
the equation
f (λ) =
 n

i=1
σ 2
i c2
i
(σ 2
i + λ)2
1/2
= γ.
(6.3.43)

656
Chapter 6. Solving Scalar Nonlinear Equations
Each term in the sum is a convex and strictly decreasing function of λ for λ > 0.
Show that this also holds for f (λ). Further, show that if f (0) > γ (6.3.43) has a
unique root λ∗> 0.
(b) Newton’s method for solving f (λ) −γ = 0 is
λk+1 = λk + hk,
hk = −f (λk) −γ
f ′(λk)
.
Show that with λ0 = 0, this gives a strictly increasing sequence {λk}∞
k=0 converging
to the solution. Derive an explicit expression for hk.
(c) The Newton method suggested in (b) will converge slowly when λk ≪λ∗.
A more efﬁcient method is obtained by instead applying Newton’s method to the
equation h(λ) = 1/f (λ) = 1/γ . Show that this iteration can be written
λk+1 = λk −hk
f (λk)
γ
,
where hk is the Newton step in (b).
(d) Let
σi = 1/i2,
ci = 1/i2 + 0.001,
i = 1 : 20.
Plot the function f (λ) for λ ∈(0, 0.0005). Solve the equation φ(λ) = α = 2.5,
using λ0 = 0, comparing the two methods with p = 1 and p = −1.
6.4
Finding a Minimum of a Function
6.4.1
Introduction
In this section we consider the problem of ﬁnding the minimum (maximum) of a real-valued
function
min g(x),
x ∈I = [a, b],
(6.4.1)
which is closely related to that of solving a scalar equation.
Probably the most common use of unidimensional minimization is in “line search”
procedures in multidimensional minimization of a function φ(z) of n variables, z ∈Rn.
For example, if zk is the current approximation to the optimal point, the next approximation
is often found by minimizing a function
g(λ) = φ(xk + λdk),
where dk is a search direction and the step length λ is to be determined. In this application,
however, the minimization is rarely performed accurately.
Most algorithms for minimizing a nonlinear function of one (or more) variables ﬁnd
at best a local minimum. For a function with several local minima, there is no guarantee
that the global minimum (i.e., the lowest local minimum) in [a, b] will be found. One
obvious remedy is to try several different starting points and hope that the lowest of the

6.4. Finding a Minimum of a Function
657
local minima found is also the global minimum. However, this approach is neither efﬁcient
or safe.
Suppose we want to ﬁnd a local minimum of g(x) in a given interval [a, b]. If g is
differentiable in [a, b], a necessary condition for an interior point of τ ∈I to be a local
minimum is that g′(τ) = 0. If g′ does not change sign on I it is also possible that the
minimum is at a or b. If this is checked for separately, then it is possible to reduce the
problem to ﬁnding the zeros of g′(x) in I. However, since g′ also vanishes at a point of
maximum and inﬂection, it is necessary to check, for example, the second derivative, to see
if the point found really is a minimum.
6.4.2
Unimodal Functions and Golden Section Search
A condition which ensures that a function g has a unique global minimum τ in [a, b] is that
g(x) is strictly decreasing for a ≤x < τ and strictly increasing for τ < x ≤b. Such a
function is called unimodal.
Deﬁnition 6.4.1.
The function g(x) is unimodal on [a, b] if there exists a unique τ ∈[a, b] such that,
given any c, d ∈[a, b] for which c < d
d < τ ⇒g(c) > g(d),
c > τ ⇒g(c) < g(d).
(6.4.2)
This condition does not assume that g is differentiable or even continuous on [a, b].
For example, |x| is unimodal on [−1, 1].
We now describe an interval reduction method for ﬁnding the local minimum of
a unimodal function, which only uses function values of g. It is based on the following
lemma.
Lemma 6.4.2.
Suppose that g is unimodal on [a, b], and τ is the point in Deﬁnition 6.4.1. Let c and
d be points such that a ≤c < d ≤b. If g(c) ≤g(d), then τ ≤d, and if g(c) ≥g(d), then
τ ≥c.
Proof. If d < τ, then g(c) > g(d). Thus, if g(c) ≤g(d), then τ ≤d. The other part
follows similarly.
Assume that g is unimodal in [a, b]. Then using Lemma 6.4.2 it is possible to ﬁnd a
reduced interval on which g is unimodal by evaluating g(x) at two interior points c and d
such that c < d. Setting
[a′, b′] =
%
[c, b]
if g(c) > g(d),
[a, d]
if g(c) < g(d),
we can enclose x∗in an interval of length at most equal to max(b−c, d−a). (If g(c) = g(d),
then τ ∈[c, d], but we ignore this possibility.) To minimize this length one should take c

658
Chapter 6. Solving Scalar Nonlinear Equations
and d so that b −c = d −a. Hence c + d = a + b, and we can write
c = a + t(b −a),
d = b −t(b −a),
0 < t < 1/2.
Then d −a = b −c = (1 −t)(b −a), and by choosing t ≈1/2 we can almost reduce the
length of the interval by a factor 1/2. But d −c = (1 −2t)(b −a) must not be too small
for the available precision in evaluating g(x).
If we only consider one step the above choice would be optimal. Note that this step
requires two function evaluations. A clever way to save function evaluations is to arrange
it so that if [c, b] is the new interval, then d can be used as one of the points in the next
step; similarly, if [a, d] is the new interval, then c can be used at the next step. Suppose
this can be achieved with a ﬁxed value of t. Since c + d = a + b the points lie symmetric
with respect to the midpoint 1
2(a + b) and we need only consider the ﬁrst case. Then t must
satisfy the following relation (cf. above and Figure 6.4.1):
d −c = (1 −2t)(b −a) = (1 −t)t(b −a).
Hence t should equal the root in the interval (0, 1/2) of the quadratic equation 1−3t+t2 = 0,
which is t = (3 −
√
5)/2. With this choice the length of the interval will be reduced by the
factor
1 −t = 2/(
√
5 + 1) = 0.618034 . . .
at each step, which is the golden section ratio. For example, 20 steps gives a reduction of
the interval with a factor (0.618034 . . .)20 ≈0.661 · 10−5.
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
ak
ak+1=ck
ck+1=dk 
bk+1=bk
Figure 6.4.1. One step of interval reduction, g(ck) ≥g(dk).
Algorithm 6.2. Golden Section Search.
The function goldsec computes an approximation m ∈I to a local minimum of a given
function in an interval I = [a, b], with an error less than a speciﬁed tolerance τ.

6.4. Finding a Minimum of a Function
659
function xmin = goldsec(fname,a,b,delta);
%
t = 2/(3 + sqrt(5));
c = a + t*(b - a);
d = b - t*(b - a);
fc = feval(fname,c);
fd = feval(fname,d);
while (d - c) > delta*max(abs(c),abs(d))
if fc >= fd
% Keep right endpoint b
a = c;
c = d;
fc = fd;
d = b - t*(b - a);
fd = feval(fname, d);
else
% Keep left endpoint a
b = d;
d = c;
fd = fc;
c = a + t*(b - a);
fc = feval(fname, c);
end;
end;
xmin = (c + d)/2;
Rounding errors will interfere when determining the minimum of a scalar function
g(x). Because of rounding errors the computed approximation f l(g(x)) of a unimodal
function g(x) is not in general unimodal. Therefore, we assume that in Deﬁnition 6.4.1 the
points c and d satisfy |c −d| > tol, where
tol = ϵ|x| + τ
is chosen as a combination of absolute and relative tolerance. Then the condition (6.4.2)
will hold also for the computed function. For any method using only computed values of g
there is a fundamental limitation in the accuracy of the computed location of the minimum
point τ in [a, b]. The best we can hope for is to ﬁnd xk ∈[a, b] such that
g(xk) ≤g(x∗) + δ,
where δ is an upper bound of rounding and other errors in the computed function values
¯g(x) = f l (g(x)). If g is twice differentiable in a neighborhood of a minimum point τ,
then by Taylor’s theorem
g(τ + h) ≈g(τ) + 1
2h2g′′(τ).
This means that there is no difference in the ﬂoating-point representation of g(τ +h) unless
h is of the order of √u. Hence we can not expect τ to be determined with an error less than
ϵα =

2 δ/|g′′(x∗)|,
(6.4.3)
unless we can also use values of g′ or the function has some special form.

660
Chapter 6. Solving Scalar Nonlinear Equations
6.4.3
Minimization by Interpolation
For ﬁnding the minimum of a unimodal function g, golden section search has the advantage
that linear convergence is guaranteed. In that respect it corresponds to the bisection method
for ﬁnding a zero of a function. If the function is sufﬁciently smooth and we have a good
initial approximation, then a process with superlinear convergence will be much faster. Such
methods can be devised using interpolation by a polynomial or rational function, chosen
so that its minimum is easy to determine. Since these methods do not always converge
they should be combined with golden section search. There is a close analogy with robust
methods for solving a nonlinear equation, where a combination of inverse interpolation and
bisection can be used; see Sec. 6.2.4.
Since linear functions generally have no minimum the simplest choice is to use a
second degree polynomial (a parabola). Suppose that at step n we have three distinct points
in u, v, and w. The quadratic polynomial interpolating g(x) at these points is (cf. (6.2.12))
p(x) = g(v) + (x −v)[u, v]g + (x −v)(x −u)[u, v, w]g.
Setting the derivative of p(x) equal to zero gives
0 = [u, v]g + (v −u)[u, v, w]g + 2(x −v)[u, v, w]g,
and solving for x gives
x = v + d,
d = −[u, v]g + (v −u)[u, v, w]g
2[u, v, w]g
.
(6.4.4)
This is a minimum point of p(x) if [u, v, w]g > 0. We assume that of all the points where
g has been evaluated v is the one with least function value. Therefore, d should be small,
so the effect of rounding errors in computing d is minimized. Initially we can take u = a,
w = b; if g(c) < g(d), then v = c, otherwise v = d, where c and d are the two golden
section points.
Multiplying the numerator and denominator of d by (v −u)(w −v)(w −u), a short
calculation shows that d = −s1/s2, where
r1 = (w −v)(g(v) −g(u)),
r2 = (v −u)(g(w) −g(v)),
s1 = (w −v)r1 + (v −u)r2,
s2 = 2(r2 −r1).
(6.4.5)
Consider parabolic interpolation at the points xi−2, xi−1, xi, i = 2, 3, . . . , and let
ϵi = xi −τ. Assuming that g(x) is sufﬁciently smooth in a neighborhood of τ it can be
shown that asymptotically the relation
ϵi+1 ∼c3
2c2
ϵi−1ϵi−2,
cr = 1
k!g(k)(ζr),
(6.4.6)
holds between successive errors. Hence the convergence order equals the real root p =
1.3247 . . . of the equation x3 −x −1 = 0.
If two or more of the points u, v, w coincide, or if the parabola degenerates into a
straight line, then s2 = 0. The parabolic interpolation step is only taken if the following
inequalities are true:
|d| < 1
2|e|,
s2 ̸= 0,
v + d ∈[a, b],

Problems and Computer Exercises
661
where e is the value of the second last cycle. Otherwise a golden section step is taken, i.e.,
x =
% (1 −t)v + ta
if v ≥1
2(a + b),
(1 −t)v + tb
if v < 1
2(a + b),
where 1 −t = 2/(
√
5 + 1).
The combination of inverse quadratic interpolation and golden section search has
been suggested by Brent [44, Chapter 5], where the many delicate points to consider in an
implementation are discussed. At a typical step there are six signiﬁcant points a, b, u, v, w,
and x, not all distinct. The positions of these points are updated at each step. Initially [a, b]
is an interval known to contain a local minimum point. At a later point in the algorithm
they have the following signiﬁcance: A local minimum lies in [a, b]; of all the points at
which g has been evaluated v is the one with the least value of g; w is the point with the
next lowest value of g; u is the previous value of w; and x is the last point at which g has
been evaluated.
Review Questions
6.4.1 How many steps are needed in golden section search to reduce an initial interval [a, b]
by a factor of 10−6?
6.4.2 Suppose the twice differentiable function f (x) has a local minimum at a point x∗.
What approximate limiting accuracy can you expect in a method for computing x∗
which uses only function values?
6.4.3 The algorithm FMIN is a standard method for ﬁnding the minimum of a function. It
uses a combination of two methods. Which?
Problems and Computer Exercises
6.4.1 Use the algorithm goldsec to ﬁnd the minimum of the quadratic function f (x) =
(x −1/2)2 starting from a = 0.25, b = 1. Plot the successive inclusion intervals.
6.4.2 Modifythealgorithm goldsectouseparabolicinterpolationinsteadofgoldensection
if this gives a point within the interval.
6.4.3 (a) Plot the function
g(x) =
1
(x −0.3)2 + 0.01 +
1
(x −0.9)2 + 0.04,
and show that it has a local minimum in each of the intervals [0.2, 0.4] and [0.8, 1.0].
(b) Use your algorithm from Problem 6.4.2 to determine the location of the two
minima of g(x) in (a).
(c) MATLAB includes a function fminbnd that also uses a combination of golden
section search and parabolic interpolation to ﬁnd a local minimum. Compare the
result using this function with the result from (b).

662
Chapter 6. Solving Scalar Nonlinear Equations
6.4.4 (Brent [44, Sec. 5.6]) The function
g(x) =
20

i=1
2i −5
x −i2
2
has poles at x = 12, 22, . . . , 202.
Restricted to the open interval (i2, (i + 1)2),
i = 1 : 19, it is unimodal. Determine the minimum points in these intervals and the
corresponding values of g(x).
6.5
Algebraic Equations
6.5.1
Some Elementary Results
The problem of solving an algebraic equation
p(z) = a0zn + a1zn−1 + · · · + an = 0,
(a0 ̸= 0),
(6.5.1)
has played a major role in the development of mathematical concepts for many centuries.
Even the “high school formula” for solving a quadratic equation requires the introduction
of irrational and complex numbers. There is a long history of investigations into algebraic
expressions for the zeros of equations of higher degree. In the sixteenth century Cardano
published formulas for the roots of a cubic equation (see Problem 2.3.8). Formulas for
the roots when n = 4 are also known. In 1826 Abel proved that it is not possible to ﬁnd
algebraic expressions for the roots for the class of algebraic equations of degree n > 4. But
even the existing formulas for n ≤4 are not, in general, suitable for numerical evaluation
of the roots. In Sec. 2.3.4, it was shown that care must be taken to avoid cancellation even
in the case of a quadratic equation.
Despitetheabsenceofclosedsolutionformulas, thefundamentaltheoremofalgebra
states that every algebraic equation has at least one root. More precisely, if a polynomial
vanishes nowhere in the complex plane, then it is identically constant. The remainder
theorem states that when a polynomial p(z) is divided by z −r the remainder is p(r), i.e.,
p(z) = (z −r)p1(z) + p(r).
(6.5.2)
It follows that r1 is a zero of p(z) if and only if z−r1 divides p(z). If p(z) is of degree n and
p(r1) = 0, then p1(z) in (6.5.2) is of degree n −1. But if n > 1, then p1(z) must vanish for
some r2, and hence has a linear factor z−r2. Continuing, we ﬁnd that an algebraic equation
p(z) = 0 of degree n has exactly n roots, counting multiplicities, and it holds that184
p(z) = a0(z −r1)(z −r2) · · · (z −rn).
(6.5.3)
By this representation it also follows that if the coefﬁcients a0, a1, . . . , an are real, then
eventual complex roots must occur in conjugate pairs. Hence, if p(z) has the complex zero
s + it it also has the zero s −it and therefore contains the quadratic factor (z −s)2 + t2
which is positive for all real values of z.
184Note the corollary that if a polynomial p(z) of degree at most n vanishes in n + 1 distinct points, it must be
identically zero, a result used repeatedly in Chapters 3 and 4.

6.5. Algebraic Equations
663
Solving algebraic equations of high degree does not play a central role in scientiﬁc
computing. Usually the applications involve only equations of moderate degree, say 10–20,
for which acceptable subroutines exist. Algebraic equations of high degree (n > 100) occur
in computer algebra. Such problems usually require symbolic, or high multiple-precision
computations. Algorithms for such problems are still a subject of research.
Let p(z) be the polynomial (6.5.3) with roots r1, r2, . . . , rn, not necessarily equal.
Comparingwiththecoefﬁcientsofzn−k intherepresentations(6.5.1)weﬁndthat(−1)kak/a0
is the sum of the products of the roots ri taken k at a time. Thus we obtain the following
relations between the coefﬁcients and zeros of a polynomial:

i
ri = −a1
a0
,

i<j
rirj = a2
a0
,

i<j<k
rirjrk = −a3
a0
, . . . ,
r1r2 · · · rn = (−1)n an
a0
.
(6.5.4)
The functions on the left sides of (6.5.4) are called elementary symmetric functions of
the variables r1, r2, . . . , rn, since interchanging any of the variables will not change the
functions. A classical result says that any symmetric polynomial in the roots r1, r2, . . . , rn
can be expressed as a polynomial in the elementary symmetric functions in (6.5.4).
Other commonly used symmetric functions are the power sums
Sk =
n

i=1
rk
i ,
k = ±1, ±2, . . . .
(6.5.5)
The Newton formulas give the connection between the coefﬁcients of the polynomial p(z)
(with a0 = 1), and the power sums:
S1 + a1 = 0,
S2 + a1S1 + 2a1 = 0,
S3 + a1S2 + a2S1 + 3a3 = 0,
(6.5.6)
...
Sn−1 + a1Sn−2 + · · · + an−2S1 + (n −1)an−1 = 0.
For a proof see Householder [204, Sec. 1.3].
If an ̸= 0, setting z = 1/y in p(z) gives the reciprocal polynomial
q(y) = ynp(1/y) = anyn + · · · + a1y + a0.
(6.5.7)
The zeros of the reciprocal polynomial are 1/r1, 1/r2, . . . , 1/rn and from (6.5.4), giving
the Newton formulas,

i
1/ri = −an−1/an, etc.
Function values of the polynomial p(z) in (6.5.1) at a (real or complex) point w can
conveniently be computed by repeated synthetic division of p(z) with z −w; cf. Sec. 1.2.2.
If we set
p(z) = (z −w)q(z) + bn,
(6.5.8)
q(z) = b0zn−1 + b1zn−2 + · · · + bn−1,

664
Chapter 6. Solving Scalar Nonlinear Equations
then the sequence {bi}n
i=0 satisﬁes the recursion
b0 = a0,
bi = bi−1w + ai,
i = 1 : n.
(6.5.9)
Here p(w) = bn is the remainder and q(z) is the quotient polynomial when dividing p(z)
with (z −w). Differentiating (6.5.8) we get
p′(z) = (z −w)q′(z) + q(z),
(6.5.10)
and setting z = w we ﬁnd that p′(w) = q(w). We can form q(w) by synthetic division of
q(z) with (z −w):
q(z) = (z −w)r(z) + cn−1,
r(z) = c0zn−2 + c1zn−3 + · · · + cn−2.
Now p′(w) = q(w) = cn−1, where
c0 = b0,
ci = ci−1w + bi,
i = 1 : n −1.
Higher derivatives can be computed in the same fashion. Differentiating once more
gives
p′′(z) = (z −w)q′′(z) + 2q′(z),
and thus 1
2p′′(w) = q′(w) = dn−2, where
d0 = c0,
di = di−1w + ci,
i = 1 : n −2.
To compute p(i)(w) using these formulas requires n −i additions and multiplications.
Intheimportantspecialcasewhereallthecoefﬁcientsa0, a1, . . . , an arereal, theabove
formulas are somewhat inefﬁcient, and one can save operations by performing synthetic
division with the quadratic factor
(z −w)(z −¯w) = z2 −2zRe(w) + |w|2,
which has real coefﬁcients (see Problem 6.5.2 (b)).
Synthetic division can also be used to shift the origin of a polynomial p(z). Given
a0, a1, . . . , an and s, we then want to ﬁnd coefﬁcients c0, c1, . . . , cn so that
p(w + s) = q(s) = c0sn + c1sn−1 + · · · + cn.
(6.5.11)
Clearly this is the Taylor expansion of p(z) at z = w. It follows that
cn = p(w),
cn−1 = p′(w),
cn−1 = 1
2p′′(w),
. . . ,
c0 = 1
n!p(n)(w),
and the coefﬁcients ci can be computed by repeated synthetic division of p(z) by (z −w)
as described above in about n2/2 multiplications.
It is often desirable to obtain some preliminary information as to where the zeros of
a polynomial p(z) are located. Some information about the location of real roots can be
obtained from a simple examination of the sign of the coefﬁcients of the polynomial.

6.5. Algebraic Equations
665
A simple observation is that if ai > 0, i = 1 : n, then p(x) can have no real positive
zero. A generalization of this result, known as Descartes’ rule of sign,185 states that the
number of positive roots is either given by the number of variations in sign in the sequence
a0, a1, . . . , an, or is less than that by an even number. (Multiple roots are counted with their
multiplicity.) By considering the sign variations for the polynomial p(−z) we similarly
get an upper bound on the number of negative roots. By shifting the origin, we can get
bounds on the number of roots larger and smaller than a given number. In Sec. 6.5.6 we
give a method to obtain precise information about the number of real roots in any given
interval.
Many classical results are known about the number of real or complex roots in a disk
or half-plane. For these we refer to surveys in the literature.
6.5.2
Ill-Conditioned Algebraic Equations
Let a = (a1, a2, . . . , an)T ∈Cn be the coefﬁcient vector of a monic polynomial p(z)
(a0 = 1) and denote the set of zeros of p by Z(p) = {z1, z2, . . . , zn}. In general, the best
we can expect from any numerical zero-ﬁnding algorithm is that it computes the zeros of a
nearby polynomial ˆp(z) with slightly perturbed coefﬁcients ˆa. Following Trefethen [358]
we deﬁne the ϵ-pseudo zero set of p(z) by
Zϵ = {z ∈C | z ∈Z( ˆp) for some ˆp with ∥ˆa −a∥≤ϵ}.
(6.5.12)
These sets describe the conditioning of the zero-ﬁnding problem. Depending on the chosen
norm they correspond to a coefﬁcientwise or normwise perturbation of the coefﬁcient vector
a. The shape of these sets is studied in [353].
The sensitivity of polynomial zeros to perturbations in the coefﬁcients was illustrated
in a famous example by Wilkinson in the early 1960s.186 The paper [379] contains an
extensive discussion of numerical problems in determining roots of polynomial equations.
Wilkinson considered the polynomial
p(z) = (z −1)(z −2) · · · (z −20) = z20 −210z19 + · · · + 20!,
with zeros 1, 2, . . . , 20. Let ¯p(z) be the polynomial which is obtained when the coefﬁcient
a1 = −210 in p(z) is replaced by
−(210 + 2−23) = −210.000000119 . . . ,
while the rest of the coefﬁcients remain unchanged. Even though the relative perturbation
in a1 is of order 10−10, many of the zeros of the perturbed polynomial ¯p(z) deviate greatly
185René Descartes (1596–1650), French philosopher and mathematician.
186Wilkinson received the Chauvenet Prize of the Mathematical Association of America 1987 for this exposition
of the ill-conditioning of polynomial zeros.

666
Chapter 6. Solving Scalar Nonlinear Equations
from those of p(z). In fact, correct to nine decimal places, the perturbed zeroes are the
following.
1.000000000
10.095266145 ± 0.643500904i
2.000000000
3.000000000
11.793633881 ± 1.652329728i
4.000000000
4.999999928
13.992358137 ± 2.518830070i
6.000006944
6.999697234
16.730737466 ± 2.812624894i
8.007267603
8.917250249
19.502439400 ± 1.940330347i
20.846908101
For example, the two zeros 16, 17 have not only changed substantially but have become a
complex pair. It should be emphasized that this behavior is quite typical of polynomials with
real coefﬁcients and real roots. Indeed, many polynomials which arise in practice behave
much worse than this.
If we assume that the coefﬁcients ai of a polynomial are given with full machine
accuracy, then the error δ in computed values of p(x) (for real x) is bounded by
δ < 1.06u
n

i=0
|(2i + 1)an−ixi| < γ2n+1
n

i=0
|an−i||x|i;
see Sec. 2.3.2. Hence by (6.1.7) the limiting accuracy of a zero α is equal to
ϵα =
δ
|p′(α)| =
n
i=0 |(2i + 1)an−iαi|
|p′(α)|
.
In particular, for the root α = 14 in the above example we get ϵα = 1.89 · 1016. But the
changes in this example are so large that this linearized perturbation theory does not apply!
For a more detailed discussion of the conditioning of algebraic equations in general and the
Wilkinson polynomial, we refer to Gautschi [144, Sec. 4].
It should be emphasized that although a problem may be given in the form (6.5.1), it
is often the case that the coefﬁcients of p(z) are not the original data. Then it may be better
to avoid computing them. An important case is when the polynomial is the characteristic
polynomial of a matrix A ∈Rn×n:
pA(z) = det(zI −A) =
(((((((((
z −a11
−a12
· · ·
−a1n
−a21
z −a22
· · ·
−a2n
...
...
...
...
−an1
−an2
· · ·
z −ann
(((((((((
.
(6.5.13)
The n roots of pA(z) = 0 are the eigenvalues of A, and the problem is an eigenvalue problem
in disguise. Then the original data are the elements of the matrix A and numerical values of
pA(z) can then be evaluated much more accurately directly from (6.5.13). It is important
to realize that, even when the roots (eigenvalues) are well determined by the elements of A

6.5. Algebraic Equations
667
and well separated, they can be extraordinarily sensitive to small relative perturbations in
the coefﬁcients of pA(z).
In classical (i.e., before 1960) methods of linear algebra, the eigenvalues of a matrix
are often found by ﬁrst computing the coefﬁcients of the characteristic polynomial. This is
not in general a good idea and one of the highly developed modern eigenvalue algorithms
should be used (consult Volume II, and references therein). Note also that if the coefﬁcients
of the characteristic polynomial det(zI −A) are required, these are often best computed by
ﬁrst computing the eigenvalues λi of A and then forming
p(z) =
n
&
i=1
(z −λi).
(6.5.14)
Example 6.5.1.
Another example where computing the coefﬁcients of the polynomial should be
avoided is the following. Suppose the largest positive root of the algebraic equation
p(x) = (x + 2)(x2 −1)6 −3 · 10−6 · x11 = 0
is to be computed. Here p(z) is a polynomial of degree 13. If the coefﬁcients are computed
using decimal ﬂoating-point arithmetic with seven digits, then the coefﬁcient of x11 which
is (12 −3 · 10−6) will be rounded to 12.00000. Thus the machine will treat the equation
(x + 2)(x2 −1)6 = 0, whose exact positive root is one.
This is a poor result. However, by writing the equation in the form
x = φ(x),
φ(x) = 1 +
0.1
x + 1
 3x11
x + 2
1/6
,
and solving this by the iteration x0 = 1, xk+1 = φ(xk), one can get the root α =
1.053416973823 to full accuracy.
Turning the tables, a polynomial zero-ﬁnding problem can be transformed into an
eigenvalue problem. The companion matrix (or Frobenius matrix) to the polynomial p(z)
in (6.5.1), normalized so that a0 = 1, is deﬁned as
C =


−a1
−a2
· · ·
−an−1
−an
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
...
0
0
· · ·
1
0


.
(6.5.15)
(Sometimes the companion matrix is deﬁned slightly differently, for example, with the
coefﬁcients of the polynomial in the last column.) Using the deﬁnition (1.6.4) it can be
veriﬁed that the characteristic polynomial of C equals
pC(z) = det(zI −C) = zn + a1zn−1 + · · · + an−1z + an.
Thus the roots of p(z) = 0 can be computed by applying an eigenvalue algorithm to C.
This trick is used in several of the zero-ﬁnding algorithms in current use; see Sec. 6.5.5.

668
Chapter 6. Solving Scalar Nonlinear Equations
For example, in MATLAB the function roots(p) computes the roots of a polynomial
p(z) using the QR algorithm to solve the eigenvalue problem for the companion matrix.
Although the operation count for this QR algorithm is O(n3) and the storage requirement
1.5n2, experiments suggest that for small and moderate values of n it is as fast as competing
algorithms and can be more accurate. Further problems with overﬂow or underﬂow are
avoided.
If the coefﬁcients are known (and stored) exactly, then by using multiple precision
arithmetic the accuracy in the zeros can be increased. It is generally true that the solution
of polynomial equations of high degree requires the use of multiple precision ﬂoating-point
arithmetic in order to achieve high accuracy.
6.5.3
Three Classical Methods
In this section we describe three classical methods which are mainly of theoretical interest,
but may be useful in special situations.
The idea in Graeffe’s method is to replace equation (6.5.1) by an equation whose
roots are the square of the roots of (6.5.1). By iterating this procedure roots which are of
unequal magnitude become more and more separated and therefore, as we shall see, more
easily determined.
If the monic polynomial p1(z) = zn + a1zn−1 + · · · + an has zeros zi, i = 1 : n, then
p1(z) = (z −z1)(z −z2) · · · (z −zn).
Setting y = z2, it follows that
p2(y) = (−1)np1(z)p1(−z) = (y −z2
1)(y −z2
2) · · · (y −z2
n);
i.e., p2(y) is a polynomial of degree n whose zeros equal the square of the zeros of p1(z).
The coefﬁcients of
p2(y) = yn + b1yn−1 + · · · + bn
are obtained by convolution of the coefﬁcients of p1(z) and p1(−z); see Theorem 3.1.6.
This gives
b0 = a2
0,
(−1)kbk = a2
k +
k

j=1
(−1)j2ak−jak+j,
k = 1 : n.
(6.5.16)
Assume now that after repeated squaring we have obtained an equation
pm+1(w) = wn + c1wn−1 + · · · + cn,
with zeros |αj| = |zj|2m, such that |α1| ≫|α2| ≫· · · ≫|αn|. Then we can use the relations
(6.5.4) between the coefﬁcients and zeros of a polynomial to deduce that
c1 = −

i
αi ≈−α1,
c2 =

i<j
αiαj ≈α1α2,
c3 =

i<j<k
αiαjαk ≈α1α2α3, . . . ,

6.5. Algebraic Equations
669
and therefore
|z1| ≈(−c1)1/2m,
|z2| ≈(−c2/c1)r1/2m,
|z3| ≈(−c3/c2)1/2m, . . . .
Example 6.5.2.
Squaring the polynomial p1(z) = z3 −8z2 + 17z −10 three times gives
p4(w) = w3 −390,882w2 + 100,390,881w −108.
We have 2m = 8 and approximations to the roots are
|z1| ≈
8
390,882 = 5.00041,
|z2| ≈
8
100,390,881/390,882 = 2.00081,
|z3| ≈
8
108/100,390,881 = 0.999512.
The exact roots are 5, 2, and 1.
In Bernoulli’s method for obtaining the zeros of
p(z) ≡zn + a1zn−1 + · · · + an = 0,
one considers the related difference equation
yn+k + a1yn+k−1 + · · · + anyk = 0,
(6.5.17)
which has p(z) as its characteristic equation. If p(z) only has simple roots z1, . . . , zn, then
by Theorem 3.3.12 the general solution of (6.5.17) is given by
yk =
n

j=1
cjzk
j,
where c1, c2, . . . , cn are constants which depend on the initial conditions.
We assume in the following that the roots are ordered in magnitude so that
|z1| > |z2| ≥· · · ≥|zn|,
where the root of largest magnitude z1 is distinct. We say that z1 is a dominant root.
Assuming that the initial conditions are chosen so that c1 ̸= 0, we have
yk = c1zk
1
0
1 +
n

j=2
cj
c1
zj
z1
k 1
= c1zk
1
0
1 + O
|z2|
|z1|
k 1
.
If z1 is real it follows that
lim
n→∞
yk
yk−1
= z1
0
1 + O
|z2|
|z1|
 1
.
(6.5.18)

670
Chapter 6. Solving Scalar Nonlinear Equations
Note that the rate of convergence is linear and depends on the ratio |z2/z1|. In practice
measures to avoid overﬂow or underﬂow must be included. By initially applying a few
steps of Graeffe’s root-squaring method, convergence can be improved.
The relation (6.5.18) holds also if z1 is a real multiple root. However, if the root of
largest magnitude is complex the method needs to be modiﬁed. It is difﬁcult to safeguard
against all possible cases.
Bernoulli’s method is closely related to the powermethod for computing approximate
eigenvalues and eigenvectors of a matrix. In this application a powerful modiﬁcation is to
shift all the eigenvalues so that the desired eigenvalue is close to zero. If we then apply the
power method to the inverse matrix rapid convergence is assured.
In Laguerre’s method187 the polynomial p(z) of degree n is approximated in the
neighborhood of the point zk by a special polynomial of the form
r(z) = a(z −w1)(z −w2)n−1,
where the parameters a, w1, and w2 are determined so that
p(zk) = r(zk),
p′(zk) = r′(zk),
p′′(zk) = r′′(zk).
(6.5.19)
If zk is an approximation to a simple zero α, then the simple zero w1 of r(z) is taken as
the new approximation zk+1 of α. Laguerre’s method has very good global convergence
properties for polynomial equations, and with cubic convergence for simple roots (real or
complex). For multiple roots convergence is only linear.
In order to derive Laguerre’s method we note that the logarithmic derivative of p(z) =
(z −α1) · · · (z −αn) is
S1(z) = p′(z)
p(z) =
n

i=1
1
z −αi
.
Taking the derivative of this expression we obtain
−dS1(z)
dz
= S2(z) =
p′(z)
p(z)
2
−p′′(z)
p(z) =
n

i=1
1
(z −αi)2 .
Using (6.5.19) to determine the parameters of the approximating polynomial r(z) we obtain
the equations
S1(zk) =
1
zk −w1
+ (n −1)
zk −w2
,
S2(zk) =
1
(zk −w1)2 +
(n −1)
(zk −w2)2 .
Eliminating zk −w2 gives a quadratic equation for the correction zk −w1 = zk −zk+1.
After some algebra we obtain (check this!)
zk+1 = zk −
np(zk)
p′(zk) ± √H(zk),
(6.5.20)
where
H(zk) = (n −1)2[p′(zk)]2 −n(n −1)p(zk)p′′(zk).
187Edmond Nicolas Laguerre (1834–1886), French mathematician at École Polytechnique, Paris and best known
for his work on orthogonal polynomials.

6.5. Algebraic Equations
671
The sign in the denominator in (6.5.20) should be chosen so that the magnitude of the
correction |zk+1 −zk| becomes as small as possible.
For polynomial equations with only real roots, Laguerre’s method is globally con-
vergent, i.e., it converges for every choice of real initial estimate z0. Suppose the roots are
ordered such that α1 ≤α2 ≤· · · ≤αn. If z0 ∈(αj−1, αj), j = 2 : n, then Laguerre’s
method converges to one of the roots αj−1, αj; if z0 < α1 or z0 > αn, then convergence is
to α1 or αn, respectively.
For polynomial equations with complex roots, Laguerre’s method no longer converges
for every choice of initial estimate. But experience has shown that the global convergence
properties are good also in this case. In particular, if we take z0 = 0, then Laguerre’s method
will usually converge to the root of smallest modulus. We ﬁnally remark that, as might be
expected, for multiple roots convergence of Laguerre’s method is only linear.
Consider the polynomial equation p(z) = 0 and assume that an ̸= 0 so that α = 0
is not a root. Now suppose that an−2an−1 ̸= 0, and take z0 = 0 in Laguerre’s method. A
simple calculation gives
z1 =
−nan
an−1 ± √H(z0),
H(z0) = (n −1)2a2
n−1 −2n(n −1)anan−2,
(6.5.21)
where the sign is to be chosen so the |z1| is minimized. In particular, for n = 2, H(z0) is
the discriminant of p(z) and z1 is the root of smallest modulus.
Example 6.5.3.
If there are complex roots, then there may be several distinct roots of smallest modulus.
For example, the equation
p(z) = z3 −2z2 + z −2
has roots ±i and 2. Using the above formula (6.5.21) for z1 with n = 3, we get
z1 =
6
1 ± 2i
√
11
= 2
15 ± i 4
√
11
15
= 0.06666666667 ± 0.88443327743i.
Continuing the iterations with Newton’s method we get convergence to one of the two roots
±i,
z2 = −0.00849761051 + 1.01435422762i, z3 = −0.00011503062 + 1.00018804502i,
z4 = −0.00000002143 + 1.00000003279i, z5 = −0.00000000000 + 1.00000000000i.
6.5.4
Deﬂation and Simultaneous Determination of Roots
Suppose we have found a root α to the equation p(z) = 0. Then taking zk = α in (6.5.9)–
(6.5.8) we have bn = p(α) = 0 and the remaining roots of p(z) are also roots of the
polynomial equation
q(z) = p(z)
z −α = 0.
Hence we can continue the iterations with the quotient polynomial q(z) of degree n−1. This
process is called deﬂation and can be repeated; as soon as a root has been found it is factored

672
Chapter 6. Solving Scalar Nonlinear Equations
out. Proceeding like this, all roots are eventually found. Since we work with polynomials
of lower and lower degree, deﬂation saves arithmetic operations. More important is that it
prevents the iterations from converging to the same simple root more than once.
So far we have ignored that roots which are factored out are only known with ﬁnite
accuracy. Also, rounding errors occur in the computation of the coefﬁcients of the quotient
polynomial q(x). Clearly there is a risk that both these types of errors can have the effect
that the zeros of the successive quotient polynomials deviate more and more from those of
p(z). Indeed, deﬂation is not unconditionally a stable numerical process. A closer analysis
performed by Wilkinson [379] shows that if the coefﬁcients of the quotient polynomials
are computed by the recursion (6.5.9), then errors resulting from deﬂation are negligible
provided that
1. the roots are determined in order of increasing magnitude;
2. each root is determined to its limiting accuracy.
Note that if the above procedure is applied to the reciprocal polynomial znp(1/z) we
obtain the zeros of p(z) in order of decreasing magnitude.
With Laguerre’s method it is quite probable that we get convergence to the root of
smallest magnitude from the initial value z0 = 0.
But this cannot be guaranteed and
therefore one often proceeds in two steps. First, all n roots are determined using deﬂation
in the process. Next, each root found in the ﬁrst step is reﬁned by doing one or several
Newton iterations using the original polynomial p(z).
Deﬂation can be avoided by using a zero suppression technique suggested by Maehly
[255]. He notes that the derivative of the reduced polynomial q(z) = p(z)/(z −ξ1) can be
expressed as
q′(z) = p′(z)
z −ξ1
−
p(z)
(z −ξ1)2 .
More generally, assume that we have determined approximations ξ1, . . . , ξj to j roots of
p(z) = 0. Then the ﬁrst derivative of the reduced polynomial qj(z) = p(z)/[(z −ξ1) · · ·
(z −ξj)] can be expressed as
q′
j(z) =
p′(z)
(z −ξ1) · · · (z −ξj) −
p(z)
(z −ξ1) · · · (z −ξj)
j

i=1
1
z −ξi
.
Hence Newton’s method applied to qj(z) can be written
zk+1 = zk −
p(zk)
p′(zk) −j
i=1 p(zk)/(zk −ξi)
,
(6.5.22)
which is the Newton–Maehly method.
This iteration has the advantage that it is not
sensitive to the accuracy in the approximations of the previous roots ξ1, . . . , ξj. Indeed, the
iteration (6.5.22) is locally quadratically convergent to simple zeros of p(z) for arbitrary
values of ξ1, . . . , ξj.
For the removal of a linear factor by deﬂation it is necessary that the zero be com-
puted to full working accuracy, since otherwise the remaining approximative zeros can be

6.5. Algebraic Equations
673
meaningless. This is a disadvantage if only low accuracy is required. An alternative to de-
ﬂation is to use an iterative method that, under appropriate separation assumptions, allows
for the simultaneous determination of all the roots of a polynomial equation. Suppose that
the numbers ξ (k)
i
, i = 1 : n, are a set of n distinct approximations of p(z). A new set of
approximations are then computed from
ξ (k+1)
i
= ξ (k)
i
−p(ξ (k)
i
)
#

a0
n
&
j=1
j̸=i
(ξ (k)
i
−ξ (k)
j )

,
i = 1 : n.
(6.5.23)
This is Weierstrass’ method, introduced in 1891 in connection with a new constructive
proof of the fundamental theorem of algebra. The method was rediscovered and analyzed
in the 1960s by Durand and is also known as the Durand–Kerner method.
With q(z) = (z −ξ (k)
1 )(z −ξ (k)
2 ) · · · (z −ξ (k)
n ) the formula may also be written
ξ (k+1)
i
= ξ (k)
i
−p(ξ (k)
i
)/q′(ξ (k)
i
),
which shows that to ﬁrst approximation the method is identical to Newton’s method. This
relation can be used to prove that for simple (real or complex) zeros the asymptotic order of
convergence of the Weierstrass method equals 2. (For multiple zeros the method will only
converge linearly.) The relation
n

i=1
ξ (k)
i
=
n

i=1
αi = −a1,
k ≥1,
which holds independent of the initial approximations, can be used as a control; see Kjell-
berg [227].
It is possible to accelerate Weierstrass’ method by using the new approximations of
the roots in (6.5.23) as they become available. This leads to the iteration
ξ (k+1)
i
= ξ (k)
i
−p(ξ (k)
i
)
#

a0
&
j<i
(ξ (k)
i
−ξ (k+1)
j
)
&
j>i
(ξ (k)
i
−ξ (k)
j )

,
i = 1 : n.
This serial version of the Weierstrass method can be shown to have an order of convergence
at least 1 + σn, where 1 < σn < 2 is the unique positive root to σ n −σ −1 = 0.
If no a priori information about the roots is available, then the initial approximations
ξ (0)
i
can be chosen equidistantly on a circle |z| = ρ, centered at the origin, which encloses
all the zeros of p(z). Such a circle can be found by using the result that all the roots of the
polynomial p(z) lie in the disk |z| ≤ρ, where
ρ = max
1≤k≤n 2
|ak|
|a0|
1/k
.
Note that this is (6.5.25) applied to the reciprocal polynomial.
The zeros zi, i = 1 : n, of a polynomial of degree n,
p(z) = b0 + b1z + · · · + bnzn,
(b0bn ̸= 0),
(6.5.24)

674
Chapter 6. Solving Scalar Nonlinear Equations
are the poles of the rational function r(z) = 1/p(z). Hence the progressive form of the qd
algorithm, described in Sec. 3.5.5, can be used to simultaneously determine the zeros. This
method can be considered as a modern, more powerful version of Bernoulli’s method.
In the qd scheme for r(z) the values in the ﬁrst two rows can be expressed in terms
of the coefﬁcients of p(z) as
q(0)
1
= −b1/b0,
q(1−k)
k
= 0,
k > 1,
e(1−k)
k
= bk−1/bk,
k = 1 : n −1.
Further, since r has a zero of order n at inﬁnity,
e(m)
n
= 0,
m ≥−n.
Thus the qd scheme is ﬂanked on both sides by a column of zeros. This allows the extended
qd scheme for r(z) to be constructed row by row. It can be shown that a sufﬁcient condition
for the qd scheme to exist is that the zeros are positive and simple. Then the kth q-column
tends to z−1
k . By reversing the order of the coefﬁcients, i.e., by considering the polynomial
zkp(z−1) instead, we can also construct a scheme where the q-columns tends to the zeros zm.
Example 6.5.4.
The Laguerre polynomial of degree four is
L4(z) = 1
24(24 −96z + 72z2 −16z3 + z4).
The corresponding qd scheme is shown in Table 6.5.1.
Table 6.5.1. The qd scheme for computing the zeros of Ly(z).
16.00000000
0
0
0
0
−4.50000000
−1.33333333
−0.25000000
0
11.50000000
3.16666667
1.08333333
0.25000000
0
−1.23913043
−0.45614035
−0.05769231
0
10.26086957
3.94965675
1.48178138
0.30769231
0
−0.47697126
−0.17112886
−0.01197982
0
9.78389831
4.25549915
1.64093042
0.31967213
0
−0.20745829
−0.06598769
−0.00233381
0
9.57644002
4.39696974
1.70458430
0.32200594
0
−0.09525333
−0.02558161
−0.00044087
0
9.48118669
4.46664146
1.72972504
0.32244681
...
...
...
...
9.39507749
4.53661379
1.74576103
0.32254769
0
−0.00000340
−0.00000004
0.00000000
0
9.39507409
4.53661715
1.74576108
0.32254769

6.5. Algebraic Equations
675
The zeros of L4(z) are correctly rounded to eight decimals:
9.39507091,
4.53662030,
1.74576110,
0.32254769.
Note that the convergence of e(n)
i
is linear with rate (zi+1/zi), i = 1 : 3, and can be
accelerated with Aitken extrapolation.
6.5.5
A Modiﬁed Newton Method
There are three competing methods in current use for determining all zeros of a given
polynomial. The Jenkins–Traub method [211], used in the IMSL library, is equivalent
to a so-called variable-shift Rayleigh quotient iteration for ﬁnding the eigenvalues and
eigenvectors of the companion matrix. By taking advantage of the matrix structure the
work per iteration can be reduced to O(n). A three stage procedure is used, each stage
being characterized by the type of shift used. The code CPOLY (see [212]) is available via
Netlib; see Sec. C.7 in Online Appendix C. For a description of the Jenkins–Traub method
we refer to Ralston and Rabinowitz [296, Sec. 8.11].
The MATLAB zero-ﬁnding code roots applies the QR algorithm, which is a standard
method for solving eigenvalue problems, to a balanced companion matrix. The balancing
involves a diagonal similarity transformation,
˜A = DAD−1,
D = diag (d1, d2, . . . , dn),
which preserves the eigenvalues. The aim is to reduce the norm of A and thereby reduce
the condition number of its eigenvalue problem. The balancing algorithm used is that of
Parlett and Reinsch [286].
Another excellent algorithm is the modiﬁed Newton algorithm PA16, due to Madsen
and Reid [253, 254], used by NAG Library. In its ﬁrst stage it uses the Newton formula to
ﬁnd a search direction for minimizing |p(z)|. Once the iterates are close to a zero it enters
stage 2 and switches to standard Newton. This will be described in more detail below.
Theoretical and experimental comparisons of the three algorithms above are given
in [353]. It is shown that the Jenkins–Traub, Madsen–Reid, and QR algorithms all have
roughly the same stability properties. The highest accuracy is typically achieved by PA16
and the next best by roots. A possible drawback with the QR algorithm is that it requires
O(n3) work and O(n2) memory. Both the Madsen–Reid and Jenkins–Traub require only
O(n2) work O(n) memory. Since one is rarely interested in solving polynomial equations
of high degree this is usually not important.
We now describe the modiﬁed Newton method due to Madsen [253]. By including
a one-dimensional search along the Newton direction this method achieves good global
convergence properties and is also effective for multiple roots.
To initialize let z0 = 0,
δz0 =
%
−p(0)/p′(0) = −an/an−1
if an−1 ̸= 0,
1
otherwise,
and take
z1 = 1
2ρ
δz0
|δz0|,
ρ = max
1≤k≤n
|an−k|
|an|
1/k
.
(6.5.25)

676
Chapter 6. Solving Scalar Nonlinear Equations
This assures that |z1| is less than the modulus of any zero of p(z) (see [204, Exercise 2.2.11]).
Further, if p′(0) ̸= 0, it is in the direction of steepest descent of |p(z)| from the origin (see
Sec. 6.3.2). This choice makes it likely that convergence will take place to a root of near
minimal modulus.
The general idea of the algorithm is that given zk, a tentative step hk is computed by
Newton’s method. The next iterate is found by taking the best point (in terms of minimizing
|f (z)|) found by a short search along the line through zk and zk + hk. When the search
yields no better value than at zk we take zk+1 = zk and make sure that the next search is
shorter and in a different direction. Since the line searches will be wasteful if we are near a
simple root, we then switch to the standard Newton’s method.
In the ﬁrst stage of the algorithm, when searches are being performed, new iterates
zk+1 are computed as follows.
1. If the last iteration was successful (zk ̸= zk−1), then the Newton correction
hk = −p(zk)/p′(zk)
(6.5.26)
is computed. The next tentative step is taken as
δzk =
% hk
if |hk| ≤3|zk −zk−1|,
3|zk −zk−1|eiθhk/|hk|
otherwise,
whereθ ischosenratherarbitrarilyasarctan(3/4). Thischangeofdirectionisincluded
because if a saddle point is being approached, the direction hk may be a bad choice.
2. If the last step was unsuccessful (zk = zk−1) the search direction is changed and the
step size reduced. In this case the tentative step is chosen to be
δzk = −1
2eiθδzk−1.
Repeated use of this is sure to yield a good search direction.
3. Once the tentative step δzk has been found the inequality |p(zk + δzk)| < |p(zk)| is
tested. If this is satisﬁed the numbers
|p(zk + p δzk)|,
p = 1, 2, . . . , n,
are calculated as long as these are strictly decreasing. Note that if we are close to
a multiple root of multiplicity m we will ﬁnd the estimate zk + mhk, which gives
quadratic convergence to this root. A similar situation will hold if we are at a fair
distance from a cluster of m zeros and other zeros are further away.
If |p(zk + δzk)| ≥|p(zk)|, we calculate the numbers
|p(zk + 2−pδzk)|,
p = 0, 1, 2,
again continuing until the sequence ceases to decrease.

6.5. Algebraic Equations
677
A switch to standard Newton is made if in the previous iteration a standard Newton
step zk+1 = zk + hk was taken, and Theorem 6.3.3 ensures the convergence of Newton’s
method with initial value zk+1, i.e., when f (zk)f ′(zk) ̸= 0 and
2 |f (zk)| max
z∈Kk |f ′′(z)| ≤|f ′(zk)|2,
Kk : |z −zk| ≤|hk|,
is satisﬁed; cf. (6.3.20). This inequality can be approximated using already computed
quantities by
2 |f (zk)||f ′(zk) −f ′(zk−1)| ≤|f ′(zk)|2|zk−1 −zk|.
(6.5.27)
The iterations are terminated and zk+1 accepted as a root whenever zk+1 ̸= zk and
|zk+1 −zk| < u|zk|
holds, where u is the unit roundoff. The iterations are also terminated if
|p(zk+1)| = |p(zk)| < 16nu|an|,
where the right-hand side is a generous overestimate of the ﬁnal roundoff made in computing
p(z) at the root of the smallest magnitude. The polynomial is then deﬂated as described in
the previous section.
More details about this algorithm and methods for computing error bounds can be
found in [253, 254].
6.5.6
Sturm Sequences
Precise information about the number of real zeros of a polynomial p(z) in an interval [a, b],
−∞≤a < b ≤∞, can be obtained from a Sturm sequence188 for p(z).
Deﬁnition 6.5.1.
A sequence of real polynomials p0(x), p1(x), . . . , pm(x) is a strict Sturm sequence
for p(x) = p0(x) on the interval [a, b] if the following conditions hold:
(i) No two consecutive polynomials in the sequence vanish simultaneously on the interval
[a, b].
(ii) If pj(r) = 0 for some j < m, then pj−1(r)pj+1(r) < 0.
(iii) Throughout the interval [a, b], pm(x) ̸= 0.
(iv) If p0(r) = 0, then p′
0(r)p1(r) > 0.
Givenapolynomialp1(x)ofdegreenotgreaterthanthatofp0(x)aSturmsequencecan
be constructed by the Euclidean algorithm as follows. Let q1(x) be the quotient polynomial
and −p2(x) the remainder in the quotient p0(x)/p1(x), i.e., p0(x) = q1(x)p1(x) −p2(x),
188J. C. F. Sturm (1803–1855), a Swiss mathematician best known for his theorem on Sturm sequences, discovered
in 1829, and his theory of Sturm–Liouville differential equations. In 1839 he succeeded Poisson in the chair of
mechanics at the École Polytechnique, Paris.

678
Chapter 6. Solving Scalar Nonlinear Equations
where the degree of p2(x) is strictly less than that of p1(x). Continuing in this way, we
compute p2(x), . . . , pm(x) by
pk+1(x) = qk(x)pk(x) −pk−1(x),
k = 1 : m −1,
(6.5.28)
where qk(x) is the quotient and −pk+1 the remainder in the quotient pk−1(x)/pk(x). We
stop when pm(x) nowhere vanishes on the interval [a, b]. Clearly, if pj(r) = 0, then
pj+1(r) = −pj−1(r) < 0, so condition (ii) is satisﬁed.
Let V (x) denote the number of variations in sign in the Sturm sequence at x. If p0(x)
and p1(x) have only simple zeros that separate each other, then it can be shown that the
number of zeros of p0(x) on [a, b] is equal to |V (a) −V (b)|.
Theorem 6.5.2.
Take p1(x) = p′
0(x) and deﬁne p2(x), . . . , pm(x) by (6.5.28), where pm(x) has a
ﬁxed sign on the interval [a, b] (p0(a) ̸= 0 and p0(b) ̸= 0). Let V(r) denote the number of
variations of sign in the sequence of values
p0(r), p1(r), . . . , pm(r),
vanishing terms not being counted. Then the number of roots of p0(x) in [a, b], each
multiple root being counted once, is exactly equal to |V (a) −V (b)|.
Note that if all real zeros of p0(x) are simple and p1(x) = p′
0(x), then (6.5.28)
generates a Sturm sequence. If p0(x) has multiple zeros, then p0(x) and p′
0(x) have a
common divisor, which divides every pi(x) in the sequence, and this will not affect V (r).
Example 6.5.5.
The equation p(x) = p0 = x5 −3x −1 = 0 has three real roots, z1 = −1.21465,
z2 = −0.33473, and z3 = 1.38879, and two complex roots. The derivative equals p′(x) =
p1 = 5x4 −3, and the rest of the Sturm chain is given by
p2 = 12
5 x + 1,
p3 = 59083
20736.
Here p2 is a polynomial of degree one and the Sturm chain ends with s = 3 < n.
We denote by [lk, uk] an interval containing the zero xk. Evaluating the sign changes
of the Sturm sequence at x = −2 and x = 2 shows that there are 3 −0 = 3 roots xk,
k = 1, 2, 3, in the interval [−2, 2]. Counting the number of sign changes at the midpoint
x = 0 allows us to deduce that uk = 0, k = 1, 2, and l3 = 0; see Table 6.5.2. The interval
[−2, 0] contains two roots so we determine next the number of sign changes at the midpoint
x = −1.
At this point we have determined three disjoint intervals [−2, −1], [−1, 0], and [0, 2],
which each contain one root. We continue bisecting each of these intervals, which can be
performed in parallel.
Methods based on Sturm sequences can be competitive, when only a relatively small
number of real roots in a given interval are of interest. Consider a real symmetric tridiagonal

6.5. Algebraic Equations
679
Table 6.5.2. Left: Sign variations in the Sturm sequence. Right: Intervals [lk, uk]
containing the zero xk.
x
p0
p1
p2
p3
δ
−2
−
+
−
+
3
+2
+
+
+
+
0
0
−
−
+
+
1
−1
+
+
−
+
2
1
−
+
+
+
1
l1
u1
l2
u2
l3
u3
−2
2
−2
2
−2
2
0
0
0
−1
−1
1
matrix,
A =


α1
β2
β2
α2
β3
...
...
...
βn−1
αn−1
βn
βn
αn


,
such that βk ̸= 0, k = 2 : n, has only simple eigenvalues. Let pk(λ) be the characteristic
polynomial of the kth leading principal minor of (A −λI). Deﬁne p0(λ) = 1, and pk(λ)
by the three-term recursion
p1(λ) = α1 −λ,
pk(λ) = (αk −λ)pk−1(λ) −β2
kpk−2(λ),
k = 2 : n.
(6.5.29)
Then the sequence
1, p0(λ), . . . , pn(λ) = det(A −λI)
is known to form a Sturm sequence. Combined with the bisection method, this recursion
can be used to develop an efﬁcient numerical method for determining the eigenvalues of
a symmetric tridiagonal matrix A in a given interval [a, b] without reference to any of the
others. It can also be used for determining the singular values of a bidiagonal matrix in a
given interval; see Volume II.
The Sturm sequence algorithm only works when f (x) is a real function of a real
variable. To determine complex zeros an algorithm that performs a search and exclusion of
the complex plane can be used. The quadtree exclusion algorithm, due to H. Weyl [372]
and illustrated in Figure 6.5.1, is such a “two-dimensional bisection algorithm.”189 It was
one of the ﬁrst algorithms with guaranteed convergence to all n zeros of a polynomial of
degree n. The algorithm is based on an exclusion test applied to squares in the complex
plane. Assume that f (z) is analytic in K and that
|f ′(z)| ≤M
∀z ∈K.
Then if |f (z0)| > ηM there can be no zero of f (z) in K. Any square that does not pass this
exclusion test may contain a root and is called suspect. (Note that it is not required that a
suspect square actually contains a root.)
The computations begin with an initial suspect square S containing all the zeros of
p(x). This square can be found from an upper bound on the absolute value of the zeros of
189In general a quadtree is a tree where each node is split along d dimensions giving 2d children.

680
Chapter 6. Solving Scalar Nonlinear Equations
∗
∗
∗
∗
∗
×
×
×
×
×
×
×
×
×
×
Figure 6.5.1. Suspect squares computed by Weyl’s quadtree method. Their centers
(marked by ×) approximate the ﬁve zeros marked by ∗.
p(x). In the algorithm, as soon as we have a suspect square, this is partitioned into four
congruent subsquares. At the center of each of them a test estimating the distance to the
closest zero of p(x) is performed. (A relative error within, say, 40% will sufﬁce.) If the
test guarantees that this distance exceeds half of the length of the diagonal of the square,
then the square cannot contain any zero and is discarded. Each remaining suspect square
undergoes the same recursive partitioning into four subsquares and the test. The zeros lying
in a suspect square are approximated by its center with errors bounded by half the length of
its diagonal. Each iteration step decreases the diagonal of the remaining squares by a factor
of two so the errors will decrease by a factor of 1/2.
6.5.7
Finding Greatest Common Divisors
A problem that arises in many applications, including computer graphics, geometric mod-
elling, and control theory, is the computation of the greatest common divisor (GCD) of
two polynomials:
p(z) = a0
m
&
j=1
(z −αi) = a0zn + a1zn−1 + · · · + an,
a0 ̸= 0,
q(z) = b0
m
&
j=1
(z −βj) = b0zm + b1zn−1 + · · · + bm,
b0 ̸= 0.
The GCD can in principle be determined by the Euclidean algorithm; see Problem 1.2.6.
Assume that n > m and perform the divisions
p(z) = q(z)s(z) + r1(z),
q(z) = r1(z)s1(z) + r2(z),

6.5. Algebraic Equations
681
q(z) = r1(z)s1(z) + r2(z),
(6.5.30)
...
rs−2(z) = rs−1(z)ss−1(z) + rs(z).
The degrees of the polynomials ri, i = 1 : s, decrease at each step and we stop when rs is a
constant. If p(z) and q(z) have a common factor, then that factor is a divisor of ri, i = 1 : s,
as well. This means that if rs ̸= 0, then p(z) and q(z) have no common divisor; otherwise
rs−1 is the greatest common divisor.
From the representation of rs(z) in (6.5.30) it follows that if p(z) and q(z) are two
polynomials, then there always exist polynomials x(z) and y(z) such that xp −yq equals
a constant. If p(z) and q(z) have a common divisor, then xp(z) −yq(z) = 0.
The Euclidean algorithm is suitable for symbolic computation, but not effective in
ﬁnite precision arithmetic because of roundoff. We now describe a suitable algorithm for
determining the GCD when the coefﬁcients of p(z) and q(z) are given as ﬂoating-point
numbers. Note that in this case we can only hope to determine “almost common factors.”
Consider the product of the mn factors,
¯R(p, q) =
n
&
i=1
m
&
j=1
(βj −αi),
(6.5.31)
of all possible differences between the two sets of roots. This is a symmetric function of the
α’s and β’s and hence expressible as a polynomial in the quotients ai/a0 and bj/b0. Clearly
¯R(p, q) is zero if and only if p(z) and q(z) have a common zero, and no polynomial of lower
degree in the α’s and β’s can have this property. The polynomial R(p, q) = am
0 bn
0 ¯R(p, q)
is called the resultant of p(z) and q(z) and can also be expressed as
R(p, q) = am
0 bn
0 ¯R(p, q) ≡am
0
n
&
i=1
q(αi) ≡bn
0
m
&
j=1
p(βj).
(6.5.32)
From the coefﬁcients of p(z) and p(z) we form the Sylvester matrix190
S(p, q) =

Tm(p)
Tn(q)

∈R(m+n)×(m+n),
(6.5.33)
where
Tm(p) =


a0
a1
· · ·
an−1
an
a0
a1
· · ·
an−1
an
...
...
...
...
a0
a1
· · ·
an−1
an

∈Rm×(m+n),
Tn(q) =


b0
b1
· · ·
bm−1
bm
b0
b1
· · ·
bm−1
bm
...
...
...
...
b0
b1
· · ·
bm−1
bm

∈Rn×(m+n).
190The Sylvester matrix appeared in [346].

682
Chapter 6. Solving Scalar Nonlinear Equations
Note the Toeplitz structure of the two blocks of rows. It can be shown that the resultant
R(p, q) equals the determinant of S(p, q) except for a possible multiplier −1. Thus p(z)
and q(z) have a common zero if and only if the Sylvester matrix is singular.
If we form m+n polynomials of degree less than or equal to m+n−1 by multiplying
p(z) with zk, k = 1 : m, and multiplying q(z) with zk, k = 1 : n, this can be written in
matrix notation as
S(p, q) ( zm+n−1
· · ·
z
1 )T =

fp
fq

,
where
fp = p(z) ( zm−1
· · ·
z
1 )T ,
fq = q(z) ( zn−1
· · ·
z
1 )T .
If γ is a common root to the two polynomials p(z) and q(z), then
S(p, q)(γ m+n−1, . . . , γ, 1)T = 0.
In other words this vector belongs to the nullspace of S(p, q). The degree of the GCD of
p(z) and q(z) equals the dimension of the nullspace of the Sylvester matrix, i.e.,
deg (gcd (p, q)) = m + n −r,
r = rank (S(p, q)).
The best way to estimate the rank is by computing the SVD of the Sylvester matrix; see
Theorem 1.4.4. The coefﬁcients of the GCD can also be obtained from the decomposition.
Review Questions
6.5.1 Describe the method of iterated successive synthetic division for computing function
values and derivatives of a polynomial.
6.5.2 Consider the polynomial p(z) = z4 −2z3 −4z2 + z + 1. Using Descartes’s rule of
sign, what can you deduce about the number of real positive roots?
6.5.3 Suppose that all roots of a polynomial equation are to be determined. Describe two
methods to avoid the problem of repeatedly converging to roots already found.
6.5.4 Discuss the ill-conditioning of roots of polynomial equations. What famous polyno-
mial did Wilkinson use as an example?
6.5.5 (a) What is the companion matrix of a polynomial p(x) = xn + a1xn−1 + · · · +
an−1x + an?
(b) One approach to computing the eigenvalues of a matrix A is to ﬁnd the coefﬁcients
of the characteristic polynomial pA(λ) = det(λI −A), and then solve the algebraic
equation pA(λ) = 0. Why should such a method usually be avoided?
6.5.6 WhatpropertiesaresatisﬁedbyaSturmsequenceofrealpolynomialsp0(x), p1(x), . . . ,
pm(x)? Describe one way of generating a Sturm sequence using the Euclidean algo-
rithm.

Problems and Computer Exercises
683
Problems and Computer Exercises
6.5.1 ApplyNewton’smethodtodetermineoneofthecomplexrootsoftheequationz2+1 =
0. Start with z0 = 1 + i.
6.5.2 Consider a polynomial with real coefﬁcients
p(z) = a0zn + a1zn−1 + · · · + an,
ai ̸= 0,
i = 0 : n.
(a) Count the number of (real) additions and multiplications needed to compute a
value p(z0) by synthetic division of p(z) by (z −z0), when z0 is a real and complex
number, respectively.
(b) For a complex number z0 = x0 + iy0, p(z0) can also be computed by performing
the synthetic division of p(z) with the real quadratic factor
d(z) = (z −z0)(z −¯z0) = z2 −2x0z + (x2
0 + y2
0).
Derive a recursion for computing the quotient polynomial q(z) and p(z0) = bn−1z0 +
bn, where
q(z) = b0zn−2 + b1zn−3 + · · · + bn−2,
p(z) = q(z)d(z) + bn−1z + bn.
Count the number of real additions and multiplications needed to compute p(z0) and
also show how to compute p′(z0).
6.5.3 (a) Using the Cardano–Tartaglia formula the real root α to the equation x3 = x + 4
can be written in the form
α =
3
$
2 + 1
9
√
321 +
3
$
2 −1
9
√
321.
Use this expression to compute α. Discuss the loss of accuracy due to cancellation.
(b) Compute α to the same accuracy by Newton’s method using the initial approxi-
mation x0 = 2.
6.5.4 Let C be the companion matrix of p(x). Show that in Bernoulli’s method the vec-
tor sequence mn = (yn+k, . . . , yn+1, yn)T can be generated by forming successive
matrix-vector products mn = Cmn−1. Conclude that mk = Ckm0.
6.5.5 (a)Assume that the zeros of a polynomial satisfy the stronger conditions |u1| > |u2| >
|u3| ≥· · · ≥|uk|, where u1 and u2 are real and simple. Show that in Bernoulli’s
method
lim
n→∞Dn/Dn−1 = u1u2,
Dn =
((((
yn
yn+1
4yn
4yn+1
(((( .
(6.5.34)
(b) The result in (a) can be combined with (6.5.18) to also compute u2. What is the
rate of convergence in this process?
(c) Apply Bernoulli’s method to the polynomial p4 in Example 6.5.2 to improve the
approximations computed by Graeffe’s method.

684
Chapter 6. Solving Scalar Nonlinear Equations
6.5.6 (a) Use the MATLAB command x = roots(poly(1:20)) to compute the roots of
the classical Wilkinson polynomial 220
k=1(x −k). Use IEEE double precision. What
is the maximum error in the roots?
(b) Using the command poly(x) to compute the coefﬁcients of the polynomial with
the erroneous roots from (a). What is the maximum error in these coefﬁcients?
6.5.7 (a) Generate the coefﬁcients of the Legendre polynomials Pn(x), n = 2 : 24, using
the three-term recursion P0(x) = 1, P1(x) = x,
Pn+1(x) = 2n + 1
n + 1 xPn(x) −
n
n + 1Pn−1(x),
n ≥1,
where the recursion coefﬁcients are exact rational numbers. Compute using IEEE
double precision the roots of P24(x) as in Problem 6.5.6. Verify that the errors in
some computed roots λ∗
k close to 1 exceed 106u, where u = 1.11 10−16 is the unit
roundoff.
(b) Compute the coefﬁcients of the polynomial 224
k=1(x −λ∗). Show that the relative
errors in the computed coefﬁcients are all less than 16u.
6.5.8 Consider the iteration zn+1 = z2
n + c, where c = p + iq is a ﬁxed complex number.
For a given z0 the sequence of iterates zn = xn + iyn, n = 0, 1, 2, . . . , may either
converge to one of the two roots of the quadratic equation z2 −z + c = 0 or diverge
to inﬁnity. Consider z0 chosen, for example, in the unit square of the complex plane.
The boundary separating the region of convergence from other points in the plane is a
very complex fractal curve known as the Julia set. The Mandelbrot set is obtained
by ﬁxing z0 = 0 and sweeping over values of c in a region of the complex plane.
(a) Picture the Julia set as follows. Set c = 0.27334 + 0.000742i. Sweep over points
of z0 in the region −1 ≤ℜz0 ≤1, −1.3 ≤ℑz0 ≤1.3. If |zN| < R, for N = 100
and R = 10, color the point z0 black; otherwise color the point from hot (red) to cool
(blue) according to how fast the iteration is diverging, i.e., according to how fast the
inequality |zn| > R becomes satisﬁed.
(b) Picture the Mandelbrot set in a similar way. Sweep over values of c in the region
−2.25 ≤ℜc ≤0.75, −1.5 ≤ℑc ≤1.5.
6.5.9 The following MATLAB function, sylvester, generates the Sylvester matrix of
two polynomials whose coefﬁcients are given in the row vectors a and b:
function S = sylvest(a,b);
% SYLVEST computes
n = length(a) - 1;
m = length(b) - 1;
r = [a,zeros(1,m-1)];
c = [a(1),zeros(1,m-1)];
T1 = toeplitz(c,r);
r = [b,zeros(1,n-1)];
c = [b(1),zeros(1,n-1)];
T2 = toeplitz(c,r);
S = [T1; T2];

Notes and References
685
Generate S(p, q) for the two polynomials
p(x) = x5 + x4 + x3 + x2 + x + 1,
q(x) = x4 −2x3 + 3x2 −x −71.
The GCD of these polynomials is x +1. How is this revealed by the SVD of S(p, q)?
(b) Try some more difﬁcult examples with several common zeros.
Notes and References
The general idea of solving an equation by repeatedly improving an estimate of the solution
has been used in many cultures for thousands of years. Examples are ancient Greek and
Babylonian methods as well as Arabic algebraists from the eleventh century.
An interesting historical account of Newton’s method is given in Ypma [388]. New-
ton’s method is contained in his book Method of Fluxions, written in 1671 but not published
until 1736. Joseph Raphson was allowed to see Newton’s work and Newton’s method was
ﬁrst published in a book by Raphson 1690. This is why the method in English literature is
often called the Newton–Raphson method. The ﬁrst to give a modern description of New-
ton’s method using derivatives seems to have been Thomas Simpson 1740. Edmond Halley
was a contemporary of Isaac Newton and his third-order method was published more than
300 years ago [179].
Several comprehensive monographs dealing with methods for solving scalar nonlinear
equations are available. Traub [354] gives an exhaustive enumeration of iteration methods
with and without memory, with their order of convergence and efﬁciency index. Much
classical material is also found in Ostrowski [279]. The recently reprinted book by Brent [44]
deals exclusively with methods which only use function values for ﬁnding zeros and minima
of functions of a single variable. It is unique in its careful treatment of algorithmic details
which are crucial when developing reliable computer codes.
Fortran and C versions of some of the zero-ﬁnding and minimization routines given
in [44] are available from Netlib. The MATLAB function fminbnd is based on the Fortran
implementation FMIN of Brent’s algorithm given in Forsythe, Malcolm, and Moler [123,
pp.184–187].
Halley’s method has been rediscovered by J. H. Lambert [234] and numerous other
people; see Traub [354, Sec. 5.22]. A nice exposition of this and other third-order methods
is given by Gander [129]. Families of iteration methods of arbitrary order are studied in
a remarkable paper by Schröder [316], which has been translated into English by G. W.
Stewart [317]. The determinant family of iteration functions Bp(x) is a special case of
a parametrized family of iteration functions for polynomials given by Traub [356]; see
also [204, Sec. 4.4]. This family was derived independently by Kalantari, Kalantari, and
Zaare-Nahandi [221].
There is a vast literature on methods for the classical problem of solving algebraic
equations. Indeed, the nonexistence of an algebraic formula for equations of ﬁfth and higher
degree gave rise to modern algebra. Householder [204] gives an elegant treatment and is an
excellent source for classical results. Detailed surveys are found also in Durand [102] (in
French) and Sendov, Andreev, and Kjurkchiev [320]. Numerical polynomial algebra is an

686
Chapter 6. Solving Scalar Nonlinear Equations
emerging area that falls between numerical analysis and computer algebra. Acomprehensive
survey of this area is given by Stetter [334]. Further studies of the speed and accuracy of
computing polynomial zeros by the MATLAB method of solving the eigenvalue problem
for the companion matrix are given in [157, 104].
The theory of Sturm sequences is treated in [204, Sec. 2.5]. The quadtree method was
used by Weyl [372] to give a constructive proof of the fundamental theorem of algebra. An
interesting analysis of the efﬁcient implementation of this method is given by Pan [282],
who also gives a brief account of the history of algorithms for solving polynomial equations.
Some background on algorithms for computing the GCD is found in [2].

Bibliography
[1] Milton Abramowitz and Irene A. Stegun (eds.). Handbook of Mathematical Functions. Dover,
Mineola, NY, 1965. Republication of work published in 1964 by National Bureau of Standards
(Cited on pp. xix, 67, 77, 164, 167, 168, 189, 208, 210, 212, 260, 260, 269, 269, 270,
270, 291, 301, 301, 315, 315, 316, 316, 318, 318, 320, 328, 349, 467, 529, 535, 545,
546, 555, 572, 572, 574, 587, 593, 606, 607, 607.)
[2] A. G.Akritas.Anew method for computing polynomial greatest common divisor and remainder
sequences. Numer. Math., 52:119–127, 1988. (Cited on p. 686.)
[3] Götz Alefeld and Jürgen Herzberger. Introduction to Interval Computation. Academic Press,
New York, 1983. Translated from German by Jon Rokne. (Cited on p. 147.)
[4] Götz Alefeld and Günter Mayer. Interval analysis: theory and applications. J. Comput. Appl.
Math., 121:421–464, 2000. (Cited on p. 147.)
[5] G. S. Ammar, W. B. Gragg, and Lothar Reichel. An analogue for Szeg˝o polynomials of the
Clenshaw algorithm. J. Comput. Appl. Math., 46:211–216, 1993. (Cited on p. 466.)
[6] Edward Anderson, Zhaojun Bai, Christian Bischof, S. Blackford, James W. Demmel, Jack J.
Dongarra, Jeremy Du Croz, Anne Greenbaum, Sven Hammarling, A. McKenney, and D. C.
Sorensen, editors. LAPACK Users’Guide. SIAM, Philadelphia, PA, third edition, 1999. (Cited
on p. 42.)
[7] Tom M.Apostol. Mathematical Analysis.Addison Wesley, Reading, MA, second edition, 1974.
(Cited on p. 84.)
[8] MarioArioli, Hans Z. Munthe-Kaas, and L. Valdettaro. Componentwise error analysis for FFTs
with applications to fast Helmholtz solvers. Numer. Algorithms, 12:65–88, 1996.
(Cited on
p. 519.)
[9] David H. Bailey. Algorithm 719: Multiprecision translation and execution of FORTRAN pro-
grams. ACM Trans. Math. Software, 19(3):288–319, 1993. (Cited on p. 105.)
[10] David H. Bailey. A Fortran 90-based multiprecision system. ACM Trans. Math. Software,
21(4):379–387, 1995. (Cited on p. 105.)
[11] David H. Bailey, Jon M. Borwein, Peter B. Borwein, and Simon Plouffe. The quest for pi.
Notices Amer. Math. Soc., 19(1):50–57, 1975. (Cited on pp. 104, 655.)
[12] N. T. J. Bailey. A study of queues and appointment systems in hospital outpatient departments,
with special reference to waiting times. J. Roy. Statist. Soc. Ser. B, 14:185–199, 1952. (Cited
on p. 79.)
[13] G. A. Baker, Jr. Essentials of Padé Approximants. Academic Press, New York, 1975. (Cited
on p. 349.)
687

688
Bibliography
[14] G. A. Baker, Jr. and P. Graves-Morris. Padé Approximants. Encyclopedia Math. Appl. 59,
Cambridge University Press, Cambridge, UK, second edition, 1996. (Cited on p. 349.)
[15] Jesse L. Barlow and E. H. Bareiss. On roundoff error distributions in ﬂoating point and loga-
rithmic arithmetic. Computing, 34:325–347, 1985. (Cited on p. 156.)
[16] R. W. Barnard, Germund Dahlquist, K. Pearce, Lothar Reichel, and K. C. Richards. Gram
polynomials and the Kummer function. J. Approx. Theory, 94:128–143, 1998.
(Cited on
p. 464.)
[17] R. H. Bartels, A. R. Conn, and C. Charalambous. On Cline’s direct method for solving overde-
termined linear systems in the l∞sense. SIAM J. Numer. Anal., 15:255–270, 1978. (Cited on
p. 472.)
[18] R. H. Bartels, A. R. Conn, and J. W. Sinclair. Minimization techniques for piecewise differ-
entiable functions: The l1 solution to an overdetermined linear system. SIAM J. Numer. Anal.,
15:224–241, 1978. (Cited on p. 473.)
[19] F. L. Bauer. Genauigkeitsfragen bei der Lösung linearer Gleichungssysteme. Z. Angew. Math.
Mech., 46:7:409–421, 1966. (Cited on p. 156.)
[20] F. L. Bauer, Heinz Rutishauser, and E. Stiefel. New aspects in numerical quadrature. In Proc.
of Symposia in Appl. Math., volume 15, pages 199–218. Amer. Math. Soc., Providence, RI,
1963. (Cited on pp. 548, 549, 550.)
[21] B. J. C. Baxter and Arieh Iserles. On the foundations of computational mathematics. In P. G.
Ciarlet and F. Cucker, editors, Handbook of Numerical Analysis, volume XI, pages 3–34.
North–Holland Elsevier, Amsterdam, 2003. (Cited on p. 84.)
[22] B. K. Beatson. On the convergence of some cubic spline interpolation schemes. SIAM J. Numer.
Anal., 23:903–912, 1986. (Cited on pp. 422, 425.)
[23] Jean-Paul Berrut and Hans D. Mittelmann. Matrices for the direct determination of the barycen-
tric weights of rational interpolation. J. Comput. Appl. Math., 78:355–370, 1997.
(Cited on
pp. 394, 395.)
[24] Jean-Paul Berrut and Lloyd N. Trefethen. Barycentric Lagrange interpolation. SIAM Review,
46(3):501–517, 2004. (Cited on pp. 367, 518.)
[25] G. W. Bickley. Difference and associated operators, with some applications. J. Math. Phys.
MIT, 27:183–192, 1948. (Cited on p. 231.)
[26] O. Biermann. Volesungen über Mathematische Näherungsmetoden. Vieweg, Braunschweig,
1905. (Cited on p. 397.)
[27] Garret Birkhoff. Piecewise polynomial interpolation and approximation. In Henry L. Garabe-
dian, editor, Approximation of Functions, pages 164–190. Elsevier, New York, 1965. (Cited
on p. 518.)
[28] Garret Birkhoff and Henry L. Garabedian. Smooth surface interpolation. J. Math. Phys.,
39:164–190, 1960. (Cited on p. 411.)
[29] Åke Björck and Tommy Elfving. Algorithms for conﬂuent Vandermonde systems. Numer.
Math., 21:130–137, 1973. (Cited on p. 382.)
[30] Åke Björck and Gene H. Golub. Eigenproblems for matrices associated with periodic boundary
problems. SIAM Review., 19(1):5–16, 1977. (Cited on p. 424.)
[31] Åke Björck and Victor Pereyra. Solution of Vandermonde systems of equations. Math. Comp.,
24:893–903, 1970. (Cited on pp. 376, 518.)

Bibliography
689
[32] Harry Björk. Contribution to the problem of least squares approximations.Tech. ReportTRITA-
NA-7137, Dept. Comp. Sci., Royal Institute of Technology, Stockholm, 1971.
(Cited on
p. 471.)
[33] Petter Bjørstad, Germund Dahlquist, and Eric Grosse. Extrapolation of asymptotic expansions
by a modiﬁed Aitken δ2-formula. BIT, 21(1):56–65, 1981. (Cited on pp. 276, 312, 312.)
[34] G. Blanche. Numerical evaluation of continued fractions. SIAM Review, 6(4):383–421, 1964.
(Cited on p. 349.)
[35] G. A. Bliss. Algebraic Functions. Amer. Math. Soc., New York, 1933. Republished in 1947 by
Edwards Brothers, Ann Arbor, MI. (Cited on p. 204.)
[36] Carl de Boor. On calculating with B-splines. J. Approx. Theory, 6:50–62, 1972.
(Cited on
pp. 418, 435.)
[37] Carl de Boor. A Practical Guide to Splines. Springer-Verlag, Berlin, 1991. Revision of ﬁrst
edition from 1978. (Cited on pp. 421, 423, 431, 432, 434, 435, 518, 518.)
[38] Carl de Boor. Divided differences. Surveys in Approximation Theory, 1:49–69, 2005. (Cited
on p. 518.)
[39] Carl de Boor, Klaus Höllig, and Sherman Riemenschneider. Box Splines.Applied Mathematical
Sciences, Volume 98. Springer-Verlag, New York, 1993. (Cited on p. 519.)
[40] Carl de Boor and Allan Pinkus. Backward error analysis for totally positive linear systems.
Numer. Math., 27:485–490, 1977. (Cited on p. 434.)
[41] Folkmar Bornemann, Dirk Laurie, Stan Wagon, and Jürg Waldvogel. The SIAM 100-digit
Challenge. A Study in High-Accuracy Numerical Computing. SIAM, Philadelphia, PA, 2004.
(Cited on p. 558.)
[42] Tibor Boros, Thomas Kailath, and Vadim Olshevsky. A fast parallel Björck–Pereyra-type algo-
rithm for parallel solutions of Cauchy linear systems. Linear Algebra Appl., 302–303:265–293,
1999. (Cited on pp. 376, 377.)
[43] Jon M. Borwein, Peter B. Borwein, and K. Dilcher. Pi, Euler numbers and asymptotic expan-
sions. Amer. Math. Monthly, 96:681–687, 1989. (Cited on p. 321.)
[44] Richard P. Brent. Algorithms for Minimization without Derivatives. Prentice-Hall, Englewood
Cliffs, NJ, 1973. Republished by Dover Publications, Mineola, NY, 2002. (Cited on pp. 478,
622, 635, 661, 662, 685, 685.)
[45] Richard P. Brent.Algorithm 524: AFortran multiple-precision arithmetic package. ACM Trans.
Math. Software, 4(1):71–81, 1978. (Cited on p. 105.)
[46] Richard P. Brent.AFortran multiple-precision arithmetic package. ACM Trans. Math. Software,
4(1):57–70, 1978. (Cited on p. 105.)
[47] Richard P. Brent and H. T. Kung. Fast algorithms for manipulating formal power series. J.
Assoc. Comput. Mach., 25:581–595, 1978. (Cited on p. 179.)
[48] Richard P. Brent, Colin Percival, and Paul Zimmermann. Error bounds on complex ﬂoating-
point multiplication. Math. Comp., 76:1469–1481, 2007. (Cited on p. 112.)
[49] Claude Brezinski. Accélération de la Convergence en Analyse Numérique. Number 584 in
Lecture Notes in Mathematics. Springer Verlag, Berlin, 1977. (Cited on p. 307.)
[50] Claude Brezinski. Padé-Type Approximations and General Orthogonal Polynomials.
Birkhäuser Verlag, Basel, 1980. (Cited on p. 332.)
[51] Claude Brezinski. History of Continued Fractions and Padé Approximants. Springer-Verlag,
Berlin, 1991. (Cited on pp. 332, 349.)

690
Bibliography
[52] Claude Brezinski. Projection Methods for Systems of Equations. Elsevier, Amsterdam, 1997.
(Cited on p. 349.)
[53] Claude Brezinski. Convergence acceleration during the 20th century. J. Comput. Appl. Math.,
122:1–21, 2000. (Cited on p. 349.)
[54] Claude Brezinski and J.Van Iseghem.Ataste of Padé approximation.Acta Numerica, 4:53–103,
1995. (Cited on p. 336.)
[55] Claude Brezinski and Michela Redivo-Zaglia. Extrapolation Methods. Theory and Practice.
North-Holland, Amsterdam, 1991. (Cited on p. 349.)
[56] Claude Brezinski and Luc Wuytack. Numerical analysis in the twentieth century. In Claude
Brezinski and Luc Wuytack, editors, Numerical Analysis: Historical Developments in the 20th
Century, pages 1–40. North Holland Elsevier, Amsterdam, 2001. (Cited on p. 84.)
[57] W. L. Briggs and Van Emden Henson. The DFT. An Owner’s Manual for the Discrete Fourier
Transform. SIAM, Philadelphia, PA, 1995. (Cited on p. 519.)
[58] E. O. Brigham. The Fast Fourier Transform and Its Application. Prentice-Hall, Englewood
Cliffs, NJ, 1988. (Cited on p. 519.)
[59] M. Buhmann. Radial basis functions. Acta Numerica, 9:1–38, 2000. (Cited on p. 519.)
[60] Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta Numerica, 13:147–270, 2004.
(Cited on p. 607.)
[61] J. C. P. Bus and T. J. Dekker. Two efﬁcient algorithms with guaranteed convergence for ﬁnding
a zero of a function. ACM Trans. Math. Software, 1:330–345, 1975. (Cited on p. 634.)
[62] Daniela Calvetti, Gene H. Golub, W. B. Gragg, and Lothar Reichel. Computation of Gauss–
Kronrod quadrature rules. Math. Comp., 69:1035–1052, 2000. (Cited on p. 607.)
[63] Françoise Chaitin-Chatelin and Valerie Frayssé. Lectures on Finite Precision Computations.
SIAM, Philadelphia, PA, 1996. (Cited on p. 156.)
[64] Françoise Chaitin-Chatelin and Elisabeth Traviesas-Cassan. PRECISE and the quality of reli-
able numerical software. In Bo Einarsson, editor, Accuracy and Reliability in Scientiﬁc Com-
puting, pages 95–108. SIAM, Philadelphia, 2005. (Cited on p. 156.)
[65] B. W. Char, K. O. Geddes, G. H. Gonnet, B. L. Leong, M. B. Monagan, and S. M. Watt. Maple
V Library Reference Manual. Springer-Verlag, Berlin, 1991. (Cited on pp. 103, 105.)
[66] E. W. Cheney. Introduction to Approximation Theory. McGraw-Hill, New York, NY, 1966.
(Cited on pp. 323, 326, 332, 410, 473.)
[67] E. W. Cheney and W. Light. A Course in Approximation Theory. Brooks/Cole, Paciﬁc Grove,
CA, 2000. (Cited on p. 356.)
[68] Ward Cheney and David Kincaid. Numerical Mathematics and Computing. Brooks/Cole, Pa-
ciﬁc Grove, CA, fourth edition, 1999. (Cited on p. 85.)
[69] E. B. Christoffel. Über die Gaussische Quadratur und einige Verallgemenierung derselbern. J.
Reine Angew. Math., 55:61–82, 1858. (Cited on p. 568.)
[70] Philippe G. Ciarlet and Jacques Louis Lions. Handbook of Numerical Analysis, volume I–XIII.
North-Holland, Amsterdam, 1990–2005. (Cited on p. 85.)
[71] C. W. Clenshaw.Anote on summation of Chebyshev series. Math. Tables Aids Comput., 9:118–
120, 1955. (Cited on p. 467.)
[72] C. W. Clenshaw andA. R. Curtis.Amethod for numerical integration on an automatic computer.
Numer. Math., 2:197–205, 1960. (Cited on p. 539.)

Bibliography
691
[73] W. J. Cody. Implementation and testing of function software. In P. C. Messina and A. Murli,
editors, Problems and Methodologies in Mathematical Software, pages 24–47. Springer-Verlag,
Berlin, 1982. (Cited on pp. 104, 121.)
[74] W. J. Cody. Algorithm 714: CELEFUNT: A portable test package for complex elementary
functions. ACM Trans. Math. Software, 14(4):1–21, 1993. (Cited on p. 104.)
[75] W. J. Cody and W. Waite. Software Manual for the Elementary Functions. Prentice-Hall, En-
glewood Cliffs, NJ, 1980. (Cited on pp. 103, 123.)
[76] James W. Cooley. The re-discovery of the fast Fourier transform algorithm. Microchimica Acta,
3:33–45, 1987. (Cited on p. 519.)
[77] James W. Cooley, Peter A. W. Lewis, and Peter D. Welsh. The fast Fourier transform and its
application. IEEE Trans. Education, E–12:27–34, 1969. (Cited on p. 519.)
[78] James W. Cooley and John W. Tukey.An algorithm for machine calculation of complex Fourier
series. Math. Comp., 19:297–301, 1965. (Cited on p. 503.)
[79] D.CoppersmithandS.Winograd.Matrixmultiplicationviaarithmeticprogressions.J.Symbolic
Comput., 9:251–280, 1990. (Cited on p. 28.)
[80] G. Corliss, C. Faure, A. Griewank, and L. Hascoet. Automatic Differentiation of Algorithms.
From Simulation to Optimization. Springer-Verlag, Berlin, 2002. (Cited on p. 349.)
[81] R. M. Corless, Gaston H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and Donald E. Knuth. On the
Lambert W function. Adv. Comput. Math., 5:329–359, 1996. (Cited on p. 190.)
[82] R. Courant. Differential and Integral Calculus, volume I. Blackie & Son, London, 1934. Re-
published 1988 in Classics Library, John Wiley. (Cited on p. 170.)
[83] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume I. Interscience, New
York, 1953. (Cited on pp. 485, 495.)
[84] Maurice G. Cox. The numerical evaluation of B-splines. J. Inst. Math. Appl., 10:134–149, 1972.
(Cited on p. 418.)
[85] H. S. M. Coxeter. Introduction to Geometry. Wiley, New York, 1969. (Cited on p. 595.)
[86] Germund Dahlquist. Preliminär rapport om premieobligationsdragning med datamaskin. (In
Swedish), Riksgäldskontoret, Stockholm, 1962. (Cited on p. 67.)
[87] Germund Dahlquist. On summation formulas due to Plana, Lindelöf and Abel, and related
Gauss–Christoffel rules II. BIT, 37(4):804–832, 1997. (Cited on pp. 212, 286, 320, 349.)
[88] Germund Dahlquist. On summation formulas due to Plana, Lindelöf and Abel, and related
Gauss–Christoffel rules I. BIT, 37(2):256–295, 1997. (Cited on pp. 582, 606.)
[89] Germund Dahlquist and Åke Björck. Numerical Methods. Prentice-Hall, Englewood Cliffs,
NJ., 1974. Republished in 2003 by Dover, Mineola, NY. (Cited on pp. 42, 85.)
[90] G. Danielson and Cornelius Lanczos. Some improvements in practical Fourier analysis and
their applications to X-ray scattering from liquids. J. Franklin Inst., 233:365–380, 435–452,
1942. (Cited on pp. 505, 519.)
[91] I. Daubechies. Ten Lectures on Wavelets. SIAM, Philadelphia, 1992. (Cited on p. 519.)
[92] Philip J. Davis. Interpolation and Approximation. Blaisdell, New York, 1963. Republished in
1975 by Dover, Mineola, NY. (Cited on pp. 450, 456, 457, 457, 458, 476.)
[93] Philip J. Davis and Philip Rabinowitz. Methods of Numerical Integration. Academic Press,
Orlando, FL, second edition, 1984. (Cited on pp. 539, 606, 606, 606, 607, 607.)

692
Bibliography
[94] James W. Demmel. Underﬂow and the reliability of numerical software. SIAM J. Sci. Stat.
Comput., 5(4):887–919, 1984. (Cited on p. 101.)
[95] James Demmel and Plamen Koev. The accurate and efﬁcient solution of a totally positive
generalizedVandermonde linear system. SIAM J. MatrixAnal.Appl., 27:142–152, 2005. (Cited
on p. 376.)
[96] Peter Deuﬂhard and Andreas Hohmann. Numerical Analysis in Modern Scientiﬁc Computing.
Springer, Berlin, second edition, 2003. (Cited on pp. 85, 260.)
[97] P. Dierckx. FITPACK User Guide Part I: Curve ﬁtting routines. TW Report 89, Department of
Computer Science, Katholieke Universiteit, Leuven, Belgium, 1987. (Cited on p. 518.)
[98] P. Dierckx. FITPACK User Guide Part II: Surface ﬁtting routines. TW Report 122, Department
of Computer Science, Katholieke Universiteit, Leuven, Belgium, 1989. (Cited on p. 518.)
[99] P. Dierckx. Curve and Surface Fitting with Splines. Oxford University Press, New York, 1993.
(Cited on p. 436.)
[100] J. Dieudonné. Foundations of Modern Analysis. Academic Press, New York, NY, 1960. Re-
published by Hesperides Press, 2006. (Cited on p. 620.)
[101] J. J. Dongarra, James R. Bunch, Cleve B. Moler, and G. W. Stewart. LINPACK Users’ Guide.
SIAM, Philadelphia, PA, 1979. (Cited on p. 42.)
[102] E. Durand. Solutions Numériques des ÉquationsAlgébriques. Tom I: Équations du Type F(x) =
0, Racines d’un Polynôme. Masson et Cie, Paris, 1960. (Cited on p. 685.)
[103] Alan Edelman. The mathematics of the Pentium division bug. SIAM Review, 39(1):54–67,
1997. (Cited on p. 89.)
[104] Alan Edelman and H. Murakami. Polynomial roots from companion matrix eigenvalues. Math.
Comp., 64:763–776, 1995. (Cited on p. 686.)
[105] Bo Einarsson. Numerical calculation of Fourier integrals with cubic splines. BIT, 8(4):279–286,
1968. (Cited on p. 555.)
[106] Bo Einarsson. On the calculation of Fourier integrals. In C. V. Freiman, editor, Information
Processing 71, volume 8, pages 1346–1350, Amsterdam, 1972. North–Holland.
(Cited on
pp. 555, 556, 557.)
[107] Bo Einarsson. Use of Richardson extrapolation for the numerical calculation of Fourier trans-
forms. J. Comput. Phys., 21(3):365–370, 1976. (Cited on p. 555.)
[108] Bo Einarsson, editor. Accuracy and Reliability in Scientiﬁc Computing. SIAM, Philadelphia,
2005. (Cited on p. 156.)
[109] Lars Eldén, Linde Wittmeyer-Koch, and Hans Bruun Nielsen. Introduction to Numerical Com-
putation. Studentlitteratur, Lund, Sweden, 2004. (Cited on p. 85.)
[110] Sylvan Elhay and Jaroslav Kautsky. Algorithm 655. IQPACK: FORTRAN subroutines for
the weights of interpolatory quadratures. ACM Trans. Math. Software, 13(4):399–415, 1987.
(Cited on p. 607.)
[111] Erik Elmroth, F. G. Gustavson, Isak Jonsson, and Bo Kågström. Recursive blocked algorithms
and hybrid data structures for dense matrix library software. SIAM Review, 46:3–45, 2004.
(Cited on p. 22.)
[112] H. Engels. Numerical Quadrature and Cubature. Computational Mathematics and Applica-
tions. Academic Press, New York, 1980. (Cited on pp. 606, 607.)
[113] K. Eriksson, D. Estep, P. Hansbo, and C. Johnson. Computational Differential Equations.
Cambridge University Press, Cambridge, UK, 1996. (Cited on p. 594.)

Bibliography
693
[114] Terje O. Espelid. On ﬂoating-point summation. SIAM Review, 37:603–607, 1995. (Cited on
p. 21.)
[115] Terje O. Espelid. Doubly adaptive quadrature routines based on Newton–Cotes rules. BIT,
43(2):319–337, 2003. (Cited on p. 606.)
[116] Gerald Farin. Curves and Surfaces for Computer Aided Geometric Design: A Practical Guide.
Academic Press, New York, 1988. (Cited on p. 518.)
[117] L. Fejér. Mechanische Quadraturen mit positiven Cotesschen Zahlen. Math. Z., 37:287–309,
1933. (Cited on p. 539.)
[118] Louis Napoleon George Filon. On a quadrature formula for trigonometric integrals. Proc. Roy.
Soc. Edinburgh, 49:38–47, 1928–1929. (Cited on p. 555.)
[119] George E. Forsythe. Generation and use of orthogonal polynomials for data-ﬁtting with a digital
computer. J. Soc. Indust. Appl. Math., 5(2):74–88, 1957. (Cited on p. 518.)
[120] George E. Forsythe. Algorithms for scientiﬁc computation. Comm. ACM, 9:255–256, 1966.
(Cited on p. 127.)
[121] GeorgeE.Forsythe.Whatisasatisfactoryquadraticequationsolver? InB.DejonandP.Henrici,
editors, Constructive Aspects of the Fundamental Theorem of Algebra, pages 53–61. Wiley-
Interscience, London, 1969. (Cited on p. 16.)
[122] George E. Forsythe. Pitfalls in computation, or why a math book isn’t enough. Amer. Math.
Monthly, 77:931–956, 1970. (Cited on p. 156.)
[123] George E. Forsythe, Michael A. Malcolm, and Cleve B. Moler. Computer Methods for Math-
ematical Computations. Prentice-Hall, Englewood Cliffs, NJ, 1977.
(Cited on pp. 85, 635,
685.)
[124] George E. Forsythe and Cleve B. Moler. Computer Solution of Linear Algebraic Systems.
Prentice-Hall, Englewood Cliffs, NJ, 1967. (Cited on p. 33.)
[125] Leslie Fox. How to get meaningless answers in scientiﬁc computation (and what to do about
it). IMA Bulletin, 7(10):296–302, 1971. (Cited on p. 156.)
[126] G. Freud. Orthogonale Polynome. Birkhäuser, Basel, 1969. (Cited on p. 573.)
[127] F. G. Frobenius. Über Relationen zwischen den Näherungsbrüchen von Potenzreihen. J. für
Math., 90:1–17, 1881. (Cited on p. 349.)
[128] Carl-Erik Fröberg. Lärobok i Numerisk Analys. Svenska Bokförlaget/Bonniers, Stockholm,
1962. In Swedish. (Cited on p. 314.)
[129] Walter Gander. On Halley’s iteration method. Amer. Math. Monthly, 92:131–134, 1985. (Cited
on pp. 648, 685.)
[130] Walter Gander. Change of basis in polynomial interpolation. Numer. Linear Algebra Appl.,
12(8):769–778, 2005. (Cited on p. 353.)
[131] Walter Gander andWalter Gautschi.Adaptive quadrature—Revisited. BIT, 40(1):84–101, 2000.
(Cited on pp. 562, 576.)
[132] Walter Gander and Dominik Gruntz. Derivation of numerical methods using computer algebra.
SIAM Review, 41(3):577–593, 1999. (Cited on p. 606.)
[133] B. S. Garbow, J. M. Boyle, J. J. Dongarra, and G. W. Stewart. Matrix Eigensystems Routines:
EISPACK Guide Extension. Springer-Verlag, New York, 1977. (Cited on p. 42.)
[134] Mariano Gasca and Thomas Sauer. On the history of multivariate polynomial interpolation. J.
Comput. Appl. Math., 122:23–35, 2000. (Cited on p. 518.)

694
Bibliography
[135] M. Gasca and J. M. Peña. On factorizations of totally positive matrices. In M. Gasca and C. A.
Micchelli, editors, Total Positivity, pages 109–130. Kluwer Academic Publ., Dordrecht, 1996.
(Cited on p. 376.)
[136] C. F. Gauss. Methodus nova integralium valores per approximationem inveniendi. Comm. Soc.
R. Sci. Göttingen Recens, 3:39–76, 1814. (Cited on p. 568.)
[137] C. F. Gauss. Theoria interpolationis methodo nova tractata. In Carl Friedrich Gauss, Werke,
volume Band 3, pages 265–303. Königlichen Gesellschaft der Wissenschaften, Göttingen,
1866. (Cited on p. 519.)
[138] Carl Friedrish Gauss. Theory of the Motion of of the Heavenly Bodies Moving about the Sun in
Conic Sections. Dover, New York, (1963), C. H. Davis, Translation, 1809. (Cited on p. 84.)
[139] Walter Gautschi. Computational aspects of three-term recurrence relations. SIAM Review,
9(1):24–82, 1967. (Cited on p. 18.)
[140] Walter Gautschi. On the construction of Gaussian quadrature rules from modiﬁed moments.
Math. Comp., 24:245–260, 1970. (Cited on p. 606.)
[141] Walter Gautschi. Attenuation factors in practical Fourier analysis. Numer. Math., 18(5):373–
400, 1972. (Cited on p. 490.)
[142] Walter Gautschi. Anomalous convergence of a continued fraction for ratios of Kummer func-
tions. Math. Comp., 31(140):994–999, 1977. (Cited on p. 329.)
[143] Walter Gautschi. A survey of Gauss–Christoffel quadrature formulae. In P. L. Butzer and F. Fe-
hér, editors, E. B. Christoffel: The inﬂuence of his work on mathematics and the physical
sciences, pages 72–147. Birkhäuser, Basel, 1981. (Cited on p. 606.)
[144] Walter Gautschi. Questions of numerical condition related to polynomials. In Gene H. Golub,
editor, Studies in Numerical Analysis, pages 140–177. The Mathematical Association of Amer-
ica, Washington, DC, 1984. (Cited on pp. 356, 666.)
[145] Walter Gautschi.Algorithm 726: ORTHPOL—a package of routines for generating orthogonal
polynomials and Gauss-type quadrature rules. ACM Trans. Math. Software, 20:21–62, 1994.
(Cited on p. 607.)
[146] Walter Gautschi. Orthogonal polynomials: applications and computation. Acta Numerica,
5:45–119, 1996. (Cited on p. 606.)
[147] Walter Gautschi. Numerical Analysis. An Introduction. Birkhäuser, Boston, MA, 1997. (Cited
on pp. 85, 135, 356.)
[148] Walter Gautschi. Orthogonal Polynomials: Computation andApproximation. Numerical Math-
ematics and Scientiﬁc Computation. Oxford University Press, New York, 2004.
(Cited on
pp. 468, 606.)
[149] Walter Gautschi. The numerical evaluation of a challenging integral. Tech. Report, Department
of Computer Science, Purdue University, West Lafayette, IN, 2005. (Cited on p. 558.)
[150] Walter Gautschi. Orthogonal polynomials (in MATLAB). J. Comput. Appl. Math., 178(1–
2):215–234, 2005. (Cited on p. 607.)
[151] A. O. Gelfond. Calculus of Finite Differences. Hindustan Publishing Corporation, Dehli, 1971.
Translation of third Russian edition. (Cited on p. 518.)
[152] James E. Gentle. Random Number generation and Monte Carlo Methods. Springer-Verlag,
New York, second edition, 2003. (Cited on p. 84.)
[153] W.M.Gentleman.Algorithm424: Clenshaw–Curtisquadrature.Comm.Assoc.Comput.Mach.,
15(5):353–355, 1972. (Cited on p. 540.)

Bibliography
695
[154] W. M. Gentleman. Implementing Clenshaw–Curtis quadrature I and II. Comm. Assoc. Comput.
Mach., 15(5):337–346, 1972. (Cited on p. 540.)
[155] W. M. Gentleman and G. Sande. Fast Fourier transforms—for fun and proﬁt. In Proceedings
AFIPS 1966 Fall Joint Computer Conference, pages 503–578. Spartan Books, Washington,
D.C., 1966. (Cited on p. 508.)
[156] John R. Gilbert, Cleve Moler, and Robert Schreiber. Sparse matrices in MATLAB: Design and
implementation. SIAM J. Matrix Anal. Appl., 13:333–356, 1992. (Cited on p. 40.)
[157] S. Goedecker. Remark on algorithms to ﬁnd roots of polynomials. SIAM J. Sci. Comput.,
15:1059–1063, 1994. (Cited on p. 686.)
[158] David Goldberg. What every computer scientist should know about ﬂoating point arithmetic.
ACM Computing Surveys, 23:5–48, 1991. (Cited on pp. 101, 102, 106, 106, 110, 116, 156.)
[159] Herman H. Goldstine. The Computer from Pascal to von Neumann. Princeton University Press,
Princeton, NJ, 1972. (Cited on pp. 245, 293, 349.)
[160] Herman H. Goldstine. A History of Numerical Analysis from the 16th through the 19th Century.
Stud. Hist. Math. Phys. Sci., Vol. 2. Springer-Verlag, New York, 1977. (Cited on p. 84.)
[161] Herman H. Goldstine and John von Neumann. Numerical inverting of matrices of high order
II. Proceedings Amer. Math. Soc., 2:188–202, 1951. (Cited on p. 156.)
[162] Gene H. Golub. Some modiﬁed matrix eigenvalue problems. SIAM Review, 15:318–334, 1973.
(Cited on pp. 581, 606.)
[163] Gene H. Golub. Matrix computations and the theory of moments. In S. D. Chatterji, editor,
Proceedings of the International Congress of Mathematicians, Zürich 1994, volume 2, pages
1440–1448, Basel, Switzerland, 1995. Birkhäuser-Verlag. (Cited on p. 574.)
[164] Gene H. Golub and J. Kautsky. Calculation of Gauss quadratures with multiple free and ﬁxed
knots. Numer. Math., 41:147–163, 1983. (Cited on p. 606.)
[165] Gene H. Golub and Gérard Meurant. Matrices, moments and quadrature. In D. F. Grifﬁths and
G. A. Watson, editors, Numerical Analysis 1993: Proceedings of the 13th Dundee Biennial
Conference, Pitman Research Notes in mathematics, pages 105–156. Longman Scientiﬁc and
Technical, Harlow, Essex, UK, 1994. (Cited on p. 582.)
[166] Gene H. Golub and James M. Ortega. Scientiﬁc Computing and Differential Equations. An
Introduction to Numerical Methods. Academic Press, San Diego, CA, 1992. (Cited on p. 85.)
[167] GeneH.GolubandChristianReinsch.Singularvaluedecompositionandleastsquaressolutions.
Numer. Math, 14:403–420, 1970. (Cited on p. 50.)
[168] Gene H. Golub and Lyle B. Smith. Chebyshev approximation of continuous functions by a
Chebyshev system of functions. Algorithm 414. Comm. ACM., 14(11):737–746, 1971. (Cited
on p. 478.)
[169] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University
Press, Baltimore, MD, third edition, 1996. (Cited on p. 27.)
[170] Gene H. Golub and J. H. Welsch. Calculation of Gauss quadrature rules. Math. Comp., 23:221–
230, 1969. (Cited on p. 606.)
[171] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series and Products. Academic Press,
London, UK, ﬁfth edition, 1993. (Cited on p. 521.)
[172] W. B. Gragg. The Padé table and its relation to certain algorithms of numerical analysis. SIAM
Review, 14:1–62, 1972. (Cited on pp. 333, 349.)

696
Bibliography
[173] P. R. Graves-Morris, D. E. Roberts, and A. Salam. The epsilon algorithm and related topics. J.
Comput. Appl. Math., 122:51–80, 2000. (Cited on p. 349.)
[174] Ulf Grenander and G. Szeg˝o. Toeplitz Forms and their Applications. Chelsea, New York, 1984.
(Cited on p. 466.)
[175] A. Griewank. Evaluating Derivatives; Principles and Techniques of Algorithmic Differentia-
tion. Frontiers in Applied Mathematics, volume 19. SIAM, Philadelphia, PA, 2000. (Cited on
p. 349.)
[176] Sven-Åke Gustafson. Control and estimation of computational errors in the evaluation of inter-
polation formulae and quadrature rules. Math. Comp., 24:847–854, 1970. (Cited on p. 357.)
[177] Sven-Åke Gustafson. Convergence acceleration on a general class of power series. Computing,
21:53–69, 1978. (Cited on p. 292.)
[178] Ernst Hairer and GerhardWanner.Analysis by Its History. Springer-Verlag, Berlin, 2000. (Cited
on p. 443.)
[179] Edmond Halley. A new and general method of ﬁnding the roots of equations. Philos. Trans.
Roy. Soc. London, 18:136, 1694. (Cited on pp. 648, 685.)
[180] John Halton. On the efﬁciency of certain quasi-random sequences of points in evaluating
multidimensional integrals. Numer. Math., 2:84–90, 1960. (Cited on p. 603.)
[181] John Halton and G. B. Smith. Algorithm 247: Radical-inverse quasi-random point sequence.
ACM Trans. Math. Software, 7:701–702, 1964. (Cited on p. 606.)
[182] Günther Hämmerlin and Karl-Heinz Hoffmann. Numerical Mathematics. Springer-Verlag,
Berlin, 1991. (Cited on p. 85.)
[183] J. M. Hammersley and D. C. Handscomb. Monte Carlo Methods. Methuen, London, UK, 1964.
(Cited on pp. 81, 84, 603.)
[184] Richard W. Hamming. Numerical Methods for Scientists and Engineers. McGraw-Hill, New
York, second edition, 1974. Republished in 1986 by Dover, Mineola, NY. (Cited on p. 85.)
[185] Hermann Hankel. Über eine besondere Klasse der symmetrischen Determinanten. Inaugural
Diss., Universität Göttingen, Germany, 1861. (Cited on p. 337.)
[186] G. I. Hargreaves. Interval analysis in MATLAB. Numer. anal. report 416, Department of Math-
ematics, University of Manchester, 2002. (Cited on p. 147.)
[187] J. F. Hart, E. W. Cheney, C. L. Lawson, H. J. Maehly, C. K. Mesztenyi, J. F. Rice, Jr.
H. G. Thacher, and C. Witzgall. Computer Approximations. John Wiley, New York, New York,
1968. (Cited on pp. 102, 103.)
[188] T. Håvie. Generalized Neville type extrapolation schemes. BIT, 19(2):204–213, 1979. (Cited
on p. 349.)
[189] J. G. Hayes. Fitting data in more than one variable. In J. G. Hayes, editor, Numerical Approx-
imation to Functions and Data, pages 84–97. The Athlon Press, London, 1970.
(Cited on
p. 518.)
[190] Michael T. Heath. Scientiﬁc Computing. An Introductory Survey. McGraw-Hill, Boston, MA,
second edition, 2002. (Cited on p. 85.)
[191] Peter Hellekalek. Good random number generators are (not so) easy to ﬁnd. Math. Comput.
Simulation, 46:485–505, 1998. (Cited on p. 84.)
[192] Peter Henrici. Discrete Variable Methods in Ordinary Differential Equations. John Wiley, New
York, 1962. (Cited on p. 254.)

Bibliography
697
[193] Peter Henrici. Elements of NumericalAnalysis. JohnWiley, NewYork, 1964. (Cited on pp. 273,
462.)
[194] Peter Henrici. Fast Fourier methods in computational complex analysis. SIAM Review,
21(4):481–527, 1979. (Cited on p. 195.)
[195] Peter Henrici. Essentials of Numerical Analysis. John Wiley, New York, 1982.
(Cited on
p. 368.)
[196] Peter Henrici. Applied and Computational Complex Analysis. Volume 1: Power Series, In-
tegration, Conformal Mapping, Location of Zeros. Wiley Classics Library, New York, 1988.
Reprint of the 1974 original. (Cited on pp. 181, 183, 191, 326, 340, 341, 342, 342, 349,
644, 650.)
[197] Peter Henrici. Applied and Computational Complex Analysis. Volume 2: Special Functions,
Integral Transforms, Asymptotics, Continued Fractions. Wiley Classics Library, New York,
1991. Reprint of the 1977 original. (Cited on pp. 301, 326, 344.)
[198] Nicholas J. Higham. Error analysis of the Björck–Pereyra algorithm for solving Vandermonde
systems. Numer. Math., 50:613–632, 1987. (Cited on p. 376.)
[199] Nicholas J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, PA,
second edition, 2002. (Cited on pp. 97, 111, 112, 156, 324, 356, 377, 518.)
[200] Nicholas J. Higham. The numerical stability of barycentric Lagrange interpolation. IMA J.
Numer. Anal., 24:547–556, 2004. (Cited on p. 369.)
[201] F. B. Hildebrand. Introduction to Numerical Analysis. McGraw-Hill, NewYork, second edition,
1956. Republished in 1988 by Dover, Mineola, NY. (Cited on pp. 85, 393.)
[202] C. A. R. Hoare. Quicksort. Computer J., 5(1):10–15, 1962. (Cited on p. 21.)
[203] RogerA. Horn and Charles R. Johnson. Topics in Matrix Analysis. Cambridge University Press,
Cambridge, 1991. (Cited on p. 518.)
[204] Alston S. Householder. The Numerical Treatment of a Single Nonlinear Equation. McGraw-
Hill, New York, 1970. (Cited on pp. 636, 663, 676, 685, 685, 686.)
[205] IEEE Standard for Binary Floating-Point Arithmetic. ANSI/IEEE Standard 754-1985. SIG-
PLAN Notices, 22(2):9–25, 1987. (Cited on pp. 99, 156.)
[206] IEEE Standard for Radix-Independent Floating Point Arithmetic. ANSI/IEEE Standard 854-
1987. Technical report, IEEE Computer Society, New York, 1987. (Cited on p. 99.)
[207] J. P. Imhof. On the method for numerical integration of Clenshaw and Curtis. Numer. Math.,
5:138–141, 1963. (Cited on p. 539.)
[208] Eugene Isaacson and Herbert B. Keller. Analysis of Numerical Methods. John Wiley, NewYork,
1966. Republished in 1994 by Dover, Mineola, NY. (Cited on pp. 85, 372, 396, 518.)
[209] Arieh Iserles. On the numerical quadrature of highly-oscillating integrals I: Fourier transforms.
IMA J. Numer. Anal., 24:365–391, 2004. (Cited on p. 557.)
[210] Glenn James and Robert C. James, editors. James & James Mathematics Dictionary. Van
Nostrand, Princeton, NJ, ﬁfth edition, 1992. (Cited on p. 84.)
[211] M. A. Jenkins and J. F. Traub. A three-stage variable shift iteration for polynomial zeros and its
relation to generalized Rayleigh quotient iteration. Numer. Math., 14:252–263, 1970. (Cited
on p. 675.)
[212] M. A. Jenkins and J. F. Traub. Algorithm 419—Zeros of a complex polynomial. Comm. ACM.,
15:97–99, 1972. (Cited on p. 675.)

698
Bibliography
[213] L. W. Johnson and D. R. Scholz. On Steffensen’s method. SIAM J. Numer. Anal., 5:296–302,
1968. (Cited on p. 632.)
[214] W. B. Jones andW. J.Thron. Continued Fractions.Analytic Theory andApplications, volume 11
of Encyclopedia of Mathematics and its Application. Addison-Wesley, Reading, MA, 1980.
(Cited on pp. 334, 349.)
[215] D. C. Joyce. Survey of extrapolation processes in numerical analysis. SIAM Review, 13(4):435–
490, 1971. (Cited on p. 349.)
[216] W. Kahan. A survey of error analysis. In B. Dejon and P. Henrici, editors, Proceedings IFIP
Congress Ljubljana, Information Processing 1971, pages 1214–1239. North-Holland, Amster-
dam, 1971. (Cited on pp. 121, 156.)
[217] W. Kahan. Branch cuts for elementary functions or much ado about nothing’s sign bit. In Arieh
Iserles and M. J. D. Powell, editors, The State of the Art in Numerical Analysis, pages 165–211.
Clarendon Press, Oxford, UK, 1987. (Cited on p. 104.)
[218] W. Kahan. Miscalculating Area and Angles of a Needle-like Triangle. Work in Progress, De-
partment of Computer Science, University of California, Berkeley, CA, 2000.
(Cited on
p. 124.)
[219] W. M. Kahan. Numerical linear algebra. Canad. Math. Bull., 9:757–801, 1966.
(Cited on
p. 37.)
[220] David Kahaner, Cleve B. Moler, and Stephen G. Nash. Numerical Methods and Software.
Prentice-Hall, Englewood Cliffs, NJ, 1989. (Cited on pp. 76, 85, 576.)
[221] Bahman Kalantari, Iraj Kalantari, and Rahim Zaare-Nahandi. A basic family of iteration func-
tions for polynomial root ﬁnding and its characterizations. J. Comput.Appl. Math., 80:209–226,
1997. (Cited on p. 685.)
[222] S. Karlin and W. J. Studden. Tchebysheff Systems with Applications in Analysis and Statistics.
Interscience, New York, 1966. (Cited on p. 518.)
[223] Alan H. Karp. Bit reversal on uniprocessors. SIAM Review, 38(1):1–26, 1996.
(Cited on
p. 519.)
[224] Jaroslav Kautsky and Sylvan Elhay. Calculation of the weights of interpolatory quadratures.
Numer. Math., 40:407–422, 1982. (Cited on p. 607.)
[225] R. Kearfott. Interval computations: Introduction, uses, and resources. Euromath. Bulletin,
2(1):95–112, 1996. (Cited on p. 147.)
[226] David Kincaid and Ward Cheney. Numerical Analysis. Brooks/Cole, Paciﬁc Grove, CA, third
edition, 2002. (Cited on p. 85.)
[227] Göran Kjellberg. Two observations on Durand–Kerner’s root ﬁnding method. BIT, 24(4):556–
559, 1984. (Cited on p. 673.)
[228] Felix Klein. Elementary Mathematics from an Advanced Standpoint. Dover, Mineola, NY,
1945. Translation from German original, 1924. (Cited on p. 324.)
[229] Konrad Knopp. Inﬁnite Sequences and Series. Dover, Mineola, NY, 1956. (Cited on p. 349.)
[230] Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms.
Addison-Wesley, Reading, MA, third edition, 1998.
(Cited on pp. 68, 69, 70, 72, 73, 74,
76, 77, 77, 84, 103, 155, 174, 179.)
[231] Donald E. Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching.
Addison-Wesley, Reading, MA, second edition, 1998. (Cited on p. 83.)

Bibliography
699
[232] A. S. Kronrod. Integration with control of accuracy. Dokl. Akad. Nauk. SSSR, 154:283–286,
1964. (In Russian.) (Cited on p. 575.)
[233] A.S.Kronrod.NodesandWeightsforQuadratureFormulae.Sixteen-placetables.Izdat.Nauka,
Moscow, 1964. (In Russian.) English translation in: Consultants Bureau, New York, 1965.
(Cited on pp. 575, 576.)
[234] Johann H. Lambert. Beiträge zum Gebrauche der Mathematik und derenAnwendungen. Zweiter
Theil. 1770. (Cited on p. 685.)
[235] Cornelius Lanczos. Applied Analysis. Prentice-Hall, Englewood Cliffs, NJ, 1956. Republished
in 1988 by Dover, Mineola, NY. (Cited on pp. 487, 519.)
[236] Cornelius Lanczos. Linear Differential Operators. D. Van Nostrand, London, UK, 1961. Re-
published in 1974 by Dover, Mineola, NY. (Cited on p. 583.)
[237] D. P. Laurie. Calculation of Gauss–Kronrod quadrature rules. Math. Comp., 66:1133–1145,
1997. (Cited on p. 607.)
[238] D. P. Laurie. Computation of Gauss-type quadrature formulas. J. Comput. Appl. Math.,
127:201–217, 2001. Numerical Analysis 2000. Vol. V. Quadrature and Orthogonal Polyno-
mials. (Cited on p. 607.)
[239] Charles L. Lawson, Richard J. Hanson, David R. Kincaid, and Fred T. Krogh. Basic Linear
Algebra Subprograms for Fortran usage. ACM Trans. Math. Software, 5:308–323, 1979. (Cited
on p. 42.)
[240] N. N. Lebedev. Special Functions and Their Applications. Dover, Mineola, NY, 1972. Trans-
lation from Russian original. (Cited on pp. 210, 212, 219, 301, 348.)
[241] H. Lebesgue. Sur l’approximation des fonctions. Bull. Sciences Math., 22:278–287, 1898.
(Cited on p. 449.)
[242] PierreL’Ecuyer.Uniformrandomnumbergeneration.Ann.Oper.Res., 53:77–120, 1994. (Cited
on p. 84.)
[243] Pierre L’Ecuyer. Random number generation. In Jerry Banks, editor, The Handbook of Simu-
lation, pages 93–137. John Wiley, New York, 1998. (Cited on p. 84.)
[244] Pierre L’Ecuyer. Software for uniform random number generation: Distinguishing the good
and bad. In Proc. 2001 Winter Simulation Conference, pages 95–105. IEEE Press, Piscataway,
NJ, 2001. (Cited on pp. 70, 73.)
[245] Adrien-Marie Legendre. Nouvelles méthodes pour la détermination des orbites des comètes.
Courcier, Paris, 1805. (Cited on p. 84.)
[246] C. C. Lin and L. A. Segel. Mathematics Applied to Deterministic Problems in the Natural
Sciences. Macmillan, New York, 1974. (Cited on p. 204.)
[247] Xiaoye S. Li, James W. Demmel, David H. Bailey, Greg Henry, Y. Hida, Jimmy Iskandar,
William Kahan, Suh Y. Kang, Anil Kapur, Michael C. Martin, Brandon J. Thompson, Teresa
Tung, and Daniel J. Yoo. Design, implementation and testing of extended and mixed precision
BLAS. ACM Trans. Math. Software, 28(2):152–205, 2002. (Cited on p. 115.)
[248] L. Lorenzen and H. Waadeland. Continued Fractions with Applications. North-Holland, Ams-
terdam, 1992. (Cited on p. 349.)
[249] Daniel W. Lozier. NIST digital library of mathematical functions. Annals of Mathematics and
Artiﬁcial Intelligence, 38:105–119, 2003. (Cited on p. 348.)

700
Bibliography
[250] D. W. Lozier and F. W. J. Olver. Numerical evaluation of special functions. In W. Gautschi, edi-
tor, Mathematics of Computation 1943–1993: A Half-Century of Computational Mathematics,
volume 48 of Proc. Sympos. Appl. Math., pages 79–125, Providence, RI, 1994. Amer. Math.
Soc. (Cited on p. 348.)
[251] James N. Lyness. Notes on the adaptive Simpson quadrature routine. J. Assoc. Comput. Mach.,
16:483–495, 1969. (Cited on p. 606.)
[252] James N. Lyness and Cleve B. Moler. Numerical differentiation of analytic functions. SIAM J.
Numer. Anal., 4:202–210, 1967. (Cited on p. 195.)
[253] Kaj Madsen. A root-ﬁnding algorithm based on Newton’s method. BIT, 13(1):71–75, 1973.
(Cited on pp. 675, 677.)
[254] Kaj Madsen and John K. Reid. Fortran subroutines for ﬁnding polynomial zeros. Tech. Report
A.E.R.E. R.7986, Computer Science and Systems Division, A.E.R.E. Harwell, 1975. (Cited
on pp. 675, 677.)
[255] H. J. Maehly. Zur iterativen Auﬂösung algebraischer Gleichungen. Z. angew. Math. Phys.,
5:260–263, 1954. (Cited on p. 672.)
[256] Michael A. Malcolm. On the computation of nonlinear spline functions. SIAM J. Numer. Anal.,
14(2):254–282, 1977. (Cited on p. 417.)
[257] George Marsaglia. Expressing a random variable in terms of uniform random variables. Ann.
Math. Statist., 32:894–898, 1961. (Cited on p. 77.)
[258] George Marsaglia. Random numbers falls mainly in the planes. Proc. Nat. Acad. Sci. USA,
61(5):25–28, 1968. (Cited on p. 69.)
[259] George Marsaglia.Acurrent view of random number generators. In L. Billard, editor, Computer
Science and Statistics: The Interface, pages 3–10. Elsevier Science Publishers, Amsterdam,
1985. (Cited on p. 84.)
[260] George Marsaglia and W. W. Tsang. A fast, easily implemented method for sampling from
decreasing or symmetric unimodal density functions. SIAM J. Sci. Stat. Comput., 5(2):349–
359, 1984. (Cited on p. 76.)
[261] M. Matsumoto and T. Nishimura. Mersenne twister: A 623-dimensionally equidistributed uni-
form pseudo-random number generator. ACM Trans. Modeling Comput. Software, 8(1):3–30,
1998. (Cited on p. 70.)
[262] Urs von Matt. Gauss quadrature. In Walter Gander and Jiˇri Hˇrebiˇcek, editors, Solving Problems
in Scientiﬁc Computing using Maple and MATLAB, pages 251–279. Springer-Verlag, Berlin,
fourth edition, 2004. (Cited on p. 607.)
[263] J. McNamee and Frank Stenger. Construction of fully symmetric numerical integration formu-
las. Numer. Math., 10:327–344, 1967. (Cited on p. 607.)
[264] Günter Meinardus. Approximation of Functions: Theory and Numerical Methods, volume 16
of Springer Tracts in Natural Philosophy. Springer-Verlag, Berlin, 1967. (Cited on p. 478.)
[265] L. M. Milne-Thomson. Calculus of Finite Differences. Macmillan & Co., London, 1933. (Cited
on p. 518.)
[266] Cleve Moler. Random thoughts, 10435 years is a very long time. MATLAB News and Notes,
Fall, 1995. (Cited on p. 70.)
[267] Cleve Moler. Normal behavior. MATLAB News and Notes, Spring, 2001. (Cited on p. 76.)
[268] Cleve B. Moler. Numerical Computing with MATLAB. SIAM, Philadelphia, PA, 2004. (Cited
on p. 635.)

Bibliography
701
[269] Cleve Moler and Steve Eddins. Fast ﬁnite Fourier transforms. MATLAB News and Notes, pages
14–15, Winter, 2001. (Cited on p. 517.)
[270] Giovanni Monegato. An overview of results and questions related to Kronrod schemes. In
G. Hämmerlin, editor, Numerische Integration, pages 231–240. Birkhäuser, Numer. Math. 45,
Basel, 1979. (Cited on p. 607.)
[271] Ramon E. Moore. Interval Analysis. Prentice-Hall, Englewood Cliffs, NJ, 1966.
(Cited on
pp. 147, 646.)
[272] J.-M. Muller. Elementary Functions: Algorithm and Implementation. Birkhäuser, Boston, MA,
1997. (Cited on pp. 103, 121.)
[273] I. P. Mysovskih. On the construction of cubature formulas with the smallest number of nodes.
Soviet Math. Dokl., 9:277–280, 1968. (Cited on p. 606.)
[274] J. C. Nash. Compact Numerical Methods for Computers: Linear Algebra and Function Mini-
mization. American Institute of Physics, New York, second edition, 1990. (Cited on p. 84.)
[275] H. Niederreiter. Random Number Generation and Quasi-Monte Carlo Methods. SIAM,
Philadelphia, PA, 1992. (Cited on pp. 84, 602.)
[276] N. E. Nörlund. Vorlesungen über Differenzenrechnung. Springer-Verlag, Berlin, 1924. Repub-
lished by Chelsea, New York, 1954. (Cited on p. 315.)
[277] W. Oettli and W. Prager. Compatibility of approximate solution of linear equations with given
error bounds for coefﬁcients and right-hand sides. Numer. Math., 6:404–409, 1964. (Cited on
p. 138.)
[278] James M. Ortega and Werner C. Rheinboldt. Iterative Solution of Nonlinear Equations in
Several Variables. Academic Press, New York, 1970. (Cited on p. 622.)
[279] A. M. Ostrowski. Solution of Equations in Euclidian and Banach Spaces.Academic Press, New
York, third edition, 1973. (Cited on pp. 615, 638, 639, 685.)
[280] Michael L. Overton. Numerical Computing with IEEE Floating Point Arithmetic. SIAM,
Philadelphia, PA, 2001. (Cited on p. 156.)
[281] H. Padé. Sur la représentation approcheée d’un fonction par des fraction rationelles. Thesis
Anal. Ecole Norm. Sup., 3:1–93, supplement, 1892. (Cited on p. 349.)
[282] VictorY. Pan. Solving a polynomial equation: Some history and recent progress. SIAM Review,
39:187–220, 1997. (Cited on p. 686.)
[283] S. K. Park and K. W. Miller. Random number generators: Good ones are hard to ﬁnd. Comm.
Assoc. Comput. Mach., 31:1192–1201, 1988. (Cited on p. 69.)
[284] T. W. Parks and J. J. McClellan. Chebyshev approximation for nonrecursive ﬁlters with linear
phase. IEEE Trans. Circuit Theory, 19:189–194, 1972. (Cited on p. 518.)
[285] Beresford N. Parlett. The new qd algorithm. Acta Numerica, 4:459–491, 1995.
(Cited on
p. 349.)
[286] Beresford N. Parlett and Christian Reinsch. Balancing a matrix for calculation of eigenvalues
and eigenvectors. Numer. Math., 13:293–304, 1969. (Cited on p. 675.)
[287] K. Pearson. On the criterion that a given system of deviations from the probable in the case of
a correlated system of variables is such that it can be reasonably supposed to have arisen from
random sampling. Phil. Mag. Series 5, 50:157–175, 1900. (Cited on p. 71.)
[288] R. Penrose. A generalized inverse for matrices. Proc. Cambridge Philos. Soc., 51:406–413,
1955. (Cited on p. 52.)

702
Bibliography
[289] O. Perron. Die Lehre von den Kettenbrüchen. Volume II. Teubner, Stuttgart, third edition, 1957.
(Cited on p. 326.)
[290] R.Piessens, E.deDoncker, C.W.Überhuber, andDavidK.Kahaner.QUADPACK,ASubroutine
Package for Automatic Integration. Springer-Verlag, Berlin, 1983. (Cited on p. 607.)
[291] Allan Pinkus.Weierstrass and approximation theory. J.Approx. Theory, 107:1–66, 2000. (Cited
on p. 449.)
[292] M. J. D. Powell. Approximation Theory and Methods. Cambridge University Press, Cambridge,
UK, 1981. (Cited on p. 430.)
[293] M. J. D. Powell.AReview of Methods for Multivariable Interpolation of Scattered Data Points.
In I. S. Duff and G. A. Watson, editors, The State of the Art in Numerical Analysis, pages 283–
309. Clarendon Press, Oxford, UK, 1997. (Cited on p. 518.)
[294] William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. Numerical
Recipes in Fortran 77. The Art of Scientiﬁc Computing. Cambridge University Press, Cam-
bridge, UK, second edition, 1992. (Cited on pp. 70, 77, 77, 83, 84, 85, 105, 219, 270, 349,
349, 481, 513, 515.)
[295] RAND Corporation. A Million Random Digits with 100,000 Normal Deviates. Free Press,
Glencoe, IL, 1955. (Cited on p. 67.)
[296] Anthony Ralston and Philip Rabinowitz. A First Course in Numerical Analysis. McGraw-Hill,
New York, second edition, 1978. Republished in 2001 by Dover, Mineola, NY.
(Cited on
pp. 85, 675.)
[297] L.B. Rall. Automatic Differentiation. Techniques and Applications. Number 120 in Lecture
Notes in Computer Science. Springer-Verlag, Berlin, 1981. (Cited on p. 349.)
[298] Lothar Reichel. On polynomial approximation in the uniform norm by the discrete least squares
method. BIT, 26(2):349–366, 1986. (Cited on p. 471.)
[299] Lothar Reichel. Newton interpolation at Leja points. BIT, 30(2):332–346, 1990.
(Cited on
pp. 366, 518.)
[300] Evgeny Yakolevich Remez. Sur le calcul effectif des polynômes d’approximation des
Tchebyscheff. C. R. Acad. Sci. Paris, 199:337–340, 1934. (Cited on p. 478.)
[301] John R. Rice. A theory of condition. SIAM J. Numer. Anal., 3(2):287–310, 1966.
(Cited on
p. 156.)
[302] Lewis F. Richardson. The approximate arithmetical solution by ﬁnite differences of physical
problems involving differential equations, with an application to the stresses in a masonry dam.
Proc. Roy. Soc. London Ser. A, 210:307–357, 1910. (Cited on p. 40.)
[303] Hans Riesel. Prime Numbers and Computer Methods for Factorization. Progr. Math. 126,
Birkhäuser, Boston, MA, second edition, 1994. (Cited on p. 349.)
[304] Friedrich Riesz and Béla Sz.-Nagy. Vorlesungen über Funktionalanalysis. VEB Deutscher
Verlag der Wissenschaften, Berlin, 1956. (Cited on p. 451.)
[305] J. L. Rigal and J. Gaches. On the compatibility of a given solution with the data of a linear
system. J. Assoc. Comput. Mach., 14(3):543–548, 1967. (Cited on p. 137.)
[306] Sara Robinson. Toward an optimal algorithm for matrix multiplication. SIAM News, 38(9),
Nov. 2005. (Cited on p. 28.)
[307] Sigfried M. Rump. Fast and parallel interval arithmetic. BIT, 39(3):534–554, 1999. (Cited on
pp. 150, 154.)

Bibliography
703
[308] Sigfried M. Rump. INTLAB—INTerval LABoratory. In T. Csendes, editor, Developments in
Reliable Computing, pages 77–104. Kluwer Academic Publishers, Dordrecht, 1999. (Cited on
p. 154.)
[309] C. Runge and H. König. Vorlesungen über Numerisches Rechnen. Band XI. Die Grundlehren der
Matematischen Wissenachaften. Verlag von Julius Springer, Berlin, 1924. (Cited on p. 519.)
[310] Heinz Rutishauser. Der Quotienten-Differenzen-Algoritmus. Z. Angew. Math. Phys., 5:233–
251, 1954. (Cited on p. 339.)
[311] Heinz Rutishauser. Solution of eigenvalue problems with the LR-transformation. Nat. Bureau
of Standards, Appl. Math. Ser., 49:47–81, 1958. (Cited on p. 349.)
[312] Heinz Rutishauser. Lectures on Numerical Mathematics. Volume I. Birkhäuser, Boston, 1990,
1990. English translation by W. Gautschi of Vorlesungen über numerische Mathematik,
Birkhäuser, Basel–Stuttgart, 1976. (Cited on pp. 85, 367.)
[313] Robert Schaback and HelmutWerner. Numerische Mathematik. Springer, Berlin, fourth edition,
1991. (Cited on p. 391.)
[314] I. J. Schoenberg. Contributions to the problem of approximation of equidistant data by analytic
functions. Quart. Appl. Math., 4:45–99 and 112–141, 1946. (Cited on pp. 418, 426.)
[315] I. J. Schoenberg andA. Whitney. On Pólya frequency functions III: The positivity of translation
determinants with an application to the interpolation problem by spline curves. Trans. Amer.
Math. Soc., 74:246–259, 1953. (Cited on p. 434.)
[316] Ernst Schröder. Über unendlich viele Algorithmen zur Auﬂösung der Gleichungen. Mathema-
tische Annalen, 2:317–365, 1870. (Cited on pp. 650, 685.)
[317] Ernst Schröder. On inﬁnitely many algorithms for solving equations (translated by G. W. Stew-
art). Tech. Report TR-92-121, Department of Computer Science, University of Maryland,
College Park, MD, 1992. (Cited on p. 685.)
[318] H.-R. Schwarz. Numerical Analysis: A Comprehensive Introduction. John Wiley, New York,
1989. English translation of Numerische Mathematik: Teubner, Stuttgart, 1986.
(Cited on
pp. 85, 369.)
[319] Hubert Schwetlick and Torsten Schütze. Least squares approximation by splines with free
knots. BIT, 35(3):361–384, 1995. (Cited on p. 436.)
[320] Bl. Sendov, A. Andreev, and N. Kjurkchiev. Numerical solution of polynomial equations. In
Philippe G. Ciarlet and Jacques-Louis Lions, editors, Handbook of Numerical Analysis, volume
III, pages 629–778. Elsevier Science, Cambridge, UK, 1994. (Cited on p. 685.)
[321] Lawrence F. Shampine. Discrete least squares polynomial ﬁts. Comm. Assoc. Comput. Mach.,
18:179–180, 1975. (Cited on p. 518.)
[322] D. Shanks. Nonlinear transformations of divergent and slowly convergent sequences. J. Math.
Phys., 34:1–42, 1955. (Cited on p. 337.)
[323] Avram Sidi.Auser-friendly extrapolation method for oscillatory inﬁnite integrals. Math. Comp.,
51:249–266, 1988. (Cited on p. 558.)
[324] R. C. Singleton. On computing the fast Fourier transform. Comm. Assoc. Comput. Mach.,
10:647–654, 1967. (Cited on p. 519.)
[325] R. C. Singleton. Algorithm 338: Algol procedure for the fast Fourier transform with arbitrary
factors. Comm. Assoc. Comput. Mach., 11:773–779, 1968. (Cited on p. 519.)
[326] Robert D. Skeel. Roundoff error and the patriot missile. SIAM News, 25(4):11, Jul. 1992. (Cited
on p. 94.)

704
Bibliography
[327] Ian H. Sloan and S. Joe. Lattice Methods for Multiple Integration. Clarendon Press, Oxford,
UK, 1994. (Cited on p. 607.)
[328] Ian H. Sloan and Henryk Wozniakowski. When are Quasi-Monte Carlo algorithms efﬁcient for
high dimensional integrals? J. Complexity, 14:1–33, 1998. (Cited on p. 607.)
[329] A. Smoktunowicz. Backward stability of Clenshaw’s algorithm. BIT, 42(3):600–610, 2002.
(Cited on p. 468.)
[330] J. F. Steffensen. Interpolation. Chelsea, NewYork, second edition, 1950. Republished by Dover
Publication, Mineola, NY. (Cited on pp. 349, 367, 518, 535.)
[331] Irene A. Stegun and Milton Abramowitz. Pitfalls in computation. J. Soc. Indust. Appl. Math.,
4:207–219, 1956. (Cited on p. 156.)
[332] Frank Stenger. Numerical Methods Based on Sinc and Analytic Functions. Springer-Verlag,
Berlin, 1993. (Cited on p. 171.)
[333] P. H. Sterbenz. Floating Point Computation. Prentice-Hall, Englewood Cliffs, NJ, 1974. (Cited
on pp. 109, 156.)
[334] Hans J. Stetter. Numerical Polynomial Algebra. SIAM, Philadelphia, 2004. (Cited on p. 686.)
[335] George W. Stewart. Matrix Algorithms Volume I: Basic Decompositions. SIAM, Philadelphia,
PA, 1998. (Cited on p. 27.)
[336] E. Stiefel. Altes und Neues über numerische Quadratur. Z. Angew. Math. Mech., 41:408–413,
1961. (Cited on p. 548.)
[337] S. M. Stigler. Gauss and the invention of least squares. Ann. Statist., 9:465–474, 1981. (Cited
on p. 84.)
[338] Joseph Stoer and Roland Bulirsch. Introduction to Numerical Analysis. Springer-Verlag, New
York, third edition, 2002. (Cited on pp. 85, 394, 518, 531.)
[339] Gilbert Strang. Introduction to Applied Mathematics. Wellesley-Cambridge Press, Wellesley,
MA, 1986. (Cited on pp. 217, 256, 497, 497, 502.)
[340] Gilbert Strang. The discrete cosine transform. SIAM Rev., 41(1):135–147, 1999.
(Cited on
p. 514.)
[341] Volker Strassen. Gaussian elimination is not optimal. Numer. Math., 13:354–356, 1969. (Cited
on p. 28.)
[342] A. H. Stroud. Approximate Calculation of Multiple Integrals. Prentice-Hall, Englewood Cliffs,
NJ, 1972. (Cited on p. 607.)
[343] A. H. Stroud and D. Secrest. Gaussian Quadrature Formulas. Prentice-Hall, Englewood Cliffs,
NJ, 1966. (Cited on p. 607.)
[344] Endre Süli and David F. Mayers. Introduction to Numerical Analysis. Cambridge University
Press, Cambridge, UK, 2003. (Cited on p. 85.)
[345] Paul N. Swarztrauber. On computing the points and weights for Gauss–Legendre quadrature.
SIAM J. Sci. Comput., 24:945–954, 2002. (Cited on p. 584.)
[346] J. J. Sylvester. On a theory of the syzygetic relations of two rational integral functions, compris-
ing an application to the theory of Sturm’s functions, and that of the greatest algebraic common
measure. Philos. Trans. Roy. Soc. London, 143:407–548, 1853. (Cited on p. 681.)
[347] Gabor Szeg˝o. Orthogonal Polynomials. Colloq. Publ., Volume 23. Amer. Math. Soc., Provi-
dence, RI, fourth edition, 1975. (Cited on p. 407.)

Bibliography
705
[348] Eihan Tadmor. Filters, molliﬁers and the computation of the Gibbs phenomenon. Acta Numer.,
16:305–378, 2007. (Cited on p. 519.)
[349] Nico M. Temme. Numerical aspects of special functions. Acta Numerica, 16:379–478, 2007.
(Cited on p. 348.)
[350] T. N. Thiele. Interpolationsrechnung. B. G. Teubner, Leipzig, 1909. (Cited on p. 393.)
[351] E. C. Titchmarsh. The Theory of Functions. Oxford University Press, London, second edition,
1939. (Cited on pp. 172, 193, 450.)
[352] John Todd. Notes on numerical analysis I, solution of differential equations by recurrence
relations. Math. Tables Aids Comput., 4:39–44, 1950. (Cited on p. 257.)
[353] Kim-Chuan Toh and Lloyd N. Trefethen. Pseudozeros of polynomials and pseudospectra of
companion matrices. Numer. Math., 68(3):403–425, 1994. (Cited on pp. 665, 675.)
[354] J. F. Traub. Iterative Methods for the Solution of Equations. Prentice-Hall, Englewood Cliffs,
NJ, 1964. Republished in 1997 by Amer. Math. Soc., Providence, RI. (Cited on pp. 632, 648,
649, 650, 685, 685.)
[355] J. F. Traub. Associated polynomials and uniform methods for the solution of linear problems.
SIAM Rev., 8(3):277–301, 1966. (Cited on p. 518.)
[356] J. F. Traub. A class of globally convergent iteration functions for the solution of polynomial
equations. Math. Comp., 20:113–138, 1966. (Cited on p. 685.)
[357] Lloyd N. Trefethen. The deﬁnition of numerical nalysis. SIAM News, 25, Nov. 1992. (Cited
on p. 87.)
[358] Lloyd N. Trefethen. Pseudospectra of linear operators. SIAM Rev., 39:383–406, 1997. (Cited
on p. 665.)
[359] Lloyd N. Trefethen. Is Gauss quadrature better than Clenshaw–Curtis? SIAM Rev., 50:67–87,
2008. (Cited on pp. 540, 606.)
[360] Lloyd N. Trefethen and Robert S. Schreiber. Average-case stability of Gaussian elimination.
SIAM J. Matrix Anal. Appl., 11:335–360, 1990. (Cited on p. 37.)
[361] W. Tucker.Arigorous ODE solver and Smale’s 14th problem. Found. Comput. Math., 2(1):53–
117, 2002. (Cited on p. 147.)
[362] A. M.Turing. Rounding-off errors in matrix processes. Quart. J. Mech.Appl. Math., 1:287–308,
1948. (Cited on p. 84.)
[363] E. Tyrtychnikov. A Brief Introduction to Numerical Analysis. Birkhäuser, Boston, 1997. (Cited
on p. 85.)
[364] Christoph W. Ueberhuber. Numerical Computation: Methods, Software, and Analysis. Volumes
1 & 2. Springer-Verlag, Berlin, 1997. (Cited on p. 607.)
[365] J. G. van der Corput. Verteilungsfunktionen I & II. Nederl. Akad. Wetensch. Proc., 38:813–820,
1058–1066, 1935. (Cited on p. 602.)
[366] Charles F. Van Loan. Computational Framework for the Fast Fourier Transform. SIAM,
Philadelphia, 1992. (Cited on pp. 507, 508, 513, 513, 516, 519.)
[367] Charles F. Van Loan. Introduction to Scientiﬁc Computing. Prentice-Hall, Upper Saddle River,
NJ, second edition, 2000. (Cited on p. 85.)
[368] Jürg Waldvogel. Fast construction of thr Fejér and Clenshaw–Curtis quadrature rules. BIT,
46:195–202, 2006. (Cited on p. 541.)

706
Bibliography
[369] H. S. Wall. Analytic Theory of Continued Fractions. Van Nostrand, Princeton, NJ, 1948. (Cited
on p. 326.)
[370] Eric W. Weisstein, editor. CRC Concise Encyclopedia of Mathematics. CRC Press, Boca Raton,
FL, 1999. (Cited on pp. 84, 642.)
[371] Wilhelm Werner. Polynomial interpolation:
Lagrange versus Newton. Math. Comp.,
43(167):205–217, 1984. (Cited on p. 518.)
[372] Hermann Weyl. Randbemerkungen zu Hauptproblemen der Mathematik, II, Fundamentalsatz
der Algebra und Grundlagen der Mathematik. Math. Z., 20:131–151, 1924. (Cited on pp. 679,
686.)
[373] D. V. Widder. The Laplace Transform. Princeton University Press, Princeton, NJ, 1941. (Cited
on p. 286.)
[374] D. V. Widder. An Introduction to Transform Theory. Academic Press, New York, 1971. (Cited
on p. 286.)
[375] James H. Wilkinson. Error analysis of direct methods of matrix inversion. J. Assos. Comput.
Mach., 8:281–330, 1961. (Cited on p. 37.)
[376] JamesH.Wilkinson.RoundingErrorsinAlgebraicProcesses.NotesonAppliedScienceNo.32.
Her Majesty’s Stationery Ofﬁce, London, UK, 1963. Republished in 1994 by Dover, Mineola,
NY. (Cited on pp. 117, 156, 156, 156.)
[377] James H. Wilkinson. The Algebraic Eigenvalue Problem. Clarendon Press, Oxford, 1965.
(Cited on pp. 114, 141.)
[378] James H.Wilkinson.Apriori error analysis of algebraic processes. In Proceedings International
Congress Math., pages 629–639. Izdat. Mir, Moscow, 1968. (Cited on p. 139.)
[379] James H Wilkinson. The perﬁdious polynomial. In G. H. Golub, editor, Studies in Numerical
Analysis, pages 1–28. American Mathematical Society, Providence, RI, 1984.
(Cited on
pp. 665, 672.)
[380] James H. Wilkinson. Error analysis revisited. IMA Bull., 22:192–200, 1986. (Cited on pp. 145,
156.)
[381] James H. Wilkinson and C. Reinsch, editors. Handbook for Automatic Computation. Vol. II.
Linear Algebra. Springer-Verlag, New York, 1971. (Cited on pp. 38, 42.)
[382] Stephen Wolfram. The Mathematica Book. Wolfram Media, Champaign, IL, ﬁfth edition, 2003.
(Cited on p. 105.)
[383] Henryk Wozniakowski.Average case complexity of multivariate integration. Bull. Amer. Math.
Soc., 24(1):185–194, 1991. (Cited on p. 603.)
[384] Peter Wynn. On a device for computing the em(sn) transformation. Math. Tables Aids Comput.,
10:91–96, 1956. (Cited on p. 338.)
[385] Peter Wynn. On a Procrustean technique for the numerical transformation of slowly convergent
sequences and series. Proc. Cambridge Phil. Soc., 52:663–671, 1956. (Cited on p. 518.)
[386] Peter Wynn. On the convergence and stability of the epsilon algorithm. J. SIAM Numer. Anal.,
3(1):91–122, 1966. (Cited on p. 338.)
[387] L. Yao and Adi Ben-Israel. The Newton and Halley methods for complex roots. Amer. Math.
Monthly, 105:806–818, 1998. (Cited on pp. 644, 645.)
[388] Tjalling J.Ypma. Historical development of the Newton–Raphson method. SIAM Rev., 37:531–
551, 1995. (Cited on p. 685.)
[389] M. Zelen. Linear estimation and related topics. In John Todd, editor, Survey of Numerical
Analysis, pages 558–584. McGraw-Hill, New York, 1962. (Cited on p. 47.)

Index
ϵ-algorithm, see epsilon algorithm
ρ-algorithm, 394
absolute error, 90
accuracy, 91
automatic control of, 145–146
Adams, 265
Adams–Bashforth’s formula, 265
Adams–Moulton’s formula, 265
adjoint operator, 453
Aitken, 273
Aitken acceleration, 272–278, 311, 621
a-version, 274
active, 621
iterated, 274
Aitken interpolation, 371–373
algebraic equations, 662–682
algorithm, 126
adaptive Simpson, 562
back-substitution, 30
bisection, 613
divide-and-conquer, 123
Euclidean norm, 119
Euler’s transformation, 282
golden section search, 658
LU factorization, 33
mathematically equivalent, 111
numerically equivalent, 111
Richardson extrapolation, 304
unstable, 137
Vandermonde inverse, 371
Vandermonde system, 375
aliasing, 217, 498
alternating series, 161
analytic continuation, 288
antithetic sequence, 79
approximation
uniform, 471
area coordinates, 595
arithmetic
circular, 150
complex, 111
ﬁxed-point, 96
ﬂoating-point, 107–112
interval, 147–153
multiple precision, 104–105
standard model, 107
unnormalized ﬂoating-point, 145
arrowhead system, 424
asymptotic
error estimate, 233–235
series, 214
attenuation factors, 490
B-spline, 426–436
basis, 430
deﬁnition, 429
evaluation, 433
exterior knots, 427
hat function, 427
multiple knots, 431
properties, 429
recurrence relation, 431
Babbage difference engine, 245
back-substitution, 29
backward
stability, 138
backward differentiation formula (BDF), 236
backward error
analysis, 113
componentwise, 137
normwise, 137
707

708
Index
Banach, 442
Banach space, 442, 620
band matrix, 34, 252
band-limited function, 497
barycentric
coordinates, 595
Lagrange interpolation, 367
rational interpolation, 394
basic linear algebra subprogram (BLAS), 42,
115
Bauer–Skeel condition number, 136
BDF (backward differentiation formula), 236
bell sum, 208, 209, 218, 268, 319
Bellman, 588
Bernštein, 287
Bernštein polynomial, 411–413
derivative, 414
Bernštein’s approximation theorem, 202
Bernoulli, 168
function, 296
numbers, 168–169, 186, 293, 299
polynomial, 296, 309
Bernoulli’s method, 669, 683
Bessel function, 218, 219, 270
modiﬁed, 211
Bessel’s inequality, 455
Bézier
control points, 413
curve, 413–417
polygon, 413
Bickley’s table, 231, 232
bilinear interpolation, 409
binary
number system, 94
point, 94
system, 94
bisection method, 610–614
Björck–Pereyra’s algorithm, 375
BLAS (basic linear algebra subprogram), 42,
115
boundary value problem, 217
bounded variation, 193, 285
branch cut, 104, 328, 385
Buffon, 78
butterﬂy relations, 505
CAD (computer aided design), 410
cancellation, 14
of errors, 146
of terms, 120–122
Cardano–Tartaglia formula, 683
cardinal basis, 355
Cauchy, 191
product, 164
sequence, 440
Cauchy–FFT (fast Fourier transform) method,
193–198, 537
Cauchy–Schwarz inequality, 451
ceiling of number, 21
centered difference, 12
characteristic equation (polynomial)
of difference equation, 252
Chebyshev, 198
approximation, 474
expansion, 198–203
interpolation, 358, 370, 377, 379, 387,
467
points, 199, 357, 378
polynomial, 198–203, 460
minimax property, 200
support coefﬁcients, 370
system, 355, 474
Chebyshev’s
equioscillation theorem, 473
chi-square distribution, 77
Cholesky, 38
Cholesky factorization, 38, 139, 578
Christoffel, 568
Christoffel–Darboux formula, 458
circulant matrix, 509
circular arithmetic, 150
Clenshaw’s algorithm, 467, 468, 480
Clenshaw–Curtis quadrature, 539–541
c.m., see completely monotonic
companion matrix, 579, 667
compensated summation, 115–116
complete space, 440
completely monotonic (c.m.), 284–292
criteria, 290
function, 294
complex
analysis, 385–407
arithmetic, 111
composite
midpoint rule, 528
Simpson’s rule, 531
trapezoidal rule, 528
computer aided design (CAD), 410

Index
709
condition number
Bauer–Skeel, 136
of matrix, 132
of problem, 126–133
continued fraction, 321–329, 391
contraction mapping, 619
contraction mapping theorem, 619
convergence
acceleration, 271–308
cubic, 648
linear, 622
order of, 622
sublinear, 622, 625
superlinear, 622
conversion
between number systems, 95
convex
hull, 383
set, 383
convolution, 164, 496
discrete, 509
coordinates
barycentric, 595
correct decimals, 91
correlation, 502
Cotes, 527
covariance matrix, 46
cubic convergence
methods of, 647–650
cubic spline
complete interpolant, 421
interpolation, 267
natural interpolant, 422
not-a-knot condition, 422
periodic, 423, 424
tridiagonal system, 420
curse of dimensionality, 599
cut (in the complex plane), 192
d.c.m. (difference between two completely
monotonic sequences), 284–292
DCT-1 (discrete cosine transform), 512
de Casteljau’s algorithm, 415
deﬂation, 18, 671–672
delta function, 501
density function, 45
Descartes, 664
Descartes’ rule of sign, 664
determinant, 34
determinant identity
Sylvester’s, 341
DFT (discrete Fourier transform), 194, 489
DFT matrix, 504
diagonally dominant, 423
difference
approximation, 11–15
centered, 12
checks, 222
equation, 20
frozen coefﬁcients, 259
linear, 251–256
nonhomogeneous, 255
of product, 225
operator, 220–256
backward, 221
forward, 220
scheme, 14, 221
difference between two completely mono-
tonic sequences (d.c.m.), 284
differential equation, 23
differentiation
algorithmic, 174
automatic, 174
formula
backward, 236
forward, 237
higher derivatives, 246
numerical, 235, 245–248
of matrices, 136
symbolic, 174
discrete
cosine transform (DCT-1), 512
distributions, 73
Fourier transform (DFT), 194, 489
sine transform (DST-1), 512
discretization error, 12
distance, 440
distribution function, 45
divergent series, 212–214
divide and conquer strategy, 20–22
divided difference, 359
inverse, 392
reciprocal, 392
table, 361
domain of uncertainty, 616
dominant root, 669
double precision
simulating, 115
double rounding, 98
drafting spline, 417
DST-1 (discrete sine transform), 512

710
Index
efﬁciency index, 623, 631
elementary
functions, 102–104
symmetric functions, 663
epsilon algorithm, 278, 336–339, 553, 558
equivalence transformation, 323
error
absolute, 90
analysis, 137–142
backward, 137
forward, 137
running, 145
bound, 90, 128
bounds
a posteriori, 138
estimate, 90
asymptotic, 250
function, 164, 210, 218
human, 88
maximal, 130
propagation, 127–154
general formula, 130
random, 87
relative, 90
rounding, 88, 113–115
sources of, 87–90
standard, 117, 131
systematic, 87
truncation, 88
Euclidean
algorithm, 25, 677, 680
norm, 119, 442
Euclidean norm, 442
Euler, 55, 293
Euler numbers, 169, 293, 320
Euler’s
constant, 315
formulas, 483
function, 212
iteration method, 648
method, 56, 57, 306
transformation, 212, 278–284
generalized, 279
optimal, 288
Euler–Maclaurin’s formula, 292–302, 548,
550
experimental perturbations, 146
exponent, 96
overﬂow, 118
underﬂow, 118
exponential
distribution, 75
integral, 329
false-position method, 626–628
fast Fourier transform (FFT), 194, 248, 267,
503–516
Cooley–Tukey, 508
Gentleman–Sande, 508
Fejér’s quadrature rules, 538
FEM (ﬁnite element method), 594
FFT (fast Fourier transform), 194
Fibonacci sequence, 267, 311
Filon’s formula, 555
ﬁnite element method, 594
ﬁve-point operator, 14
ﬁxed-point iteration, 2, 618–621
with memory, 628
ﬂoating-point
number, 96
representation, 96
standard arithmetic, 99–102
ﬂoor of number, 21
forward
difference, 12
stability, 138
substitution, 29
Fourier, 482
analysis
continuous case, 485
discrete case, 488
coefﬁcients, 454, 484
matrix, 504
series, 191–193, 482
transform, 548
fractal curve, 684
Fredholm integral equation, 53
frozen coefﬁcients, 270
function
aliased, 498
analytic, 385–407
band-limited, 497
spaces, 441
functionals, 227
fundamental subspaces, 47, 52
fundamental theorem of algebra, 662
fused multiply–add, 108
gamma function, 164, 206, 301, 316
incomplete, 77, 164, 328, 348

Index
711
Gauss, 29
Gauss quadrature, 565–584
Hermite, 573
Jacobi, 572
Laguerre, 572
Legendre, 571
remainder in, 570
Gauss–Kronrod quadrature, 575
Gauss–Markov theorem, 46
Gaussian elimination, 30–38
GCA (Gustafson’s Chebyshev acceleration),
292
generating function, 256
geometric series, 165
comparison with, 158
Gibbs’ phenomenon, 487
global convergence, 9
golden section
ratio, 658
search, 658
gradual underﬂow, 101
Graeffe’s method, 668
Gram, 464
matrix, 577
polynomials, 464
greatest common divisor, 680
Green’s function, 268
Gregory, 547
Gregory’s quadrature formula, 547
grid
irregular triangular, 594–599
rectangular, 591
guard digits, 101
Gustafson’s Chebyshev acceleration (GCA),
292
Haar, 474
Haar condition, 474
Halley, 648
Halley’s method, 648
Halton sequences, 603
Hankel, 337
determinant, 340
matrix, 337, 339–345, 568, 577
harmonic points, 357
Hausdorff, 287
Heaviside, 238
Hermite, 381
interpolation, 381–385, 388, 570
polynomials, 464, 573, 585
Heron’s
formula, 124
rule, 4
Hessenberg matrix, 44, 651
Hilbert
matrix, 135, 334, 577
space, 451
Horner’s rule, 17
hypergeometric function, 167, 327
idempotent operator, 452
IEEE 754 standard, 99–102
ill-conditioned
multiple roots, 616
polynomial roots, 665–666
problem, 133
series, 206–212
ill-posed problem, 53
Illinois method, 636
importance sampling, 81, 601
inner product
accurate, 114
error analysis, 114
space, 450–453
input data, 126
integral
algebraic singularity, 552
inﬁnite interval, 533
singular, 525–527
integral domain, 182
integration by parts, 525
repeated, 193, 214
intermediate-value theorem, 610
interpolation
Birkhoff, 382
broken line, 419
condition number, 356
error in linear, 363
formulas, 242
Hermite, 381–385
inverse, 366–367
iterative linear, 371–373
lacunary, 382
Lagrange, 355
barycentric form, 368
linear, 10
Newton, 359
osculatory, 381–385
piecewise cubic, 420
polynomial, 385–389

712
Index
rational, 389–395
remainder term, 361
trigonometric, 488
two variables, 396–398
with derivatives, 381–385
interpolatory quadrature formula, 522
interval
reduction method, 657
interval arithmetic, 147–153
complex, 150
inclusion monotonic operations, 149
Newton’s method, 646–647
operational deﬁnitions, 148
interval representation
inﬁmum-supremum, 147
midpoint-radius, 149–150
INTLAB, 154
inverse
divided difference, 392
interpolation, 366–367, 634
irregular errors, 223
iteration
ﬁxed-point, 2, 618–621
iteration method
Euler’s, 648
Halley’s, 648, 654
Laguerre’s, 670–671
Newton’s, 637–645
Newton–Maehly’s, 672
Newton–Raphson’s, 637
Schröder’s, 650
Jacobi, 463
Jacobi’s
identity, 340
matrix, 580
polynomials, 463, 572
Jacobian matrix, 8, 132
Julia set, 684
Kahan, 99
kernel polynomial, 458
Klein, 324
Kronrod quadrature, 575
Kummer, 168
Kummer’s
ﬁrst identity, 189, 209
hypergeometric function, 168
Lagrange, 163
Lagrange’s
barycentric interpolation, 367–371
interpolation, 355
two variables, 396
polynomial, 354
generalized, 388
remainder formula, 163
Laguerre, 670
Laguerre’s
method, 670–671
polynomials, 463, 572
Lambert, 190
Lambert’s W-function, 559, 642, 653
Lanczos σ-factor, 487
Laplace, 14
Laplace’s equation, 14
Laplace’s transform, 256, 285
Laplace–Stieltjes transform, 285
Laurent series, 191–193, 202
least squares, 355
approximation, 355
characterization of solution, 46–50
data ﬁtting, 466
general problem, 51
principle of, 46–47
problem, 45
statistical aspects, 469–471
Lebesgue, 192
Lebesgue constant, 356, 369
Legendre polynomials, 462, 571
Leibniz, 5, 431
Leibniz’ formula, 431
Leja ordering, 366
limiting accuracy
multiple root, 616
simple root, 616
Lin–Segel’s balancing procedure, 204
line search, 657–661
linear
approximation, 441
difference equation, 251–256
functional, 227
interpolation, 10
operator, 226
space, 441
linear congruential generator, 69
linear interpolation
on triangular grid, 596

Index
713
linear system
overdetermined, 45, 355
linearization, 7
logarithmic potential, 399
low discrepancy sequences, 601
LU factorization, 32–34, 139
Möbius, 595
machine epsilon, 97
Maclaurin, 162
magnitude of interval, 147, 153
Mandelbrot set, 684
mantissa, 96
matrix
companion, 667
diagonally dominant, 423
Hankel, 337
Hessenberg, 44, 651
ill-conditioned, 135
nilpotent, 175
positive deﬁnite, 48
semicirculant, 175
shift, 175
Toeplitz, 175, 651
totally nonnegative, 434
tridiagonal, 34
matrix multiplication, 26–28
error bound, 115
maximal error, 130
maximum modulus, 164, 194
theorem, 197
maximum norm, 442
Maxwell, 311
Mersenne twister, 70
method
of normal equations, 49
method of undetermined coefﬁcients, 565
metric space, 440
mignitude
of interval, 147
Miller’s formula, 187
minimax property, 200
minimization
one-dimensional, 657–661
mixed congruential method, 69
modulus of continuity, 449
moment, 522
modiﬁed, 576
sequence, 286
Monte Carlo method, 64–83
Moore–Penrose inverse, 52
Muller–Traub’s method, 633
Mulprec, 105
multidimensional
integration, 587–601
interpolation, 395–398
multiple recursive generator, 69
multiplicity
of interpolation point, 382
of root, 616
multisection method, 614
multivalued function, 166, 173, 192
Neville’s algorithm, 306, 371–373
Newton, 5
Newton polynomials, 353
Newton’s formulas, 663
Newton’s interpolation formula, 359
constant step size, 243
two variables, 396
Newton’s method, 5, 637–645
complex case, 644
convergence of, 638–645
in several dimensions, 7–9
interval, 152, 646–647
modiﬁed, 675
Newton–Cotes’
9-point formula, 546
quadrature rule, 533–541, 552
Newton–Maehly’s method, 672
Newton–Raphson’s method, see Newton’s
method
node polynomial, 524, 566, 567
nonlinear spline, 417
norm, 441
Lp, 443
lp, 443
of matrix, 131
of operator, 444–445
of vector, 131
norm and distance formula, 445–448
normal
deviates, 75
distribution function, 75, 329
equations, 47, 454
normal equations
method of, 49
null space
numerical, 52
of matrix, 47

714
Index
number system
binary, 94
ﬂoating-point, 96
hexadecimal, 94
octal, 94
position, 93–94
numerical
cubature, 587
differentiation, 235, 245–248
instability, 19
method, 127
null space, 52
problem, 126
rank, 52
simulation, 55, 56
Numerov, 258
Numerov’s method, 258, 306, 317
Nyquist’s critical frequency, 498
Oettli–Prager error bounds, 138
operation count, 28
operator
adjoint, 453
averaging, 226
calculus of, 225–250
central difference, 226
commutative, 227
differentiation, 226
expansions, 220–256
linear, 226
norm, 444–445
positive deﬁnite, 453
self-adjoint, 453
order of accuracy, 522
orthogonal
coefﬁcients, 454
expansion, 455
function, 452
polynomials, 457–466, 565–584
and Padé table, 334
construction, 459
projection, 49, 52
projector, 50, 55
system, 450–453
orthonormal system, 452
oscillating integrand, 554–560
osculating polynomial, 382
osculatory interpolation, 381–385
output data, 126
Padé, 329
Padé table, 329–336
of exponential function, 330
parametric
curve, 411
spline, 424
Parseval’s identity, 455, 485
partial pivoting, 36
Pascal matrix, 24, 263
Peano, 238
Peano kernel, 237, 429, 430
Penrose’s conditions, 52
permutation
bit-reversal, 506, 507
matrix, 35
perfect shufﬂe, 507
perturbation
componentwise bound, 135
expansion, 203–205
regular, 203
experimental, 146
of linear systems, 134–136
singular, 204
pivot element, 31
point of
attraction, 3, 618
repulsion, 3, 618
Poisson, 14
Poisson’s
distribution, 208, 329
equation, 14
process, 77
summation formula, 272, 532, 548
polar algorithm, 75
polynomial
reciprocal, 663, 672
shift of origin, 664
position system, 93–94
positive deﬁnite, 37
matrix, 48
power
basis, 352
truncated, 427
method, 670
power series, 162–184
composite, 173
composite function, 179
division, 168

Index
715
expansion, 22–23
inverse function, 179
reversion, 179
precision, 91
double, 99
multiple, 104–105
single, 99
problem
ill-conditioned, 133
well-conditioned, 133
projection
orthogonal, 49, 52
projection operator, 452
pseudoinverse, 52
pseudoinverse solution, 50, 52
pseudorandom numbers, 66–77
Pythagoras’ theorem, 452
Pythagorean sum, 118
qd algorithm, 339–345, 673
qd scheme, 339
quadratic interpolation
on triangular grid, 596
quadrature
adaptive rule, 560–563
closed rule, 529
Gauss–Christoffel, 565–584
Gauss–Lobatto, 573
Gauss–Radau, 574
midpoint rule, 528
Monte Carlo methods, 599–601
Newton–Cotes’ rule, 533–541
open rule, 529
product rule, 590–594
Simpson’s rule, 530–531
successive one-dimensional, 589–590
trapezoidal rule, 527
quadtree algorithm, 679
quasi–Monte Carlo methods, 601–604
quicksort, 21
radical inverse function, 602
radix, 96
random
normal deviates, 75
number, 66–77
antithetic sequence of, 79
generator, 68
uniformly distributed, 67
variable, 45
mean, 46
variance, 46
random number generator (RNG), 69
range of matrix, 47
range reduction, 102
rational interpolation, 389–395
Neville-type, 394
reciprocity relations, 498
rectangle-wedge-tail method, 77
rectangular grid, 14
rectangular wave, 486
recurrence
backward, 252
forward, 252
relation, 17–20
reduction of variance, 77–81
regula falsi, see false-position method
rejection method, 77
relative error, 90
remainder term
in series, 161
interpolation, 361
Remez algorithm, 478
repeated averaging, 278
residual vector, 45
resultant, 681
of polynomials, 682
rhombus rules, 340
Richardson, 10
Richardson’s
correction, 303
extrapolation, 10, 59, 302–308, 548
repeated, 302
iteration, 40
Riemann, 192
Riemann–Lebesgue theorem, 192
RNG (random number generator), 69
Rodrigues’ formula, 458, 462
Romberg, 548
Romberg’s method, 11, 302, 548–552
error bound, 549
root condition, 256
rounding, 92
chopping, 92
error, 88
Runge, 59, 377

716
Index
Runge’s
phenomenon, 377–379, 398–407
second order method, 60, 306
Rutishauser, 339
sampling theorem, 497
scalar of operator, 230
scale factors (ﬁxed-point), 96
scaling and squaring, 170
Scheutz, 245
Schoenberg–Whitney condition, 434
Schröder methods, 650
Scylla and Charybdis, 195, 197, 250
secant method, 7, 267, 628–631
modiﬁed, 636
rate of convergence, 630
waltz rhythm, 631
seed, 251
Seidel, 325
Seidel’s theorem, 325
self-adjoint operator, 453
semicirculant matrix, 175
semiconvergent series, 212–214
seminorm, 451
semiseparable matrix, 268
series
alternating, 161
asymptotic, 214
convergence acceleration, 271–308
divergent, 212–214
geometric, 165
ill-conditioned, 206–212
semiconvergent, 212–214, 218
tail of, 158
Taylor’s, 162
with positive terms, 212, 271, 292
Shanks’ sequence transformation, 337
Shannon’s sampling theorem, 497
shift
matrix, 175, 579
operator, 220
Sievert, 587
sign, 160
signiﬁcant digits, 91
simple root, 615
Simpson, 530
Simpson’s rule, 530, 542
with end correction, 543
single precision, 99
singular
value, 50
value decomposition (SVD), 50–54
vector, 50
smoothing, 355
sparse matrix, 38
spectral analysis, 483
spline
best approximation property, 423
function, 418–436
deﬁnition, 418
interpolation, 417–434
closed curves, 425
least squares, 434–436
parametric, 424
truncated power basis, 427
splitting technique, 81
square root
Heron’s rule, 4
Schröder’s method, 650
square wave
Fourier expansion, 486
stability of algorithm, 137–142
standard deviation, 46
standard error, 117, 131
Steffensen’s method, 621, 631
Stieltjes, 285, 329
Stieltjes’
integral, 285
procedure, 468
Stirling, 243
Stirling’s formula, 206, 300
Sturm, 677
Sturm sequence, 677–680
subdistributivity, 148
sublinear convergence, 622, 625
subtabulation, 265
successive approximation, 2
summation
algorithms, 271–308
by parts, 225
repeated, 262
compensated, 115–116
superlinear convergence, 622
superposition principle, 222
support coefﬁcients, 367
SVD (singular value decomposition), 50–54
Sylvester’s
determinant identity, 341
matrix, 682

Index
717
synthetic division, 18, 663
with quadratic factor, 664
Szeg˝o polynomials, 466
tablemaker’s dilemma, 92
tail of a series, 158
Taylor, 162
Taylor’s
formula
integral form of remainder, 162
series, 162
symbolic form of, 229
termination criteria, 158, 617–618
Thiele’s reciprocal differences, 393
thinned sequence, 275
thinning, 275–284
geometric, 276
three-term recurrence, 467, 468
titanium data, 435
Todd, 257
Toeplitz, 175
Toeplitz matrix, 140, 175, 509, 651
method, 175
total differential, 130
totally positive matrix, 376
transform
z, 256
Fourier, 194, 548
Laplace, 256
Laplace–Stieltjes’, 285
translation operator, 220
transposition matrix, 36
trapezoidal rule, 9
composite, 528
error in, 302
superconvergence, 531–533
triangle
family of polynomials, 352–353
inequality, 451, 452
triangular
grid
interpolation on, 596
reﬁnement of, 594
systems of equations, 28–30
tridiagonal
matrix, 252
system
algorithm for, 34
trigonometric
interpolation, 488
polynomial, 483
truncation error, 88, 158
global, 528
local, 528
Ulam, 65
ulp (unit in last place), 98
unattainable point, 390
unbiased estimator, 46
uncorrelated random variables, 46
uniform approximation, 471
uniform convergence, 443
unimodal function, 657
unit in last place (ulp), 98
unit roundoff, 98
unitary operator, 480
Van der Corput sequence, 602
Vandermonde, 352
determinant, 228, 261
-like systems, 377
matrix, 228, 352, 568
complex, 504
conﬂuent, 382
inverse, 370
systems, 373–377
variance, 46
reduction of, 77–81
vector
conjugate even, 511
conjugate odd, 511
space, 441
von Neumann, 65
Wallis, 653
Weierstrass, 449
Weierstrass’
approximation theorem, 449
method, 673
weight function, 522
weighted quadrature rule, 522
well-conditioned problem, 133
Wiberg, 245
Wilkinson, 37
wobbling, 97
word-length, 95
wrapping effect, 153
z-transform, 256
zero suppression, 672
ZEROIN algorithm, 635
ziggurat algorithm, 76


Online Appendix A
Introduction to Matrix
Computations
A.1
Vectors and Matrices
A.1.1
Linear Vector Spaces
In this appendix we recall basic elements of ﬁnite-dimensional linear vector spaces and
related matrix algebra, and introduce some notations to be used in the book. The exposition
is brief and meant as a convenient reference.
We will be concerned with the vector spaces Rn and Cn, that is, the set of real or
complex n-tuples with 1 ≤n < ∞. Let v1, v2, . . . , vk be vectors and α1, α2, . . . , αk
be scalars. The vectors are said to be linearly independent if none of them is a linear
combination of the others, that is,
k

i=1
αivi = 0 ⇒αi = 0,
i = 1 : k.
Otherwise, if a nontrivial linear combination of v1, . . . , vk is zero, the vectors are said to be
linearly dependent. Then at least one vector vi will be a linear combination of the rest.
A basis in a vector space V is a set of linearly independent vectors v1, v2, . . . , vn ∈V
such that all vectors v ∈V can be expressed as a linear combination:
v =
n

i=1
ξivi.
The scalars ξi are called the components or coordinates of v with respect to the basis {vi}.
If the vector space V has a basis of n vectors, then every system of linearly independent
vectors of V has at most k elements and any other basis of V has the same number k of
elements. The number k is called the dimension of V and denoted by dim(V).
The linear space of column vectors x = (x1, x2, . . . , xn)T , where xi ∈R is denoted
Rn; if xi ∈C, then it is denoted Cn. The dimension of this space is n, and the unit vectors
e1, e2, . . . , en, where
e1 = (1, 0, . . . , 0)T ,
e2 = (0, 1, . . . , 0)T , . . . , en = (0, 0, . . . , 1)T ,
A-1

A-2
Online Appendix A. Introduction to Matrix Computations
constitute the standard basis. Note that the components x1, x2, . . . , xn are the coordinates
when the vector x is expressed as a linear combination of the standard basis. We shall use
the same name for a vector as for its coordinate representation by a column vector with
respect to the standard basis.
AnarbitrarybasiscanbecharacterizedbythenonsingularmatrixV = (v1, v2, . . . , vn)
composed of the basis vectors. The coordinate transformation reads x = V ξ. The standard
basis itself is characterized by the unit matrix
I = (e1, e2, . . . , en).
If W ⊂V is a vector space, then W is called a vector subspace of V. The set of all
linear combinations of v1, . . . , vk ∈V form a vector subspace denoted by
span {v1, . . . , vk} =
k

i=1
αivi,
i = 1 : k,
where αi are real or complex scalars. If S1, . . . , Sk are vector subspaces of V, then their
sum deﬁned by
S = {v1 + · · · + vk| vi ∈Si, i = 1 : k}
is also a vector subspace. The intersection T of a set of vector subspaces is also a subspace,
T = S1 ∩S2 · · · ∩Sk.
(The union of vector spaces is generally not a vector space.) If the intersections of the
subspaces are empty, Si ∩Sj = 0, i ̸= j, then the sum of the subspaces is called their
direct sum and denoted by
S = S1 ⊕S2 · · · ⊕Sk.
A function F from one linear space to another (or the same) linear space is said to be
linear if
F(αu + βv) = αF(u) + βF(v)
for all vectors u, v ∈V and all scalars α, β. Note that this terminology excludes nonho-
mogeneous functions like αu + β, which are called afﬁne functions. Linear functions are
often expressed in the form Au, where A is called a linear operator.
Avector space for which an inner product is deﬁned is called an inner product space.
For the vector space Rn the Euclidean inner product is
(x, y) =
n

i=1
xiyi.
(A.1.1)
Similarly Cn is an inner product space with the inner product
(x, y) =
n

k=1
¯xkyk,
(A.1.2)
where ¯xk denotes the complex conjugate of xk.

A.1. Vectors and Matrices
A-3
Two vectors v and w in Rn are said to be orthogonal if (v, w) = 0. A set of vectors
v1, . . . , vk in Rn is called orthogonal with respect to the Euclidean inner product if
(vi, vj) = 0,
i ̸= j,
and orthonormal if also (vi, vi) = 1, i = 1 : k. An orthogonal set of vectors is linearly
independent.
The orthogonal complement S⊥of a subspace S ∈Rn is the subspace deﬁned by
S⊥= {y ∈Rn| (y, x) = 0, x ∈S}.
More generally, the subspaces S1, . . . , Sk of Rn are mutually orthogonal if, for all 1 ≤i, j ≤
k, i ̸= j,
x ∈Si,
y ∈Sj,
⇒(x, y) = 0.
The vectors q1, . . . , qk form an orthonormal basis for a subspace S ⊂Rn if they are
orthonormal and span {q1, . . . , qk} = S.
A.1.2
Matrix and Vector Algebra
A matrix A is a collection of m × n real or complex numbers ordered in m rows and n
columns:
A = (aij) =


a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn

.
We write A ∈Rm×n, where Rm×n denotes the set of all real m × n matrices. For some
problems it is more relevant and convenient to work with complex vectors and matrices;
Cm×n denotes the set of m × n matrices whose components are complex numbers. If
m = n, then the matrix A is said to be square and of order n. If m ̸= n, then A is said to be
rectangular.
Amatrix A ∈Rm×n or Cm×n can be interpreted as representing a linear transformation
on ﬁnite-dimensional vector spaces over Rn or Cn. Consider a linear function u = F(v),
v ∈Cn, u ∈Cm. Let x and y be the column vectors representing the vectors v and F(v),
respectively, using the standard basis of the two spaces. Then there is a unique matrix
A ∈Cm×n representing this map such that
y = Ax.
This gives a link between linear maps and matrices.
We will follow a convention introduced by Householder191 and use uppercase letters
(e.g., A, B) to denote matrices. The corresponding lowercase letters with subscripts ij then
refer to the (i, j) component of the matrix (e.g., aij, bij). Greek letters α, β, . . . are usually
used to denote scalars. Column vectors are usually denoted by lower case letters (e.g., x, y).
191A. S. Householder (1904–1993), at Oak Ridge National Laboratory and University of Tennessee, was a pioneer
in the use of matrix factorization and orthogonal transformations in numerical linear algebra.

A-4
Online Appendix A. Introduction to Matrix Computations
Two matrices in Rm×n are said to be equal, A = B, if
aij = bij,
i = 1 : m,
j = 1 : n.
The basic operations with matrices are deﬁned as follows. The product of a matrix A with
a scalar α is
B = αA,
bij = αaij.
The sum of two matrices A and B in Rm×n is
C = A + B,
cij = aij + bij.
(A.1.3)
The product of two matrices A and B is deﬁned if and only if the number of columns in A
equals the number of rows in B. If A ∈Rm×n and B ∈Rn×p, then
C = AB ∈Rm×p,
cij =
n

k=1
aikbkj,
(A.1.4)
and can be computed with 2mnp ﬂops. The product BA is deﬁned only if m = p.
Matrix multiplication satisﬁes the distributive rules
A(BC) = (AB)C,
A(B + C) = AB + AC.
(A.1.5)
Note, however, that the number of arithmetic operations required to compute, respectively,
the left- and right-hand sides of these equations can be very different. Matrix multiplication
is, however, not commutative, that is, even when both products are deﬁned AB ̸= BA, in
general. In the special case that AB = BA the matrices are said to commute.
The transpose AT of a matrix A = (aij) is the matrix whose rows are the columns
of A, i.e., if C = AT , then cij = aji. For the transpose of a product we have
(AB)T = BT AT ,
(A.1.6)
i.e., the product of the transposed matrices in reverse order. For a complex matrix, AH
denotes the complex conjugate transpose of A
A = (aij),
AH = (¯aji),
and it holds that (AB)H = BHAH.
A column vector is a matrix consisting of just one column and we write x ∈Rn
instead of x ∈Rn×1. Note that the Euclidean inner product (A.1.1) can be written as
(x, y) = xT y.
If A ∈Rm×n, x ∈Rn, then
y = Ax ∈Rm,
yi =
n

j=1
aijxj,
i = 1 : m.
A row vector is a matrix consisting of just one row and is obtained by transposing a column
vector (e.g., xT ).

A.1. Vectors and Matrices
A-5
It is useful to deﬁne array operations, which are carried out element by element on
vectors and matrices. Let A = (aij) and B = (bij) be two matrices of the same dimensions.
Then the Hadamard product192 is deﬁned by
C = A . ∗B
⇔
cij = aij · bij.
(A.1.7)
Similarly A ./B is a matrix with elements aij/bij. For the operations + and −the array
operations coincide with matrix operations so no distinction is necessary.
A.1.3
Rank and Linear Systems
For a matrix A ∈Rm×n the maximum number of independent row vectors is always equal
to the maximum number of independent column vectors. This number r is called the rank
of A and thus we have r ≤min(m, n). If rank (A) = n, A is said to have full column rank;
if rank (A) = m, A is said to have full row rank.
The outer product of two vectors x ∈Rm and y ∈Rn is the matrix
xyT =


x1y1
. . .
x1yn
...
...
xmy1
. . .
xmyn

∈Rm×n.
(A.1.8)
Clearly this matrix has rank equal to one.
Asquarematrixisnonsingularandinvertibleifthereexistsaninversematrixdenoted
by A−1 with the property that
A−1A = AA−1 = I.
This is the case if and only if A has full row (column) rank. The inverse of a product of two
matrices is
(AB)−1 = B−1A−1;
i.e., it equals the product of the inverse matrices taken in reverse order.
The operations of taking transpose and inverse commutes, i.e., (A−1)T = (AT )−1.
Therefore, we can denote the resulting matrix by A−T .
The range and the nullspace of a matrix A ∈Rm×n are
R(A) = {z ∈Rm| z = Ax, x ∈Rn},
(A.1.9)
N(A) = {y ∈Rn| Ay = 0}.
(A.1.10)
These are related to the range and nullspace of the transpose matrix AT by
R(A)⊥= N(AT ),
N(A)⊥= R(AT );
(A.1.11)
i.e., N(AT ) is the orthogonal complement to R(A) and N(A) the orthogonal complement
to R(AT ). This result is sometimes called the Fundamental Theorem of Linear Algebra.
192Jacques Salomon Hadamard (1865–1963) was a French mathematician active at the Sorbonne, Collège de
France and Ècole Polytechnique in Paris. He made important contributions to geodesics of surfaces and functional
analysis. He gave a proof of the result that the number of primes ≤n tends to inﬁnity as n/ ln n.

A-6
Online Appendix A. Introduction to Matrix Computations
A square matrix A ∈Rn×n is nonsingular if and only if N(A) = {0}. A linear
system Ax = b, A ∈Rm×n, is said to be consistent if b ∈R(A) or, equivalently if
rank (A, b) = rank (A). A consistent linear system always has at least one solution x. If
b ̸∈R(A) or equivalently, rank (A, b) > rank (A), the system is inconsistent and has no
solution. If m > n, there are always right-hand sides b such that Ax = b is inconsistent.
A.1.4
Special Matrices
Any matrix D for which dij = 0 if i ̸= j is called a diagonal matrix. If x ∈Rn is a vector,
then D = diag (x) ∈Rn×n is the diagonal matrix formed by the elements of x. For a matrix
A ∈Rn×n the elements aii, i = 1 : n, form the main diagonal of A, and we write
diag (A) = diag (a11, a22, . . . , ann).
For k = 1 : n −1 the elements ai,i+k (ai+k,i), i = 1 : n −k, form the kth superdiagonal
(subdiagonal) of A. The elements ai,n−i+1, i = 1 : n, form the (main) antidiagonal of A.
The unit matrix I = In ∈Rn×n is deﬁned by
In = diag (1, 1, . . . , 1) = (e1, e2, . . . , en),
and the kth column of In is denoted by ek. We have that In = (δij), where δij is the
Kronecker symbol δij = 0, i ̸= j, and δij = 1, i = j. For all square matrices of order n,
it holds that AI = IA = A. If desirable, we set the size of the unit matrix as a subscript of
I, e.g., In.
A matrix A for which all nonzero elements are located in consecutive diagonals is
called a band matrix.
A is said to have upper bandwidth r if r is the smallest integer
such that
aij = 0,
j > i + r,
and similarly to have lower bandwidth s if s is the smallest integer such that
aij = 0,
i > j + s.
The number of nonzero elements in each row of A is then at most equal to w = r + s + 1,
which is the bandwidth of A. For a matrix A ∈Rm×n which is not square, we deﬁne the
bandwidth as
w = max
1≤i≤m{j −k + 1 | aijaik ̸= 0}.
Several classes of band matrices that occur frequently have special names. Thus, a
matrix for which r = s = 1 is called tridiagonal; if r = 0, s = 1 (r = 1, s = 0), it is
called lower (upper) bidiagonal, etc. Amatrix with s = 1 (r = 1) is called an upper (lower)
Hessenberg matrix.
An upper triangular matrix is a matrix R for which rij = 0 whenever i > j. A
square upper triangular matrix has the form
R =


r11
r12
. . .
r1n
0
r22
. . .
r2n
...
...
...
...
0
0
. . .
rnn

.

A.1. Vectors and Matrices
A-7
If also rij = 0 when i = j, then R is strictly upper triangular. Similarly a matrix L is lower
triangular if lij = 0, i < j, and strictly lower triangular if lij = 0, i ≤j. Sums, products,
and inverses of square upper (lower) triangular matrices are again triangular matrices of the
same type.
A square matrix A is called symmetric if its elements are symmetric about its main
diagonal, i.e., aij = aji, or equivalently, AT = A. The product of two symmetric matrices
is symmetric if and only if A and B commute, that is, AB = BA. If AT = −A, then A is
called skew-symmetric.
For any square nonsingular matrix A, there is a unique adjoint matrix A∗such that
(x, A∗y) = (Ax, y).
The matrix A ∈Cn×n is called self-adjoint if A∗= A. In particular, for A ∈Rn×n with
the standard inner product, we have
(Ax, y) = (Ax)T y = xT AT y.
Hence A∗= AT , the transpose of A, and A is self-adjoint if it is symmetric. A symmetric
matrix A is called positive deﬁnite if
xT Ax > 0
∀x ∈Rn,
x ̸= 0,
(A.1.12)
and positive semideﬁnite if xT Ax ≥0 for all x ∈Rn. Otherwise it is called indeﬁnite.
Similarly, A ∈Cn×n is self-adjoint or Hermitian if A = AH, the conjugate transpose
of A. A Hermitian matrix has analogous properties to a real symmetric matrix. If A is
Hermitian, then (xHAx)H = xHAx is real, and A is positive deﬁnite if
xHAx > 0
∀x ∈Cn,
x ̸= 0.
(A.1.13)
Any matrix A ∈Cn×n can be written as the sum of its Hermitian and a skew-Hermitian part,
A = H(A) + S(A), where
H(A) = 1
2(A + AH),
S(A) = 1
2(A −AH).
A is Hermitian if and only if S(A) = 0. It is easily seen that A is positive deﬁnite if and
only if its symmetric part H(A) is positive deﬁnite. For the vector space Rn (Cn), any inner
product can be written as
(x, y) = xT Gy
((x, y) = xHGy),
where the matrix G is positive deﬁnite.
Let q1, . . . , qn ∈Rm be orthonormal and form the matrix
Q = (q1, . . . , qn) ∈Rm×n,
m ≥n.
Then Q is called an orthogonal matrix and QT Q = In. If Q is square (m = n), then it
also holds that Q−1 = QT , QQT = In.
Two vectors x and y in Cn are called orthogonal if xHy = 0. A square matrix U for
which U HU = I is called unitary, and from (A.1.2) we ﬁnd that
(Ux)HUy = xHU HUy = xHy.

A-8
Online Appendix A. Introduction to Matrix Computations
A.2
Submatrices and Block Matrices
A matrix formed by the elements at the intersection of a set of rows and columns of a matrix
A is called a submatrix. For example, the matrices

a22
a24
a42
a44

,

a22
a23
a32
a33

are submatrices of A. The second submatrix is called a contiguous submatrix since it is
formed by contiguous elements of A.
Deﬁnition A.2.1.
A submatrix of A = (aij) ∈Rm×n is a matrix B ∈Rp×q formed by selecting p rows
and q columns of A,
B =


ai1j1
ai1j2
· · ·
ai1jq
ai2j1
ai2j2
· · ·
ai2jq
...
...
...
...
aipj1
aipj2
· · ·
aipjq

,
where
1 ≤i1 ≤i2 ≤· · · ≤ip ≤m,
1 ≤j1 ≤j2 ≤· · · ≤jq ≤n.
If p = q and ik = jk, k = 1 : p, then B is a principal submatrix of A. If in addition,
ik = jk = k, k = 1 : p, then B is a leading principal submatrix of A.
It is often convenient to think of a matrix (vector) as being built up of contiguous
submatrices (subvectors) of lower dimensions. This can be achieved by partitioning the
matrix or vector into blocks. We write, e.g.,
A =


q1
q2
. . .
qN
p1 {
A11
A12
· · ·
A1N
p2 {
A21
A22
· · ·
A2N
...
...
...
...
...
pM {
AM1
AM2
· · ·
AMN

,
x =


p1 {
x1
p2 {
x2
...
...
pM {
xM

,
(A.2.1)
where AIJ is a matrix of dimension pI × qJ. We call such a matrix a block matrix. The
partitioning can be carried out in many ways and is often suggested by the structure of the
underlying problem. For square matrices the most important case is when M = N, and
pI = qI, I = 1 : N. Then the diagonal blocks AII, I = 1 : N, are square matrices.
The great convenience of block matrices lies in the fact that the operations of addition
and multiplication can be performed by treating the blocks AIJ as non-commuting scalars.
Let A = (AIK) and B = (BKJ) be two block matrices of block dimensions M × N
and N × P , respectively, where the partitioning corresponding to the index K is the same
for each matrix. Then we have C = AB = (CIJ), where
CIJ =
N

K=1
AIKBKJ,
1 ≤I ≤M,
1 ≤J ≤P.
(A.2.2)

A.2. Submatrices and Block Matrices
A-9
Therefore many algorithms deﬁned for matrices with scalar elements have another simple
generalization to partitioned matrices. Of course the dimensions of the blocks must cor-
respond in such a way that the operations can be performed. When this is the case, the
matrices are said to be partitioned conformally.
The colon notation used in MATLAB is very convenient for handling partitioned
matrices and will be used throughout this volume:
j : k
is the same as the vector [j, j + 1, . . . , k],
j : k
is empty if j > k,
j : i : k
is the same as the vector [j, j + i, , j + 2i, . . . , k],
j : i : k
is empty if i > 0 and j > k or if i < 0 and j < k.
The colon notation is used to pick out selected rows, columns, and elements of vectors and
matrices, for example,
x(j : k)
is the vector[x(j), x(j + 1), . . . , x(k)],
A(:, j)
is the jth column of A,
A(i, :)
is the ith row of A,
A(:, :)
is the same as A,
A(:, j : k)
is the matrix[A(:, j), A(:, j + 1), . . . , A(:, k)],
A(:)
is all the elements of the matrix A regarded as a single column.
The various special forms of matrices have analogue block forms. For example, R is
block upper triangular if it has the form
R =


R11
R12
R13
· · ·
R1N
0
R22
R23
· · ·
R2N
0
0
R33
· · ·
R3N
...
...
...
...
...
0
0
0
· · ·
RNN


.
Example A.2.1.
Partitioning a matrix into a block 2 × 2 matrix with square diagonal blocks is partic-
ularly useful. For this case we have

A11
A12
A21
A22
 
B11
B12
B21
B22

=

A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22

.
(A.2.3)
Be careful to note that since matrix multiplication is not commutative the order of the factors
in the products cannot be changed! In the special case of block upper triangular matrices
this reduces to

R11
R12
0
R22
 
S11
S12
0
S22

=

R11S11
R11S12 + R12S22
0
R22S22

.
Note that the product is again block upper triangular and its block diagonal simply equals
the products of the diagonal blocks of the factors.

A-10
Online Appendix A. Introduction to Matrix Computations
A.2.1
Block Gaussian Elimination
Let
L =

L11
0
L21
L22

,
U =

U11
U12
0
U22

(A.2.4)
be 2 × 2 block lower and block upper triangular matrices, respectively. We assume that the
diagonal blocks are square but not necessarily triangular. Generalizing (A.3.5) it then holds
that
det(L) = det(L11) det(L22),
det(U) = det(U11) det(U22).
(A.2.5)
Hence L and U are nonsingular if and only if the diagonal blocks are nonsingular. If they
are nonsingular, their inverses are given by
L−1 =

L−1
11
0
−L−1
22 L21L−1
11
L−1
22

,
U −1 =

U −1
11
−U −1
11 U12U −1
22
0
U −1
22

.
(A.2.6)
This can be veriﬁed by forming the products L−1L and U −1U using the rule for multiplying
partitioned matrices.
We now give some formulas for the inverse of a block 2 × 2 matrix,
M =

A
B
C
D

,
(A.2.7)
where A and D are square matrices. If A is nonsingular, we can factor M in a product of a
block lower and a block upper triangular matrix,
M =

I
0
CA−1
I
 
A
B
0
S

,
S = D −CA−1B.
(A.2.8)
This identity, which is equivalent to block Gaussian elimination, can be veriﬁed directly.
The matrix S is the Schur complement of A in M.193
From M−1 = (LU)−1 = U −1L−1, using the formulas (A.2.6) for the inverses of
2 × 2 block triangular matrices we get the Banachiewicz inversion formula194
M−1 =

A−1
−A−1BS−1
0
S−1
 
I
0
−CA−1
I

=

A−1 + A−1BS−1CA−1
−A−1BS−1
−S−1CA−1
S−1

.
(A.2.9)
Similarly, assuming that D is nonsingular, we can factor M into a product of a block upper
and a block lower triangular matrix
M =

I
BD−1
0
I
 
T
0
C
D

,
T = A −BD−1C,
(A.2.10)
193Issai Schur (1875–1941) was born in Russia but studied at the University of Berlin, where he became full
professor in 1919. Schur is mainly known for his fundamental work on the theory of groups, but he also worked
in the ﬁeld of matrices.
194Tadeusz Banachiewicz (1882–1954) was a Polish astronomer and mathematician. In 1919 he became the
director of Cracow Observatory. In 1925 he developed a special kind of matrix algebra for “cracovians” which
brought him international recognition.

A.2. Submatrices and Block Matrices
A-11
where T is the Schur complement of D in M.
(This is equivalent to block Gaussian
elimination in reverse order.) From this factorization an alternative expression of M−1 can
be derived,
M−1 =

T −1
−T −1BD−1
−D−1CT −1
D−1 + D−1CT −1BD−1

.
(A.2.11)
If A and D are nonsingular, then both triangular factorizations (A.2.8) and (A.2.10) exist.
An important special case of the ﬁrst Banachiewicz inversion formula (A.2.9) is when
the block D is a scalar,
M =

A
b
cT
δ

.
(A.2.12)
Then if the Schur complement σ = δ −cT A−1b ̸= 0, we obtain for the inverse the formula
M−1 =

A−1 + σ −1A−1bcT A−1
−σ −1A−1b
−σ −1cT A−1
σ −1

.
(A.2.13)
This formula is convenient to use in case it is necessary to solve a system for which the
truncated system, obtained by crossing out one equation and one unknown, has been solved
earlier. Such a situation is often encountered in applications.
The formula can also be used to invert a matrix by successive bordering, where one
constructs in succession the inverse of matrices
( a11 ) ,

a11
a12
a21
a22

,
 a11
a12
a13
a21
a22
a23
a31
a32
a33

, . . . .
Each step is then carried by using the formula (A.2.13).
The formulas for the inverse of a block 2×2 matrix can be used to derive expressions
for the inverse of a matrix A ∈Rn×n modiﬁed by a matrix of rank p. Any matrix of
rank p ≤n can be written as BD−1C, where B ∈Rp×n, C ∈Rp×n, and D ∈Rn×n is
nonsingular. (The factor D is not necessary but included for convenience.) Assuming that
A −BD−1C is nonsingular and equating the (1, 1) blocks in the inverse M−1 in (A.2.9)
and (A.2.11), we obtain the Woodbury formula,
(A −BD−1C)−1 = A−1 + A−1B(D −CA−1B)−1CA−1.
(A.2.14)
This gives an expression for the inverse of a matrix A after it has been modiﬁed by a
matrix of rank p, a very useful result in situations where p ≪n.
If we specialize the Woodbury formula to the case where D is a scalar and
M =

A
u
vT
1/σ

,
we get the well-known Sherman–Morrison formula,
(A −σuvT )−1 = A−1 + α(A−1u)(vT A−1),
α =
σ
1 −σ vT A−1u.
(A.2.15)
It follows that A −σuvT is nonsingular if and only if σ ̸= 1/vT A−1u. The Sherman–
Morrison formula can be used to compute the new inverse when a matrix A is modiﬁed by
a matrix of rank one.

A-12
Online Appendix A. Introduction to Matrix Computations
Frequently it is required to solve a linear problem where the matrix has been modiﬁed
by a correction of low rank. Consider ﬁrst a linear system Ax = b, where A is modiﬁed by
a correction of rank one,
(A −σuvT )ˆx = b.
(A.2.16)
Using the Sherman–Morrison formula, we can write the solution as
(A −σuvT )−1b = A−1b + α(A−1u)(vT A−1b),
α = 1/(σ −1 −vT A−1u).
Here x = A−1b is the solution to the original system and vT A−1b = vT x is a scalar. Hence,
ˆx = x + βw,
β = vT x/(σ −1 −vT w),
w = A−1u,
(A.2.17)
which shows that the solution ˆx can be obtained from x by solving the system Aw = u.
Note that computing A−1 can be avoided.
We caution that the updating formulas given here cannot be expected to be numeri-
cally stable in all cases. This is related to the fact that pivoting is necessary in Gaussian
elimination.
A.3
Permutations and Determinants
The classical deﬁnition of the determinant195 requires some elementary facts about per-
mutations which we now state.
Let α = {α1, α2, . . . , αn} be a permutation of the integers {1, 2, . . . , n}. The pair
αr, αs, r < s, is said to form an inversion in the permutation if αr > αs. For example,
in the permutation {2, . . . , n, 1} there are (n −1) inversions (2, 1), (3, 1), . . . , (n, 1). A
permutation α is said to be even and sign (α) = 1 if it contains an even number of inversions;
otherwise the permutation is odd and sign (α) = −1.
The product of two permutations σ and τ is the composition στ deﬁned by
στ(i) = σ[τ(i)],
i = 1 : n.
Atransposition τ is a permutation which interchanges only two elements. Any permutation
can be decomposed into a sequence of transpositions, but this decomposition is not unique.
A permutation matrix P ∈Rn×n is a matrix whose columns are a permutation of
the columns of the unit matrix, that is,
P = (ep1, . . . , epn),
where p1, . . . , pn is a permutation of 1, . . . , n. Notice that in a permutation matrix every
row and every column contains just one unity element. Since P is uniquely represented by
the integer vector p = (p1, . . . , pn) it need never be explicitly stored. For example, the
195Determinants were ﬁrst introduced by Leibniz in 1693 and then by Cayley in 1841. Determinants arise in
many parts of mathematics such as combinatorial enumeration, graph theory, representation theory, statistics, and
theoretical computer science. The theory of determinants is covered in the monumental ﬁve-volume work The
Theory of Determinants in the Historical Order of Development by Thomas Muir (1844–1934).

A.3. Permutations and Determinants
A-13
vector p = (2, 4, 1, 3) represents the permutation matrix
P =


0
0
1
0
1
0
0
0
0
0
0
1
0
1
0
0

.
If P is a permutation matrix, then PA is the matrix A with its rows permuted and AP is A
with its columns permuted. Using the colon notation, we can write these permuted matrices
as PA = A(p,:) and PA = A(:,p), respectively.
The transpose P T of a permutation matrix is again a permutation matrix. Any permu-
tation may be expressed as a sequence of transposition matrices. Therefore any permutation
matrix can be expressed as a product of transposition matrices P = Ii1,j1Ii2,j2 · · · Iik,jk. Since
I −1
ip,jp = Iip,jp, we have
P −1 = Iik,jk · · · Ii2,j2Ii1,j1 = P T ;
that is, permutation matrices are orthogonal and P T performs the reverse permutation, and
thus,
P T P = PP T = I.
(A.3.1)
Lemma A.3.1.
A transposition τ of a permutation will change the number of inversions in the per-
mutation by an odd number, and thus sign (τ) = −1.
Proof. If τ interchanges two adjacent elements αr and αr+1 in the permutation {α1, α2, . . . ,
αn}, this will not affect inversions in other elements. Hence the number of inversions
increases by 1 if αr < αr+1 and decreases by 1 otherwise. Suppose now that τ interchanges
αr and αr+q. This can be achieved by ﬁrst successively interchanging αr with αr+1, then
with αr+2, and ﬁnally with αr+q. This takes q steps. Next the element αr+q is moved
in q −1 steps to the position which αr previously had. In all it takes an odd number
2q −1 of transpositions of adjacent elements, in each of which the sign of the permutation
changes.
Deﬁnition A.3.2.
The determinant of a square matrix A ∈Rn×n is the scalar
det(A) =

α∈Sn
sign (α) a1,α1a2,α2 · · · an,αn,
(A.3.2)
where the sum is over all n! permutations of the set {1, . . . , n} and sign (α) = ±1 according
to whether α is an even or odd permutation.
Note that there are n! terms in (A.3.2) and each term contains exactly one factor from
each row and each column in A. For example, if n = 2, there are two terms, and
det

a11
a12
a21
a22

= a11a22 −a12a21.

A-14
Online Appendix A. Introduction to Matrix Computations
From the deﬁnition, it follows easily that
det(αA) = αn det(A),
det(AT ) = det(A).
If we collect all terms in (A.3.2) that contain the element ars, the sum of these terms
can be written as arsArs, where Ars is called the complement of ars. Since the determinant
contains only one element from row r and column s, the complement Ars does not depend
on any elements in row r and column s. Since each product in (A.3.2) contains precisely
one element of the elements ar1, ar2, . . . , arn in row r, it follows that
det(A) = ar1Ar1 + ar2Ar2 + · · · + arnArn.
(A.3.3)
This is called to expand the determinant after the row r. It is not difﬁcult to verify that
Ars = (−1)r+sDrs,
(A.3.4)
where Drs is the determinant of the matrix of order n−1 obtained by striking out row r and
column s in A. Since det(A) = det(AT ), it is clear that we can similarly expand det(A)
after a column.
The direct use of the deﬁnition (A.3.2) to evaluate det(A) would require about nn!
operations, which rapidly becomes infeasible as n increases. A much more efﬁcient way to
compute det(A) is by repeatedly using the following properties.
Theorem A.3.3.
(i) The value of the det(A) is unchanged if a row (column) in A multiplied by a scalar is
added to another row (column).
(ii) The determinant of a triangular matrix equals the product of the elements in the main
diagonal; i.e., if U is upper triangular,
det(U) = u11u22 · · · unn.
(A.3.5)
(iii) If two rows (columns) in A are interchanged, the value of det(A) is multiplied by
(−1).
(iv) The product rule det(AB) = det(A) det(B).
If Q is an orthogonal matrix, then QT Q = In. Then using (iv) it follows that
1 = det(I) = det(QT Q) = det(QT ) det(Q) = (det(Q))2,
and hence det(Q) = ±1. If det(Q) = 1, then Q is a rotation.
Theorem A.3.4.
The matrix A is nonsingular if and only if det(A) ̸= 0. If the matrix A is nonsingular,
then the solution of the linear system Ax = b can be expressed as
xj = det(Bj)/ det(A),
j = 1 : n.
(A.3.6)

A.3. Permutations and Determinants
A-15
Here Bj is the matrix A, where the jth column has been replaced by the right-hand side
vector b.
Proof. We have
a1jA1r + a2jA2r + · · · + anjAnr =
%
0
if j ̸= r,
det(A)
if j = r,
(A.3.7)
where the linear combination is formed with elements from column j and the complements
of column r. If j = r, this is an expansion after column r of det(A). If j ̸= r, the expression
is the expansion of the determinant of a matrix equal to A except that column r is equal to
column j. Such a matrix has a determinant equal to 0.
Now take the ith equation in Ax = b,
ai1x1 + ai2x2 + · · · ainxn = bi,
multiply by Air, and sum over i = 1 : n. Then by (A.3.7) the coefﬁcients of xj, j ̸= r,
vanish and we get
det(A)xr = b1A1r + b2A2r + · · · bnAnr.
The right-hand side equals det(Br) expanded by its rth column,
which proves
(A.3.6).
The expression (A.3.6) is known as Cramer’s rule.196 Although elegant, it is both
computationally expensive and numerically unstable, even for n = 2.
Let U be an upper block triangular matrix with square diagonal blocks UII, I = 1 : N.
Then
det(U) = det(U11) det(U22) · · · det(UNN),
(A.3.8)
and thus U is nonsingular if and only if all its diagonal blocks are nonsingular. Since
det(L) = det(LT ), a similar result holds for a lower block triangular matrix.
Example A.3.1.
For the 2 × 2 block matrix M in (A.2.8) and (A.2.10), it follows by using (A.3.8) that
det(M) = det(A −BD−1C) det(D) = det(A) det(D −CA−1B).
In the special case that D−1 = λ, B = x, and B = y, this gives
det(A −λxyT ) = det(A)(1 −λyT A−1x).
(A.3.9)
This shows that det(A−λxyT ) = 0 if λ = 1/yT A−1x, a fact which is useful for the solution
of eigenvalue problems.
196Named after the Swiss mathematician Gabriel Cramer (1704–1752).

A-16
Online Appendix A. Introduction to Matrix Computations
A.4
Eigenvalues and Norms of Matrices
A.4.1
The Characteristic Equation
Of central importance in the study of matrices are the special vectors whose directions are
not changed when multiplied by A. A complex scalar λ such that
Ax = λx,
x ̸= 0,
(A.4.1)
is called an eigenvalue of A and x is an eigenvector of A. Eigenvalues and eigenvectors
give information about the behavior of evolving systems governed by a matrix or operator
and are fundamental tools in the mathematical sciences and in scientiﬁc computing.
From (A.4.1) it follows that λ is an eigenvalue if and only if the linear homogeneous
system (A−λI)x = 0 has a nontrivial solution x ̸= 0, or equivalently, if and only if A−λI
is singular. It follows that the eigenvalues satisfy the characteristic equation
p(λ) = det(A −λI) = 0.
(A.4.2)
Obviously, if x is an eigenvector, so is αx for any scalar α ̸= 0.
The polynomial p(λ) = det(A −λI) is the characteristic polynomial of the matrix
A. Expanding the determinant in (A.4.2), it follows that p(λ) has the form
p(λ) = (a11 −λ)(a22 −λ) · · · (ann −λ) + q(λ),
(A.4.3)
where q(λ) has degree at most n −2. Hence p(λ) is a polynomial of degree n in λ with
leading term (−1)nλn. By the fundamental theorem of algebra the matrix A has exactly n
(possibly complex) eigenvalues λi, i = 1, 2, . . . , n, counting multiple roots according to
their multiplicities. The set of eigenvalues of A is called the spectrum of A. The largest
modulus of an eigenvalue is called the spectral radius and denoted by
ρ(A) = max
i
|λi(A)|.
(A.4.4)
Putting λ = 0 in p(λ) = (λ1 −λ)(λ2 −λ) · · · (λn −λ) and (A.4.2), it follows that
p(0) = λ1λ2 · · · λn = det(A).
(A.4.5)
Consider the linear transformation y = Ax, where A ∈Rn×n. Let V be nonsingular
and suppose we change the basis by setting x = V ξ, y = V η. The column vectors ξ
and η then represent the vectors x and y with respect to the basis V = (v1, . . . , vn). Now
V η = AV ξ, and hence η = V −1AV ξ. This shows that the matrix
B = V −1AV
represents the operator A in the new basis. The mapping A →B = V −1AV is called a
similarity transformation. If Ax = λx, then
V −1AVy = By = λy,
y = V −1x,
which shows the important facts that B has the same eigenvalues as A and that the eigen-
vectors of B can be easily computed from those of A. In other words, eigenvalues are

A.4. Eigenvalues and Norms of Matrices
A-17
properties of the operator itself and are independent of the basis used for its representation
by a matrix.
The trace of a square matrix of order n is the sum of its diagonal elements
trace (A) =
n

i=1
aii =
n

i=1
λi.
(A.4.6)
The last equality follows by using the relation between the coefﬁcients and roots of the
characteristic equation. Hence the trace of the matrix is invariant under similarity transfor-
mations.
A.4.2
The Schur and Jordan Normal Forms
Given A ∈Cn×n there exists a unitary matrix U ∈Cn×n such that
U HAU = T =


λ1
t12
. . .
t1n
λ2
. . .
t2n
...
...
λn

,
where T is upper triangular. This is the Schur normal form of A. (A proof will be given
in Chapter 9 of Volume II.) Since
det(T −λI) = (λ1 −λ)(λ2 −λ) · · · (λn −λ),
the diagonal elements λ1, . . . , λn of T are the eigenvalues of A.
Each distinct eigenvalue λi has at least one eigenvector vi. Let V = (v1, . . . , vk) be
eigenvectors corresponding to the eigenvalues X = diag (λ1, . . . , λk) of a matrix A. Then,
we can write
AV = V X.
If there are n linearly independent eigenvectors, then V = (v1, . . . , vn) is nonsingular and
A = V XV −1,
X = V −1AV.
Then A is said to be diagonalizable.
A matrix A ∈Cn×n is said to be normal if AHA = AAH. For a normal matrix the
upper triangular matrix T in the Schur normal form is also normal, i.e.,
T HT = T T H.
It can be shown that this relation implies that all nondiagonal elements in T vanish, i.e.,
T = X. Then we have AU = UT = UX, where X = diag (λi), or with U = (u1, . . . , un),
Aui = λiui,
i = 1 : n.
This shows the important result that a normal matrix always has a set of mutually unitary
(orthogonal) eigenvectors.

A-18
Online Appendix A. Introduction to Matrix Computations
Important classes of normal matrices are Hermitian (A = AH), skew-Hermitian
(AH = −A), and unitary (A−1 = AH). Hermitian matrices have real eigenvalues, skew-
Hermitian matrices have imaginary eigenvalues, and unitary matrices have eigenvalues on
the unit circle (see Chapter 9 of Volume II).
An example of a nondiagonalizable matrix is
Jm(λ) =


λ
1
λ
...
...
1
λ

∈Cm×m.
The matrix Jm(λ) is called a Jordan block. It has one eigenvalue λ of multiplicity m to
which corresponds only one eigenvector,
Jm(λ)e1 = λe1,
e1 = (1, 0, . . . , 0)T .
A.4.3
Norms of Vectors and Matrices
In many applications it is useful to have a measure of the size of a vector or a matrix.
An example is the quantitative discussion of errors in matrix computation. Such measures
are provided by vector and matrix norms, which can be regarded as generalizations of the
absolute value function on R.
A norm on the vector space Cn is a function Cn →R denoted by ∥· ∥that satisﬁes
the following three conditions:
1.
∥x∥> 0
∀x ∈Cn,
x ̸= 0
(deﬁniteness),
2.
∥αx∥= |α| ∥x∥
∀α ∈C,
x ∈Cn
(homogeneity),
3.
∥x + y∥≤∥x∥+ ∥y∥
∀x, y ∈Cn
(triangle inequality).
The triangle inequality is often used in the form (see Problem A.11)
∥x ± y∥≥
(( ∥x∥−∥y∥
((.
ThemostcommonvectornormsarespecialcasesofthefamilyofHöldernorms, orp-norms,
∥x∥p = (|x1|p + |x2|p + · · · + |xn|p)1/p,
1 ≤p < ∞.
(A.4.7)
The p-norms have the property that ∥x∥p = ∥|x| ∥p. Vector norms with this property are
said to be absolute. The three most important particular cases are p = 1 (the 1-norm),
p = 2 (the Euclidean norm), and the limit when p →∞(the maximum norm):
∥x∥1 = |x1| + · · · + |xn|,
∥x∥2 = (|x1|2 + · · · + |xn|2)1/2 = (xHx)1/2,
(A.4.8)
∥x∥∞= max
1≤i≤n |xi|.

A.4. Eigenvalues and Norms of Matrices
A-19
If Q is unitary, then
∥Qx∥2
2 = xHQHQx = xHx = ∥x∥2
2,
that is, the Euclidean norm is invariant under unitary (orthogonal) transformations.
The proof that the triangle inequality is satisﬁed for the p-norms depends on the
following inequality. Let p > 1 and q satisfy 1/p + 1/q = 1. Then it holds that
αβ ≤αp
p + βq
q .
Indeed, let x and y be any real number and λ satisfy 0 < λ < 1. Then by the convexity of
the exponential function, it holds that
eλx+(1−λ)y ≤λex + (1 −λ)ey.
We obtain the desired result by setting λ = 1/p, x = p log α, and y = q log β.
Another important property of the p-norms is the Hölder inequality
|xHy| ≤∥x∥p∥y∥q,
1
p + 1
q = 1,
p ≥1.
(A.4.9)
For p = q = 2 this becomes the Cauchy–Schwarz inequality
|xHy| ≤∥x∥2∥y∥2.
Norms can be obtained from inner products by taking
∥x∥2 = (x, x) = xHGx,
where G is Hermitian and positive deﬁnite. It can be shown that the unit ball {x : ∥x∥≤1}
corresponding to this norm is an ellipsoid, and hence such norms are also called elliptic
norms. A special useful case involves the scaled p-norms deﬁned by
∥x∥p,D = ∥Dx∥p,
D = diag (d1, . . . , dn),
di ̸= 0,
i = 1 : n.
(A.4.10)
All norms on Cn are equivalent in the following sense: For each pair of norms ∥· ∥
and ∥· ∥′ there are positive constants c and c′ such that
1
c ∥x∥′ ≤∥x∥≤c′∥x∥′
∀x ∈Cn.
(A.4.11)
In particular, it can be shown that for the p-norms we have
∥x∥q ≤∥x∥p ≤n

1
p −1
q

∥x∥q,
1 ≤p ≤q ≤∞.
(A.4.12)
We now consider matrix norms. We can construct a matrix norm from a vector norm
by deﬁning
∥A∥= sup
x̸=0
∥Ax∥
∥x∥= sup
∥x∥=1
∥Ax∥.
(A.4.13)

A-20
Online Appendix A. Introduction to Matrix Computations
This norm is called the operator norm, or the matrix norm subordinate to the vector norm.
From this deﬁnition it follows directly that
∥Ax∥≤∥A∥∥x∥,
x ∈Cn.
Whenever this inequality holds, we say that the matrix norm is consistent with the vector
norm. For any operator norm it holds that ∥In∥p = 1.
Itisaneasyexercisetoshowthatoperatornormsaresubmultiplicative; i.e., whenever
the product AB is deﬁned it satisﬁes the condition
4.
∥AB∥≤∥A∥∥B∥.
The matrix norms
∥A∥p = sup
∥x∥=1
∥Ax∥p,
p = 1, 2, ∞,
subordinate to the vector p-norms are especially important. The 1-norm and ∞-norm are
easily computable from
∥A∥1 = max
1≤j≤n
m

i=1
|aij|,
∥A∥∞= max
1≤i≤m
n

j=1
|aij|,
(A.4.14)
respectively. Note that the 1-norm equals the maximal column sum and the ∞-norm equals
the maximal row sum of the magnitude of the elements. Consequently ∥A∥1 = ∥AH∥∞.
The 2-norm is also called the spectral norm,
∥A∥2 = sup
∥x∥=1
(xHAHAx)1/2 = σ1(A),
(A.4.15)
where σ1(A) is the largest singular value of A. Its major drawback is that it is expensive
to compute. Since the nonzero eigenvalues of AHA and AAH are the same it follows that
∥A∥2 = ∥AH∥2. A useful upper bound for the matrix 2-norm is
∥A∥2 ≤(∥A∥1∥A∥∞)1/2.
(A.4.16)
The proof of this bound is left as an exercise.
Another way to proceed in deﬁning norms for matrices is to regard Cm×n as an mn-
dimensional vector space and apply a vector norm over that space. With the exception of
the Frobenius norm197 derived from the vector 2-norm,
∥A∥F =


m

i=1
n

j=1
|aij|2


1/2
;
(A.4.17)
such norms are not much used. Note that ∥AH∥F = ∥A∥F. Useful alternative characteri-
zations of the Frobenius norm are
∥A∥2
F = trace (AHA) =
k

i=1
σ 2
i (A),
k = min(m, n),
(A.4.18)
197Ferdinand George Frobenius (1849–1917) a German mathematician, was a professor at ETH Zürich from
1875 to 1892 before he succeeded Weierstrass at Berlin University.

A.4. Eigenvalues and Norms of Matrices
A-21
where σi(A) are the nonzero singular values of A. The Frobenius norm is submultiplicative.
However, it is often larger than necessary, e.g., ∥In∥F = n1/2. This tends to make bounds
derived in terms of the Frobenius norm not as sharp as they might be. From (A.4.15) and
(A.4.18) we also get lower and upper bounds for the matrix 2-norm,
1
√
k
∥A∥F ≤∥A∥2 ≤∥A∥F,
k = min(m, n).
An important property of the Frobenius norm and the 2-norm is that both are invariant with
respect to unitary (real orthogonal) transformations.
Lemma A.4.1.
For all unitary (real orthogonal) matrices U and V (U HU = I and
V HV = I) of appropriate dimensions, it holds that
∥UAV ∥= ∥A∥
(A.4.19)
for the Frobenius norm and the 2-norm.
We ﬁnally remark that the 1-, ∞- and Frobenius norms satisfy
∥|A| ∥= ∥A∥,
|A| = (|aij|),
but for the 2-norm the best result is that ∥|A| ∥2 ≤n1/2∥A∥2.
One use of norms is in the study of limits of sequences of vectors and matrices (see
Sec. 9.2.4 in Volume II). Consider an inﬁnite sequence x1, x2, . . . of elements of a vector
space V and let ∥· ∥be a norm on V. The sequence is said to converge (strongly if V is
inﬁnite-dimensional) to a limit x ∈V, and so we write limk→∞xk = x if
lim
k→∞∥xk −x∥= 0.
For a ﬁnite-dimensional vector space the equivalence of norms (A.4.11) shows that con-
vergence is independent of the choice of norm. The particular choice of ∥· ∥∞shows that
convergence of vectors in Cn is equivalent to convergence of the n sequences of scalars
formed by the components of the vectors. By considering matrices in Cm×n as vectors in
Cmn, we see that the same conclusion holds for matrices.
Review Questions
A.1. Deﬁne the following concepts:
(i) Real symmetric matrix.
(ii) Real orthogonal matrix.
(iii) Real skew-symmetric matrix.
(iv) Triangular matrix.
(v) Hessenberg matrix.
A.2. (a) What is the Schur normal form of a matrix A ∈Cn×n?
(b) What is meant by a normal matrix? How does the Schur form simplify for a normal
matrix?

A-22
Online Appendix A. Introduction to Matrix Computations
A.3. Deﬁne the matrix norm subordinate to a given vector norm.
A.4. Deﬁne the p-norm of a vector x. Give explicit expressions for the matrix p-norms for
p = 1, 2, ∞. Show that
∥x∥1 ≤√n∥x∥2 ≤n∥x∥∞,
which are special cases of (A.4.12).
Problems
A.1. Let A ∈Rm×n have rows aT
i , i.e., AT = (a1, . . . , am). Show that
AT A =
m

i=1
aiaT
i .
If A is instead partitioned into columns, what is the corresponding expression for
AT A?
A.2. (a) Show that if A and B are square upper triangular matrices, then AB is upper
triangular, and that A−1 is upper triangular, if it exists. Is the same true for lower
triangular matrices?
(b) Let A, B ∈Rn×n have lower bandwidth r and s, respectively. Show that the
product AB has lower bandwidth r + s.
(c) An upper Hessenberg matrix H is a matrix with lower bandwidth r = 1. Using
the result in (a) deduce that the product of H and an upper triangular matrix is again
an upper Hessenberg matrix.
(d) Show that if R ∈Rn×n is strictly upper triangular, then Rn = 0.
A.3. Use row operations to verify that the Vandermonde determinant is
det


1
x1
x2
1
1
x2
x2
2
1
x3
x2
3

= (x2 −x1)(x3 −x1)(x3 −x2).
A.4. To solve a linear system Ax = b, A ∈Rn×n, by Cramer’s rule (A.3.6) requires the
evaluation of n + 1 determinants of order n. Estimate the number of multiplications
needed for n = 50 if the determinants are evaluated in the naive way. Estimate the
time it will take on a computer performing 109 ﬂoating point operations per second!
A.5. Consider an upper block triangular matrix,
R =

R11
R12
0
R22

,
and suppose that R11 and R22 are nonsingular. Show that R is nonsingular and give
an expression for R−1 in terms of its blocks.

Problems
A-23
A.6. (a) Show that if w ∈Rn and wT w = 1, then the matrix P(w) = I −2wwT is both
symmetric and orthogonal.
(b) Let x, y ∈Rn, x ̸= y, be two given vectors with ∥x∥2 = ∥y∥2. Show that
P (w)x = y if w = (y −x)/∥y −x∥2.
A.7. Show that for any matrix norm there exists a consistent vector norm.
Hint: Take ∥x∥= ∥xyT ∥for any vector y ∈Rn, y ̸= 0.
A.8. Derive the formula for ∥A∥∞given in (A.4.14).
A.9. Show that ∥A∥2 = ∥PAQ∥2 if A ∈Rm×n and P and Q are orthogonal matrices of
appropriate dimensions.
A.10. Use the result ∥A∥2
2 = ρ(AT A) ≤∥AT A∥, valid for any matrix operator norm ∥· ∥,
where ρ(AT A) denotes the spectral radius of AT A, to deduce the upper bound in
(A.4.16).
A.11. (a) Let T be a nonsingular matrix, and let ∥· ∥be a given vector norm. Show that the
function N(x) = ∥T x∥is a vector norm.
(b) What is the matrix norm subordinate to N(x)?
(c) If N(x) = maxi |kixi|, what is the subordinate matrix norm?

Online Appendix B
A MATLAB Multiple
Precision Package
B.1
The Mulprec Package
In the following we describe the basics of Mulprec, a collection of MATLAB m-ﬁles for, in
principle, unlimited multiple precision ﬂoating-point computation. The version of Mulprec
presented here was worked out by the ﬁrst author in 2001. It is a preliminary version so
bugs may still exist.
Originally, a shorter version of this package and text was meant as motivation for a
Master’s project at the Royal Institute of Technology (KTH), Stockholm. Some new ideas
about chopping strategies and error estimation and control have been applied in some of the
m-ﬁles for the basic operations and elementary functions.
In the following we also give several examples of the possible use of the Mulprec
package.
B.1.1
Number Representation
A normalized Mulprec number is a row vector x with the usual MATLAB notations; the
value of x reads
val(x) = P x(1) 
j=2:k
x(j)P k−j,
P = 107,
k ≥2.
The x(j), j > 1, are integers called gytes (or gits), i.e., giant digits. They should all have
the same sign (equal to sgn(x)), and
|x(j)| < P,
j = 2 : k,
x(2) ̸= 0.
Thus, we have a position system with base P = 107, where x(1) is the exponent of val(x)
in a ﬂoating-point representation with base P.
Please note that P x(1) denotes the unit of the least signiﬁcant gyte, contrary to the
traditional ﬂoating-point convention.198 The length k of a Mulprec number x may vary
198It seems to be rather easy to change this if desirable.
B-1

B-2
Online Appendix B. A MATLAB Multiple Precision Package
during a computation. As an example, with an absolute error less than P −10 = 10−70, π
equals the following 12-gyte number.
−10
3
1415926
5358979
3238462
6433832
7950288
4197169
3993751
0582097
4944592
3078164
The decimal point, or rather the gyte point, is located immediately after column 12−10 = 2.
Note that
1 = [0, 1],
0.5 = [−1, 5000000],
−0.125 = [−1, −1250000].
We call the MATLAB numbers ﬂoats. You rarely have to write the Mulprec form
of numbers that are exactly representable as ﬂoats. The commands for the elementary
operations and most functions are constructed in such a way that they accept single ﬂoats
(not expressions) as input data and convert them to normalized Mulprec numbers by means
of the command npr, or npc for complex ﬂoats; see below. (Expressions with Mulprec
operations are, however, allowed as input data.) Mulprec distinguishes between ﬂoats and
Mulprec numbers by the length, which is equal to one or larger than one, respectively.
For a complex normalized Mulprec number, these conditions typically hold for both
the real and the imaginary part. The exponent and the length are common for both parts
with an exception: x(2) may thus be zero for one of the parts.
It is fundamental for Mulprec that P can be squared without overﬂow with some
margin. In fact,
253 > 90P 2.
Hence, if the shorter one of two positive normalized Mulprec numbers has at most 90 gytes,
we can obtain their product by the multiplication of gytes and addition of integers so that
the sums do not exceed 253. Typically, there is only one normalization in a multiplication.
The normalized representation of x is unique (if x ̸= 0). For example, note that if
you subtract two positive normalized Mulprec numbers, the gytes of the result may have
varying signs, unless you normalize the result by the Mulprec operation nize (or the simpler
operation rnize if the number is real). Since the operation rnize is not fast compared to the
operations add and sub, there is as a rule no normalization in add and sub.
For such reasons we now introduce a more general concept: the legal Mulprec
number. val(x) has the same value and the same form as the normalized Mulprec number,
but all the x(j) need not have the same sign and they have a looser bound:199 |x(j)| < 45 P 2.
Evidently such a representation of a number is not unique.
Allowing this more general type of Mulprec number in additions and subtractions
makes it unnecessary to transport carry digits inside these operations; this is typically done
later if a normalization is needed.200
A typical suboperation of the normalization is to subtract a multiple cP from one of
the x(j); this is typically compensated for by adding c to x(j −1), in order to keep val(x)
constant. (Is that not how we learned to handle the carry in addition in elementary school?)
Multiplication, division, elementary functions, etc. do include normalization, both of
the operands and of the results. Only normalized numbers should be printed.
199The addition of two legal numbers does not cause overﬂow, but the sum can be illegal at ﬁrst and must be
immediately normalized; see the next footnote.
200An exception: if the result of an add or a subtraction has become illegal, then it becomes acceptable after an
automatic call of nize inside add.m (or sub.m).

B.1. The Mulprec Package
B-3
B.1.2
The Mulprec Function Library
About 60 m-ﬁles for different Mulprec operations and functions have been developed so
far. The numbers in the beginning of the lines of the listings below are only for making
references in the text more convenient. They are thus not to be used in the codes and your
commands. Since the condensed comments in the table below may be unclear, you are
advised to study the codes a little before you use the system.
Mulprec numbers are typically denoted x, y, z. As mentioned above, most of the
commands also accept ﬂoats as input if it makes sense. In a command like z = mul(x, y, s),
the parameter s means the number of gytes wanted in the result (including the exponent;
hence it equals the length in the MATLAB sense). It is optional; if s is omitted, the exact
product is computed and normalized (not chopped).
An asterisk means that the code is longer than 500 bytes. The absence of an aster-
isk usually indicates, e.g., that the code is a relatively short combination of other library
codes. The numbers at the beginning of the lines of the following tables are not used in the
computations; they are just for easy reference to the table and to Mulprec.lib.
B.1.3
Basic Arithmetic Operations
Addition and subtraction were commented on above. Two routines for subtraction are given;
2b is shorter, but slower than 2a. Multiplication is performed as in elementary school—the
amount of work is approximately proportional to the product of the sizes of the factors.
Perhaps one of the fast algorithms, presented in Knuth [2, Sec. 4.3.3] in the binary case,
will be adapted to the gyte system in the future.
In the table below, the shorter of the operands in mul.m is chosen to be the multiplier.
In order to avoid overﬂow (in the additions inside the multiplication), the multiplier is
chopped to 90 gytes (at most 623 decimal places). There are bounds also for the accuracy
for division, square root, elementary functions, etc., since multiplication is used in their
codes.
1/x and 1/sqrt(x) are implemented by Newton’s iteration method, with variable preci-
sion that (roughly) doubles the number of gytes in each iteration. The initial approximation is
obtained by the ordinary MATLAB operations (giving approximately 16 correct decimals).
See more details in the Mulprec library. The square root algorithm is division-free.
At present, some limitations of Mulprec are set by the restriction of the length of the
shorter operand of a multiplication to at most 90 gytes. It does not seem to be very difﬁcult
to remove these bounds, or at least to widen them considerably.
No.
m-ﬁle
Function
Operation
1*
add.m
z = add(x,y)
z = x + y
2a*
sub.m
z = sub(x,y)
z = x −y
2b
subb.m
z = subb(x,y)
z = x + (−y),
3*
mul.m
z = mul(x,y,s)
z = x · y, s optional
4
recip.m
z = recip(x,s)
z = 1/x
5
div.m
z = div(x,y,s)
z = x/y
6
mi.m
z = mi(x)
z = −x
7*
musqrt.m
[y,iny] = musqrt(x,s)
returns (√x)±1

B-4
Online Appendix B. A MATLAB Multiple Precision Package
B.1.4
Special Mulprec Operations
The operation chop.m is more general than just chopping to a desired length; see the code in
the Mulprec library. The normalization code rnize still has a bug that violates the uniqueness.
It can happen, e.g., that the last two gytes of a positive number read, say, −1 9999634. Such
nine-sequencesmayalsooccuratotherplacesinthevector. Sometimessucharepresentation
is more easily interpreted than a strictly normalized Mulprec number. I have therefore not
yet tried to eliminate this bug.
No.
m-ﬁle
Function
Operation
8
npr.m
xx = npr(x)
converts real ﬂoat to normalized
Mulprec number
9
npc.m
xx = npc(x)
converts complex ﬂoat to normalized
Mulprec number
10
ﬂo.m
y = flo(x)
approximates Mulprec number by ﬂoat
11*
chop.m
y = chop(x,s)
returns approximately equivalent
Mulprec number, length s
12*
rnize.m
y = rnize(x)
normalizes real Mulprec number
13
nize.m
y = nize(x)
normalizes complex Mulprec number
14
elizer.m
y = elizer(x)
eliminates zero gytes in Mulprec
number, left and right
15
muzero.m
y = muzero(x)
if x = 0, y = 1, else y = 0
B.2
Function and Vector Algorithms
B.2.1
Elementary Functions
In the computation of ex, x real, we ﬁrst seek ¯x and an integer n such that
ex = e ¯xP n
and
|¯x| < 1
2 log P.
Then, for an appropriate integer m, e ¯x/2m is computed by the k −1 term of the Maclaurin
expansion, a Horner scheme with variable precision. e ¯x is then obtained by squaring the
sum of the Maclaurin expansion m times.
By a combination of heuristic theory and experiment it has been found that the volume
of computation is proportional to m + ck, where c = 0.4. At each call, the code computes
approximately optimal values of the parameters m and k from a formula for ﬁnding the
minimum of m + ck with the constraint that the bound for the relative error of e ¯x/2m, due to
the Maclaurin truncation and the squarings, does not exceed P −s.
A similar idea is applied for eix. Now ¯x ∈[−8π, 8π], and we use k −1 terms of the
Taylor expansion into powers of x/2m. These methods are inspired from ideas developed
by Napier and Briggs when they computed the ﬁrst tables of logarithms; see Goldstine [1,
Sec. 1.3].
The algorithms in lnr.m and muat2.m are based on Newton’s method for the equations
ey = x and tan y = x, respectively, with initial approximations from the MATLAB opera-

B.2. Function and Vector Algorithms
B-5
tions ln x and atan2(y, x). The commands muat2, lnc, and mulog do not yet allow ﬂoats as
input, and the codes are not well tested.
No.
m-ﬁle
Function
Operation
16*
expo.m
y = expo(x,s)
y = ex, x real
17*
expi.m
[cox,six,eix] = expi(x,s)
cos x and (optionally)
sin x, eix, x real
18
muexp.m
w = muexp(z,s)
y = ez, z complex;
not yet implemented
19*
lnr.m
y = lnr(x,s)
y = log z, x > 0
20*
muat2.m
v = muat2(y,x,s)
adapted from atan2(y, x);
not yet with ﬂoat input
21a
lnc.m
w = lnc(z,s)
w = log z, z ̸= 0;
not yet with ﬂoat input
21b
mulog.m
w = mulog(z,s)
a better name for 21a
B.2.2
Mulprec Vector Algorithms
A Mulprec column vector is represented by a (MATLAB) rectangular matrix. A Mulprec
row vector is a row of Mulprec numbers (where each Mulprec number is a row of gytes).
In a rectangular Mulprec matrix, each column is a Mulprec column vector, and each row
is a Mulprec row vector. Thus, we can say that a Mulprec matrix is a row of rectangular
(MATLAB) matrices, all of the same size. The following set of operations is very prelim-
inary. The function ﬁxcom.m returns, if possible, a Mulprec number (or Mulprec vector)
with the same ﬁxed comma representation (i.e., same exponent and size) as the normalized
Mulprec number a.
These vector functions were worked out for an application to repeated Richardson h2
extrapolation; see the m-ﬁle rich3.m.
No.
m-ﬁle
Function
Operation
30*
ﬁxcom.m
y = fixcom(x,a)
x, y Mulprec vectors; returns
y ≈x, y(1) = a(1),
length(y(i)) = length(a)
31*
musv.m
y = musv(sca,vec)
sca is Mulprec scalar, vec is
Mulprec vector y = sca · vec
32*
scalp.m
y = scalp(vec1,vec2)
scalar prod. in Euclidean space,
vec1, vec2 Mulprec column vectors
33
adv.m
z = adv(x,y)
z = x + y; x, y, z Mulprec vectors
34
rnizev.m
y = rnizev(x)
normalizes real Mulprec vector
35
chopv.m
y = chopv(x,s)
chops components of Mulprec
vector to length s
36
chonizv.m
y = chonizv(x,s)
normalizes and chops a Mulprec
vector

B-6
Online Appendix B. A MATLAB Multiple Precision Package
B.2.3
Miscellaneous
No.
m-ﬁle
Operation
50*
intro.m
starting routine for Mulprec
51*
rich3.m
Mulprec algorithm for repeated
Richardson h2 extrapolation
52*
polygons.m
compute circumference for a
sequence of polygons; calls rich3.m
B.2.4
Using Mulprec
A tar ﬁle named Mulprec.tar containing the Mulprec m-ﬁles can be downloaded from
the homepage of the book at www.siam.org/books/ot103. This tar ﬁle also contains
some edited diaries of a few test experiments (comparisons of computations with differ-
ent precision), e.g., pippi2.dia (π computed by polygons.m and rich3.m), muat2est.dia
(π = 4 arctan 1), etest.dia (e computed by expo.m). It can be easily unpacked so that the
separate ﬁles become accessible.
To start Mulprec, change the directory to the seat of the Mulprec ﬁles. Start MATLAB
and run intro.m. (If you forget this, you are likely to obtain confusing error messages.
Ignore them and run intro.m!) Then intro.m loads the ﬁle const.mat from the disk. The ﬁle
const.mat contains, e.g., 50 gytes Mulprec approximations to π (called pilong) and to ln P,
P = 107 (called LP), and the default values of some other global variables. Now MATLAB
is ready for your Mulprec adventures. The ﬁrst time, you may type “whos” and then type
the constants.
It should be kept in mind that the m-ﬁles are preliminary versions. Some of them are
not thoroughly tested, although most of them were used in successful computations of π to
up to 330 decimal places.
Computer Exercises
B.1. Make up and run some simple examples with several choices of the parameter s
such that you can easily check the accuracy of the result. For example: 1/7,
√
0.75,
sin(π/3), e, 4 arctan 1. (Compare also the calculations in the dia ﬁles.)
B.2. Aregular n-sided polygon inscribed in a circle with radius 1 has circumference 2an =
2n sin(π/n). If we put h = 1/n, then
c(h) = cn = 1
h sin(πh) = 2π −π3
3 h2 + π5
60 h4 −. . . ,
and thus cn satisﬁes the assumptions for repeated Richardson extrapolation with pk =
2k.
A recursion formula that leads from cn to c2n is given in Example 3.4.13. The script
ﬁle polygons.m uses a similar recursion after the substitutions
n = 6 2m−1,
m = 1 : M, M ≤36,
cn = p(m + 1),
q = p/n.

Computer Exercises
B-7
The script polygons.m then calls the function rich3.m that performs Richardson ex-
trapolations until the list of M polygons is exhausted or the sequence of estimates of
the limit 2π ceases to be monotonic.
Choose a suitable M, M ≤36, and call polygons.m. Compare with the diary ﬁle
pippi2.dia that contains previous runs of this. Study the elapsed time.
B.3. Write a code for the summation of an inﬁnite series by Euler–Maclaurin’s summation
formula, assuming that convenient algorithms exist for the integral and for derivatives
of arbitrary order. Consider also how to handle generalized cases where a limit is asked
for, rather than a sum, e.g., Stirling’s asymptotic expansion for log(T(z)) or the Euler
constant γ .
The numerators and denominators of some Bernoulli numbers B2n, n = 1 : 17, are
found in the ﬁle const.dia, in the vectors B2nN and B2nD, respectively. B0 = 1 and
B1 = −1/2 are given separately.
B.4. An interesting table of mathematical constants (40 decimal places) is given in Knuth
[2, Appendix A]. Compute a few of them to much higher accuracy. For some of them
an estimate of the accuracy may be most easily obtained by comparing results obtained
using different values of the parameter s. (Compare also the given diary ﬁles.) Some
of the constants may require some version of the Euler–Maclaurin formula; see Exer-
cise B.3 above. Incorporate them to your const.mat if they are interesting. T(1/3) and
−ζ ′(2) (the derivative of Riemann’s ζ-function) seem to be relatively advanced tasks.
B.5. Write and test a code for the product of a Mulprec matrix by a Mulprec vector. In-
corporate it into your Mulprec library.
B.6. (a) Implement an operation tentatively called mullong.m (z = mul(x,y,s)) that
can handle a multiplier by partitioning it into 90-gyte pieces and calling mul.m once
for each piece. Do something about the consequences of this for expo.m, if you want
to try Exercise B.8.
(b) Implement an operation called muabs.m (z = |x|) for computing the absolute
value of a real or complex Mulprec number.
B.7. Write a Mulprec analogue to the MATLAB command rat for ﬁnding accurate (or
exact) rational approximations to ﬂoating-point results, in connection with the basic
operations of exact rational arithmetic and continued fractions, including gcd and lcm.
(See Knuth [2, Sec. 4.5.2, in particular p. 327]. Mulprec can, of course, not compete
with Maple and similar systems for rational arithmetic. Minor tasks of this type may,
however, appear in a context where Mulprec is used.
B.8. Poisson’s summation formula reads, in the case f (t) = e−t2h2 with the Fourier trans-
form ˆf (ω) = (√π/h)e−ω2/(4h2),
h
N

n=−N
e−n2h2 = √π
K−1

k=−K+1
e−π2k2/h2 + Rh,N,K,choppings.
This particular case is also known as the theta transformation formula. To our (lim-
ited) knowledge, it has not been previously applied to the high precision computation
of good old π.

B-8
Online Appendix B. A MATLAB Multiple Precision Package
Suppose that you want to compute √π to an extreme accuracy by letting your desk
computer work over a weekend with the use of Mulprec (with a few amendments).
For a given (appproximate) bound for Rh,N,K,choppings, determine a good choice of the
parameters h, N, K and the parameter s in the various terms. Estimate roughly the
relation of computing time to error. Exercise B.6 must have been resolved, at least in
principle, before you can solve this.
Before you make a full scale experiment, make sure that neither your computer—nor
your ofﬁce—will be a ruin when you return after the weekend.
Hints: Note that the function evaluation can be arranged as a set of recursion formulas
with basic arithmetic operations only. We believe that only two or three evaluations
of the exponential will be needed in the whole computation.
Leave the door open for the use of variable precision, although this may not reduce
the computing time by a terriﬁc amount in this exercise.
Note that π appears in several places in the equation. Think of the computation as an
iterative process (although in practice one iteration is perhaps enough).
B.9. Implement the qd algorithm and apply it to the classical ill-conditioned problem to
transform a power series to a continued fraction or, equivalently, ﬁnding the coefﬁ-
cients of the three-term recurrence relation of the orthogonal polynomials for a weight
distribution, the moments of which are given (to an enormous number of decimal
places).
M U L P R E C
G I V E S
Y O U
A L L
T H E
T H I N G S
Y O U
D O
N O T
N E E D.
Bibliography
[1] Herman H. Goldstine. A History of Numerical Analysis from the 16th through the
19th Century. Stud. Hist. Math. Phys. Sci., Vol. 2. Springer-Verlag, New York, 1977.
(cited on p. B-4)
[2] Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical
Algorithms. Addison-Wesley, Reading, MA, third edition, 1998. (Cited on pp. B-3,
B-7.)

Online Appendix C
Guide to Literature
C.1
Introduction
For many readers numerical analysis is studied as an important applied subject. Therefore,
we shall give here a more complete overview of the literature than is usually given in
textbooks. We restrict ourselves to books written in English. Further, only a restricted
guide to literature on numerical linear algebra and differential equations is given here.
These topics will be covered elsewhere.
The bibliography which we give is by necessity a subjective selection and by no
means complete. We hope, however, that it can serve as a good guide for a reader who out
of interest (or necessity!) wishes to deepen his or her knowledge.
Besides the reference periodicals such as Mathematical Reviews, SIAM Review and
Mathematics of Computation publish extensive reviews of most books of interest. The
weekly electronic newsletter NA Digest is a valuable source of information about current
topics. To subscribe and for archives visit www.netlib.org/na-net.
MathSciNet (http://www.ams.org/mathscinet), an electronic index, is the ba-
sic source for mathematics published in the United States, with coverage on an interna-
tional scope beginning in 1940. Web of Science (http://scientific.thomsom.com/
products/wos) is an electronic multidisciplinary index in which mathematical journals
are indexed. Coverage starts with 1945.
C.2
Textbooks in Numerical Analysis
We restrict ourselves to books published after 1965. Avaluable source book to the literature
before 1956 is Parke [25]. An interesting account of the history of numerical analysis from
the sixteenth through the nineteenth centuries can be found in Goldstine [13].
Many outstanding textbooks in numerical analysis were originally published in the
1960s and 70s. The classical text by Hildebrand [18] can still be used as an introduction.
IsaacsonandKeller[19]givearigorousmathematicaltreatmentofclassicaltopics, including
differential equations and orthogonal polynomials. The authors’ previous textbook [6] was
used at many universities in the USA and is still available.
C-1

C-2
Online Appendix C. Guide to Literature
Hamming [16] is a more applied text and aims at combining mathematical theory,
heuristic analysis, and computing methods. It emphasizes the message that the purpose
of computing is insight, not numbers. An in-depth treatment of several areas such as nu-
merical quadrature and approximation is found in the comprehensive book by Ralston and
Rabinowitz [29]. This book also contains a great number of interesting and fairly advanced
problems.
A popular introductory textbook is Fröberg [11], a revised and updated version of an
earlier Swedish textbook. The book by Forsythe, Malcolm, and Moler [10] is notable in
that it includes a set of Fortran subroutines of unusually high quality. It is not surprising
to note that Cleve Moler, the creator of MATLAB, is one of the coauthors. Conte and de
Boor [5] is a well-written introductory text, with a particularly good treatment of polynomial
approximation.
Kahaner, Moler, and Nash [20] comes with a disk containing software. A good in-
troduction to scientiﬁc computing is given by Golub and Ortega [14]. A matrix-vector
approach is used in the MATLAB-oriented text of Van Loan [35]. Analysis is comple-
mented with computational experiments using a package of more than 200 m-ﬁles. Another
good introductory text using MATLAB is Eldén, Wittmeyer-Koch, and Nielson [9]. Cheney
and Kincaid [4] is an undergraduate text with many examples and exercises. Kincaid and
Cheney [21] is a related textbook but more mathematically oriented.
Heath [17] is a popular, more advanced, comprehensive text. Gautschi [12] is an
elegant introductory text containing a wealth of computer exercises. Much valuable and
hard-to-ﬁnd information is included in notes after each chapter. The bestseller by Press et
al. [27] gives a good survey of contemporary numerical methods for the applied scientist,
but is weak on analysis.
Several good textbooks have been translated from German, notably the excellent
book by Stoer and Bulirsch [32]. This is particularly suitable for a reader with a good
mathematical background. Deuﬂhard and Hohmann [8] is less comprehensive but with
a modern and careful treatment. Schwarz [31] is a mathematically oriented text which
also covers ordinary and partial differential equations. Rutishauser [30] is an annotated
translation of a highly original textbook by one of the pioneers of numerical analysis.
Although brief, the book by Tyrtychnikov [33] is original and thorough. It also contains
references to Russian literature unavailable in English.
Ueberhuber [34] is a two-volume introduction to numerical computation, which also
emphasizessoftwareaspects. Itisnotableforitsthoroughcoverageofnumericalintegration.
A modern textbook which covers iterative methods for linear systems as well as ordinary
and partial differential equations is Quarteroni, Sacco, and Saleri [28].
The problem of adapting algorithms to run efﬁciently on modern parallel computer
architecture is an important and nontrivial part of scientiﬁc computing. The textbook by
Petersen and Arbenz [26] gives a good introduction to this topic.
We ﬁnally mention two standard references. For topics on linear spaces and approxi-
mation theory the comprehensive monograph by Davis [7] is invaluable. For a background
in modern analysis including analytic functions, the classical textbook by Apostol [2] is
recommended.
[1] F. S. Acton. Numerical Methods That (Usually) Work, Harper and Row, New York,
1970, 541pages.ReprintedbytheMathematicalAssociationofAmerica,Washington,
DC, 1990.

C.2. Textbooks in Numerical Analysis
C-3
[2] Tom M. Apostol. Mathematical Analysis. Addison Wesley, Reading, MA, second
edition, 1974. ISBN-13 0201002881. (Cited on p. C-2.)
[3] Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley, New York,
second edition, 1989, 712 pages. ISBN 0-471-62489-6
[4] Ward Cheney and David Kincaid. Numerical Mathematics and Computing.
Brooks/Cole, Paciﬁc Grove, CA, fourth edition, 1999. 562 pages. ISBN 0-534-04356-
9 (Cited on p. C-2.)
[5] Samuel D. Conte and Carl de Boor. Elementary Numerical Analysis. An Algorithmic
Approach. McGraw-Hill, New York, third edition, 1980, 432 pages. ISBN 0-07-
012447-7 (Cited on p. C-2.)
[6] Germund Dahlquist and Åke Björck. Numerical Methods. Prentice-Hall, Englewood
Cliffs, NJ., 1974, 573 pages. Republished by Dover, Mineola, NY, 2003. ISBN 0-
486-42807-9 (Cited on p. C-1.)
[7] Philip J. Davis. Interpolation and Approximation. Blaisdell, New York, 1963. (Cited
on p. C-2.)
[8] Peter Deuﬂhard and Andreas Hohmann. Numerical Analysis in Modern Scientiﬁc
Computing. Springer, Berlin, second edition, 2003, 337 pages. ISBN 0-387-95410-4
(Cited on p. C-2.)
[9] Lars Eldén, Linde Wittmeyer-Koch, and Hans Bruun Nielsen. Introduction to Nu-
merical Computation. Studentlitteratur, Lund, Sweden, 2004, 375 pages. ISBN 91-
44-03727-9 (Cited on p. C-2.)
[10] George E. Forsythe, MichaelA. Malcolm, and Cleve B. Moler. Computer Methods for
Mathematical Computations. Prentice-Hall, Englewood Cliffs, NJ, 1977, 259 pages.
ISBN 0-13-165332-6 (Cited on p. C-2.)
[11] Carl-Erik Fröberg. Numerical Mathematics. Theory and ComputerApplications. Ben-
jamin/Cummings, Menlo Park, CA, 1985, 436 pages. ISBN 0-8053-2530-1 (Cited
on p. C-2.)
[12] Walter Gautschi. Numerical Analysis. An Introduction. Birkhäuser, Boston, MA,
1997, 506 pages. ISBN 0-8176-3895-4 (Cited on p. C-2.)
[13] Herman H. Goldstine. A History of Numerical Analysis from the 16th through the
19th Century. Stud. Hist. Math. Phys. Sci., Vol. 2. Springer-Verlag, New York, 1977,
348 pages. ISBN 0-387-90237-5. (Cited on p. C-1.)
[14] Gene H. Golub and James M. Ortega. Scientiﬁc Computing and Differential Equa-
tions. An Introduction to Numerical Methods.Academic Press, San Diego, CA, 1992,
337 pages. ISBN 0-12-289253-4 (Cited on p. C-2.)
[15] Gene H. Golub and James M. Ortega. Scientiﬁc Computing. An Introduction with
Parallel Computing. Academic Press, San Diego, CA, 1993, 442 pages. ISBN 0-12-
289255-0
[16] Richard W. Hamming. Numerical Methods for Scientists and Engineers. McGraw-
Hill, New York, second edition, 1974, 721 pages. Republished by Dover, Mineola,
NY, 1986. ISBN 0-486-65241-6 (Cited on p. C-2.)

C-4
Online Appendix C. Guide to Literature
[17] Michael T. Heath. Scientiﬁc Computing. An Introductory Survey. McGraw-Hill,
Boston, MA, second edition, 2002, 563 pages. ISBN 0-07-239910-4
(Cited on
p. C-2.)
[18] F. B. Hildebrand. Introduction to Numerical Analysis. McGraw-Hill, New York, sec-
ond edition, 1974, 669 pages. Republished by Dover, Mineola, NY, 1987. ISBN
0-486-65363-3 (Cited on p. C-1.)
[19] Eugene Isaacson and Herbert B. Keller. Analysis of Numerical Methods. John Wiley,
New York, 1966, 541 pages. Republished by Dover, Mineola, NY, 1994. ISBN 0-
486-63029-0 (Cited on p. C-1.)
[20] David Kahaner, Cleve B. Moler, and Stephen G. Nash. Numerical Methods and Soft-
ware. Prentice-Hall, Englewood Cliffs, NJ, 1989, 495 pages + disk.
(Cited on
p. C-2.)
[21] David Kincaid and Ward Cheney. Numerical Analysis. Brooks/Cole, Paciﬁc Grove,
CA, third edition, 2002, 804 pages. ISBN 0-534-38905-8. (Cited on p. C-2.)
[22] Cornelius Lanczos. Applied Analysis. Prentice-Hall, Englewood Cliffs, NJ, 1956, 559
pages. Republished by Dover, Mineola, NY, 1988. ISBN 0-486-65656-0
[23] G. I. Marchuk. Methods in Numerical Mathematics. Springer-Verlag, Berlin, second
edition, 1982, 510 pages. ISBN 0-387-90614-2
[24] James Ortega. Numerical Analysis: A Second Course. Academic Press, New York,
1972. Republished by SIAM, Philadelphia, PA, 1990. ISBN 0-89871-5.
[25] N. G. Parke. Guide to the Literature of Mathematics and Physics. Dover, Mineola,
NY, second edition, 1958. (Cited on p. C-1.)
[26] W. P. Petersen and P. Arbenz. Introduction to Parallel Computing. A Practical Guide
with Examples in C. Oxford Texts in Applied and Engineering Mathematics. Oxford
University Press, Oxford, UK, 2004. ISBN 0 19 851577 4. (Cited on p. C-2.)
[27] William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling.
Numerical Recipes in Fortran 77: The Art of Scientiﬁc Computing. Cambridge Uni-
versity Press, Cambridge, UK, second edition, 1992. (Cited on p. C-2.)
[28] Alﬁo Quarteroni, Riccardo Sacco, and Fausto Saleri. Numerical Mathematics.
Springer-Verlag, New York, 2000, 654 pages. ISBN 0-387-98959-5 (Cited on p. C-
2.)
[29] Anthony Ralston and Philip Rabinowitz. A First Course in Numerical Analysis.
McGraw-Hill, New York, second edition, 1978. Republished by Dover, Mineola,
NY, 2001. ISBN 0-486-41454-0 (Cited on p. C-2.)
[30] Heinz Rutishauser. Lectures on Numerical Mathematics Band 1 & 2. Birkhäuser,
Boston, MA, 1990, 164 + 228 pages. (Cited on p. C-2.)
[31] H.-R. Schwarz. Numerical Analysis: A Comprehensive Introduction. John Wiley,
New York, 1989, 517 pages. ISBN 0-471-92064-9 (Cited on p. C-2.)
[32] Joseph Stoer and Roland Bulirsch. Introduction to Numerical Analysis. Springer-
Verlag, New York, third edition, 2002, 744 pages. (Cited on p. C-2.)

C.3. Handbooks and Collections
C-5
[33] E. Tyrtychnikov, A Brief Introduction to Numerical Analysis. Birkhäuser, Boston,
1997. ISBN: 0-8176-3916-0 6501 (Cited on p. C-2.)
[34] Christoph W. Ueberhuber. Numerical Computation: Methods, Software, and Anal-
ysis. Volumes 1 & 2. Springer-Verlag, Berlin, 1997, 474 + 495 pages. ISBN 3-540-
62058-3, 3-540-62057-5 (Cited on p. C-2.)
[35] Charles F. Van Loan. Introduction to Scientiﬁc Computing. Prentice-Hall, Upper Sad-
dle River, NJ, second edition, 2000 367 pages. ISBN 0-13-125444-8
(Cited on
p. C-2.)
C.3
Handbooks and Collections
A series of conferences on the State of the Art in Numerical Analysis were organized by
the Institute of Mathematics and Its Applications with the aim of accounting for recent
developments in the ﬁeld. Proceedings from the last two held in 1986 (Iserles and Powell [5])
and 1996 (Duff and Watson [3]) contain excellent surveys of many topics.
Another source of survey articles on topics of current interest can be found in Acta
Numerica, a Cambridge University PressAnnual started in 1992. The journal SIAM Review
also publishes high-quality review papers.
The Journal of Computational and Applied Mathematics published in Volumes 121–
128 (2000–2001) a series of papers on “Numerical Analysis of the 20th Century,” with the
aim of presenting the historical development of numerical analysis and reviewing current
research. The papers were arranged in seven-volume book set:
Vol. I: Approximation Theory.
Vol. II: Interpolation and Extrapolation.
Vol. III: Linear Algebra.
Vol. IV: Nonlinear Equations and Optimisation.
Vol. V: Quadrature and Orthogonal Polynomials.
Vol. VI: Ordinary Differential Equations and Integral Equations.
Vol. VII: Partial Differential Equations.
A collection of outstanding survey papers on special topics are being published in a
multivolume sequence in the Handbook of Numerical Analysis [2], edited by Philippe G.
Ciarlet and Jacques-Louis Lions. It offers comprehensive coverage in all areas of numerical
analysis as well as many actual problems of contemporary interest. Each volume concen-
trates on one to three particular subjects under the following headings:
• Solution of Equations in Rn.
• Finite Difference Methods.
• Finite Element Methods.
• Techniques of Scientiﬁc Computing.
• Optimization Theory and Systems Science.
• Numerical Methods for Fluids.
• Numerical Methods for Solids.

C-6
Online Appendix C. Guide to Literature
Other topics covered include meteorology, seismology, petroleum mechanics and celestial
mechanics. By 2005, 13 volumes had appeared.
We also mention here some additional useful references which do not ﬁt into any of the
above categories. Knuth [6] is the ultimate reference on number systems and arithmetic. The
book by Gander and Hˇrebiˇcek [4] contains a collection of well-chosen problems in scientiﬁc
computing and their solutions via modern software tools like MATLAB and Maple. Another
collection of solved problems which is entertaining and highly instructive is contained in
The SIAM 100-Digit Challenge [1]. Strang [7] gives an excellent modern introduction to
applied mathematics.
[1] Folkmar Bornemann, Dirk Laurie, Stan Wagon, and Jörg Waldvogel. The SIAM 100-
Digit Challenge. A Study in High-Accuracy Numerical Computing. SIAM, Philadel-
phia, PA, 2004. (Cited on p. C-6.)
[2] Philippe G. Ciarlet and Jacques Louis Lions. Handbook of Numerical Analysis, vol-
ume I–XIII. North-Holland, Amsterdam, 1990–2005. (Cited on p. C-5.)
[3] Iain S. Duff and G.Alistair Watson, editors. The State of the Art in Numerical Analysis.
Clarendon Press, Oxford, 1997. (Cited on p. C-5.)
[4] W. Gander and J. Hˇrebiˇcek. Solving Problems in Scientiﬁc Computing using Maple
and Matlab. Springer-Verlag, Berlin, fourth edition, 2004. (Cited on p. C-6.)
[5] Arieh Iserles and M. J. D. Powell, editors. The State of the Art in Numerical Analysis.
Clarendon Press, Oxford, 1987. (Cited on p. C-5.)
[6] Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical
Algorithms. Addison-Wesley, Reading, MA, third edition, 1998. (Cited on p. C-6.)
[7] Gilbert Strang. Introduction to Applied Mathematics. Wellesley-Cambridge Press,
Wellesley, MA, 1986. (Cited on p. C-6.)
C.4
Encyclopedias, Tables, and Formulas
Although mathematical tables no longer play the same role in computing as they used to,
occasionally they can be an aid in checking or planning calculations on a computer. Fletcher,
Miller, and Rosenhead [3] list nearly every table of mathematical functions published up
to approximately 1959. Many excellent tables were produced in Russia; see Lebedev and
Federova [7].
Advice about the use and choice of tables is given in Todd [13, pp. 93–106]. He
distinguishesbetweentablesthatshouldbeonone’sdeskandthosewhichshouldbeavailable
in a library. Among tables in the ﬁrst category is Comrie [2]. Comrie’s book also contains
a handy collection of useful formulas for trigonometric and hyperbolic functions. The
tables of higher functions by Jahnke, Emde, and Lösch [5] were ﬁrst published in 1909.
This collection is now dated, but contains some amazing hand calculated two- and three-
dimensional graphs showing the behavior of different functions in the complex plane. A
modern version is the Atlas of Functions [12], which contains computer-generated color
graphs of families of functions. It comes with a CD containing software that can provide
values of functions in the Atlas.
The most comprehensive source of information on useful mathematical functions and
formulas is the Handbook of Mathematical Functions, with Formulas, Graphs, and Mathe-

C.4. Encyclopedias, Tables, and Formulas
C-7
matical Tables, edited by MiltonAbramowitz and Irene Stegun [1]. This was ﬁrst published
in 1964 by the National Bureau of Standards (renamed National Institute of Standards and
Technology (NIST) in 1988), and more than 150,000 copies have been sold. Although
still available and among one of the most cited references, it is becoming increasingly out
of date. A replacement more suited to today’s needs is being developed at NIST. This is
planned to be made available soon both in print and as a free electronic publication on the
World Wide Web; see the mockup version at http://dlmf.nist.gov.
An outline of the features of new NIST Digital Library of Mathematical functions
is given by D. W. Lozier [9]. The internet version will come with hyperlinks, interactive
graphics, and tools for downloading and searching. The part of the old Handbook devoted to
massive tables of values will be superseded. To summarize, data-intensive and operation-
preserving methods are replaced by data-conserving and operation-intensive techniques.
A good overview of software for mathematical special functions is given by Lozier and
Olver [8].
The James & James Mathematics Dictionary [6] is a high-quality general mathe-
matics dictionary that covers arithmetic to calculus and includes a multi-lingual index.
CRC Concise Encyclopedia of Mathematics [14] by Eric Weisstein is a comprehensive
compendium of mathematical deﬁnitions, formulas, and references. A free Web ency-
clopedia containing surveys and references is Eric Weisstein’s World of Mathematics at
mathworld.wolfram.com.
[1] MiltonAbramowitzandIreneA.Stegun(eds.).HandbookofMathematicalFunctions.
Dover, New York, NY, 1965. (Cited on p. C-7.)
[2] L. J. Comrie. Chamber’s Shorter Six-Figure Mathematical Tables.W. & R. Chambers,
Edinburgh, 1968. (Cited on p. C-6.)
[3] A. Fletcher, J. C. P. Miller, and L. Rosenhead. Index of Mathematical Tables. Two
volumes. Blackwell’s, Oxford, UK, second edition, 1962. (Cited on p. C-6.)
[4] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series and Products. Academic
Press, London, UK, ﬁfth edition, 1993.
[5] Eugene Jahnke, Fritz Emde, and Friedrich Lösch. Tables of Higher Functions.
McGraw-Hill, New York, sixth edition, 1960. (Cited on p. C-6.)
[6] Glenn James and Robert C. James, editors. James & James Mathematics Dictionary.
Van Nostrand, Princeton, NJ, ﬁfth edition, 1992. (Cited on p. C-7.)
[7] A. V. Lebedev and R. M. Federova. A Guide to Mathematical Tables. Van Nostrand,
New York, 1960. (Cited on p. C-6.)
[8] D. W. Lozier and F. W. J. Olver. Numerical evaluation of special functions. In
W. Gautschi, editor, Mathematics of Computation 1943–1993: A Half-Century of
Computational Mathematics, volume 48 of Proc. Symposium Appl. Math., pages
79–125, Amer. Math. Soc., Providence, RI, 1994. (Cited on p. C-7.)
[9] Daniel W. Lozier NIST digital library of mathematical functions. Annals of Mathe-
matics and Artiﬁcial Intelligence, 38:105–119, 2003. (Cited on p. C-7.)
[10] A. P. Prudnikov, Y. A. Brychkov, and O. I. Marichev. Integrals and Series. Volume 1:
Elementary Functions. Gordon and Breach, New York, 1986.

C-8
Online Appendix C. Guide to Literature
[11] A. P. Prudnikov, Yu. A. Brychkov, and O. I. Marichev. Integrals and Series. Volume
2: Special Functions. Gordon and Breach, New York, 1986.
[12] Jerome Spanier, Keith B. Oldham, and Jan Myland. An Atlas of Functions. Springer-
Verlag, Berlin, second edition, 2008. (Cited on p. C-6.)
[13] John Todd, editor. Survey of Numerical Analysis, McGraw-Hill, New York, 1962.
(Cited on p. C-6.)
[14] Eric W. Weisstein, editor. CRC Concise Encyclopedia of Mathematics. CRC Press,
Boca Raton, FL, 1999. (Cited on p. C-7.)
C.5
Selected Journals
We list here a selection of journals covering the areas of numerical analysis and mathemati-
cal software. Most journals are now available in electronic form. There are several journal
platforms: Science Direct for Elsevier’s journals; Springer Link for Springer’s journals, in-
cluding Lecture Notes in Mathematics; and Wiley Interscience for Wiley’s electronic journal
platform. The SIAM journals are published by the Society for Industrial andApplied Math-
ematics and are available electronically at epubs.siam.org. JSTOR at www.jstor.org
contains back issues of several numerical journals, e.g., Mathematics of Computation, and
some SIAM journals.
The early issues of The Computer Journal contain many classical papers, but lately
few papers are relevant to numerical methods.
ACM Transactions on Mathematical Software (1975–), Association for
Computing Machinery.
Acta Numerica (1992–), Cambridge University Press.
Applied Numerical Mathematics (1985–), Elsevier.
BIT Numerical Mathematics (1961–), Springer.
Calcolo (1964–), Springer Milan.
Chinese Journal of Numerical Mathematics and Applications (1979–),
Allerton Press.
Computing (1966–), Springer Wien.
IMA Journal on Numerical Analysis (1981–), Oxford University Press.
Journal of Computational and Applied Mathematics (1975–), North–Holland.
Linear Algebra and Its Applications (1968–), Elsevier.
Mathematics of Computation (1960–), American Mathematical Society;
previously called Mathematical Tables and Other Aids to Computing (1943–1959).
Numerical Algorithms (1991–), Springer.
Numerical Linear Algebra with Applications (1994–), Wiley Interscience.
Numerische Mathematik (1959–), Springer.

C.6. Algorithms and Software
C-9
SIAM Journal on Matrix Analysis and Applications (1988–), SIAM.
SIAM Journal on Numerical Analysis (1996–), SIAM.
SIAM Journal on Scientiﬁc Computing (1994–), SIAM; previously called SIAM Journal on
Scientiﬁc & Statistical Computing (1980–1993).
SIAM Review (1959–), SIAM.
The Computer Journal (1959–), The British Computer Society.
Several journals only available in electronic form are also of interest. Electronic
Transactions on Numerical Analysis (ETNA) is available at etna.math.kent.edu and
at several mirror sites. Electronic Journal of Linear Algebra is available at www.math.
technion.ac.il/iic/ela/ and at several mirror sites.
C.6
Algorithms and Software
Some principal questions concerning the production of software for mathematical compu-
tation are discussed by Rice [3, 4].
Starting in the 1960s much general purpose software, often collected in large libraries
or packages, has been developed. There are two large suppliers of commercial scientiﬁc
subroutine libraries. Numerical Algorithms Group (NAG), based in England, started in
1970 as an interuniversity collaboration; see www.nag.co.uk. IMSL, now part of Visual
Numerics (www.vni.com) was founded about the same time in the USA. Both companies
offer a wide range of software in Fortran and C.
MATLAB, developed by The Mathworks (www.mathworks.com), is a widespread
interactive system for matrix computations. It offers many “toolboxes” for speciﬁc applica-
tion areas, e.g., control problems. Maple, developed by Maplesoft (www.maplesoft.com),
offers symbolic computation. It is available as a MATLAB toolbox, which makes it possible
to combine numerical and symbolic computations.
Mathematica, developed by Wolfram Research (www.wolfram.com), also integrates
numerical and symbolical computing. It can handle symbolical calculations that involve
hundreds of thousands of terms. Wolfram Research also sponsors the free resource Math-
World (www.mathworld.com), an encyclopedia of mathematics on the Web. COMSOL
(www.comsol.com), started by students of the ﬁrst author, has developed a unique modular
software package for multiphysics simulation.
The book Numerical Recipes in Fortran 77, by Press et al. (see Sec. C.2) exists in
several other versions with programs in Fortran 90, Pascal, C++, and C. The programs
printed in the book are, however, for reading purposes only, and the copyrighted software is
sold separately and can be downloaded from an online store. The book [2] is a complement
to the original Fortran 77 book. It contains reworked versions of all the original codes
adapted to the parallel computing facilities in Fortran 90.
Many programs and packages are available in the public domain and can be down-
loaded for free. A prime example is LAPACK, which superseded LINPACK and EISPACK
inthemid-1990s, andcontainsprogramsforsolvinglinearsystemsandeigenvalueproblems.
Other packages, such as DASSL, are available for solving ordinary systems of differential
equations.

C-10
Online Appendix C. Guide to Literature
The National Institute of Standards andTechnology (NIST) Guide toAvailable Mathe-
matical Software (GAMS) is available at gams.nist.gov. GAMS is an online cross-index
of mathematical and statistical software providing abstracts, documentation, and source
codes of software modules, and it provides access to multiple repositories operated by oth-
ers. Currently four repositories are indexed, three within NIST and one on Netlib. Both
public domain and proprietary software are indexed although source codes of proprietary
software are not redistributed by GAMS.
Netlib is a repository of public domain mathematical software, data, address lists,
and other useful items for the scientiﬁc computing community.
Access Netlib is via
www.netlib.org. Background about Netlib is given in [1].
A top-level index which describes the chapters of Netlib, each of which has an indi-
vidual index, ﬁle is available. Note that many of these codes are designed for use by profes-
sional numerical analysts who are capable of checking for themselves whether an algorithm
is suitable for their needs. One routine can be superb and the next awful. So be careful!
[1] Jack. J. Dongarra and Eric Grosse. Distribution of mathematical software via elec-
tronic mail. Comm. ACM, 30:403–407, 1987. (Cited on p. C-10.)
[2] William H. Press, Brian P. Flannery, SaulA. Teukolsky, and William T. Vetterling. Nu-
merical Recipes in Fortran 90: The Art of Parallel Scientiﬁc Computing. Cambridge
University Press, Cambridge, UK, 1996. (Cited on p. C-9.)
[3] John R. Rice, editor. Mathematical Software. Academic Press, New York, 1971.
(Cited on p. C-9.)
[4] John R. Rice. Numerical Methods, Software, and Analysis. Academic Press, Boston-
New York, second edition, 1993. (Cited on p. C-9.)
C.7
Public Domain Software
Below is a selective list indicating the scope of software available from Netlib. The ﬁrst few
libraries here are widely regarded as being of high quality. The likelihood of your encoun-
tering a bug is relatively small; if you do, report it by email to ehg@research.att.com
Algorithms published in ACM Transactions on Mathematical Software can also be
downloaded from www.acm.org/pubs/calgo. As of October 2007, 866 algorithms were
available.
BLAS: Basic Linear Algebra Subprograms levels 1, 2, and 3, and machine constants.
FFTPACK: A package of Fortran programs for the fast Fourier transform of periodic and
other symmetric sequences. This package consists of programs which perform fast Fourier
transforms for both complex and real periodic sequences and certain other symmetric se-
quences. Developed by Paul Swarztrauber, NCAR.
VFFTPK: A vectorized version of FFTPACK for multiple sequences. Developed by Sweet,
Lindgren, and Boisvert.
FISHPACK: A package of Fortran subprograms providing ﬁnite difference approximations
for elliptic boundary value problems. Developed by Paul Swarztrauber and Roland Sweet.
FN: Wayne Fullerton’s special function library (single and double).

C.7. Public Domain Software
C-11
GO: Golden Oldies: Routines that have been widely used but aren’t available through the
standard libraries.
HARWELL: Sparse matrix routine MA28 from the Harwell library made available by Iain
Duff.
LAPACK: For the most common problems in numerical linear algebra such as linear equa-
tions, linear least squares problems, eigenvalue problems, and singular value problems.
Designed to be efﬁcient on a wide range of modern high-performance computers. De-
veloped by Ed Anderson, Z. Bai, Chris Bischof, Jim Demmel, Jack Dongarra, Jeremy Du
Croz,Anne Greenbaum, Sven Hammarling,Alan McKenney, Susan Ostrouchov, and Danny
Sorensen.
PPPACK: Subroutines from the book A Practical Guide to Splines, by Carl de Boor. Some
calling sequences differ slightly from those in the book.
TOMS: Collected algorithms of the ACM. When requesting a speciﬁc item, please refer to
the algorithm number.
In contrast to the above libraries, the following are collections of codes from a variety
of sources. Most are excellent, but you should exercise caution. We include research
codes that we haven’t tested and codes that may not be state of the art but are useful for
comparisons. The following list is alphabetical, not by merit:
AMOS: Bessel and Airy functions of complex arguments and nonnegative order.
BIHAR: Biharmonic solver in rectangular geometry and polar coordinates. Made available
by Petter Bjørstad, University of Bergen.
BMP: Multiple precision package. By Richard P. Brent.
C++: Miscellaneous codes written in C++. Eric Grosse, editor, Bell Laboratories.
C: Miscellaneous codes written in C. Not all C software is in this miscellaneous library. If
it clearly ﬁts into a domain-speciﬁc library, it is assigned there. Eric Grosse, editor.
CONFORMAL: Routines to solve the “parameter problem” associated with conformal map-
ping. Includes SCPACK (polygons with straight sides) from Nick Trefethen; CAP (circular
arc polygons) from Petter Bjorstad and Eric Grosse; gear-like domains by Kent Pearce, and
CONFPACK (Symm’s integral equation) by Hough and Gutknecht.
CONTIN: Methods for continuation and limit points, notably PITCON, by Werner Rhein-
boldt and John Burkardt, University of Pittsburgh.
DDSV: Programs from Linear Algebra Computations on Vector and Parallel Computers,
by Jack Dongarra, Iain Duff, Danny Sorensen, and Henk Van der Vorst.
DIERCKX: Spline ﬁtting routines for various kinds of data and geometries. Paul Dierckx,
Leuven.
ELEFUNT: Transportable Fortran programs for testing the elementary function programs
provided with Fortran compilers. Reference: Software Manual for the Elementary Func-
tions by W. J. Cody and W. Waite, Prentice–Hall, 1980.
FITPACK: Splines under tension (an early version). Developed by Alan K. Cline.

C-12
Online Appendix C. Guide to Literature
FMM: Routines from the book Computer Methods for Mathematical Computations, by
George Forsythe, Mike Malcolm, and Cleve Moler.
GCV: Generalized cross validation spline smoothing and ridge regression. Developed by
Grace Wahba.
HOMPACK: Fortran 77 subroutines for solving nonlinear systems of equations by homo-
topy methods. There are subroutines for ﬁxed point, zero ﬁnding, and general homotopy
curve tracking problems, utilizing both dense and sparse Jacobian matrices. Three different
algorithms are implemented: ODE-based, normal ﬂow, and augmented Jacobian. Layne T.
Watson, Blacksburg, VA.
ITPACK: Iterative linear system solver based on a number of methods: Jacobi method,
SOR, and SSOR with conjugate gradient or Chebyshev (semi-iteration) acceleration. David
Young, David Kincaid, and the group at the University of Texas, Austin.
JAKEF: A precompiler that analyzes a given Fortran 77 source code for the evaluation of
a scalar or vector function and then generates an expanded Fortran subroutine that simul-
taneously evaluates the gradient or Jacobian, respectively. For scalar functions the ratio
between the run time of the resulting gradient routine and that of the original evaluation
routine is never greater than a ﬁxed bound of about ﬁve. The storage requirement may
be considerable, as it is also proportional to the run time of the original routine. Since no
differencing is done, the partial derivative values obtained are exact up to roundoff errors.
By Andreas Griewank.
LANZ: Large Sparse Symmetric Generalized Eigenproblem. Mark T. Jones, Argonne Na-
tional Laboratory and Merrell L. Patrick, Duke University.
LANCZOS: For computing a few eigenvalues/eigenvectors of large (sparse) real symmetric
and Hermitian matrices, and singular values and vectors of rectangular matrices. By Jane
Cullum and Ralph Willoughby.
LAWSON–HANSON: Least squares codes from the book Solving Least Squares Problems,
by C. L. Lawson and R. J. Hanson, SIAM, 1995.
LP: A set of test problems for linear programming in MPS format. David Gay, Bell Labo-
ratories.
MINPACK: Solution of systems of nonlinear equations and nonlinear least squares prob-
lems. Consists of ﬁve algorithmic paths, each including a core subroutine and an easy-to-use
driver. The algorithms proceed either from an analytic speciﬁcation of the Jacobian matrix
or directly from the problem functions. The paths include facilities for systems of equations
with a banded Jacobian matrix, for least squares problems with a large amount of data, and
for checking the consistency of the Jacobian matrix with the functions. Developed by Jorge
Moré, Burt Garbow, and Ken Hillstrom, Argonne National Laboratory.
MISC: Contains various pieces of software collected over time. Jack Dongarra.
MPFUN: Multiple precision arithmetic. David Bailey.
NAPACK: Solving linear systems; estimating the condition number or the norm of a ma-
trix; computing determinants, matrix inverse, least squares problems. Also for performing
unconstrained minimization, compute eigenvalues, eigenvectors, the singular value de-

C.7. Public Domain Software
C-13
composition, or the QR decomposition. The package has special routines for general,
band, symmetric, indeﬁnite, tridiagonal, upper Hessenberg, and circulant matrices. By Bill
Hager, University of Florida, Gainesville. Related book: Applied Numerical Linear Alge-
bra, Prentice–Hall, 1988.
NUMERALGO: Algorithms from the journal Numerical Algorithms.
ODE: Various initial and boundary value ordinary differential equation solvers: colsys,
dverk, rksuite, ode. Erik Grosse.
ODEPACK: The ODE package from Hindmarch and others. Alan Hindmarch, Lawrence
Livermore National Laboratory.
ODRPACK: Orthogonal Distance Regression. Aportable collection of Fortran subprograms
for ﬁtting a model to data. It is designed primarily for instances when the independent as
well as the dependent variables have signiﬁcant errors, implementing a highly efﬁcient algo-
rithm for solving the weighted orthogonal distance regression problem, i.e., for minimizing
the sum of the squares of the weighted orthogonal distances between each data point and
the curve described by the model equation. By Boggs, Byrd, Rogers, and Schnabel.
PARANOIA: A program to explore the ﬂoating-point system on your computer. Devised by
W. Kahan, University of California, Berkeley.
PDES: A compact package for solving systems of linear equations using multigrid or
aggregation–disaggregation methods.
Imbedded in the algorithms are implementations
for sparse Gaussian elimination and symmetric Gauss–Seidel (unaccelerated or accelerated
by conjugate gradients or Orthomin(1)). This package is particularly useful for solving
problems which arise from discretizing partial differential equations, regardless of whether
ﬁnite differences, ﬁnite elements, or ﬁnite volumes are used. Craig Douglas.
PLTMG: Elliptic partial differential equations in general regions of the plane. It features
adaptive local mesh reﬁnement, multigrid iteration, and a pseudoarclength continuation
option for parameter dependencies. The package includes an initial mesh generator and
several graphics packages. By Randy Bank. Reference: PLTMG: A Software Package for
Solving Elliptic Partial Differential Equations: Users’Guide 8.0, SIAM, 1998.
POLYHEDRA: A database of angles, vertex locations, and so on for over 100 hundred geo-
metric solids. Compiled by Andrew Hume.
PORT: Public subset of the PORT library, which includes the latest version of David Gay’s
NL2SOL nonlinear least squares. The rest of the PORT3 library is available by license from
Lucent Technologies.
RANDOM: Random number generators. Eric Grosse, editor.
QUADPACK: Univariate quadrature. By Robert Piessens (Leuven), Elise de Donker (Leu-
ven), and David Kahaner (National Bureau of Standards).
SLAP: Sparse linear algebra package for iterative symmetric and nonsymmetric linear sys-
tem solutions. Included are core routines to do iterative reﬁnement, preconditioned conju-
gate gradient, preconditioned conjugate gradient on the normal equations, preconditioned
biconjugate gradient, preconditioned biconjugate gradient squared, orthomin, and general-
ized minimum residual. Mark K. Seager and Anne Greenbaum.

C-14
Online Appendix C. Guide to Literature
SLATEC: Comprehensive software library containing over 1400 general purpose mathe-
matical and statistical routines written in Fortran 77.
SPARSPAK: Subroutines from the book Computer Solution of Large Sparse Positive Deﬁnite
Systems by Alan George and Joseph Liu, Prentice–Hall, 1981.
SPARSE: Subroutines written in C that solve large sparse systems of linear equations using
LU factorization. The package is able to handle arbitrary real and complex square matrix
equations. Besides being able to solve linear systems, it solves transposed systems, ﬁnds
determinants, multiplies a vector by a matrix, and estimates errors due to ill-conditioning
in the system of equations and instability in the computations. SPARSE does not require or
assume symmetry and is able to perform numerical pivoting (either diagonal or complete)
to avoid unnecessary error in the solution. SPARSE also has an optional interface that allow
it to be called from FORTRAN programs. Ken Kundert, Alberto Sangiovanni-Vincentelli,
Berkeley.
SPARSE-BLAS: An extension to the set of Basic Linear Algebra Subprograms (BLAS).
The extension is targeted at sparse vector operations, with the goal of providing efﬁcient,
but portable, implementations of algorithms for high-performance computers. By Dave
Dodson.
SPECFUN: Fortran programs for special functions, and accompanying test programs simi-
lar in concept to those in ELEFUNT. W. J. Cody, Argonne National Laboratory.
SVDPACK: Singular values and singular vectors of large sparse matrices. By Mike Berry.
TEXTBOOK: Codes associated with numerical analysis textbooks. Erik Grosse, editor.
TOEPLITZ: Solution of Toeplitz or circulant systems of linear equations and for orthogonal
factorization of column-circulant matrices. A Soviet–American collaborative effort. Burt
Garbow, Argonne National Laboratory,
TRANSFORM: Fast Fourier Transform (FFT) and other digital processing tools.
Erik
Grosse, editor.
UNCON: Unconstrained optimization. Problems from Moré, Garbow, and others.
VANHUFFEL: Total least squares (TLS) problem by using a partial singular value decom-
position (PSVD). By Sabine Van Huffel.



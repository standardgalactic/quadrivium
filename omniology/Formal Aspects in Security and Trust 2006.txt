Lecture Notes in Computer Science
4691
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
University of Dortmund, Germany
Madhu Sudan
Massachusetts Institute of Technology, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Moshe Y. Vardi
Rice University, Houston, TX, USA
Gerhard Weikum
Max-Planck Institute of Computer Science, Saarbruecken, Germany

Theo Dimitrakos Fabio Martinelli
Peter Y. A. Ryan Steve Schneider (Eds.)
Formal Aspects
in Security
and Trust
Fourth International Workshop, FAST 2006
Hamilton, Ontario, Canada, August 26-27, 2006
Revised Selected Papers
1 3

Volume Editors
Theo Dimitrakos
BT Group Chief Technology Ofﬁce, Ipswich IP5 3RE, UK
E-mail: Theo.Dimitrakos@bt.com
Fabio Martinelli
National Research Council - C.N.R., Pisa, Italy
E-mail: fabio.martinelli@iit.cnr.it
Peter Y. A. Ryan
University of Newcastle, UK
E-mail: peter.ryan@ncl.ac.uk
Steve Schneider
University of Surrey, UK
E-mail: S.Schneider@surrey.ac.uk
Library of Congress Control Number: Applied for
CR Subject Classiﬁcation (1998): C.2.0, D.4.6, E.3, K.4.4, K.6.5
LNCS Sublibrary: SL 4 – Security and Cryptology
ISSN
0302-9743
ISBN-10
3-540-75226-9 Springer Berlin Heidelberg New York
ISBN-13
978-3-540-75226-4 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2007
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
SPIN: 12164677
06/3180
5 4 3 2 1 0

Preface
The present volume contains the post-proceedings of the 4th International Work-
shop on Formal Aspects in Security and Trust (FAST2006), held in Hamilton,
Ontario, Canada, August 26–27, 2006. FAST is an event aﬃliated with the For-
mal Methods 2006 Congress (FM06). FAST 2006 was held under the auspices of
the IFIP WG 1.7 on Foundations of Security Analysis and Design.
FAST2006 aimed at continuing the successful eﬀort of the previous three
FAST workshop editions for fostering the cooperation among researchers in the
areas of security and trust. The new challenges oﬀered by the so-called ambient
intelligence space, as a future paradigm in the information society, demand for a
coherent and rigorous framework of concepts, tools and methodologies to provide
users with trust and conﬁdence in the underlying communication/interaction
infrastructure. It is necessary to address issues relating to both guaranteeing
security of the infrastructure and the perception of the infrastructure being se-
cure. In addition, user conﬁdence in what is happening must be enhanced by
developing trust models eﬀectively but that are also easily comprehensible and
manageable by users.
FAST sought for original papers focusing on formal aspects in: security and
trust policy models; security protocol design and analysis; formal models of trust
and reputation; logics for security and trust; distributed trust management sys-
tems; trust-based reasoning; digital assets protection; data protection; privacy
and ID issues; information ﬂow analysis; language-based security; security and
trust aspects in ubiquitous computing; validation/analysis tools; Web service se-
curity/trust/privacy; GRID security; security risk assessment; and case studies.
The FAST2006 post-proceedings collect the revised versions of 18 papers, se-
lected out of 47 submissions. Each paper was reviewed by at least three members
of the Program Committee.
We wish to thank the the Program Committee members for their valuable
eﬀorts in properly evaluating the submissions, and the FM06 organizers for ac-
cepting FAST as an aﬃliated event and for providing a perfect environment for
running the workshop.
Thanks are also due to the Center for Software Reliability (CSR) of Newcastle
University and IIT-CNR for sponsoring FAST2006.
February 2007
Theo Dimitrakos
Fabio Martinelli
Peter Y.A. Ryan
Steve Schneider

Organization
Workshop Organizers
Theo Dimitrakos, BT
Fabio Martinelli, IIT-CNR
Peter Y.A. Ryan,University of Newcastle
Steve Schneider, University of Surrey
Invited Speakers
Joshua D. Guttman, MITRE, USA
Program Committee
Gilles Barthe, INRIA Sophia-Antipolis, France
Stefano Bistarelli, University of Pescara, Italy
Gregor v. Bochmann, University of Ottawa, Canada
John A. Clark, University of York, UK
Fr´ed´eric Cuppens, ENST Bretagne, France
Roberto Gorrieri, University of Bologna, Italy
Joshua D. Guttman, MITRE, USA
Masami Hagiya, University of Tokyo, Japan
Chris Hankin, Imperial College (London), UK
Christian Jensen, DTU, Denmark
Audun Jøsang, DSTC, Australia
Jan J¨urjens, TU M¨unchen, Germany
Yuecel Karabulut, SAP, Germany
Igor Kotenko, SPIIRAS, Russia
Heiko Krumm, University of Dortmund, Germany
Ninghui Li, Purdue University, USA
Steve Marsh, Institute for Information Technology, NRC, Canada
Catherine Meadows, Naval Research Lab, USA
Ron van der Meyden, University of New South Wales, Australia
Mogens Nielsen, University of Aarhus, Denmark
Flemming Nielson, Danish Technical University, Denmark
Indrajit Ray, Colorado State University, USA
Babak Sadighi Firozabadi, SICS, Sweden
Pierangela Samarati, University of Milan, Italy
Jean-Marc Seigneur, University of Geneva, Switzerland
Paul Syverson, Naval Research Laboratory, USA
Ketil Stolen, SINTEF, Norway
William H. Winsborough, George Mason University, USA
Local Organization
Alessandro Falleni, IIT-CNR

Table of Contents
Strategic Games on Defense Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Stefano Bistarelli, Marco Dall’Aglio, and Pamela Peretti
Timed Calculus of Cryptographic Communication . . . . . . . . . . . . . . . . . . . .
16
Johannes Borgstr¨om, Olga Grinchtein, and Simon Kramer
A Semantic Paradigm for Component-Based Speciﬁcation Integrating a
Notion of Security Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
Gyrd Brændeland and Ketil Stølen
Game-Based Criterion Partition Applied to Computational Soundness
of Adaptive Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
M. Daubignard, R. Janvier, Y. Lakhnech, and L. Mazar´e
Measuring Anonymity with Relative Entropy . . . . . . . . . . . . . . . . . . . . . . . .
65
Yuxin Deng, Jun Pang, and Peng Wu
Formalizing and Analyzing Sender Invariance . . . . . . . . . . . . . . . . . . . . . . . .
80
Paul Hankes Drielsma, Sebastian M¨odersheim, Luca Vigan`o, and
David Basin
From Simulations to Theorems: A Position Paper on Research in the
Field of Computational Trust . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
Karl Krukow and Mogens Nielsen
A Tool for the Synthesis of Controller Programs . . . . . . . . . . . . . . . . . . . . .
112
Ilaria Matteucci
Where Can an Insider Attack? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
Christian W. Probst, Ren´e Rydhof Hansen, and Flemming Nielson
Maintaining Information Flow Security Under Reﬁnement and
Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
Fredrik Seehusen and Ketil Stølen
A Classiﬁcation of Delegation Schemes for Attribute Authority . . . . . . . .
158
Ludwig Seitz, Erik Rissanen, and Babak Sadighi
Program Partitioning Using Dynamic Trust Models . . . . . . . . . . . . . . . . . .
170
Dan Søndergaard, Christian W. Probst,
Christian Damsgaard Jensen, and Ren´e Rydhof Hansen
Locality-Based Security Policies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
Terkel K. Tolstrup, Flemming Nielson, and Ren´e Rydhof Hansen

VIII
Table of Contents
A Theorem-Proving Approach to Veriﬁcation of Fair Non-repudiation
Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Kun Wei and James Heather
A Formal Speciﬁcation of the MIDP 2.0 Security Model. . . . . . . . . . . . . . .
220
Santiago Zanella B´eguelin, Gustavo Betarte, and Carlos Luna
A Comparison of Semantic Models for Noninterference . . . . . . . . . . . . . . . .
235
Ron van der Meyden and Chenyi Zhang
Hiding Information in Multi Level Security Systems . . . . . . . . . . . . . . . . . .
250
Dani`ele Beauquier and Ruggero Lanotte
A New Trust Model Based on Advanced D-S Evidence Theory for P2P
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270
Chunqi Tian, Shihong Zou, Wendong Wang, and Shiduan Cheng
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285

Strategic Games on Defense Trees⋆
Stefano Bistarelli1,2, Marco Dall’Aglio1, and Pamela Peretti1
1 Dipartimento di Scienze, Universit`a degli Studi “G. d’Annunzio”, Pescara, Italy
{bista,maglio,peretti}@sci.unich.it
2 Istituto di Informatica e Telematica, CNR, Pisa, Italy
Stefano.Bistarelli@iit.cnr.it
Abstract. In this paper we use defense trees, an extension of attack
trees with countermeasures, to represent attack scenarios and game the-
ory to detect the most promising actions attacker and defender. On one
side the attacker wants to break the system (with as little eﬀorts as pos-
sible), on the opposite side the defender want to protect it (sustaining
the minimum cost).
As utility function for the attacker and for the defender we consider
economic indexes (like the Return on Investment (ROI) and the Return
on Attack (ROA)). We show how our approach can be used to evaluate
eﬀectiveness and economic proﬁtability of countermeasures as well as
their deterrent eﬀect on attackers, thus providing decision makers with
a useful tool for performing better evaluation of IT security investments
during the risk management process.
Keywords: Security, Risk Analysis, Game Theory.
1
Introduction
Security has become today a fundamental part of the enterprise investment. In
fact, more and more cases are reported showing the importance of assuring an
adequate level of protection to the enterprise’s assets.
In order to focus on the real and concrete threats that could aﬀect the enter-
prise’s assets, a risk management process is needed in order to identify, describe
and analyze the possible vulnerabilities that must be eliminated or reduced. The
ﬁnal goal of the process is to make security managers aware of the possible risks,
and to guide them toward the adoption of a set of countermeasures which bring
the overall risk under an acceptable level.
The determination of the acceptable risk level and the selection of the best
countermeasure is unfortunately not an easy task. There are no standard
methodologies for the process, and often security managers have to decide among
too many alternatives.
To model the attack scenario and the defender possibilities we use defense
trees [1], an extension of attacks trees with countermeasures. The vulnerabilities
are represented as leaf nodes of the tree and are decorated with the counter-
measures able to mitigate the damage of threats using such a vulnerability.
⋆Partially supported by the MIUR PRIN 2005-015491.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 1–15, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

2
S. Bistarelli, M. Dall’Aglio, and P. Peretti
Moreover, economic indexes are used as labels for countermeasures and attacks.
The Return on Investment (ROI) [18,17] index gives a measure of the eﬃcacy of
a speciﬁc security investment in a countermeasure w.r.t. a speciﬁc attack. The
Return on Attack (ROA) [3] is instead an index that is aimed at measuring the
convenience of attacks, by considering the impact of a security solution on the
attacker’s behavior.
The computed ROI and ROA function are then considered as utility functions
(payoﬀs) in a two player strategic game. On one side the system administrator
wants to protect the system by buying and adopting countermeasures; on the
other side the attacker wants to exploit the vulnerabilities and obtain some proﬁt
by breaking the system.
We solve the games by looking at their Nash equilibria with both pure and
mixed strategies. Our results show that is always worth installing countermea-
sures for the defender; however, it is not true that increasing the number of
countermeasure gives an overall better beneﬁt to the enterprise (as showed in [7]
investing in security measure is not proﬁtable beyond a certain level). This is
not completely surprising, since more and more sophisticated protection may
be accompanied by escalating marginal costs, while the probability that any
given type of protection will be needed (that is, its expected beneﬁt) may re-
main constant. Also interesting is the fact that the strategies of no-attacks and
no-countermeasures is not (unfortunately) a point of equilibrium.
After an introduction to the concepts of security risk management and of
defense trees (Section 2) we study the selection of the most promising counter-
measures by interpreting the scenario as a game with two players: the defender
and the attacker (Section 3). Section 4, instead, shows a realistic example where
the attacker wants to steal information about customers maintained in a server.
Finally, Section 5 summarizes the paper results and sketches some directions for
future work.
2
Security Risk Management and Defense Trees
Defending an IT system is hard because many are the risks that can aﬀect each
asset of the system. Organizations need a process that enable to identify, describe
and analyze the possible vulnerability that can be exploited by an adverse indi-
vidual, and identify the security measures necessary to reduce the risks.
In [1] we propose the use of the defense tree (extension of attack trees [15,16]),
an instrument for representing an attack against a system and how it can be
mitigated by a set of countermeasures.
The diﬀerence between an attack tree and a defense tree is that the ﬁrst
represents only the attack strategies that an attacker can perform, while the
second adds the set of countermeasures that can be introduced into the system
to mitigate the possible damages produced by an attack.
Integrating countermeasures into threat trees, and more generally into di-
rected acyclic graphs, is not new. In the early 90s researchers used ”threat coun-
termeasure diagrams”. One may also see examples of countermeasures in DAGs

Strategic Games on Defense Trees
3
in both Nathalie Foster’s thesis [4] and Stuart Schechter’s thesis [14], both of
which include discussions and histories of the evolution of these structures. Even
in the popular Microsoft text by Howard and LeBlanc, ”Writing Secure Code”,
one can ﬁnd threat trees (another name for attack trees) in which countermea-
sures are integrated [8].
Attack tree
+
Countermeasures
Fig. 1. A defense tree
Figure 1 shows an example of a defense tree: round nodes form the attack
tree and square nodes represent the corresponding countermeasures. The root
of the tree is associated with an asset of the IT system under consideration and
represents the attacker’s goal. Leaf nodes in the attack tree represent simple
subgoals which lead the attacker to (partially) damage the asset by exploiting
a single vulnerability. Non-leaf nodes (including the tree root) can be of two
diﬀerent types: or-nodes and and-nodes. Subgoals associated with or-nodes are
completed as soon as any of its child nodes is achieved, while and-nodes represent
subgoals which require all of its child nodes to be completed (in Figure 1 we draw
an horizontal line between the children of an and-node to distinguish it from the
or-node).
We consider defense trees [1] enriched with economic indexes that quantify
the cost of attacks and the return on security investments in any branch of the
tree. We interpret such indexes as utility functions for the system administrator
and for the attacker, by viewing the scenario as a classical game with two player
looking for diﬀerent and usually opposite results (see Section 3).
In particular we label the tree with:
1. the Return On Investment (ROI) [17] measuring the return that a defender
expects from a security investment over the costs he sustains for counter-
measures. It is calculated with the formula:
ROI = ALE × RM −CSI
CSI
where:
– the Annualized Loss Expectancy (ALE) [9] measures the expected annual
ﬁnancial loss which can be ascribed to a threat to the organization. It is
calculated as ALE = AV × EF × ARO, where:

4
S. Bistarelli, M. Dall’Aglio, and P. Peretti
• the Asset Value (AV ) is a measure of the cost of creation, develop-
ment, support, replacement and ownership values of an asset,
• the Exposure Factor (EF) represents a measure of the magnitude
of loss or impact on the value of an asset arising from a threat (ex-
pressed as a percentage of the asset value),
• the Annualized Rate of Occurrence (ARO) is a number that repre-
sents the estimated number of annual occurrences of a threat.
– the Risk Mitigated by a countermeasure (RM) represents the eﬀective-
ness of a countermeasure in mitigating the risk of loss deriving from
exploiting a vulnerability (RM is a numeric value in [0,1] that measures
the proportion of reduced risk),
– the Cost of Security Investment (CSI) is the cost that an enterprise
sustains for implementing a given countermeasure.
2. the Return On Attack (ROA) [3] measures the gain that an attacker expects
from a successful attack over the losses that he sustains due to the adoption
of security measures by his target. It is calculated as:
ROA = GI × (1 −RM) −(costa + costac)
costa + costac
where:
– GI is the expected gain from the successful attack on the speciﬁed target,
– costa is the cost sustained by the attacker to succeed,
– costac is the additional cost brought by the countermeasure c adopted
by the defender to mitigate the attack a.
We will see in Section 3 that other choices for the utility functions are possible.
For instance we could consider ROI and ROA without dividing the gain by the
costs (CSI and costa + costac respectively), or by considering the damage of an
attack without considering its (often unknown) rate of occurrence (ARO).
3
Defense Trees as Strategic Games
In this section we will show how game theory can be used to analyze the possible
strategies of the system administrator and of the attacker. In our scenario we
consider a strategic game [6] that consists of:
– n players (n is usually just 2, but we plan to extend it to the case of 1
defender and k attackers),
– a set of strategies Si for each player i,
– the utility function (or payoﬀ) ui for each player i.
We consider here the case with n = 2 players: the defender (Bob) and the
attacker (Alice) of a system. The set of defender’s strategies is the set of coun-
termeasures that he can introduce into the systems while the set of attacker’s
strategies is the set of vulnerability that she can exploit. The payoﬀfunctions
we will consider are the Return on Investment (ROI) for the defender and the

Strategic Games on Defense Trees
5
Return on Attack (ROA) for the attacker. Notice that ROI and ROA represent
normalized payoﬀs; in some cases a not normalized utility function could be used
instead, that may lead to diﬀerent equilibrium strategies (because each player is
trying to maximize its return rather than its payoﬀ).
a1
a2
c2
ud=1
ua=1
ud=0
ua=2
ud=1
ua=2
ud=1
ua=0
c3
c3
c1
(a) Defence tree
c1
c2
c3
a1
1,1
1,1
0,2
1,0
0,2
1,2
a2
(b) Strategic game
Fig. 2. Defense tree and the corresponding strategic game (with a pure strategy Nash
Equilibrium)
As an example consider the defense tree depicted in Figure 2(a). It can be
modeled as the strategic game in Figure 2(b), where:
– the players of the game are the defender of the enterprise that can select
actions represented in the rows, and the attacker that can choose possible
attacks (represented as columns in the table),
– the defender’s set of strategies is Sd = {c1, c2, c3}, that consists of the pos-
sible countermeasures that he can enforce to protect the system,
– the attacker’s set of action is Sa = {a1, a2} that represents the two possible
attack strategies (the columns in Figure 2(b));
– the goal of each player is to maximize his/her own payoﬀfunction (the
number in each box of Figure 2(b)). The payoﬀs associated to a strategy
(ci, ai) are ud(ci, ai) for the defender, and ua(ci, ai) for the attacker.
Each player chooses the best available action given his belief about the other
player’s action.
The solution of the game is the (set of) countermeasure that the defender is
more likely to adopt, and the (set of) vulnerability that the attacker feels more
suitable to exploit. In some special cases the best strategy of the attacker and of
the defender converges to a speciﬁc action proﬁle s∗with the property that the
defender cannot do better by choosing an action diﬀerent from s∗
d, given that
the attacker adopt s∗
a, and viceversa. In this case we say that the game admits
a Nash Equilibrium [13].
Deﬁnition 1 (Nash Equilibrium [6]). In a strategic game with 2 players,
consider the sets S1, S2 and the functions u1, u2 that are the set of possible

6
S. Bistarelli, M. Dall’Aglio, and P. Peretti
strategies and the utility functions of players 1 and 2 respectively. The combina-
tion of strategy (s∗
1, s∗
2) with s∗
1 ∈S1 and s∗
2 ∈S2 is a Nash Equilibrium if and
only if, for each player i, the action s∗
i is the best response to the other player:
u1(s∗
1, s∗
2) ≥u1(s1, s∗
2) for any s1 ∈S1
u2(s∗
1, s∗
2) ≥u2(s∗
1, s2) for any s2 ∈S2
Figure 2(a) shows an example of defense tree where two possible attacks are
represented: a1 and a2. The ﬁrst one can be mitigated by two countermeasure
c2 and c3, the second one can be mitigated by c1 and c3. Figure 2(b) shows
the corresponding strategic game, where the numbers in the bimatrix are the
payoﬀs associated to each player (associated as label to the tree as we will see
in Section 3).
Using Deﬁnition 1 we can calculate the possible Nash Equilibria of the game.
Notice that if the attacker plays strategy a1 the best response for the defender
is to play the strategies c1 or c2 (by looking at the ﬁrst column on the left we
can see that he can gain 1 instead of 0), while if the attacker plays strategy a2
the best response is to play the strategies c1 or c3.
Conversely if the defender plays the strategy c1 the best response for the
attacker is play strategy a1, if the defender plays the strategy c2 the best response
is to play strategy a2 and if the defender plays strategy c3 the best response for
the attacker is to play strategies a1 or a2. The game admits two diﬀerent Nash
Equilibria (the circled payoﬀs): the couple of strategies {c1, a1} and {c3, a2}.
The Nash Equilibrium represents the best strategies for both the attacker
and the defender (with the hypothesis that neither the attacker nor the defender
have any knowledge of the other). In the case depicted in Figure 2, the defender
will select, if possible, both countermeasure c1 and c3. However if the ﬁnancial
resources available to the system administrator are limited, only countermeasure
c3 will be selected (because it will cover both strategy of the attacks). In Section 4
a complete more realistic example will be presented where the economic indexes
will be used for the selection.
Sometimes in a strategic game it is impossible to ﬁnd a Nash Equilibrium.
Moreover we often need to take into account the uncertainty of the player’s
behavior. In this case a player may consider a mixed strategy.
Deﬁnition 2 (Mixed strategy [6]). Consider a strategic game with 2 players,
G = {S1, S2; u1, u2} where Si = {si1, . . . , sik} the strategies of player i. A mixed
strategy for player 1 ≤i ≤2 is a probability distribution pi = (pi1, . . . , pik),
where 0 ≤pik.
In our context the use of mixed strategies ﬁnds a justiﬁcation in the fact that a
player, especially the defender, deals with a single attacker, whose behavior is not
known. He may assume, however, that this players is drawn from a population
of attackers whose actions can be estimated as frequencies from previous attacks
(leading to the notion of repeated games where the players can randomize their
strategies).

Strategic Games on Defense Trees
7
What we obtain is shown in Figure 3. The Attacker A can play the strategy
a1 with probability pa1, and the strategy a2 with probability pa2, whilst the
Defender D plays the strategy ci with probability pci, with 1 ≤i ≤3.
c1
c2
c3
pc1
pc2
pc3
a1
ud(c1,a1),ua(c1,a1)
ud(c2,a1),ua(c2,a1)
ud(c3,a1),ua(c3,a1)
ud(c1,a2),ua(c1,a2)
ud(c2,a2),ua(c2,a2)
ud(c3,a2),ua(c3,a2)
a2
pa1
pa2
Fig. 3. Mixed strategies
We can compute payoﬀs in presence of mixed strategies by taking into account
probability distributions and computing expectations. If the defender uses a pure
strategy1 in response to a mixed strategy of the attacker, the resulting payoﬀs
for each possible countermeasure ci is:
ud(ci) = ud(ci, a1) × pa1 + ud(ci, a2) × pa2
If the attacker uses a pure strategy in response of a mixed strategy of the defender
the resulting payoﬀs for each attack ai is:
ua(ai) = ua(c1, ai) × pc1 + ua(c2, ai) × pc2 + ua(c3, ai) × pc3
Deﬁnition 3. Given a game with 2 players, and 2 sets of strategies S1 =
{s11, . . . , s1K1} and S2 = {s21, . . . , s2K2}, if player i believes that player j will
play the strategies (sj1, . . . , sjKj) with probability (pj1, . . . , pjKj), the expected
payoﬀfor player i obtained with the pure strategy sij is:
Kj

k=1
pjkui(sij, sjk)
We can use Deﬁnition 3 to solve the game in Figure 2 by using the mixed
strategies. In particular suppose that the defender uses a pure strategy and the
attacker plays a mixed strategy {a1, a2} with probability (pa1, pa2) (as shown in
Figure 4). The expected payoﬀfor the defender, if the attacker plays a mixed
strategy are:
1 · pa1 + 1 · pa2 = pa1 + pa2 for countermeasure c1
1 · pa1 + 0 · pa2 = pa1
for countermeasure c2
0 · pa1 + 1 · pa2 = pa2
for countermeasure c3
1 A pure strategy is a strategy that a player plays with probability 1.

8
S. Bistarelli, M. Dall’Aglio, and P. Peretti
c1
c2
pc1
c1
pa1
a1
pc2
c2
pa2
a2
pc3
c3c3
a1
1,1
1,1
0,2
1,0
0,2
1,2
a2
Fig. 4. Example of mixed strategy
Conversely, if the attacker uses a pure strategy and the defender plays a mixed
strategy {c1, c2, c3} with probability (pc1, pc2, pc3), the expected payoﬀfor the
defender are:
1 · pc1 + 1 · pc2 + 2 · pc3 = pc1 + pc2 + 2pc3 for attack a1
0 · pc1 + 2 · pc2 + 2 · pc3 = 2pc2 + 2pc3
for attack a2
Deﬁnition 4. If the players 1 and 2 play respectively the strategies (s11, . . . , s1J)
with probability p1 = (p11, . . . , p1J), and (s21, . . . , s2K) with probability p2 =
(p21, . . . , p2K), the expected payoﬀfor the players are computed as follows:
v1(p1, p2) =
J

j=1
p1j
 K

k=1
p2ku1(s1j, s2k)

=
J

j=1
K

k=1
p1j · p2ku1(s1j, s2k)
v2(p1, p2) =
K

k=1
p2k
 J

j=1
p1ju2(s1j, s2k)

=
J

j=1
K

k=1
p1j · p2ku2(s1j, s2k)
The mixed strategies (p∗
1, p∗
2) are a Nash Equilibrium only if the mixed strategy
for each player is the best response to the mixed strategy of the other player:
v1(p∗
1, p∗
2) ≥v1(p1, p∗
2) for any p1
v2(p∗
1, p∗
2) ≥v2(p∗
1, p2) for any p2.
By applying Deﬁnition 4 we can now compute the Nash Equilibrium when the
defender and the attacker adopt mixed strategies(Figure 4).
The utility of the defender ud and of the attacker ua are respectively:
ud = 1pc1pa1 + 1pc1pa2 + 1pc2pa1 + 1pc3pa2
ua = 1pc1pa1 + 1pc2pa1 + 2pc2pa2 + 2pc3pa1 + 2pc3pa2.
Figure 5 shows an equilibrium with mixed strategy for the game: the defender
plays the strategy c1 with probability 1
2 and c2 with probability 1
2, the attacker
plays a1 with probability 1.

Strategic Games on Defense Trees
9
c1
c2
1
2
c3
1
1
2
a1
1,1
1,1
0,2
1,0
0,2
1,2
a2
Fig. 5. Example of mixed strategy
4
Using Economic Indexes as Payoﬀs
In this section we show how to model a security problem by using the
results highlighted in the previous section. An enterprise’s server is used to
store information about customers. Consider the defense tree depicted in Fig. 6
reﬂecting attacks to the server (the asset) and the corresponding mitigation
countermeasures.
Install a video
survellaince
equipment
Assume a
security guard
Install a video
survellaince
equipment
Assume a
security guard
Install a
security door
Install a
safety lock
Steal the
server
Go out
unobserved
Have the keys
Break down
the door
Go out
unobserved
c1
c2
a1
a2
c3
c4
c2
c3
Fig. 6. Example of defense tree: theft of a server
In the example we consider a server with a value of 100.000e. The Exposure
Factor (EF) and the Annualized Rate of Occurrence (ARO) of each attack are
shown in Table 1. Notice that associated to the risk management process is the
lack of reliable statistical data to use in a quantitative analysis. In our paper
we use (when available) statistics collected in [12] that combine the information
from two surveys: a magazine survey in Information Week (October 1996) that
asked “What Security Problems have resulted in ﬁnancial losses?”, and another
magazine survey, in InfoSecurity News May 1997 that asked “In the past 12
months, which of the following breaches have you experienced?”.
We need now to compute ALE for each of the possible attacks. Considering
the ﬁrst attack of Figure 6 we can notice that for a successful attack we need
both to break down the door and to go out unobserved. So, the EF and ARO

10
S. Bistarelli, M. Dall’Aglio, and P. Peretti
Table 1. Computation of ROI
Attack
EF ARO Countermeasures
RM
CSI ROI
a1 Break down
90%
0,1
c1 Install a security door
0,7
1500
3.20
the door
c2 Install a video surveillance equip.
0,1
3000 -0.70
and go out
c3 Employ a security guard
0,5 12000 -0.63
unobserved
c4 Install a security lock
0
300
-1
a2 Open the door
93%
0,1
c1 Install a security door
0
1500
-1
with keys
c2 Install a video surveillance equip.
0,1
3000 -0.69
and go out
c3 Employ a security guard
0,5 12000 -0.61
unobserved
c4 Install a security lock
0,2
300
5.20
Table 2. Computation of ROA
Attack
Costa Countermeasures
Costac ROA
a1 Break down
4000 c1 Install a security door
2000
0.50
the door
c2 Install a video surveillance equipment
1000
4.40
and go out
c3 Employ a security guard
1500
1.73
unobserved
c4 Install a security lock
0
6.50
a2 Open the door
4200 c1 Install a security door
0
6.14
with keys
c2 Install a video surveillance equipment
1000
4.19
and go out
c3 Employ a security guard
1500
1.63
unobserved
c4 Install a security lock
200
4.45
are associated to the pair of actions (and not to the leaf). We proceed similarly
for the second attack.
The ALE associated to the attack are, respectively, ALE =100.000e×0.9×0.1
=9.000e and ALE =100.000e×0.93 × 0.1 =9.300e.
The second step is to compute the ROI for each countermeasure by considering
the cost (CSI) and the amount of risk mitigated (RM) of Table 1.Notice that
the countermeasures c1 and c4 have two diﬀerent RM values: in Figure 6 we can
see that c1 is used only to mitigate the attack a1 in this case the value of RM
is 0.7, but if it is used to mitigate the attack a2 the value of decreases to 0. The
same is true for the countermeasure c4, if it is used to mitigate the attack a2 the
value of RM is 0.2 but if it is used for the attack a1 RM is 0.
For the ﬁrst countermeasure (installing a security door to mitigate the threat of
breaking down a door), we have ROI = (ALE×RM)−CSI
CSI
= (9.000 e×0.7)−1.500 e
1.500 e
=
3.20. Similarly we can compute the ROI for all the other countermeasure as
shown in Table 1.
For ROA we analyze the scenario from the attacker perspective. Let us sup-
pose that the attacker has an advantage that can be economically quantiﬁed as
30.000e for a successful attack to the server. By using the data in Table 2 we
compute the ROA for each countermeasure.
Notice that the cost an attacker has to pay depends on the attack and on the
countermeasure installed. In Table 2, for instance, the ﬁxed cost to be sustained

Strategic Games on Defense Trees
11
by the attacker from stealing the server is diﬀerent (4.000e or 4.200e): the
variable costs instead depends on the speciﬁc countermeasure (2.000 e when
encountering a security door vs 1.000e for a video surveillance installation).
The data in the table are used to compute ROA for all the countermeasures
in the tree. So, for instance when installing a security door we can obtain a
ROA = GI×(1−RM)−(costa+costac)
costa+costac
= 30.000 e×(1−0.7)−(4.000 e+2.000 e)
4.000 e+2.000 e
= 0.50. In a
similar manner we can compute ROA for all the other countermeasures as shown
in Table 2.
The resulting defense tree labeled with ROI and ROA for each countermeasure
and attack is depicted in Figure 7.
Install a video
survellaince
equipment
Assume a
security guard
Install a video
survellaince
equipment
Assume a
security guard
Install a
security door
Install a
safety lock
Steal the
server
Go out
unobserved
Have the keys
Break down
the door
Go out
unobserved
ROA=4.40
ROA=0.50
ROA=1.73
ROA=4.19
ROA=4.45
ROA=1.63
ROI= -0.70
ROI=3.20
ROI= -0.63
ROI=-0,69
ROI=5.20
ROI=-0.61
c1
c2
c3
c4
c2
c3
a1
a2
Fig. 7. The defense tree of Fig. 6 decorated with ROIs and ROAs
4.1
Selection of a Single Countermeasure/Attack
To model the defense tree as a strategic game we consider the set of strategies for
the defender as composed by single countermeasures as represented in Figure 6.
In a similar manner the strategies for the attacker are just a1 (the left hand side
of the tree) and a2 (the right hand side). The utility functions are the indexes
ROI and ROA introduced in Section 2 as described in the bimatrix of Figure 8.
Now, using Deﬁnition 1, we look for a Nash Equilibrium of the game.
From the attacker’s viewpoint: if the defender plays the strategy c1 the best
response of the attacker is to play the strategy a2, if he plays strategies c2, c3
or c4 the best response is strategy a1. Instead, from the defender’s viewpoint: if
the attacker plays strategy a1 the defender’s best response is to play c1, while if
she plays a2 or a3 the defender plays c4.
As consequence there are no Nash Equilibrium with pure strategies. In fact,
our game is similar to a constant sum game where the payoﬀs of the two players
have opposite rewards. The absence of so-called equilibrium points or saddle
points (optimal for all players at once) means that there are no optimal situations
in the sense that they provide to each participant the maximum of what he/she
can get given the acts of the other players.

12
S. Bistarelli, M. Dall’Aglio, and P. Peretti
a1
a2
c1
3.20,0.50 -1.00,6.14
c2 -0.70,4.40 -0.69,4.19
c3 -0.63,1.73 -0.61,1.63
c4 -1.00,6.50 5.20,4.45
Fig. 8. Bimatrix for the attacker/defender game with single selection of countermea-
sures/attacks
So there are no stable strategies to follow for the defender and the attacker in
the game. In spite of the absence of a rational choice (some advice can be however
given following other approaches [1]), when the game is repeated many times,
some optimal lines of behavior can be found. To ﬁnd them one must extend
the analysis to include the adoption of mixed strategies by the players. As the
criterion for the choice of optimal mixed strategies one takes the mathematical
expectation value of the payoﬀwhich shows how much one can win on average
by repeating the game many times.
Using Deﬁnition 4 (and Gambit [11], a tool for computing equilibria2) we look
for mixed strategy equilibria.
The result is that there is one equilibrium if the defender plays the strategy
c1 with probability 205
769 and c4 with probability 564
769, and if the attacker plays
the strategy a1 with probability
31
52 and a2 with probability 21
52. We see that
the probability for the two attacks are pretty close, so the system administrator
cannot consider to reduce the attention to only one of the two branches. More-
over, it seems that the best that a system administrator can do is to invest in
the countermeasure c1 to avoid the ﬁrst attack and in the countermeasure c4 to
avoid the second attack.
Notice however that this strategy is not so natural; in fact, why not to invest in
countermeasure c3 to be able to partially cover both the attacks? In the example
studied here we do not study indeed the possibility to have both the attacks oc-
curring simultaneously and to have more then one countermeasure implemented.
4.2
Selection of Set of Countermeasures/Attacks
In the previous strategic game we considered only one attack/countermeasure
strategy by each players. Here, instead, each player can play any set of counter-
measures/attacks together (but we have also the possibility to select no attack
or countermeasure).
In order to avoid some technical problems (division by 0) when dealing with
empty sets of countermeasures or attacks we change the utility functions for the
two players. We retain the numerator from the old utility functions.
ud = ALE × RM −CSI
ua = GI × (1 −RM) −(costa + costac)
2 Available at http://econweb.tamu.edu/gambit/

Strategic Games on Defense Trees
13
∅
a1
a2
{a1,a2}
∅
0, 0
0, 26.000
0, 25.800
0, 21.800
c1
-1.500, 0
4.800, 3.000
-1.500, 25.800 11.310, -1.200
c2
-3.000, 0
-2.100, 22.000
-2.070, 21.800 -1.170, 17.800
c3 -12.000, 0
-7.500, 9.500
-7.350, 9.300
-2.850, 5.300
c4
-300, 0
-300, 26.000
1.560, 19.600
3.360, 15.500
{c1,c2}
-4.500, 0
1.800, 2.000
-3.570, 21.800
8.310, -2.200
{c1,c3} -13.500, 0
-7.200, 1.500
-8.850, 9.300
-690, -2.700
{c1,c4}
-1.800, 0
4.500, 3.000
60, 18.600 11.010, -1.500
{c2,c3} -15.000, 0 -10.500, 8.500
-10.350, 8.300
-5.850, 4.300
{c2,c4}
-3.300, 0
-2.400, 22.000
-1.440, 18.600
360, 14.500
{c3,c4} -12.300, 0
-7.800, 9.500
-7.650, 9.100
-3.150, 5.000
{c1,c2,c3} -16.500, 0 -10.200, 500
-11.850, 8.300
-3.690, -3.700
{c1,c2,c4}
-4.800, 0
1.500, 2.000
-2.940, 18.600
8.010, -2.500
{c1,c3,c4} -13.800, 0
-7.500, 1.500
-9.150, 9.100
-990, -3.000
{c2,c3,c4} -15.300, 0 -10.800, 8.500
-10.650, 8.100
-6.150, 4.000
{c1,c2,c3,c4} -16.800, 0 -10.500, 500
-12.150, 8.100
-3.990, -4.000
Fig. 9. Bimatrix for the attacker/defender game with a set selection of countermea-
sures/attacks
Using the new utility functions we obtain the strategic game of Figure 9.
Once again there are no Nash Equilibria with pure strategy, but Gambit com-
putes a mixed equilibrium where the defender plays the strategy c4 with prob-
ability 39
55 and {c1, c4} with probability 16
55, and the attacker plays the strategy
a1 with probability
5
21 and a2 with probability 16
21.
As a side result we note that two compound strategies by the attacker, namely
∅and {a1, a2}, are uniformly dominated by the simple strategies a1 and a2. This
shows that the attacker has no interest in combining the actions together.
5
Conclusions and Future Work
The use of game theory, allow us to model the interaction between the attacker
and the defender: they represent two players with opposite goals. The tactical
choices of each one of the player strictly depends from the moves of the other.
In particular, when an attacker has to select a possible attack for an asset,
he/she has to consider necessarily the possible countermeasure that the defender
have introduced into the system; vice-versa, when a system administrator has
to select which countermeasure introduce in order to protect the system, he has
to consider the possible attacks that the attacker could perform.
Using the Nash Equilibria allow us to model the above situation where at-
tacker and defender need to take some decision. The Nash equilibrium has been
used [10][5] to determine the best move of the two players, by considering the
ﬁx point of the interactions between attacker and defender.
In this paper we ﬁrst, used defense trees as extension of attack trees with coun-
termeasures and economic quantitative indexes for modeling attack scenarios.

14
S. Bistarelli, M. Dall’Aglio, and P. Peretti
Then such scenarios are analyzed as strategic games. The strategies of the two
players (the defender can select countermeasures and the attacker can choose
among several vulnerabilities to exploit) lead to diﬀerent payoﬀs represented as
economic indexes. In particular ROI and ROA are used. The study conﬁrms that
investments beyond a certain level do not produce any beneﬁcial eﬀect after a
certain point are not anymore useful [7] (so, only a subset of the countermeasure
usually has to be considered).
The methodology presented in this paper provides a basis for future work
along several research directions.
While it may seem obvious to compute the solution cost of a set C = {c1, c2}
of countermeasures as the sum CSIC = CSIc1 + CSIc2 of the costs of the single
countermeasures in C, it should be noticed that the total cost of implementing
a set of countermeasures could realistically be less than CSIC (e.g. discounted
price of bundled security solutions) or greater than CSIC (e.g. when countermea-
sures must be managed by diﬀerent employees, due to the existence of separation
of duty constraints [2]).
On the other hand, it is not clear how to compute the value of the Risk
Mitigated attribute for a set of countermeasures {c1, c2}, as any value between
max(RMc1, RMc2) (one countermeasure strictly entails the other) and (RMc1 +
RMc2) (completely independent countermeasures) appears to be acceptable de-
pending on the type and nature of countermeasures and the asset being
protected.
We plan to extend this work by considering n player games (where we have 1
defender and n−1 attackers). This could lead to interesting discussion about the
amount of cooperation between the attacker. Usually attackers try to cooperate,
unless the cooperation reduces their gain too much (that is, the beneﬁt coming
from the attack has to be divided among them).
When considering several attackers also notions of types (and bayesian games)
could be important. From which type of attacker we expect to have the attack?
We can diﬀerentiate between attacker w.r.t. their propension/aversion to risk?
Dynamic games provide another source for extension. Repeat games with the
normal games described above as a stage game could be considered. As well a
game when both players reﬁne their information as the sequence of attacks and
countermeasures progress.
We hope our work can help encourage research and experimentation with
the use of economic indexes and combined development of attacker/defender
perspectives during evaluation of alternative security investments.
References
1. Bistarelli, S., Fioravanti, F., Peretti, P.: Defense tree for economic evaluations of
security investment. In: 1st International Conference on Availability, Reliability
and Security (ARES’06), pp. 416–423 (2006)
2. Clark, D.D., Wilson, D.R.: A comparison of commercial and military computer
security policies. In: IEEE Symposium on Computer Security and Privacy (1987)

Strategic Games on Defense Trees
15
3. Cremonini, M., Martini, P.: Evaluating information security investments from at-
tackers perspective: the Return-On-Attack (ROA). In: Fourth Workshop on the
Economics of Information Security (June 2005)
4. Foster, N.L.: The application of software and safety engineering techniques to se-
curity protocol development. PhD thesis, University of York, Department of Com-
puter Science (2002)
5. Fudenberg, D., Tirole, J.: Game Theory. MIT Press, Cambridge (1991)
6. Gibbons, R.: A Primer in Game Theory. Pearson Higher Education (1992)
7. Gordon, L.A., Loeb, M.P.: The economics of information security investment. ACM
Trans. Inf. Syst. Secur. 5(4), 438–457 (2002)
8. Howard, LeBlanc.: Writing Secure Code. Microsoft Press, Redmond (2002)
9. Krutz, R.L., Vines, R.D., Stroz, E.M.: The CISSP Prep Guide: Mastering the Ten
Domains of Computer Security. Wiley, Chichester (2001)
10. Liu, Y.: Intrusion Detection for Wireless Networks. PhD thesis, Stevens Institute
of Technology (2006)
11. McKelvey, R.D., McLennan, A.M., Turocy, T.L.: Gambit: Software tools for game
theory (version 0.2006.01.20) (2006), http://econweb.tamu.edu/gambit
12. Meritt, J.W.: A method for quantitative risk analysis. In: Proceedings of the 22nd
National Information Systems Security Conference (October 1999)
13. Osborne, M.J.: An introduction to game theory. Oxford Univ. Press, Oxford (2003)
14. Schechter, S.E.: Computer Security Strength & Risk: A Quantitative Approach.
PhD thesis, Harvard University (May 2004)
15. Schneier, B.: Attack trees: Modeling security threats. Dr. Dobb’s Journal (1999)
16. Schneier, B.: Secrets & Lies: Digital Security in a Networked World. John Wiley
& Sons, Chichester (2000)
17. Sonnenreich, W., Albanese, J., Stout, B.: Return On Security Investment (ROSI):
A practical quantitative model. In: Security in Information Systems, Proceedings
of the 3rd International Workshop on Security in Information Systems, WOSIS
2005, pp. 239–252. INSTICC Press (2005)
18. Stoneburner, G., Goguen, A., Feringa, A.: Risk management guide for information
technology systems. Nist special publication 800–830, NIST, National Institute of
Standard Technology (July 2002)

Timed Calculus of Cryptographic Communication
Johannes Borgstr¨om1, Olga Grinchtein2, and Simon Kramer3
1 EECS, Technical University of Berlin
jobo@cs.tu-berlin.de
2 IT, Uppsala University
olgag@it.uu.se
3 Ecole Polytechnique F´ed´erale de Lausanne (EPFL)
simon.kramer@a3.epfl.ch
Abstract. We extend the (core) Calculus of Cryptographic Communication (C3)
with real time, e.g., time stamps and timed keys. We illustrate how to use this
extended calculus (tC3) on a speciﬁcation and veriﬁcation case study, namely the
failure of the Wide-Mouthed-Frog protocol in its original, i.e., timed, version.
Keywords: Applied process calculi, timed cryptographic protocols, formal mod-
elling, model-based speciﬁcation and veriﬁcation.
1
Introduction
Timed — as opposed to untimed — cryptographic protocols have received compar-
atively little attention from the formal methods community so far. The only timed
formalisms for the modelling, speciﬁcation, and veriﬁcation of such protocols we are
aware of are Timed CSP [1], tock-CSP [2], tCryptoSPA [3], the timed Spi-Calculus [4],
and the (unnamed) process model from [5] (which we will refer to as tBEL). (Although
Timed CSP and tock-CSP are special-purpose w.r.t. the temporal aspect, they — like
core CSP — are actually not special-purpose w.r.t. the cryptographic aspect.) For prac-
tical usability, special-purpose models of timed cryptographic protocols are preferable
over their general-purposecounterparts because (untimed) general-purposemodels tend
to create considerable (en)coding overhead: “[...] the coding up required would make
the complex behaviour difﬁcult to understand, and it is preferable to use a language
designed to express such real-time behaviour.” [2].
In tock-CSP, tCryptoSPA, and timed Spi-Calculus time is natural-number valued.
tock-CSP and tCryptoSPA provide local processes that globally synchronise through
so-called tock events resp. tick actions, which represent the passage of one unit of time.
And the timed Spi-Calculus provides a process constructor for querying a global clock.
Thus, tock-CSP, tCryptoSPA, and the timed Spi-Calculus lack local clocks that po-
tentially advance at different rates across different processes/locations. However [6],
“[c]locks can become unsynchronized due to sabotage on or faults in the clocks or the
synchronization mechanism, such as overﬂows and the dependence on potentially unre-
liable clocks on remote sites [...]”. Moreover [6], “[e]rroneous behaviors are generally
expected during clock failures [...] ” Hence, a faithful model of timed cryptographic
protocols must allow for potentially desynchronised, local clocks.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 16–30, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

Timed Calculus of Cryptographic Communication
17
In tBEL, time — in particular, a time stamp — is real-number valued, yielding a
dense time domain. We contend that real-valued time-stamps are too ﬁne-grained be-
cause protocol messages have ﬁnite length, which implies that real numbers are not
transmittable as such. Moreover, real clocks only have ﬁnite precision. tBEL does pro-
vide local clocks, yet they “advance at the same rate as time.” [5, Page 2]. Further,
adversarial break of short-term keys is modelled only indirectly with a parallel process
rather than directly as part of the adversary model. Furthermore, tBEL lacks a process
equivalence. On the other hand, tBEL comes with a (third-order) special-purpose logic
for reasoning about tBEL models, and a decision procedure for a class of reachability
properties of bounded protocols based on syntactic control points. In our opinion, tBEL
and its associated logic are unnecessarily domain-speciﬁc. They seem to have been built
from scratch rather than as Occham’s-razor extensions of untimed formalisms. Adding
real-time to a model or logic without explicit time can be simple [7].
In contrast to the models discussed, tC3 extends C3 [8] with (1) rational-number
valued time (which still is dense), (2) local clocks that may progress at different rates
across different locations, and (3) adversarial break of short-term keys based on
ciphertext-only attacks enabled by key expiration. Moreover, tC3 comes with a notion
of observational process equivalence for model-based protocol speciﬁcation and veri-
ﬁcation. As a property-based complement, we have also co-designed a logic, namely
tCPL [9,10], for tC3. The three primary features of the co-design of tC3 and tCPL are
that (1) tC3’s notion of execution is a temporal accessibility relation for tCPL’s tem-
poral modalities, (2) tC3’s notion of observational equivalence and tCPL’s notion of
propositional knowledge have a common deﬁnitional basis, namely an epistemic acces-
sibility relation deﬁned in terms of structurally indistinguishable protocol histories, and
(3) execution constraints of tC3-processes are checkable via tCPL-satisfaction. These
three features, especially Feature 2, are the result of our wholistic conception of model-
based (process algebra) and property-based (modal logic) speciﬁcation and veriﬁcation
as two truly complementary approaches. Other important features of C3, and thus of
tC3, are explicit out-of-band communication and history-based key (and for tC3, clock
value) lookup. C3 neatly extends to tC3 by maintaining backwards compatibility. Es-
sentially, only two additional axioms (and no modiﬁed axioms/rules!) are needed in its
operational semantics.
2
Deﬁnition
Our timed Calculus of Cryptographic Communication is a conservative extension of a
core calculus [8]. Core C3 consists of a language of distributed processes and an as-
sociated notion of concurrent execution in the style of structural operational semantics
(SOS). Its modelling hypotheses are those of abstract ideal cryptography and interleav-
ing concurrency. Cryptography is abstract in the sense that communicated information
atoms (names) are logical constants and communicated information compounds are
syntactic terms. We use pattern matching as a linguistic abstraction for cryptographic
computation. Cryptography is ideal in the sense that cryptographic primitives are as-
sumed to be perfect, i.e., unbreakable. Process execution engenders the activity of pro-
tocol participants and the standard Dolev-Yao adversary
 , i.e., the generation of a

18
J. Borgstr¨om, O. Grinchtein, and S. Kramer
history of legitimate resp. illegitimate protocol events, and the evolution of their respec-
tive knowledge computed from that history.
We extend C3 to tC3 according to the following two design principles: ﬁrst, for every
legitimate participant and the adversary, we introduce a rational-number valued local
clock with an associated drift rate, where the adversary’s clock always displays the ac-
tual time; and second, we model the passage of (real) time by means of the adversary,
who, at any (logical) time of protocol execution, may advance the time by an arbitrary
but always ﬁnite amount by advancing her local clock. In this way, the adversary is in
full control of time (subject to monotonicity). Adding communication and/or computa-
tion delays imposes non-zero lower bounds on the advancement of time between certain
pairs of actions, and could be handled by only considering traces where the adversary
respects these bounds.
2.1
Syntax
The syntactic novelties in tC3 w.r.t. C3 are the following. A set
  :  
  of
time values t with the associated sort
; temporal expressions E ::  t
    E
E
    E
E for
the calculation of temporal intervals and bounds; a binary relational symbol E@a for
testing the clock of a participant a with the time value E; temporal inequalities E
 E
 for the comparison of time values; and an action
 E for clock (re)setting.
The full, sorted alphabet for information atoms in tC3 is the following.
Deﬁnition 1 (Sorted names). Names n are participant names c   d
	
 , the adversary’s
name
 , symmetric keys k
	
, private keys p
	

, and nonces x
	
 (also
used as session identiﬁers). We assume that given a private key, one can compute the
corresponding public key, as in DSA and Elgamal.
The sorts
 corresponding to these kinds of names are
,
	
,
,

, and
, respec-
tively.
,

, and
 are the sorts
 of freshly generable names.
These sorted names (logical constants) are part of the language

 of communicated
messages in tC3.

 contains, besides the usual cryptographic constructions, a distin-
guished abstract message
 . This message is a computational artiﬁce to represent the
absence of intelligibility, just as the number zero is a computational artiﬁce to represent
the absence of quantity. The abstract message is very useful for doing knowledge-based
calculations (e.g., as effectuated by tC3’s observational equivalence, cf. Section 2.3) just
as the number zero is very useful (to say the least) for doing number-based calculations.
The abstract message is also very useful for adding backwards-compatible extensions to
the core calculus as we will show on the example of the present (real-time) extension in
the sequel. Symmetric keys may be compound for key agreement (in addition to mere
key transport).
Deﬁnition 2 (Messages and Message forms). The structure of protocol messages M
	

 is deﬁned in Table 1. Message forms F are messages with variables v
	
, and are
used in process terms where they may instantiate to (transmittable) messages. Sorts of
compound messages are macro-deﬁnable (cf. [8]).
tC3-processes are parallel compositions of located threads. A non-idle thread T located
at the participant c and session x, written cx[ T ], has either an action preﬁx or a lookup

Timed Calculus of Cryptographic Communication
19
Table 1. Protocol messages
M ::   n
(names)
    (the abstract message)
    t
(time values)
    p
 (public keys)
    M

(message hashes)
   
M

M
(symmetric message ciphers)
   
M


 p
 (asymmmetric message ciphers)
   
M


 p
(signed messages)
    (M
  M)
(message tuples)
preﬁx. The action preﬁxes
a F and
a F express insecure (intercepted by the
adversary) resp. secure (unobservable by the adversary) output of F to a;




and
a


 express insecure resp. secure input (from a) of a message matching
the pattern
 and having the property
;
 (v :

  (O  V)) expresses the generation
and binding to the variable v of a fresh name of type
 tagged with (O  T), where O
(a tuple of participant names) stipulates intended ownership and V (a pair of temporal
expressions) validity of the generated name; and
 E expresses the (re)setting of the
local clock to the value E. The lookup preﬁx
a (v :

  O)
 expresses the lookup
and binding to v of a name of type
 generated by a with ownership tag O.
Observe that thanks to sorts, tags, and the abstract message the new-name and the
lookup-preﬁx can also elegantly handle timed information: new-timed-key generation
as
 (v :
  (O  V)), timed-key lookup as
a (v :
  O)
 , and clock lookup as
a (v :

  )
 . Finally, our communication model is based on participants rather
than channels for separating speciﬁcation from implementation concerns.
Deﬁnition 3 (Processes). The structure of protocol processes P
	
 is deﬁned in Ta-
ble 2. There, a   b
	
 
;
 means “freshly generated”, : “has sort”, k “knows”,
and
 “is a subterm of”. A process P is epistemically local :iff for all located threads
cx[ T ] in P and for all a x
 n (O  V), a k F, and t@a in T, a
  c. Implementa-
tion processes must be epistemically local, whereas speciﬁcation processes need not
be. In addition, in implementation processes
v must not bind v in any F, whereas in
speciﬁcation processes this need not be.
Action preﬁxes
, as opposed to the lookup preﬁx, generate events when executed. Ac-
tion preﬁxes for secure I/O generate unobservable events; they model out-of-band com-
munication such as trusted couriers, personal contact between communicating parties,
and dedicated communication links. The same preﬁxes can also be used for (1) encoding
extensions to the core calculus, such as vertical composition (i.e., sub-protocol calls),
conditionals (i.e., execution guards); (2) deﬁning speciﬁcation processes relied upon
by equivalence-based speciﬁcation of secrecy; and (3) deﬁning initialisation traces (cf.
Section 3). The purpose of including session ids x in locations cx[ T ] is to enable the
factorisation of the history of a protocol execution into individual sessions (cf. strands
in strand spaces).

20
J. Borgstr¨om, O. Grinchtein, and S. Kramer
Table 2. Protocol processes
P ::   cx[ T ]
    P  P
T ::   1
   T
    a (v :

  O)
 T
 ::  a F
   a F
   	




   	a




   
 (v :

  (O  V))
    E
 ::   F
    (
 )
   


F
   



 F
   



 F
 ::   E@a
    E
 E
    ax
 n(O  V)
    n :

    a k F
    F
 F
   
   


   	v()
2.2
Semantics
The semantic novelties in tC3 w.r.t. C3 are the following. Calculation with temporal
expressions: a (partial) evaluation function
  takes temporal expressions to time values
(where
 

 and suchlike are undeﬁned). Protocol events: we add events
(c  x  t)
for the setting of c’s clock to clock value t by c in session x. By convention, these events
are unobservable by the adversary, i.e., they are secure. Clock setting: we add the rule
SET (cf. Table 6) to the operational semantics. Clock drifting: we assume the existence
of a total function
Æ from participants to drift rates (rational numbers). Advancement of
time: we add the rule TIME (cf. Table 6) to the operational semantics.
tC3 is a reduction calculus on protocol states (P
  ), i.e., pairs of a process term P
	

and a protocol history
 	
. Protocol histories are records of past protocol events
,
comprising: generation of a fresh name n with intended ownership O and (absolute)
temporal validity V in session x by c, written
(c  x  n   (O  V)); insecure input of M [...],
written
(c  x  M); secure input of M from d [...], written
(c  x  M
  d); insecure output
of M to d [...], written
(c  x  M
  d); secure output of M to d [...], written
(c  x  M
  d);
and setting of a clock to some time value, as mentioned above. Protocol histories
 	

are simply ﬁnite words of protocol events
, i.e., event traces:
  :: 	
    
 where
	
denotes the empty protocol history. tC3 employs pattern matching as a linguistic ab-
straction for cryptographic computation, which is straightforward to implement [11].
When a message matches a pattern, the result is a substitution relating variables to
matched subterms. Substitutions are partial functions from variables to messages, and
are lifted to terms as usual. Matching is computed by the partial function match deﬁned
in Table 3. There,
 denotes composition, if the domains of the operands are disjoint,
and is otherwise undeﬁned. Input guards
 are expressed in a decidable sub-language
of our co-designed logic CPL. Their satisfaction is history dependent. Speciﬁcally, the
knowledge of a participant depends only on the preceeding events witnessed by the par-
ticipant in question. As an example, this lets us easily express that keys to be looked up
must already be known to the participant looking them up.
Deﬁnition 4 (Satisfaction). Satisfaction is deﬁned in Table 4. There datac(  ) denotes
the set of data that c has generated, received, or sent in
 ; and analz
  and synth de-
note message analysis resp. synthesis [12,8]. Our analysis has the additional rule “if
 
  t
 @ 

vv
 v
  (vv
  k
(v
    (tb
  te))) and
  
 m(   k

m
k)

t(t@ 

te
 tb

 t
 
 t) for some preﬁx
   of
  then k
	 analz
 ( )” for some set
 of data
known to the adversary. This rule models adversarial break of short-term keys based

Timed Calculus of Cryptographic Communication
21
Table 3. Pattern matching
match(v  M) :  M
v

match(M
  M) :  
match((
 
)  (M
  M
)) :   match(
  M)
 match( 

  M
)
match(


M
  
M

M
 ) :   match(
  M)
match(



 p
 
M


 p
 ) :   match(
  M)
match(



 p
  
M


 p) :   match(
  M)
on ciphertext-only attacks enabled by key expiration (te
 tb

 t
 
 t). Finally, our
synthesis has the additional axiom “t
	 synth( )” for some set
 of data known to the
concerned agent.
Table 4. Satisfaction
 
  E@a
:iff
 E

  t

Æa

	 where
– t denotes the time value of a’s last clock-set event in
 , i.e., there are
 1
  2, x s.t.
   1

(a  x  t)
Æ
 2 and there is no x

  t
 s.t.
(a  x

  t
)

 2
–
Æa


 denotes the drift rate of a’s local clock
–
	 denotes the temporal difference between Eve’s last clock-set event
before
(a  x  t) and Eve’s last clock-set event so far in
 , i.e.,
	
 


















t2
 t1
if for i

1  2 there are
 i
   
i , ti s.t.
 i
  
i

(
    ti)
Æ
 
i and there is no t

i s.t.
(
    t

i)

 
i , and
0
otherwise.
 
  E
 E

:iff
 E
 is smaller than or equal to
 E


 
  cx
 n(O  V)
:iff
(c  x  n  t
  (O  V))

  
  c k M
:iff M
 synth(analz
 (datac( )))
 
  M
 M

:iff M is a subterm of M

 
  n :

:iff n has sort

 
 
:iff not
 
 
 
 



:iff
 
 
 and
 
 

 
 	v(
)
:iff for all M

,
 
 M
v


The key and clock store of each participant are induced by protocol events. Keys and
clocks are looked up in the protocol history w.r.t. the local view of each participant.
Lookup of keys additionally refers to their creator and the tag they were given at cre-
ation. We assume that the creator and the tag associated with a key are authentic and
universally available (given possession of the key). This gives an approximation of
certiﬁcate-based key lookup, where we explicitly model key distribution but key meta-
data is magically protected. A lookup succeeds when the desired tag is a subterm of the
one used at the creation of the key. This lets us model cases where the same shared key
is to be used whenever two participants take part in a protocol, independently of their

22
J. Borgstr¨om, O. Grinchtein, and S. Kramer
roles, as well as making more complex key-sharing arrangements possible. We model
lookup with the predicate looksUp(c  x  n  
  d
  O) pronounced “c in session x looks up
n of type
 generated by d with a tag containing O” and deﬁned in Table 5. The condi-
tions enforce that the retrieved name n is either the local time (n@c) or is a key which
(1) has the desired type (n :
); (2) was known by c (c k n); (3) was generated by d
(d
x
  v(o   (tb
  te))); (4) has a compatible, intended ownership (O
 o); and (5) is
perceived as timely by c ( tc(tc@c
 tb
 tc
 te)).
Table 5. Lookup predicate
looksUp(c  x  n 
  d
  O) :   n :

 c k n
 (n@c

x

votb
te(d
x

 v(o  (tb
  te))

(n
  v
 n
  v
 )
 O
 o

tc(tc@c
 tb
 tc
 te)))
We are now ready to deﬁne our (process) reduction calculus tC3.
Deﬁnition 5 (Process calculus). Let


 ( 

)
 ( 

), deﬁned in Table 6,
denote reduction of protocol states

	


. Then tC3 denotes the Timed Calculus of
Cryptographic Communication as deﬁned below.
tC3 : 


 

The generation of a new name (Rule NEW and NEW-EVE) is possible only if that name
has not been generated yet, i.e., names are always fresh w.r.t. the current state. The ad-
versary
  may generate a new name at any time. Insecure input (Rule IN) is generated
by the adversary and may consist in any message from her knowledge that matches the
input pattern
 and that satisﬁes the input constraint
. Successful input results in the
substitution of the matching message parts for the matched variables in the receiving
thread. Secure communication (Rule SOUT, SIN and SCOM-L, with SCOM-R being
tacit) is synchronous. To achieve this, we introduce two auxiliary transition relations


 and


 not visible on the top level. Insecure communication between two legit-
imate participants is asynchronous because it goes through the adversary, and secure
communication is synchronous because it does not. Execution of parallel processes
happens via interleaving concurrency (Rule PAR-L, PAR-R being tacit). Finally, ob-
serve how non-determinism abstracts away four determining choices in the execution
of a protocol, i.e., the choice of (1) the time value by which the adversary advances
the time, (2) the message sent by the adversary in an insecure input, (3) the new name
selected at name generation time, and (4) the scheduling of located threads.
2.3
Process Equivalence
We deﬁne a notion of observational equivalence for tC3-processes based on the con-
cepts of cryptographic parsing (inspired by [13]) and structurally indistinguishable
protocol histories. Cryptographic parsing captures an agent’s capability to understand
the structure of a cryptographically obfuscated message. The idea is to parse unintelli-
gible messages to the abstract message
 .

Timed Calculus of Cryptographic Communication
23
Table 6. Process and thread execution
Below,
 



  

  

.
TIME
 
 t
(t
@ 
 t

 t

)
P
 	



P
 
(
    t)
	
SET
ax[
 t
T ]
 	


 ax[ T ]
 
(a  x  t)
	
NEW
 
  n :


axov(ax
 n(o  v))
cx[

 (v :

  (O  V))
T ]
 	



cx[
n
v
T ]
 
(c  x  n  (O  V))
	
NEW-EVE
 
  k (O  V)

axov(ax
 n(o  v))
P
 	



P
 
(
    n  (O  V))
	
OUT
cx[
d M
T ]
 	



cx[ T ]
 
(c  x  M
  d)

	(
    M)
	
IN
 
  k M
 match( 
  M)
cx[
	



T ]
 	



cx[ match(
  M)T ]
 
(
    M
  c)

	(c  x  M)
	
SOUT
cx[
d M
T ]
 	
 



cx[ T ]
 
(c  x  M
  d)
	
SIN
 
  match(
  M)
d
x[
	c




T ]
 
(c  x

  M
  d)
	
 



d
x[ match(
  M)T ]
 
(c  x

  M
  d)

	(d
  x  M
  c)
	
SCOM-L
P
 	
 


P

 
	
Q
 
	
 


Q

 
	
P  Q
 	


P
  Q

 
	
LOOKUP
cx[
n
v
T ]
 	
 

cx[ T
 ]
 
	
 
  looksUp(c  x  n 
  d
  O)
cx[
 d (v :

  O)
 T ]
 	
 

cx[ T
 ]
 
	
PAR-L
P
 	
 

P

 
	
P  Q
 	
 

P
  Q
 
	

24
J. Borgstr¨om, O. Grinchtein, and S. Kramer
Deﬁnition 6 (Cryptographic parsing). The cryptographic parsing function 
 
 a as-
sociated with an agent a
	
 and a protocol history
 	
 (and complying with the
assumptions of perfect cryptography) is an identity on names, the abstract message,
time values, and public keys; and otherwise acts as deﬁned in Table 7.
Table 7. Parsing on cryptographic messages

 M
 
 a :  






  M 
 a

if
 
  a k M, and
 otherwise.


M

M
  
 a :  







 M 
 a

 M
  
 a
if
 
  a k M
, and
 otherwise.


M


 p
  
 a :  







 M 
 a


 p
 if
 
  a k p
 (a k M
 a k p
 ), and
 otherwise.


M


 p 
 a :  







 M 
 a


 p
if
 
  a k p
 , and
 otherwise.
 (M
  M
) 
 a :   ( M 
 a
   M
 
 a)
For notational convenience, we subsequently write
(a) for any protocol event as de-
ﬁned in Section 2.2,
(a   n) for any of these name-generation events,
(a   M) for any of
these communication events, and ˆ
(a) for any of these secure events.
Deﬁnition 7 (Structurally indistinguishable protocol histories). Two protocol histo-
ries
  and
   are structurally indistinguishable from the viewpoint of an agent a, written
 a
  , :iff a observes the same event pattern and the same data patterns in
  and
  .
Formally, for all
    	
,
 a
   :iff
 (    )
a
   where,
– given that a is a legitimate participant or the adversary
 ,
1.
	
(    )
a
	
2.
 l
(    )
a
 r
 l

(a   n)
(    )
a
 r

(a   n)
3.
 l
(    )
a
 r
 l

(a   M)
(    )
a
 r

(a   M
 )
 M 
 a
   M
  
  a
– given that a is a legitimate participant,
4.
 l
(    )
a
 r
 l

(b)
(    )
a
 r
a
 b
 l
(    )
a
 r
 l
(    )
a
 r

(b)
a
 b
– given that a is the adversary
 ,

Timed Calculus of Cryptographic Communication
25
4.
 l
(    )
 
 r
 l
 ˆ
(b)
(    )
 
 r
 
 b
 l
(    )
 
 r
 l
(    )
 
 r
 ˆ
(b)
 
 b
5.
 l
(    )
 
 r
 l

(b   x  M)
(    )
 
 r

(b   x  M
 )
 M 
  
   M
  
   
6.
 l
(    )
 
 r
 l

(b   x  M
  c)
(    )
 
 r

(b   x  M
   c)
 M 
  
   M
  
   
Note that the observations at the different (past) stages
 l and
 r in
  and
   respectively
must be made with the whole (present) knowledge of
  and
   (cf.
 l
(    )

 r). Learning
new keys may render intelligible past messages to an agent a in the present that were
not intelligible to her before.
Remark 1. For all agents a including
 ,
a



 is
1. an equivalence with an inﬁnite index due to fresh-name generation
2. not a right-congruence due to the possibility of learning new keys
3. a reﬁnement on the projection

a of
 onto a’s view [14]
4. decidable
We lift structural indistinguishability from protocol histories to protocol states, i.e., tu-
ples of a protocol term and a protocol history.
Deﬁnition 8 (Structurally indistinguishable protocol states). Let P1 and P2 denote
two tC3-processes. Then two protocol states (P1
  1) and (P2
  2) are structurally indis-
tinguishable from the viewpoint of an agent a, written (P1
  1)
a (P2
  2), :iff
 1
a
 2.
This relation coincides with the relation of epistemic accessibility deﬁning the epistemic
modality of CPL [9], which reﬂects the intimate co-design of C3 and CPL. We are
ﬁnally ready to deﬁne observational equivalence for tC3-processes.
Deﬁnition 9 (Trace-equivalent cryptographic processes). For all agents a including
  and for all
1
 2
	


,
–
2 trace-reﬁnes
1 from the viewpoint of a, written
1


a
2, :iff for all

 2
	


,
if
2




 2 then there is

 1
	


 s.t.
1




 1 and

 2
a

 1; and
–
1 and
2 are trace-equivalent from the viewpoint of a, written
1


a
2, :iff
1


a
2
and
2


a
1.
3
Case Study
We illustrate how to use tC3 on a speciﬁcation and veriﬁcation case study, namely the
failure of the Wide-Mouthed-Frog protocol (WMF), a server-based key-transport pro-
tocol employing symmetric cryptography, (cf. Table 8).
We recall that model-based (e.g., process algebraic) correctness statements of cryp-
tographic protocols enunciate an observational equivalence that is supposed to hold

26
J. Borgstr¨om, O. Grinchtein, and S. Kramer
Table 8. Protocol narration for WMF
1a


 :

1b


 :

((t

 )  k( 	
))
k(  
 	)
2



:

((t

 )  k( 
	
))
k( 

 	)
between two process models (terms) of the protocol under scrutiny. The choice of the
actual process terms depends on the cryptographic goal that the correctness statement
is intended to encode. For example, authenticity goals for a cryptographic protocol can
be encoded as an observational equivalence between, on the one hand, an obviously
(via different kinds of “magic”) correct speciﬁcation process and, on the other hand,
an implementation process expressing the protocol as it would be coded in a realistic
implementation. In our case, we are interested in a timeliness goal for WMF, namely
that the responder only accepts the session key within a ﬁxed interval of time. It is
well known that WMF fails to meet this goal (cf. Table 9). As it turns out, this goal
can be checked in a similar set-up as for authenticity properties, i.e., as an equivalence
from Eve’s viewpoint ( 

 ) between an implementation and a speciﬁcation term (cf.
Table 10).
Table 9. Attack narration for WMF
1a



	



:

1b



	



:

((t

 )  k( 	
))
k( 

 	)
2





 :

((t


 )  k( 	
))
k(  
 	)
1a






:

1b






:

((t


 )  k( 	
))
k(  
 	)
2





:

((t


 )  k( 	
))
k( 

 	)
The “magic” in our speciﬁcation process is the non-trivial and epistemically non-
local input guard of the last input of the responder process. In the implementation, we
only check the types of the atoms in the message and that the time stamp is recent. In
the speciﬁcation, we additionally check that the key has been created by the initiator
for communication with the responder and that the local time is within the validity
interval of the key, as expressed by the formula
t
t
 x(othx
 key
((slf
  oth)   (t
  t
 ))

ts(ts@slf
t
 ts
 t
 )). It is possible to have the simple time stamp check succeed but
the “more obviously correct” validity check fail, as evidenced below.
Observe that lookup of local time is done in two different ways, namely imperatively
by means of the get-preﬁx, and declaratively by means of the @-predicate. We assume
that the local clocks are accurate, i.e., all drift rates are 1.
WMF presumes that the server shares a symmetric long-term key with each
corresponding client. We model this a priori knowledge by name-generation and subse-
quent secure-communication events, recorded in the initialisation trace shown in
Table 11.

Timed Calculus of Cryptographic Communication
27
Table 10. Speciﬁcation (
def
 t
t

x(othx
 key((slf
  oth)  (t
  t
))

ts(ts@slf
 t
 ts
 t
)) and implementation (
def
 ) process template for WMF
WMFINIT(slf, srv, oth, δ)
def=
WMFSERV(slf, δ)
def=
WMFRESP(slf, srv, δ)
def=
Getslf (ts : TV, ■) in
New (kso : K, ((slf, oth), (ts, ts + δ + δ))).
Getsrv (kss : K, (slf, srv)) in
Outsrv slf.
In fst when fst : P.
Getslf (ksf : K, (slf, fst)) in
Outsrv {|((ts, oth), kso)|}kss.1
In {|((t, snd), key)|}ksf when t : TV ∧
∃ts(ts : TV ∧ts@slf ∧t + δ ≤ts) ∧
snd : P ∧
key : K.
Getslf (kss : K, (slf, snd)) in
Getsrv (kss : K, (slf, srv)) in
Outsnd {|((ts, fst), key)|}kss.1
In {|((t, oth), key)|}kss when t : TV ∧
∃ts(ts : TV ∧ts@slf ∧t + δ ≤ts) ∧
oth : P ∧
key : K ∧
ϕ.
OutEve t.1
WMF(init, srv, resp, xi, xs, xr, δ)
def= init.xi[ WMFINIT(init, srv, resp, δ) ] 
srv.xs[ WMFSERV(srv, δ) ] 
resp.xr[ WMFRESP(resp, srv, δ) ]

28
J. Borgstr¨om, O. Grinchtein, and S. Kramer
Table 11. Initialisation trace for WMF
 init :  

(
  x

  k( 
)
  ((
 )  ( )))
(
  x

  k( 	
)
  ((
 )  ( )))
(
  x

  k( 
)
 )

	(
  x

  k( 
)
 )
(
  x

  k( 	
)
 )

	(
  x
	

  k( 	
)
 )
(    0)

(
  x

  0)

(
  x
	

  0)

(
  x

  0)
Table 12. Attack set-up for WMF

x
[ WMF
(
 
 
 Æ) ] 
x
	
[ WMF
(
 
 Æ) ] 

x
[ WMF
(
 Æ) ] 

x

[ WMF
(
 Æ) ] 

x

[ WMF
(
 Æ) ]
We now reconstruct the impersonation attack on WMF (cf. Table 9) in tC3. The at-
tack involves adversarial replay and impersonation across three different sessions. The
corresponding attack set-up in tC3 is shown in Table 12. This set-up can, via process
reduction, produce the family of traces that is generated by instantiating the parameters
t1, t2, and t3 of the history template shown in Table 13 — for clarity, without intercep-
tion events (cf. Rule OUT and IN in Table 6). The only possible time values for the
parameters t1, t2, and t3 in this history template are those mentioned in the set-events
of the adversary. For the implementation, we may have t1
 Æ, t2
  2 Æ, and t3
  3 Æ,
where t3 is observed by the adversary in clear in the last event. However, the speciﬁca-
tion cannot accept a stale key, i.e., a key older than 2 Æ. Hence, the speciﬁcation cannot
generate a history that conforms to the above template, in particular a history such that
the respondent outputs 3 Æ as her last action. Thus the implementation and the speciﬁ-
cation are not equivalent from the point of view of the adversary, so the speciﬁcation is
not met.
Table 13. History template for WMF
  :   init

(
  x

  k( 	
)
  ((
 )  ( 2Æ)))
(
  x

 
 )
(
  x

 
((0 )  k( 
	
))
k(  
 	)
 )

(   Æ)
	(
  x

 )

	(
  x

 
((0 )  k( 
	
))
k(  
 	))
(
  x

 
((t1
 )  k( 
	
))
k( 

 	)
 )

(
    2Æ)
	(
  x


 )

	(
  x


 
((t1
 )  k( 	
))
k( 

 	))
(
  x


 
((t2
 )  k( 	
))
k(  
 	)
 )

(
    3Æ)
	(
  x


 )

	(
  x


 
((t2
 )  k( 	
))
k(  
 	))
(
  x


 
((t3
 )  k( 
	
))
k( 

 	)
 )
	(
  x
	

 
((t3
 )  k( 
	
))
k( 

 	))

(
  x
	

  t3
 )

Timed Calculus of Cryptographic Communication
29
4
Conclusion
We have demonstrated that adding (dense-valued)real-time to an untimed formalism for
the model-based speciﬁcation and veriﬁcation of cryptographic protocols can be simple,
when properly conceived. Essentially, only two additional axioms (and no modiﬁed
axioms/rules!) are needed in the operational semantics of our Calculus of Cryptographic
Communication. Moreover, the extension of the untimed calculus to the calculus with
real-time is backwards-compatible in the sense that the untimed fragment of the process
syntax is — thanks to sorts, tags, and the abstract message — a subset (up to tags) of the
timed process syntax; and that the process equivalence of the calculus can — thanks to
protocol histories — handle both timed and untimed process terms. Another advantage
of our process equivalence is that it has a common deﬁnitional basis with the epistemic
modality (cf. property-based speciﬁcation) of our co-designed logic CPL. We believe
that the simplicity and backwards-compatibility of our extension is the result of a well-
designed core calculus, and of a simple generalisation of the adversary. Notably, the
adversary’s scheduling power is generalised from the control of the (relative) temporal
order (logical time) of protocol events in the network (space) to the control of their
absolute scheduling time (real time).
Our calculus provides all the necessary ingredients for realistic, timed cryptographic
protocols, i.e., rational-number valued time, time stamps, timed keys, and potentially
drifting local clocks. We conjecture that count-down and count-up timers are encodable,
and checkable on process input. We are planning to extend our calculus with probabilis-
tic polynomial time.
References
1. Schneider, S.: Concurrent and Real-Time Systems. Wiley, Chichester (1999)
2. Evans, N., Schneider, S.: Analysing time-dependent security properties in CSP using PVS.
In: Proceedings of the European Symposium on Research in Computer Security (2000)
3. Gorrieri, R., Martinelli, F.: A simple framework for real-time cryptographic protocol analysis
with compositional proof rules. Science of Computer Programming 50(1–3) (2004)
4. Haack, C., Jeffrey, A.: Timed Spi-calculus with types for secrecy and authenticity. In: Abadi,
M., de Alfaro, L. (eds.) CONCUR 2005. LNCS, vol. 3653, Springer, Heidelberg (2005)
5. Bozga, L., Ene, C., Lakhnech, Y.: A symbolic decision procedure for cryptographic protocols
with time stamps. The Journal of Logic and Algebraic Programming 65 (2005)
6. Gong, L.: A security risk of depending on synchronized clocks. ACM SIGOPS Operating
Systems Review 26(1) (1992)
7. Lamport, L.: Real time is really simple. Technical Report MSR-TR-2005-30, Microsoft Re-
search (2005)
8. Borgstr¨om, J., Kramer, S., Nestmann, U.: Calculus of Cryptographic Communication. In:
Proceedings of the LICS-Afﬁliated Workshop on Foundations of Computer Security and
Automated Reasoning for Security Protocol Analysis (2006)
9. Kramer, S.: Logical concepts in cryptography. Cryptology ePrint Archive, Report 2006/262
(2006), http://eprint.iacr.org/
10. Kramer, S.: Timed Cryptographic Protocol Logic presented at the Nordic Workshop on Pro-
gramming Theory (2006)

30
J. Borgstr¨om, O. Grinchtein, and S. Kramer
11. Haack, C., Jeffrey, A.: Pattern-matching Spi-calculus. In: Proceedings of the Workshop on
Formal Aspects in Security and Trust (2004)
12. Paulson, L.C.: The inductive approach to verifying cryptographic protocols. Journal of Com-
puter Security 6(1) (1998)
13. Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the computational sound-
ness of formal encryption). Journal of Cryptology 15(2) (2002)
14. Fagin, R., Halpern, J.Y., Moses, Y., Vardi, M.Y.: Reasoning about Knowledge. MIT Press,
Cambridge (1995)

A Semantic Paradigm for Component-Based
Speciﬁcation Integrating a Notion of Security Risk
Gyrd Brændeland1,2,⋆and Ketil Stølen1,2
1 Department of Informatics, University of Oslo, Norway
2 SINTEF, Norway
gyb@sintef.uio.no
Abstract. We propose a semantic paradigm for component-based spec-
iﬁcation supporting the documentation of security risk behaviour. By
security risk, we mean behaviour that constitutes a risk with regard to
ICT security aspects, such as conﬁdentiality, integrity and availability.
The purpose of this work is to investigate the nature of security risk in the
setting of component-based system development. A better understand-
ing of security risk at the level of components facilitates the prediction
of risks related to introducing a new component into a system. The se-
mantic paradigm provides a ﬁrst step towards integrating security risk
analysis into the system development process.
Keywords: component, formal speciﬁcation, risk analysis, security.
1
Introduction
The ﬂexibility of component-oriented software systems enabled by component
technologies such as Sun’s Enterprise Java Beans (EJB), Microsoft’s .NET or
the Open Source Gateway initiative (OSGi) gives rise to new types of security
concerns. In particular the question of how a system owner can know whether to
trust a new component to be deployed into a system. A solution to this problem
requires integrating the process of security risk analysis in the early stages of
component-based system development. The purpose of security risk analysis is
to decide upon the necessary level of asset protection against security risks, such
as a conﬁdentiality or integrity breach. Unfortunately, the processes of system
development and security risk analysis are often carried out independently with
little mutual interaction. The result is expensive redesigns and unsatisfactory
security solutions. To facilitate a tighter integration we need a better under-
standing of security risk at the level of components. But knowing the security
risks of a single component is not enough, since two components can aﬀect the
risk level of each other. An example is the known buﬀer overﬂow vulnerability
of previous versions of the media player Winamp, that may allow an unauthen-
ticated attacker using a crafted ﬁle to execute arbitrary code on a vulnerable
system. By default Internet Explorer opens aﬀected ﬁles without prompting the
user [20]. Hence, the probability of a successful attack is much higher if a user
⋆Corresponding author.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 31–46, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

32
G. Brændeland and K. Stølen
utilises both Internet Explorer and Winamp, than only one of them. As this
example illustrates we need a strategy for predicting system level risks that may
be caused by introducing a new component. A better understanding of security
risk at the level of components is a prerequisite for compositional security level
estimation. Such understanding also provides the basis for trust management,
because as argued by Jøsang and Presti [12] there is a close dependency between
trust and risk. The contributions of this paper is a novel semantic paradigm for
component-based speciﬁcation explaining
– basic components with provided and required interfaces;
– the composition of components into composite components;
– unpredictability which is often required to characterise conﬁdentiality prop-
erties (secure information ﬂow);
– the notion of security risk as known from asset-oriented security risk analysis.
This paper is divided into ten sections. In Sections 2 and 3 we explain our no-
tions of security risk analysis and component. In Section 4 we introduce the
basic vocabulary of the semantic model. In Sections 5 to 7 we deﬁne the se-
mantic paradigm for component-based speciﬁcations. In Section 8 we describe
how security risk analysis concepts relate to the component model and how they
are represented in the semantics. In Section 9 we attempt to place our work in
relation to ongoing research within related areas and ﬁnally, in Section 10, we
summarise our ﬁndings.
2
Asset-Oriented Security Risk Analysis
By security risk analysis we mean risk analysis applied to the domain of infor-
mation and communication technology (ICT) security. For convenience we often
use security analysis as a short term for security risk analysis. ICT security in-
cludes all aspects related to deﬁning, achieving and maintaining conﬁdentiality,
integrity, availability, non-repudiation, accountability, authenticity and reliabil-
ity of ICT [11].
Hogganvik and Stølen [7] have provided a conceptual model for security analy-
sis based on a conceptual model originally developed in the CORAS project [3].
The CORAS risk management process is based on the the “Code of practise
for information security management” (ISO/IEC 17799:2000) [9] and the Aus-
tralian/New Zealand standard “Risk Management” (AS/NZS 4360:2004) [22].
With some adjustments the model is expressed as a class diagram in UML
2.0 [18], see Figure 1. The associations between the elements have cardinali-
ties specifying the number of instances of one element that can be related to
one instance of the other. The hollow diamond symbolises aggregation and the
ﬁlled composition. Elements connected with an aggregation can also be part
of other aggregations, while composite elements only exist within the speciﬁed
composition.
We explain Figure 1 as follows: Stakeholders are those people and organi-
sations who may aﬀect, be aﬀected by, or perceive themselves to be aﬀected
by, a decision or activity or risk [22]. The CORAS security analysis process is

A Semantic Paradigm for Component-Based Speciﬁcation
33
Consequence
Information
security incident
Probability
Threat
Stakeholder
1
1
Asset
Vulnerability
Treatment
Risk
1..*
*
*
*
*
1..*
Weakens
*
1
1 1
1
1..*
*
1
1
1
1..*
1
1 1..*
* 1..*
Causes ▼
▲Exploits
▲Values
▲Affects
Affects ►
Fig. 1. CORAS conceptual model of security analysis terms
asset-oriented. An asset is something to which a stakeholder directly assigns
value and, hence, for which the stakeholder requires protection [23]1. CORAS
links assets uniquely to their stakeholders. A vulnerability is a weakness of an as-
set or group of assets that can be exploited by one or more threats [11]. A threat is
a potential cause of an incident that may result in harm to a system or organisa-
tion [11]. An information security incident refers to any unexpected or unwanted
event that might cause a compromise of business activities or information secu-
rity, such as malfunction of software or hardware and access violations [11]. A
risk is the combination of the probability of an event and its consequence [10].
Conceptually, as illustrated in Figure 1, a risk consist of an information security
incident, the probability of its happening and its consequence. Probability is the
extent to which an event will occur [10]. Consequence is the outcome of an event
expressed qualitatively or quantitatively, being a loss, injury or disadvantage.
There may be a range of possible outcomes associated with an event [23]. This
implies that an information security incident may lead to the reduction in value
of several assets. Hence, an information security incident may be part of several
risks. Risk treatment is the process of selection and implementation of measures
to modify risks [10].
3
The Component Model
There exist various deﬁnitions of what a software component is. The classic
deﬁnition by Szyperski [24] provides a basic notion of a component that is widely
adopted in later deﬁnitions: “A software component is a unit of composition
with contractually speciﬁed interfaces and explicit context dependencies only. A
software component can be deployed independently and is subject to composition
by third parties.”
1 The Australian handbook [23] uses the term organisation instead of the broader
term stakeholder. For simplicity of the conceptual model we prefer the broader term
stakeholder which includes organisation.

34
G. Brændeland and K. Stølen
Lau and Wang [15] criticise Szyperski’s deﬁnition for not relating the com-
ponent concept to a component model. Lau and Wang emphasise the impor-
tance of a component model as a provider of an underlying semantic framework,
deﬁning:
– the syntax of components, i.e., how they are constructed and represented;
– the semantics of components, i.e. what components are meant to be;
– the composition of components, i.e. how they are composed or assembled.
Lau and Wang present a taxonomy of current component models, comparing
their similarities and diﬀerences with regard to these three criteria. They com-
pare the component models facilities for composition both in the design phase
and the deployment phase. Our approach focuses on the speciﬁcation of compo-
nents. Hence, composition takes place in the design phase.
According to Cheesman and Daniels [2] the main motivation for using a
component-oriented approach is to make dependencies explicit, in order to facili-
tate management of component systems and independent deployment of compo-
nents. Since the client of a component is not necessarily the same as the deployer
of the component, they distinguish between two types of contracts correspond-
ing to these two roles: usage and realisation contracts. This distinction motivates
the separation of speciﬁcations into interfaces and components. Our conceptual
model of a component, shown in Figure 2, is inspired by the deﬁnitions given
in [2].
Requires ►
Name
Type
Operation
Composite
Component 
*
1
2..*
*
1
*
*
*
*
*
1..*
*
Provides ►
Provides ►
Requires ►
Interface
Basic
Component
Fig. 2. Conceptual model of a component
We explain the conceptual component model as follows: An interface is a
contract with a client, describing a set of behaviours provided by a component
object. It deﬁnes a list of operations that the interface provides, their signatures
and semantics. A component is a contract with the realiser. It describes pro-
vided interfaces and component dependencies in terms of required interfaces. By
required interface we mean the calls the component needs to make, in order to
implement the operations described in the provided interfaces. We distinguish
between basic components and composite components. A basic component pro-
vides only one interface. We obtain components with more than one provided
interface by the composition of basic components. Composite components can
also be combined to obtain new composite components.

A Semantic Paradigm for Component-Based Speciﬁcation
35
4
The Semantic Model of STAIRS
We build our semantic paradigm on top of the trace semantics of STAIRS [6,5].
STAIRS is an approach to the compositional development of UML 2.0 interac-
tions. For a thorough account of the STAIRS semantics, see Haugen et al. [6,5].
The most common interaction diagram is the sequence diagram, which shows
a set of messages arranged in time sequence [18]. A sequence diagram typically
captures the behaviour of a single scenario. A sequence diagram describes one
or more positive (i.e. valid) and/or negative (i.e. invalid) behaviours.
The sequence diagram in Figure 3 speciﬁes a scenario in which the client
lifeline sends the message displayAcc to the bank lifeline, which then sends the
message check with argument pin to the environment. When the bank lifeline
receives the message ok it sends the message acc to the client lifeline.
sd Bank
:Bank
acc
displayAcc(pin)
ok
check(pin)
:Client
Fig. 3. Example interaction
Formally STAIRS uses denotational trace semantics in order to explain the
meaning of a single interaction. A trace is a sequence of events, representing a
system run. There are two kinds of events: sending and reception of a message,
where a message is a triple (s, re, tr) consisting of a signal s, a transmitter lifeline
tr and a receiver lifeline re. We let E denote the set of all events.
The set of traces described by a diagram like the one in Figure 3 are all positive
sequences consisting of events such that the transmit event is ordered before the
corresponding receive event, and events on the same lifeline are ordered from
the top downwards. Shortening each message to the ﬁrst letter of each signal, we
thus get that Figure 3 speciﬁes the trace ⟨!d, ?d, !c, ?o, !a, ?a⟩where ! denotes
transmission and ? reception of the message.
Formally we let H denote the set of all well-formed traces. A trace is well-
formed if, for each message, the send event is ordered before the corresponding
receive event. An interaction obligation (pi, ni) is a classiﬁcation of all of the
traces in H into three categories: the positive traces pi, representing desired and
acceptable behaviour, the negative traces ni, representing undesired or unac-
ceptable behaviour, and the inconclusive traces H \ (pi ∪ni). The inconclusive
traces are a result of the incompleteness of interactions, representing traces that
are not described as positive or negative by the current interaction.
The reason we operate with inconclusive traces is that sequence diagrams
normally gives a partial description of a system behaviour. It is also possible to
specify complete behaviour. Then every trace is either positive or negative.

36
G. Brændeland and K. Stølen
5
Semantics of Basic Components
In this section we describe how basic components can be described semantically
using STAIRS. Our semantic paradigm is independent of the concrete syntactic
representation of speciﬁcations. In this paper we use sequence diagrams based
on the semantic mapping deﬁned in STAIRS, as they are simple to understand
and well suited to exemplify parts of a component behaviour. We could have
deﬁned similar mappings for other speciﬁcation languages.
A basic component has a unique identiﬁer. In STAIRS this identiﬁer is rep-
resented by a lifeline. As explained in Section 3 the provided interface of a basic
component corresponds to the method calls it can receive and the required in-
terface corresponds to the method calls the component needs to make to other
component interfaces, in order to implement the operations described in the
provided interface.
The denotation [[ K ]] of a basic component speciﬁcation K in the STAIRS se-
mantics is an interaction obligation (PK , NK) where PK and NK are the positive
and negative traces over some set of component events EK , respectively.
Example 1. The sequence diagram in Figure 4 speciﬁes a scenario where a login
lifeline receives the message login with arguments id and pwd. The login lifeline
then sends the message authenticate to the environment. STAIRS uses the alt
operator to describe that a system can include alternative behaviours. There are
three alternatives: Firstly, when a user attempts to login she can either succeed
or fail. If she fails there are two alternatives, of which only one is legal: When
the login lifeline receives the reply fail it should reply with fail.
fail
session
sd Login
:ILogin
login(id,pwd)
login(id,pwd)
authenticate(id,pwd)
ok
authenticate(id,pwd)
fail
wrong user-name
login(id,pwd)
authenticate(id,pwd)
fail
alt
alt
neg
Fig. 4. Specifying component dependencies
We specify that the component should never return simply the message wrong
user-name, by placing the event of returning this message within a neg construct
in the sequence diagram. That is because we do not wish to reveal information

A Semantic Paradigm for Component-Based Speciﬁcation
37
that can be useful for a potential impostor, if a login attempt fails. The se-
quence diagram in Figure 4 speciﬁes an interaction obligation (P, N ) where
P = {⟨?l, !a, ?o, !s⟩, ⟨?l, !a, ?f , !f ⟩} and N = {⟨?l, !a, ?f , !w⟩} when shortening
each message to the ﬁrst letter of each signal.
2
We deﬁne an interface and its denotation as an abstraction over a basic compo-
nent. An interface describes the view of the user, who does not need to know how
the login operation is implemented. We obtain the provided interface of a basic
component by ﬁltering away the interactions on the required interface. Hence, a
provided interface corresponds to a basic component, if the component has no
required interface.
6
Semantics of Composite Components
As described in Section 5 we distinguish between basic and composite compo-
nents. A basic component provides only one interface. We obtain components
with more than one interface by the composition of basic components. Composite
components can also be combined to obtain new composite components.
In order to deﬁne composition we need the functions
S
⃝for ﬁltering of se-
quences, and
T
⃝for ﬁltering of pairs of sequences, deﬁned by Haugen et al. [6,5].
The ﬁltering function
S
⃝is used to ﬁlter away elements. By B
S
⃝a we denote the
sequence obtained from the sequence a by removing all elements in a that are
not in the set of elements B. For example, we have that
{1, 3} S
⃝⟨1, 1, 2, 1, 3, 2⟩= ⟨1, 1, 1, 3⟩
The ﬁltering function
T
⃝may be understood as a generalisation of
S
⃝. The func-
tion
T
⃝ﬁlters pairs of sequences with respect to pairs of elements in the same way
as
S
⃝ﬁlters sequences with respect to elements. For any set of pairs of elements
P and pair of sequences t, by P
T
⃝t we denote the pair of sequences obtained
from t by
– truncating the longest sequence in t at the length of the shortest sequence
in t if the two sequences are of unequal length;
– for each j ∈[1, . . . , k], where k is the length of the shortest sequence in
t, selecting or deleting the two elements at index j in the two sequences,
depending on whether the pair of these elements is in the set P.
For example, we have that
(1, f ), (1, g) T
⃝(⟨1, 1, 2, 1, 2⟩, ⟨f , f , f , g, g⟩) = (⟨1, 1, 1⟩, ⟨f , f , g⟩)
Parallel execution of trace sets is deﬁned as:
s1 ⊗s2
def
= {h ∈H | ∃p ∈{1, 2}∞: π2(({1} × E
T
⃝(p, h))) ∈s1 ∧
π2(({2} × E
T
⃝(p, h))) ∈s2}

38
G. Brændeland and K. Stølen
In this deﬁnition, we make use of an oracle, the inﬁnite sequence p, to resolve
the non-determinism in the interleaving. It determines the order in which events
from traces in s1 and s2 are sequenced. π2 is a projection operator returning the
second element of a pair.
Given two components K1 and K2 with distinct component identiﬁers (life-
lines). By K1 ⊗K2 we denote their composition. Semantically, composition is
deﬁned as follows:
[[ K1 ⊗K2 ]] = [[ K1 ]] ⊗[[ K2 ]]
where for all interaction obligations (p1, n1), (p2, n2) we deﬁne
(p1, n1) ⊗(p2, n2)
def
= (p1 ⊗p2, (n1 ⊗p2) ∪(n1 ⊗n2) ∪(p1 ⊗n2))
Note how any trace involving a negative trace will remain negative in the result-
ing interaction obligation.
We also introduce a hiding operator δ that hides all behaviour of a component
which is internal with regard to a set of lifelines L. Formally
[[ δL : K ]]
def
= (δL : π1.[[ K ]], δL : π2.[[ K ]])
where for a set of traces H and a trace h
δL : H
def
= {δL : h | h ∈H }
δL : h
def
= {e ∈E | re.e ̸∈L ∨tr.e ̸∈L} S
⃝h
where the functions tr.e and re.e yields the transmitter and receiver of an event.
Finally we deﬁne composition with hiding of local interaction as:
K1 ⊕K2
def
= δ(ll.[[ K1 ]] ∪ll.[[ K2 ]]) : K1 ⊗K2
where the function ll yields the set of lifelines of an interaction obligation.
7
Generalising the Semantics to Support Unpredictability
As explained by Zakinthinos and Lee [26], the purpose of a conﬁdentiality prop-
erty is to prevent low level users from being able to make deductions about
the events of the high level users. A conﬁdentiality property will often typically
require nondeterministic behaviour (unpredictability) to achieve this. Unpre-
dictability in the form of non-determinism is known to be problematic in re-
lation to speciﬁcations because non-determinism is also often used to represent
underspeciﬁcation and when underspeciﬁcation is reﬁned away during system de-
velopment we may easily also reduce the required unpredictability and thereby
reduce security. For this reason, STAIRS (as explained carefully by Seehusen and
Stølen [21]) distinguishes between mandatory and potential choice. Mandatory
choice is used to capture unpredictability while potential choice captures under-
speciﬁcation. One of the main concerns in STAIRS is the ability to distinguish

A Semantic Paradigm for Component-Based Speciﬁcation
39
between traces that an implementation may exhibit (e.g. due to underspeciﬁca-
tion), and traces that it must exhibit (e.g. due to unpredictability). Semantically,
this distinction is captured by stating that the semantics of an interaction d is
a set of interaction obligations [[ d ]] = {(p1, n1), . . . , (pm, nm)}. Intuitively, the
traces allowed by an interaction obligation (i.e. its positive and inconclusive
traces) represent potential alternatives, where being able to produce only one of
these traces is suﬃcient for an implementation. On the other hand, the diﬀerent
interaction obligations represent mandatory alternatives, each obligation speci-
fying traces where at least one must be possible for any correct implementation
of the speciﬁcation.
We adapt the deﬁnition of a basic component to allow mandatory behaviour
alternatives as follows: The denotation [[ K ]] of a basic component is a set of
interaction obligations over some set of events EK. We also lift the deﬁnition of
composition to handle unpredictability by point-wise composition of interaction
obligations
[[ K1 ⊗K2 ]]
def
= {o1 ⊗o2 | o1 ∈[[ K1 ]] ∧o2 ∈[[ K2 ]]}
The δ operator is overloaded to sets of interaction obligations:
[[ δL : K ]]
def
= {[[ δL : o ]] | o ∈[[ K ]]}
and composition with hiding is deﬁned as before.
8
Relating Security Risk to the Semantic Paradigm
Having introduced the underlying semantic component paradigm and formalised
unpredictability, the next step is to relate this paradigm to the main notions of
security analysis and generalise the paradigm to the extent this is necessary.
The purpose of extending the component model with security analysis concepts
is to be able to specify security risks and document security analysis results of
components. This facilitates integration of security analysis into the early stages
of component-based system development. Security analysis documentation pro-
vides information about the risk level of the component with regard to its assets,
i.e., the probability of behaviour leading to reduction of asset values. At this point
we do not concern ourselves with how to obtain such security analysis results.
We refer to [1] for an evaluation of an integrated process, applying the semantic
paradigm. In the following we focus on how security analysis concepts can be
understood in a component-setting and how they can be represented formally. In
Sections 8.1– 8.4 we explain how the security analysis concepts of Figure 1 may
be understood in a component setting. In Section 8.5 we formalise the required
extensions of the semantic paradigm.
8.1
Representing Stakeholders and Threats
We represent stakeholders as lifelines, since the stakeholders of a component can
be understood as entities interacting with it via its interfaces. We also represent

40
G. Brændeland and K. Stølen
threats as lifelines. A threat can be external (e.g. hackers or viruses) or internal
(e.g. system failures). An internal threat of a component is a sub-component,
represented by a lifeline or a set of lifelines. An external threat may initiate
a threat scenario by calling an operation of one of the component’s external
interfaces.
8.2
Representing Assets
For each of its stakeholders a component holds a (possibly empty) set of assets.
An asset is a physical or conceptual entity of value for a stakeholder. There are
diﬀerent strategies we can choose for representing assets, their initial values and
the change in asset values over time: Represent assets (1) as variables and add an
operator for assignment; (2) as data using extensions to STAIRS introduced by
Runde et al. [19] or (3) as lifelines indicating the change in asset value through
the reception of special messages. We have chosen the latter because it keeps
our semantics simpler (we do not have to add new concepts) and provides the
same extent of expressive power as the other alternatives. Formally an asset is a
triple (a, c, V ) of an asset lifeline a, a basic component lifeline c and an initial
value V . In a trace we represent the reduction of asset value by a special kind
of message called reduce, which takes as argument the amount by which the
asset value should be reduced. The value of an asset at a given point in time
is computed by looking at its initial value and all occurrences of reduce, with
the asset as receiver, up to that point in the trace. Events on the lifeline of an
asset can only be receptions of reduce messages. The value of an asset can not
go below zero.
8.3
Representing Vulnerabilities
As pointed out by Verdon and McGraw [25] vulnerabilities can be divided into
two basic categories: ﬂaws, which are design level problems, and bugs, which
are implementation level problems. When conducting security analysis during
the early stages of system development, the vulnerabilities that can be detected
are of the former type. I.e., a vulnerability is a weakness in the component
speciﬁcation, allowing interactions that can be exploited by threats to cause
harm to assets.
8.4
Representing Incidents and Risks
As explained in Section 2 an information security incident is an unexpected or
unwanted event that might compromise information security. In a component
setting we can represent security incidents in the same manner as we represent
normal behaviour; by sets of traces. A risk is measured in terms of the probability
and consequence of an information security incident. Hence, in order to represent
risks we need to be able to represent the probability of a set of traces constituting
an information security incident and its consequence.

A Semantic Paradigm for Component-Based Speciﬁcation
41
Inspired by Refsdal et al. [17] in our semantic model we represent this set
of traces by a so called risk obligation. A risk obligation is a generalisation of
an interaction obligation. Formally a risk obligation is a triple (o, Q, A) of an
interaction obligation o, a set of probabilities Q and a set of assets A. The
probability of the risk is an element of Q. We operate with a set of probabilities
instead of a single probability to allow the probability to range freely within an
interval.
Example 2. Figure 5 illustrates how we can specify a risk in accordance with
the extensions to the semantic paradigm, described above. As most dynamic
web applications, the login component pass data on to a subsystem. This may
be an SQL data base or a component interacting with a database. If the system
is not protected against SQL injection an attacker can modify or add queries
that are sent to a database by crafting input to the web application. The attack
example is from Sverre H. Huseby’s [8] book on web-server security.
p >  0.16
p <= 0.16
reduce($50)
:ILogin
:UserName
Value=k
login(id,--)
fail
login(id,--)
authenticate(id,--)
ok
:IHacker
authenticate(id,--)
fail
palt
session
sd LoginFraud($ k)
Fig. 5. Login without password using SQL injection
The sequence diagram in Figure 5 shows the interactions of a hacker using a
modiﬁed query to attempt an SQL injection and the login lifeline receiving the
query. Instead of a password the hacker writes a double hyphen (- -). Unless the
system is programmed to handle such metacharacters in a secure manner, this
has the eﬀect that the test for a matching password is inactivated allowing
the hacker to login with only a user name. We have assigned the asset UserName
to the basic component ILogin. As the sequence diagram illustrates an example
run, we assume the initial asset value has been set elsewhere and parameterise
the speciﬁcation with the asset value k of type $. If the SQL attack is successful
the asset value is reduced with $50.
We specify the risk as a probabilistic choice between the scenario where the
attack is successful and the scenario where it fails. Probabilistic STAIRS [17] uses
the palt construct to specify probabilistic alternatives, as illustrated in Figure 5.
In order to estimate the probability of a successful login using SQL injection,
we must know both the probability of an attack (threat probability) and the

42
G. Brændeland and K. Stølen
probability of the success of an attack (degree of vulnerability), given that an
attack is attempted. We assume that an attack has been estimated to have a
0.2 probability. The probability that the attack will be successful is determined
from looking at the system’s existing vulnerabilities, such as lack of control
mechanisms. In the example there is not speciﬁed any protection mechanisms
against attempt at SQL injection. The probability of success given an attack is
therefore estimated as high: 0.8. The alternative to the risk is that the modiﬁed
query is rejected, and hence the asset value is not reduced. The consequence
of the risk is the loss of $50 in asset value. We multiply the probability of an
attack with the probability of its success to obtain the total probability of the
risk. Hence, the probability of a successful false login is 0.2 ∗0.8 = 0.16.
2
8.5
Generalising the Paradigm to Support Security Risk
Above we have outlined the relation between security risks as described in Fig-
ure 1 and our semantic paradigm. We now go on to adapt the semantic paradigm
to capture this understanding formally.
In order to allow assignment of probabilities to trace sets, we represent basic
components by sets of risk obligations instead of sets of interaction obligations.
Moreover, contrary to earlier a basic component may now have more than one
lifeline, namely the lifeline of the component itself and one additional lifeline for
each of its assets. Hence, the denotation [[ K ]] of a basic component K is a set of
risk obligations. Composition of components is deﬁned point-wise as previously,
i.e.:
[[ K1 ⊗K2 ]]
def
= {r1 ⊗r2 | r1 ∈[[ K1 ]] ∧r2 ∈[[ K2 ]]}
Composition of risk obligations is deﬁned as follows
(o1, Q1, A1) ⊗(o1, Q1, A2)
def
= (o1 ⊗o2, Q1 ∗Q2, A1 ∪A2)
where
Q1 ∗Q2
def
= {q1 ∗q2 | q1 ∈Q1 ∧q2 ∈Q2}
and ∗is the multiplication operator. The use of the ⊗-operator requires that K1
and K2 are described independently as components. In STAIRS the ⊗operator
corresponds to parallel composition (||) (which is the same as ≿since K1 and
K2 have disjoint lifelines). The scenario described in Figure 5 involves the palt
construct, which imposes a global constraint on the interactions between the
hacker and the login lifelines. Calculating the semantics of the overall scenario
involves the use of several additional operators. See [6,5] for further details.
We also update the hiding operator δ to ensure that external assets are not
hidden. An asset is external if it is associated with the interfaces of a basic
component that has externally visible behaviour. We deﬁne the function AExt to
yield the external assets with regard to a set of assets A, a set of basic component
lifelines L and an interaction obligation o:
AExt(A, L, o)
def
= {a ∈A | π2.a ∈ll.δL : o}

A Semantic Paradigm for Component-Based Speciﬁcation
43
Given a component K and a set of basic component lifelines L, at the component
level hiding is deﬁned as the pointwise application of the hiding operator to each
risk obligation:
[[ δL : K ]]
def
= {δL : r | r ∈[[ K ]]}
where hiding at the level of risk obligation is deﬁned as:
δL : (o, Q, A)
def
= (δ(L \ AExt(A, L, o)) : o, Q, AExt(A, L, o))
Composition with hiding is deﬁned as before.
9
Related Work
Fenton and Neil [4] addresses the problem of predicting risks related to introduc-
ing a new component into a system, by applying Bayesian networks to analyse
failure probabilities of components. They combine quantitative and qualitative
evidence concerning the reliability of a component and use Bayesian networks
to calculate the overall failure probability. Although Fenton and Neil address
the same problem as we do, the focus is diﬀerent. At this point we do not con-
cern ourselves with how the security analysis results are obtained. Rather than
focusing on the process we look at how the results of security analysis can be
represented at the component level to facilitate composition of security analysis
results in a development process.
There are a number of proposals to integrate security requirements into the re-
quirements speciﬁcation, such as for example in SecureUML [16] and in
UMLsec [13]. SecureUML is a method for modelling access control policies and
their integration into model-driven software development. SecureUML is based
on role-based access control and models security requirements for well-behaved
applications in predictable environments. UMLsec is an extension to UML that
enables the modelling of security-related features such as conﬁdentiality and
access control. These approaches have no particular focus on component-based
speciﬁcation. One approach that has a particular focus on component security
is the security characterisation framework proposed by Khan and Han [14] to
characterise and certify the security properties of components as a basis for de-
riving system-level risks during the deployment phase. These methods focus on
specifying security properties of systems which is orthogonal to what we do.
They include no notion of risk or probability. Rather than specifying security
properties of systems, we focus on representing risks, i.e., we integrate the docu-
mentation of the probability that unwanted behaviour may occur into component
speciﬁcations.
10
Conclusion
We have provided a semantic paradigm for component-based speciﬁcations ex-
plaining: basic components with provided and required interfaces; the compo-
sition of components into composite components and unpredictability which is

44
G. Brændeland and K. Stølen
Probability
Threat
Stakeholder
Risk
*
1
1
1
Asset value
1
1
Basic
Component
Interface
Composite
Component 
*
1
2..*
*
*
*
*
*
1..*
*
1
1
*
1
1
1..*
*
Affects ►
Provides ►
Provides ►
Requires ►
Requires ►
◄Interact
◄Threaten
1
1
1
1
*
1
Name
Type
Operation
Causes ▼
▲Values
Consequence
Asset
1..*
Vulnerability
Weakens►
*
1
1..*
*
**
▲Exploits
*
Affects ►
Fig. 6. Integrated conceptual model of a component risk speciﬁcation
often required to characterise conﬁdentiality properties. Furthermore we have
extended the semantic paradigm with the notion of security risk as known from
asset-oriented security analysis. Figure 6 summarises the relations between the
conceptual component model and security assessment concepts: A component
holds a set of assets that has value for its stakeholders. We limit the notion of a
stakeholder to that of a component client or supplier interacting with it through
its interfaces. We represent threats as lifelines that may interact with a compo-
nent through its interface. There is a one-to-one association between an interface
on the one hand and stakeholder and threat on the other, as a component in-
terface can interact with one stakeholder or threat at a time. A vulnerability
is represented implicitly as an interaction that may be exploited by a threat to
cause harm to a components assets. Instead of representing the two concepts of
information security incident and risk, we represent only the concept of a risk
as a probabilistic interaction leading to the reduction of an asset value. In the
extended component model we associate a threat directly with a risk, as someone
or something that may initiate a risk.
The formal representation of security analysis results at the component-level
allows us to specify security risks and document security analysis results of com-
ponents. This is a step towards integration of security analysis into the system
development process. Component-based security analysis can be conducted on
the basis of requirement speciﬁcation in parallel with conventional analysis. If
new components are accompanied by security risk analysis, we do not need to
carry out a security analysis from scratch each time a system is upgraded with
new components, but can apply rules for composition to update the security risk
analysis.
Acknowledgements
The research on which this paper reports has been funded by the Research
Council of Norway via the two research projects COMA 160317 (Component-
oriented model-based security analysis) and SECURIS (152839/220).

A Semantic Paradigm for Component-Based Speciﬁcation
45
References
1. Brændeland, G., Stølen, K.: Using model-based security analysis in component-
oriented system development. A case-based evaluation. In: Proceedings of the sec-
ond Workshop on Quality of Protection (QoP’06) (to appear, 2006)
2. Cheesman, J., Daniels, J.: UML Components. A simple process for specifying
component-based software. Component software series. Addison-Wesley, Reading
(2001)
3. den Braber, F., Dimitrakos, T., Gran, B.A., Lund, M.S., Stølen, K., Aagedal, J.Ø.:
UML and the Uniﬁed Process, chapter The CORAS methodology: model-based
risk management using UML and UP, pp. 332–357. IRM Press (2003)
4. Fenton, N., Neil, M.: Combining evidence in risk analysis using bayesian networks.
Agena White Paper W0704/01 (2004)
5. Haugen, Ø., Husa, K.E., Runde, R.K., Stølen, K.: Why timed sequence diagrams
require three-event semantics. Technical Report 309, University of Oslo, Depart-
ment of Informatics (2004)
6. Haugen, Ø., Stølen, K.: STAIRS – steps to analyze interactions with reﬁnement
semantics. In: Stevens, P., Whittle, J., Booch, G. (eds.) UML 2003 LNCS, vol.
2863, pp. 388–402. Springer, Heidelberg (2003)
7. Hogganvik, I., Stølen, K.: On the comprehension of security risk scenarios. In: 13th
International Workshop on Program Comprehension (IWPC 2005), pp. 115–124.
IEEE Computer Society, Los Alamitos (2005)
8. Huseby, S.H.: Innocent code. A security wake-up call for web programmers. Wiley,
Chichester (2004)
9. ISO/IEC.: Information technology – Code of practice for information security man-
agement. ISO/IEC 17799:2000
10. ISO/IEC.: Risk management – Vocabulary – Guidelines for use in standards,
ISO/IEC Guide 73:2002 (2002)
11. ISO/IEC.: Information Technology – Security techniques – Management of infor-
mation and communications technology security – Part 1: Concepts and models
for information and communications technology security management, ISO/IEC
13335-1:2004 (2004)
12. Jøsang, A., Presti, S.L.: Analysing the relationship between risk and trust. In:
Jensen, C., Poslad, S., Dimitrakos, T. (eds.) iTrust 2004. LNCS, vol. 2995, pp.
135–145. Springer, Heidelberg (2004)
13. Jürjens, J. (ed.): Secure systems develoment with UML. Springer, Heidelberg
(2005)
14. Khan, K.M., Han, J.: A process framework for characterising security properties
of component-based software systems. In: Australian Software Engineering Con-
ference, pp. 358–367. IEEE Computer Society, Los Alamitos (2004)
15. Lau, K.-K., Wang, Z.: A taxonomy of software component models. In: Proc. 31st
Euromicro Conference, pp. 88–95. IEEE Computer Society Press, Los Alamitos
(2005)
16. Lodderstedt, T., Basin, D.A., Doser, J.: SecureUML: A UML-based modeling lan-
guage for model-driven security. In: Jézéquel, J.-M., Hussmann, H., Cook, S. (eds.)
UML 2002 - The Uniﬁed Modeling Language. Model Engineering, Concepts, and
Tools. LNCS, vol. 2460, pp. 426–441. Springer, Heidelberg (2002)
17. Refsdal, A., Runde, R.K., Stølen, K.: Underspeciﬁcation, inherent nondetermin-
ism and probability in sequence diagrams. In: Gorrieri, R., Wehrheim, H. (eds.)
FMOODS 2006. LNCS, vol. 4037, pp. 138–155. Springer, Heidelberg (2006)

46
G. Brændeland and K. Stølen
18. Rumbaugh, J., Jacobsen, I., Booch, G.: The uniﬁed modeling language reference
manual. Addison-Wesley, Reading (2005)
19. Runde, R.K., Haugen, Ø., Stølen, K.: Reﬁning UML interactions with underspec-
iﬁcation and nondeterminism. Nordic Journal of Computing (2005)
20. Winamp skin ﬁle arbitrary code execution vulnerability. Secunia Advisory:
SA12381. Secunia (2006)
21. Seehusen, F., Stølen, K.: Information ﬂow property preserving transformation of
uml interaction diagrams. In: 11th ACM Symposium on Access Control Models
and Technologies (SACMAT 2006), pp. 150–159. ACM, New York (2006)
22. Standards Australia: Standards New Zealand. Australian/New Zealand Standard.
Risk Management, AS/NZS 4360:2004 (2004)
23. Standards Australia: Standards New Zealand. Information security risk manage-
ment guidelines, HB 231:2004 (2004)
24. Szyperski, C., Pﬁster, C.: Workshop on component-oriented programming. In: Mül-
hauser, M. (ed.) Special Issues in Object-Oriented Programming – ECOOP’96
Workshop Reader, dpunkt Verlag, pp. 127–130 (1997)
25. Verdon, D., McGraw, G.: Risk analysis in software design. IEEE Security & Pri-
vacy 2(4), 79–84 (2004)
26. Zakinthinos, A., Lee, E.S.: A general theory of security properties. In: IEEE Sym-
posium on Security and Privacy, pp. 94–102. IEEE Computer Society, Los Alamitos
(1997)

Game-Based Criterion Partition Applied to
Computational Soundness of Adaptive Security
M. Daubignard, R. Janvier, Y. Lakhnech, and L. Mazar´e
VERIMAG, 2, av. de Vignates, 38610 Gi`eres - France
{marion.daubignard,romain.janvier,yassine.lakhnech,
laurent.mazare}@imag.fr
Abstract. The composition of security deﬁnitions is a subtle issue. As
most security protocols use a combination of security primitives, it is
important to have general results that allow to combine such deﬁnitions.
We present here a general result of composition for security criteria (i.e.
security requirements). This result can be applied to deduce security of
a criterion from security of one of its sub-criterion and an indistinguisha-
bility criterion. To illustrate our result, we introduce joint security for
asymmetric and symmetric cryptography and prove that it is equivalent
to classical security assumptions for both the asymmetric and symmetric
encryption schemes. Using this, we give a modular proof of computa-
tional soundness of symbolic encryption. This result holds in the case of
an adaptive adversary which can use both asymmetric and symmetric
encryption.
Keywords:
Provable Security, Security Games, Probabilistic Encryp-
tion, Computational Soundness of Formal Methods.
1
Introduction
Provable security consists in stating the expected security properties in a for-
mally deﬁned adversarial model and providing a mathematical proof that the
properties are satisﬁed by the designed system/protocol. Micali and Goldwasser
are probably the ﬁrst to put forward the idea that security can be proved in
a formally deﬁned model under well-believed rigorously deﬁned complexity-
assumptions [GM84]. Although provable security has by now become a very
active research ﬁeld there is a lack of a general “proof theory” for cryptographic
systems. As underlined by V. Shoup in [Sho04], security proofs often become
so messy, complicated, and subtle as to be nearly impossible to understand. Ide-
ally there should be a veriﬁcation theory for cryptographic systems in the same
way as there are veriﬁcation theories for “usual” sequential and concurrent sys-
tems (cf. [Cou90, MP92]).
As security proofs are mostly proofs by reduction a promising approach seems
to be one that is based on transforming the system to be veriﬁed into a sys-
tem that obviously satisﬁes the required properties. Sequences of games have
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 47–64, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

48
M. Daubignard et al.
been recently proposed as a tool for taming the complexity of security proofs
[Sho04, BR04] and ﬁrst implementations of tools that assisted in deriving such se-
quences have been developed [Bla06]. In particular, three types of transitions be-
tween games are proposed. One of the most powerful transitions is based on indis-
tinguishability. Informally, to bound the probability of an event Ei in game i and
the probability of event Ei+1 in game i+1, one shows that there is a distinguisher
algorithm D that interpolates between Game i and Game i+1, such that given an
element from distribution Pi, for i = 1, 2, D outputs 1 with probability Pr[Ei].
Hence, Pr[Ei] −Pr[Ei+1] = Pr[D(x) →1|x ∈P1] −Pr[D(x) →1|x ∈P2],
and hence, the indistinguishability assumption implies that Pr[Ei] −Pr[Ei+1]
is negligible.
In this paper we prove a theorem that provides a powerful instance of the
indistinguishability-based transition technique. This theorem can be used for
compositional veriﬁcation of cryptographic libraries as it allows one to reduce
a security criterion into simpler ones. A typical use is to allow the comparison
of a criterion that involves a set of oracles (which can for example all use the
same challenge bit b) with a criterion that only involves a subset of the oracles.
As a simple application of this result, we can for instance prove the equivalence
of semantic security of one key and semantic security in the multi-party set-
ting [BBM00]. The advantage of applying our theorem in that case is that the
proof is done without having to design adversaries, the only thing to do is to
provide a partition of the criterion.
Moreover we believe that our main result is helpful when proving computa-
tional soundness of symbolic analysis for cryptographic protocols. This recent
trend in bridging the gap that separates the computational and symbolic views
of protocols has been initiated by Abadi and Rogaway [AR00]. In this paper,
they prove that symbolic equivalence of messages implies computational indis-
tinguishability provided that the cryptographic primitives are secure. This result
has then been adapted for protocols where the adversary is an eavesdropper and
has a passive behavior and the only allowed cryptographic primitive is symmetric
encryption [AJ01].
Various extensions of [AR00, AJ01] have been presented recently by adding
new cryptographic primitives [BCK05] or by removing the passive adversary hy-
pothesis. There are diﬀerent ways to consider non-passive adversaries, this can
be done by using the simulatability approach [BPW03], by proving trace prop-
erties on protocols [MW04, CW05, JLM05]. Another possibility is to consider an
adaptive adversary as introduced by Micciancio and Panjwani [MP05]. In this
context, the adversary issues a sequence of adaptively chosen equivalent pairs of
messages (m1
0, m1
1) to (mq
0, mq
1). After query (mi
0, mi
1) the adversary receives a
bit-string that instantiates either mi
0 or mi
1 and it has to tell which is the case.
The main improvement with respect to the result of Abadi and Rogaway [AR00]
is that the adversary has an adaptive behavior: it can ﬁrst send a query (m1
0, m1
1)
then using the result determine a new query and submit it. However Miccian-
cio and Panjwani only consider symmetric encryption. In order to illustrate how

Game-Based Criterion Partition
49
our main result can be used in such situations, we prove a similar result when
considering both asymmetric and symmetric encryption. Besides by using our
partition theorem, the proof we give is modular and hence easier to extend
to more cryptographic primitives than the original one. For that purpose, we
introduce new security criteria which deﬁne pattern semantic security and prove
that these criteria are equivalent to classical semantic security requirements. The
main interest of these criteria is to easily allow encryption of secret keys (either
symmetric or private keys).
Organization. In section 2 after recalling some basic deﬁnitions, we intro-
duce security criteria and some examples of cryptography-related criteria. A
powerful way of composing security criteria is introduced and proved in sec-
tion 3: the criterion partition theorem. Section 4 shows how to use this result
soundly. To illustrate this we prove that some composition of asymmetric and
symmetric encryption schemes can be directly stated secure by using the par-
tition theorem. Using this last result, section 5 proves computational sound-
ness of symbolic equivalence for an adaptive adversary using both asymmetric
and symmetric encryption schemes. Eventually, section 6 draws some concluding
remarks.
2
Preliminaries
2.1
Cryptographic Schemes
We ﬁrst recall classical deﬁnitions for cryptographic schemes in the computa-
tional setting. In this setting, messages are bit-strings and a security parameter
η is used to characterize the strength of the diﬀerent schemes, for example η can
denote the length of the keys used to perform an encryption.
An asymmetric encryption scheme AE = (KG, E, D) is deﬁned by three algo-
rithms. The key generation algorithm KG is a randomized function which given
a security parameter η outputs a pair of keys (pk, sk), where pk is a public key
and sk the associated secret key. The encryption algorithm E is also a random-
ized function which given a message and a public key outputs the encryption
of the message by the public key. Finally the decryption algorithm D takes as
input a cipher-text and a secret key and outputs the corresponding plain-text,
i.e. D(E(m, pk), sk) = m, if key pair (pk, sk) has been generated by KG. The
execution time of the three algorithms is assumed to be polynomially bounded
by η.
A symmetric encryption scheme SE = (KG, E, D) is also deﬁned by three
algorithms. The key generation algorithm KG is a randomized function which
given a security parameter η outputs a key k. The encryption algorithm E is
also a randomized function which given a message and a key outputs the en-
cryption of the message by this key. Finally the decryption algorithm D takes
as input a cipher-text and a key and outputs the corresponding plain-text, i.e.
D(E(m, k), k) = m. The execution time of the three algorithms is also assumed
polynomially bounded by η.

50
M. Daubignard et al.
A function g : R →R is negligible, if it is ultimately bounded by x−c, for each
positive c ∈
 , i.e. for all c > 0 there exists Nc such that |g(x)| < x−c, for all
x > Nc.
2.2
Turing Machines with Oracles
Adversaries are polynomial-time random Turing machines (PRTM) with oracles.
Oracles are also implemented using PRTMs. In order to detail the oracles an
adversary can query, the deﬁnition of an adversary A is for example:
Adversary A/O1, O2:
Code of A e.g: s ←O1(x)
Where the code of A can call two oracles using names O1 and O2. When ex-
ecuting this adversary A, we use the notation A/B1, B2 where B1 and B2 are
two PRTMs to denote that names O1 and O2 are respectively implemented with
oracles B1 and B2.
We use the standard λ-notation to concisely describe PRTMs obtained from
others by ﬁxing some arguments. For instance, let G be a PRTM that has two
inputs. Then, we write λs.G(s, θ) to describe the machine that is obtained from
G by ﬁxing the second argument to the value θ. Thus, A/λs.G(s, θ) denotes the
machine A that may query an oracle obtained from G by instantiating its second
argument by θ. The argument θ of G is deﬁned in the context of A and may not
be known by A. So typically, A may be trying to compute some information on
θ through successive queries.
Moreover, adversaries are often used as sub-routines in other adversaries. Con-
sider the following description of a randomized algorithm with oracles. Here ad-
versary A′ uses A as a sub-routine. Moreover, A′ may query oracle O1. On its
turn A may query the same oracle O1 and additionally the oracle λs.F2(s, θ2).
The latter is obtained from F2 by ﬁxing the second argument to θ2 which is
generated by A′.
Adversary A′/O1:
θ2←...
s←A/O1,
λs.F2(s, θ2)
2.3
Games and Criteria
A security criterion is deﬁned as a game involving an adversary (represented by
a PRTM). The game proceeds as follows. First some parameters θ are generated
randomly using a PRTM Θ. The adversary is executed and can query an oracle F
which depends on θ. At the end, the adversary has to answer a bit-string whose
correctness is checked by an algorithm V which also uses θ (e.g. θ includes a bit
b and the adversary has to output the value of b). Thus, a criterion is given by
a triple consisting of three randomized algorithms:

Game-Based Criterion Partition
51
– Θ is a PRTM that randomly generates some challenge θ.
– F is a PRTM that takes as arguments a bit-string s and a challenge θ and
outputs a new bit-string. F represents the oracles that an adversary can call
to solve its challenge.
– V is a PRTM that takes as arguments a bit-string s and a challenge θ and
outputs either true or false. It represents the veriﬁcation made on the result
computed by the adversary. The answer true (resp. false) means that the
adversary solved (resp. did not solve) the challenge.
As an example let us consider an asymmetric encryption scheme (KG, E, D). Se-
mantic security against chosen plain-text attacks (IND-CPA) can be represented
using a security criterion (Θ; F; V ) deﬁned as follows: Θ randomly samples the
challenge bit b and generates a key pair (pk, sk) using KG; F represents the pub-
lic key oracle (this oracle returns pk) and the left-right encryption oracle (given
bs0 and bs1 this oracle returns E(bsb, pk)); and V checks whether the returned
bit equals b.
Note that Θ can generate several parameters and F can represent several ora-
cles. Thus, it is possible to deﬁne criteria with multiples Θ and F. For example,
a criterion with two challenge generators Θ1 and Θ2, two oracles F1 and F2 and
a veriﬁer V is denoted by (Θ1, Θ2; F1, F2; V ).
Let γ = (Θ; F; V ). The advantage of a PRTM A against γ is deﬁned as the
probability that A has to win its game minus the probability that an adversary
can get without accessing oracle F.
Advγ
A(η) = 2 (Pr[Gγ
A(η) = true] −PrRandγ(η))
where Gγ
A(η) is the Turing machine deﬁned by:
Game Gγ
A(η):
θ←Θ(η)
d←A(η)/λs.F(s, θ)
return V (d, θ)
and PrRandγ(η) is the best probability to solve the challenge that an adversary
can have without using oracle F. Formally, let γ′ be the criterion (Θ; ϵ; V ) then
PrRandγ(η) is deﬁned by:
PrRandγ(η) = max
A

Pr[Gγ′
A(η) = true]

where A ranges over any possible PRTM. For example when considering a crite-
rion γ = (Θ; F; V ) where a challenge bit b is generated in Θ and V checks that
the adversary guessed the value of b, then PrRandγ(η) equals 1/2, in particular
this is the case for IND-CPA.
3
The Criterion Partition Theorem
Consider a criterion γ = (Θ1, Θ2; F1, F2; V1), composed of two challenge gen-
erators Θi, their related oracles Fi, and a veriﬁer V1. Assume that F1 and V1

52
M. Daubignard et al.
do not depend on θ2 (which is the part generated by Θ2). Because of these
assumptions, γ1 = (Θ1; F1; V1) is a valid criterion. We are going to relate the
advantages against γ and γ1. To do so, let us consider the game Gγ
A(η) played
by an adversary A against γ:
Game Gγ
A(η):
θ1←Θ1(η)
θ2←Θ2(η)
s←A/λs.F1(s, θ1),
λs.F2(s, θ1, θ2)
return V1(s, θ1)
We deﬁne an adversary A′ against γ1 which tries to act like A. However, A′
does not have access to its challenge θ1 and hence it generates a new challenge
θ′
1 (using Θ1) and uses it to answer queries made by A to F2.
Adversary A′/O1:
θ′
1←Θ1(η)
θ2←Θ2(η)
s←A/O1,
λs.F2(s, θ′
1, θ2)
return s
The game involving A′ against γ1, Gγ1
A′(η), is given by:
Game Gγ1
A′(η):
θ1←Θ1(η)
θ′
1←Θ1(η)
θ2←Θ2(η)
s←A/λs.F1(s, θ1),
λs.F2(s, θ′
1, θ2)
return V1(s, θ1)
Our aim is to establish a bound on
|Pr[Gγ
A(η) = true] −Pr[Gγ1
A′(η) = true]|
To do so, we construct an adversary B that tries to distinguish game Gγ
A(η) from
game Gγ1
A′(η), i.e. B tries to distinguish the case where A uses correlated oracles
(i.e. the same θ1 is used by F1 and F2) from the case where A uses decorrelated
oracles (i.e. θ1 is used by F1 and a diﬀerent θ′
1 is used by F2), ﬁgure 1 gives
the intuition of how B works: B either simulates A with correlated oracles in
the upper part of the ﬁgure or A with decorrelated oracles. Finally, B uses the
answer of A in order to win its challenge. We introduce a new indistinguishability
criterion γ2 that uses a challenge bit b, in this criterion the adversary has to
guess the value of bit b. Our objective is to build a distinguisher B such that the
following equations hold:
Pr[Gγ2
B = true | b = 1] = Pr[Gγ
A(η) = true]
(1)
Pr[Gγ2
B = false | b = 0] = Pr[Gγ1
A′(η) = true]
(2)

Game-Based Criterion Partition
53
θ1
θ2
θ1
θ′
1
θ2
F1
F2
F1
F2
A
A
V1
B
s
b
Fig. 1. Correlated and Decorrelated Oracles
Indeed, using these equations we will be able to derive the following bound:
|Pr[Gγ
A(η) = true] −Pr[Gγ1
A′(η) = true]| = Advγ2
B (η)
3.1
Construction of the Distinguisher
In the following, we give a methodology that tells us how to build the indistin-
guishability criterion γ2 and the adversary B. To do so, we need an assumption
on the form of the second oracle F2 from γ. This assumption is stated through
the following hypothesis.
Hypothesis 1. There exist three probabilistic random functions f, g and f ′
such that oracle F2’s implementation consists of two parts: λs.f(g(s, θ1), θ2) and
λs.f ′(s, θ2). The ﬁrst part depends on both θ1 and θ2 whereas the second depends
only on θ2.
The idea when introducing two parts for oracle F2 is to separate the oracles
contained in F2 that really depend on both θ1 and θ2 (these oracles are placed
in f(g(...))) from the oracles that do not depend on θ1 (placed in f ′). Let us
illustrate this on the IND-CPA criterion with two keys: there are one left-right
encryption oracle and one public key oracle for each key. Θ1 generates the chal-
lenge bit b and the ﬁrst key pair (pk1, sk1), Θ2 generates the other key pair
(pk2, sk2). Oracle F2 contains the left-right oracle related to pk2 and the pub-
lic key oracle that reveals pk2. Hence f ′ is used to store the public key oracle
whereas the left-right oracle has the form λs.f(g(s, θ1), θ2) where f performs
an encryption using key pk2 from θ2 and g((s0, s1), θ1) returns sb according to
the value of challenge bit b from θ1. It is possible to split the oracles diﬀerently
but this would not lead to interesting sub-criteria. In general it is always pos-
sible to perform a splitting that satisﬁes the previous hypothesis (for example,

54
M. Daubignard et al.
f ′ is empty and g(s, θ1) outputs both s and θ1), however this can lead to some
criteria against which adversaries may have a non-negligible advantage. In that
situation the partition theorem cannot be used to obtain that the advantage of
any adversary against the original criterion γ is negligible.
Adversary B plays against an indistinguishability criterion. It has access to
two oracles: ˆO1 is implemented by the left-right oracle f ◦LRb, where LRb takes
as argument a pair and returns either the ﬁrst or the second element according to
the value of bit b, i.e. LRb(x0, x1) = xb. Hence, we have f◦LRb(s0, s1) = f(sb, θ2)
and ˆO2 is simply implemented by f ′. Notice now that we have the following
equations:
f ◦LRb(g(s, θ′
1), g(s, θ1)) = F2(s, θ1, θ2), if b = 1
f ◦LRb(g(s, θ′
1), g(s, θ1)) = F2(s, θ′
1, θ2), if b = 0
More formally, our γ2 criterion is given by γ2 = (b, Θ2; f ◦LRb, f ′; vb), where vb
just checks whether the bit returned by the adversary equals b.
We are now ready to give a distinguisher B such that equations (1) and (2)
hold:
Adversary B/ ˆO1, ˆO2:
θ1←Θ1(η)
θ′
1←Θ1(η)
s←A/λs.F1(s, θ1),
// oracle F1
λs. ˆO1(g(s, θ′
1), g(s, θ1)),
// part f of oracle F2
ˆO2
// part f ′ of oracle F2
ˆb ←V1(s, θ1)
return ˆb
Recall that A may query two oracles: F1 and F2 while B may query the left-
right oracle f ◦LRb and f ′. Therefore, B uses Θ1 to generate θ1 and θ′
1. It is
important to notice that θ1 and θ′
1 are generated independently. Then, B uses A
as a sub-routine using λs.F1(s, θ) for A’s ﬁrst oracle, and the pair of functions
λs. ˆO1(g(s, θ′
1), g(s, θ1)) and f ′ for F2.
The game corresponding to B playing against γ2 can now be detailed:
Game Gγ2
B (η):
b ←{0, 1}
θ2←Θ2(η)
ˆb ←B/λs.f(LRb(s), θ2),
λs.f ′(s, θ2)
return vb(ˆb)
3.2
Comparing the Games
Let us now check equations (1) and (2). To do so, we ﬁrst consider that b equals
1. Then game Gγ2
B can be detailed by introducing the deﬁnition of B within the
game:

Game-Based Criterion Partition
55
Game Gγ2
B (η)|b = 1:
θ2←Θ2(η)
θ1←Θ1(η)
θ′
1←Θ1(η)
s←A/λs.F1(s, θ1)
λs.f(g(s, θ1), θ2),
λs.f ′(s, θ2)
ˆb←V1(s, θ1)
return ˆb = 1
After the hypothesis we made about the decomposition of oracle F2, and when
detailing B, this game can be rewritten as follows, and rigorously compared to
the game played by adversary A against criterion γ:
Game Gγ2
B (η)|b = 1:
θ1←Θ1(η)
θ′
1←Θ1(η)
θ2←Θ2(η)
s ←A/λs.F1(s, θ1),
λs.F2(s, θ1, θ2)
ˆb←V1(s, θ1)
return ˆb = 1
Game Gγ
A(η):
θ1←Θ1(η)
θ2←Θ2(η)
s←A/λs.F1(s, θ1),
λs.F2(s, θ1, θ2)
return V1(s, θ1)
Therefore these two games are equivalent and so equation (1) holds:
Pr[Gγ2
B = true | b = 1] = Pr[Gγ
A(η) = true]
We now detail the game played by adversary B against γ2 when the challenge
bit b is 0. This game is compared to the game played by A′ against γ1.
Game Gγ2
B (η)|b = 0:
θ2←Θ2(η)
θ1←Θ1(η)
θ′
1←Θ1(η)
s ←A/λs.F1(s, θ1),
λs.F2(s, θ′
1, θ2)
ˆb←V1(s, θ1)
return ˆb = 0
Game Gγ1
A′(η):
θ1←Θ1(η)
θ′
1←Θ1(η)
θ2←Θ2(η)
s←A/λs.F1(s, θ1),
λs.F2(s, θ′
1, θ2)
return V1(s, θ1)
It is easy to see that these two games can be compared: adversary B wins
anytime A′ loses, and thus:
Pr[Gγ1
A′(η) = false] = Pr[Gγ2
B (η) = true|b = 0]
We can therefore evaluate our distinguisher’s advantage. For that purpose let us
ﬁrst notice that as γ2 consists in guessing the value of a random bit b, PrRandγ2
equals 1/2. Furthermore γ and γ1 have the same veriﬁer V1, hence PrRandγ is
equal to PrRandγ1.

56
M. Daubignard et al.
Advγ2
B (η) = 2

Pr[Gγ2
B (η) = true] −PrRandγ2
= 2Pr[Gγ2
B (η) = true|b = 1]Pr[b = 1] +
2Pr[Gγ2
B (η) = true|b = 0]Pr[b = 0] −1
= Pr[Gγ
A(η) = true] + Pr[Gγ1
A′(η) = false] −1
= Pr[Gγ
A(η) = true] −Pr[Gγ1
A′(η) = true]
= Pr[Gγ
A(η) = true] −PrRandγ
+PrRandγ1 −Pr[Gγ1
A′(η) = true]
= 1
2Advγ
A(η) −1
2Advγ1
A′(η)
Given an adversary A against γ, we were able to build an adversary A′ against
γ1 and an adversary B against γ2 such that:
∀η, Advγ
A(η) = 2Advγ2
B (η) + Advγ1
A′(η)
This is summed up in the following theorem which is our core result.
Theorem 1 (Criterion Partition). Let γ be the criterion (Θ1, Θ2; F1, F2; V1)
where:
1. V1 and F1 only depend on the challenge generated by Θ1, denoted by θ1.
2. There exist some PRTMs f, f ′ and g such that F2 is constituted of two parts:
λs.f(g(s, θ1), θ2) and λs.f ′(s, θ2)
Then, for any adversary A against criterion γ, there exist two adversaries B and
A′, such that:
∀η, Advγ
A(η) = 2Advγ2
B (η) + Advγ1
A′(η)
where γ2 = (Θ2, b; f ◦LRb, f ′; vb) is an indistinguishability criterion and γ1 =
(Θ1; F1; V1).
This theorem can be used to prove that the advantage of any adversary against
a criterion γ is negligible. For that purpose, one has to provide a partition of
γ such that the advantage of any adversary against γ1 or γ2 is negligible. Then
we get that for an adversary A against γ, the advantage of A can be bounded
by the advantage of an adversary against γ1 and the advantage of an adversary
against γ2. The advantage of these two new adversaries are negligible and so the
advantage of A is also negligible.
4
Mixing Asymmetric and Symmetric Encryption
4.1
Cryptographic Game: N-PAT-IND-CCA
We introduce a security criterion that turns out to be useful for protocols where
secret keys are exchanged. This criterion is an extension of semantic security
against chosen cipher-text attacks (IND-CCA). In the classical N-IND-CCA

Game-Based Criterion Partition
57
criterion (see [BBM00] about N-IND-CCA and its reduction to IND-CCA), a
random bit b is sampled. For each key, the adversary has access to a left-right
oracle (the adversary submits a pair of bit-strings bs0, bs1 and receives the en-
coding of bsb) and a decryption oracle (that does not work on the outputs of the
left-right oracle). The adversary has to guess the value of b. Criterion IND-CPA
is the same as IND-CCA except that the adversary does not have access to the
decryption oracle.
Since it has no information concerning secret keys, the adversary cannot get
the encryption of a challenge secret key under a challenge public key. Therefore,
we introduce the N-PAT-IND-CCA criterion where the adversary can obtain the
encryption of messages containing challenge secret keys, even if it does not know
their values. For that purpose, the adversary is allowed to give pattern terms to
the left-right oracles.
Pattern terms are terms where new atomic constants have been added: pat-
tern variables. These variables represent the diﬀerent challenge secret keys and
are denoted by [i] (this asks the oracle to replace the pattern variable by the
value of ski). Variables can be used as atomic messages (data pattern) or at a
key position (key pattern). When a left-right oracle is given a pattern term, it
replaces patterns by values of corresponding keys and encodes the so-obtained
message.
More formally, patterns are given by the following grammar where bs is a
bit-string and i is an integer. In the deﬁnition of pattern terms, we use two
binary operators: concatenation and encryption. Concatenation of patterns pat0
and pat1 is written (pat0, pat1). Encryption of pat with key bs is denoted by
{pat}bs. Similarly, when the key is a challenge key, it is represented by a pattern
variable [i].
pat ::= (pat, pat) | {pat}bs | {pat}[i]
| bs | [i]
The computation (evaluation) made by the oracle is easily deﬁned recursively
in a context θ associating bit-string values to the diﬀerent keys. Its result is a
bit-string and it uses the encryption algorithm E and the concatenation denoted
by “·” in the computational setting.
v(bs, θ) = bs
v([i], θ) = θ(ski)
v((p1, p2), θ) = v(p1, θ) · v(p2, θ)
v({p}bs, θ) = E(v(p, θ), bs)
v({p}[i], θ) = E(v(p, θ), θ(pki))
There is yet a restriction. Keys are ordered and a pattern [j] can only be
encrypted under pki if i < j to avoid key cycles. This restriction is well-known
in cryptography and widely accepted [AR00]. When the left-right pattern en-
cryption oracle related to key i is given two pattern terms pat0 and pat1, it tests
that none contains a pattern [j] with j ≤i. If this happens, it outputs an error
message, else it produces the encryption of the message corresponding to patb,
v(patb, θ), using public key pki. To win, the adversary has to guess the value of

58
M. Daubignard et al.
secret bit b. In fact our acyclicity hypothesis only occurs on secret keys: when
considering pattern {{p}[j]}[j], the public key oracle related to key j can be called
and returns bit-string bs, then pattern {{p}bs}[j] can be used to get the awaited
result. We do not detail restrictions on the length of arguments submitted to
the left-right oracle, an interesting discussion on that point appears in [AR00].
The most simple restriction is to ask that both submitted patterns can only be
evaluated (using v) to bit-strings of equal length.
Henceforth, let AE = (KG, E, D) be an asymmetric encryption scheme. Then,
criterion N-PAT-IND-CCA is given by γN = (Θ; F; V ), where Θ randomly gen-
erates N pairs of keys (pk1, sk1) to (pkN, skN) using KG and a bit b; V veriﬁes
whether the adversary gave the right value for bit b; and F gives access to three
oracles for each i between 1 and N: a left-right encryption oracle that takes as
argument a pair of patterns (pat0, pat1) and outputs patb completed with the
secret keys (v(patb, θ)) and encoded using pki; a decryption oracle that decodes
any message that was not produced by the former encryption oracle; and an
oracle that simply makes the public key pki available.
Then, AE is said N-PAT-IND-CCA iﬀfor any adversary A, AdvγN
A (η) is
negligible. Note that N-PAT-IND-CCA with N = 1 corresponds to IND-CCA.
Proposition 1. Let N be an integer. If an asymmetric encryption scheme AE
is IND-CCA, then AE is N-PAT-IND-CCA.
Proof. We want to establish ﬁrst that an IND-CCA asymmetric encryption
scheme is an N-PAT-IND-CCA secure one. We use the criterion reduction
theorem on N-PAT-IND-CCA (denoted by δN). We now consider δN
=
(Θ1, Θ2; F1, F2; V1), where the criterion partition has been performed the fol-
lowing way:
– Θ1 randomly generates the bit b and N −1 pairs of matching public and
secret keys (pk2, sk2) to (pkN, skN) using KG.
– Θ2 randomly generates the ﬁrst key pair (pk1, sk1).
– F1 contains the oracles related to θ1; hence as neither pk1 nor sk1 can be
asked to this oracle (because of acyclicity), F1 does not depend on θ2.
– F2 contains the oracles related to key pair (pk1, sk1), it uses θ1 for the bit b
and the diﬀerent keys needed to ﬁll in patterns.
– V1 compares the output to b, and therefore only depends on θ1.
This splitting complies with the ﬁrst hypothesis of theorem 1. Let us then check
whether the second hypothesis holds. The decryption and public key oracles
included in F2 only depend on θ2, we place them in f ′. We let the encryption
oracle be λs.f(g(s, θ1), θ2) where g((pat0, pat1), θ1) = v(patb, θ1) plays the role
of a left-right oracle, b being the challenge bit included in θ1, composed with the
valuation function v that completes patterns, and f(bs, θ2) = E(bs, pk1) is the
original encryption oracle.
The theorem can now be applied. It thus follows that for any adversary A
against criterion δN, there exist two adversaries B and A′, such that:
∀η, AdvδN
A (η) = 2Advγ2
B (η) + Advγ1
A′(η)

Game-Based Criterion Partition
59
where γ2 = (Θ2, b; f ◦LRb, f ′; vb) is IND-CCA and γ1 = (Θ1; F1; V1) is criterion
δN−1.
Hence if we suppose that the asymmetric encryption scheme AE is IND-CCA
and N −1-PAT-IND-CCA, then the advantages of A′ and B are negligible, so
the advantage of A is also negligible and AE is N-PAT-IND-CCA. Moreover,
as 0-PAT-IND-CCA consists in guessing a challenge bit without access to any
oracle, any adversary’s advantage against it is thus null, which obviously implies
that any encryption scheme is 0-PAT-IND-CCA. Using a quick recursion, it now
appears clearly that if an asymmetric encryption scheme is IND-CCA, it is also
N-PAT-IND-CCA for any integer N.
In this proof, we bound the advantage against N-PAT-IND-CCA by 2N times
the advantage against IND-CCA. This bound is not contradictory with the one
proposed by [BBM00] as the number of queries to each oracle is unbounded in
our model.
4.2
Cryptographic Game: N-PAT-SYM-CPA
In this section, we introduce a new criterion describing safety of a symmetric
encryption scheme. This deﬁnition is an extension of semantic security against
chosen plain-text attacks. The main diﬀerence with the N-PAT-IND-CCA cri-
terion is that there are no public key oracles and no decryption oracles. Hence
the left-right encryption oracles are similar to those presented in the previous
section and the adversary still has to guess the value of the challenge bit b. The
hypothesis related to acyclicity of keys still holds: ki can only appear encoded
by kj if i > j.
The N-PAT-SYM-CPA criterion is γN = (Θ, F, V ) where Θ generates N
symmetric keys and a bit b; F gives access to one oracle for each key: a left-
right encryption oracle that takes as argument a pair of patterns (pat0, pat1)
and outputs patb completed with the secret keys (v(patb, θ)) and encoded with
ki. Finally, V returns true when the adversary returns bit b.
Let γN be a criterion including the oracles detailed above. A symmetric en-
cryption scheme SE is said N-PAT-SYM-CPA iﬀfor any adversary A, the ad-
vantage of A against γN, AdvγN
SE,A(η), is negligible in η.
Using the criterion partition theorem, it is possible to reduce criterion N-
PAT-SYM-CPA to criterion SYM-CPA. This can be done by using the same
partition as for criterion N-PAT-IND-CCA.
Proposition 2. Let N be an integer. If a symmetric encryption scheme SE is
SYM-CPA, then SE is N-PAT-SYM-CPA.
4.3
Cryptographic Games: N-PAS-CCA and N-PAS-CPA
These criteria combine both precedent ones. N asymmetric and symmetric keys
are generated along with a single challenge bit b. The adversary can access oracles
it was granted in both previous criteria (left-right encryption, public key and
decryption for the asymmetric scheme in N-PAS-CCA) and has to deduce the

60
M. Daubignard et al.
value of the challenge bit b. The acyclicity condition still holds on both primitives.
However, we authorize patterns using symmetric keys when accessing left-right
oracles from the asymmetric part. Hence symmetric encryption and symmetric
keys can be used under asymmetric encryption but the converse is forbidden.
The pattern deﬁnition has to be extended so that the adversary can ask for both
asymmetric and symmetric encryptions and asymmetric and symmetric keys.
Let γN be the criterion including the oracles detailed above. A cryptographic
library (AE, SE) is said N-PAS-CCA iﬀfor any adversary A the advantage of A,
AdvγN
AE,SE,A(η), is negligible. The challenge bit b is common to asymmetric and
symmetric encryption, thus it is non trivial to prove that IND-CCA and SYM-
CPA imply N-PAS-CCA. However using our partition theorem, it is possible to
prove this implication.
Proposition 3. Let N be an integer. If an asymmetric encryption scheme AE
is IND-CCA and a symmetric encryption scheme SE is SYM-CPA, then the
cryptographic library (AE, SE) is N-PAS-CCA.
This can easily be adapted to prove variants of this property, for example let
us consider the IND-CPA criterion for the symmetric encryption scheme (the
adversary only has access to the left-right oracle and has to guess the challenge
bit) and the N-PAS-CPA criterion for a cryptographic library (the adversary has
access to public keys for the asymmetric encryption scheme, to left-right oracles
using patterns such that asymmetric secret keys cannot be asked to symmetric
encryption oracles).
Proposition 4. Let N be an integer. If an asymmetric encryption scheme AE
is IND-CPA and a symmetric encryption scheme SE is SYM-CPA, then the
cryptographic library (AE, SE) is N-PAS-CPA.
5
Computational Soundness of Adaptive Security
In this section, we prove computational soundness of symbolic equivalence for
messages that use both asymmetric and symmetric encryption in the case of an
adaptive adversary. This model has been introduced in [MP05]. Roughly, speak-
ing it corresponds to the case of a passive adversary that however can adaptively
chose symbolic terms and ask for their computational evaluation whereas in the
passive case [AR00], the adversary is confronted with two ﬁxed symbolic terms.
The practical signiﬁcance of this model is discussed in [MP05]. Our result is
an extension of the soundness result from [MP05], moreover we propose a more
modular approach which does not use any hybrid argument but is based on
proposition 4. Another improvement is that we allow the adversary to reuse
computational values within symbolic terms, constants in messages can be used
to represent any bit-string. To simplify things up, we do not consider polynomial
sequences of messages as in [MP05] but rather bounded sequences of messages.
In fact, to cope with the polynomial case, we need to extend theorem 1 in or-
der to handle a polynomial number of challenges. This extension is presented
in [Maz06].

Game-Based Criterion Partition
61
5.1
A Symbolic Treatment of Cryptography
Let SymKeys,PKeys,SKeys and Const be four disjoint sets of symbols rep-
resenting symmetric keys, public keys, secret keys and constants. Let Atoms be
the union of the previous sets. We assume the existence of a bijection []−1 from
PKeys to SKeys that associates to each public key the corresponding secret
key. The inverse of this function is also denoted []−1. The set Msg of messages
is deﬁned by the following grammar.
Msg ::= SymKeys | Const | (Msg, Msg) | {Msg}s
SymKeys | {Msg}a
PKeys
Elements of SymKeys can be thought of as randomly sampled keys, elements
of Const as bit-strings. Term (m, n) represents the pairing of message m and n,
{m}s
k represents the symmetric encryption of m using key k and {m}a
pk repre-
sents the asymmetric encryption of m using public key pk. In the sequel, when
presenting examples, we use symbols 0 and 1. These are to be understood as ele-
ments of Const which computational interpretations are respectively bit-strings
0 and 1.
Next we deﬁne when a message m ∈Msg can be deduced from a set of
messages E ⊆Msg (written E ⊢m) by a passive eavesdropper. The deduction
relation ⊢is deﬁned by the standard Dolev-Yao inference system [DY83] and is
given by the following rules:
m∈E
E⊢m
E⊢(m1,m2)
E⊢m1
E⊢(m1,m2)
E⊢m2
E⊢m1
E⊢m2
E⊢(m1,m2)
E⊢m
E⊢k
E⊢{m}s
k
E⊢{m}s
k
E⊢k
E⊢m
E⊢m
E⊢pk
E⊢{m}a
pk
E⊢{m}a
pk
E⊢pk−1
E⊢m
The information revealed by a symbolic expression can be characterized using
patterns [AR00, MP05]. For a message m ∈Msg its pattern is deﬁned by the
following inductive rules:
pattern

(m1, m2)

=

pattern(m1), pattern(m2)

pattern

{m′}s
k

= {pattern(m′)}s
k
if m ⊢k
pattern

{m′}s
k

= {□}s
k
if m ̸⊢k
pattern

{m′}a
pk

= {pattern(m′)}a
pk
if m ⊢pk−1
pattern

{m′}pk

= {□}a
pk
if m ̸⊢pk−1
pattern(m′) = m′
if m′ ∈Atoms
The symbol □represents a cipher-text that the adversary cannot decrypt. As □
does not store any information on the length or structure of the corresponding
plain-text, we assume that the encryption schemes used here do not reveal plain-
text lengths (see [AR00] for details). Two messages are said to be equivalent if
they have the same pattern: m ≡n if and only if pattern(m) = pattern(n).
Two messages are equivalent up to renaming if they are equivalent up to some
renaming of keys: m ∼= n if there exists a renaming σ of keys from n such that
m ≡nσ.
Example 1. Let us illustrate this equivalence notion. We have that:

62
M. Daubignard et al.
– {0}s
k ∼= {1}s
k encryptions with diﬀerent plain-text cannot be distinguished if
the key is not deducible.
– ({0}s
k, {k}a
pk, pk−1) ̸∼= ({1}s
k, {k}a
pk, pk−1) but it is not the case if the key can
be deduced.
5.2
Computational Soundness
This model is parameterized by an asymmetric encryption scheme AE
=
(KGa, Ea, Da) and a symmetric encryption scheme SE = (KGs, Es, Ds). Com-
putational semantics are given by a concretization function concr which can
be derived from the v function that was introduced previously. This algorithm
uses a computational substitution θ which stores bit-string values for keys. Con-
stants from Const represents bit-strings so the concretization of c from Const
is c itself.
concr((m1, m2), θ) = concr(m1, θ) · concr(m2, θ)
concr({m}a
pk, θ) = Ea(concr(m, θ), θ(pk))
concr({m}s
k, θ) = Es(concr(m, θ), θ(k))
concr(k, θ) = θ(k)
concr(c, θ) = c
Thus the computational distribution generated by a message can be obtained
by randomly sampling the necessary keys and using the concr function.
We consider a model where the adversary can see the computational version
of a bounded sequence of adaptively chosen messages. Let α be a bound on the
sequence length. The adaptive experiment proceeds as follows: the adversary
has access to one oracle which takes as argument a pair of messages (m0, m1)
and either outputs a concretization of m0 (oracle O0) or a concretization of m1
(oracle O1). These oracles work by randomly sampling the necessary keys then
using the concr function on either m0 or on m1. Finally, the adversary has to tell
against which oracle it is playing, O0 or O1. The advantage of A is deﬁned by:
Advadpt
AE,SE,A(η) = Pr[A/O1 = 1] −Pr[A/O0 = 1]
Moreover there are restrictions on the sequence of messages submitted by the
adversary (m1
0, m1
1) to (mq
0, mq
1). Such a sequence is said to be legal if:
1. Messages (m1
0, ..., mq
0) and (m1
1, ..., mq
1) are equivalent up to renaming.
2. Messages (m1
0, ..., mq
0) and (m1
1, ..., mq
1) contain no encryption cycles, more-
over secret keys cannot be sent under symmetric encryptions.
3. The lengths of (m1
0, ..., mq
0) and (m1
1, ..., mq
1) are lower than α.
Proposition 5. If AE is an IND-CPA secure encryption scheme and SE is a
SYM-CPA secure encryption scheme, then the advantage of any legal adversary
A, Advadpt
AE,SE,A(η), is a negligible function in η.
This result can be used to model secure multicast as presented in [MP05].

Game-Based Criterion Partition
63
6
Conclusion
This paper contributes to the development of a proof theory for cryptographic
systems by providing a theorem that allows to decompose the proof of correctness
of a security criterion to the correctness of a sub-criterion and an indistinsguisha-
bility criterion. We apply this decomposition result to prove that given secure
asymmetric and symmetric encryption schemes we can combine them to obtain
a secure cryptographic library.
This security result can be used to easily prove computational soundness of
formal methods. This has been illustrated in the case of the adaptive setting for
asymmetric and symmetric encryption.
In future works, we intend to develop this computational soundness result to
the case of security protocols in general against an active adversary. We believe
that our partition theorem will also be useful in this situation, in particular by
giving simpler and more modular proofs of soundness.
References
[AJ01]
Abadi, M., J¨urjens, J.: Formal eavesdropping and its computational in-
terpretation. In: Kobayashi, N., Pierce, B.C. (eds.) TACS 2001. LNCS,
vol. 2215, pp. 82–94. Springer, Heidelberg (2001)
[AR00]
Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the compu-
tational soundness of formal encryption). In: IFIP International Conference
on Theoretical Computer Science (IFIP TCS2000), Sendai, Japan, Springer,
Berlin (2000)
[BBM00]
Bellare, M., Boldyreva, A., Micali, S.: Public-key encryption in a multi-user
setting: Security proofs and improvements. In: Preneel, B. (ed.) EURO-
CRYPT 2000. LNCS, vol. 1807, pp. 259–274. Springer, Heidelberg (2000)
[BCK05]
Baudet, M., Cortier, V., Kremer, S.: Computationally sound implemen-
tations of equational theories against passive adversaries. In: Caires, L.,
Italiano, G.F., Monteiro, L., Palamidessi, C., Yung, M. (eds.) ICALP 2005.
LNCS, vol. 3580, Springer, Heidelberg (2005)
[Bla06]
Blanchet, B.: A computationally sound mechanized prover for security pro-
tocols. In: IEEE Symposium on Security and Privacy, Oakland, California
(May 2006)
[BPW03]
Backes, M., Pﬁtzmann, B., Waidner, M.: A composable cryptographic li-
brary with nested operations. In: Proceedings of the 10th ACM conference
on Computer and communication security, pp. 220–230 (2003)
[BR04]
Bellare, M., Rogaway, P.: The game-playing technique. Cryptology ePrint
Archive, Report 2004/331 (2004), http://eprint.iacr.org/
[Cou90]
Cousot, P.: Methods and Logics for Proving Programs. In: Handbook of
Theoretical Computer Science, vol. B: Formal Methods and Semantics, pp.
841–994. Elsevier Science Publishers B.V, Amsterdam (1990)
[CW05]
Cortier, V., Warinschi, B.: Computationally sound, automated proofs for se-
curity protocols. In: Sagiv, M. (ed.) ESOP 2005. LNCS, vol. 3444, Springer,
Heidelberg (2005)
[DY83]
Dolev, D., Yao, A.C.: On the security of public key protocols. IEEE Trans-
actions on Information Theory 29(2), 198–208 (1983)

64
M. Daubignard et al.
[GM84]
Goldwasser, S., Micali, S.: Probabilistic encryption. Journal of Computer
and System Sciences 28(2), 270–299 (1984)
[JLM05]
Janvier, R., Lakhnech, Y., Mazar´e, L.: Completing the picture: Soundness
of formal encryption in the presence of active adversaries. In: Sagiv, M. (ed.)
ESOP 2005. LNCS, vol. 3444, Springer, Heidelberg (2005)
[Maz06]
Mazar´e, L.: Computational Soundness of Symbolic Models for Crypto-
graphic Protocols. PhD thesis, INPG, Grenoble (October 2006) (to appear)
[MP92]
Manna, Z., Pnueli, A.: The temporal logic of reactive and concurrent sys-
tems. Springer, Heidelberg (1992)
[MP05]
Micciancio, D., Panjwani, S.: Adaptive security of symbolic encryption. In:
Kilian, J. (ed.) TCC 2005. LNCS, vol. 3378, pp. 169–187. Springer, Heidel-
berg (2005)
[MW04]
Micciancio, D., Warinschi, B.: Soundness of formal encryption in the pres-
ence of active adversaries. In: Proceedings of the Theory of Cryptography
Conference, pp. 133–151. Springer, Heidelberg (2004)
[Sho04]
Shoup, V.: Sequences of games: a tool for taming complexity in security
proofs (2004)

Measuring Anonymity with Relative Entropy
Yuxin Deng1,2, Jun Pang3, and Peng Wu4
1 The University of New South Wales
School of Computer Science and Engineering, 2052 Sydney, Australia
yuxind@cse.unsw.edu.au
2 Shanghai Jiaotong University
Department of Computer Science and Engineering, 200240 Shanghai, China
3 Carl von Ossietzky Universit¨at Oldenburg
Department f¨ur Informatik, 26111 Oldenburg, Germany
jun.pang@informatik.uni-oldenburg.de
4 INRIA Futurs and LIX, ´Ecole Polytechnique
Rue de Saclay, 91128 Palaiseau, France
wu@lix.polytechnique.fr
Abstract. Anonymity is the property of maintaining secret the iden-
tity of users performing a certain action. Anonymity protocols often use
random mechanisms which can be described probabilistically. In this pa-
per, we propose a probabilistic process calculus to describe protocols for
ensuring anonymity, and we use the notion of relative entropy from infor-
mation theory to measure the degree of anonymity these protocols can
guarantee. Furthermore, we prove that the operators in the probabilis-
tic process calculus are non-expansive, with respect to this measuring
method. We illustrate our approach by using the example of the Dining
Cryptographers Problem.
1
Introduction
With the growth and commercialisation of the Internet, users become more
and more concerned about their anonymity and privacy in the digital world.
Anonymity is the property of keeping secret the identity of the user who has per-
formed a certain action. The need for anonymity may arise in a wide range of sit-
uations, from votings and donations to postings on electronic forums. Anonymity
protocols often use random mechanisms. Typical examples are the Dining Cryp-
tographers [5], Crowds [22], Onion Routing [27], SG-MIX [16], and many others.
Quantifying the degree of anonymity a protocol can guarantee is a line of
active research. Various notions, like anonymity set and information theoretic
metric, have been investigated in the literature [5,22,2,25,9,6]. (See detailed dis-
cussions in Section 5.) In particular, [25,9] used the notion of entropy from infor-
mation theory as a measure for anonymity. It takes into account the probability
distribution of the users performing certain actions, where the probabilities are
assigned by an attacker after observing the system. However, it does not take
into account the attacker’s knowledge about the users before running a protocol.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 65–79, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

66
Y. Deng, J. Pang, and P. Wu
In this paper we propose to use relative entropy as a general extension of the
aforementioned approaches. Our method quantiﬁes the amount of probabilistic
information revealed by the protocol, i.e. how much information an attacker can
obtain after observing the outcomes of the protocol, together with the informa-
tion he has before the protocol running. For a protocol that contains both non-
deterministic and probabilistic behaviours, we extend this measuring method to
deal with two sets of probability distributions by using Hausdorﬀdistance (see
Deﬁnition 4).
Nowadays, the need for applying formal methods to security protocols has
been widely recognised. To our knowledge, there have been several attempts to
develop a formal framework for specifying and reasoning about anonymity prop-
erties. Schneider and Sidiropoulos [23] studied anonymity in CSP [14], but they
only considered non-deterministic behaviour. Bhargava and Palamidessi [3] pro-
posed a notion of probabilistic anonymity with careful distinction between non-
deterministic and probabilistic behaviours and used a probabilistic π-calculus
as a speciﬁcation language. This work was extended by Deng, Palamidessi and
Pang in [6], where a weak notion of probabilistic anonymity was deﬁned to cap-
ture the amount of probabilistic information that may be revealed by a protocol.
Other researchers deﬁne their notions of anonymity in terms of epistemic logic
[13,12] and “function views” [15].
In this paper, we follow the approach based on process calculi. Speciﬁcally, we
propose a probabilistic extension of CCS [19], in the style of [1], to describe pro-
tocols for ensuring anonymity. It allows us to specify both non-deterministic
and probabilistic behaviours. The operational semantics of a process is de-
ﬁned in terms of a probabilistic automaton [24]. Our formal characterisation
of anonymity is then based on permutations over the traces of a probabilistic
automaton. Inspired by [7,8], we prove that except for the parallel composition
all operators in our probabilistic CCS are non-expansive, with respect to the
measuring method using relative entropy, which allows us to estimate the degree
of anonymity of a complex system from its components, rather than analyse the
system as a whole. We illustrate our ideas by using the example of the Dining
Cryptographers Problem (DCP), in which a number of cryptographers cooperate
to ensure that the occurrence of a certain action is visible, while the user who
has performed it remains anonymous.
We summarise our main contributions of this work as follows:
– We propose to use relative entropy for measuring the degree of anonymity a
protocol can guarantee. It is an extension of the results in [25,9].
– We deﬁne a probabilistic CCS for specifying protocols, and prove the non-
expansiveness of some operators.
– We show how to use our framework to reason about the degree of anonymity
of protocols by the example of the Dining Cryptographers Problem.
Plan of the paper. In next section we recall some basic notations which are
used throughout the paper. In Section 3, we use relative entropy to measure
anonymity, and we present the non-expansiveness proof for the operators in a

Measuring Anonymity with Relative Entropy
67
probabilistic CCS. In Section 4, we apply our framework to the Dining Cryp-
tographers Problem. In Section 5, we compare our approach with some related
work. Finally, we conclude the paper by discussing our future work in Section 6.
2
Preliminaries
In this section, we present some basic deﬁnitions from probability theory, the
notion of probabilistic automata, and a probabilistic CCS.
2.1
Probability Measure
Let Ω be a set. A σ-ﬁeld over Ω is a collection F of subsets of Ω containing ∅
and closed under complements and countable unions. A probability measure on
a σ-ﬁeld F is a function η : F →[0, 1] such that η(Ω) = 1 and, for each family
{Zi}i∈N of pairwise disjoint elements of F, η(
i∈N Zi) = 
i∈N η(Zi). A discrete
probability measure over Ω is a probability measure whose σ-ﬁeld is the powerset
of Ω. A discrete probability distribution is a function η : Ω →[0, 1] such that

s∈Ω η(s) = 1. The support of η is deﬁned to be the set supp(η) = {s ∈Ω |
η(s) ̸= 0}. We denote by D(Ω) the set of probability distributions over Ω.
2.2
Probabilistic Automata
We give a brief review of the formalism probabilistic automata [24].
Deﬁnition 1. A probabilistic automaton is a tuple M = (S, s0, E, H, →) where
– S is a set of states,
– s0 is the start state,
– E is a set of external actions,
– H is a set of internal (hidden) actions,
– →⊆S × (E ∪H) × D(S) is a transition relation.
We often write s
a→η for (s, a, η) ∈→. Informally, a probabilistic automaton
is like an ordinary automaton except that a labelled transition leads to a prob-
ability distribution over a set of states instead of a single state. We will use
probabilistic automata to give operational semantics for the probabilistic CCS
that will be introduced in next section.
A path π of M is a (ﬁnite or inﬁnite) sequence of the form s0a1η1s1a2η2s2...
such that
1. each si (resp. ai, ηi) denotes a state (resp. action, distribution over states);
2. s0 is the initial state;
3. if π is ﬁnite, then it ends with a state;
4. si
ai+1
→ηi+1 and si+1 ∈supp(ηi+1), for each non-ﬁnal i.

68
Y. Deng, J. Pang, and P. Wu
The set of all paths of M is denoted Path(M), while the set of ﬁnite paths is
denoted Path∗(M). The last state of a ﬁnite path π is written last(π). A path
π is maximal if either it is inﬁnite or it is a ﬁnite path without any outgoing
transitions from last(π).
A scheduler σ of M is a partial function of type Path∗(M) →(E ∪H)×D(S)
such that (i) for each path π that is not maximal σ(π) is deﬁned, (ii) σ(π) = (a, η)
implies last(π)
a→η. A scheduler σ of M induces a discrete probability measure
on the σ-ﬁeld generated by cones of paths as follows. If π is a ﬁnite path, then
the cone generated by π is the set of paths Cπ = {π′ ∈Path(M) | π ⪯π′},
where ⪯denotes the preﬁx ordering on sequences. The measure ϵ of a cone Cπ
is deﬁned by
ϵ(Cπ) =
⎧
⎨
⎩
1
if π = s0
ϵ(Cπ′) · η(s′)
if π = π′aηs′ and σ(π′) = (a, η)
0
otherwise.
The measure ϵ is called a probabilistic execution of M.
The trace of a path π of an automaton M, written tr(π), is the sequence
obtained by restricting π to the set of external actions of M. A trace is maximal
if it is so obtained from a maximal path. The cone of a ﬁnite trace γ is deﬁned by
Cγ = {γ′ ∈Eω | γ ⪯γ′}. Given a probabilistic execution ϵ, the trace distribution
of ϵ, td(ϵ), is the measure on the σ-ﬁeld generated by cones of traces deﬁned by
td(ϵ)(Cγ) =

tr(π)=γ
ϵ(Cπ).
If there are only countably many maximal traces in a probabilistic automaton
(which is the case in many applications including all examples in this paper),
a trace distribution corresponds to a discrete probability distribution on the
maximal traces of a fully probabilistic automaton resulted from resolving all
non-determinism of the probabilistic automaton. We denote the set of trace dis-
tributions of probabilistic executions of a probabilistic automaton M by tds(M).
2.3
Probabilistic CCS
In this section we give a probabilistic extension of Milner’s CCS [19] which is
based on the calculus of [1] that allows for non-deterministic and probabilistic
choice. We assume a countable set of variables, Var = {X, Y, ...}, and a countable
set of atomic actions, A = {a, b, ...}. Given a special action τ not in A, we let
u, v, ... range over the set of actions, Act = A∪A ∪{τ}. The class of expressions
E is deﬁned by the following syntax:
E ::= 0 |

i∈I
upi.Ei | E1 ⊞E2 | E1 | E2 | E \ A | f[E] | X | μXE
where A ⊆A, f : Act →Act is a renaming function, I is a nonempty countable
indexing set and {pi}i∈I a family of probabilities such that 
i∈I pi = 1. For

Measuring Anonymity with Relative Entropy
69
ﬁnite indexing set I = {i1, ..., in} we also write upi1.Epi1 +...+upin.Epin instead
of 
i∈I upi.Ei. The construction E1 ⊞E2 stands for non-deterministic choice,
which is denoted by + in CCS. We use | to denote the usual parallel composition.
The restriction and renaming operators are as in CCS: E \ A behaves like E
as long as E does not perform an action a ∈A; f[E] behaves like E where
each action a ∈Act is replaced by f(a). We let variables range over process
expressions. The notation μX stands for a recursion which binds the variable X.
We use fv(E) for the set of free variables (i.e., not bound by any μX) in E. As
usual we identify expressions which diﬀer only by a change of bound variables.
We use P, Q, ... to range over Pr, the set of expressions without free vari-
ables, called processes. The operational semantics of a process P is deﬁned as
a probabilistic automaton whose states are the processes reachable from P and
the transition relation is deﬁned by the rules in Figure 1, where P
u→η de-
scribes a transition that, by performing an action u, leaves from P and leads to
a distribution η over Pr.
The presence of both probabilistic and non-deterministic choice in the prob-
abilistic CCS allows us to specify systems that have both probabilistic and non-
deterministic behaviour. Given a process P, we denote by pa(P) the probabilistic
automaton that represents the operational semantics of P via the rules in Fig-
ure 1. If there is no occurrence of non-deterministic choice in P, the automaton
pa(P) is fully probabilistic. In this case tds(pa(P)) is a singleton set of trace
distribution.
1. 
i∈I upi.Pi
u→η where η(P) = {pi | i ∈I, Pi = P}
2. P1 ⊞P2
u→η if P1
u→η or P2
u→η
3. P1 | P2
u→η if one the following four conditions is satisﬁed:
(a) P1
u→η1 and η(P) =
 η1(P ′
1) if P = P ′
1 | P2
0
otherwise
(b) P2
u→η2 and η(P) =
 η2(P ′
2) if P = P1 | P ′
2
0
otherwise
(c) u = τ and there exists a ∈A with P1
a→η1 and P2
¯a→η2 such that
η(P) =
 η1(P ′
1) · η2(P ′
2) if P = P ′
1 | P ′
2
0
otherwise
(d) the symmetric case of (c)
4. P \ A
u→η if u ̸∈A ∪A, P
u→η1, and η(P) =
 η1(P ′) if P = P ′ \ A
0
otherwise
5. f[P]
u→η if P
v→η1, f(v) = u and η(P) =
 η1(P ′) if P = f[P ′]
0
otherwise
6. μXE
u→η if E{μXE/X}
u→η
Fig. 1. Operational semantics for Probabilistic CCS

70
Y. Deng, J. Pang, and P. Wu
3
Measuring Anonymity
3.1
Relative Entropy
We make a convention 0 log ∞= 0.
Deﬁnition 2 (Relative entropy [17]). Let θ, θ′ be two discrete probability
distributions on a set S. The relative entropy of θ w.r.t. θ′ is deﬁned by
D(θ, θ′) =

s∈S
θ(s) · log θ(s)
θ′(s).
In the sequel, whenever we write D(θ, θ′), it is implicitly assumed that the do-
mains of θ and θ′ are the same, i.e., dom(θ) = dom(θ′).
In general, we have D(θ, θ′) ̸= D(θ′, θ), so relative entropy is not a true
metric. But it satisﬁes many important mathematical properties, e.g. it is always
nonnegative, and equals zero only if θ = θ′. It plays an important role in quantum
information theory, as well as statistical mechanics.
We now present a few properties of relative entropy.
Proposition 3. Relative entropy D has the following properties:
1. (Nonnegativity) D(η, η′) ≥0, with D(η, η′) = 0 if and only if η = η′;
2. (Possibility of extension) D(η1, η2) = D(η′
1, η′
2) where dom(η′
1) = dom(η) ∪
{s} and η′
1(s) = 0, similarly for η′
2 w.r.t. η2;
3. (Additivity) D(η1×η2, η′
1×η′
2) = D(η1, η′
1)+D(η2, η′
2), where (η1×η2)(s1, s2)
is deﬁned as η1(s1) · η2(s2);
4. (Joint convexity) For 0 ≤r ≤1, we have
D(rη1 + (1 −r)η2, rη′
1 + (1 −r)η′
2) ≤rD(η1, η′
1) + (1 −r)D(η2, η′
2).
5. (Strong additivity) Let dom(η1) = dom(η′
1) = S∪{s}, dom(η2) = dom(η′
2) =
S ∪{s1, s2} with η1(s) = η2(s1) + η2(s2) and η′
1(s) = η′
2(s1) + η′
2(s2). Then
it holds that D(η1, η′
1) ≤D(η2, η′
2).
Proof. Similar properties for Tsallis relative entropy have been proved in [11];
their proofs can be adapted for relative entropy.
⊓⊔
We extend D to sets of distributions by using Hausdorﬀdistance.
Deﬁnition 4. Given two sets of discrete probability distributions Θ = {θi}i∈I
and Θ′ = {ρj}j∈J, the relative entropy of Θ w.r.t. Θ′ is deﬁned by
D(Θ, Θ′) = supi∈Iinfj∈JD(θi, ρj)
where inf ∅= ∞and sup ∅= 0.

Measuring Anonymity with Relative Entropy
71
3.2
Anonymity Systems
The concept of anonymity is relative to a certain set of anonymous actions,
which we denote by A. Note that the actions in A normally depend on the
identity of users, and thus are not visible to the observer. However, for the
purpose of deﬁning and verifying anonymity we model the elements of A as
visible outcomes of the system. We write FA for the set of all renaming functions
that are permutations on A and identity elsewhere.
The idea of measuring anonymity is to consider a fully probabilistic automaton
(resp. a probabilistic automaton) M as a trace distribution (resp. a set of trace
distributions) tds(M), and then apply the distance deﬁned in Deﬁnition 2 (resp.
Deﬁnition 4). The interpretation of a probabilistic automaton as a set of trace
distributions is given in Section 2.2. Usually we ﬁnd it convenient to describe a
system as a process in the probabilistic CCS. To measure the distance between
two processes, we just view a process P as its corresponding automaton pa(P)
and simply write D(P, Q) for the distance between the set of trace distributions
represented by tds(pa(P)) and that represented by tds(pa(Q)).
Deﬁnition 5 (α-anonymity1). Given α ∈[0, 1], a process P is α-anonymous
on a set of actions A if
∀f ∈FA : D(P, f[P]) ≤α
In the particular case α = 0, we say P is strongly anonymous or P provides
strong anonymity.
In [23] Schneider and Sidiropoulos consider a process as a set of traces, thus P
is strongly anonymous if f[P], the process after the permutation of anonymous
actions, represents the same set of traces as that of P. The non-determinism plays
a crucial role in their formalism. A system is anonymous if the set of the possible
outcomes is saturated with respect to the intended anonymous users, i.e. if one
such user can cause a certain observable trace in one possible computation, then
there must be alternative computations in which each other anonymous user can
give rise to the same observable trace (modulo the identity of the anonymous
users). In our case, P is strongly anonymous if P and f[P] represent the same
set of trace distributions. Thus, we extend their deﬁnition to the probabilistic
setting in a natural way. We deﬁne DA(P) as max{D(P, f[P]) | f ∈FA}. Thus,
P is α-anonymous if and only if DA(P) ≤α.
Proposition 6 (Non-expansiveness). All the operators of the probabilistic
CCS except for parallel composition are non-expansive.
1. DA(
i∈I upi.Pi) ≤
i∈I piDA(Pi) if u ̸∈A;
2. DA(P1 ⊞P2) ≤max{DA(P1), DA(P2)};
1 The notion of α-anonymity already appeared in [6] to describe weak probabilistic
anonymity, but the measuring method used here is diﬀerent and no explicit notion
of schedulers is considered.

72
Y. Deng, J. Pang, and P. Wu
3. DA(P \ B) ≤DA(P) if A ∩B = ∅;
4. DA(f[P]) ≤DA(P) if f(a) = a for all a ∈A;
5. DA(μXE) = DA(E{μXE/X}).
Proof. We sketch the proof for each clause.
1. Given any f ∈FA, we show that for each η ∈tds(pa(
i∈I upi.Pi)) there ex-
ists some η′ ∈tds(pa(
i∈I upi.f[Pi])) such that D(η, η′) ≤
i∈I piDA(Pi).
Note that η is determined by a scheduler σ. Restricting σ to pa(Pi), for each
i ∈I, we have a scheduler σi that resolves all non-deterministic choices in
Pi, resulting in a trace distribution ηi ∈tds(pa(Pi)). It is easy to see that
η(Cu) = 1 and η(Cuγ) = 
i∈I pi · ηi(Cγ)
for any trace γ. Observe that, as a graph, pa(
i∈I upi.f[Pi]) is isomor-
phic to pa(
i∈I upi.Pi). Hence there is a scheduler σ′ of 
i∈I upi.f[Pi]
that resolves all non-deterministic choices in the same way as σ does for

i∈I upi.Pi. It follows that each scheduler σi also has a counterpart σ′
i that
is a scheduler of f[Pi], for each i ∈I. Each σ′
i determines a trace distribution
η′
i ∈tds(pa(f[Pi])) satisfying
η′(Cu) = 1 and η′(Cuγ) = 
i∈I pi · η′
i(Cγ).
for some η′ ∈tds(pa(
i∈I upi.f[Pi])). Therefore, it holds that
D(η, η′) = D(

i∈I
piηi,

i∈I
piη′
i) ≤

i∈I
piD(ηi, η′
i) ≤

i∈I
piDA(Pi).
The ﬁrst inequality above is justiﬁed by the joint convexity property of
relative entropy given in Proposition 3.
2. Given any f ∈FA, we let Θ = tds(pa(P1 ⊞P2)) and Θ′ = tds(pa(f[P1] ⊞
f[P2])). Each η ∈Θ is determined by a scheduler σ. We consider the inter-
esting case in which σ chooses an outgoing transition from Pi, for i = 1, 2.
It follows from the isomorphism between pa(P1 ⊞P2) and pa(f[P1] ⊞f[P2])
that σ has a counterpart σ′ which chooses an outgoing transition from f[Pi],
and which determines a trace distribution η′ ∈Θ′ satisfying
D(η, η′) ≤DA(Pi) ≤max{DA(P1), DA(P2)}.
3. Note that pa(P \ B) is the same as pa(P) except that all transitions labelled
with actions in B are blocked. If η1 ∈tds(pa(P \ B)), there is some η2 ∈
tds(pa(P)) such that all probabilities assigned by η2 to maximal traces of
the form γaγ′ with a ∈B are now assigned by η1 to γ. Similar relation holds
between the peer trace distribution η′
1 ∈tds(pa(f[P]\B)) of η1 and the peer
trace distribution η′
2 ∈tds(pa(f[P])) of η2, for any f ∈FA. By the strong
additivity property given in Proposition 3, we derive that
D(η1, η′
1) ≤D(η2, η′
2) ≤DA(P).

Measuring Anonymity with Relative Entropy
73
4. If f is an injective renaming function, i.e., a ̸= b implies f(a) ̸= f(b), then it
is immediate that DA(f[P]) = DA(P). Otherwise, two diﬀerent actions may
be renamed into the same one. As a result, two diﬀerent maximal traces in P
may become the same in f[P]. We can then appeal to the strong additivity
property of relative entropy to infer that DA(f[P]) ≤DA(P).
5. The result follows from the fact that μXE and E{μXE/X} have the same
transition graph.
⊓⊔
The above proposition shows a nice property of our approach using relative
entropy to measure anonymity. The non-expansiveness of the operators in the
probabilistic CCS allows us to estimate the degree of anonymity of a complex
system from its components, rather than analyse the system as a whole.
Remark 7. Unfortunately, the parallel composition operator is expansive. For
example, let A = {b, c}, P = a 1
3 .b + a 2
3 .c and Q = ¯a 1
3 .b + ¯a 2
3 .c. We have
DA(P) = DA(Q) = 1
3 log
1
3
2
3
+ 2
3 log
2
3
1
3
= 1
3.
However, (P | Q) \ a = τ 1
9 .(b | b) + τ 2
9 .(b | c) + τ 2
9 .(c | b) + τ 4
9 .(c | c) and
DA((P | Q) \ a) = 1
9 log
1
9
4
9
+ 2
9 log
2
9
2
9
+ 2
9 log
2
9
2
9
+ 4
9 log
4
9
1
9
= 2
3.
It follows from Proposition 6 that DA(P | Q) ≥DA((P | Q) \ a) = 2
3.
3.3
Small Examples
We present some toy examples to show the basic ideas of our approach.
Example 8. Consider a communication system that provides anonymous email
with 2 potential senders, a mix network and a recipient. The attacker wants to
ﬁnd out which sender sent an email to the recipient. By means of traﬃc analysis,
the attacker obtains a communication system described by the process P.
P = τp.sender(0).email.receive.0 + τ1−p.sender(1).email.receive.0
The senders require anonymity, i.e., anonymity is required for the set A =
{sender(0), sender(1)}. In this case, FA is a singleton set {f} with f[P] tak-
ing the form:
f[P] = τ1−p.sender(0).email.receive.0 + τp.sender(1).email.receive.0
It is easy to see that DA(P) = p log
p
1−p + (1 −p) log 1−p
p . If p =
1
2, the
attacker cannot distinguish the two senders, and indeed the system provides
strong anonymity. If p →0 or p →1, we have DA(P) = +∞, which means that
the system does not ensure any anonymity of the senders.

74
Y. Deng, J. Pang, and P. Wu
Example 9. Now suppose the actual system in Example 8 has a built-in non-
determinism and behaves in a way described by the process Q.
Q = (τ 1
3 .sender(0).email.receive.0 + τ 2
3 sender(1).email.receive.0) ⊞
(τ 2
3 .sender(0).email.receive.0 + τ 1
3 .sender(1).email.receive.0)
We observe that f[Q] = Q for f ∈FA, thus DA(Q) = 0 and the system provides
strong anonymity.
4
The Dining Cryptographers
The general Dining Cryptographers Problem [5] is described as follows: A num-
ber of cryptographers sitting around a table are having dinner. The representa-
tive of their organisation (master) may or may not pay the bill of the dinner.
If he does not, then he will select exactly one cryptographer and order him to
pay the bill. The master will tell secretly each cryptographer whether he has to
pay or not. The cryptographers would like to reveal whether the bill is paid by
the master or by one of them, but without knowing who among them, if any, is
paying. In this paper we consider a DCP with three cryptographers connected
by a ring. It is not diﬃcult to extend it to the general case.
A possible solution to this problem, as described in [5], is to associate a coin
to every two neighbouring cryptographers. The result of each coin-tossing is
only visible to the adjacent cryptographers. Each cryptographer examines the
two adjacent coins: If he is not paying, he announces “agree” if the results are
the same, and “disagree” otherwise. If he is paying, he says the opposite. If the
number of “disagree” is even, then the master is paying. Otherwise, one of the
cryptographers is paying.
4.1
Fully Probabilistic Users
We consider the case in which the master probabilistically select one cryptog-
rapher to pay. We formalise the DCP as a process in the probabilistic CCS, as
illustrated in Figure 22, where Π is the parallel composition. We use ⊕(resp. ⊖)
to represent the sum (resp. the subtraction) modulo 3. Messages p and n are the
instructions sent by the master, requiring each cryptographer to pay or not to
pay, respectively. The set of anonymous actions is A = {pay(i) | i = 0, 1, 2}. The
restriction operator \ over the action sequences −→c and −→
m enforces these actions
into internal communications. The traces of DCP are in the form of pay(i)xyz
with i ∈{0, 1, 2} and x, y, z ∈{a, d} (a for “agree” and d for “disagree”).
FA contains two elements, one renames pay(i) according to the permutation
f1 = {0 →1, 1 →2, 2 →0} and the other f2 = {0 →2, 1 →0, 2 →1}.
We assume that all the coins are uniform. With a probabilistic master,
tds(pa(DCP)) contains only one trace distribution. Each maximal trace of DCP
can only contain one of the following sequences: ddd, aad, ada, and daa.
2 For the sake of brevity, we formalise the DCP in a value-passing version of the
probabilistic CCS, which can be encoded into the probabilistic CCS in the standard
way [19]; incorporating the “if-then-else” construct is also straightforward.

Measuring Anonymity with Relative Entropy
75
Master = 2
i=0 τpi. mi(p). mi⊕1(n). mi⊕2(n). 0
Coini = τph. Head i + τpt. Taili
Head i = ci,i(head). ci⊖1,i(head). 0
Taili
= ci,i(tail). ci⊖1,i(tail). 0
Crypt i = mi(x). ci,i(y). ci,i⊕1(z).
if x = p then pay i.
if y = z then outi(disagree) else outi(agree)
else if y = z then outi(agree) else outi(disagree)
DCP
= (Master | (Π2
i=0Crypti | Π2
i=0Coini) \ −→c ) \ −→
m
Fig. 2. Speciﬁcation of the DCP in the probabilistic CCS
Fair coins. With fair coins, if the master assigns the initial probabilities p0 = 1
3,
p1 = 1
3 and p2 = 1
3, i.e., each cryptographer has an equal chance to pay, then it is
easy to see that f1[DCP] = DCP and f2[DCP] = DCP. Therefore, DA(DCP) = 0
and the DCP provides strong anonymity.
If the master assigns the initial probabilities p0 = 1
2, p1 = 1
3 and p2 = 1
6. The
probabilities of traces with ddd, aad, ada, and daa are all 1
4. By the deﬁnition
of D, we can check that
D(DCP, f1[DCP]) = 0.431
and
D(DCP, f2[DCP]) = 0.362
Hence, the degree of anonymity of such DCP is 0.431.
Table 1. A trace distribution (with biased coins: ph = 2
5)
crypt0 pays (i = 0)
crypt1 pays (i = 1)
crypt2 pays (i = 2)
p(pay(i)ddd)
0.120
0.080
0.040
p(pay(i)aad)
0.120
0.080
0.047
p(pay(i)ada)
0.120
0.093
0.040
p(pay(i)daa)
0.140
0.080
0.040
Biased coins. We assume the coins are biased, e.g. ph = 2
5. We also consider the
case that p0 = 1
2, p1 = 1
3 and p2 = 1
6. Then the probabilities of traces can be
calculated as in Table 1. We have
D(DCP, f1[DCP]) = 0.209
and
D(DCP, f2[DCP]) = 0.878
Hence, the degree of anonymity of such DCP is 0.878, which is greater than
0.431. Therefore, the biased coins leak more information to the attacker than
fair coins. If ph →1.0, then D(DCP, f1[DCP]) = D(DCP, f2[DCP]) = +∞.
Hence, the degree of anonymity of such DCP is +∞, in such case the DCP does
not provide any anonymity.

76
Y. Deng, J. Pang, and P. Wu
Table 2. Three trace distributions (with biased coins: ph = 2
5)
crypt0 pays (i = 0)
crypt1 pays (i = 1)
crypt2 pays (i = 2)
p(pay(i)ddd)
0.24
0.24
0.24
p(pay(i)aad)
0.24
0.24
0.28
p(pay(i)ada)
0.24
0.28
0.24
p(pay(i)daa)
0.28
0.24
0.24
4.2
Non-deterministic Users
We now consider the case in which the master non-deterministically choose a
cryptographer to pay, i.e., the master is of the form
Master = m0(p). m1(n). m2(n). 0 ⊞m0(n). m1(p). m2(n). 0 ⊞
m0(n). m1(n). m2(p). 0
Fair coins. With fair coins, it is easy to see that f1[DCP] = DCP and f2[DCP] =
DCP, i.e., tds(pa(f1[DCP])) and tds(pa(f2[DCP])) represent the same set of
trace distributions as tds(pa(DCP)). Therefore, DA(DCP) = 0 and the DCP
provides strong anonymity.
Biased coins. We assume the coins are biased, e.g. ph = 2
5. Then tds(pa(DCP))
contains the three trace distributions shown in the last three columns of Table 2.
It can then be checked that
D(DCP, f1[DCP]) = D(DCP, f2[DCP]) = 0.009
Hence, the degree of anonymity of such DCP is 0.009
Remark 10. The master of a DCP models the a priori knowledge of the attacker.
In the particular case that the master is purely non-deterministic, the attacker
has no a priori knowledge of the users. The attacker simply assumes that there is
a uniform probability distribution among the users, we then get an ideal situation
of anonymity similar to that considered in [9].
5
Related Work
In his seminal paper, Chaum [5] used the size of an anonymity set to indicate
the degree of anonymity provided by a DC network. An anonymity set is deﬁned
as the set of participants who could have sent a particular message as observed
by the attacker. Berthold et al. [2] deﬁned the degree of anonymity as ln(N),
where N is the number of users of the protocols. Both [5] and [2] only deal with
non-deterministic cases, and do not consider the probabilistic information of the
users the attacker can gain by observing the system.
Reiter and Rubin [22] deﬁned the degree of anonymity as 1−p, where p is the
probability assigned to a particular user by the attacker. Halpern and O’Neill

Measuring Anonymity with Relative Entropy
77
have proposed in [13] several notions of probabilistic anonymity. Their basic
notion is formulated as a requirement on the knowledge of the attacker about
the probability of the user. They have given both strong and weak version of
this notion, proposing a formal interpretation of the three levels of the hierarchy
proposed in [22]. Deng, Palamidessi and Pang [6] proposed a weak notion of
probabilistic anonymity as an extension of [3] to measure the leaked information,
which can be used by an attacker to infer the likeliness that the action has
been performed by a certain user. Thus, the degree of anonymity is formalised
as an factor by which the probability the attacker attributes to a user as the
performer of the anonymous action has increased, after observing the system.
All these methods focus on the probability of the users. Thus, they do not give
any information on how distinguishable the user is within the anonymity set.
Serjantov and Danezis [25] and Claudia et al. [9] independently proposed an
information theoretic metric based on the idea of measuring probability distri-
butions. They used entropy to deﬁne the quality of anonymity and to compare
diﬀerent anonymity systems. Compared to [9], [25] does not normalise the de-
gree in order to get a value relative to the anonymity level of the ideal system
for the same number of users. Both [25] and [9] take into account the probabil-
ities of the users performing certain actions which are assigned by an attacker
after observing the system. However, they do not take into account the a priori
information that the attacker might have. The attacker simply assumes a uni-
form distribution among the users before observation. Our method uses relative
entropy, and it quantiﬁes the amount of probabilistic information revealed by
the protocol, i.e. how much information an attacker can achieve after observing
the outcomes of the protocol, together with the information he has before the
protocol running. Furthermore, we extend the measuring method to two sets of
probability distributions using Hausdorﬀdistance for protocols containing both
non-deterministic and probabilistic behaviours.
Moskowitz et al. [20] proposed to use a related notion of mutual information
to measure the capacity of covert channels. They have applied it to the analysis
of a wide range of Mix-networks [21]. Recently, Chatzikokolakis, Palamidessi
and Panangaden [4] developed a framework in which anonymity protocols can
be interpreted as noisy channels. They also used it to express various notions of
anonymity. Our work is still diﬀerent from them in the sense that we use relative
entropy instead of mutual information, and we focus on the non-expansiveness of
the operators of the probabilistic CCS, which potentially allows for compositional
analysis.
6
Conclusion and Future Work
In this paper, we have proposed to use relative entropy as a distance of two dis-
crete probability distributions to measure anonymity for protocols which can be
interpreted as a fully probabilistic automaton. This deﬁnition has been extended
for two sets of probability distributions to also capture the non-deterministic as-
pect of these protocols. We have proved that based on this measuring method,

78
Y. Deng, J. Pang, and P. Wu
most of the operators in the probabilistic CCS are non-expansive. We have
demonstrated our approach by using the example of the Dining Cryptographers
Problem.
Model checking anonymity protocols in the logic of knowledge was considered
in [28]. It would also be interesting to investigate the problem in a probabilis-
tic setting. The probabilistic model checker PRISM [18] was used to ﬁnd novel
attacks on Crowds [26], and to analyse cascase networks [10]. We intend to
integrate our method with PRISM to build an automatic tool to assist the cal-
culation of the degree of anonymity as deﬁned in the paper. We also plan to
apply our approach to more complex and real protocols to justify its usefulness.
Acknowledgments. We are very grateful to Kostas Chatzikokolakis, Catuscia
Palamidessi, and Prakash Panangaden. From discussions with them we learnt
more about relative entropy, which helped us to improve the paper a lot.
References
1. Baier, C., Kwiatkowaska, M.Z.: Domain equations for probabilistic processes.
Mathematical Structures in Computer Science 10(6), 665–717 (2004)
2. Berthold, O., Pﬁztmann, A., Standtke, R.: The disavantages of free mix routes
and how to overcome them. In: Federrath, H. (ed.) Designing Privacy Enhancing
Technologies. LNCS, vol. 2009, pp. 30–45. Springer, Heidelberg (2001)
3. Bhargava, M., Palamidessi, C.: Probabilistic anonymity. In: Abadi, M., de Al-
faro, L. (eds.) CONCUR 2005. LNCS, vol. 3653, pp. 171–185. Springer, Heidelberg
(2005)
4. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: Anonymity protocols as
noisy channels. In: Proc. 2nd Symposium on Trustworthy Global Computing.
LNCS, Springer, Heidelberg (2006) (to appear)
5. Chaum, D.: The dining cryptographers problem: Unconditional sender and recipi-
ent untraceability. Journal of Cryptology 1, 65–75 (1988)
6. Deng, Y., Palamidessi, C., Pang, J.: Weak probabilistic anonymity. In: Proc. 3rd
Workshop on Security Issues in Concurrency, ENTCS (2006) (to appear)
7. Desharnais, J., Jagadeesan, R., Gupta, V., Panangaden, P.: The metric analogue
of weak bisimulation for probabilistic processes. In: Proc. 17th IEEE Symposium
on Logic in Computer Science, pp. 413–422. IEEE Computer Society, Los Alamitos
(2002)
8. Desharnais, J., Jagadeesan, R., Gupta, V., Panangaden, P.: Metrics for labelled
Markov processes. Theoretical Computer Science 318(3), 323–354 (2004)
9. D´ıaz, C., Seys, S., Claessens, J., Preneel, B.: Towards measuring anonymity. In:
Dingledine, R., Syverson, P.F. (eds.) PET 2002. LNCS, vol. 2482, pp. 54–68.
Springer, Heidelberg (2003)
10. Dingledine, R., Shmatikov, V., Syverson, P.F.: Synchronous batching: From cas-
cades to free routes. In: Martin, D., Serjantov, A. (eds.) PET 2004. LNCS, vol. 3424,
pp. 186–206. Springer, Heidelberg (2005)
11. Furuichi, S., Yanagi, K., Kuriyama, K.: Fundamental properties of Tsallis relative
entropy. Journal of Mathematical Physics 45(12), 4868–4877 (2004)
12. Garcia, F.D., Hasuo, I., Pieters, W., van Rossum, P.: Provable anonymity. In:
Proc.3rd ACM Workshop on Formal Methods in Security Engineering, pp. 63–72.
ACM Press, New York (2005)

Measuring Anonymity with Relative Entropy
79
13. Halpern, J.Y., O’Neill, K.R.: Anonymity and information hiding in multiagent
systems. In: Proc. 16th IEEE Computer Security Foundations Workshop. IEEE
Computer Society, Los Alamitos (2003)
14. Hoare, C.A.R.: Communicating Sequential Processes. Prentice Hall, Englewood
Cliﬀs (1985)
15. Hughes, D., Shmatikov, V.: Information hiding, anonymity and privacy: a modular
approach. Journal of Computer Security 12(1), 3–36 (2004)
16. Kesdogan, D., Egner, J.: Stop-and-go MIXes: Providing probabilistic anonymity
in an open system. In: Aucsmith, D. (ed.) IH 1998. LNCS, vol. 1525, pp. 83–98.
Springer, Heidelberg (1998)
17. Kullback, S., Leibler, R.A.: On information and suﬃciency. Annals of Mathematical
Statistics 22(1), 79–86 (1951)
18. Kwiatkowska, M., Norman, G., Parker, D.: PRISM: Probabilistic symbolic model
checker. In: Field, T., Harrison, P.G., Bradley, J., Harder, U. (eds.) TOOLS 2002.
LNCS, vol. 2324, pp. 200–204. Springer, Heidelberg (2002)
19. Milner, R.: Communication and Concurrency. Prentice-Hall, Englewood Cliﬀs
(1989)
20. Moskowitz, I.S., Newman, R.E., Crepeau, D.P., Miller, A.R.: Covert channels and
anonymizing networks. In: Proceedings of 2nd ACM Workshop on Privacy in the
Electronic Society, pp. 79–88. ACM Press, New York (2003)
21. Newman, R.E., Nalla, V.R., Moskowitz, I.S.: Anonymity and covert channels in
simple timed mix-ﬁrewalls. In: Martin, D., Serjantov, A. (eds.) PET 2004. LNCS,
vol. 3424, pp. 1–16. Springer, Heidelberg (2005)
22. Reiter, M.K., Rubin, A.D.: Crowds: Anonymity for Web transactions. ACM Trans-
actions on Information and System Security 1(1), 66–92 (1998)
23. Schneider, S., Sidiropoulos, A.: CSP and anonymity. In: Martella, G., Kurth,
H., Montolivo, E., Bertino, E. (eds.) Computer Security - ESORICS 96. LNCS,
vol. 1146, pp. 198–218. Springer, Heidelberg (1996)
24. Segala, R.: Modeling and Veriﬁcation of Randomized Distributed Real-Time Sys-
tems. PhD thesis, MIT, Deptartment of EECS (1995)
25. Serjantov, A., Danezis, G.: Towards an information theoretic metric for anonymity.
In: Dingledine, R., Syverson, P.F. (eds.) PET 2002. LNCS, vol. 2482, pp. 41–53.
Springer, Heidelberg (2003)
26. Shmatikov, V.: Probabilistic model checking of an anonymity system. Journal of
Computer Security 12(3/4), 355–377 (2004)
27. Syverson, P.F., Goldschlag, D.M., Reed, M.G.: Anonymous connections and onion
routing. In: Proc. 18th IEEE Symposium on Security and Privacy, pp. 44–54. IEEE
Computer Society Press, Los Alamitos (1997)
28. van der Meyden, R., Su, K.: Symbolic model checking the knowledge of the dining
cryptographers. In: Proc. 17th IEEE Computer Security Foundations Workshop,
pp. 280–291. IEEE Computer Society Press, Los Alamitos (2004)

Formalizing and Analyzing Sender Invariance⋆
Paul Hankes Drielsma, Sebastian M¨odersheim, Luca Vigan`o, and David Basin
Information Security Group, Dep. of Computer Science, ETH Zurich, Switzerland
{drielsma,moedersheim,vigano,basin}@inf.ethz.ch
www.infsec.ethz.ch/~{drielsma,moedersheim,vigano,basin}
Abstract. In many network applications and services, agents that share
no secure channel in advance may still wish to communicate securely
with each other. In such settings, one often settles for achieving security
goals weaker than authentication, such as sender invariance. Informally,
sender invariance means that all messages that seem to come from the
same source actually do, where the source can perhaps only be identiﬁed
by a pseudonym. This implies, in particular, that the relevant parts of
messages cannot be modiﬁed by an intruder.
In this paper, we provide the ﬁrst formal deﬁnition of sender invari-
ance as well as a stronger security goal that we call strong sender invari-
ance. We show that both kinds of sender invariance are closely related
to, and entailed by, weak authentication, the primary diﬀerence being
that sender invariance is designed for the context where agents can only
be identiﬁed pseudonymously. In addition to clarifying how sender in-
variance and authentication are related, this result shows how a broad
class of automated tools can be used for the analysis of sender invari-
ance protocols. As a case study, we describe the analysis of two sender
invariance protocols using the OFMC back-end of the AVISPA Tool.
1
Introduction
The establishment of a secure channel between communicating parties requires a
pre-existing relationship between them. Examples of such relationships include
shared passwords and transitive relationships, for instance where the parties
exchange public-key certiﬁcates issued by a trusted certiﬁcation authority. Com-
mon to these types of relationships is the prerequisite that some data is available
in order to bootstrap the secure channel. This data may have been exchanged in
advance, or it might be produced on the ﬂy by a reliable source like a certiﬁcation
authority.
In many network applications and services, however, agents that share no
such bootstrap data in advance may still wish to communicate securely with
each other. Indeed, they might have no prior relationship whatsoever, not even
⋆This work was partially supported by the National Competence Center in Research
on Mobile Information and Communication Systems (NCCR-MICS), a center sup-
ported by the Swiss National Science Foundation under grant number 5005-67322,
and by the Zurich Information Security Center. It represents the views of the authors.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 80–95, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Formalizing and Analyzing Sender Invariance
81
a transitive one. In such situations, classical authentication (e.g. via public keys)
is impossible. To remedy this problem, one would have to deﬁne a process by
which bootstrap data is established and distributed, but in many settings this is
too expensive or cumbersome, as one must require that every participant must
somehow “register” before using a service. For a small coﬀee shop oﬀering its
customers a wireless hotspot, such a registration process could detract from one
of its main selling points: convenience.
In settings where no bootstrap data is available, a weaker security goal is
still achievable, namely sender invariance. Informally, sender invariance means
that all messages that seem to come from the same source actually do, where the
source can perhaps only be identiﬁed by a pseudonym. This implies, in particular,
that the messages cannot be modiﬁed by an intruder, at least not their relevant
parts. Moreover, one may want to ensure the secrecy of the communicated data
between the participants.
Sender invariance arises in a variety of situations, both wired and wireless.
Consider, for instance, any of the many free web-based e-mail services available
online. In general, users register for an online e-mail service via an informal pro-
cess whereby a new username and password are established. At no point does
a formal authentication process take place involving, for example, photo identi-
ﬁcation or a physical signature. This process is acceptable, as an email address
can be seen as a pseudonym that is not necessarily linkable with its owner’s true
identity. This, however, has ramiﬁcations concerning how one should describe the
login process, as there is no reliable means of linking the established username
with the identity of the user. If one considers the login process and ignores regis-
tration, simply assuming that credentials have been exchanged sometime in the
past, then the user login process can be called authentication. However, in light
of this informal registration process, the login process should more accurately be
described as ensuring sender invariance: that is, the user with pseudonym John
Doe is the same user who originally opened the account registered as John Doe,
although the e-mail provider does not know his proper identity.
Such online services, whose users can be logged in based only on creden-
tials which are not linkable to their actual identities, are already prevalent. As
networks move towards increased mobility and ad-hoc connections, situations
in which reliable pre-shared cryptographic credentials are limited or unavail-
able will arise with increasing frequency. Understanding what security goals can
be achieved in such situations is important for the design of next-generation
network protocols. In this paper, we aim to further this understanding by exam-
ining sender invariance in detail.
Contributions: Our ﬁrst contribution is a formal deﬁnition of sender invariance
and a stronger, related security goal which we call strong sender invariance. We
also show that (strong) sender invariance is closely related to weak authentica-
tion, the primary diﬀerence being that sender invariance assumes that agents
can only be identiﬁed pseudonymously. Based on this, we show that the three
security goals constitute a hierarchy. Furthermore, we show how a broad class of
automated analysis tools can be used to analyze sender invariance protocols. We

82
P. Hankes Drielsma et al.
also describe the analysis of two protocols using the On-the-Fly Model Checker
OFMC [7], one of the back-ends of the AVISPA Tool for security protocol
analysis [3].
Related Work: Our work focuses on the formal deﬁnition security goals and the
relationships between them. Gollmann [14] considered authentication in detail,
and Lowe [16] subsequently deﬁned a hierarchy of authentication goals which
apply in settings where relationships between agents have been established in
advance. In §2.5, we similarly deﬁne a hierarchy that relates the two forms of
sender invariance and weak authentication. Our focus, however, is on settings in
which the kinds of relationships that one would normally require to bootstrap a
secure channel are not available.
Our motivation to examine settings where agents know each other perhaps
only via pseudonyms was inspired by current trends in protocol development, in
particular the work of the Internet Engineering Task Force on Mobile IPv6 [15].
In [17], for instance, the authors identify sender invariance as a goal that should
be ensured by the IPv6 SEcure Neighbor Discovery protocol (SEND [2]) in ad
hoc networks.1 In SEND, this is achieved via a mechanism for providing sender
invariance called Cryptographically Generated Addresses (CGA [4]). In this pa-
per, we consider a similar idea, the Purpose-Built Keys Framework [10].
Organization: In §2, we deﬁne and discuss sender invariance and strong sender
invariance. In §3, we present a case study based on the Purpose-Built Keys
Framework that illustrates the formal analysis of sender invariance with the
OFMC tool. In §4, we discuss settings in which agents share some bootstrap
data, but not enough to achieve mutual authentication. In §5, we summarize our
results and discuss future work.
2
Sender Invariance
Designers of modern security protocols face a challenge. On the one hand, pro-
tocols need to be designed with ever-increasing mobility in mind. On the other
hand, this very mobility means that designers should also make few assump-
tions about the amount of information shared, in advance, among protocol par-
ticipants; indeed, one must often assume that participants share no a priori
relationships at all. Yet authentication protocols tend to rely on just such pre-
shared information, such as a public key or a shared password. Indeed, in [9],
Boyd argues that in the absence of authenticated shared information, no secure
channels can be established.
Sender invariance protocols are based on the idea that, in many situations,
one party of a protocol does not need to be authenticated in the classical sense,
1 Note that the authors do not actually call the goal sender invariance, but merely
describe the intuition: “nodes ensure that they are talking to the same nodes (as
before)” [17, §3.3].

Formalizing and Analyzing Sender Invariance
83
but rather could pick a pseudonym and be identiﬁed by that pseudonym there-
after. The protocols ensure that an intruder cannot “take over” somebody else’s
pseudonym, i.e. generate messages that appear to originate from the owner of
the pseudonym, or read messages that are sent to the owner of the pseudonym.
A variety of mechanisms can be used to realize sender invariance. Perhaps the
most common one, and the one used in our running example PBK, is as follows.
An agent creates an asymmetric key pair, publishes the public key, and uses a
hash value of the public key as a pseudonym. Clearly, the intruder can generate
his own pseudonym, but he cannot sign or decrypt messages with the private key
associated with somebody else’s pseudonym. The remarkable thing about these
mechanisms is thus that we get—out of nothing—variants of authentic channels
that only diﬀer from the classical ones by the fact that one end point is identiﬁed
by a pseudonym.
The goals that are considered for sender invariance protocols are thus similar
to classical authentication and secrecy goals, but with the twist that one side is
identiﬁed by a pseudonym rather than a real name. By sender invariance, we
informally mean that all messages come from the same source that is identiﬁed
by a pseudonym:
A two-party protocol P guarantees the responder role sender invariance
with respect to the initiator role iﬀthe following holds: whenever an
agent b in the responder role receives a message that appears to have
been sent by an agent with pseudonym id, then this message originates
from the same agent playing the initiator role as all previous messages
that appeared to come from pseudonym id.
Note that sender invariance diﬀers in several respects from privacy (for in-
stance, the privacy properties deﬁned in [1]). Privacy means to protect the iden-
tities of the communicating agents from being observable (to an outstanding
party or even to each other); for sender invariance, the protection of identities
is not an issue (and agents may expose their identities, even if they cannot
prove them). Sender invariance is rather the best we can achieve when identiﬁ-
cation/authentication is not possible.
The relation of this goal with classical authentication will be discussed shortly.
We note that one may similarly develop a concept of receiver invariance as an
analogue of secrecy goals in this pseudonym-based communication; we do not,
however, consider this further in this paper.
2.1
Purpose-Built Keys
As a running example, we introduce a protocol based on the Purpose-Built Keys
Framework (PBK [10]), a mechanism for achieving sender invariance. PBK uses
freshly generated, temporary, asymmetric key pairs. A user’s pseudonym is sim-
ply a hash of the temporary public key, the so-called PBID. In an initialization
phase, the sender agent transmits his purpose-built public key. If this exchange is
not tampered with, then the sender can sign subsequent messages, thus assuring
the receiver that the source of the messages has not changed.

84
P. Hankes Drielsma et al.
1. A →B : PBKA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2. A →B : {Msg}PBKA−1.H(PBKA)
3. B →A : NB.H(PBKA)
4. A →B : {NB}PBKA−1.H(PBKA)
Protocol 1. An example PBK protocol
We note that denial of service attacks are possible, in the sense that the
intruder can drop messages from an honest initiator. We do not consider such
attacks here, however, as they do not constitute violations of sender invariance.
Example. Protocol 1 is an example protocol which uses PBK to ensure sender
invariance between an initiator A and a responder B. Upon starting the protocol,
A generates her purpose-built key pair PBKA and PBKA
−1. She sends the former
to B in message 1. The dotted line separates the initialization phase from the
rest of the protocol. In message 2, A sends some payload Msg to B signed with
her PBK. Messages 3 and 4 perform a challenge-response exchange in order to
prove to B that the party purporting to possess PBKA
−1 is indeed active and the
signed messages are not simply being replayed. We assume that A and B might
want to exchange multiple payload messages with the pseudonym H(PBKA), so
messages 2 through 4 might be repeated arbitrarily often.
The running example of Protocol 1 will serve as a basis for the discussions below,
where we describe our model and deﬁne sender invariance formally.
2.2
Formalizing Sender Invariance
The informal deﬁnition given above is meant to provide the intuition behind the
goal of sender invariance: namely, that a sequence of messages that apparently
all originate from the same sender truly do. Note that we do not assume that
the agent playing the responder role knows the real identity of the initiator with
whom he communicates; this property should hold even if the receiver knows the
sender only via some pseudonym. It is this intuition that we strive to capture in
our formal deﬁnition of sender invariance below.
To formulate sender invariance independently of which particular formalism
or tool is adopted for modeling and analysis, we deﬁne requirements for protocol
models (summarized in Fig. 1), that are suﬃcient to formalize sender invariance.
We assume that there exists a set Msg of all messages, which we represent as
free terms with the standard perfect cryptography assumption. Let Agent ⊆Msg
denote the set of all possible agent identiﬁers, including both real names and
a set ID ⊆Agent of pseudonyms. We also assume that there exists a set of
honest agent identiﬁers, which we denote HAgent ⊆Agent, and a set of honest
pseudonyms, which is H ID = ID ∩HAgent. As notation, we will use upper case
A, B, ... to denote role names and lower case a, b, ... for agent names.

Formalizing and Analyzing Sender Invariance
85
E
Set of events
AE ⊇{witness, request} Auxiliary events, with AE ⊆E
Msg
Set of all possible messages
Agent ⊆Msg
Agent identiﬁers, both names and pseudonyms
HAgent ⊆Agent
Honest agent identiﬁers
ID ⊆Agent
Pseudonyms
H ID = ID ∩HAgent
Pseudonyms belonging to honest agents
Vars
Set of protocol variable identiﬁers
Fig. 1. Notation
We follow the standard Dolev-Yao model [13] of an active intruder who con-
trols the network but cannot break cryptography: the intruder can intercept
messages and analyze them if he possesses the respective keys for decryption,
and he can generate messages from his knowledge and send them under any
party’s name.
The protocol models must also provide some means to reason about the way
that an agent interprets particular concrete messages. In Protocol 1, for instance,
the responder B might want to ensure that a concrete value he receives and
interprets as A’s payload message Msg was indeed intended by A as a payload
message and not, for instance, as a response to the challenge NB. To this end, we
require the existence of a set Vars of identiﬁers for the variables of the protocol.
The elements of Vars are logical identiﬁers indicating how an agent interprets a
given value. The deﬁnition of the set itself is protocol speciﬁc. For instance, for
Protocol 1, the set Vars = {PBKA, Msg, NB} would be appropriate.
We assume that protocol models have behaviors that can be expressed as lin-
early ordered traces of events from a ﬁxed event set E. Traces contain events from
a set AE of auxiliary events that express information about an honest agent’s
assumptions or intentions when executing a protocol. These events provide a
language over which we then deﬁne the goals of the protocol.2 We assume that
the intruder can neither generate events from AE nor modify those AE events
generated by honest agents. By convention, we call the events in AE witness and
request. For a, b ∈Agent, v ∈Vars, and m ∈Msg,
– witness(a, b, v, m) expresses that initiator a intends to execute the protocol
with responder b and wishes to use value m as the protocol variable v; and
– request(b, a, v, m) expresses that responder b accepts the value m and now
relies on the guarantee that agent a exists and agrees with him on this value
for protocol variable v.
Consider an honest initiator a who wishes to execute a protocol with a re-
sponder b. For all v ∈Vars that are of interest (where the deﬁnition of interest
2 This approach to formalizing protocol goals is standard. It is adopted, for instance,
in the AVISPA Tool [3,7], and it is analogous to other approaches like that of [16],
where goals are formulated in terms of “status signals” exchanged on special channels
to which the intruder has no access.

86
P. Hankes Drielsma et al.
will depend strongly on the goals of the protocol in question), a will generate an
event witness(a, b, v, m) upon setting a value m for v, and each honest respon-
der will generate an event request(b, a, v, m′) after reaching an accepting state
in which he has assigned the value m′ to v. Following [3,7], we deﬁne protocol
goals below as conditions on traces that specify how witness and request events
must correspond with one another.
Example. In Protocol 1, one can deﬁne the variables of interest to be those
which the responder wants to be sure originated from the pseudonym H(PBKA):
namely, Msg and the signed NB. Honest agents will, as mentioned, generate
auxiliary events for each of these variables of interest, but we consider only Msg
in this example. We assume that agent a with PBK pbka wishes to execute
Protocol 1 with agent b, and that a wishes to transmit the payload message 17.
Furthermore, for the sake of example, we ignore possible manipulations by the
intruder and assume that messages are transmitted without modiﬁcation.
Upon sending message 1, a generates the event witness(H(pbka), b, Msg, 17),
expressing that, under her pseudonym, she intends to send to b the value 17,
interpreting it as protocol variable Msg. The responder accepts the protocol
run only after receiving message 4, which conﬁrms recentness. After receiving
message 4, b will generate the event request(b, H(pbka), Msg, 17), indicating that
he accepts the value 17, believes that it originates from the agent associated
with pseudonym H(pbka), and interprets it as the protocol variable Msg.
We now formally deﬁne the security goal of sender invariance as the following
temporal property of traces of events over the set E, where 2 and ♦- denote
the linear time temporal operators “always in the future” and “sometime in the
past”, respectively:
si: ∀b ∈HAgent.∀id ∈H ID.∀m ∈Msg.∀v ∈Vars.
2(request(b, id, v, m) →∃v′ ∈Vars.♦- witness(id, b, v′, m))
We assume, in this deﬁnition, that the initiator knows the real name of the
responder b, but we do not require that b knows the real name of the initia-
tor. This deﬁnition expresses that every honest agent b is guaranteed that, if
id ∈H ID, then there exists an honest agent who sent all the values m that b be-
lieves originated from pseudonym id. Recall that only honest agents generate the
auxiliary events in AE, therefore the presence of a witness event implies that it
was generated by an honest agent. Moreover, for each incoming message m that
b associates with the protocol variable v in the request, there exists some proto-
col variable v′ that expresses how the honest owner of pseudonym id intended
to send the value m. This implies that the values m have not been modiﬁed in
transit, but the sender and receiver may have assigned diﬀerent interpretations
to the transmitted values.
2.3
Strong Sender Invariance
A stronger goal results when the interpretations must agree. We deﬁne strong
sender invariance, a modiﬁcation of sender invariance, by requiring that the

Formalizing and Analyzing Sender Invariance
87
sender and the receiver agree on the interpretation of each message. We formalize
this as follows:
strongsi: ∀b ∈HAgent.∀id ∈H ID.∀m ∈Msg.∀v ∈Vars.
2(request(b, id, v, m) →♦- witness(id, b, v, m))
Strong sender invariance, as the name implies, provides a stronger guarantee
than sender invariance itself (we will show this formally in §2.5). Speciﬁcally, it
requires that all values m received by b apparently from an honest pseudonym
id indeed originated from the same honest agent. Moreover, for each m, the pro-
tocol variable v with which b associates m must be the same as the v for which
the value was intended by the sender id; that is, v is the same in both auxiliary
events. As before, this implies that the value was not modiﬁed in transit, but
we now additionally require that the interpretations agree. In the extreme case,
that the protocol-speciﬁc set of “interesting” protocol variables includes all pro-
tocol variables, this implies that the exact messages sent by the initiator arrive,
without tampering, at the responder.
2.4
Discussion
The informal notion that the source of a communication does not change suﬀers
from ambiguities that one must resolve when deﬁning sender invariance formally.
Perhaps most importantly, one must deﬁne to what extent sender invariance
implies message integrity.
Conservatively, one can deﬁne sender invariance in such a way that any mes-
sage modiﬁcation violates sender invariance. This would be akin to the notion
of matching conversations, deﬁned in [8]. Such a deﬁnition is quite restrictive
and of limited practical use, particularly in ad-hoc settings with potentially no
relationships among protocol participants.
Instead, we opt for a ﬁner-grained deﬁnition in which integrity must be guar-
anteed only for relevant parts of the exchanged messages, where “relevant” can
be deﬁned in a protocol-speciﬁc way. The case described above is then a special
case of this more general approach in which all parts of the protocol messages
are considered relevant. In order to pursue this ﬁne-grained approach, we formal-
ize sender invariance over the auxiliary trace events witness and request rather
than, for instance, over the communication events themselves.
The auxiliary events witness and request confer a further beneﬁt; namely,
they contain all the information one needs to formalize authentication itself.
This facilitates a direct comparison of the two forms of sender invariance with
authentication, discussed in the next subsection.
Finally, we note that alternate deﬁnitions of (strong) sender invariance are
also possible and may be appropriate for certain settings. In our deﬁnition, we
assume a setting in which the owner of pseudonym id knows the identity of the
agent b with whom he wants to communicate. This assumption is appropriate
for one of our larger case-study protocols, Mobile IPv6 [15]. One could, however,
envision protocols in which the recipient is unimportant, or indeed known via

88
P. Hankes Drielsma et al.
a pseudonym. For such protocols, one might deﬁne sender invariance as follows
(and strong sender invariance analogously):
si′: ∀b ∈HAgent.∀id ∈H ID.∀m ∈Msg.∀v ∈Vars.∃b′ ∈Agent.
2(request(b, id, v, m) →∃v′ ∈Vars.♦- witness(id, b′, v′, m)) .
For the rest of the paper, however, we will focus on our original deﬁnition si.
2.5
Relating Sender Invariance and Authentication
We now examine the relationship between sender invariance, strong sender in-
variance, and authentication. We ﬁrst recall the informal deﬁnition of weak au-
thentication (adapted from [16], where it is termed non-injective agreement):
A protocol guarantees weak authentication to a responder B on a set
of protocol variables V iﬀwhenever B completes a run of the protocol,
apparently with initiator A, then A has previously been executing the
protocol as initiator, apparently with responder B, and the two agents
agree on the data values corresponding to all the variables in V.
In our model, we equate the responder’s completion of a protocol run with his ar-
rival in an accepting state. Since we assume that responders issue request events
only after reaching an accepting state, we can formally deﬁne weak authentica-
tion as follows:
wauth: ∀b ∈HAgent.∀a ∈HAgent.∀m ∈Msg.∀v ∈Vars.
2(request(b, a, v, m) →♦- witness(a, b, v, m))
Observe that strong sender invariance diﬀers from weak authentication only
in the inclusion of the pseudonym id ∈H ID rather than an actual agent iden-
tiﬁer b ∈HAgent, which may be either a pseudonym or a real name. Thus,
strong sender invariance is the direct analogue to weak authentication for the
pseudonymous setting, and we have that wauth implies strongsi. The con-
verse, however, does not hold, as expressed as Proposition 1.
Proposition 1. Weak authentication is a strictly stronger security goal than
strong sender invariance.
Proof. We ﬁrst show that every trace that satisﬁes weak authentication also
satisﬁes strong sender invariance; thus, if all traces induced by a protocol satisfy
weak authentication, then they also satisfy strong sender invariance. To that end,
consider an arbitrary trace that satisﬁes weak authentication and any event
on this trace of the form request(b, id, v, m), for arbitrary b ∈HAgent, id ∈
H ID, v ∈Vars and m ∈Msg. We have to show that this event is preceded by
the event witness(id, b, v, m). This follows directly, since H ID ⊆HAgent, and
weak authentication demands that any event request(b, a, v, m)—where now a ∈
HAgent—is preceded by witness(a, b, v, m). Note that if a /∈H ID for all initiators
a ∈HAgent for which request terms are generated, then sender invariance holds

Formalizing and Analyzing Sender Invariance
89
trivially. Since we have not assumed any speciﬁc property about b, id, v, and
m, or where in the trace the request event occurs, every request(b, id, v, m) is
preceded by witness(id, b, v, m).
To see that weak authentication is strictly stronger than strong sender invari-
ance, consider a trace with the event request(b, a, v, m) with a ∈HAgent \ H ID,
and no other witness or request events. This trace trivially satisﬁes strong sender
invariance (as a /∈H ID) but not weak authentication. This example is a bit con-
trived, but we give a more realistic example (Protocol 2) in §3.1.
2
We now examine the relationship between the two types of sender invariance
itself.
Proposition 2. Strong sender invariance is a strictly stronger security goal than
sender invariance.
Proof. As before, we show that strong sender invariance is at least as strong as
sender invariance by showing that any trace satisfying the stronger form also
satisﬁes the weaker one. Consider an arbitrary trace that satisﬁes strong sender
invariance, and consider any event of the form request(b, id, v, m) in the trace,
again for arbitrary values b, id, v, and m of the respective types. We have to show
that this event is preceded on the trace by the event witness(b, id, v′, m) for some
v′ ∈Vars. This holds for v = v′, since the trace satisﬁes strong sender invariance,
which requires that witness(id, b, v, m) must precede said request event. As we
have not assumed anything about the arguments of the request event and its
position in the trace, this holds for all such request events, which shows that
sender invariance holds of the trace.
A trivial example to show that sender invariance does not imply strong sender
invariance is a trace that contains request(b, id, v, m), preceded by witness(id, b,
v′, m) for arbitrary constants b, id, v, v′, and m, where v ̸= v′, and such that
the trace contains no other witness and request events. This satisﬁes sender
invariance, but not strong sender invariance. Another example is Protocol 1,
which will be discussed in the following section.
2
It follows from these propositions that there is a hierarchy of security goals in
which weak authentication is strongest, followed by strong sender invariance, and
ﬁnally sender invariance itself. Speciﬁcally, we have seen that strong sender in-
variance is precisely weak authentication in which pseudonyms are used in place
of true agent names. We can observe the same of sender invariance, modulo the
fact that the agreement of protocol variables is also ignored. As we will discuss
in the next section, this result also illustrates the potential to take existing tools
for the automated analysis of authentication protocols and directly use them for
the analysis of (strong) sender invariance as well.
3
Analyzing Sender Invariance
We now show how to apply automated tools to analyze (strong) sender invari-
ance protocols. We illustrate this with a case study: the formal analysis of two
protocols that use the Purpose-Built Keys Framework.

90
P. Hankes Drielsma et al.
Classically, the model checking problem M ⊨ϕ veriﬁes whether a model M of
a system fulﬁlls a speciﬁcation of the goal ϕ. We have analyzed our case-study
protocols using the On-the-Fly Model Checker OFMC [5,6,7], a state-of-the art
tool for protocol analysis. OFMC is one of the back-ends of the AVISPA Tool,
in which protocols are speciﬁed using the High-Level Protocol Speciﬁcation Lan-
guage HLPSL [3,11].3 Protocol models built using this speciﬁcation language
capture the requirements for formalizing sender invariance that we identiﬁed
in Fig. 1. OFMC allows the modeler to specify ϕ, where goals are speciﬁed
negatively as attack states. Thus, for the analyses described in the coming sec-
tions, we were able to translate the formulas strongsi and si into HLPSL
directly.
Note that while OFMC allows for user-deﬁned goals, some model checkers for
security protocols consider a ﬁxed, built-in set of goals ϕ tailored to the appli-
cation domain: in general, authentication and secrecy. In the previous section,
however, we showed that both forms of sender invariance can be seen as a gen-
eralization of weak authentication. Based on this, we can identify the following
additional requirements on protocol models for use with such ﬁxed-goal model
checkers. If
– one can construct protocol speciﬁcations in which authentication is per-
formed on pseudonyms,
– honest pseudonyms can be distinguished in the model from those belonging
to the intruder, and
– in the case of si, agreement on protocol variables can be ignored,
then model checkers that are tailored to check wauth can be employed, out of
the box, to also check strongsi and si.
3.1
Case Study: Purpose-Built Keys
Analyzing Protocol 1. We return to Protocol 1, introduced in §2.1. We
constructed a formal model of the Protocol 1 in HLPSL and analyzed it in
a scenario with a bounded number of protocol sessions. For brevity, we omit the
HLPSL speciﬁcation itself and describe only the aspects most important for the
analysis.
In our model of the protocol, honest agents generate new PBKs freshly for
each session, and these are added to the set H ID upon generation. HLPSL
3 The HLPSL is an expressive, modular, role-based, formal language that allows for the
speciﬁcation of control ﬂow patterns, data structures, complex security properties, as
well as diﬀerent cryptographic operators and their algebraic properties. The AVISPA
Tool automatically translates a user-deﬁned security problem into an equivalent
speciﬁcation written in the rewrite-based formalism IF (for Intermediate Format).
An IF speciﬁcation describes an inﬁnite-state transition system amenable to formal
analysis: this speciﬁcation is input to OFMC and the other back-ends of the AVISPA
Tool, which implement a variety of techniques to search the corresponding inﬁnite-
state transition system for states that represent attacks on the intended properties
of the protocol.

Formalizing and Analyzing Sender Invariance
91
1. A →i : PBKA
2. A →i : {MsgA}PBKA−1.H(PBKA)
3. i →A : MsgI .H(PBKA)
4. A →i : {MsgI }PBKA−1.H(PBKA)
1′. i →B : PBKA
2′. i →B : {MsgI }PBKA−1.H(PBKA)
Fig. 2. An attack on Protocol 1 that violates strongsi
supports the modeling of sets, and using this we maintain a single, global H ID.
The intruder, who is active and has full Dolev-Yao [13] control over the network
as described, may also generate fresh PBKs (and may apply the function H to
generate valid pseudonyms), but he may not add them to H ID. He may, however,
replay any of the keys in that set. We assume that the responder wants sender
invariance guarantees on the contents of every message after the initialization
phase. Thus, the set Vars = {Msg, NB}, and in the model the responder issues
two request facts after receiving message 4. In turn, the initiator role issues
witness facts upon sending messages 2 and 4.
OFMC employs a number of symbolic techniques to perform falsiﬁcation (by
ﬁnding an attack on the input protocol) and bounded veriﬁcation, i.e. veriﬁcation
for a ﬁnite number of protocol sessions. In our analysis scenario, we assumed four
concurrent protocol sessions (four instances each of the initiator and responder
roles). In OFMC, these sessions are speciﬁed symbolically, so we need not spec-
ify concretely which agent plays which role. Rather, the identities of the agents
participating in each session are given simply as variables, and OFMC searches
symbolically through all possible assignments of these variables. Our ﬁrst analy-
sis used si, sender invariance, as the goal of the protocol. As our analysis found
no attacks, this amounts to bounded veriﬁcation of all possible analysis scenar-
ios consisting of four protocol sessions. This shows that PBK is indeed a strong
mechanism for providing sender invariance.
Strong Sender Invariance. We also analyzed Protocol 1 against the goal of
strong sender invariance. Recall that, by design, sender invariance ignores the
interpretation that agents assign to messages, which we express via the protocol
variables in set Vars. Thus, protocols guaranteeing sender invariance may well
suﬀer from vulnerabilities in which an intruder succeeds in causing a confusion
between the interpretation assigned to a message by the sender and that assigned
by the receiver. Indeed, our analysis found an attack on strong sender invariance,
shown in Fig. 2.
In this execution, the intruder intercepts the purpose-built key PBKA and
wishes to pass himself oﬀto B as someone who possesses the associated pri-
vate key. To this end, after receiving A’s second message, he replies with the
challenge MsgI , the payload message he actually wants to send to B. A replies
in good faith, signing the challenge with PBKA
−1. In a second session, i then
claims PBKA as his own purpose-built key and sends the signed payload message

92
P. Hankes Drielsma et al.
1. A →B : PBKA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2. A →B : {tag 1.Msg}PBKA−1.H(PBKA)
3. B →A : NB.H(PBKA)
4. A →B : {tag 2.NB}PBKA−1.H(PBKA)
Protocol 2. A reﬁned PBK-Based protocol
{MsgI }PBKA−1. Recall that we assume that messages 2 through 4 may be re-
peated multiple times to transmit payload data over the lifetime of a pseudonym,
therefore the intruder can even perform the challenge-response exchange with B
as soon as A sends another payload message.
This attack represents a confusion of the protocol variables assigned to mes-
sage 4 by A and message 2′ by B. Although A did indeed once send the message
{MsgI }PBKA−1.H(PBKA), she sent it interpreting it as message 4 of the protocol
and thus assigned NB = MsgI , whereas B interprets it as message 2 upon re-
ceipt, assigning Msg = MsgI . Thus, this attack violates the goal of strong sender
invariance, but not sender invariance itself. As discussed in §2.5, Protocol 1 il-
lustrates that si is strictly weaker than strongsi.
Analyzing Protocol 2. Protocol 2 shows an alternative example of a protocol
that uses the PBK framework. It is identical to Protocol 1 save for the fact that
so-called tags have been added to messages 2 and 4. The tags tag1 and tag2 are
intended as identiﬁers for the signed messages that signify the purpose of the
signature. They avoid, for instance, that B or an intruder can bring A to sign
arbitrary data without indicating what the signature is intended for.
We used OFMC to analyze Protocol 2 in the same setting as used in our
analysis of Protocol 1. The witness and request facts generated contain protocol
variables indicating the interpretation assigned by the agents, m2 and m4 for
messages 2 and 4, respectively. As speciﬁed in the formula strongsi, matching
request and witness events must agree on this interpretation. Our analysis results
show that, for scenarios consisting of four protocol sessions, Protocol 2 is safe
from attacks on strong sender invariance. From Proposition 2, we can conclude
that Protocol 2 is thus safe from attacks on sender invariance as well.
4
Varying Amounts of Pre-shared Information
Sender invariance appears to be an appealing security goal that is appropriate
for settings such as those that arise in mobile networks where users do not know
one another in advance. Naturally, situations arise that fall between the case in
which two agents involved in a protocol run initially share cryptographically au-
thenticated information and the other extreme case in which they share nothing.
Perhaps the most prevalent example of this arises in E-commerce situations in
which the selling party presents a public-key certiﬁcate signed by a well-known

Formalizing and Analyzing Sender Invariance
93
1. A →B : PBKA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2. A →B : {tag 1.{K}PKB }PBKA−1.H(PBKA)
3. B →A : {NB.B}K.H(PBKA)
4. A →B : {NB.H(PBKA)}K.H(PBKA)
Protocol 3. A PBK-Based protocol in which A knows B’s public key in advance
certiﬁcation authority, while the buyer’s credentials comprise, at most, a user-
name and password set up via an informal registration procedure.
In cases like these, where the amount of information shared between protocol
participants is greater, we can achieve accordingly stronger security goals. We
illustrate this with a brief example: Protocol 3 is another protocol that employs
the PBK framework. Unlike in the previous ones, however, we assume that the
initiator A knows the public key PKB of the responder B in advance. After A
sends her PBK, she generates a new session key K for use between A and B.
She encrypts this key with PKB and signs it, together with a tag indicating the
purpose of the signature. B responds, encrypting a nonce NB together with his
name using the new key K. A responds to the challenge and returns the nonce
NB together with her pseudonym H(PBKA) (twice, once encrypted and once in
plaintext).
In discussing the previous protocols, we focused on role B’s guarantee of
(strong) sender invariance with respect to role A. The agent playing role A,
however, could say little or nothing about the security of her communication
with B. As in Protocol 2, B is ensured sender invariance with respect to role A.
Here, however, A is able to leverage the fact that she knows B’s public key to
send a new session key K secretly. Messages 3 and 4 serve to ensure recentness
and key conﬁrmation to both parties. Subsequent communication secured with
the key K should then enjoy the following security properties:
– secrecy of the communication,
– responder B should be guaranteed sender invariance with respect to role A,
and
– initiator A should be guaranteed authenticity of the communication from B
(as only B should have been able to decrypt K).
This simple example shows how a pre-existing relationship, even a unilateral
one, enables signiﬁcantly greater security. A more prominent example of this is
found in the use of SSL/TLS [12] in E-commerce. Most E-commerce applications
employ server certiﬁcates for servers to authenticate themselves to clients, but
forgo the use of client certiﬁcates. Hence, this situation is analogous to the one
just described: the client is guaranteed the authenticity of the server, but—
at least on the transport layer—the server can only refer to the client via a
pseudonym.
Overall, as mobile and ad-hoc networks gain ground, we expect to see an
increase in situations in which some measure of information, though perhaps

94
P. Hankes Drielsma et al.
not as much as is assumed by traditional authentication protocols, is initially
shared. It is therefore important to precisely understand what security goals are
achievable in the diﬀerent settings.
5
Conclusion
Sender invariance is a variant of authentication, with the diﬀerence that the
identity of a sender is not known to the receiver, but rather the sender is identiﬁed
by a pseudonym. The key point is that sender invariance can be achieved out of
nothing, i.e. even when the agents have no previous security relationship (like
shared keys, public keys, or a relationship via a trusted third party) and therefore
classical authentication cannot be achieved.
In this paper, we have formalized two forms of sender invariance as variants
of classical authentication, and showed that these goals form a hierarchy in the
sense that one goal is strictly stronger than the other, with classical authentica-
tion being the strongest.
This relationship with classical authentication has allowed us to formalize
sender invariance goals for an existing protocol analysis system, the OFMC
back-end of the AVISPA Tool. As a case study, we have analyzed protocols
using the Purpose-Built Keys Framework (PBK [10]), showing that a na¨ıve pro-
tocol implementation has vulnerabilities but still provides a weak form of sender
invariance, while an improved implementation with tags provides strong sender
invariance.
Our current work includes further investigations into sender invariance pro-
tocols. We have recently completed a formal analysis of the secure neighbor
discovery protocol of Mobile IPv6 [2] and will report on our ﬁndings in an up-
coming paper. Moreover, we plan to examine a further generalization of the view
on sender invariance and, conversely, investigate “receiver invariance”, i.e. the
property that only the party that sent the ﬁrst message (and who created the
respective pseudonym) can read the messages directed to him. Receiver invari-
ance can then be the counterpart of classical secrecy goals in the realm of sender
invariance protocols.
References
1. Abadi, M.: Private Authentication. In: Dingledine, R., Syverson, P.F. (eds.) PET
2002. LNCS, vol. 2482, pp. 27–40. Springer, Heidelberg (2003)
2. Arkko, J., Kempf, J., Zill, B., Nikander, P.: RFC3971 – SEcure Neighbor Discovery
(SEND) (March 2005)
3. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Hankes Drielsma, P., He`am, P.C., Mantovani, J., Moedersheim, S., von Oheimb,
D., Rusinowitch, M., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.: The
AVISPA Tool for the Automated Validation of Internet Security Protocols and
Applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576,
pp. 281–285. Springer, Heidelberg (2005), http://www.avispa-project.org

Formalizing and Analyzing Sender Invariance
95
4. Aura, T.: RFC3972 – Cryptographically Generated Addresses (CGA) (March 2005)
5. Basin, D., M¨odersheim, S., Vigan`o, L.: Constraint Diﬀerentiation: A New Reduc-
tion Technique for Constraint-Based Analysis of Security Protocols. In: CCS’03.
Proceedings of CCS’03, pp. 335–344. ACM Press, New York (2003)
6. Basin, D., M¨odersheim, S., Vigan`o, L.: Algebraic intruder deductions. In: Sut-
cliﬀe, G., Voronkov, A. (eds.) LPAR 2005. LNCS (LNAI), vol. 3835, pp. 549–564.
Springer, Heidelberg (2005)
7. Basin, D., M¨odersheim, S., Vigan`o, L.: OFMC: A Symbolic Model-Checker for
Security Protocols. International Journal of Information Security 4(3), 181–208
(2005)
8. Bellare, M., Rogaway, P.: Entity authentication and key distribution. In: Stinson,
D.R. (ed.) CRYPTO 1993. LNCS, vol. 773, pp. 232–249. Springer, Heidelberg
(1994)
9. Boyd, C.: Security architectures using formal methods. IEEE Journal on Selected
Areas in Communications 11(5), 694–701 (1993)
10. Bradner, S., Mankin, A., Schiller, J.I.: A framework for purpose built keys (PBK),
Work in Progress (Internet Draft) (June 2003)
11. Chevalier, Y., Compagna, L., Cuellar, J., Hankes Drielsma, P., Mantovani, J.,
M¨odersheim, S., Vigneron, L.: A High Level Protocol Speciﬁcation Language for
Industrial Security-Sensitive Protocols. In: Proceedings of SAPS’04, pp. 193–205.
Austrian Computer Society (2004)
12. Dierks, T., Allen, C.: RFC2246 – The TLS Protocol Version 1 (January 1999)
13. Dolev, D., Yao, A.: On the Security of Public-Key Protocols. IEEE Transactions
on Information Theory 2(29) (1983)
14. Gollmann, D.: What do we mean by Entity Authentication. In: Proceedings of
the 1996 IEEE Symposium on Security and Privacy, pp. 46–54. IEEE Computer
Society Press, Los Alamitos (1996)
15. Johnson, D., Perkins, C., Arkko, J.: RFC3775 – Mobility Support in IPv6 (June
2004)
16. Lowe, G.: A hierarchy of authentication speciﬁcations. In: Proceedings of CSFW’97,
pp. 31–43. IEEE Computer Society Press, Los Alamitos (1997)
17. Nikander, P., Kempf, J., Nordmark, E.: RFC3756 – IPv6 Neighbor Discovery (ND)
Trust Models and Threats (May 2004)

From Simulations to Theorems:
A Position Paper on Research in
the Field of Computational Trust
(Extended Abstract)⋆
Karl Krukow and Mogens Nielsen
BRICS⋆⋆
University of Aarhus,
Denmark
{krukow, mn}@brics.dk
Abstract. Since the millennium, a quickly increasing number of research
papers in the ﬁeld of“computational trust and reputation”have appeared
in the Computer Science literature. However, it remains hard to compare
and evaluate the respective merits of proposed systems. We argue that
rigorous use of formal probabilistic models enables the clear speciﬁca-
tion of the assumptions and objectives of systems, which is necessary
for comparisons. To exemplify such probabilistic modeling, we present
a simple probabilistic trust model in which the system assumptions as
well as its objectives are clearly speciﬁed. We show how to compute (in
this model) the so-called predictive probability: The probability that the
next interaction with a speciﬁc principal will have a speciﬁc outcome.
We sketch preliminary ideas and ﬁrst theorems indicating how the use of
probabilistic models could enable us to quantitatively compare proposed
systems in various diﬀerent environments.
1
Introduction
What are the fundamental models in the ﬁeld of computational trust?
While this question is highly relevant for researches in the ﬁeld of computational
trust and reputation, in fact, it is hard to identify one model (or even a few) ac-
cepted widely by the community. One common classiﬁcation of proposals is into
“probabilistic”and “non-probabilistic”systems [1,2,3]. The non-probabilistic sys-
tems may be further classiﬁed into various diﬀerent types (e.g., social networks
and cognitive); in contrast, the probabilistic systems usually have a common ob-
jective and structure: Probabilistic systems (i) assume a particular (probabilis-
tic) model for principal behavior; and (ii) propose algorithms for approximating
⋆Full Paper will be published in a special collection dedicated to Gordon Plotkin (to
appear). Available online: http://www.brics.dk/∼krukow
⋆⋆BRICS: Basic Research in Computer Science (www.brics.dk), funded by the Danish
National Research Foundation.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 96–111, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

From Simulations to Theorems
97
the behavior of principals (i.e., prediction in the model). In systems based on
such models, the trust information about a principal is information about its
past behavior. Probabilistic systems usually do not classify this information as
‘good,’ ‘bad,’ ‘trustworthy’ or ‘untrustworthy;’ rather, such systems attempt to
approximate the probability of various outcomes in a potential next interaction,
given the past behavior. The probabilistic systems, known as “game-theoretical”
in the terminology of Sabater and Sierra [2], are based on the view on trust of
Gambetta: “(. . . ) trust (or, symmetrically, distrust) is a particular level of the
subjective probability with which an agent assesses that another agent or group
of agents will perform a particular action, both before he can monitor such action
(or independently of his capacity ever to be able to monitor it) and in a context
in which it aﬀects his own action” [4].
The contributions of this paper relate only to this, i.e., the probabilistic or
predictive view on trust. We restrict ourselves to this approach for two primary
reasons: (i) It is founded on well-understood mathematical theory and models
(i.e., probability theory);1 and (ii) the assumptions and objectives of systems are
precise. Lack of formal models leads to in-clarity about the exact objectives of
proposed systems; as Samuel Karlin was quoted to have said in a tribute lecture
to honor R.A. Ficher:“The purpose of models is not to ﬁt the data but to sharpen
the questions.” We need to sharpen our questions. Our position is that for any
approach to computational trust, probabilistic or not, it should be possible to:
(i) Specify precisely the assumptions about the intended environments for the
proposed system, i.e., in which applications does the system do well? (ii) Specify
precisely the objective of the system, i.e., exactly what does the system compute?
The purpose of this paper is to highlight some of the advantages of formal
probabilistic models. We show how formal probabilistic models enable systems
that satisfy our two mentioned criteria of foundation and precision. Further,
we sketch ideas towards a theoretically well-founded technique for comparing
probabilistic systems in various diﬀerent environments.
Outline. To illustrate probabilistic models, we develop a probabilistic extension
of the event structure framework [6], used previously in the SECURE project
[7] to model outcomes of interactions. The probabilistic event structure model
generalizes previous probabilistic models from binary outcomes, e.g., each in-
teraction is either ‘good’ or ‘bad,’ to multiple structured outcomes (technically,
we obtain probabilities on the conﬁgurations of ﬁnite confusion-free event struc-
tures).2 This is developed in Section 2 and Section 3.
To further illustrate the beneﬁts of probabilistic models, we present prelim-
inary ideas towards solving one open problem in computational trust research:
Comparison of algorithms for probabilistic trust computation. We develop a
measure which ‘scores’ a probabilistic algorithm in a given probabilistic model
1 We follow the Bayesian approach to probability theory, as advocated by Jaynes [5].
2 For those familiar with Bayesian analysis, we generalize models with beta priors
to multiple structured outcomes where the prior pdfs are products of Dirichlet
distributions.

98
K. Krukow and M. Nielsen
of principal behavior. The measure is parametric in the model in the sense that
for any probabilistic model, λ, the ‘score’ quantitatively measures how well a
given algorithm approximates the true principal behavior in λ. Algorithms can
then be compared by comparing their scores in various models. This work is
presented in Section 4.
2
Probabilistic Event Structures
We have previously proposed to use event structures to model outcomes of prin-
cipal interactions in distributed systems [6]; the model was used in the SECURE
project [8,7] to formalize the notions of outcomes and observations. However,
we did not present a formal probabilistic model of principal behavior; hence,
although we showed how to compute “trust values” which could be interpreted
as probabilities of outcomes, there was no probabilistic model to justify the com-
putation. In the next two sections, we augment the event structure framework
with a probabilistic model which generalizes the model used in systems based on
the beta distribution [9,10,11,12]. We show how to compute the probabilities of
outcomes given a history of observations. This could be valuable in its own right;
however, we would like to emphasize that our primary reason is to illustrate an
example of a formal probabilistic model which enables “sharp” questions; the
heart of this paper is really Section 4. The system proposed in Sections 2 and 3
is well-founded on probability theory and it generalizes many existing systems;
however, it not yet practical: There are many issues it does not handle, e.g.,
dynamic principal behavior-change, lying reputation sources, multiple contexts,
etc. We believe that the probabilistic models must be properly understood before
we can deal with such issues in a theoretically well-founded manner. For further
examples of probabilistic systems we refer to Aberer and Despotovic [1], and to
most of the systems based on Bayesian analysis with beta prior distributions
[9,10,11,12].
Observations and interaction outcomes. Agents in a distributed system
obtain information by observing events which are typically generated by the
reception or sending of messages. The structure of these message exchanges are
given in the form of protocols known to both parties before interaction begins. By
behavioral observations, we mean observations that the parties can make about
speciﬁc runs of such protocols. These include information about the contents of
messages, diversion from protocols, failure to receive a message within a certain
time-frame, etc.
We will use the event-structure framework that we have proposed previously
for modeling observations and outcomes in the SECURE project [6,7]. The frame-
work is suitable for our purpose as it provides a generic model for observations
that is independent of any speciﬁc programming language. In the framework, the
information that an agent has about the behavior of another agent p, is informa-
tion about a number of (possibly active) protocol-runs with p, represented as a
sequence of sets of events, x1x2 · · · xn, where event-set xi represents information

From Simulations to Theorems
99
about the ith initiated protocol-instance. Note that, as opposed to many existing
systems, we are not rating the behavior of principals, but instead, we record the
actual behavior, i.e., which events occurred in the interaction.
Event structures. We brieﬂy recapture the basic deﬁnitions (for more details
and examples, we refer to Nielsen and Krukow [6] and Krukow et al. [13,3]). An
event structure is a triple (E, ≤, #) consisting of a set E of events which are
partially ordered by ≤, the necessity relation (or causality relation), and # is a
binary, symmetric, irreﬂexive relation # ⊂E × E, called the conﬂict relation.
The relations satisfy
[e]
(def)
= {e′ ∈E | e′ ≤e} is ﬁnite; and
if e # e′ and e′ ≤e′′ then e # e′′
for all e, e′, e′′ ∈E. We say that two events are independent if they are not in
either of the two relations.
The two basic relations on event structures have an intuitive meaning in our
set up. An event may exclude the possibility of the occurrence of a number
of other events; this is what the conﬂict relation models. The necessity rela-
tion is also natural: Some events are only possible when others have already
occurred. Finally, if two events are in neither of the relations, they are said to be
independent.
The event structure models the set of events that can occur in a protocol;
however, due to the relations on event structures, not all sets of events can occur
in a particular run. The notion of conﬁgurations formalizes this: A set of events
x ⊆E is a conﬁguration (of ES) if it satisﬁes the following two properties:
Conﬂict free, i.e., for any e, e′ ∈x : e ̸# e′; Causally closed, i.e., for any e ∈
x, e′ ∈E : e′ ≤e ⇒e′ ∈x. Write CES for the set of conﬁgurations of ES. Note
that the set of all maximal conﬁgurations deﬁnes a set of mutually exclusive and
exhaustive outcomes of an interaction.
Histories. A ﬁnite conﬁguration models information regarding a single interac-
tion, i.e., a single run of a protocol. In general, the information that one principal
possesses about another will consist of information about several protocol runs;
the information about each individual run being represented by a conﬁguration
in the corresponding event structure. The concept of a (local) interaction his-
tory models this. An interaction history in ES is a ﬁnite ordered sequence of
conﬁgurations, h = x1x2 · · · xn ∈C∗
ES. The entries xi (for 1 ≤i ≤n) are called
the sessions (of h).
Remarks. While the order of sessions is recorded (histories are sequences), in
contrast, the order of independent events within a single session is not. Hence
independence of events is a choice of abstraction one may make when designing
an event-structure model (because one is not interested in the particular order
of events, or because the exact recording of the order of events is not feasible).

100
K. Krukow and M. Nielsen
However, note that this is not a limitation of event structures: In a scenario where
this order of events is relevant (and observable), one can always use a “serialized”
event structure in which this order of occurrences is recorded. A serialization of
events consists of splitting the events in question into diﬀerent events depending
on the order of occurrence.
2.1
Confusion-Free Event Structures
We consider a special type of event structures, the confusion free event structures
with independence, for which it is especially simple to adjoin probabilities [14].
Consider the following event structure (∼represents conﬂict, and →represents
causality).
c

d
e

f
a

b












The events c and e are independent; as are c and f; d and e; and ﬁnally, d and f.
However, in terms of the relations of event structures, independent simply means
that both events can occur in the same conﬁguration and in any order. Later we
shall consider a probabilistic model where independence means also probabilistic
independence. To do this we ﬁrst introduce a notion of cells and immediate
conﬂict [14]. In the following ES = (E, ≤, #) is a ﬁxed event structure.
Write [e) for [e] \ {e}, and say that events e, e′ ∈E are in immediate conﬂict,
writing e #μ e′, if e # e′ and both [e) ∪[e′] and [e] ∪[e′) are conﬁgurations.
It is easy to see that a conﬂict e # e′ is immediate if-and-only-if there exists
a conﬁguration x where both e and e′ are enabled (i.e., can occur in x). For
example the conﬂict a # b is immediate, whereas a # c is not.
A partial cell is a non-empty set of events c ⊆E such that e, e′ ∈c implies e #μ e′
and [e) = [e′). A maximal partial cell is called a cell. There are three cells in the
above event structure: {a, b}, {c, d} and {e, f}. Cells represent choices; in proba-
bilistic event structures, probabilistic choices. A confusion free event structure is
an event structure where (the reﬂexive closure of) immediate conﬂict is an equiv-
alence relation and within cells (i.e., that e #μ e′ implies [e) = [e′)). We suspect
that most event structures for simple interaction protocols are confusion free.
In confusion-free event structures, if an event e of a cell c is enabled at con-
ﬁguration x, then all events e′ ∈c are also enabled at x. If the event structure
is also ﬁnite, a maximal conﬁguration (i.e., an outcome of an interaction) is
obtained by starting with the empty conﬁguration and then repeating the fol-
lowing: Let C be the set of cells that are enabled in the current conﬁguration.
If C is empty then stop: The current conﬁguration is maximal; otherwise, non-
deterministically select a cell c ∈C, and then non-deterministically select, or
probabilistically sample, an event e ∈c; ﬁnally, update the current conﬁguration
by adding e.

From Simulations to Theorems
101
The following notion of cell-valuation formalizes probabilistic sampling in
cells.
Deﬁnition 1 (Cell valuation, Varacca et al. [14]). When f : X →[0, +∞]
is a function, for every Y ⊆X, we deﬁne f[Y ] = 
y∈Y f(y). A cell valuation
on a confusion free event structure ES = (E, ≤, #) is a function p : E →[0, 1]
such that for every cell c, we have p[c] = 1.
If cell choices are probabilistic, say given by a cell-valuation p, and if we assume
independence between cells, then one can obtain the probability of any conﬁgu-
ration x (i.e., any outcome) as the product of the probabilities of each event in
x given p.
3
A Probabilistic Framework
We will be concerned with adjoining probabilities to the conﬁgurations of a ﬁnite
confusion-free event structure ES. As mentioned in the previous section, we can
do this by ﬁnding a cell valuation p : E →[0, 1], or, equivalently, for each cell c,
a function pc : c →[0, 1] with pc[c] = 1. The functions pc should be derived from
the past experience obtained from interacting with an entity in ES. In the fol-
lowing paragraph, we state the assumptions about the behavior of entities in our
model. We then proceed to (i) ﬁnd abstractions that preserve suﬃcient informa-
tion under the model; and (ii) derive equations for the predictive probabilities,
i.e., answering“what is the probability of outcome x in the next interaction with
entity q (in the model)?”
The model. Let us consider a ﬁnite and confusion-free event structure ES. Let
us write C(ES) for the set of cells (which are then the equivalences classes of
immediate conﬂict). Write C(ES) = {c1, c2, . . . , ck}, and let Ni = |ci| for each
i. Let us make the following assumptions about principal behavior, and write
λDES for the assumptions of this model:
Each principal’s behavior is so that there are ﬁxed parameters such that
at each interaction we have, independently of anything we know about
other interactions, the probability θc,e for event e at cell c.
Each θci for ci ∈C(ES) is a vector of size Ni such that 
e∈ci θci,e = 1. Hence,
the collection θ = (θc | c ∈C(ES)) deﬁnes a cell valuation on ES. For each
conﬁguration x ∈CES the probability of obtaining x in any run of ES with a
principal parametrized by θ is
P(x | θλDES) =

e∈x
θe
(1)
where θe is deﬁned by θc,e where c is the unique cell with e ∈c.
The goal of our probabilistic framework is to estimate the parameters θ given
a prior distribution and data regarding past interactions. In the λDES model,

102
K. Krukow and M. Nielsen
we need only estimate the parameters of each cell c, i.e., θc, to obtain a proba-
bility distribution on conﬁgurations (Equation 1). Furthermore, it follows from
λDES that given a sequence h = x1x2 · · · xn ∈C∗
ES of observed data (about a
ﬁxed principal), we need only keep track of event counts of h to estimate the
parameters of each θc (e.g., according to λDES, the order of sessions does not
matter). This means that an event count, i.e., a function X : E →N, is suﬃcient
information to estimate θc for each cell c.
To estimate the parameters θ, we shall use Bayesian analysis. Hence, we need
prior distributions. It turns out that the family of Dirichlet distributions are a
family of conjugate prior distributions to the family of multinomial trials. A fam-
ily F of distributions is a conjugate prior for a likelihood function L if whenever
the prior distribution belongs to F then also the posterior distribution belongs
to F. The use of conjugate priors represents a computational convenience com-
mon for Bayesian analysis: The distributions always maintain the same algebraic
form (i.e., that of family F). As we shall see, the uniform distribution belongs
to the Dirichlet family; this means that the prior, if desired, can be chosen not
to bias any event over another.
Since each sampling from a cell is a multinomial trial (according to λDES), we
use Dirichlet distributions as our prior distributions. Speciﬁcally, a prior Dirichlet
distribution is assigned to each cell c of ES. Event counts are then used to update
the Dirichlet at each cell. Hence, at any time we have, for each cell c, a Dirichlet
distribution fc on the parameters θc of the events of that cell; we show that the
probability of an outcome x ⊆E is then the product of certain expectations of
these distributions. We explain the Dirichlet distributions in the following.
3.1
The Dirichlet Distribution
The Dirichlet family D of order K, where 2 ≤K ∈N, is a parametrized collec-
tion of continuous probability density functions deﬁned on [0, 1]K. There are K
parameters of positive reals, α = (αi)K
i=1, that select a speciﬁc Dirichlet distri-
bution from the family. For a variable θ ∈[0, 1]K, the pdf D(θ | α) is given by
the following:
D(θ | α) = Γ(
i αi)

i Γ(αi)

i
θαi−1
i
(where Γ is the Gamma function, Γ(z) =
 ∞
0
dt tz−1e−t, for z > 0). Deﬁne
[α]= 
j αj; the expected value and variance of each parameter θi are given by
ED(θ|α)(θi) = αi
[α],
σ2
D(θ|α)(θi) = αi([α] −αi)
[α]2([α] + 1)
A conjugate prior. Consider sequences of independent experiments with K’ary
outcomes (K ∈N), each yielding outcome i with some ﬁxed probability θi; let us
call such experiments multinomial trials (in our framework, such experiments will
correspond to probabilistic event-choices at a cell). Let λD denote background
information encoding this. Let Xi, for i = 1, 2, . . . , n, represent the ith trial, i.e.,

From Simulations to Theorems
103
Xi = j is the statement that the ith trial has outcome j ∈{1, 2, . . ., K}. Let X
be a conjunction of n statements, (Zi)n
i=1, of the form:
Zi ≡(Xi = ji),
where each ji ∈{1, . . . , K}.
Suppose there are mj statements of the form Xi = j, and let θ = (θi)K
i=1. Then,
by deﬁnition of multinomial trials, we have the following likelihood:
P(X | θλD) =
n

i=1
P(Zi | θλD) =
K

i=1
θmi
i
.
The Dirichlet distributions constitute a family of conjugate prior distributions
for this likelihood. In other words, if the prior distribution on θ, say g(θ | λD), is
a Dirichlet D(θ | α), for α = (αi)K
i=1, then the posterior given data X (obtained
via Bayes’ Theorem), g(θ | XλD), is also Dirichlet. In the language equations:
g(θ | XλD) = g(θ | λD)P(X | θλD)
P(X | λD) .
In fact, it is not hard to show that g(θ | XλD) = D(θ | α1 + m1, . . . , αK + mK).
Note, choosing αi = 1 (for all i) in the prior gives the uniform prior
distribution.
The predictive probability (λD). Now, let Zn+1 ≡(Xn+1 = i), then one can
interpret P(Zn+1 | XλD) as a predictive probability: Given no direct knowledge
of θ, but only past evidence (X) and the model (λD), then P(Zn+1 | XλD) is
the probability that the next trial will result in a type i outcome. It is easy to
show that:
P(Zn+1 | XλD) = Eg(θ|XλD)(θi) = αi + mi
[α] + n
(since g(θ | XλD) is D (θ | (α1 + m1, α2 + m2, . . . , αK + mK)) and 
i mi = n).
To summarize, in the Dirichlet model, λD, one can compute the probability
of outcome i in the next multinomial trial as the expectation of the ith param-
eter of the Dirichlet pdf g(θ | XλD) which results via Bayesian updating given
history X.
3.2
Dirichlets on Cells
Let us return to our probabilistic model. For each cell c ∈C(ES) we will as-
sociate a prior distribution on the parameters θc determining the behavior of a
ﬁxed principal for the events of c. As we interact, we obtain data about these
parameters, and the distribution on each cell is updated via Bayes’ Theorem.
Each cell c ∈C(ES) presents a choice between the mutually exclusive and ex-
haustive events of c, and by the assumptions of λDES a sequence of such choices
from c is a sequence multinomial trials. We use Dirichlet priors on each cell so
that the posterior distributions are also Dirichlets. At any time, we obtain the

104
K. Krukow and M. Nielsen
predictive probability of the next interaction resulting in a particular conﬁgura-
tion by multiplying the expectations (according to the current cell distributions)
of the parameters for each event in the conﬁguration.
Let us be precise: Let fc(θc | λDES) denote the prior distribution on the
parameters for each cell c ∈C(ES) (when interacting with a ﬁxed principal).
Let αc be a vector of positive real numbers of size Nc = |c|; we take,
fc(θc | λDES) = D(θc | αc) = Γ(Nc
i=1 αc,i)
Nc
i=1 Γ(αc,i)
Nc

i=1
θαc,i−1
c,i
For example, taking αc,j = 1 (for all j) gives the uniform distribution. Let
X : E →N be an event count modeling data about past runs with a speciﬁc
principal. Let Xc = X|c (i.e., the restriction of X to cell c), then the posterior
pdf is given by the following: Assume that c = {e1, e2, . . . , eNc} then,
fc(θc | XλDES) = Γ(Nc
i=1 αc,i + X(ei))
Nc
i=1 Γ(αc,i + X(ei))
Nc

i=1
θαc,i+X(ei)−1
c,i
= D(θc | αc + Xc)
Hence, each event count X : E →N can be used to do Bayesian updating of the
distribution at each cell.
The predictive probability (λDES). By Bayesian updating, we obtain a
Dirichlet distribution for each cell c of ES. Let X be an event count correspond-
ing to n previously observed conﬁgurations, and let Z be the proposition that
“the (n+1)’st interaction results in outcome i” (where 1 ≤i ≤M and M is the
number of maximal conﬁgurations in ES). Let xi be the i’th maximal conﬁgu-
ration, and for e ∈xi let c(e) denote the unique cell c with e ∈c. The predictive
probability is the product of the expectations of each of the cell parameters.
P(Z | XλDES) =

e∈xi
Efc(e)(θc(e)|XλDES)(θc(e),e) =

e∈xi
αc(e),e + X(e)
[αc(e)] + X[c(e)]
3.3
Summary
We have presented a probabilistic model λDES based on probabilistic confusion-
free event structures. The model generalizes previous work on probabilistic mod-
els using binary outcomes and beta priors. In our model, given a past history
with a principal we need only remember the event counts of the past, i.e., a
function X : E →N. Given such an event count, there is a unique probability of
any particular conﬁguration occurring as the next interaction. We have derived
equations for this probability and it is easily computed in real systems.
For example, suppose we have the following event count, X.
c : 7

d : 1
e : 3

f : 5
a : 2

b : 8













								

From Simulations to Theorems
105
With the following prior pdfs for the cells:
fab((θa, θb) | λDES) = D((θa, θb) | (1, 1)),
fcd((θc, θd) | λDES) = D((θc, θd) | (1, 1)) and
fef((θe, θf) | λDES) = D((θe, θf) | (1, 1));
this count gives rise to three updated Dirichlets
fab((θa, θb) | XλDES) = D((θa, θb) | (1 + 2, 1 + 8)),
fcd((θc, θd) | XλDES) = D((θc, θd) | (1 + 7, 1 + 1)) and
fef((θe, θf) | XλDES) = D((θe, θf) | (1 + 3, 1 + 5)).
As an example, the probability of conﬁguration {b, c} is
P({b, c} | XλDES) = 9
12 × 8
10 = 3
5.
4
Advantages of Probabilistic Models:
A Preliminary Idea
While the purpose of models may not be to ﬁt the data but to sharpen the
questions, good models must do both! Our probabilistic models must be more
realistic. For example, the beta model of principal behavior (which we consider
to be state-of-the-art) assumes that for each principal p there is a single ﬁxed
parameter θp so at each interaction, independently of anything else we know,
there is probability θp for a ‘good’ outcome and probability 1 −θp for a ‘bad’
outcome. For some applications, one might argue that this is unrealistic, e.g.:
(i) The parameter θp is ﬁxed, independent of time, i.e., no dynamic behavior;
and (ii) principal p’s behavior when interacting with us is likely to depend on
our behavior when interacting with p; let us call this property ‘recursive be-
havior.’ (Note, the same issues are present in the Dirichlet model λD and the
Dirichlet-Event-Structure model λDES that we propose). Some beta-based rep-
utation systems attempt to deal with the ﬁrst problem by introducing so-called
“forgetting factors;” essentially this amounts to choosing a number 0 ≤δ ≤1,
and then each time the parameters (α, β) of the pdf for θp are updated, they
are also scaled with δ, e.g., when observing a single ‘good’ interaction, (α, β)
become (αδ + 1, βδ). In eﬀect, this performs a form of exponential “decay” on
the parameters. The idea is that information about old interactions should weigh
less than information about new ones; however, this represent a departure from
the probabilistic beta model, where all interactions “weigh the same.” Since a
new model is not introduced, i.e., to formalize this preference towards newer
information, it is not clear what the exact beneﬁts of forgetting factors are, e.g.,
why exponential decay as opposed to linear? As far as we know, no-one has
considered the ‘recursive behavior’ problem before.
The notion of context is also relevant for computational trust models, as
have been recognized by many. Given a single-context model, one can obtain a

106
K. Krukow and M. Nielsen
multi-context model by instantiating the single-context model in each context.
However, as Sierra and Sabater [2] argue, this is too naive: The goal of a true
multi-context model is not just to model multiple contexts, but to provide the
basis for transferring information from one context to another related context.
To our knowledge, there are no techniques that deal with this problem within
the ﬁeld of trust and reputation.
Finally, we believe (as do Sierra and Sabater [2]) that our ﬁeld is lacking a
way of comparing the qualities of the many proposed trust-based systems. Sierra
and Sabater propose that our ﬁeld develop “(. . . ) test-beds and frameworks to
evaluate and compare the models under a set of representative and common con-
ditions” [2]. We agree with Sierra and Sabater (note that “a set of representative
and common conditions” could be a formal probabilistic model).
In the following, we sketch ideas towards solving this last problem: We de-
velop what one might call “a theoretical test-bed” for comparing systems for
probabilistic trust computation.
4.1
Towards Comparing Probabilistic Trust-Based Systems
We shall propose a generic measure to “score” speciﬁc probabilistic trust-based
systems in a particular environment (i.e., “a set of representative and common
conditions”). The score, which is based on the so-called Kullback-Leibler diver-
gence, is a measure of how well an algorithm approximates the “true” probabilis-
tic behavior of principals.
Consider a probabilistic model of principal behavior, say λ. We consider only
the behavior of a single ﬁxed principal p, and we consider only algorithms that
attempt to solve the following problem: Suppose we are given an interaction
history X = [(x1, t1), (x2, t2), . . . , (xn, tn)] obtained by interacting n times with
principal p, observing outcome xi at time ti. Suppose also that there are m
possible outcomes (y1, . . . , ym) for each interaction. The goal of a probabilistic
trust-based algorithm, say A, is to approximate a distribution on the outcomes
(y1, . . . , ym) given this history X. That is, A satisﬁes:
A(yi | X) ∈[0, 1] (for all i),
m

i=1
A(yi | X) = 1.
We assume that the probabilistic model, λ, deﬁnes the following probabilities:
P(yi | Xλ), i.e., the probability of “yi in the next interaction given a past history
of X” and P(X | λ), i.e., the “a priori probability of observing sequence X in
the model.”3
Now, (P(yi | Xλ) | i = 1, 2, . . ., m) deﬁnes the true distribution on outcomes
for the next interaction (according to the model); in contrast, (A(yi | X) |
i = 1, 2, . . . , m) attempts to approximate this distribution. The Kullback-Leibler
3 In a way, this model takes into account also the ‘recursive behavior’ problem: The
probabilities P(yi | Xλ) and P(yi | λ) are distinguished. We have not yet given this
further thoughts.

From Simulations to Theorems
107
divergence [15], which is closely related to Shannon entropy, is a measure of the
distance from a true distribution to an approximation of that distribution. The
Kullback-Leibler divergence from distribution ˆp = (p1, p2, . . . , pm) to distribution
ˆq = (q1, q2, . . . , qm) on a ﬁnite set of m outcomes, is given by
DKL(ˆp || ˆq) =
m

i=1
pi log2(pi
qi
)
(any log-base could be used). The Kullback-Leibler divergence is almost a dis-
tance (in the mathematical sense), but the symmetry property fails. That is DKL
satisﬁes DKL(ˆp || ˆq) ≥0 and DKL(ˆp || ˆq) = 0 only if ˆp = ˆq. The asymmetry comes
from considering one distribution as “true” and the other as approximating.
For each n let On denote the set of interaction histories of length n. Let us
deﬁne, for each n, the n’th expected Kullback-Leibler divergence from λ to A:
Dn
KL(λ || A)
(def)
=

X∈On
P(X | λ)DKL(P(· | Xλ) || A(· | X)),
that is,
Dn
KL(λ || A) =

X∈On
P(X | λ)(
m

i=1
P(yi | Xλ) log2(P(yi | Xλ)
A(yi | X) )).
Note that, for each input sequence X ∈On to the algorithm, we evaluate its
performance as DKL(P(· | Xλ) || A(· | X)); however, we accept that some
algorithms may perform poorly on very unlikely training sequences, X. Hence,
we weigh the penalty on input X, i.e., DKL(P(· | Xλ) || A(· | X)), with the
intrinsic probability of sequence X; that is, we compute the expected Kullback-
Leibler divergence.
The Kullback-Leibler divergence is a well-established measure in statistic;
however, to our knowledge, the measure Dn
KL on probabilistic algorithms is
new. Due to the relation to Shannon’s Information Theory, one can interpret
Dn
KL(λ || A) quantitatively as the expected number of bits of information one
would gain if one would know the true distribution instead of A’s approximation
on n-length training sequences.
An example. For an example of our measure, we compare the beta-based
algorithm of Mui et al. [10] with the maximum-likelihood algorithm of Aberer
and Despotovic [16]. We can compare these because they both deploy the same
fundamental assumptions:
Assume that the behavior of each principal is so that there is a ﬁxed
parameter such that at each interaction we have, independently of any-
thing we know about other interactions, the probability θ for a ‘success’
and therefore probability 1 −θ for ‘failure.’
This gives us the beta model, λB. Let s stand for ‘success’ and f stand for ‘failure,’
and let X ∈{s, f}n for some n > 0.

108
K. Krukow and M. Nielsen
We have the following likelihood for any X ∈{s, f}n:
P(X | λBθ) = θNs(X)(1 −θ)Nf (X)
(where Nx(X) denotes the number of x occurrences in X).
Let A denote the algorithm of Mui et al., and let B denote the algorithm of
Aberer and Despotovic. Then,
A(s | X) = Ns(X) + 1
n + 2
and A(f | X) = Nf(X) + 1
n + 2
,
and it is easy to show that
B(s | X) = Ns(X)
n
and B(f | X) = Nf(X)
n
.
For each choice of θ ∈[0, 1], and each choice of training-sequence length, we can
compare the two algorithms by computing and comparing Dn
KL(λBθ || A) and
Dn
KL(λBθ || B). For example:
Theorem 1. If θ = 0 or θ = 1 then the algorithm B of Aberer and Despotovic
[16] computes a better approximation of principal behavior than the algorithm A
of Mui et al. [10]. In fact, B always computes the exact probability of success on
any possible training sequence.
Proof. Assume that θ = 0, and let n > 0. The only sequence of length n which
has non-zero probability is f n, and we have B(f | f n) = 1; in contrast, A(f |
f n) = n+1
n+2, and A(s | f n) =
1
n+2). Since P(s | f nλBθ) = θ = 0 = B(s | f n) and
P(f | f nλBθ) = 1 −θ = 1 = B(f | f n), we have
Dn
KL(λBθ || B) = 0.
Since Dn
KL(λBθ || A) > 0 we are done (the argument for θ = 1 is similar).
⊓⊔
Now let us compare A and B with 0 < θ < 1. Since B assigns probability 0
to s on input f k (for all k ≥1) which results in Dn
KL(λ || B) = ∞, then ac-
cording to our measure Dn
KL, algorithm A is always better than B. However,
this results from a property of the Kullback-Leibler measure: Given two distri-
bution ˆp = (p1, . . . , pn) and ˆq = (q1, . . . , qn), if one of the “real” probabilities, pi
is non-zero and the corresponding “approximating” probability qi is zero, then
DKL(ˆp || ˆq) = ∞. To obtain a stronger and more informative result, we shall
consider a continuum of algorithms, denoted Aϵ for a real number 0 < ϵ < 1,
deﬁned as
Aϵ(s | X) = Ns(X) + ϵ
n + 2ϵ
and Aϵ(f | X) = Nf(X) + ϵ
n + 2ϵ
.
One can think of Aϵ as approximating B(= A0) for small epsilon.
We have the following theorem which compares Aϵ and the algorithm of
Mui et al., A, in a continuum of diﬀerent environments.

From Simulations to Theorems
109
Theorem 2. Let λB be the beta model with parameter θ ∈[ 1
2 −
1
√
12, 1
2 +
1
√
12].
For any n ≥0 we have
Dn
KL(λBθ || A) < Dn
KL(λBθ || Aϵ),
for all ϵ ∈(0, 1).
Proof. See the full paper, to appear soon.
⊓⊔
What does this mean? Another way to put it is that if θ is in the interval
[.215, .785] (approximately), then independently of training-sequence length (n),
then the algorithm of Mui et al. is better (on average) than any algorithm Aϵ
(for 0 < ϵ < 1). To our knowledge, this is the ﬁrst theorem which compares
two algorithms for trust computation: All previous comparisons have been via
computer simulations. In fact, it is not so much the concrete comparison of algo-
rithms A and B that interests us; rather, our message is that using probabilistic
models enables the possibility of such theoretical comparisons. Notice that with-
out formal probabilistic models we would be unable to even state precisely such
theorems.
5
Conclusion
Our “position” on computational trust research is that any proposed system
should be able to answer two fundamental questions precisely: What are the
assumptions about the intended environments for the system? And, what is the
objective of the system? An advantage of formal probabilistic models is that
they enable rigorous answers to these questions. To illustrate this point, we have
presented an example of a formal probabilistic model, λDES. There are other
examples: The beta model speciﬁes the assumption of the computational trust
model of Jøsang et al. [9], and under these assumptions their algorithm computes
the probability of a principal well-behaving in the next interaction.
There are further beneﬁts of formal probabilistic models: As we have illus-
trated, it is possible to compare two algorithms, say X and Y, under the same
type of principal behavior, say model λ, by examining which algorithm best
approximates the true principal behavior (as speciﬁed by λ). For example, we
propose to compute and compare:
Dn
KL(λ || X) and Dn
KL(λ || Y).
Note, no simulations of algorithms X and Y are necessary; the numbers give
a theoretical justiﬁcation, e.g., stating that “in environment λ, on the average,
algorithm X outperforms algorithm Y on training sequences of length n.” If one
can further show that this holds for all n, or for all n greater than some number,
this gives a way of saying that X is better than Y. Another type of property one
might desire is the following: Suppose X satisﬁes for each ϵ > 0 there exists an

110
K. Krukow and M. Nielsen
N > 0 so that for all n ≥N we have Dn
KL(λ || X) < ϵ. This means that given
a long enough training sequence, algorithm X approximates the true principal
behavior to an arbitrary precision.
We have further results which will be published in the full paper. We consider
Aϵ for all epsilon, not just [0, 1]. We show that for each choice of θ there is an
optimal ϵθ for which Aϵθ is best among algorithms Aϵ. Recall that the results
of Section 4 were all based on the simple beta model, λB. We illustrate how our
measure is parametric by considering a probabilistic model of dynamic principal
behavior based on Hidden Markov Models. We show how one can use our measure
“out-of-the-box” to compare algorithms working in this model.
Acknowledgments. We thank the anonymous reviewers for pointing to several
places where clariﬁcation of our position and intention was necessary.
References
1. Despotovic, Z., Aberer, K.: P2P reputation management: Probabilistic estimation
vs. social networks. Computer Networks 50(4), 485–500 (2006)
2. Sabater, J., Sierra, C.: Review on computational trust and reputation models.
Artiﬁcial Intelligence Review 24(1), 33–60 (2005)
3. Krukow, K.: Towards a Theory of Trust for the Global Ubiquitous Computer.
PhD thesis, University of Aarhus, Denmark (2006) available online (submitted)
http://www.brics.dk/~krukow
4. Gambetta, D.: Can we trust trust? In Gambetta, D. (ed.) Trust: Making and
Breaking Cooperative Relations. University of Oxford, Department of Sociology,
pp. 213–237. ch. 13. Electronic edition (2000)
http://www.sociology.ox.ac.uk/papers/gambetta213-237.pdf
5. Jaynes, E.T.: Probability Theory: The Logic of Science. In: The Edinburgh Build-
ing, Cambridge, CB2 2RU, United Kingdom, Cambridge University Press, Cam-
bridge (2003)
6. Nielsen, M., Krukow, K.: On the formal modelling of trust in reputation-based
systems. In: Karhum¨aki, J., Maurer, H., P˘aun, G., Rozenberg, G. (eds.) Theory Is
Forever: Essays Dedicated to Arto Salomaa on the Occasion of His 70th Birthday
LNCS, vol. 3113, pp. 192–204. Springer, Heidelberg (2004)
7. Cahill, V., Seigneur, J.M.: The SECURE website (2004)
http://secure.dsg.cs.tcd.ie
8. Cahill, V., Gray, E., et al.: Using trust for secure collaboration in uncertain envi-
ronments. IEEE Pervasive Computing 2(3), 52–61 (2003)
9. Jøsang, A., Ismail, R.: The beta reputation system. In: Proceedings from the 15th
Bled Conference on Electronic Commerce, Bled. (2002)
10. Mui, L., Mohtashemi, M., Halberstadt, A.: A computational model of trust and
reputation (for ebusinesses). In: HICSS’02. Proceedings from 5th Annual Hawaii
International Conference on System Sciences, p. 188. IEEE, Orlando, Florida, USA
(2002)
11. Buchegger, S., Le Boudec, J.Y.: A Robust Reputation System for Peer-to-Peer and
Mobile Ad-hoc Networks. In: P2PEcon 2004 (2004)

From Simulations to Theorems
111
12. Teacy, W.T.L., Patel, J., Jennings, N.R., Luck, M.: Coping with inaccurate reputa-
tion sources: experimental analysis of a probabilistic trust model. In: AAMAS ’05.
Proceedings of the fourth international joint conference on Autonomous agents and
multiagent systems, New York, NY, USA, pp. 997–1004. ACM Press, New York,
NY, USA (2005)
13. Krukow, K., Nielsen, M., Sassone, V.: A logical framework for reputation systems.
(2006) Submitted. Available online www.brics.dk/~krukow
14. Varacca, D., V¨olzer, H., Winskel, G.: Probabilistic event structures and domains.
In: Gardner, P., Yoshida, N. (eds.) CONCUR 2004. LNCS, vol. 3170, pp. 481–496.
Springer, Heidelberg (2004)
15. Kullback, S., Leibler, R.A.: On information and suﬃciency. Annals of Mathematical
Statistics 22(1), 79–86 (1951)
16. Despotovic, Z., Aberer, K.: A probabilistic approach to predict peers’ performance
in P2P networks. In: Klusch, M., Ossowski, S., Kashyap, V., Unland, R. (eds.) CIA
2004. LNCS (LNAI), vol. 3191, pp. 62–76. Springer, Heidelberg (2004)

A Tool for the Synthesis of Controller Programs⋆
Ilaria Matteucci
Istituto di Informatica e Telematica - C.N.R., Pisa, Italy
Dipartimento di Scienze Matematiche ed Informatiche, Universit`a degli Studi di Siena
Ilaria.Matteucci@iit.cnr.it
Abstract. In previous works we have developed a theory based on formal meth-
ods for enforcing security properties by deﬁning process algebra controller op-
erators. In this paper we continue our line of research, by describing a tool
developed for synthesizing a model for a given security property that is also a
control program for a given controller operator. The tool implements the partial
model checking technique and the satisﬁability procedure for a modal μ-calculus
formula.
1
Overview
In the last few years the amount of information and sensible data that circulate on the
net has been growing up. This is one of important reasons that have contribute to in-
crease research on the deﬁnition of formal methods for the analysis and the veriﬁcation
of secure systems, that are systems that satisfy some security properties that specify ac-
ceptable executions of programs. The interest in this topic is mainly due to the practical
relevance of these systems and moreover to preliminary encouraging results achieved
by the application of formal methods to security properties analysis.
More recently there also has been interest on developing techniques to study how to
enforce security policy (e.g., see [3,4,5,11,12,17]).
In [12,13,14] we have given a methodology, based on known techniques in concur-
rency and process logics theory, for automatically enforcing a desired security property.
In particular, we have shown how to secure a system S with a possible un-speciﬁed
component X, through the usage of a controller operator Y  X, where Y is a con-
troller program that prevents X to express a behavior that, in cooperation with S, could
make the overall system violate a security property φ.
In this paper we continue our line of research by showing the implementation of a
tool that is effectively able to generate a controller program Y starting from a system S
and φ.
The tool is made up of two main parts, the ﬁrst one is the MuDiv tool developed by
Nielsen and Andersen, that implements the partial model checking function for process
algebra operators (see [1,2]). It takes in input a system S and a formula of equational μ-
calculus, φ, and calculate φ′ = φ//S that is the partial evaluation of φ w.r.t. the system
⋆Work partially supported by CNR project “Trusted e-services for dynamic coalitions”
and by EU-funded project “Software Engineering for Service-Oriented Overlay Comput-
ers”(SENSORIA) and by EU-funded project “Secure Software and Services for Mobile Sys-
tems ”(S3MS).
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 112–126, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

A Tool for the Synthesis of Controller Programs
113
S. The second part is developed in O’caml 3.09 (see [9]) and implements a satisﬁability
procedure in order to generate a model for φ′ that is also a controller program for a
given controller operator. In particular, it implements the satisﬁability procedure devel-
oped by Walukiewicz in [19] for a modal μ-calculus formula. It takes in input φ′, that is
the output of the MuDiv tool, and, by exploiting the satisﬁability procedure, generates
a model for it that is also a controller program for chosen controller operator. It is im-
portant to note that, in order to generate a control program for a controller operator that
models a security automaton (see [4,5]), we have to apply a relabeling function. Hence
we have implemented relabeling functions, one for each kind of security automata.
The satisﬁability procedure is given for a modal μ-calculus formula while the Mu-
Div tool works on equational μ-calculus formulae. Hence we have developed a trans-
lation function that permits to translate an equational μ-calculus formula into a modal
μ-calculus one.
This paper is organized as follows. Section 2 recalls basic theory about process alge-
bras, modal logic and the partial model checking technique. Section 3 brieﬂy explains
our theory for the synthesis of process algebra controller operators able to enforce secu-
rity properties. Section 4 describes our synthesis tool and Section 5 presents an example
of application. Eventually, Section 6 concludes the paper.
2
Background
2.1
CCS Process Algebra
CCS of Milner (see [15]) is a language for describing concurrent systems. Here, we
present a formulation of Milner’s CCS in the Generalized Structured Operational Se-
mantics (GSOS) (see [6]) format1.
Let L be a ﬁnite set of actions, ¯L = {¯a | a ∈L} be the set of complementary actions
where¯is a bijection with ¯¯a = a, Act be L ∪¯L ∪{τ}, where τ is a special action that
denotes an internal action.
The syntax of CCS is the following:
E ::= 0 | A | a.E | E1 + E2 | E1∥E2 | E\L | E[f]
where L ⊆Act. To give a formulation of CCS dealing with GSOS, we deﬁne the
function arity ≥0 as follows: arity(0) = 0, ∥and + are binary operators and the other
ones are unary operators.
We will often use some common syntactic simpliﬁcations, e.g., omission of trailing
0’s as well as omission of brackets on restriction on a single action. Der(E) denotes
the set of derivatives of a (closed) term E, i.e. the set of process that can be reached
through the transition relation.
The operational semantics of CCS closed terms is given by means of the GSOS
system in Table 1 and it is described by a labeled transition system, LT S for short. An
LT S, is a structure of the form (E, T ) where E is the set of terms and T is a ternary
1 We decide to omit the description of GSOS framework that can be found in [6]. The reader is
expected to be familiar with the SOS notation used for operational semantics.

114
I. Matteucci
Table 1. GSOS system for CCS
Preﬁxing:
a.x
a
−→x
Choice:
x
a
−→x′
x + y
a
−→x′
y
a
−→y′
x + y
a
−→y′
Parallel:
x
a
−→x′
x∥y
a
−→x′∥y
y
a
−→y′
x∥y
a
−→x∥y′
x
l
−→x′ y
¯l
−→y′
x∥y
τ
−→x′∥y′
Restriction:
x
a
−→x′
x\L
a
−→x′\L
a ̸∈L
Relabeling:
x
a
−→x′
x[f]
f(a)
−→x′[f]
relation T ⊆(E × Act × E), known as a transition relation. The transition relation is
deﬁned by structural induction as the least relation generated by the set of GSOS rules
of Table 1. CCS operators have the following informal meaning:
– 0 is a process that does nothing;
– a.E (preﬁx) is a process that can perform an a action and then behaves as E;
– E1+E2 (choice) represents the nondeterministic choice between the two processes
E1 and E2;
– E1∥E2 (parallel) is the parallel composition of two processes that can proceed in
an asynchronous way, synchronizing on complementary actions, represented by an
internal action τ, to perform a communication;
– E\L (restriction) is the process E when actions in L ∪L are prevented;
– E[f] (relabeling) is the process E when names of actions are changed according to
the function f : Act →Act.
2.2
Behavioral Equivalences
Several behavioral relations are deﬁned to compare the behavior of processes. Here we
recall the deﬁnition of weak simulation and weak bisimulation.
First of all we give the notion of observational relations as follows: E
τ⇒E′ (or
E ⇒E′) if E
τ→
∗E′ (where
τ→
∗is the reﬂexive and transitive closure of the
τ→
relation); for a ̸= τ, E
a⇒E′ if E
τ⇒
a→
τ⇒E′2.
The weak bisimulation relation (see [16]) permits to abstract to some extent from the
internal behavior of the systems, represented by the invisible τ actions.
2 Note that it is a short notation for E
τ⇒Eτ
a→E′
τ
τ⇒E′ where Eτ and E′
τ denote interme-
diate states that is not important for this framework.

A Tool for the Synthesis of Controller Programs
115
Table 2. Modal μ-calculus
[[T]]ρ = S
[[F]]ρ = ∅
[[X]]ρ = ρ(X)
[[¬A]]ρ = S\[[A]]ρ
[[A1 ∧A2]]ρ = [[A1]]ρ ∩[[A2]]ρ
[[A1 ∨A2]]ρ = [[A1]]ρ ∪[[A2]]ρ
[[⟨a⟩A]]ρ = {s|∃s′ : s
a
−→s′ ands′ ∈[[A]]ρ}
[[[a]A]]ρ = {s|∀s′ : s
a
−→s′ impliess′ ∈[[A]]ρ}
[[μX.A]]ρ = {S′|[[A]]ρ[S′/X] ⊆S′}
[[νX.A]]ρ = {S′|S′ ⊆[[A]]ρ[S′/X]}
Deﬁnition 1. Let (E, T ) be an LTS of concurrent processes, and let R be a binary
relation over E. Then R is called weak simulation, denoted by ⪯, over (E, T ) if and
only if, whenever (E, F) ∈R we have:
if E
a
−→E′ then there exists F ′ s.t. F
a
=⇒F ′ and (E′, F ′) ∈R,
A weak bisimulation is a relation R s.t. both R and R−1 are weak simulations. We
represent with ≈the union of all the weak bisimulations.
2.3
Modal Logic: Two Variants of μ-Calculus
Modal μ-calculus is a process logic well suited for speciﬁcation and veriﬁcation of sys-
tems whose behavior is naturally described by using state changes by means of actions.
It permits to express a lot of interesting properties like safety (“nothing bad happens”)
and liveness (“something good happens”) properties, and it allows us to express equiv-
alence conditions over LT S.
Let a be in Act, X be a variable ranging over a set of variables V ar. Modal μ-
calculus formulae are generated by the following grammar:
A ::= X | T | F | ¬A | A1 ∧A2 | A1 ∨A2 | ⟨a⟩A | [a]A | μX.A | νX.A
where the meaning of ⟨a⟩A is “it is possible to do an a-action to a state where A holds”
and the meaning of [a]A is “for all a-actions that are performed then A holds”. We con-
sider the usual deﬁnitions of bound and free variables. The interpretation of μX.α(X)
(νX.α(X)) is the least (greatest) ﬁxpoint of this function.
Formally, given an LT S, let S be the set of states of it, the semantics of a formula A
is a subset [[A]]ρ of S, as deﬁned in Table 2, where ρ is a function (called environment)
from free variables of A to subsets of S. The environment ρ[S′/X](Y ) is equal to ρ(Y )
if Y ̸= X, otherwise ρ[S′/X](X) = S′. The denotational semantics is given in Table 2.
Equational μ-calculus is a variant of μ-calculus that is very suitable for partial model
checking, that is described later (see [1,2]).

116
I. Matteucci
In equational μ-calculus recursion operators are replaced by ﬁxpoint equations. This
permits to recursively deﬁne properties of a given system. Hence the grammar is the
same given before in which μX.A and νX.A are replaced by the following ﬁxpoint
equations:
D ::= X =ν AD | X =μ AD | ϵ
X =ν A is a maximal ﬁxpoint equation, where A is an assertion (i.e. a simple modal
formula without recursion operator), and X =μ A is a minimal ﬁxpoint equation.
Roughly, the semantics D of the list of equations D is the solution of the system
of equations corresponding to D. According to this notation, D(X) is the value of
the variable X, and E |= D ↓X can be used as a short notation for E ∈D(X). The
formal semantics of equational μ-calculus is the same as the one given in Table 2 in
which the semantics of recursion operators is replaced by the semantics of ﬁxed point
equations. As a matter of fact, let ⊔be the symbol that represents union of disjoint en-
vironments. Let ρ be the environment and σ be in {μ, ν}, then σU.f(U) represents the
σ ﬁxpoint of the function f in one variable U.
ϵρ = []
X =σ AD′ρ = D′(ρ⊔[U′/X]) ⊔[U ′/X]
where U ′ = σU.A′
(ρ⊔[U/X]⊔ρ′(U)) and ρ′(U) = D′(ρ⊔[U/X]).
It informally says that the solution to (X =σ A)D is the σ ﬁxpoint solution U ′ of
A where the solution to the rest of the lists of equations D is used as environment.
For both of these logics the following theorem holds.
Theorem 1 ([18]). Given a formula φ it is possible to decide in exponential time in the
length of φ if there exists a model of φ and it is also possible to give an example of such
model.
2.4
Partial Model Checking
Partial model checking is a technique that relies upon compositional methods for prov-
ing properties of concurrent system. It has been ﬁrstly introduced by Andersen (see
[1,2]). The intuitive idea underlying partial evaluation is the following: proving that
(P∥Q) satisﬁes an equational μ-calculus formula φ is equivalent to proving that Q sat-
isﬁes a modiﬁed formula φ′ = φ//P , where //P is the partial evaluation function for
the operators of parallel composition. In Table 3 we give the deﬁnition of the partial
evaluation function for the CCS parallel and relabeling operators. Andersen proves the
following lemma:
Lemma 1 ([2]). Given a process P∥Q and a formula φ we have: P∥Q |= φ iff Q |=
φ//P .
A lemma similar to the previous one holds for all CCS operators (see [1,2]). It is worth
noticing that partial model checking function may be automatically derived from the
semantics rules used to deﬁne a language semantics. Thus, the proposed technique is
very ﬂexible.

A Tool for the Synthesis of Controller Programs
117
Table 3. Partial evaluation function for parallel operator and relabeling operator
Parallel:
(D↓X)//t = (D//t)↓Xt
ϵ//t = ϵ
(X =σ AD)//t = ((Xs =σ A//s)s∈Der(E))(D)//t
X//t = Xt
[a]A//s = [a](A//s) ∧
s a
−→s′ A//s′, if a ̸= τ
A1 ∧A2//s = (A1//s) ∧(A2//s)
⟨a⟩A//s = ⟨a⟩(A//s) ∨
s
a
−→s′ A//s′, if a ̸= τ
A1 ∨A2//s = (A1//s) ∨(A2//s)
[τ]A//s = [τ](A//s) ∧
s τ
−→s′ A//s′ ∧
s a
−→s′ [a](A //s′)
⟨τ⟩A//s = ⟨τ⟩(A//s) ∨
s τ
−→s′ A//s′ ∨
s a
−→s′⟨¯a⟩(A//s′)
T//s = T
F//s = F
Relabeling:
X//[f] = X
(X =σ AD)//[f] = (X =σ A//[f](D)//[f])
⟨a⟩A//[f] = 
b:f(b)=a⟨b⟩(A//[f])
[a]A//[f] = 
b:f(b)=a[b](A//[f])
A1 ∧A2//[f] = (A1//[f]) ∧(A2//[f])
A1 ∨A2//[f] = (A1//[f]) ∨(A2//[f])
T//[f] = T
F//[f] = F
3
Controllers Synthesis Theory
Starting from works of Schneider ([17]) and Ligatti & al. ([4,5]), in [12,13,14] we
have deﬁned process algebra operators, called controller operators. We denote them
by Y ▷X, where X is an unspeciﬁed component (target) and Y is a controller pro-
gram. The controller program is the process that controls X in order to guarantee that
a given security property is satisﬁed. Moreover we are able to automatically synthesize
a controller program for a controller operator.
In this section we recall our theory, then, in the next one, we will describe the tool
based on it.
Let S be a system and let X be the target that works in parallel with S, S∥X. We
want to guarantee that, given a security property φ, the system S∥X satisﬁes it. Hence
we use controller operator in such way the speciﬁcation of the system becomes:
∃Y
∀X
s.t.
S∥(Y ▷X) |= φ
(1)
By partially evaluating φ w.r.t. S by the usage of partial model checking the Formula
(1) is reduced as follows:
∃Y
∀X
Y ▷X |= φ′
(2)
where φ′ = φ//S.
In [12,13] we have dealt with security automata (truncation, suppression, insertion,
edit) deﬁned in [4,5] by modeling them by process algebra controller operators Y ▷K X,
where K ∈{T, S, I, E}.3 We have proved that for every K ∈{T, S, I, E} Y ▷K X ⪯
Y [fK] holds, where fK is a relabeling function depending on K. In particular, fT is the
identity function on Act4 and
fS(a) =
τ if a = −a
a othw
fI(a) =
τ if a = +a
a othw
fE(a) =
τ if a ∈{+a, −a}
a othw
3 T stays for Truncation, S for Suppression, I for Insertion and E for Edit.
4 Here the set Act must be consider enriched by control actions.

118
I. Matteucci
According to [4,5], these operators are applied in order to enforce safety properties. For
this class of formulae it is possible to prove that if E and F are two processes, such that
F ⪯E then E |= φ ⇒F |= φ. Hence, in order to satisfy the Formula (2) it is sufﬁcient
to prove the following one:
∃Y
Y |= φ′′
(3)
where φ′′ = φ′
//[fK] (see Table 3). In this case we obtain a satisﬁability problem in
μ-calculus, that can be solved by Theorem 1.
Other controller operators able to enforce not only safety properties but also infor-
mation ﬂow properties5 can be deﬁned (see [14]).
In this case we prove that if a controller operator Y ▷∗X satisfy the following
assumption:
For every X and Y
Y ▷∗X ≈Y .
Then the Formula (2) becomes:
∃Y
Y |= φ′
(4)
Also in this case we obtain a satisﬁability problem in μ-calculus that can be solved by
Theorem 1. Hence, by exploiting satisﬁability procedure for μ-calculus we are able to
synthesize appropriate controller program for a given controller operator.
3.1
Examples of Controller Operators
Here we give the semantics of some controller operators.
Controller operators for Safety properties
Truncation Y ▷T X
E
α→E′ F
a→F ′
E ▷T F
a→E′ ▷T F ′
If F and E perform the same action a, then E ▷T F performs it, otherwise the system
halts.
Suppression Y ▷S X
E
a→E′ F
a→F ′
E ▷S F
a→E′ ▷S F ′
E ̸
a
−→
E
−a
−→E′
F
a→F ′
E ▷S F
τ→E′ ▷S F ′
where −a is a control action not in Act (so it does not admit a complementary action).
If F and E perform the same action a then also E ▷S F performs it. On the contrary,
if F performs an action a that E does not perform, and E performs −a then E ▷S
F suppresses the action a by performing τ, i.e. a becomes not visible from external
observation.
5 To describe this property, we can consider two users, High and Low interacting with the same
computer system. We wonder if there is any ﬂow of information from High to Low. We can
ﬁnd several formal deﬁnitions in the literature, e.g. [7,10], for concurrent processes.

A Tool for the Synthesis of Controller Programs
119
Insertion Y ▷I X
E
a→E′′
F
a→F ′
E ▷I F
a→E′ ▷I F ′
E ̸
a→E′′
E
+a.b
−→E′
F
a→F ′
E ▷I F
b→E′ ▷I F
6
where +a is an action not in Act. If F and E perform an action a then also E ▷I F
performs it. If F performs an action a that E does not perform and E performs +a
followed by b, then the whole system performs b.
Edit Y ▷E X
E
a→E′
F
a→F ′
E ▷E F
a→E′ ▷E F ′
E ̸
a
−→
E
−a
−→E′
F
a→F ′
E ▷E F
τ→E′ ▷E F ′
E ̸
a→E′′
E
+a.b
→E′
F
a→F ′
E ▷E F
b→E′ ▷E F
This operator combines the power of the previous two ones. Hence it is able to suppress
and insert actions.
Controller operators for Information ﬂow properties. We deﬁne the controller
operator ▷′ by the following rules.
E
a→E′
F
a→F ′
E ▷′ F
a→E′ ▷′ F ′
a ̸= τ
E
a→E′
E ▷′ F
a→E′ ▷′ F
F
τ→F ′
E ▷′ F
τ→E ▷′ F ′
This operator forces the system to perform always the right action also if we do not
know what action the target is going to perform.
Another controller operator, ▷′′, can be deﬁned as follows.
E
a→E′ F
a→F ′
E ▷′′ F
a→E′ ▷′′ F ′
a ̸= τ
E
a→E′ F
a
̸→F ′
E ▷′′ F
a→E′ ▷′′ F
F
τ→F ′
E ▷′′ F
τ→E ▷′′ F ′
If E and F perform the same action a then the whole system performs it. On the con-
trary, the whole system performs the action performed by E. The τ action can be always
performed by both of the processes.
4
Controller Synthesis Tool
In order to solve the satisﬁability problem described by the Formula (3) and Formula
(4), we have developed a tool that, given a system S and a formula φ, generates a process
Y . This process is a model for φ′, the formula obtained by the partial evaluation of φ
by S. Moreover, such Y guarantees that S∥(Y ▷X) satisﬁes φ whatever X is. As a
matter of fact, according to the synthesis theory presented before, the tool is made up
of two main parts: the ﬁrst part implements the partial model checking function; the
second one implements the satisﬁability procedure developed by Walukiewicz in [19]
and generates a process Y for each controller operator we want to apply in order to
enforce the property φ.
6 This means E
+a
−→Ea
b
−→E′. However we consider +a.b as a single action, i.e. the state Ea
is hide.

120
I. Matteucci
−controllers.ml
MuDiv
b) A zoom of the Synthesis module
a) The architecture of the whole tool
Translator
−calc.ml
−fparser.ml
−flexer.ml
Synthesis
−convert.ml
−types_for.ml
−goodgraph.ml
−model.ml
−simplify.ml
−main.ml
−printGraph.ml
Synthesis
φ′ = φ//S
Y
Y
φ, S
φ′ = φ//S
φ′
mod
Fig. 1. Architecture of the tool
4.1
Architecture of the Tool
The tool is composed by several parts. We can divide them in two main modules. In
Figure 1 there is a graphical representation of the architecture of the whole tool. The
Figure 1.a) represents the whole tool by showing the two main modules, the MuDiv
module and the Synthesis module. The MuDiv tool implements the partial model check-
ing function. It has been developed in C++ by J.B. Nielsen and H.R. Andersen. The
MuDiv takes in input a process S described by an LT S and an equational μ-calculus
formula φ and returns an equational μ-calculus formula φ′ = φ//S. The Synthesis mod-
ule is developed in O’caml 3.09 (see [9]) and it is described better in Figure 1.b).
The Figure 1.b) shows the Synthesis module in more detail. It is composed by sev-
eral functions as we can see in the ﬁgure. Roughly, it is able to build a model for a
given modal μ-calculus formula by exploiting the satisﬁability procedure developed by
Walukiewicz in [19]. In order to describe this module we can see that it consists of two
submodules: the Translator and the Synthesis.
The Translator. manages the formula φ, output of the MuDiv tool and input of our
tool, by translating it from an equational to a modal μ-calculus formula. It consists in
four functions: fparser.ml and flexer.ml that permit to read the MuDiv out-
put ﬁle and analyze it as input sequence in order to determine its grammatical struc-
ture with respect to our grammar. The function calc.ml calls flexer.ml and
fparser.ml on a speciﬁed ﬁle. In this way we obtain an equational μ-calculus for-
mula φ′ according to the type that we have deﬁned in type for.ml. The last func-
tion, convert.ml, translates the equational μ-calculus formula φ′ in the modal one

A Tool for the Synthesis of Controller Programs
121
Table 4. System of rules
(and) ϕ1 ∧ϕ2, Γ ⊢D
ϕ1, ϕ2, Γ ⊢D
(or)
ϕ1 ∨ϕ2, Γ ⊢D
ϕ1, Γ ⊢D
ϕ2, Γ ⊢D
(cons) U, Γ ⊢D
ϕU, Γ ⊢D whenever (U = σX.ϕ(X)) ∈D
(μ) μX.ϕ(X), Γ ⊢D
U, Γ ⊢D
whenever (U = μX.ϕ(X)) ∈D
(ν) νX.ϕ(X), Γ ⊢D
U, Γ ⊢D
whenever (U = νX.ϕ(X)) ∈D
(all ⟨⟩)
Γ ⊢D
{ϕ1, {ϕ2 : [α]ϕ2 ∈Γ} ⊢D
: ⟨α⟩ϕ1 ∈Γ}
φ′
mod. This translation is necessary because the Walukiewicz’s satisﬁability procedure
was developed for modal μ-calculus formulae instead the partial model checking was
developed for equational μ-calculus ones7.
The Synthesis: An implementation of Walukiewicz satisﬁability procedure. Let
deﬁnition list be a ﬁnite sequence of equations: D = ((U1 = σ1X.α1(X)), · · · , (Un =
σnX.αn(X)) where U1, · · · , Un are new constant symbols and σiX.αi(X) is a for-
mula such that all deﬁnition constants appearing in αi are among U1, · · · , Ui−1. Let
tableau sequent be a pair (Γ, D) where D is a deﬁnition list and Γ is a ﬁnite set of
formulae such that the only constants that occur in them are those from D. Let tableau
axiom be a sequent Γ ⊢D such that some formula and its negation occurs in Γ, a tableau
is built using the system of rules S presented in Table 4.
Deﬁnition 2. Given a positive guarded formula φ, a tableau for φ is any labeled tree
⟨K, L⟩, where K is a tree and L a labeling function, such that
1. the root of K is labeled with φ ⊢D where D is the deﬁnition list of φ;
2. if L(n) is a tableau axiom then n is a leaf of K;
3. if L(n) is not an axiom then the sons of n in K are created and labeled according
to the rules of the system S.
Walukiewicz has proven that it is possible to extract from tableau either a refutation or
a model for a modal μ-calculus formula. In order to do this he has deﬁned two different
systems of rules, Smod and Sref. The system Smod is obtained from S by replacing
the rule (or) by two rules (orleft) and (orright) deﬁned in the obvious way; the system
Sref is obtained from S by replacing the rule (all⟨⟩) by the rule
(⟨⟩)
⟨a⟩α, Γ ⊢D
α, {β : [a]β ∈Γ} ⊢D
7 As a matter of fact, the equational μ-calculus is close for partial model checking. This means
that applying the partial model checking function to an equational μ-calculus formula we
pbtain an equational μ-calculus formula.

122
I. Matteucci
with the same restrictions on formulae in Γ as in the case of (all⟨⟩) rule, i.e. each
formula in Γ is a propositional constant, a variable, or a formula of the form ⟨b⟩ϕ or
[b]ϕ for some action b and a formula α. A quasi-model of φ is deﬁned in a similar way
to tableau, except the system Smod is used instead of S and we impose the additional
requirement that no leaf is labeled by a tableau axiom. A quasi-refutation of φ is deﬁned
in a similar way to tableau, except the system Sref is used instead of S and we impose
the additional requirement that every leaf is labeled by a tableau axiom.
Let P = (v1, v2, · · · ) be a path in the tree K. A trace T r on the path P is any
sequence of formulas {ϕi}i∈I such that ϕi ∈L(vi) and ϕi+1 is either ϕi, if formula
ϕi was not reduced by the rule applied in vi, or otherwise ϕi+1 is one of the formu-
lae obtained by applying the rule to ϕi. A constant U regenerates on the trace T r if
for some i, αi = U and αi+1 = ϕ(U) where (U = σX.ϕ(X)) ∈D. The trace T r
is called a ν-trace iff it is ﬁnite and does not end with a tableau axiom, or if the old-
est constant in the deﬁnition list D which is regenerated inﬁnitely often on T r is a
ν-constant. Otherwise the trace is called a μ-trace.
Deﬁnition 3. A quasi model PM is called pre-model iff any trace on any path of PM
is a ν-trace .
A quasi-refutation of φ is called a refutation of φ iff on every path of there exists a
μ-trace.
We represent a tableau as a graph. Hence we deﬁne the type graph as a list of triple
(n, a, n) ∈GNode × Act × GNode where GNode is the set of graph nodes. Each
node of the graph represents a state L(n) of the tableau. Each node is characterized by
the set of formulae that it satisﬁes.
We build the pre-model by exploiting the system of rules Smod in model.ml. First
of all we check if conditions for the pre-model given in the Deﬁnition 3 are satisﬁed
by using the function goodgraph.ml. This function takes in input a graph and gives
back the boolean value TRUE if pre-model conditions are respected, FALSE otherwise.
The function model.ml takes as input a pair in GNode × Graph and, in a recursive
way, builds the graph that is the pre-model for φ. At the beginning we give in input
a node labeled by φ and Empty Graph, that represents the empty graph. Then, in a
recursive way, we build the pre-model by checking, at each step, that the pre-model
condition is satisﬁed, by using goodgraph.ml.
According to the Walukiewicz theory, from the pre-model it is possible to ﬁnd a
model for φ as follow.
Deﬁnition 4. Given a pre-model PM, the canonical structure for PM is a structure
M = ⟨SM, RM, ρM⟩such that
1. SM is the set of all nodes of PM which are either leaves or to which (all⟨⟩) rule
was applied. For any node n of PM we will denote by sn the closest descendant
of n belonging to SM.
2. (s, s′) ∈RM(a) iff there is a son n of s with sn = s′, such that L(n) was obtained
from L(s) by reducing a formula of the form ⟨a⟩ϕ.
3. ρM(p) = {s : p occurs in the sequent L(s)}.

A Tool for the Synthesis of Controller Programs
123
Through the application of the function simplify.ml we are able to ﬁnd the canon-
ical structure from the pre-model generated by model.ml. In this way we have ob-
tained a model for φ′
mod. This is a controller program for controller operators that do
not require a relabeling function, e.g. Y ▷T X.
As we can seen in Figure 1.b) there are other functions in the Synthesis module.
The controllers.ml implements relabeling functions fK, one for each controller
operators described in Section 3.1. The function printGraph.ml permits to print the
graph as a sequence of nodes labeled by a list of formulae, connected by arrows labeled
by an action. The last function is the main.ml that calls all the other functions and
permits to create the executable ﬁle (.exe).
5
A Cases Study
In order to explain better how the tool works, we present an example in which a system
must satisfy a safety property. We generate a controller program for each of the four
controllers deﬁned in Section 3.1.
Let S be a system. We suppose that all users that work on S have to satisfy the
following rule:
You cannot open a new ﬁle while another ﬁle is open.
It can be formalized by an equation system D as follows:
X =ν [τ]X ∧[open]Y
Y =ν [τ]Y ∧[close]X ∧[open]F
5.1
Truncation
We halt the system if the user try to open a ﬁle while another is already open. In this
case we generate a controller program Y for Y ▷T X and we obtain:
Y = open.close.Y
Y is a model for D.
In order to show how it works as controller program for Y ▷T X we suppose to have
a possible user X that tries to open two different ﬁles. Hence X = open.open.0.
Applying Y ▷T X we obtain:
Y ▷T X = open.close.Y ▷T open.open.0
open
−→close.Y ▷T open.0
Since Y and X are going to perform a different action, i.e. Y is going to perform close
while X is going to perform open, the whole system halts.
5.2
Suppression
We suppose to decide to suppress any possible open action that can violate the property
D. In this case we generate a controller program Y for the controller Y ▷SX. We obtain:
Y = −open.Y + open.Y ′
Y ′ = −open.Y ′ + close.Y

124
I. Matteucci
Let we suppose to be in the same scenario described for the previous operator. Let X
be a user that tries to open two different ﬁles. Hence X = open.open.0. Applying
Y ▷S X we obtain:
Y ▷S X = −open.Y + open.Y ′ ▷S open.open.0
open
−→−open.Y ′ + close.Y ▷S open.0
τ
−→Y ′ ▷S 0
The whole system halts again because, even if a wrong action is suppressed, this con-
trollers cannot introduce right actions.
5.3
Insertion
We decide to implement a controller program Y for the controller Y ▷I X. We obtain:
Y = +open.close.open.Y + open.Y ′
Y ′ = +open.close.open.Y ′ + close.Y
We consider X that tries to open two different ﬁles. Hence X = open.open.0. We
obtain:
Y ▷I X = +open.close.open.Y + open.Y ′ ▷I open.open.0
open
−→+open.close.open.Y ′ + close.Y ▷I open.0
close
−→open.Y ′ ▷I open.0
open
−→Y ′ ▷I 0
We can note the Y permits X to perform the ﬁrst action open. Then it checks that
X is going to perform another open by the action +open. Hence Y insert an action
close. After this action it permits X to perform the action open. Since X does not
perform any another actions the whole system halts.
5.4
Edit
We consider to apply the controller operator Y ▷E X. The controller program that we
generate is the following:
Y = −open.Y + +open.close.open.Y +
open.Y ′
Y ′ = −open.Y ′ + +open.close.open.Y ′ + close.Y
8
We suppose again that X = open.open.0. We have:
Y ▷E X =
−open.Y + +open.close.open.Y +
open.Y ′ ▷E open.open.0
open
−→−open.Y ′ + +open.close.open.Y ′ + close.Y ▷E open.0
close
−→open.Y ′ ▷E open.0
open
−→Y ′ ▷E 0
Also in this case, after the ﬁrst action open, Y checks if X is going to perform another
open by the action +open and then it inserts the action close in order to satisfy the
property D. Then it permit to perform another open action.
8 In order to obtain a deterministic process we say that, when the target is going to perform a
wrong action we apply the third rule before the second one.

A Tool for the Synthesis of Controller Programs
125
6
Conclusion and Future Work
In this paper we have described a tool for the synthesis of a controller program based
on Walukiewicz’s satisﬁability procedure as well as on the partial model checking tech-
nique. In particular, starting from a system S and a formula φ that describes a security
property, the tool generates a process that, by monitoring a possible un-trusted compo-
nent, guarantees that a system S∥X satisﬁes φ whatever X is.
This tool is made up of several parts. Starting from the MuDiv tool, developed to
calculate the partial model checking function, we create a module that implements the
satisﬁability procedure of Walukiewicz for modal μ-calculus formula. As we have al-
ready said (Theorem 1), this procedure is exponential in the size of the formula. There
are some classes of μ-calculus formulae for which the satisﬁability procedure is not
exponential, e.g. universal disjunctive formulae (see [8]).
Some optimizations can be done in order to obtain manageable formulae from the
MuDiv tool.
We plan to extend this approach to GRID systems and also we would like to extend
our work to systems with several un-trusted components.
Acknowledgement. We thank the anonymous referees of FAST06 for valuable com-
ments that helped us to improve this paper.
References
1. Andersen, H.R.: Veriﬁcation of Temporal Properties of Concurrent Systems. PhD thesis,
Department of Computer Science, Aarhus University, Denmark (1993)
2. Andersen, H.R.: Partial model checking. In: LICS ’95. In: Proceedings of the 10th Annual
IEEE Symposium on Logic in Computer Science, p. 398. IEEE Computer Society Press, Los
Alamitos (1995)
3. Bartoletti, M., Degano, P., Ferrari, G.: Policy framings for access control. In: Proceedings of
the 2005 workshop on Issues in the theory of security, pp. 5 – 11, Long Beach, California
(2005)
4. Bauer, L., Ligatti, J., Walker, D.: More enforceable security policies. In: Cervesato, I. (ed.)
Foundations of Computer Security. proceedings of the FLoC’02 workshop on Foundations of
Computer Security, Copenhagen, Denmark, 25–26 July 2002, pp. 95–104. DIKU Technical
Report (2002)
5. Bauer, L., Ligatti, J., Walker, D.: Edit automata: Enforcement mechanisms for run-time se-
curity policies. International Journal of Information Security. 4(1–2) (2005)
6. Bloom, B., Istrail, S., Meyer, A.R.: Bisimulation can’t be traced. J.ACM 42(1) (1995)
7. Focardi, R., Gorrieri, R.: A classiﬁcation of security properties. Journal of Computer Secu-
rity 3(1), 5–33 (1997)
8. Janin, D., Walukiewicz, I.: Automata for the μ-calculus and related results. In: H´ajek, P.,
Wiedermann, J. (eds.) MFCS 1995. LNCS, vol. 969, Springer, Heidelberg (1995)
9. Leroy, X., Doligez, D., Garrigue, J., R´emy, D., Vouillon, J.: The objective caml systemrelease
3.09 (2004)
10. Lowe, G.: Semantic models for information ﬂow. Theor. Comput. Sci. 315(1), 209–256
(2004), doi:10.1016/j.tcs.2003.11.019

126
I. Matteucci
11. Martinelli, F., Matteucci, I.: Partial model checking, process algebra operators and satis-
ﬁability procedures for (automatically) enforcing security properties. In: Presented at the
International Workshop on Foundations of Computer Security (FCS05) (2005)
12. Martinelli, F., Matteucci, I.: Modeling security automata with process algebras and related
results. In: Informal proceedings Presented at the 6th International Workshop on Issues in
the Theory of Security (WITS ’06) (March 2006)
13. Martinelli, F., Matteucci, I.: Through modeling to synthesis of security automata. In: Pro-
ceedings of ENTCS STM06 (2006)
14. Matteucci, I.: Automated synthesis of enforcing mechanisms for security properties in a
timed setting. In: Proceedings of ENTCS ICS’06 (2006)
15. Milner, R.: Synthesis of communicating behaviour. In: Proceedings of 7th MFCS, Poland
(1978)
16. Milner, R.: Communicating and mobile systems: the π-calculus. Cambridge University
Press, Cambridge (1999)
17. Schneider, F.B.: Enforceable security policies. ACM Transactions on Information and Sys-
tem Security 3(1), 30–50 (2000)
18. Street, R.S., Emerson, E.A.: An automata theoretic procedure for the propositional μ-
calculus. Information and Computation 81(3), 249–264 (1989)
19. Walukiewicz, I.: A Complete Deductive System for the μ-Calculus. PhD thesis, Institute of
Informatics, Warsaw University (June 1993)

Where Can an Insider Attack?
Christian W. Probst1, Ren´e Rydhof Hansen2, and Flemming Nielson1
1 Informatics and Mathematical Modelling, Technical University of Denmark⋆
{probst,nielson}@imm.dtu.dk
2 Department of Computer Science, University of Copenhagen
rrhansen@diku.dk
Abstract. By deﬁnition an insider has better access, is more trusted,
and has better information about internal procedures, high-value targets,
and potential weak spots in the security, than an outsider. Consequently,
an insider attack has the potential to cause signiﬁcant, even catastrophic,
damage to the targeted organisation. While the problem is well recog-
nised in the security community as well as in law-enforcement and intel-
ligence communities, the main resort still is to audit log ﬁles after the
fact. There has been little research into developing models, automated
tools, and techniques for analysing and solving (parts of) the problem.
In this paper we ﬁrst develop a formal model of systems, that can de-
scribe real-world scenarios. These high-level models are then mapped to
acKlaim, a process algebra with support for access control, that is used
to study and analyse properties of the modelled systems. Our analysis of
processes identiﬁes which actions may be performed by whom, at which
locations, accessing which data. This allows to compute a superset of
audit results—before an incident occurs.
1
Introduction
One of the toughest and most insidious problems in information security, and in-
deed in security in general, is that of protecting against attacks from an insider.
By deﬁnition, an insider has better access, is more trusted, and has better infor-
mation about internal procedures, high-value targets, and potential weak spots
in the security. Consequently, an insider attack has the potential to cause signif-
icant, even catastrophic, damage to the targeted IT-infrastructure. The problem
is well recognised in the security community as well as in law-enforcement and
intelligence communities, cf. [1,13,6]. In spite of this there has been relatively
little focused research into developing models, automated tools, and techniques
for analysing and solving (parts of) the problem. The main measure taken still
is to audit log ﬁles after an insider incident has occurred [13].
In this paper we develop a model that allows to deﬁne a notion of insider
attacks and thereby enables to study systems and analyse the potential con-
sequences of such an attack. Formal modelling and analysis is increasingly im-
portant in a modern computing environment with widely distributed systems,
⋆Part of this work has been supported by the EU research project #016004, Software
Engineering for Service-Oriented Overlay Computers.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 127–142, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

128
C.W. Probst, R.R. Hansen, and F. Nielson
computing grids, and service-oriented architectures, where the line between in-
sider and outsider is more blurred than ever.
With this in mind we have developed a formal model in two parts: an abstract
high-level system model based on graphs and a process calculus, called acKlaim,
providing a formal semantics for the abstract model. As the name suggests, the
acKlaim calculus belongs to the Klaim family of process calculi [9] that are
all designed around the tuple-space paradigm, making them ideally suited for
modelling distributed systems like the interact/cooperate and service-oriented
architectures. Speciﬁcally, acKlaim is an extension of the μKlaim calculus with
access-control primitives. In addition to this formal model we also show how tech-
niques from static program analysis can be applied to automatically compute a
sound estimate, i.e., an over-approximation, of the potential consequences of an
insider attack. This result has two immediate applications—on the one hand it
can be used in designing access controls and assigning security clearances in such
a way as to minimise the damage an insider can do. On the other hand, it can
direct the auditing process after an insider attack has occurred, by identifying
before the incident which events should be monitored. The important contribu-
tion is to separate the actual threat and attack from reachability. Once we have
identiﬁed, which places an insider can reach, we can easily put existing models
and formalisms on top of our model.
The rest of the paper is structured as follows. In the remainder of this sec-
tion the terms insider and insider threat are deﬁned. Section 2 introduces our
abstract system model and an example system, and Section 3 deﬁnes acKlaim,
the process calculus we use to analyse these systems. The analysis itself is intro-
duced in Section 4, followed by a discussion of related work (Section 5). Section 6
concludes our paper and gives an outlook and future work.
1.1
The Insider Problem
Recently, the insider problem has attracted interest by both researchers and
intelligence agencies [13,14]. However, most of the work is on detecting insider
attacks, modelling the threat itself, and assessing the threat. This section gives
an overview of existing work.
Bishop [1] introduces diﬀerent deﬁnitions of the insider threat found in liter-
ature. The RAND report [13] deﬁnes the problem as “malevolent actions by an
already trusted person with access to sensitive information and information sys-
tems”, and the insider is deﬁned as “someone with access, privilege, or knowledge
of information systems and services”. Bishop also cites Patzakis [12] to deﬁne
the insider as “anyone operating inside the security perimeter”, thereby con-
trasting it from outside attacks like denial-of-service attacks, which originate
from outside the perimeter. Bishop then moves on to deﬁne the terms insider
and insider threat:
Deﬁnition 1. (Insider, Insider threat). An insider with respect to rules R is a
user who may take an action that would violate some set of rules R in the security
policy, were the user not trusted. The insider is trusted to take the action only

Where Can an Insider Attack?
129
User Office
Server / Printer
Janitor Workshop
C
C
Janitor
User
Hallway
Fig. 1. The example system used to illustrate our approach. The user can use the
network connection to print some potentially conﬁdential data in the server/printer
room. Depending on the conﬁguration on the cipher locks, the janitor might or might
not be able to pick up that print out.
when appropriate, as determined by the insider’s discretion. The insider threat
is the threat that an insider may abuse his discretion by taking actions that would
violate the security policy when such actions are not warranted.
Obviously, these deﬁnitions are expressive in that they connect actors in a system
and their actions to the rules of the security policy. On the other hand, they are
rather vague, since in a given system it is usually hard to identify vulnerabilities
that might occur based on an insider taking unwarranted actions. In the rest of
the paper we will use Bishop’s deﬁnitions to analyse abstractions of real systems
for potential attacks by insiders. To do so, in the next section we deﬁne the
abstract model of systems, actors, data, and policies.
2
Modelling Systems
This section introduces our abstract model of systems, which we will analyse
in Section 4 to identify potential insider threats. Our system model is at a level
that is high enough to allow easy modelling of real-world systems. The system
model naturally maps to an abstraction using the process calculus acKlaim (Sec-
tion 3). Here it is essential that our model is detailed enough to allow expressive
analysis results. The abstraction is based on a system consisting of locations
and actors. While locations are static, actors can move around in the system. To
support these movements, locations can be connected by directed edges, which
deﬁne freedoms of movements of actors. This section ﬁrst motivates the choice
of our abstraction by an example, and thereafter formally deﬁnes the elements.

130
C.W. Probst, R.R. Hansen, and F. Nielson
2.1
Example System
In Figure 1 we show our running example system inspired by [1]. It models part
of an environment with physical locations (a server/printer room with a waste-
basket, a user oﬃce, and a janitor’s workshop connected through a hallway),
and network locations (two computers connected by a network, and a printer
connected to one of them). The access to the server/printer room and the user
oﬃce is restricted by a cipher lock, and additional by use of a physical master
key. The actors in this system are a user and a janitor.
Following Bishop’s argumentation, the janitor might pose an insider threat
to this system, since he is able to access the server room and pick up printouts
from the printer or the wastebasket. We assume a security policy, which allows
the janitor access to the server room only in case of ﬁre. Modelling these soft
constraints is part of ongoing and future work.
2.2
System Deﬁnition
We start with deﬁning the notion of an infrastructure, which consists of a set of
locations and connections:
Deﬁnition 2. (Infrastructure, Locations, Connections). An infrastructure is a
directed graph (Loc, Con), where Loc is a set of nodes representing locations, and
Con ⊆Loc × Loc is a set of directed connections between locations. nd ∈Loc is
reachable from ns ∈Loc, if there is a path π = n0, n1, n2, · · · , nk, with k ≤1,
n0 = ns, nk = nd, and ∀0 ≤i ≤k −1 : ni ∈Loc ∧(ni, ni+1) ∈Con.
Next, we deﬁne actors, which can move in systems by following edges between
nodes, and data, which actors can produce, pickup, or read. In the example
setting, actors would be the user, the janitor, or processes on the computers,
whereas data for example would be a printout generated by a program. Usually,
actors can only move in a certain domain. In the example system, the user and
the janitor can move in the physical locations, but they can only access, e.g.,
the printer and the waste basket to take items out of them. This is modelled by
nodes falling in diﬀerent domains.
Deﬁnition 3. (Actors, Domains). Let I = (Loc, Con) be an infrastructure,
Actors be a set. An actor α ∈Actors is an entity that can move in I. Let Dom
be a set of unique domain identiﬁers. Then D : Loc →Dom deﬁnes the domain
d for a node n, and D−1 deﬁnes all the nodes that are in a domain.
Deﬁnition 4. (Data). Let I = (Loc, Con) be an infrastructure, Data be a set
of data items, and α ∈Actors an actor. A data item d ∈Data represents data
available in the system. Data can be stored at both locations and actors, and
K : (Actors ∪Loc) →P(Data) maps an actor or a location to the set of data
stored at it.
Finally, we need to model how actors can obtain the right to access locations
and data, and how these can decide whether to allow or to deny the access.

Where Can an Insider Attack?
131
CLSRV
CLUSR
JAN
USR
SRV
HALL
PC1
PC2
PRT
WASTE
User Office
Server / Printer
Janitor Workshop
Hallway
Fig. 2. Abstraction for the example system from Figure 1. The diﬀerent kinds of arrows
indicate how connections can be accessed. The solid lines, e.g., are accessible by actors
modelling persons, the dashed lines by processes executing on the network. The dotted
lines are special in that they express possible actions of actors.
We associate actors with a set of capabilities, and locations and data with a
set of restrictions. Both restrictions and capabilities can be used to restrain the
mobility of actors, by requiring, e.g., a certain key to enter a location, or allowing
access only for certain actors or from certain locations. In the example, the code
stored in the cipher locks is a restriction, and an actor’s knowledge of that code
is a capability. Similarly, data items can have access restrictions based on the
security classiﬁcation of the user or based on encryption.
Deﬁnition 5. (Capabilities and Restrictions). Let I = (Loc, Con) be an infras-
tructure, Actors be a set of actors, and Data be a set of data items. Cap is a
set of capabilities and Res is a set of restrictions. For each restriction r ∈Res,
the checker Φr : Cap →{true, false} checks whether the capability matches the
restriction or not. C : Actors →P(Cap) maps each actor to a set of capabilities,
and R : (Loc ∪Data) →P(Res) maps each location and data item to a set of
restrictions.
Figure 2 shows the modelling for the example system from Figure 1. Locations
are the rooms and cipher locks (circles), and the computers, the printer, and the
wastebasket (squares). The diﬀerent kinds of arrows indicate how connections
can be accessed. The solid lines, e.g., are accessible by actors modelling persons,
the dashed lines by processes executing on the network. The dotted lines are
special in that they express possible actions of actors. An actor at the server
location, e.g., can access the wastebasket. Finally, we combine the above elements
to a system:
Deﬁnition 6. (System). Let I = (Loc, Con) be an infrastructure, Actors a set
of actors in I, Data a set of data items, Cap a set of capabilities, Res a set

132
C.W. Probst, R.R. Hansen, and F. Nielson
of restrictions, C : Actors →P(Cap) and R : (Loc ∪Data) →P(Res) maps
from actors and location and data, respectively, to capabilities and restrictions,
respectively, and for each restriction r, let Φr : Cap →{true, false} be a checker.
Then we call S = ⟨I, Actors, Data, C, R, Φ⟩a system.
3
The acKlaim Calculus
In this section we present the process calculus that provides the formal underpin-
ning for the system model presented in the previous section. The calculus, called
acKlaim, belongs to the Klaim family of process calculi [9]; it is a variation of
the μKlaim calculus enhanced with access control primitives and equipped with
a reference monitor semantics (inspired by [8]) that ensures compliance with the
system’s access policy. The main additions are access policies (Section 3.1), the
tagging of processes with names and keys (Section 3.2), and the addition of a
reference monitor in the semantics (Figure 7). In addition to providing a conve-
nient and well-understood formal framework, the use of a process calculus also
enables us to apply a range of tools and techniques originally developed in a pro-
gramming language context. In particular it facilitates the use of static analyses
to compute a sound approximation of the consequences of an insider attack.
3.1
Access Policies
We start by deﬁning the access policies that are enforced by the reference mon-
itor. Access policies come in three varieties: access can be granted based on the
location it is coming from, based on the actor that is performing the access, or
based on the knowledge of the secret, e.g., a key in (a)symmetric encryption. In
the above system model, these are modelled by capabilities and restrictions.
CLSRV
CLUSR
JAN
USR
SRV
HALL
PC1
PC2
PRT
WASTE
*: e
U: e
*: e
*: e
U: e,i,r,o
SRV: i,r
PC2: o
PC1: e
U: e,i,r,o
SRV: i,r,o
*: e
U: e
J: e
Loc = {
JAN, USR, SRV, HALL,
CLusr, CLsrv, WASTE,
PC1, PC2, PRT }
Name = {U,J}
Fig. 3. The abstracted example system from Figure 2, extended with policy annota-
tions. There are two actors, janitor J and user U, who, e.g., have diﬀerent access rights
to the user oﬃce and the server room.

Where Can an Insider Attack?
133
π ⊆AccMode = {i, r, o, e, n}
κ ⊆Keys = {unique key identiﬁers}
δ ∈Policy = (Loc ∪Name ∪Keys ∪{⋆}) →P(AccMode)
The access modes i, r, o, e, n correspond to destructively read a tuple, non-de-
structively read a tuple, output a tuple, remote evaluation, and create new location
respectively. These modes reﬂect the underlying reference monitor semantics and
are explained in detail below. The special element ⋆allows to specify a set of
access modes that are allowed by default. The separation of locations and names
is artiﬁcial in that both sets simply contain unique identiﬁers. They are separated
to stress the distinction between locations as part of the infrastructure and actors
that are moving around in the infrastructure.
The elements of the domain Keys are keys used, e.g., for symmetric or asym-
metric encryption. We assume that each key uniquely maps to the method used
to check it (a checker Φr).
Intuitively, every locality in the system deﬁnes an access policy that speciﬁes
how other localities and actors are allowed to access and interact with it. This
approach aﬀords ﬁne-grained control for individual localities over both who is
allowed access and how. Semantically the access control model is formalised by a
reference monitor embedded in the operational semantics for the calculus. The
reference monitor veriﬁes that every access to a locality is in accordance with
that locality’s access policy.
We use the function grant to decide whether an actor n at location l knowing
keys κ should be allowed to perform an action a on the location l′ (Figure 4).
Additionally, access policies can be deﬁned for every data item. In this case,
only the subset {i, r} of access modes can be used for name- or location based
speciﬁcation, as well as keys specifying how the data item has been encrypted.
grant : Names × Loc × Keys × AccMode × Loc →{true, false}
grant(n, l, κ, a, l′) =

true
if a ∈δl′(n) ∨a ∈δl′(l) ∨∃k ∈κ : a ∈δl′(k)
false
otherwise
l = t
⟨I, n, κ⟩≻(l, t)
∃(l, l′) ∈Con : grant(n, l, κ, e, l′) ∧⟨I, n, κ⟩≻(l′, t)
⟨I, n, κ⟩≻(l, t)
grant(n, l, κ, a, t) ∧⟨I, n, κ⟩≻(l, t)
⟨I, n, κ⟩⇝(l, t, a)
Fig. 4. Function grant (upper part) checks whether an actor n at location l knowing
keys κ should be allowed to perform an action a on the location l′ based on the location
it is at, its name, or a key it knows. The judgement ≻(lower part) decides whether an
actor n at location s can reach location t based on the edges present in the infrastructure
I, by testing ⟨I, n, κ⟩≻(s, t). Finally, the judgement ⇝uses grant and judgement ≻
to test whether n is allowed to execute action a at location t.

134
C.W. Probst, R.R. Hansen, and F. Nielson
ℓ::= l
locality
N ::= l ::δ [P]⟨n,κ⟩
single node
|
u
locality variable
|
l ::δ ⟨et⟩
located tuple
|
N1 ∥N2
net composition
P ::= nil
null process
a ::= out (t) @ℓ
output
|
a.P
action preﬁxing
|
in (T) @ℓ
input
|
P1 | P2 parallel composition
|
read (T) @ℓ
read
|
A
process invocation
|
eval (P) @ℓ
migration
|
newloc(uπ : δ) creation
Fig. 5. Syntax of nets, processes, and actions
T ::= F | F, T
templates
F ::= f | !x | !u
template ﬁelds
t ::= f | f, t
tuples
f ::= e | l | u
tuple ﬁelds
et ::= ef | ef , et
evaluated tuple
ef ::= V | l
evaluated tuple ﬁeld
e ::= V | x | . . .
expressions
Fig. 6. Syntax for tuples and templates
3.2
Syntax and Semantics
The Klaim family of calculi, including acKlaim, are motivated by and designed
around the tuple space paradigm in which a system consists of a set of distributed
nodes that interact and communicate through shared tuple spaces by reading
and writing tuples. Remote evaluation of processes is used to model mobility.
The acKlaim calculus, like other members of the Klaim family, consists
of three layers: nets, processes, and actions. Nets give the overall structure where
tuple spaces and processes are located; processes execute by performing actions.
The syntax is shown in Figure 5 and Figure 6. The main diﬀerence to standard
Klaim calculi is, that processes are annotated with a name, in order to model
actors moving in a system, and a set of keys to model the capabilities they
have.
The semantics for acKlaim (Figure 7) is speciﬁed as a small step operational
semantics and follows the semantics of μKlaim quite closely. A process is either
comprised of subprocesses composed in parallel, an action (or a sequence of ac-
tions) to be executed, the nil-process, i.e., the process that does nothing, or it can
be a recursive invocation through a place-holder variable explicitly deﬁned by
equation. The out action outputs a tuple to the speciﬁed tuple space; the in and
read actions read a tuple from the speciﬁed tuple space in a destructive/non-
destructive manner, respectively. When reading from a tuple space, using either
the in or the read action, only tuples that match the input template (see Fig-
ure 6) are read. This pattern matching is formalised in Figure 8. The eval action
implements remote process evaluation, and the newloc action creates a new lo-
cation, subject to a speciﬁed access policy. While locations representing physical

Where Can an Insider Attack?
135
[[t]] = et
⟨I, n, κ⟩⇝(l, l′, o)
l ::δ [out (t) @l′.P]⟨n,κ⟩∥l′ ::δ′ [P ′]⟨n′,κ′⟩≻−→I
l ::δ [P]⟨n,κ⟩∥l′ ::δ′ [P ′]⟨n′,κ′⟩∥l′ ::δ′ ⟨et⟩
match([[T]], et) = σ
⟨I, n, κ⟩⇝(l, l′, i)
l ::δ [in (T) @l′.P]⟨n,κ⟩∥l′ ::δ′ ⟨et⟩≻−→I l ::δ [Pσ]⟨n,κ⟩∥l′ ::δ′ nil
match([[T]], et) = σ
⟨I, n, κ⟩⇝(l, l′, r)
l ::δ [read (T) @l′.P]⟨n,κ⟩∥l′ ::δ′ ⟨et⟩≻−→I l ::δ [Pσ]⟨n,κ⟩∥l′ ::δ′ ⟨et⟩
⟨I, n, κ⟩⇝(l, l′, e)
l ::δ [eval (Q) @l′.P]⟨n,κ⟩∥l′ ::δ′ [P ′]⟨n′,κ′⟩≻−→I
l ::δ [P]⟨n,κ⟩∥l′ ::δ′ [Q]⟨n,κ⟩∥l′ ::δ′ [P ′]⟨n′,κ′⟩
l′ /∈L
⌊l′⌋= ⌊u⌋
L ⊢l ::δ [newloc(uπ : δ′).P]⟨n,κ⟩≻−→I
L ∪{l′} ⊢l ::δ[l′→π] [P[l′/u]]⟨n,κ⟩∥l′ ::δ′[l′/u] [nil]⟨n,κ⟩
L ⊢N1 ≻−→I L′ ⊢N ′
1
L ⊢N1 ∥N2 ≻−→I L′ ⊢N ′
1 ∥N2
N ≡N1
L ⊢N1 ≻−→I L′ ⊢N2
N2 ≡N ′
L ⊢N ≻−→I L′ ⊢N ′
Fig. 7. Operational semantics for acKlaim. The semantics is annotated with the spatial
structure I of the underlying physical system. We omit the structure wherever it is clear
from context or is not needed. The boxes contain the reference monitor functionality,
that uses the structure I to verify that an intended action is allowable.
structures usually would be static in our system view, newloc can be used
to model, e.g., the spawning of processes in computer systems. Note that the
semantic rule for the newloc action is restricted through the use of canonical
names; these give a convenient way for the control ﬂow analysis (cf. Section 4)
to handle the potentially inﬁnite number of localities arising from unbounded
use of newloc. These will be explained in detail in Section 4. As is common for
process calculi, the operational semantics is deﬁned with respect to a built-in
structural congruence on nets and processes. This simpliﬁes presentation and
reasoning about processes. The congruence is shown in Figure 9.
In addition to the features of the standard semantics of μKlaim, we add the
spatial structure of the system to the semantics of acKlaim. This structure is
used to limit how access to other locations is granted. The system component
S is, among others, represented by a graph I as speciﬁed in Deﬁnition 6. The
reference monitor passes I to the judgement ⇝(Figure 4) to check whether
there is a path from the current location of a process and the target location
of the action. The reference monitor is formalised as additional premises of the
reduction rules, shown as boxes in Figure 7.

136
C.W. Probst, R.R. Hansen, and F. Nielson
match(V, V ) = ϵ
match(!x, V ) = [V/x]
match(l, l) = ϵ
match(!u, l′) = [l′/u]
match(F, ef ) = σ1
match(T, et) = σ2
match((F, T ), (ef , et)) = σ1 ◦σ2
Fig. 8. Semantics for template matching
N1 ∥N2 ≡N2 ∥N1
(N1 ∥N2) ∥N3 ≡N1 ∥(N2 ∥N3)
l ::δ [P]⟨n,κ⟩≡l ::δ [(P | nil)]⟨n,κ⟩
l ::δ [A]⟨n,κ⟩≡l ::δ [P]⟨n,κ⟩
if A
△= P
l ::δ [(P1 | P2)]⟨n,κ⟩≡l ::δ [P1]⟨n,κ⟩∥l ::δ [P2]⟨n,κ⟩
Fig. 9. Structural congruence on nets and processes
3.3
The Example Revisited
We now use acKlaim to model the system as speciﬁed in Figure 3, resulting in
the acKlaim program in Figure 10. The system property we are most interested
in is the spatial structure of the system, therefore most locations run either the
nil process or have an empty tuple space, if their location does not allow any
actor to execute processes at them. The user’s oﬃce and the janitor’s workshop
contain process variables, that can be used to plug in and analyse arbitrary
processes for these actors.
4
Analysing the System Abstraction
In this section we describe several analyses that we perform on the infrastruc-
ture underlying a system as well as on possible actors in the system. The ﬁrst
analysis (Section 4.1) determines, which locations in a system an actor with
name n and keys κ can reach from location l—either directly or by performing
an action on them. With respect to an insider threat this allows to determine
which locations an insider can reach and which data he can potentially access.
HALL ::⟨⋆→e⟩nil
∥USR ::⟨⋆→e⟩U
∥JAN ::⟨⋆→e⟩J ∥
CLUSR ::⟨U→e⟩nil
∥PC1 ::⟨U→e,i,r,o⟩nil ∥
CLSRV ::⟨U→e,J→e⟩nil
∥SRV ::⟨⋆→e⟩nil
∥WASTE ::⟨SRV→i,o,r⟩⟨⟩∥
PC2 ::⟨PC1→e,U→e,i,r,o⟩nil ∥PRT ::⟨SRV→i,r,PC2→o⟩⟨⟩
Fig. 10. The example system translated into acKlaim. The two process variables J and
U can be instantiated to hold actual process deﬁnitions. The system property we are
most interested in is the spatial structure of the system, therefore most locations run
either the nil process or have an empty tuple space, if their location does not allow
any actor to execute processes at them.

Where Can an Insider Attack?
137
This analysis can be compared to a before-the-fact system analysis to identify
possible vulnerabilities and actions that an audit should check for.
The second analysis (Section 4.2) is a control-ﬂow analysis of actors in a
system. It determines, which locations a speciﬁc process may reach, which actions
it may execute, and which data it may read. This can be compared to an after-
the-fact audit of log ﬁles.
4.1
Attack Points
In identifying potential insider attacks in a system, it is important to understand,
which locations in the system an insider can actually reach. This reachability
problem comes in two variations—ﬁrst we analyse the system locally for a speciﬁc
actor located at a speciﬁc location. Then, we put this information together to
identify all system-wide locations in the system that an actor possibly can reach.
Finally, the result of the reachability analyses can be used in computing which
data an actor may access on system locations, by evaluating which actions he
can execute from the locations he can reach.
Given that this analysis is very similar to a reachability analysis, we only
sketch how it works. Figure 11 shows the pseudo code speciﬁcation for both
reachability analyses and the global data analysis, parametrised in the system
structure, the name n of the actor, and the set of keys κ that the actor knows.
For the example system from Figure 3, the analysis ﬁnds out, that the actor
with name J and an empty set of keys can reach the location SRV and can
therefore execute the read action on both the waste basket and the printer,
checkloc : Names × Loc × Keys × (Con × Loc) →P(Loc)
for all (l, l′) ∈Con
if ⟨I, n, κ⟩≻(l, l′) ∨grant(n, l, κ, e, l′)
return {l′} ∪checkloc(n, l′, κ, I)
checksys : Names × Keys × (Con × Loc) →P(Loc)
checksys(n, κ, I) = 
l∈Loc checkloc(n, l, κ, I)
checkdata : Names × Keys × (Con × Loc) →P(AccMode × Loc)
checkdata(n, κ, I) = 
l∈checkloc(n,l,κ,I){(a, l)|∃a ∈AccMode, (l, l′) ∈Con :
grant(n, l, κ, a, l′)}
Fig. 11. Analysing a given system for attack points. The local analysis (upper part)
takes a name n, a location l and a key set κ, and returns the set of all locations
that n could possibly reach using κ. The system-wide analysis (middle part) puts these
together to obtain the global view, by executing the local analysis on all locations in the
system. Finally, the data analysis uses the system-wide analysis to compute at which
locations the actor may invoke which actions, allowing to identify which data he may
possibly access. A local data-analysis similar to the upper part is deﬁned analogous.

138
C.W. Probst, R.R. Hansen, and F. Nielson
possibly accessing conﬁdential data printed or trashed. While this is obvious for
the simplistic example system, those properties are usually not easily spotted in
complex systems.
4.2
Control-Flow Analysis
While the reachability analysis deﬁned in the previous section is easily computed
and veriﬁed, it also is highly imprecise in that it does not take into account
the actual actions executed in an attack. As described before, the reachability
analysis can be used in identifying vulnerable locations that might have to be
put under special scrutiny.
In this section we specify a static control ﬂow analysis for the acKlaim calcu-
lus. The control ﬂow analysis computes a conservative approximation of all the
possible ﬂows into and out of all the tuple spaces in the system. The analysis
is speciﬁed in the Flow Logic framework [10], which is a speciﬁcation-oriented
framework that allows “rapid development” of analyses. An analysis is speciﬁed
through a number of judgements that each determine whether or not a particular
analysis estimate correctly describes all conﬁgurations that are reachable from
the initial state. Concretely we deﬁne three judgements: for nets, processes, and
actions respectively. The deﬁnitions are shown in Figure 12.
Information is collected by the analysis in two components: ˆT and ˆσ. The
former records for every tuple space (an over-approximation of) the set of tuples
possibly located in that tuple space at any point in time. The latter component
tracks the possible values that variables may be bound to during execution, i.e.,
this component acts as an abstract environment.
( ˆT, ˆσ, I) |=N l ::δ [P]⟨n,κ⟩
iﬀ
( ˆT, ˆσ, I) |=⌊l⌋,n,κ
P
P
( ˆT, ˆσ, I) |=N l ::δ ⟨et⟩
iﬀ
⟨et⟩∈ˆT(⌊l⌋)
( ˆT, ˆσ, I) |=N N1 ∥N2
iﬀ
( ˆT, ˆσ, I) |=N N1 ∧( ˆT, ˆσ, I) |=N N2
( ˆT, ˆσ, I) |=l,n,κ
P
nil
iﬀ
true
( ˆT, ˆσ, I) |=l,n,κ
P
P1 | P2
iﬀ
( ˆT, ˆσ, I) |=l,n,κ
P
P1 ∧( ˆT, ˆσ, I) |=l,n,κ
P
P2
( ˆT, ˆσ, I) |=l,n,κ
P
A
iﬀ
( ˆT, ˆσ, I) |=l,n,κ
P
P
if A
△= P
( ˆT, ˆσ, I) |=l,n,κ
P
a.P
iﬀ
( ˆT, ˆσ, I) |=l,n,κ
A
a ∧( ˆT, ˆσ, I) |=l,n,κ
P
P
( ˆT, ˆσ, I) |=l,n,κ
A
out (t) @ℓ′
iﬀ
∀ˆl ∈ˆσ(ℓ′): (⟨I, n, κ⟩⇝(l, ˆl, o) ⇒ˆσ[[t]] ⊆ˆT(ˆl))
( ˆT, ˆσ, I) |=l,n,κ
A
in (T) @ℓ′
iﬀ
∀ˆl ∈ˆσ(ℓ′): (⟨I, n, κ⟩⇝(l, ˆl, i) ⇒
ˆσ |=1 T : ˆT(ˆl) ▷ˆ
W•)
( ˆT, ˆσ, I) |=l,n,κ
A
read (T) @ℓ′
iﬀ
∀ˆl ∈ˆσ(ℓ′): (⟨I, n, κ⟩⇝(l, ˆl, r) ⇒
ˆσ |=1 T : ˆT(ˆl) ▷ˆ
W•)
( ˆT, ˆσ, I) |=l,n,κ
A
eval (Q) @ℓ′
iﬀ
∀ˆl ∈ˆσ(ℓ′) : (⟨I, n, κ⟩⇝(l, ˆl, e) ⇒
( ˆT, ˆσ, I) |=
ˆl,n,κ
P
Q)
( ˆT, ˆσ, I) |=l,n,κ
A
newloc(uπ : δ)
iﬀ
{⌊u⌋} ⊆ˆσ(⌊u⌋)
Fig. 12. Flow Logic speciﬁcation for control ﬂow analysis of acKlaim

Where Can an Insider Attack?
139
ˆσ |=i ϵ : ˆV◦▷ˆV•
iﬀ
{ ˆet ∈ˆV◦| | ˆet| = i} ⊑ˆV•
ˆσ |=i V, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| πi( ˆet) = V } ⊑ˆV•
ˆσ |=i l, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| πi( ˆet) = V } ⊑ˆV•
ˆσ |=i x, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| πi( ˆet) ∈ˆσ(x)} ⊑ˆV•
ˆσ |=i u, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| πi( ˆet) ∈ˆσ(u)} ⊑ˆV•
ˆσ |=i !x, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧ˆV◦⊑ˆV• ∧πi( ˆ
W•) ⊑ˆσ(x)
ˆσ |=i !u, T : ˆV◦▷ˆ
W•
iﬀ
ˆσ |=i+1 T : ˆV• ▷ˆ
W• ∧ˆV◦⊑ˆV• ∧πi( ˆ
W•) ⊑ˆσ(u)
Fig. 13. Flow Logic speciﬁcation for pattern match analysis
We brieﬂy mention a technical issue before continuing with speciﬁcation of the
analysis: the handling of dynamic creation of locations. In order to avoid having
the analysis keep track of a potentially inﬁnite number of locations we deﬁne
and use so-called canonical names that divides all concrete location names and
location variables into equivalence classes in such a way that all (new) location
names generated at the same program point belong to the same equivalence class
and thus share the same canonical name. The canonical name (equivalence class)
of a location or location variable, ℓ, is written ⌊ℓ⌋. In the interest of legibility
we use a unique representative for each equivalence class and thereby dispense
with the ⌊·⌋notation whenever possible. We avoid possible inconsistencies in
the security policy for two locations with the same canonical names we only
consider policies that are compatible with the choice of canonical names; a policy
is compatible if and only if ⌊ℓ1⌋= ⌊ℓ2⌋⇒δ(ℓ1) = δ(ℓ2). This implies that the
policy assigns the exact same set of capabilities to all locations with the same
canonical name. Throughout this paper we assume that policies are compatible
with the chosen canonical names.
A separate Flow Logic speciﬁcation, shown in Figure 13, is developed in order
to track the pattern matching performed by both the input actions.
Having speciﬁed the analysis it remains to be shown that the information
computed by the analysis is correct. In the Flow Logic framework this is usually
done by establishing a subject reduction property for the analysis:
Theorem 1 (Subject Reduction). If ( ˆT, ˆσ, I) |=N N and L ⊢N ≻−→I L′ ⊢
N ′ then ( ˆT , ˆσ, I) |=N N ′.
Proof. (Sketch) By induction on the structure of L ⊢N ≻−→I L′ ⊢N ′ and
using auxiliary results for the other judgements.
Now we return to the abstract system model for our example (Figure 10). To
analyse it, we replace the two process variables J and U with processes as spec-
iﬁed in Figure 14. The janitor process J moves from the janitor’s workshop
location JAN to the server room SRV where he picks up a the review for the in-
sider paper from the printer (in (”review”, ”insiderpaper”, !r) @PRT). The user
U prints the review from PC1 via the print server PC2. The two interesting
locations for the control-ﬂow analysis are JAN and USR, where processes J
and U are plugged in, respectively. When analysing U, the analysis starts by

140
C.W. Probst, R.R. Hansen, and F. Nielson
J
△= eval (in ((”review”, ”insiderpaper”, !r)) @PRT) @SRV
U
△= eval (eval (out ((”review”, ”insiderpaper”, ”accept”)) @PRT) @PC2) @PC1
Fig. 14. Processes for the janitor and the user analysed in the abstract system model
from Figure 10
analysing eval (out (· · ·) @PRT) @PC2, resulting in an analysis of the action
out ((”review”, ”insiderpaper”, ”accept”)) @PRT. As speciﬁed in Figure 12, this
results in the tuple (”review”, ”insiderpaper”, ”accept”) being stored in ˆT(PRT),
representing the fact that the user started a print job at the print server, and the
resulting document ended up in the printer. The analysis of J results in analysing
in ((”review”, ”insiderpaper”, !r)) @PRT, which tries to read a tuple matching
the ﬁrst two components from the tuple space at locations PRT. Since that
is available after U has been analysed, the local variable r contains the string
”accept”, even though the janitor might not be authorised to access this data.
Note that for sake of simplicity we do not have added security classiﬁcations to
our model, but any mechanism could easily be added on top of the model and
the analysis.
5
Related Work
Recently, insiders and the insider threat [13,14,3,2] have attracted increased
attention due to the potential damage an insider can cause. Bishop [1] gives an
overview of diﬀerent deﬁnitions and provides uniﬁed deﬁnition, which is the basis
for our work. By separating the reachability analysis from the actual threat, we
are able to easily model other deﬁnitions of the insider threat, or insiders that
are more capable.
While the aforementioned papers discuss the insider problem, only very lit-
tle work can be found on the static analysis of system models with respect to
a potential insider threat. Chinchani et al. [4] describe a modelling methodol-
ogy which captures several aspects of the insider threat. Their model is also
based on graphs, but the main purpose of their approach is to reveal possible
attack strategies of an insider. They do so by modelling the system as a key
challenge graph, where nodes represent physical entities that store some infor-
mation or capability. Protections like, e.g., the cipher locks in our example, are
modelled as key challenges. For legitimate accesses these challenges incur no
costs, while for illegitimate accesses they incur a higher cost representing the
necessary “work” to guess or derive the key. The diﬀerence to our approach is
that they start with a set of target nodes and compute an attack that com-
promises these nodes. However, it is mostly unclear how the cost of breaking a
key challenge is determined. We are currently working on incorporating prob-
abilities into our model to express the likelihood of a certain capability being
acquired by a malicious insider. It might be interesting to incorporate this into

Where Can an Insider Attack?
141
Chinchani’s approach and to execute our analysis on their graphs to compare
these two approaches.
In a more general setting, fault trees have been used for analysing for system
failures [7]. However, they have not been used to model attacks, but to compute
the chance of combinations of faults to occur. Beside these, graphs have been used
in diﬀerent settings to analyse attacks on networks. Examples include privilege
graphs [5,11] and attack graphs [15]. The major diﬀerence to our work is the
level of detail in modelling static and dynamic properties of the system, and the
ability to analyse the dynamic behaviour of actors.
6
Conclusion and Future Work
One of the most insidious problems in information security is that of protecting
against attacks from an insider. Even though the problem is well recognised in
the security community as well as in law-enforcement and intelligence communi-
ties, there has been relatively little focused research into developing models and
techniques for analysing and solving (parts of) the problem. The main measure
taken still is to audit log ﬁles after an insider incident has occurred.
We have presented a formal model that allows to formally deﬁne a notion of
insider attacks and thereby enables to study systems and analyse the potential
consequences of such an attack. Formal modelling and analysis is increasingly
important in a modern computing environment with widely distributed systems,
computing grids, and service-oriented architectures, where the line between in-
sider and outsider is more blurred than ever.
The two components of our model—an abstract high-level system model based
on graphs and the process calculus acKlaim—are expressive enough to allow
easy modelling of real-world systems, and detailed enough to allow expressive
analysis results. On the system model, we use reachability analysis to identify
possible vulnerabilities and actions that an audit should check for, by computing
locations that actors in the system can reach and/or access—independent of
the actual actions they perform. On the abstract acKlaim model we perform a
control-ﬂow analysis of speciﬁc processes/actors to determine which locations a
speciﬁc process may reach, which actions it may execute, and which data it may
read. This can be compared to an after-the-fact audit of log ﬁles. To the best
of our knowledge this is the ﬁrst attempt in applying static analysis techniques
to tackle the insider problem, and to support and pre-compute possible audit
results. By separating the actual threat and attack from reachability, we can
easily put existing models and formalisms on top of our model.
The main limitation of the analysis described in this paper is that we assume
that all actions are logged. We are currently on adding logging to system speciﬁ-
cations, and on extensions of this model to malicious insiders, who try to obtain
keys as part of their actions in a system. Further we plan to extend both the
system model and the precision and granularity of our analyses. In the policy
model we are looking at how to integrate policies like “only in the case of ﬁre”
as mentioned in Section 2.1.

142
C.W. Probst, R.R. Hansen, and F. Nielson
References
1. Bishop, M.: The Insider Problem Revisited. In: Proc. of New Security Paradigms
Workshop 2005, Lake Arrowhead, CA, USA, Septenber 2005. ACM Press, NewYork
(2005)
2. Caruso, V.L.: Outsourcing information technology and the insider threat. Master’s
thesis, Air Force Inst. of Technology, Wright-Patterson Air Force Base, Ohio (2003)
3. CERT/US Secret Service: Insider threat study: Illicit cyber activity in the banking
and ﬁnance sector (August 2004) available at
www.cert.org/archive/pdf/bankfin040820.pdf
4. Chinchani, R., Iyer, A., Ngo, H.Q., Upadhyaya, S.: Towards a theory of insider
threat assessment. In: Proceedings of the 2005 International Conference on De-
pendable Systems and Networks, pp. 108–117. IEEE Computer Society Press, Los
Alamitos (2005)
5. Dacier, M., Deswarte, Y.: Privilege graph: an extension to the typed access matrix
model. In: Proceedings of the European Symposium On Research In Computer
Security (1994)
6. Gollmann, D.: Insider Fraud. In: Christianson, B., Crispo, B., Harbison, W.S., Roe,
M. (eds.) Security Protocols. LNCS, vol. 1550, pp. 213–219. Springer, Heidelberg
(1999)
7. Gorski, J., Wardzinski, A.: Formalising fault trees. In: Redmill, F., Anderson, T.
(eds.) Achievement and Assurance of Safety: Proceedings of the 3rd Safety-critical
Systems Symposium, Brighton, pp. 311–328. Springer, Heidelberg (1995)
8. Hansen, R.R., Probst, C.W., Nielson, F.: Sandboxing in myKlaim. In: ARES’06.
The First International Conference on Availability, Reliability and Security, Vi-
enna, Austria, April 2006, IEEE Computer Society, Los Alamitos (2006)
9. Nicola, R.D., Ferrari, G., Pugliese, R.: KLAIM: a Kernel Language for Agents
Interaction and Mobility. IEEE Transactions on Software Engineering 24(5), 315–
330 (1998)
10. Nielson, H.R., Nielson, F.: Flow Logic: a multi-paradigmatic approach to static
analysis. In: Mogensen, T.E., Schmidt, D.A., Sudborough, I.H. (eds.) The Essence
of Computation. LNCS, vol. 2566, Springer, Heidelberg (2002)
11. Ortalo, R., Deswarte, Y., Kaˆaniche, M.: Experimenting with quantitative evalu-
ation tools for monitoring operational security. IEEE Transactions on Software
Engineering 25(5), 633–650 (1999)
12. Patzakis, J.: New incident response best practices: Patch and proceed is no
longer acceptable incident response procedure. White Paper, Guidance Software,
Pasadena, CA (September 2003)
13. Anderson, R.H., Brackney, R.C.: Understanding the Insider Threat. RAND Cor-
poration, Santa Monica, CA, U.S.A., March 2005 (2005)
14. Shaw, E.D., Ruby, K.G., Post, J.M.: The insider threat to information systems.
Security Awareness Bulletin No. 2-98, Department of Defense Security Institute
September 1998 (1998)
15. Swiler, L., Phillips, C., Ellis, D., Chakerian, S.: Computer-attack graph generation
tool (June 12, 2001)

Maintaining Information Flow Security Under
Reﬁnement and Transformation
Fredrik Seehusen1,2 and Ketil Stølen1,2
1 SINTEF Information and Communication Technology, Norway
2 University of Oslo, Norway
{fse,kst}@sintef.no
Abstract. We address the problem of maintaining information ﬂow security un-
der reﬁnement and transformation. To this end we deﬁne a schema for the speciﬁ-
cation of secure information ﬂow properties and show that all security properties
deﬁned in the schema are preserved by a notion of reﬁnement. Reﬁnement is a
process that requires human guidance and is in general not subject for automation.
A transformation on the other hand, is an executable function mapping speciﬁca-
tions to speciﬁcations. We deﬁne an interpretation of transformations and propose
a condition under which transformations maintain security.
1
Introduction
We address the problem of maintaining information ﬂow security during the process of
making an abstract speciﬁcation more concrete. This problem has received little atten-
tion, yet it is of relevance in any real-life scenario in which security analysis is carried
out on the basis of a speciﬁcation that abstracts away details of the full implementation.
For example, it is of little help to know that Java code or some state machine speciﬁ-
cation is secure w.r.t. some property if validity of the property is not maintained by the
compiler. Hence, we need means and a corresponding theory to ensure that the trans-
formation from the abstract level to the more concrete level maintains the security of
the abstract level.
Proving security once and for all is in general not possible. One reason for this is
that the concrete level often includes peculiarities that do not have any abstract equiv-
alent. Consequently security must be proven again at the concrete level to ensure that
these additional peculiarities introduced via transformation do not violate security. Al-
though additional veriﬁcation is often needed at the concrete level, we still want to
check and maintain security properties on the basis of the abstract speciﬁcations. There
are three main reasons for this. First, analysis is in general more feasible at the abstract
level since the concrete level may include too much detail to make analysis practical.
Second, abstract speciﬁcations are in general more platform independent than concrete
speciﬁcations. This means that analysis results are more reusable at the abstract levels.
Third, abstract speciﬁcations tend to be more understandable than concrete speciﬁca-
tions, hence it is in general easier to specify and check security requirements at abstract
levels as opposed to the concrete levels.
In this paper we consider security in the sense of secure information ﬂow properties
(see e.g. [4,5,16]). The notion of secure information ﬂow provides a way of specifying
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 143–157, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

144
F. Seehusen and K. Stølen
security requirements by selecting a set of observers, i.e. abstractions of system entities,
and then restricting allowed ﬂow of information between the observers.
The process of making an abstract speciﬁcation more detailed is known as reﬁne-
ment, and the relationship between information ﬂow security and reﬁnement has been
researched for a fairly long time. In 1989 it was shown by Jacob [13] that secure in-
formation ﬂow properties in general are not preserved by the standard notion of reﬁne-
ment. It has later been observed that the problem originates in the inability of most
speciﬁcation languages to distinguish between underspeciﬁcation and unpredictability1
[10,14,19]. We argue that this distinction is essential if secure information ﬂow prop-
erties are to be preserved under reﬁnement. To this end, both the standard notion of
reﬁnement and all secure information ﬂow properties proposed in literature have to be
redeﬁned such that this distinction is taken into consideration. We show how to do this
in a formalism similar to STAIRS [7,8,9] by deﬁning a schema (based on [16]) for spec-
ifying secure information ﬂow properties such that all properties deﬁned in the schema
are preserved by reﬁnement.
Reﬁnement is a relation on speciﬁcations that formalizes the process of stepwise de-
velopment by the removal of underspeciﬁcation. A transformation on the other hand is
a computable function mapping speciﬁcations to speciﬁcations. For example, a com-
piler mapping a program to machine code is a kind of transformation. Currently, there
is much ongoing work on transformation in relation to OMG’s standardization activi-
ties on MDA (Model Driven Architecture) [18], where transformations characterize the
mapping of PIM’s (Platform Independent Model) to PSM’s (Platform Speciﬁc Model).
Motivated by this we give a semantic interpretation of transformations and propose a
condition under which transformations maintain security.
In summary, the main contributions of this paper are: (1) the deﬁnition of a schema
for specifying secure information ﬂow properties that are preserved by the STAIRS no-
tion of reﬁnement. (2) The deﬁnition of a notion of secure transformation that preserves
security properties deﬁned in our schema.
This paper is structured as follows: Sect. 2 formalizes a notion of system speciﬁca-
tion. Sect. 3 describes what is meant by secure information ﬂow. In Sect. 4 we present
the STAIRS notion of reﬁnement and propose a schema for specifying secure infor-
mation ﬂow properties that are preserved by this notion of reﬁnement. In Sect. 5, we
discuss security maintaining transformations. Sect. 6 provides conclusions and related
work.
2
System Speciﬁcations
We model the input-output behavior of systems by ﬁnite sequences of events called
traces. An event represents the transmission or the reception of a message. Formally,
an event is a pair (k, m) consisting of a kind k and a message m. An event whose kind
equals ! represents the transmission of a message, whereas an event whose kind equals ?
represents the reception of a message. A message is a triple (a1, a2, s) consisting of
a transmitter a1, a receiver a2, and a signal s representing the message body. Both
1 Also termed probabilistic non-determinism [19].

Maintaining Information Flow Security Under Reﬁnement and Transformation
145
transmitters and receivers are referred to as agents, i.e. system entities such as objects
or components.
Deﬁnition 1. The semantics of a system speciﬁcation, denoted Φ, is a preﬁx-closed set
of traces. A set of traces A is preﬁx-closed iff
t ∈A ∧t′ ⊑t ⇒t′ ∈A
where ⊑is the standard preﬁx ordering on sequences.
The reason why we require preﬁx-closure is that the deﬁnition of many secure informa-
tion ﬂow properties proposed in literature rely on this requirement [16,24].
In the sequel, we will for short write “speciﬁcation” instead “the semantics of system
speciﬁcation” when it clear from the context what is meant.
2.1
Notational Convention
We deﬁne some notational conventions and standard operations. P(A) denotes the
power set of A deﬁned {X|X ⊆A}. A∗denotes the set of all ﬁnite sequences over
the set A. A sequence of events, i.e. a trace, is written ⟨e1, e2, ..., en⟩. The empty trace,
i.e. the trace with no events is written ⟨⟩. The projection of a trace t on a set of events
E, written t|E, is obtained from t by deleting all elements not in E.
Further notational conventions are listed in Table 1. Here the notion a ∈A means
that the set A is ranged over by a.
Table 1. Notational conventions
Set
Deﬁnition
Meaning
a ∈A
Set of agents.
o ∈O
P(A)
Set of observers.
s ∈S
Set of signals.
m ∈M A × A × S
Set of messages.
Set
Deﬁnition
Meaning
e ∈E
{!, ?} × M
Set of events.
h ∈H
Set of high-lvl. events.
l ∈L
Set of low-lvl. events.
t ∈T
E ∗
Set of traces.
3
Information Flow Security
By secure information ﬂow we understand a restriction on allowed ﬂow of information
between observers, i.e. sets of agents. Secure information ﬂow can be described by a
ﬂow policy and a secure information ﬂow predicate, referred to as a security predicate
for short. The ﬂow policy restricts information ﬂow between observers, while the secu-
rity predicate deﬁnes what is meant by information ﬂow. Formally, a ﬂow policy is a
relation on observers
↛⊆O × O
where (o1, o2) ∈↛requires that there shall be no information ﬂow from o1 to o2.
For simplicity, we will in the sequel assume a ﬁxed ﬂow policy {(H, L)} consisting
of two observers only: H, the high-level observer and L, the low-level observer.

146
F. Seehusen and K. Stølen
Security predicates that describe what is meant by information ﬂow are expressed
in terms of the observations that observers can make. Formally, the observation that an
observer o can make of a trace t is obtained from t by deleting all events that can not be
observed by o:
t|(E.o)
Here E.o yields the set of all events that can be observed by o, i.e. all events that can be
transmitted from or received by the agents in o:
E.o ≜{(k, (a, a′, m)) ∈E | (k =! ∧a ∈o) ∨(k =? ∧a′ ∈o)}
For short, we let L and H denote E.L and E.H, respectively.
To ensure that L cannot observe high-level events directly, we demand that H ∩
L = ∅. This alone does not in general prevent information ﬂow from H to L because
L may infer conﬁdential information from H based on the observations that L can
make. A central notion in deﬁning what is meant by inferences is that of low-level
indistinguishability.
Deﬁnition 2. Two traces t and t′ are indistinguishable from L’s point of view, written
t ∼l t′, iff
t|L = t′|L
That is, iff L’s observation of t is equal to L’s observation of t′.
In the sequel we assume that L has complete knowledge of the speciﬁcation Φ that
describes the set of all possible behaviors represented by traces. This means that L may
construct the set of all traces in Φ that are indistinguishable or compatible with a given
observation. Formally, L may from the observation of any trace t, construct a so-called
low-level equivalence set [24] (abbreviated LLES in the sequel):
{t′ ∈Φ | t ∼l t′}
In other words, if L makes an observation t, then L can infer that some trace in the
LLES constructed from t has occurred, but not which one. Security predicates must
demand that L shall not be able to deduce conﬁdential information from the LLESs that
L may construct. This is illustrated in the following example.
Example 1. Let Φ = {⟨⟩, ⟨l1⟩, ⟨h1⟩, ⟨h2⟩, ⟨h1, l1⟩, ⟨h1, l2⟩, ⟨h2, l2⟩}, and assume that L
may observe events l1 and l2 and that h1 and h2 are high-level events. Assume further
a deﬁnition of security that states that L shall not with certainty be able to infer that a
high-level event has occurred. If L makes the observation ⟨l1⟩, then L may infer that
either trace ⟨l1⟩or trace ⟨h1, l1⟩have occurred. Since L cannot know which of these
has occurred (because L cannot observe high-level events directly), and the former trace
does not contain any high-level events, L cannot infer with certainty that a high-level
event has occurred. If L on the other hand observes the trace ⟨l2⟩, then L can infer that
⟨h1, l2⟩or ⟨h2, l2⟩have occurred. Again, L does not know which of these has occurred,
but since both traces contain a high-level event and there are no other traces in Φ that
are compatible with L’s observation, L can infer with certainty that a high-level event
has occurred. Hence, Φ is not secure w.r.t. our deﬁnition of security.

Maintaining Information Flow Security Under Reﬁnement and Transformation
147
In order for Φ to be secure, one must demand that the LLESs that L can construct be
closed w.r.t. some criterion [16]. In the above example, this amounts to demanding that
there must be a trace with no high-level events in each LLES that L can construct.
3.1
Mantel’s Assembly Kit
The schema we propose for describing security predicates is based on a schema pro-
posed by Mantel [16]. He presents an assembly kit in which different notions of security
can be deﬁned. We give here a brief description of this assembly kit. The reader is re-
ferred to [16] for a more details.
In Mantel’s framework, security properties are represented as security predicates
where a security predicate SP is either a single basic security predicate BSP, or a con-
junction of basic security predicates. Each basic security predicate BSP demands that
for any trace t of the speciﬁcation Φ there must be another trace t′ that is indistin-
guishable from t from L’s point of view, and which fulﬁlls a condition Q, the closure
requirement of BSP. The existence of t′, however, is only required if a condition R, the
restriction of BSP, holds. This results in the following schema for the formal deﬁnition
of basic security predicates:
Deﬁnition 3. Speciﬁcation Φ satisﬁes the basic security predicate BSPQR(Φ) for re-
striction R and closure requirement Q iff
∀t ∈Φ · R(Φ, t) ⇒∃t′ ∈Φ · t ∼l t′ ∧Q(t, t′)
(1)
Example 2. The notion of security that is informally described in Ex. 1, may be deﬁned
by instantiating the schema as follows: R ≜TRUE, and Q ≜t′|H = ⟨⟩. That is, for
every trace t there must be a trace t′ such that t′ is indistinguishable from t w.r.t. L and
such that t′ does not contain any high-level events.
4
Reﬁnement
Reﬁnement is the process of making an abstract speciﬁcation more concrete by re-
moving underspeciﬁcation. The standard notion of reﬁnement [11] states that a system
speciﬁcation Φ′ is a reﬁnement of a system speciﬁcation Φ iff
Φ′ ⊆Φ
(2)
Intuitively, there are at least as many implementations that satisfy Φ as there are imple-
mentations that satisfy Φ′. In this sense Φ′ describes its set of implementations more
accurately than Φ, ergo Φ′ is less abstract than Φ.
The reason why secure information ﬂow properties are not preserved by reﬁnement
becomes apparent when one considers again the manner in which these properties are
deﬁned (see Def. 3). That is, Φ is secure if some of its traces satisfy the closure re-
quirement Q. However, by (2) there is no guarantee that a reﬁnement of Φ will include
those traces that satisfy Q, hence secure information ﬂow properties are in general not
preserved by reﬁnement.

148
F. Seehusen and K. Stølen
Intuitively, the cause of this problem is that security properties depend on unpre-
dictability. E.g. the strength of ones password may be measured in terms of how hard
it is for an attacker to guess the password one has chosen. The closure requirement Q
may be seen as the security predicate’s requirement of unpredictability, but traces that
provide this unpredictability may be removed during reﬁnement. This motivates a re-
deﬁnition of the notions of speciﬁcation and reﬁnement where the distinction between
underspeciﬁcation and unpredictability is taken into consideration.
Deﬁnition 4. A system speciﬁcation, denoted Ω, is a set of trace sets. Each trace set in
a speciﬁcation is called an obligation. We demand that the set obtained by collapsing
Ω into a set of traces must be preﬁx-closed, i.e. we demand
t ∈Ω ∧t′ ⊑t ⇒t′ ∈Ω
where Ω is deﬁned 
φ∈Ω φ .
Deﬁnition 5. System speciﬁcation Ω′ is a reﬁnement of system speciﬁcation Ω, written
Ω ⇝Ω′, iff
(∀φ ∈Ω · ∃φ′ ∈Ω′ · φ′ ⊆φ) ∧(∀φ′ ∈Ω′ · ∃φ ∈Ω · φ′ ⊆φ)
This corresponds to so-called limited reﬁnement in STAIRS [20]. For an arbitrary obli-
gation φ at the abstract level, there must be an obligation φ′ at the concrete level such
that φ′ is a reﬁnement of φ in the sense of the standard notion of reﬁnement (see (2)).
Moreover, each obligation at the concrete level must be a reﬁnement of an obligation at
the abstract level. The latter ensures that behavior that was not considered at the abstract
level is not introduced at the concrete level.
The intuition is that the traces within the same obligation may provide underspeci-
ﬁcation, while the obligations provide unpredictability in the sense that an implemen-
tation is required to fulﬁll all obligations of a speciﬁcation. Any valid implementation
must potentially exhibit the behavior described by at least one trace in each obligation.
By implementation we understand a speciﬁcation with no underspeciﬁcation. Given
some program P, let T races(P) be the preﬁx-closed set of traces that can be generated
by executing P, and let
 P  ≜{{t} | t ∈T races(P)}
Then P implements speciﬁcation Ω iff
Ω ⇝ P 
Example 3. Let Ω = {{⟨⟩}, {⟨l⟩}, {⟨l⟩, ⟨h1⟩, ⟨h2⟩, ⟨h1, l⟩, ⟨h2, l⟩}}, and assume that it
is conﬁdential that high-level events have occurred. Ω is secure in this respect; it is easy
to verify that this holds for all implementations of Ω.
Lemma 1. ⇝is transitive2:
Ω ⇝Ω′ ∧Ω′ ⇝Ω′′ ⇒Ω ⇝Ω′′
2 The proofs of all the results in this paper can be in [22].

Maintaining Information Flow Security Under Reﬁnement and Transformation
149
Instances of the schema of Def. 3 are in general not preserved by our notion of reﬁne-
ment. We need to modify the schema such that the distinction of unpredictability and
underspeciﬁcation is exploited. Instead of demanding that there is a trace t′ that satisﬁes
some criterion, we demand that there is an obligation φ such that all its traces satisfy
that criterion.
Deﬁnition 6. Speciﬁcation Ω satisﬁes the basic security predicate BSPQR(Ω) for re-
striction R and closure requirement Q iff
∀t ∈Ω · R(Ω, t) ⇒∃φ ∈Ω · ∀t′ ∈φ · t ∼l t′ ∧Q(t, t′)
The intuition of (Def. 6) is that obligations, as opposed to individual traces, may be
seen as providing the unpredictability required by instances of the schema. Note that
the schema may be instantiated by the same instances of R and Q that are presented in
Mantel’s paper.
In order to ensure that instances of the schema are preserved by reﬁnement, we need
to disallow some restrictions R whose truth value depend on the absence of traces. We
therefore require that all restrictions R satisfy the following condition
(T ′ ⊆T ∧R(T ′, t)) ⇒R(T, t)
(3)
for arbitrary traces t and trace sets T and T ′. All instances of R presented in Mantel’s
paper satisfy condition (3).
Theorem 1. BSPQR is preserved by reﬁnement for arbitrary restrictions R satisfying
(3) and closure requirements Q:
Ω ⇝Ω′ ∧BSPQR(Ω) ⇒BSPQR(Ω′)
The notion of reﬁnement introduced above corresponds to what is often referred to as
property reﬁnement or behavioral reﬁnement [2]. Property reﬁnement does not capture
change in data-structure, i.e. the replacement of abstract event representations by con-
crete event representations. This is in contrast to reﬁnement notions such as data reﬁne-
ment [12], interface reﬁnement [2], or action reﬁnement [23] which roughly speaking
may be understood as property reﬁnement modulo a translation between the concrete
and the abstract data structure. Our notion of property reﬁnement may be generalized
into a notion of action reﬁnement (actually event reﬁnement in our case) using upwards
and downwards simulation [3,12] in a fairly standard manner. To characterize under
which conditions this notion of reﬁnement is security preserving is, however, far from
trivial. In the following, attention is restricted to a special case of this problem, namely
under which conditions transformations are security preserving. A transformation may
be understood a special case of action reﬁnement where the concrete speciﬁcation is
generated automatically from the abstract speciﬁcation.
5
Transformation
The notion of reﬁnement addressed above is a binary relation on speciﬁcations that
formalizes the process of stepwise development by removal of underspeciﬁcation. A

150
F. Seehusen and K. Stølen
sd HTTP
sd TCP
:Client
:Server
xalt
:Client
:Server
Session1
ref
Sessionn
ref
...
request
response
Fig. 1. HTTP to TCP
transformation on the other hand is an executable function taking an abstract syntactic
speciﬁcation as input and yielding a concrete syntactic speciﬁcation as output. Thus
transformation is a syntactic notion. Since security properties are deﬁned on the seman-
tics of speciﬁcations (i.e. on traces), we deﬁne a semantic interpretation of transforma-
tions which enables us to assert whether a transformation maintains security.
In Sect. 5.1, we give an example of a transformation that motivates our semantic
interpretation of transformations given in Sect. 5.2. In Sect. 5.3, we propose a condi-
tion under which interpretations of transformations maintain security. Sect. 5.4 gives an
example that clariﬁes some of the points made in Sect. 5.3.
5.1
Example: Transforming HTTP Speciﬁcations to TCP Speciﬁcations
The HTTP protocol is bound to the TCP protocol. One way of doing this binding dur-
ing runtime is to create a new so-called TCP session for each HTTP request-response
pair. A TCP-session consists of three phases: First a connection is established between
the two sides of communication, then the HTTP request and response messages are
segmented, encapsulated by TCP frames, and transmitted. Finally the connection is ex-
plicitly terminated.
A transformation that takes a speciﬁcation that describes communication at the
HTTP level and produces a speciﬁcation that describes communication at the TCP level
may be deﬁned in accordance to how the HTTP protocol is bound to the TCP proto-
col. Such a transformation may be regarded as a transformation from the abstract to
the concrete if one takes HTTP speciﬁcations as being at the abstract level and TCP
speciﬁcations as being at the concrete level.
The UML interaction diagram on the left hand side of Fig. 1 describes a simple
communication scenario at the HTTP level. The diagram on the right hand side is the
result of applying a transformation from HTTP to TCP to the HTTP diagram. Here the
so-called xalt-operator from STAIRS [8] speciﬁes unpredictability3 between different
interaction scenarios. The ref-operator references other interaction diagrams that in this
example describe different TCP-sessions. The reason why the HTTP request-response
pair described in the diagram on the left hand side is translated to more than one TCP-
session is that the TCP protocol must handle issues that are transparent at the HTTP
3 Termed explicit non-deterministic choice in STAIRS.

Maintaining Information Flow Security Under Reﬁnement and Transformation
151
level, e.g. message overtaking. One TCP session may for example describe the situation
in which messages are received in the same order that they are transmitted, another may
describe the situation in which this is not the case and so on. The reader is referred to
[7,8,9,21] to see how UML interaction diagrams can be given trace semantics.
To assert whether the transformation from HTTP to TCP maintains security, we need
to interpret the transformation in terms of how abstract traces (representing HTTP com-
munication) are translated to concrete traces (representing TCP communication). There
are three considerations that need to be taken into account when deﬁning such an inter-
pretation. First, an abstract trace may correspond to several concrete traces. The reasons
for this is that TCP protocol must handle issues that are transparent at the HTTP level.
Second, an abstract event may be decomposed into several concrete events because
each HTTP package may be segmented into more than one TCP package during the
TCP transmission phase. Third, there may be traces at the concrete level that have no
abstract equivalent. To see this, let ⟨e⟩represent the transmission of a HTTP package.
Since a HTTP package may be segmented into several TCP packages, ⟨e⟩may for ex-
ample be translated into the trace ⟨e1, e2, e3⟩where events e1, e2, and e3 represent the
transmission of TCP packages. Traces ⟨e1⟩and ⟨e1, e2⟩may also be valid traces at the
TCP level (these traces are in fact required to be present in a concrete speciﬁcation
since we assume that speciﬁcations are preﬁx-closed). Now, the TCP trace ⟨e1, e2, e3⟩
corresponds to ⟨e⟩at the HTTP level since the TCP trace is complete in the sense that it
represents the transmission of the entire HTTP message. But what about the TCP traces
⟨e1⟩and ⟨e1, e2⟩, do these traces correspond to ⟨e⟩at the abstract level? The answer is
no if the trace ⟨e⟩is meant to represent the transmission of an entire HTTP package.
The TCP traces ⟨e1⟩and ⟨e1, e2⟩do not correspond to the empty HTTP trace (⟨⟩) either,
because the empty trace is meant (by any reasonable interpretation) to represent a sce-
nario in which no communication occurs. From this we can conclude that, in general,
there may be traces at the concrete level for which there are no corresponding traces at
the abstract level.
5.2
Transformations from a Semantic Perspective
Syntactically, a transformation is an executable function translating abstract (syntactic)
speciﬁcations to concrete (syntactic) speciﬁcations. Semantically, we interpret traces in
terms of how abstract traces are translated to concrete traces.
In the following, let A denote some ﬁxed but arbitrary abstract syntactic speciﬁca-
tion, T be a some transformation, and T (A) denote the concrete speciﬁcation obtained
by applying T to A. Let Ωa denote the semantics of A and Ωc denote the semantics of
T (A). T is interpreted by a set of functions
F ⊆Ωa →Ωc
mapping traces in Ωa to traces in Ωc. The reason why we use a set of functions and not
a single function is, as explained in the example, that a syntactic transformation may
represent the same abstract trace by several concrete traces.
We say that the set of functions F is a valid interpretation of T w.r.t. A if Ωc is a
translation of Ωa w.r.t. F as deﬁned below. We ﬁrst deﬁne the notion of translation for
obligations, then we lift this notion to (semantic) speciﬁcations.

152
F. Seehusen and K. Stølen
Deﬁnition 7. Obligation φc is a translation of obligation φa w.r.t. function f, written
φa →f φc, iff
φc ⊆{f(t) | t ∈φa}
Deﬁnition 8. Speciﬁcation Ωc is a translation of speciﬁcation Ωa w.r.t. interpretation
F, written, Ωa →F Ωc, iff
∀f ∈F · ∀φa ∈Ωa · ∃φc ∈Ωc · φa →f φc
Our interpretation of transformations is similar to data reﬁnement in that both notions
roughly speaking may be understood as reﬁnement modulo a translation of traces. More
precisely:
Lemma 2. Let Ωc be contained in the image of Ωa under the identity transformation
id, then
Ωa →id Ωc ⇔Ωa ⇝Ωc
Here im(Ωa, F), the image of Ωa under some F, is the set of obligations that are
translations of obligations in Ωa w.r.t. F:
im(Ωa, F) ≜{φc | ∃φa ∈Ωa · ∃f ∈F · φa →f φc}
(4)
A concrete speciﬁcation is not necessarily contained in the image of the abstract speciﬁ-
cation it is translated from. The reason for this is, as explained in the previous example,
that there may be concrete traces that do not have any abstract equivalent.
If F1 and F2 are interpretations, then F1 ◦F2 is understood as the interpretation
obtained by functional point-to-point composition of all functions from F1 and F2.
That is,
F1 ◦F2 ≜{f1 ◦f2 |f1 ∈F1 ∧f2 ∈F2}
(5)
where f1 ◦f2(t) = f1(f2(t)).
Lemma 3. →is transitive:
Ω0 →F1 Ω1 ∧Ω1 →F2 Ω2 ⇒Ω0 →F2◦F1 Ω2
We denote by F −1(tc), the set of all traces in Ωa that can be translated to tc by the
functions in F:
F −1(tc) ≜{ta ∈Ωa | ∃f ∈F · f(ta) = tc}
(6)
5.3
Secure Transformations
A transformation T maintains the security of an abstract speciﬁcation A if there is a
valid secure interpretation of T w.r.t. A. The notion of secure interpretation obviously
depends on what is meant by secure, and should therefore be parameterized by secu-
rity properties. In doing so, one must take into account that the security requirement at
the abstract level may be syntactically and semantically different from the “correspond-
ing” security requirement at the concrete level. One reason for this is that events at the

Maintaining Information Flow Security Under Reﬁnement and Transformation
153
abstract level may differ from the events at the concrete level. Another reason is that
there may be concrete traces that do not have any corresponding abstract trace. The
concrete security requirement must therefore handle traces that may not be taken into
account by the security requirement at the abstract level.
The notion of secure interpretation is formally deﬁned in the following
Deﬁnition 9. Let Ωa →F Ωc, then the interpretation F is secure w.r.t. the abstract and
concrete restrictions Ra and Rc and abstract and concrete closure requirements Qa
and Qc if the following conditions are satisﬁed
Rc(Ωc, tc) ⇒∃t′ ∈F −1(tc) · Ra(Ωa, t′)
(7)
(Rc(Ωc, tc) ∧∀t′ ∈φa · ta ∼l t′ ∧Qa(ta, t′)) ⇒
∃φc ∈Ωc · ∀t′ ∈φc · tc ∼l t′ ∧Qc(tc, t′)
(8)
for all tc ∈Ωc, φa ∈Ωa, and ta ∈F −1(tc).
Def. 9 may be understood to capture a rule that allows us to exploit that we have estab-
lished Qa at the abstract level when establishing Qc at the concrete level. We believe
that verifying (7) and (8) in most practical situations will be straightforward and more
feasible than checking the security property at the concrete level directly.
Put simply, the ﬁrst condition of Def. 9 just ensures that the transformation does not
weaken the restriction R. The second condition ensures that the transformation does not
strengthen the closure requirement Q and that low-level equality is preserved.
It follows from (7) that the concrete restriction Rc must ﬁlter away (i.e. yield false
for) the concrete traces tc that do not have any corresponding abstract trace. This is
reasonable because one cannot take advantage of the fact that the abstract speciﬁcation
is secure when proving that tc does not compromise security. It may therefore be the
case that a new security analysis must be carried out at the concrete level for those traces
that do not have an abstract equivalent.
When we relate Def. 9 to rules that describe date reﬁnement in an assumption /
guarantee or pre-post setting, we note that weakening/strengthening is the other way
around. E.g., when reﬁning a pre-post speciﬁcation, one may weaken the pre-condition
and strengthen the post-condition. The reason is that a pre-post condition is a speciﬁ-
cation that is reﬁned into another speciﬁcation while a restriction-closure predicate is a
property that has been proved to hold for a speciﬁcation that is translated to a concrete
speciﬁcation and whose validity should be maintained.
Theorem 2. Let F be a interpretation that is secure w.r.t. restrictions Ra and Rc, and
closure requirements Qa and Qc, then F maintains security in the following sense:
BSPQaRa(Ωa) ∧Ωa →F Ωc ⇒BSPQcRc(Ωc)
5.4
Example: Why Security Requirements Change
Let Ωa be an abstract speciﬁcation consisting of two clients cl and ch that communi-
cate with a server s via the HTTP protocol (see Fig. 2). Assume that cl, based on its

154
F. Seehusen and K. Stølen
Client-server specification
ClientL
Server
ClientH
Fig. 2. Client-server example
observation of its communication with s and its knowledge of the system speciﬁca-
tion, should not be able to deduce information about the behavior of ch. More formally,
both the clients and the server can be represented as agents (recall the role of agents
from Sect. 2). Thus the low-level observer is deﬁned {cl} and the high-level observer
is deﬁned {ch}. The security requirement on the HTTP level may be formalized by
instantiating the schema of Def. 6 by some predicates Ra and Qa.
Assume that F interprets a transformation from HTTP to TCP deﬁned such that each
event representing the transmission or reception of a HTTP message is translated into
a complete (in the sense of the previous example) sequence of events describing the
corresponding TCP messages. Let Ωc be a translation of Ωa w.r.t. F and assume that
Ωc also contains non-complete traces that do not correspond to any traces at the HTTP
level.
Assume that Ωa is secure (i.e. BSPRaQa(Ωa) is true) and that we want to check if
the traces on the concrete level that have a corresponding abstract representation are
secure w.r.t. our information ﬂow property. In order to do this, we can create predicate
on the concrete level that ﬁlters away those traces that do not have a corresponding
abstract representation. More formally, the concrete restriction Rc is deﬁned Rc(Ω, t) ≜
Ra(Ω, t) ∧T CP OK(t) where T CP OK is a predicate that yields true if the TCP
trace t corresponds to a HTTP trace and false otherwise. Note that we are assuming that
Ra may take HTTP traces as well as TCP traces as arguments. The concrete security
property can now be obtained by instantiating the schema of Def. 6 by the predicates
Rc and Qa (since the closure requirement Q is left unchanged in this example).
If one wants to check that the TCP traces that do not correspond to any HTTP traces
are secure w.r.t. some requirement, one cannot take advantage of the fact that Ωa is
secure. Therefore additional security analysis may be required w.r.t. these traces.
6
Conclusions and Related Work
In [21], we deﬁned a secure information ﬂow property in the semantics of STAIRS
[7,8,9] and showed that this property was preserved by reﬁnement and transformation.
This paper simpliﬁes and generalizes these results by considering, not only one, but
many kinds of information ﬂow properties. This paper also considers a more general no-
tion of security preservation than considered in [21]. More precisely, this paper makes
two contributions to the study of secure information ﬂow. The ﬁrst is a schema for
specifying information ﬂow properties that are preserved by the STAIRS notion of re-
ﬁnement. The second is the deﬁnition of a semantic interpretation of transformations
and a condition under which transformations maintain security.

Maintaining Information Flow Security Under Reﬁnement and Transformation
155
There are a number of papers related to information ﬂow security and reﬁnement.
Jacob is the ﬁrst person that we are aware of to show that secure information ﬂow
properties are not preserved by the traditional notion of reﬁnement [13]. This became
known as the reﬁnement paradox. It has later been observed that this “paradox” is a
manifestation of failing to clearly distinguish between underspeciﬁcation and unpre-
dictability. As far as we are aware of, this observation was ﬁrst made in [19].
To the extent of our knowledge, the work of Heisel. et. al. [10] is similar to ours in
that they both distinguish between underspeciﬁcation and unpredictability and consider
the notion of data reﬁnement. The main differences between their work and ours are: (1)
They work in a probabilistic setting, and thus their formalism differs from ours. (2) They
do not consider information ﬂow properties but a notion of conﬁdentiality based on low-
level indistinguishability only. (3) Their notion of conﬁdentiality preserving reﬁnement
is different from ours in that they build the condition of conﬁdentiality preservation
into the deﬁnition of reﬁnement. W.r.t. reﬁnement, we have taken the dual approach of
strengthening the notion of security.
The work of J¨urjens [14,15] is also related to ours. Some of the main differences
between his work and ours are: (1) His formalism differs from ours. (2) While J¨urjens
distinguishes between underspeciﬁcation and unpredictability in order to deﬁne reﬁne-
ment preserving properties of conﬁdentiality (secrecy) and integrity, he does not rely
on this distinction in the deﬁnition of his information ﬂow property. That is, the secure
information ﬂow property is satisﬁed iff each behavior reﬁnement to a deterministic
speciﬁcation satisﬁes this property, i.e. he effectively closes the property under a no-
tion of behavior reﬁnement that does not distinguish between underspeciﬁcation and
unpredictability.
Three notable papers that addresses information ﬂow security and reﬁnement are
[1,6,17]. The main difference between these papers and ours is that all these investigate
conditions under which certain notions of reﬁnement are security preserving without
distinguishing between underspeciﬁcation and unpredictability. Since this distinction
is made in our formalism, we consider one notion of reﬁnement only, and strengthen
instead our notion of security in an intuitive manner. Hence, there is no need to propose
conditions with which to check that a given reﬁnements preserve security.
We are not aware of any work that explicitly address transformation and information
ﬂow security. Moreover, we are not aware of any work that show how to preserve secure
information ﬂow properties under a notion of reﬁnement that takes the distinction of
underspeciﬁcation and unpredictability into consideration.
The main emphasis of this paper is on semantics. In future work, we will address
syntactic transformations in more detail. We are also planning to address composition
and transformation of security predicates. Eventually, we would like to develop a com-
puterized tool that will check whether transformations maintain security.
Acknowledgments
This work has been funded by the Research Council of Norway through the project SE-
CURIS (152839/220). We would like to thank the anonymous referees for their useful
comments and suggestions.

156
F. Seehusen and K. Stølen
References
1. Bossi, A., Focardi, R., Piazza, C., Rossi, S.: Reﬁnement operators and information ﬂow se-
curity. In: SEFM 2003. 1st International Conference on Software Engineering and Formal
Methods, pp. 44–53. IEEE Computer Society Press, Los Alamitos (2003)
2. Broy, M., Stølen, K.: Speciﬁcation and development of interactive systems. In: FOCUS on
streams, interface, and reﬁnement, Springer, Heidelberg (2001)
3. de Roever, W.-P., Engelhardt, K.: Data Reﬁnement: Model-Oriented Proof Methods and their
Comparison. In: Cambridge tracts on theoretical computer science, vol. 47, Cambridge Uni-
versity Press, Cambridge (1998)
4. Focardi, R., Gorrieri, R.: Classiﬁcation of security properties (part i: Information ﬂow).
In: Focardi, R., Gorrieri, R. (eds.) Foundations of Security Analysis and Design. LNCS,
vol. 2171, pp. 331–396. Springer, Heidelberg (2001)
5. Goguen, J.A., Meseguer, J.: Security policies and security models. In: IEEE Symposium on
Security and Privacy, pp. 11–20. IEEE Computer Socity Press, Los Alamitos (1982)
6. Graham-Cumming, J., Sanders, J.W.: On the reﬁnement of non-interference. In: Proceedings
of the IEEE Computer Security Foundations Workshop, pp. 35–42. IEEE Computer Society
Press, Los Alamitos (1991)
7. Haugen, Ø., Husa, K.E., Runde, R.K., Stølen, K.: Why timed sequence diagrams require
three-event semantics. Research Report 309, Department of Informatics, University of Oslo
(2004)
8. Haugen, Ø., Husa, K.E., Runde, R.K., Stølen, K.: STAIRS towards formal design with se-
quence diagrams. Journal of Software and Systems Modeling 4(4), 355–367 (2005)
9. Haugen, Ø., Stølen, K.: STAIRS – steps to analyse interactions with reﬁnement semantics.
In: Stevens, P., Whittle, J., Booch, G. (eds.) UML2003 - The Uniﬁed Modeling Language.
Modeling Languages and Applications. LNCS, vol. 2863, pp. 388–402. Springer, Heidelberg
(2003)
10. Heisel, M., Pﬁtzmann, A., Santen, T.: Conﬁdentiality-preserving reﬁnement. In: CSFW-14
2001. 14th IEEE Computer Security Foundations Workshop, pp. 295–306. IEEE Computer
Society Press, Los Alamitos (2001)
11. Hoare, C.A.R.: Communicating Sequential Processes. Series in computer science. Prentice-
Hall, Englewood Cliffs (1985)
12. Hoare, C.A.R., He, J., Sanders, J.W.: Prespeciﬁcation in data reﬁnement. Information Pro-
cessing Letters 25(2), 71–76 (1987)
13. Jacob, J.: On the derivation of secure components. In: Proc. of the IEEE Symposium on
Security and Privacy, pp. 242–247. IEEE Computer Society Press, Los Alamitos (1989)
14. J¨urjens, J.: Secrecy-preserving reﬁnement. In: Oliveira, J.N., Zave, P. (eds.) FME 2001.
LNCS, vol. 2021, pp. 135–152. Springer, Heidelberg (2001)
15. J¨urjens, J.: Secure systems development with UML. Springer, Heidelberg (2005)
16. Mantel, H.: Possibilistic deﬁnitions of security - an assembly kit. In: CSFW’00. IEEE Com-
puer Security Foundations Workshop, pp. 185–199. IEEE Computer Society Press, Los
Alamitos (2000)
17. Mantel, H.: Preserving information ﬂow properties under reﬁnement. In: IEEE Symposium
on Security and Privacy, pp. 78–91. IEEE Computer Society Press, Los Alamitos (2001)
18. Object Management Group.: Architecture Board ORMSC. Model Driven Architecture
(MDA). Document number ormsc/2001-07-01 (2001)
19. Roscoe, A.: CSP and determinism in security modelling. In: IEEE Symposium on Security
and Privacy, pp. 114–127. IEEE Computer Society Press, Los Alamitos (1995)
20. Runde, R.K., Haugen, Ø., Stølen, K.: Reﬁning uml interactions with underspeciﬁcation and
nondeterminism. Nordic Journal of Computing 12(2), 157–188 (2005)

Maintaining Information Flow Security Under Reﬁnement and Transformation
157
21. Seehusen, F., Stølen, K.: Information ﬂow property preserving transformation of UML inter-
action diagrams. In: SACMAT 2006. 11th ACM Symposium on Access Control Models and
Technologies, pp. 150–159. ACM Press, New York (2006)
22. Seehusen, F., Stølen, K.: Maintaining information ﬂow security under reﬁnement and trans-
formation. Technical report SINTEF A311, SINTEF ICT (2006)
23. van Glabbeek, R.J., Goltz, U.: Reﬁnement of actions and equivalence notions for concurrent
systems. Acta Informatica 37(4/5), 229–327 (2001)
24. Zakinthinos, A., Lee, E.S.: A general theory of security properties. In: Proc. of the IEEE
Computer Society Symposium on Research in Security and Privacy, pp. 94–102. IEEE Com-
puter Society Press, Los Alamitos (1997)

A Classiﬁcation of Delegation Schemes for
Attribute Authority⋆
Ludwig Seitz, Erik Rissanen, and Babak Sadighi
SPOT, SICS
Box 1263, SE-16429 KISTA, Sweden
{ludwig, mirty, babak}@sics.se
Abstract. Recently assertions have been explored as a generalisation of
certiﬁcates within access control. Assertions1 are used to link arbitrary
attributes (e.g. roles, security clearances) to arbitrary entities (e.g. users,
resources). These attributes can then be used as identiﬁers in access
control policies to refer to groups of users or resources.
In many applications attribute management does not happen within
the access control system. External entities manage attribute assign-
ments and issue assertions that are then used in the access control sys-
tem. Some approaches also allow for the delegation of attribute authority,
in order to spread the administrative workload. In such systems the con-
sumers of attribute assertions issued by a delegated authority need a
delegation veriﬁcation scheme.
In this article we propose a classiﬁcation for schemes that allow to
verify delegated authority, with a focus on attribute assertion. Using our
classiﬁcation, one can deduce some advantages and drawbacks of diﬀerent
approaches to delegated attribute assertion.
1
Introduction
Attributes are currently regarded as the most generic way of referring to users
and resources in access control systems. User identity, group memberships, secu-
rity clearances, and roles can all be expressed using attributes. The eXtensible
Access Control Markup Language (XACML)[1] uses attributes in order to spec-
ify applicable subjects, resources and actions in access control policies.
Access control systems are important components of distributed computing
infrastructures (e.g. in Grid computing [2]). Often such infrastructures allow to
share resources belonging to diﬀerent entities, based on a mutually agreed access
control policy. In order to enforce these policies locally for external users, the ac-
cess control system needs to be capable to fetch attributes from external sources.
In complex cooperative scenarios, where diﬀerent resources are administrated
by diﬀerent authorities and used by a variety of users, one can increase the
eﬃciency of administration by delegating attribute authority. Delegation helps
⋆This work was carried out during the tenure of an ERCIM “Alain Bensoussan”
Fellowship Programme.
1 We use assertions synonymous for attribute certiﬁcates.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 158–169, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

A Classiﬁcation of Delegation Schemes for Attribute Authority
159
to spread the responsibility for speciﬁc security decisions to the people who are
capable and in charge of taking them, instead of burdening a single administrator
with it. This makes the system less error prone and makes it faster to implement
attribute assignments on a local level.
In such a scenario, the problem arises how to verify whether an attribute
assertion is valid and properly authorised. Various approaches for doing this
exist, which all have diﬀerent drawbacks and advantages.
In this article we present a method to classify diﬀerent veriﬁcation approaches
based on two simple properties and present advantages and drawbacks of the
various models based on our classiﬁcation.
The remainder of this article is organised as follows: In section 2 we give a
short overview of background knowledge necessary to reason about authority
delegation. Section 3 presents related work. In section 4 we present our classiﬁ-
cation approach. We then discuss the advantages and drawbacks of the diﬀerent
models that can be obtained from the classiﬁcation in section 5. Finally we
summarise and give a conclusion in section 6.
2
Background
We assume an access control model, where every attribute has a source of author-
ity (SOA), which is the initial entity that has the power to assign this attribute
to other users. In our model a SOA can delegate administrative rights concerning
the attribute, thus allowing other entities to act as Attribute Authorities (AA).
Such a delegation allows an AA to assign attributes to entities and can also
authorise further delegation of their authority. The delegation can be subject to
conditions speciﬁed by its issuer, that constrain the authority it conveys.
The underlying delegation model that we implicitly use is more formally spec-
iﬁed in the publications “Constrained Delegation” [3] and “Using Authority Cer-
tiﬁcates to Create Management Structures” [4].
3
Related Work
Attribute assertion has received intensive attention from the research community
in the last years. As a result of this, two competing standards have emerged
for issuing attribute assertions. Furthermore several systems using either one
of these standards provide attribute assertion services. In this section we ﬁrst
discuss attribute assertion standards and then present some systems that use
these standards to provide attribute assertion services.
3.1
Attribute Assertion Standards
The SAML [5] standard by the OASIS consortium deﬁnes an XML based syntax
and protocols for requesting and providing diﬀerent kinds of assertions, including
attribute assertions. The current core speciﬁcation of SAML does not address

160
L. Seitz, E. Rissanen, and B. Sadighi
delegated assertions in any way. There are however proposals to use the existing
extension points of SAML to address this issue. The ﬁrst approach by Navarro
et al. [6] is based on the same delegation model as our approach (see section 2).
It uses assertion chains to verify delegated authority. The second approach by
Wang et al. [7] uses a slightly diﬀerent delegation model, however veriﬁcation is
also done with assertion chains.
RFC 3281 [8] deﬁnes a proﬁle for attribute certiﬁcates (AC) based on the
popular X.509 standard for authentication. The current proﬁle speciﬁcally rec-
ommends not to support delegation because the administration and processing
of AC chains is deemed to be too complex. Furthermore for each particular set of
attributes only one SOA may exist that functions as AC issuer. However several
systems using X.509 ACs have nevertheless implemented delegation mechanisms
(c.f. next section).
3.2
Authority Delegation Approaches
The public key infrastructures (PKI) [9] have been dealing with a restricted form
of attribute assertion since quite a while. Within PKI, the need for a delega-
tion mechanism was quickly discovered. Therefore PKI implements a delegation
mechanism based on roots of trust and certiﬁcate chains.
The PRIMA [10], [11] access control system, developed since 2003 at the
Virginia Polytechnic Institute uses X.509 ACs to encode attribute assertions.
PRIMA has implemented AC chain veriﬁcation for delegation of attributes.
PERMIS [12] is a role based access control system that also uses X.509 ACs
to provide attribute assertions (limited to role assignments). PERMIS makes use
of a Delegation Issuing Service [13] in order to allow for delegation of attribute
assertions. In this approach a delegation service issues attribute assertions on
behalf of the attribute authorities. Three diﬀerent modes of operation for this
service have been proposed.
In the ﬁrst mode2, the service holds a delegation assertion from the SOA that
gives it the right to issue attribute assertions on behalf of the other attribute
authorities.
The second mode3, the attribute authorities provide the service with delega-
tion assertions that give the server the right to issue assertions on their behalf.
When using the third mode4, the service does not have any delegation as-
sertion and the veriﬁcation of the attribute assertions it issues has to be done
externally.
4
Modelling Delegated Attribute Assertion
Our approach for modelling delegated attribute assertion was to examine dif-
ferent possible schemes and to extract general properties. We ﬁrst present the
examined schemes and then the properties we have found.
2 Called PMI, for Privilege Management Infrastructure.
3 Called AC PKI mode, for Attribute Certiﬁcate and Public Key Infrastructure.
4 Called PKI mode.

A Classiﬁcation of Delegation Schemes for Attribute Authority
161
4.1
Examined Assertion Schemes with Delegation
We have considered four schemes for issuing and verifying delegated assertions,
which we describe in the following sections. These may not cover all possible
approaches, but they provide us with enough diversity to ﬁnd two general prop-
erties. Furthermore all four of the schemes we have examined assume an attribute
push approach, where the user somehow acquires assertions of his attributes and
pushes them to the veriﬁer. Equivalent models can easily be derived for an at-
tribute pull approach, where the veriﬁer pulls the relevant attributes of a user
upon a request.
Shared key assertion. In the shared key assertion scheme, each attribute is
represented by a single asymmetric key pair. The private key is used to sign
assertions of the attribute and the public key is distributed to veriﬁers, so they
can check the validity of these assertions.
The SOA of the attribute controls the private key, and can issue it to any
entity he wishes to make an AA for this attribute. These AAs can further delegate
attribute assertion to new AAs by disclosing the private key to them5.
This scheme is represented in ﬁgure 1.
issue
assertions
public
Attribute:
private
AAs
Verifiers
User
distribute
key
publish
key
use
attribute
SOA
creates
distribute
key
Fig. 1. Shared key attribute assertion and delegation
Sharing a group key is a common form of authority delegation. The security
weaknesses are obvious and well known, but the simplicity of this mechanism
makes it useful in some special cases (e.g. controlling access to a photocopying
machine).
Assertion chaining. In the Assertion chaining scheme, every attribute is
linked to a speciﬁc SOA. The relation of attribute to SOA is published to the
5 Note that the SOA does not use his own private key for this, a special asymmetric
key pair is generated to represent the attribute.

162
L. Seitz, E. Rissanen, and B. Sadighi
veriﬁers. The SOA can sign assertions with an asymmetric key pair that is some-
how linked to his identity (e.g. by a X.509 public key certiﬁcate).
Furthermore the SOA and authorised AAs can delegate attribute assertion to
other AAs by issuing attribute delegation assertions.
The veriﬁer has to check that the initial certiﬁcate is signed by the SOA of
the attribute and that the assertion chain is valid. The precise format of these
delegation assertions and the veriﬁcation algorithm for such assertion chains are
out of the scope of this article.
PKI delegation, PRIMA and the AC PKI mode of the PERMIS Delegation
Issuing Service correspond to this approach6.
Figure 2 shows this assertion scheme.
User
issue
assertions
Verifier
use
attribute
AA
verify
assertion
chain
issue
delegation
issue
delegation
AA
SOA
Fig. 2. Attribute assertion and delegation through assertion chains
Delegation server. In the Delegation server scheme, every attribute is linked
to a speciﬁc delegation server, which is under control of the attribute’s SOA.
The delegation server maintains a list of authorised attribute AAs, which can
also specify the power to further delegate attribute authority.
AAs issue attribute assertions using their personal asymmetric key pair for
signatures.
Veriﬁers must check the validity of the assertions signature and then proceed
to query the delegation server in order to determine if the AA that signed this
assertion had the administrative power to do so.
The PMI mode of the PERMIS Delegation Issuing Service corresponds to this
approach.
This assertion scheme is illustrated by ﬁgure 3.
Assertion server. The Assertion server scheme is somewhat similar to the
Delegation server scheme, as it also features a speciﬁc server for each attribute.
As in the previous scheme this server is under control of the attribute’s SOA.
6 However in PERMIS the last AA in a delegation chain is always the Delegation
Issuing Service.

A Classiﬁcation of Delegation Schemes for Attribute Authority
163
Delegation server
authorizes
AAs
User
issue
assertions
Verifier
verify AA
use
attribute
SOA and AAs
create
delegations
Fig. 3. Attribute assertion and delegation using a delegation server
However it not only stores the authorised attribute AA’s, but also the attribute
assignments they have issued.
The assertion server uses an asymmetric key pair in order to sign attribute
assertions. The public key is previously published to the assertion veriﬁers.
Upon reception of an attribute assertion the veriﬁer only has to check the
validity of the assertion’s signature and to make sure the signature key belongs
to the corresponding assertion server.
We have implemented this scheme as an attribute provider for XACML
PDPs7. It is based on the upcoming XACML 3.0 standard [14] and supports
delegation of attribute authority as well as attribute hierarchies.
The PKI mode of the PERMIS Delegation Issuing Service also corresponds
to this approach.
This scheme is illustrated by ﬁgure 4.
Assertion server
assign
attributes
SOA and AAs
User
Verifier
assert
attributes
public
private
publish key
use
attribute
create
delegations
Fig. 4. Attribute assertion and delegation using an assertion server
7 The sourcecode is available under http://www.sics.se/spot/assertion server.html

164
L. Seitz, E. Rissanen, and B. Sadighi
4.2
General Properties of Assertion Schemes
If one examines the previously presented assertion schemes, one can ﬁnd two
general properties, which account for the similarities and the diﬀerences of these
schemes.
The ﬁrst property is how attribute authorities are represented for veriﬁers.
There can either be an individual representation (i.e. each AA signs assertions
with his own key) or a group representation where only a single AA persona is
visible for the veriﬁers (i.e. there is only one key per attribute that is used to
sign assertions). Figure 5 illustrates this property.
Opaque
AA
Verifier3. verify AA’s
signature
Bob (AA)
Bob
Authority
service
group representation of AAs:
individual representation of AAs:
1. assign
attribute
2. issue
assertion
Verifier
1. issue
assertion
2. verify Bob’s
signature
3. query Bob’s
authority
4. confirm/deny
Bob’s authority
Fig. 5. Representation of AA’s for the veriﬁer
The second property is how delegation statements are issued. They can either
be pre-deﬁned (i.e. authority is transferred previously, independent of a speciﬁc
request) or issued on-demand (i.e. authority is veriﬁed individually for each
request). This property is illustrated in ﬁgure 6.
We can now assign property values to the previously presented assertion
schemes as follows:
In the Shared key approach, each attribute is associated to one assertion
signer: the asymmetric key pair of the attribute. Therefore only a group repre-
sentation of AAs for the attribute is visible to the veriﬁers. Furthermore delega-
tion statements are implicitly made by sharing the private key. Since this must
happen before a delegated AA can issue an assertion, delegation statements must
be considered pre-deﬁned.
The Assertion chaining approach makes use of the individual asymmetric
key pairs of the AAs in order to sign assertions. It has therefore an individual
representation of AAs for the veriﬁers. In general delegation certiﬁcates would

A Classiﬁcation of Delegation Schemes for Attribute Authority
165
time
delegation
authority
is distributed
attribute
is asserted
assertion &
authority of AA’
is verified
time
delegation
authority
is stored
attribute
is asserted
assertion
is verified
authority of AA’ is
confirmed on−line
pre−defined delegation statements:
on−demand delegation statements:
SOA/AA
AA’
AA’
Verifier
Verifier
SOA/AA
AA’
Verifier
Verifier
SOA
Verifier
Fig. 6. Delegation statements about AA’s for the veriﬁer
be relatively long lived and delegation statements can therefore considered to be
pre-deﬁned.
The approach using a delegation server also makes use of the individual asym-
metric key pairs of the AAs to sign assertions. It is therefore classiﬁed as having
individual AA representation. Delegation statements for an AA are checked by
the delegation server on-demand, based on a request by a veriﬁer.
When using an assertion server as in the fourth approach, we have a single
asymmetric key pair signing all assertions and therefore a group representation
of AAS for the assertion veriﬁer. As the assertion server functions in the same
way as the delegation server as far as delegation is concerned, it is also considered
to issue delegation statements on-demand.
These results are summarised in table 1.
Table 1. Classifying delegated assertion schemes by AA representation for veriﬁers
and issuing of delegation statements
AA representation for veriﬁers
delegation statements
individual
group
pre-deﬁned
Shared key
Assertion chaining
on-demand
Assertion server Delegation server
It is clear that the borders between these properties are not strictly speaking
clear cut. One could imagine a shared key scheme where an attribute would
be associated to several asymmetric key pairs. However this remains a group
representation of AAs, unless you assign speciﬁc keys to speciﬁc AAs. In the
latter case all of these keys would have to be published to the veriﬁers in a
trusted way, which can then be seen as a variant of assertion chaining.
Another example would be if the delegation assertions used in assertion chain-
ing would only be issued with a very short lifetime, supposedly for the use with

166
L. Seitz, E. Rissanen, and B. Sadighi
a speciﬁc attribute assertion. Since either the veriﬁer or the user would have to
somehow gather these delegation assertions from a previously known authority,
this can be seen as a variant of a delegation server.
5
Discussion
We now proceed to analyse the diﬀerent properties found in the previous sec-
tion, with regard to the following desired functionality of a scheme for delegated
attribute assertion:
– Oﬄine veriﬁcation (i.e. the possibility to verify an assertion without having
to contact a trusted third party in the veriﬁcation process).
– Easy revocation of delegations and assertions.
– Possibility of non-cascading revocation of AA delegation rights (i.e. the as-
sertions of an AA can stay valid after this AA’s authority has been revoked)8.
– Traceability of both delegations and assertions issued by AAs.
– Vulnerability and resilience against attacks.
– Possibility to create attribute hierarchies, where one attribute is considered
to include another.
On-demand delegation statements prevent oﬄine veriﬁcation, since the ser-
vice issuing the statements is a trusted third party that needs to be contacted in
the process of verifying an AA’s assertions. When using pre-deﬁned delegation
statements, attribute SOAs can publish the information necessary to verify an
assertion issued by an AA (i.e. a delegation assertion), and veriﬁers having this
information can do the veriﬁcation oﬄine.
The representation of AA for the veriﬁers does not aﬀect the functionality of
oﬄine veriﬁcation.
Revocation of assertions is only easy when using a group representation of
AA and on-demand delegation statements. If AAs are individually represented,
it is diﬃcult to keep track of which assertions each individual AA has issued and
therefore diﬃcult to locate and revoke one.
If we have pre-deﬁned delegation statements, it is diﬃcult to keep track of
AAs, especially if we consider the possibility of multiple delegation steps. It
is therefore diﬃcult to locate and revoke both delegations and assertions by
delegated AAs.
If AAs are individually represented, it is diﬃcult to have non cascading revo-
cation of a delegation. In order to make this possible, one would need to keep a
history of permissions over time, to check if an AA was authorised to issue an
assertion at a certain point of time. This in turn requires a mechanism to pre-
vent AAs from backdating their assertions (e.g. a trusted time stamping service).
8 This is a desirable feature, if the actions of the AA before its authority was revoked
are still considered to be correct and one does not want to redo all of them.

A Classiﬁcation of Delegation Schemes for Attribute Authority
167
Table 2. The eﬀect of the properties AA representation and Delegation statements on
desirable functionality (read table as row-to-column relation)
AA representation
delegation statements
Oﬄine
Not
prevented by
veriﬁcation
aﬀected
on-demand statements
Assertion
Easy only for group representation
revocation
and on-demand statements
Noncascading
Diﬃcult for individual
Not
revocation possible
representation
aﬀected
Traceability
Easy only for group representation
of AA actions
and on-demand statements
Vulnerability
Greater risk with
TTP required for
and resilience
group representation
on-demand statements
Easy dissemination of hierarchies for group representation
Attribute
and on-demand statements
hierarchies possible Imposing restrictions diﬃcult with group representation
and pre-deﬁned statements
With a group representation of AAs, assertions are not linked to a speciﬁc AA
and therefore remain valid even if the authority of the issuing AA is revoked9.
This functionality is not aﬀected by the way how delegation statements are
issued.
Traceability is similar to revocation, since it requires locating attribute and
delegation assertions. Therefore the same considerations apply.
Issuing delegation statements on-demand requires a trusted third party (TTP)
to be available online at every moment an attribute assertion is to be used. This
is more vulnerable than an approach without TTPs since the TTP can become
a bottleneck and also a single point of failure. Furthermore an attacker who
subverts this TTP has complete control of the attributes that it manages. Pre-
deﬁned delegation statements make the system more robust, since there is no
central service that attackers can target.
Having a group representation for AAs, creates a greater risk, since a service
that produces assertions signed with the asymmetric key pair that represents
this group, needs to be available for all AAs. Thus an attacker can take control
of the entire system if he gains access to the private key of the group. Using an
individual representation is more resilient, since only the subverted AAs keys
need to be replaced.
None of the examined properties explicitly prevent attribute hierarchies. How-
ever a system designed like the Assertion server example makes it easier to col-
lect such hierarchies and make them available to users and veriﬁers. In systems
following the design of the Shared key assertion example, it is diﬃcult to restrict
9 This does not prevent the revocation of assertions issued by rogue AAs, it only allows
to keep assertions valid even though the AA that issued them is not allowed to do
so anymore.

168
L. Seitz, E. Rissanen, and B. Sadighi
a delegation in a way that would prevent an AA from creating a hierarchy. The
other models would allow for such a design.
Table 2 summarises these observations.
6
Conclusion
In this article we have investigated the problem of verifying assertions issued by
a delegated authority. We have extracted two criteria by which such veriﬁcation
schemes can be classiﬁed. From these criteria 4 diﬀerent models for veriﬁcation
schemes result. We present an analysis of the drawbacks and advantages of these
models. We do not give speciﬁc recommendations on which model is to be pre-
ferred over another one, since this strongly depends on the requirements of the
actual application in which delegated attribute assertion is used. System design-
ers can use our model to either evaluate diﬀerent available products for use in
their system or to design custom systems corresponding to their requirements.
References
1. Godik, S., Moses, T., (eds.).: eXtensible Access Control Markup Language
(XACML). Standard, Organization for the Advancement of Structured Information
Standards (OASIS) (2003) http://www.oasis-open.org/committees/xacml
2. Nagaratnam, N., Janson, P., Dayka, J., Nadalin, A., Siebenlist, F., Welch, V.,
Tuecke, S., Foster, I.: Security Architecture for Open Grid Services. Technical re-
port, GGF OSGA Security Workgroup, Revised 6/5/2003 (2002), available from
https://forge.gridforum.org/projects/ogsa-sec-wg
3. Bandmann, O., Dam, M., Firozabadi, B.S.: Constrained Delegation. In: Proceed-
ings of 2002 IEEE Symposium on Security and Privacy, Oakland, CA, USA (2002)
4. Firozabadi, B.S., Sergot, M., Bandmann, O.: Using Authority Certiﬁcates to Create
Management Structures. In: proceedings of Security Protocols, 9th International
Workshop, Cambridge, UK, pp. 134–145 (2001)
5. Maler, E., Mishra, P., Philpott, R., (eds.).: The OASIS Security Assertion Markup
Language (SAML) v1.1. Standard, Organization for the Advancement of Struc-
tured Information Standards (OASIS) (2003), http://www.oasis-open.org
6. Navarro, G., Firozabadi, B.S., Rissanen, E., Borrell, J.: Constrained delegation in
XML-based Access Control and Digital Rights Management Standards. In: Pro-
ceedings of the IASTED International Conference on Communication, Network,
and Information Security, New York, USA (2003)
7. Wang, J., Vecchio, D.D., Humphrey, M.: Extending the Security Assertion Markup
Language to Support Delegation for Web Services and Grid Services. In: Proceed-
ings of the International Conference on Web Services, Orlando, Florida, USA (2005)
8. Farrell, S., Housley, R.: An Internet Attribute Certiﬁcate Proﬁle for Authoriza-
tion. Request For Comments (RFC) 3281, Internet Egnineering Task Force (IETF)
(2002), http://www.ietf.org/rfc/rfc3281.txt
9. PKIX Working Group: Public Key Infrastructure (X.509). Technical report, Inter-
net Engineering Task Force (IETF) (2002),
http://www.ietf.org/html.charters/pkix-charter.html

A Classiﬁcation of Delegation Schemes for Attribute Authority
169
10. Lorch, M., Kafura, D.: Supporting Secure Ad-hoc User Collaboration in Grid Envi-
ronments. In: Proceedings of the 3rd International Workshop on Grid Computing,
Baltimore, MD, USA, pp. 181–193. Springer, Heidelberg (2002)
11. Lorch, M., Adams, D., Kafura, D., Koneni, M., Rathi, A., Shah, S.: The PRIMA
System for Privilege Management, Authorization and Enforcement. In: Proceedings
of the 4th International Workshop on Grid Computing, Phoenix, AR, USA, pp.
109–116. IEEE Computer Society, Los Alamitos (2003)
12. Chadwick, D., Otenko, A.: The PERMIS X.509 Role Based Privilege Management
Infrastructure. In: Proceedings of the 7th ACM Symposium on Access Control
Models and Technologies, Monterey, CA, USA, pp. 135–140. ACM Press, New
York (2002)
13. Chadwick, D.: Delegation Issuing Service. In: NIST 4th Annual PKI Work-
shop,
Gaithersberg,
USA,
62–73
Available
from:
g.pdf
(2005),
http://
middleware.internet2.edu/pki05/proceedings/chadwick-delegation-issuin
14. Rissanen, E., Lockhart, H., Moses, T., (eds.).: XACML v3.0 administrative policy.
Standard, Organization for the Advancement of Structured Information Standards
(OASIS) (2006), http://www.oasis-open.org/committees/xacml

Program Partitioning Using
Dynamic Trust Models⋆
Dan Søndergaard1, Christian W. Probst1, Christian Damsgaard Jensen1,
and Ren´e Rydhof Hansen2
1 Informatics and Mathematical Modelling, Technical University of Denmark
s011283@student.dtu.dk, {probst,cdj}@imm.dtu.dk
2 Department of Computer Science, University of Copenhagen
rrhansen@diku.dk
Abstract. Developing distributed applications is a diﬃcult task. It is
further complicated if system-wide security policies shall be speciﬁed and
enforced, or if both data and hosts are owned by principals that do not
fully trust each other, as is typically the case in service-oriented or grid-
based scenarios. Language-based technologies have been suggested to
support developers of those applications—the Decentralized Label Model
and Secure Program Partitioning allow to annotate programs with secu-
rity speciﬁcations, and to partition the annotated program across a set
of hosts, obeying both the annotations and the trust relation between
the principals. The resulting applications guarantee by construction that
safety and conﬁdentiality of both data and computations are ensured.
In this work, we develop a generalised version of the splitting frame-
work, that is parametrised in the trust component, and show the result
of specialising it with diﬀerent trust models. We also develop a metric to
measure the quality of the result of the partitioning process.
1
Introduction
There is an increasing reliance on open computing environments; virtual organ-
isations, supply chain management, grid computing, and pervasive computing
increase the need for spontaneous and dynamic formation of short term coali-
tions to solve a speciﬁc problem. The security implications of this evolution are
profound and new techniques must be developed to prevent violation of con-
ﬁdentiality and integrity in applications that execute on a widely distributed
computing platform, e.g., a computational grid infrastructure.
Developing applications for these distributed environments, however, is a dif-
ﬁcult task. In scenarios where both data and hosts are owned by principals that
do not necessarily trust each other, as is typically the case in service-oriented or
grid-based scenarios, development is further complicated. Language-based tech-
nologies have been suggested to support developers of those applications—the
⋆This work has in part been supported by the EU research project #016004, Software
Engineering for Service-Oriented Overlay Computers.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 170–184, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Program Partitioning Using Dynamic Trust Models
171
Decentralized Label Model and Secure Program Partitioning allow to annotate
programs with ownership and access control information, and to partition the
annotated program across a set of hosts, obeying both the annotations and the
trust relation between the principals. Starting from annotated sequential code,
the resulting distributed applications guarantee by construction that safety and
conﬁdentiality of both data and computations are ensured. While this is a very
powerful mechanism, the trust model used in the original framework [19,20] is
very limited. We start in this work with developing a generalised version of the
partitioning framework, that is parametrised in the trust component, and show
the result of specialising it with diﬀerent, increasingly dynamic and realistic trust
models.
Another important aspect is the trust that principals in the system have in
the partitioning generated by the splitter. Even though the sub-programs are
guaranteed to obey the annotations as well as the conﬁdentiality and integrity
speciﬁcations by construction, principals will want a way to measure how well
a certain partitioning fulﬁls all their constraints. Therefore we develop a metric
to measure the quality of the result of the partitioning process.
The approach we base our work on is fundamentally diﬀerent from simply
securing regular algorithms by encrypting data. While there are algorithms that
do operate on encrypted data [1,2,16], such algorithms are usually very spe-
cialised and in the general case it is not possible to transform an algorithm into
an equivalent algorithm that operates on encrypted data. This means that users
of grid applications must generally accept that both algorithm and data will be
known to the computer that performs the calculations. It is therefore important
to consider the segmentation of programs and data, in order to prevent sending
sensitive data to the wrong, that is untrusted nodes (e.g., processing personal
information, such as an electronic patient record, from a European citizen on a
computer in the U.S.A. constitutes a possible violation of the European Data
Protection Directive [8]).
This paper is structured as follows. The rest of this section introduces a run-
ning example, that we will use to illustrate diﬀerent approaches. It is followed by
a description of the splitting framework in Sec. 2. In Sec. 3 we develop a metric
to judge the quality of a partitioning computed by the splitting framework, and
in Sec. 4 we describe dynamic trust scenarios and their eﬀect on the splitting
process. Finally, Sec. 5 discusses related work and Sec. 6 concludes the paper
and discusses future work.
1.1
Example
This section introduces our running example, that we will use to illustrate both
the general framework and diﬀerent trust models.
Consider the example of a pharmaceutical company that is in the process
of developing a new drug. An important element of the drug discovery phase
consists of high performance computing on large volumes of data, e.g., simulating
chemical reactions, comparing gene sequences, etc. It is common for smaller
pharmaceutical companies to specialise in particular types of drugs, resulting in

172
D. Søndergaard et al.
many companies that belong to the same (pharmaceutical) trade association,
but do not directly compete with each other. This is the typical scenario for
a setting where one wants to develop applications that deal with data from
companies that potentially distrust each other in handling some of their data,
but are willing to share other parts. The fact that some companies are registered
in the same country and belong to the same trade association may increase the
initial conﬁdence in their sites and serve as a recommendation in case no previous
experience with that site has been recorded.
The case that we will use as example is that of two companies A and B who want
to execute a gene sequencing. In order to save costs, they use the same provider C
of such a service. In our setting, company A has a license agreement with C, such
that they receive a gene once C has found a match. In contrast, B only receives
the boolean result and would have to pay for actually obtaining the gene.
2
The Splitting Framework
This section describes the components of the original framework for Secure Pro-
gram Partitioning as described by Myers et al. [19,20]. The main components
are a security-typed programming language and a trust-based splitter. The par-
titioning process then essentially consists of two steps—compiling and splitting.
The compiler processes the source code with security annotations as provided
by a user, and the splitter tries to ﬁnd a partitioning based on the trust relation
of the principals. Fig. 1 shows the schematic overview of the framework. The
input is an application written in a security-typed language. An example for
a security-typed language is JIF [13] that extends Java with the Decentralised
Label Model [14,15]. Before discussing the framework in more detail we introduce
the notion of trust used in it.
        

    

        

    

            



            



host Alice
host Bob
host Charlie
verifier
compiler
splitter
(1)
Alice
Charlie
Bob
(3)
(2)
Fig. 1. The general splitting framework. Each principal in the system speciﬁes a trust
and an integrity graph (1), that is combined into the global trust graph and is used by
the central splitter to partition the application across hosts in the net (2). Then the
generated partitioning is distributed across the network and executed (3).

Program Partitioning Using Dynamic Trust Models
173
Bob
Bob
Alice
Charlie
Charlie
Diana
Alice
Fig. 2. Static trust graph for the example application. Solid edges denote conﬁden-
tiality, while dashed edges denote integrity. The left graph depicts the initial static
situation, the right hand graph the situation after Diana has joined the network.
2.1
Conﬁdentiality and Integrity
Trust in the original framework is divided in the two components conﬁdentiality
and integrity. It is noteworthy that these two components are independent from
each other.
Conﬁdentiality is used to express that an actor A in the system has trust
into another actor B as to keeping A’s data conﬁdential. This means that A
is willing to store its data on a host operated by B. For the rest of this paper
we tacitly identify a host with the user operating it. Integrity is used to express
that an actor A assumes that results obtained from actor B have been computed
correct. This means that A is willing to split computations on a host operated
by B. Both conﬁdentiality and integrity are complete and non-transitive, i.e., if
A trusts B and B trusts C, then A does not automatically trust C.
A third measure that could easily be added is that of availability of a host
in a distributed system. Principals could use this to express their trust in a
certain host to be available to execute their computation. In contrast to conﬁ-
dentiality and integrity, which are subjective and hard to validate, availability
is measurable in that one can check whether or not a host performed a certain
operation. Therefore the availability assigned to a host could be increased or de-
creased over time. We are planning to further investigate this with our prototype
implementation.
The two trust components conﬁdentiality and integrity can be expressed as sets:
Cp = {a1 :, . . . , an :}
and
Ip = {? : b1, . . . , bn}
(1)
where p, ai, and bi are principals in the system. Using this notation, Cp speciﬁes,
who has conﬁdence in p, and Ip speciﬁes who believes in p’s integrity. For instance
the trust graph on the left hand side of Fig. 2 can be expressed as
CAlice ={Alice :}
CBob ={Bob :}
CCharlie ={Alice : ; Bob : ; Charlie :}
IAlice = {? : Alice}
IBob = {? : Bob}
ICharlie = {? : Alice, Bob, Charlie}
For the graph on the right hand side of Fig. 2, the results for Alice, Bob, and
Charlie remain the same, and for Diana one gets

174
D. Søndergaard et al.
CDiana = {Alice : ; Charlie :}
IDiana = {? : Diana, Charlie}
Because the trust model is non-transitive, Bob does not trust Diana with regards
to conﬁdentiality.
2.2
Security-Typed Languages
This section introduces the necessary background on the Decentralized Label
Model (DLM) and shows the annotated high-level source code for our example
application (Fig. 3).
The central notion in the DLM is the principal (p ∈Principals), which rep-
resents any entity that can own data or operate on it (that is users, processes,
hosts, etc.). Principals are used in two ways—on the one hand in expressing
ownership and access rights of data, on the other hand to deﬁne the authority
that the program at any given point in time has. In the example, principals are
A and B, representing the pharmaceutical companies, and C, representing the
service provider mentioned in Sec. 1.1.
Both ownership and access rights to data are expressed by security labels
L ∈Labels = P(Principals × P(Principals)), which can be written as sets. Each
label speciﬁes a set of owners for a piece of data and for each owner the principals
that are allowed to access the data. In Fig. 3, the ﬁrst part of the label of variable
seq1 ({A : C}) indicates that the variable is owned by principal A and that A
allows C to read it. Similarly, the label part {A : C; C : A} at variable t1
indicates that this variable is owned by A and C, and each allows the other to
read it. All components of a label L must be obeyed in accessing data labelled
with L. Compound labels with several owners also arise during computations
sequence seq1 {A:C; ?:A};
sequence seq2 {B:C; ?:B};
gene g {C:A; ?:A,B};
bool t1 {A:C; C:A; ?:A};
// temporary variables
bool t2 {B:C; C:B; ?:B};
bool r1 {A:; ?:A};
// return values
bool r2 {B:; ?:B};
t1 := scan(g, seq1);
t2 := scan(g, seq2);
r1 := declassify(t1,{A:; ?:A});
r2 := declassify(t2,{B:; ?:B});
Fig. 3. High level code for the example from Sec. 1.1. Companies A and B use a gene-
sequencing service provided by C. While A has a license agreement with S that allows
free access to genes (hence the label allowing A to read g), B only is allowed to read
the result of the sequencing (and would have to pay for receiving the gene). For an
explanation of the annotations c.f. Sec. 2.2.

Program Partitioning Using Dynamic Trust Models
175
using data tagged with only one owner, e.g., when invoking the method scan
in Fig. 3 with arguments g labelled {C : A} and seq2 labelled {B : C}. The
resulting label {C : A, B : C} allows only C to read data since the owner of data
is automatically allowed to read it.
In addition to conﬁdentiality information, labels can also contain integrity in-
formation. A label {? : p1, · · · , pn} speciﬁes that principals p1 through pn believe
that the data is correctly computed. As shown in Fig. 3, we combine conﬁden-
tiality and integrity components in labels, and use the extraction functions C(L)
and I(L), respectively.
Following [19,20] we write L1 ⊑L2 whenever L1 is less restrictive than L2. In the
case of conﬁdentiality this means that data labelled with L1 allows more principals
to read it than data labelled with L2, and in the case of integrity this means that
code labelled with L1 is trusted by less principals than code labelled with L2.
Before any attempt to split the program is made, the compiler veriﬁes that
the security labels are statically consistent. This initial veriﬁcation ensures that
all the security annotations are obeyed, for example that variables are not read
by principals who lack the right to do so. For a detailed description see [19,20].
2.3
Splitting
Once the program has been veriﬁed, the next step is to compute a partitioning of
the program onto available hosts in the network. Beside the security annotations
in the code, the partitioning is based on the conﬁdentiality and the integrity
relation, which each principal speciﬁes. As stated above, together these relations
specify the amount of trust a principal has in the other principals in the system.
The splitter takes the annotated program and these relations, and produces a
split if possible. [19] discuss situations and counter-measures in situations that
do not allow a split. The resulting sub-programs are then distributed on the
individual hosts, and together implement the original program. In the generated
sub-programs each ﬁeld and statement has been assigned to a principal. In the
rest of this section we brieﬂy describe how this split is determined.
Assigning ﬁelds. In determining, whether a ﬁeld of the program can be placed
at a host h operated by principal p, the constraints C(Lf) ⊑Cp and Ip ⊑I(Lf)
must be satisﬁed. The equations express that the principal p has at least as
much conﬁdentiality as the ﬁeld f and at most as much integrity as the ﬁeld. In
the example (Fig. 3), the ﬁeld seq1 has C(seq1) = {A, C} and I(seq1) = {A},
resulting in host A being the only possible host this ﬁeld can be scheduled to.
Assigning statements. A statement S can be assigned to a principal p if the
principal has at least the conﬁdentiality of all values used in the statement.
Additionally the principal must have the integrity of all values deﬁned. To ensure
this, the two constraint Lin = 
v∈U(S) Lv and Lout = 
l∈D(S) Ll are enforced.
Here, U(S) denotes all values used in S, and D(S) denotes all deﬁnitions in
S. For a principal p to be able to execute S, the constraints C(Lin) ⊑Cp and
Ip ⊑I(Lout) must be satisﬁed. In the example (Fig. 3), the arguments to the
second call to function scan have readers {A, C} and {B, C}, resulting in C

176
D. Søndergaard et al.
being the only possible host. A similar result is obtained for integrity, therefore
this statement is scheduled on host C.
Declassiﬁcation. One problem with security-typed languages is that labels tend
to become more and more restrictive in terms of allowed readers and required
integrity. To circumvent this, explicit declassiﬁcation has been introduced, which
allows the programmer to specify a new label for a data item. Since a declassi-
ﬁcation statement is executed with a certain authority, all principals P whose
authority is needed must trust the integrity of the execution reaching the de-
classiﬁcation statement. This information is recorded in the label of the program
counter, so we require I(pc) ⊑{? : P}. In the example program declassiﬁcation is
needed since the result of the gene scanning has to be sent back to the individual
hosts, allowing them to read the data.
The result of the partitioning with respect to the two trust graphs from Fig. 2
is shown in Fig. 4.
2.4
Infrastructure
Before investigating splitting using more dynamic types of trust graphs, we brieﬂy
summarise the infrastructure used in our approach. As shown in Fig. 1, the sys-
tem consists of a set of hosts and a central splitter. The splitter takes requests from
clients, that send a program written in a security-typed language, and computes a
partitioning of the program on the hosts in the system. The partitioning is guaran-
teed to obey all ownership and access control annotations present in the program.
To compute the partitioning, the splitter uses a trust graph as speciﬁed by each
client upon entering the network. This trust graph combines the two components
conﬁdentiality and integrity as described in Sec. 2.1. As a result, clients can rely
on the fact that their data and code is only stored or executed on hosts that are
operated by users who they trust to ensure conﬁdentiality and/or integrity.
3
Quality of a Partitioning
The partitioning of the original program as computed by the splitter is guar-
anteed to obey all ownership- and access control-annotations, as well as the
conﬁdentiality and integrity speciﬁed by the principals. However, the original
framework does not specify how to choose between several possible solutions for
the constraint system generated from the program (Sec. 2). This section intro-
duces a metric to judge the quality of a partitioning from the view point of a
principal A. The idea is to judge the risk of a set of principals violating the
conﬁdentiality of data that is owned by A and has been partitioned to a host
operated by B.
The mathematical foundation for this approach is Maurer’s work on prob-
abilistic trust [12], where trust values can be speciﬁed by numbers between
0 and 1, representing no and total trust, respectively. If we reconsider the static
trust graph introduced in the previous section, we can annotate the conﬁdence
and integrity edges with probability 1, since the trust model is complete.

Program Partitioning Using Dynamic Trust Models
177
A: sequence seq1 {A:C; ?:A};
B: sequence seq2 {B:C; ?:B};
C: gene g {C:A; ?:A,B};
// temporary variables
C: bool t1 {A:C; C:A; ?:A};
C: bool t2 {B:C; C:B; ?:B};
// return values
A: bool r1 {A:; ?:A};
B: bool r2 {B:; ?:B};
C: t1 := scan(g, seq1);
C: t2 := scan(g, seq2);
C: r1 := declassify(t1,{A:; ?:A});
C: r2 := declassify(t2,{B:; ?:B});
A: sequence seq1 {A:C; ?:A};
B: sequence seq2 {B:C; ?:B};
C: gene g {C:A; ?:A,B};
// temporary variables
D: bool t1 {A:C; C:A; ?:A};
C: bool t2 {B:C; C:B; ?:B};
// return values
A: bool r1 {A:; ?:A};
B: bool r2 {B:; ?:B};
D: t1 := scan(g, seq1);
C: t2 := scan(g, seq2);
D: r1 := declassify(t1,{A:; ?:A});
C: r2 := declassify(t2,{B:; ?:B});
Fig. 4. The result of splitting the example code from Fig. 3. The code on the left has
been split using the left trust graph from Fig. 2, the code on the right has been split
using the right trust graph in that ﬁgure. As a result of adding the host operated by
Diana, statements that A has rights on have been partitioned to host D.
Now assume that a piece of data or code owned by principal A should be
scheduled to a host h. How would A evaluate whether h is well-suited to host
its data, or not. Again there are two components to this question. First of all, A
will want to inspect which other principals might allow the splitter to partition
their code to h. Any other code running on the host might result in an increased
risk of A’s data being leaked.
To compute the trust that a principal has in this not happening, we follow
Maurer’s formula. For a principal A’s view we use his conﬁdentiality graph CGA.
Like Maurer we are interested in computing the probability that particular sub-
sets, from which the trust of A in another principal B’s conﬁdentiality can be
derived, are contained in this view. The probability is
P(ν ⊆CGp) =

S∈ν
P(S)
To compute the overall trust that principal A has in B we use Maurer’s formula
and get
conﬁdence(A, B) =
k

i=1
P(νi ⊆CGA)
(2)
−

1≤i1<i2≤k
P((νi1 ∪νi2) ⊆CGA)
+

1≤i1<i2<i3≤k
P((νi1 ∪νi2 ∪νi3) ⊆CGA)
−· · ·

178
D. Søndergaard et al.
Obviously, the risk that B leaks the data is the inverse of the conﬁdence value,
that is 1 −conﬁdence(A, B).
In computing the probability that any of the principals that trust the host
h will leak A’s data we face the problem that these events are certainly not
independent, so that it is hard to compute the probability for the case that this
happens. However, the metric just needs to allow to compare diﬀerent scenarios,
but does not need to be a probability. So the ﬁrst part of the metric computes
the average of the inverse of all conf values as deﬁned above:
leak(A, h) =

p∈Ch(1 −conﬁdence(A, p))
|Ch|
The higher the value of leak(A, h) is, the less is the average conﬁdence that A
has in other principals that have conﬁdence in host h. The second component is
the conﬁdence that principal A has in host h—this is a measure for how likely
it is that h leaks data owned by A.
The metrics we suggest is deﬁned as the quotient of these two numbers
M(A, h) =
leak(A, h)
conﬁdence(A, h)
The higher it is, the higher A judges the likelihood, that h will leak its data
if stored on h. We voluntarily base the metrics only on the conﬁdence values
speciﬁed by A, since we consider the conﬁdentiality of data to be of uttermost
importance. One could easily extend the metrics to include integrity, or re-
place it altogether—our framework allows for easy experimentation with diﬀerent
metrics.
The metrics is used by the splitter whenever there are several hosts that data
or code owned by a principal could be partitioned to. For example in the right
hand side of Fig. 4 the metrics has been used to pick host D over C since the
metrics values are smaller for the former.
4
Improving Dynamics
In dynamic distributed systems, the static trust model described in Sec. 2 is
insuﬃcient. Complete trust and non-transitivity are too simple concepts for most
realistic trust scenarios. Additionally, the inability to handle dynamic networks
makes the static trust model insuﬃcient for dynamic distributed systems.
In order to make the Secure Program Partitioning applicable to more realistic
applications, we extend the trust model in the rest of this paper to ﬁrst handle
dynamic extensions when hosts join or leave the network (Sec. 4.1), and to allow
speciﬁcation of partial or probability-based trust (Sec. 4.2).
Like in the static setting, the centralised splitter in the dynamic setting main-
tains a global trust graph, which contains trust relations (that is conﬁdence and
integrity values) of all principals.

Program Partitioning Using Dynamic Trust Models
179
        

        



        

            



    

    

  
Diana
verifier
compiler
splitter
host Diana
host Alice
host Bob
host Charlie
Charlie
Bob
Alice
(3)
(2)
(1)
Fig. 5. Principal D joins the network. As a result, the trust graph is updated (1), the
program is re-split (2), and if the splitting results in new sub-programs, the changed
parts are re-distributed (3). In this scenario, part of the sub-program that had been
scheduled to host A has been re-partitioned to run at host D (c.f. Fig. 1).
4.1
A Dynamic Network
In the dynamic scenario the set of available principals and the trust graph are no
longer static [9]. When a principal p joins the network as depicted in Fig. 5, the
principal is added to the set Pactive of active principals. The joining principal
informs the splitter of its trust relations, i.e., who it trusts. The splitter asks
all the other principals about their conﬁdence and integrity values with respect
to the new principal and updates the trust graph with this information. It then
decides if the program should be re-split, which happens if the trust of principals
into the partitioning can be increased, as measured by the metrics.
When a principal p leaves the network, the program needs to be re-split if the
leaving principal was storing data or code of the program. p ∈Psplit. The splitter
will try to re-split the program. If this is not possible, the program cannot be
executed. Execution is halted until the set of active principals, Pactive, again can
produce a legal split.
4.2
Dynamic Probabilistic Model
In the real world complete trust rarely exists. Therefore a binary trust model as
the one proposed in [19] is too simple for many applications.
Instead we use probabilistic trust [12]. In this model, trust edges are annotated
with probability values in the range 0 to 1, where 0 is complete distrust, and 1
complete trust. Trust is divided into conﬁdentiality and integrity. Trust from a
principal A to B is expressed as P(T rustCA,B) = φTC and P(T rustIA,B) = φTI,

180
D. Søndergaard et al.
where C and I denote conﬁdentiality and integrity, respectively. The probability
value essentially states, how probable it is that a principal is trustworthy.
Each principal can then specify what its lowest acceptable conﬁdence level is,
allowing principals to inﬂuence the safety of their data. A principal’s conﬁdence
level might vary with how sensitive the information is.
4.3
Recommended Trust
Another modiﬁcation to the original trust model is adding recommended trust.
A principal can declare trust in another principals ability to recommend other
principals by p(RecCA,B,i) = φRC and p(RecIA,B,i) = φRI. Here i is the max-
imum allowable recommendation distance. For instance i = 1 will allow the
principal to recommend only neighbouring principals in the trust graph. This
is motivated by recommendations in, e.g., public-key infrastructures. We have
left certiﬁcates out of our model to keep the presentation simpler, but of course
frameworks like [12] can easily be put on top of our approach.
The trust graph now is constructed from a set of trust and recommendation
statements, e.g.
T G = {T rustCA,B, T rustI,A,B, RecCA,B,1, T rustCB,C}
The ﬁrst step of calculating the conﬁdence in a trust statement, e.g., that A
trusts in B (T rustCA,B), is to ﬁnd all minimal paths from A to B. A minimal
path is a path from A to B which is not a superset of any other path from
A to B. Using minimal paths we deﬁne conﬁdence similar to the deﬁnition of
conﬁdence in [12] using the same idea as in Sec. 3:
conf(T rustA,B) =
k

i=1
P(νi ⊆T G)
(3)
−

1≤i1<i2≤k
P((νi1 ∪νi2) ⊆T G)
+

1≤i1<i2<i3≤k
P((νi1 ∪νi2 ∪νi3) ⊆T G)
−· · ·
Then the probability that a path of trust statements is valid can be calculated
by multiplying the probabilities of the individual statements:
P(ν ⊆T G) =

S∈ν
P(S)
(4)
Our model diﬀers from Maurer’s [12] in that it deals with trust in conﬁ-
dentiality and integrity, not certiﬁcates. Maurer calculates the conﬁdence in a
certiﬁcate, while we calculate the conﬁdence in a principal’s trustworthiness.
In practise this means that recommendations are fundamentally diﬀerent. In a
public key infrastructure a certiﬁcate relies on the principal who issued it. If the

Program Partitioning Using Dynamic Trust Models
181
0.9/0.9
0.9/0.9
0.9/0.9
0.9/0.9
0.9/0.9
0.9/0.9
0.8
0.9
Diana
Charlie
Eddie
Bob
Alice
Fig. 6. A dynamic trust graph for the example application. Solid edges denote both
conﬁdentiality and integrity (we use the same values for both to simplify computations),
while dashed edges denote trust in recommendation. The annotations on edges denote
the probability measures assigned by principals.
principal issuing it has an invalid certiﬁcate, the issued certiﬁcate is also invalid.
This dependency is used when calculating a recommendation path.
In our model, a principal might recommend a principal who is trustworthy,
even though the recommender is not trustworthy. This means that the recom-
mendation is not dependent on the recommender’s trustworthiness.
4.4
Example with Probabilistic Trust Model
We now return to our example, now using the probabilistic model. A new princi-
pal F is added, and probabilities are added to the trust graph as show in Fig. 6.
All principals require that the conﬁdence in a trust statement must be greater
than or equal to 0.90. Now the conﬁdence in the trust statements has to be cal-
culated. To simplify calculations the probabilities of conﬁdentiality and integrity
have been chosen identical. Paths in the trust graph starting in A, B, or C, are
unique, resulting, e.g., in
conf(T rustA,C) = P(T rustA,C ⊆T G) = p(T rustA,C) = 0.9
More interesting is the calculation of E’s conﬁdence in C and D. There exist
two minimal paths V1 = {RecE,A,1, T rustA,C} and V2 = {RecE,B,1, T rustB,C}.
The probability for each path is calculated using (4):
P(V1 ⊆T G) = p(RecE,A,1) · p(T rustA,C) = 0.9 · 0.9 = 0.81
P(V2 ⊆T G) = p(RecE,B,1) · p(T rustB,C) = 0.8 · 0.9 = 0.72
Because the paths are disjoint, the combined probability becomes:
P((V1 ∪V2) ⊆T G) = P(V1 ⊆T G) · P(V2 ⊆T G)
The conﬁdence can now be calculated using (4):
conf(T rustE,C) = P((V1 ⊆T G) ∨(V1 ⊆T G))
= P(V1 ⊆T G) + P(V1 ⊆T G) −P((V1 ∪V2) ⊆T G)
= 0.81 + 0.72 −0.81 · 0.72 = 0.9468

182
D. Søndergaard et al.
A: sequence seq1 {A:C; ?:A};
B: sequence seq2 {B:C; ?:B};
E: sequence seq3 {E:C; ?:E};
C: gene g {C:A; ?:A,B};
D: bool t1 {A:C; C:A; ?:A};
// temporary variables
C: bool t2 {B:C; C:B; ?:B};
C: bool t3 {E:C; C:E; ?:E};
A: bool r1 {A:; ?:A};
// return values
B: bool r2 {B:; ?:B};
E: bool r3 {C:; ?:C};
D: t1 := scan(g, seq1);
C: t2 := scan(g, seq2);
C: t3 := scan(g, seq3);
D: r1 := declassify(t1,{A:; ?:A});
C: r2 := declassify(t2,{B:; ?:B});
C: r3 := declassify(t3,{E:; ?:E});
Fig. 7. Extended version of the example from Fig. 3, using three client to the splitting
service. The partitioning of the program is performed using the trust graph from Fig. 6.
Since there is only one path from F to E we get
conf(T rustE,D) = 0.9 · 0.9 = 0.81
Using these conﬁdence values, the program can be split as shown in Fig. 7. Note
that the calculations using E’s data has been assigned to C as 0.9468 ≥0.90.
5
Related Work
Trust and explicit trust management has recently emerged as an important in-
novation in computer security [5]. Common trust management systems [3,4,7]
implement a decentralised access control mechanism based on assertions, which
may be contained in signed credentials. Assertions allow a principal to prove that
she is suﬃciently trusted to be granted certain privileges. Recent trust manage-
ment frameworks, e.g., Trustbuilder [18], extend this access control framework
with a mechanism that gradually discloses credentials in order to build the nec-
essary level of trust without compromising the privacy of either party [10,17].
Security mechanisms based on the human notion of trust has also been proposed
for pervasive computing [6,11]. We note that all these systems implement func-
tionality that is orthogonal to our notions of dynamic trust graphs, and therefore
can easily extend our system (and enhance the usability).
Regarding the splitting framework we have already covered the related work
as we rely on Myers work [14,15,19,20] as the underlying foundation for our
prototype implementation.

Program Partitioning Using Dynamic Trust Models
183
6
Conclusion and Future Work
We have presented an extension of a framework for secure program partitioning
with a notion of dynamic trust and shown the results that this dynamics has
on the computed partitioning. Given the increasing reliance on open comput-
ing environments, there also is an increased need for spontaneous and dynamic
formation of short term coalitions to solve a speciﬁc problem. Our extensions
make automatic partitioning of annotated code a viable technique to prevent
violation of conﬁdentiality and integrity in applications that execute on a widely
distributed computing platform, e.g., a computational grid infrastructure. While
the partitioning is already provided by the original infrastructure, only the added
dynamics of network and trust make the technique applicable in this setting.
Another important addition of our work is the development of a metric to
judge how well a partitioning to a speciﬁc host fulﬁls the requirements of a
principal—on top of the guarantee that the sub-programs obey the annotations
as well as the conﬁdentiality and integrity speciﬁcations by construction. This
allows a qualitative assessment of a partitioning on top of automated guarantees.
We plan to use our prototype implementation to experiment with more dy-
namic trust models and investigate more closely the interdependence between
our metric, the computed partitioning, and the trust model. Another research
avenue is to look closer at how to model availability as a measure of trust and
speciﬁcation, and to add some of the tools that allow trust management on top
of our framework.
References
1. Abadi, M., Feigenbaum, J., Kilian, J.: On hiding information from an oracle. Jour-
nal of Computer and System Sciences 39(1), 21–50 (1989)
2. Blakley, G.R., Meadows, C.: A database encryption scheme which allows the com-
putation of statistics using encrypted data. In: SSP ’85. Proceedings of the 1985
Symposium on Security and Privacy, Los Angeles, Ca., USA, April 1990, pp. 116–
122. IEEE Computer Society Press, Los Alamito (1990)
3. Blaze, M., Feigenbaum, J., Ioannidis, J., Keromytis, A.D.: The KeyNote trust man-
agement system. Internet Request for Comment RFC 2704, Internet Engineering
Task Force, Version 2 (September 1999)
4. Blaze, M., Feigenbaum, J., Ioannidis, J., Keromytis, A.D.: The role of trust man-
agement in distributed systems security. In: Vitek, J. (ed.) Secure Internet Pro-
gramming. LNCS, vol. 1603, pp. 185–210. Springer, Heidelberg (1999)
5. Blaze, M., Feigenbaum, J., Lacy, J.: Decentralized trust management. In: IEEE
Computer Society, Technical Committee on Security and Privacy. Proceedings of
the IEEE Symposium on Research in Security and Privacy, Oakland, CA, May
1996, IEEE Computer Society Press, Los Alamitos (1996)
6. Cahill, V., Gray, E., Seigneur, C.J.J.-M., Chen, Y., Shand, B., Dimmock, N., Twigg,
A., Bacon, J., English, C., Wagealla, W., Terzis, S., Nicon, P., di Marzo Serugendo,
G., Bryce, C., Carbone, M., Krukow, K., Nielsen, M.: Using trust for secure collab-
oration in uncertain environments. IEEE Pervasive Computing 2(3), 52–61 (2003)

184
D. Søndergaard et al.
7. Chu, Y.-H., Feigenbaum, J., LaMacchia, B.A., Resnick, P., Strauss, M.: REFEREE:
Trust management for web applications. Computer Networks 29(8-13), 953–964
(1997)
8. European Parliament.: Directive 95/46/EC of the European Parliament and of the
Council of 24 October 1995 on the protection of individuals with regard to the
processing of personal data and on the free movement of such data (October 1995)
9. Hansen, R.R., Probst, C.W.: Secure dynamic program partitioning. In: Proceedings
of Nordic Workshop on Secure IT-Systems (NordSec’05), Tartu, Estonia (October
2005)
10. Jones, V.E., Winsborough, W.H.: Negotiating disclosure of sensitive credentials,
(December 09 1999)
11. Kagal, L., Finin, T.W., Joshi, A.: Communications - trust-based security in per-
vasive computing environments. IEEE Computer 34(12), 154–157 (2001)
12. Maurer, U.: Modelling a public-key infrastructure. In: Martella, G., Kurth, H.,
Montolivo, E., Bertino, E. (eds.) Computer Security - ESORICS 96. LNCS,
vol. 1146, Springer, Heidelberg (1996)
13. Myers, A.C.: JFlow: Practical mostly-static information ﬂow control. In: POPL,
pp. 228–241 (1999)
14. Myers, A.C., Liskov, B.: A decentralized model for information ﬂow control. In:
Symposium on Operating Systems Principles, pp. 129–142 (1997)
15. Myers, A.C., Liskov, B.: Protecting privacy using the decentralized label model.
ACM Transactions on Software Engineering and Methodology 9(4), 410–442 (2000)
16. Song, D.X., Wagner, D., Perrig, A.: Practical techniques for searches on encrypted
data. In: IEEE Symposium on Security and Privacy, pp. 44–55 (2000)
17. Winsborough, W., Seamons, K., Jones, V.: Automated trust negotiation. Techni-
cal Report TR-2000-05, Department of Computer Science, North Carolina State
University, April 24 2000. Monday, 24 April 2000 17:07:47 GMT
18. Winslett, M., Yu, T., Seamons, K.E., Hess, A., Jacobson, J., Jarvis, R., Smith, B.,
Yu, L.: The trustbuilder architecture for trust negotiation. IEEE Internet Com-
puting 6(6), 30–37 (2002)
19. Zdancewic, S., Zheng, L., Nystrom, N., Myers, A.: Untrusted hosts and conﬁdential-
ity: Secure program partitioning. In: Symposium on Operating Systems Principles,
pp. 1–14 (2001)
20. Zdancewic, S., Zheng, L., Nystrom, N., Myers, A.C.: Secure program partitioning.
ACM Transactions on Computer Systems 20(3), 283–328 (2002)

Locality-Based Security Policies⋆
Terkel K. Tolstrup1, Flemming Nielson1, and Ren´e Rydhof Hansen2
1 Informatics and Mathematical Modelling, Technical University of Denmark
{tkt,nielson}@imm.dtu.dk
2 Department of Computer Science, University of Copenhagen
rrhansen@diku.dk
Abstract. Information ﬂow security provides a strong notion of end-
to-end security in computing systems. However sometimes the policies
for information ﬂow security are limited in their expressive power, hence
complicating the matter of specifying policies even for simple systems.
These limitations often become apparent in contexts where conﬁdential
information is released under speciﬁc conditions.
We present a novel policy language for expressing permissible infor-
mation ﬂow under expressive constraints on the execution traces for
programs. Based on the policy language we propose a security condi-
tion shown to be a generalized intransitive non-interference condition.
Furthermore a ﬂow-logic based static analysis is presented and shown
capable of guaranteeing the security of programs analysed.
1
Introduction
The number of computing devices with built-in networking capability has expe-
rienced an explosive growth over the last decade. These devices range from the
highly mobile to the deeply embedded and it has become standard for such de-
vices to be “network aware” or even “network dependent” in the sense that these
devices can use a wide variety of networking technologies to connect to almost
any kind of computer network. Consequently modern software is often expected
to utilise resources and services available over the network for added functionality
and user collaboration. In such an environment where devices routinely contain
highly sensitive or private information and where information ﬂow is complex and
often unpredictable it is very challenging to maintain the conﬁdentiality of sensi-
tive information. Predictably it is even more challenging to obtain formal guaran-
tees or to formally verify that a given device or system does not leak conﬁdential
information. The problem is further exacerbated by the often complicated and
ever changing security requirements of users. Examples include a user’s medical
records that should be inaccessible unless the user is at a hospital, or personal
ﬁnancial information that may be accessed by a bank or a ﬁnancial advisor but
not by the tax authorities except during a tax audit. The above examples expose
one of the major drawbacks of traditional approaches to secure information ﬂow,
⋆This work has in part been supported by the EU research project #016004, Software
Engineering for Service-Oriented Overlay Computers.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 185–201, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

186
T.K. Tolstrup, F. Nielson, and R.R. Hansen
Localities
Nets
ℓ::= l
locality
N ::= l :: P
single node
|
u
locality variable
|
l :: ⟨et⟩
located tuple
|
N1 ∥N2
net composition
Processes
Actions
P ::= nil
null process
a ::= out(t)@ℓ
output
|
a.P
action preﬁxing
|
in(T)@ℓ
input
|
P1 | P2
parallel composition
|
read(T)@ℓread
|
A
process invocation
|
eval(P)@ℓmigration
|
newloc(u) locality creation
Tuples
Fields
T ::= F | F, T
templates
et ::= ef | ef , et
evaluated tuple
F ::= f | !x | !u template ﬁelds
ef ::= V | l
evaluated tuple ﬁeld
t ::= f | f, t
tuples
e ::= V | x | . . .
expressions
f ::= e | l | u
tuple ﬁelds
Fig. 1. Syntax
namely the lack of support for dynamic and ﬂexible security policies. Even for-
mulating, let alone formalising, an information ﬂow policy for such diverse uses
and changing requirements seems to be an insurmountable problem for the tra-
ditional approaches where lattice-based policies are formalised and enforced by
non-interference. This has recently led researchers to look for better and more
appropriate ways of specifying information ﬂow policies and their concomitant
notions of secure information ﬂow, incorporating concepts such as downgrading
(or declassiﬁcation), delimited release, and non-disclosure.
In this paper we develop a novel notion of locality-based security policies in
conjunction with a strong security condition for such policies: History-based Re-
lease. The locality-based security policies are powerful and ﬂexible enough to be
used in systems with a high degree of network connectivity and network based
computing such as those described above. In this paper we model such systems
in the μKlaim calculus, which is based on the tuple space paradigm, making it
uniquely suited for our purposes. In addition we deﬁne what we believe to be
the ﬁrst tuple-centric notion of non-interference and show how History-based
Release is a strict generalisation. Finally we construct a static analysis for pro-
cesses modelled in μKlaim and demonstrate how it can be used to formally verify
automatically that a given system is secure with respect to a given locality-based
security policy. Such an analysis is an invaluable tool, both when designing and
when implementing a complex system, and can be used to obtain security guar-
antees that are essential for critical systems.
2
The μKlaim Calculus
The Klaim family of process calculi were designed around the notion of a tuple
space. In this paradigm systems are composed of a set of nodes distributed at

Locality-Based Security Policies
187
N1 ∥N2
≡
N2 ∥N1
(N1 ∥N2) ∥N3
≡
N1 ∥(N2 ∥N3)
l :: P
≡
l :: (P | nil)
l :: A
≡
l :: P
if A
△= P
l :: (P1 | P2)
≡
l :: P1 ∥l :: P2
Fig. 2. Structural congruence
match(V, V ) = ϵ match(!x, V ) = [V/x]
match(l, l) = ϵ
match(!u, l′) = [l′/u]
match(F, ef) = σ1
match(T, et) = σ2
match((F, T ), (ef, et)) = σ1 ◦σ2
Fig. 3. Tuple matching
various localities. The nodes communicate by sending and receiving tuples to
and from various tuple spaces based at diﬀerent localities. Mobility in the Klaim
calculi is modelled by remote evaluation of processes. In the standard tuple space
model a tuple space is a resource shared among peers and therefore no attempt
is made to restrict or control access to these.
The μKlaim calculus [8] comprises three parts: nets, processes, and actions.
Nets give the overall structure in which tuple spaces and processes are located.
Processes execute by performing actions. The syntax is shown in Fig. 1. Processes
execute by performing an action, a, or by “invocation” of a process place-holder
variable. The latter is used for iteration and recursion. Processes can be com-
posed in parallel and ﬁnally a process can be the nil-process representing the
inactive process. The following actions can be performed by a process. The out-
action outputs a tuple into a tuple space at a speciﬁc locality; the in and read
actions input a tuple from a speciﬁc tuple space and either remove it or leave it
in place respectively; the eval-action remotely evaluates a process at a speciﬁed
locality; and newloc creates a new locality.
The semantics for μKlaim, shown in Fig. 4, is a straightforward operational
semantics and we shall not go into further detail here but refer instead to [8]. As
is common for process calculi the semantics incorporates a structural congruence,
see Fig. 2. In Fig. 3 the semantics for tuple matching, as used in the rules for the
in- and read-actions, is shown. We assume the existence of a semantic function
for evaluating tuples denoted as [[·]].
Semantically we deﬁne an execution trace as the sequence of locations where
processes are executed when evaluating the processes. Hence we write L ⊢N
l↣
L′ ⊢N ′ when evaluating one step of a process located at the location l. Clearly
the execution originates from an action being evaluated at the process space of
location l. For the transitive reﬂexive closure we write L ⊢N
ω↣∗L′ ⊢N ′ where
ω ∈Ω is a string of locations.
3
Policies for Security
To protect conﬁdentiality within a system, it is important to control how in-
formation ﬂows so that secret information is prevented from being released on
unintended channels. The allowed ﬂow of information in a system is speciﬁed in
a security policy. In this section we present a policy language based on graphs.
Here vertices represent security domains, describing the resources available in the

188
T.K. Tolstrup, F. Nielson, and R.R. Hansen
match([[T]], et) = σ
l :: in(T)@l′.P ∥l′ :: ⟨et⟩
l↣l :: Pσ ∥l′ :: nil
[[t]] = et
l :: out(t)@l′.P ∥l′ :: P ′
l↣l :: P ∥l′ :: P ′ ∥l′ :: ⟨et⟩
L ⊢N1
l↣L′ ⊢N ′
1
L ⊢N1 ∥N2
l↣L′ ⊢N ′
1 ∥N2
match([[T]], et) = σ
l :: read(T)@l′.P ∥l′ :: ⟨et⟩
l↣l :: Pσ ∥l′ :: ⟨et⟩
N ≡N1
L ⊢N1
l↣L′ ⊢N2
N2 ≡N ′
L ⊢N
l↣L′ ⊢N ′
l :: eval(Q)@l′.P ∥l′ :: P ′
l↣l ::P ∥l′ ::P ′ ∥l′ ::Q
l′ /∈L
⌊l′⌋= ⌊u⌋
L ⊢l :: newloc(u).P
l↣L ∪

l′
⊢l :: P[l′/u] ∥l′ :: nil
Fig. 4. Operational semantics for μKlaim
net. A security domain is related to sets of resources available in the considered
system.
This model allows for the granularity of the mapping from resources to secu-
rity domains to be very ﬁne-grained. For example we can introduce a security
domain for each location. For improved precision we could partition the usage of
a location into lifetime periods and introduce a domain for each, hence having
more than one security domain for each location in the net. This would allow
us to abstract from e.g. reuse of limited resources in the implementation. For
further discussion of this see [23].
In our setting the tuple spaces are our resources, hence the localities are
related to security domains. This allows us to reason about groups of localities
together, as well as singling out speciﬁc localities and isolate these in their own
security domains. The security policies therefore focus on the information ﬂow
between intended domains of localities. Consequently we assume that locations
can be uniquely mapped to security domains.
Deﬁnition 1. (Security Domains) For a given net we have a mapping from
localities L to security domains V
· : L →V
We write l for the security domain of the location l.
Edges specify permitted ﬂows of information. Information ﬂow between re-
sources can be restricted subject to fulﬁllment of constraints with respect to
certain events taking place prior to the ﬂow. Formally we propose the following
deﬁnition of security polices.

Locality-Based Security Policies
189
Deﬁnition 2. (Locality-based security policies) A security policy is a labelled
graph G = (V, λ), consisting of a set of vertices V representing security domains
and a total function λ mapping pairs of vertices to labels λ : V × V →Δ. We
deﬁne G to be the set of policies. The structure, Δ, of labels is deﬁned below.
In a ﬂow graph the set of vertices V represent security domains. A security
domain indicates the usage of a resource in the system. The edges in the ﬂow
graph describe the allowed information ﬂow between resources. Hence an edge
from the vertex for security domain v1 to the vertex for security domain v2
indicates that information is allowed to ﬂow between the resources in these
domains subject to the label attached to the edge.
The edges in the ﬂow graph are described by the function λ : V × V →Δ.
We write v1
δ; v2 for an edge from vertex v1 to v2 constrained by δ ∈Δ,
i.e. λ(v1, v2) = δ. Information ﬂow might be constrained by certain obligations
that the system must fulﬁll before the ﬂow can take place. Here we describe a
novel constraint language that allows the security policy to be speciﬁc about
intransitive information ﬂows in the ﬂow graph. Constraints are speciﬁed in the
following syntax δ ∈Δ:
δ ::= true | false | v | δ1 · δ2 | δ1 ∧δ2 | δ1 ∨δ2 | δ ∗
A constraint may be trivially true or false. The constraint v enforces that ﬂows
occur at speciﬁc locations, thus that the ﬂow is only permitted at locations that
is part of the security domain v (i.e. l = v). The constraint δ1·δ2 enforces that the
ﬂow is only allowed to occur if events described in δ1 precedes events described
in δ2. Common logical operators ∧and ∨are available to compose constraints.
Finally Kleene’s star ∗allows the policies to express cyclic behaviour.
We might omit the constraint, writing v1 ; v2 for v1
true
; v2. Similarly we
might omit an edge in a ﬂow graph indicating that the constraint can never be
fulﬁlled, i.e. v1
false
; v2.
Example 1. To illustrate the usage of ﬂow graphs as security policies we here
discuss the three examples given in Figure 5. The ﬁrst ﬂow graph (a) allows a
ﬂow from v1 to v2 and v2 to v3 but not from v1 to v3. That is neither directly nor
through v2! In this manner the intransitive and temporal nature of the policies
allow us to have constraints on the order of information ﬂows.
The second ﬂow graph (b) allows the ﬂow from v1 to v2, v2 to v3 and from
v1 to v3 as well. The ﬂow can be directly from v1 to v3 or through v2. If we
wish to restrict the ﬂow to go through v2 it could be done as in ﬂow graph (c).
We assume that l and l′ map to security domains that no other locations are
mapped to. Hence the last ﬂow graph (c) restricts the ﬂows between the security
domains to certain locations. This ensures that for information to ﬂow from v1
to v3 both locations need to participate.
To give intuition to the above example policies we relate them back to the personal
ﬁnance scenario mentioned in the introduction. In the following we let v1, v2, and

190
T.K. Tolstrup, F. Nielson, and R.R. Hansen
v1

v3
v2

v1


v3
v2

v1
l

l·l′

v3
v2
l′

(a)
(b)
(c)
Fig. 5. Examples of ﬂow graphs
v3 denote the user, the user’s ﬁnancial advisor, and the tax authorities respec-
tively. The ﬁrst policy (Figure 5(a)) states that the user’s ﬁnancial information
may be accessed by the ﬁnancial advisor but not by the tax authorities while still
allowing the ﬁnancial advisor to send (other) information to the tax authorities.
The second policy (Figure 5(b)) then states that the user’s ﬁnancial information
may be accessed both by the ﬁnancial advisor and by the tax authorities; this may
be necessary during a tax audit. Finally the policy shown in (Figure 5(c)) deﬁnes
a situation where the user’s ﬁnancial information may be accessed by both the
ﬁnancial advisor and the tax authorities but only through the ﬁnancial advisor;
this security policy ensures that the ﬁnancial advisor can review all relevant in-
formation from the user before the tax authorities gain access to it.
A system is given by a net N and a security policy G together with a mapping
· and might be written N subject to (G, ·). However, as the G and · components
are clear from context we choose not to incorporate the policies in the syntax of
μKlaim.
3.1
Semantics of Constraints
In this subsection we present the semantics of our security policy language. The
semantics is given as a translation to regular expressions over execution traces.
The intuition is that if an execution trace is in the language of the regular
expression derived by the semantics, then the constraint is fulﬁlled.
The main idea behind the locality-based security policies is that they specify
constraints that must be fulﬁlled rather than the total behaviour of the system.
This result in succinct policies. Therefore the semantics of constraints is based
on an understanding of fulﬁlment of a constraint whenever an execution trace
produces the locations speciﬁed. Hence if a trace that fulﬁlls a constraint is
preceded or succeeded by other traces the constraint remains fulﬁlled.
The true constraint is always fulﬁlled and hence is translated to the regu-
lar expression L ∗accepting all execution traces. Similarly the constraint false
cannot be fulﬁlled for any trace, and hence the generated language is ∅. The
constraint v gives the language L ∗· {l | l = v} · L ∗as we wish to allow the ﬂow
only for executions taking place at l. The constraint δ1 · δ2 indicates that the
trace ω can be split into ω1 and ω2, where ω1 must be in the language of δ1, and
respectively ω2 must be in the language of δ2. The constraints for δ1 ∧δ2 and
δ1 ∨δ2 are straightforward. One obvious deﬁnition of [[δ ∗]] is [[δ]] ∗; however this

Locality-Based Security Policies
191
[[true]] = L ∗
[[false]] = ∅
[[v]] = L ∗· {l | l = v} · L ∗
[[δ1 · δ2]] = [[δ1]] · [[δ2]]
[[δ1 ∧δ2]] = [[δ1]] ∩[[δ2]]
[[δ1 ∨δ2]] = [[δ1]] ∪[[δ2]]
[[δ ∗]] = L ∗· [[δ]] ∗· L ∗
Fig. 6. Semantics of constraints
choice would invalidate Lemma 1 below. Consequently, it is natural to deﬁne
[[δ ∗]] = L ∗· [[δ]] ∗· L ∗as then Lemma 1 continues to hold.
The semantics of the security policy language are given in Figure 6.
Lemma 1. The semantical interpretation of constraint δ does not change by
preceding or succeeding it by other traces ∀δ : [[δ]] = L ∗· [[δ]] · L ∗.
We deﬁne an ordering of constraints as the relation ≤Δ⊆Δ×Δ, i.e. we say that
δ is a restriction of δ′ if (δ, δ′) ∈≤Δ, normally we write δ ≤Δ δ′.
Deﬁnition 3. We say that a constraint δ is a restriction of δ′, written δ ≤Δ δ′
if we have [[δ]] ⊆[[δ′]].
Similarly we deﬁne a restriction relation between ﬂow graphs.
Deﬁnition 4. We say that a ﬂow graph G = (V, λ) is a restriction of G′ =
(V ′, λ′), written G ≤G′ if we have that
V = V ′ ∧∀v1, v2 : λ(v1, v2) ≤Δ λ′(v1, v2)
4
Security Condition
To determine whether a program is secure or not, we need some condition for
security. In this section we therefore present our deﬁnition of secure nets. The
intuition is similar to that of non-interference, however we aim to generalize
the traditional non-interference condition to permit release of conﬁdential infor-
mation, based on constraints on the history of the execution and it’s present
location. We call this condition History-based Release.
4.1
Security Condition
Before we formalize the main security condition we need to formalize what an
attacker can observe. Consider an attacker that has access to a subset V of the
tuplespaces that are available in the net under consideration. We formalize the
observable part of a net by nullifying the tuple spaces that the attacker cannot
observe.

192
T.K. Tolstrup, F. Nielson, and R.R. Hansen
Deﬁnition 5. (V-observable) The V-observable part of a net N written N|V is
(l :: P)|V = l :: P
(l :: ⟨et⟩)|V =

l :: ⟨et⟩
if l ∈V
l :: nil
otherwise
(N1 ∥N2)|V = N1|V ∥N2|V
Furthermore we assume that the attacker has knowledge of all the processes that
exist in the net, and hence can reason about the absence of a tuple at a given
location. Similar to the probabilistic attacker in [25] it is feasible to assume that
two tuple spaces can be compared on all tuples. Thus for two nets N1 and N2 an
attacker that observes at security domain V can compare the observable parts
of the nets as N1|V ∼N2|V.
Deﬁnition 6. (Observable equivalence) Two nets N1 and N2 are observably
equivalent N1 ∼N2 iﬀ
{{(l, ⟨et⟩) | N1 = (· · · ∥l :: ⟨et⟩∥· · · )}} = {{(l, ⟨et⟩) | N2 = (· · · ∥l :: ⟨et⟩∥· · · )}}
where we write {{·}} for a multi-set.
We deﬁne the function ∇: P(V ) × G × Ω →P(V ) for extending a set of
security domains with the domains that are permitted to interfere with the
observed domains due to the fulﬁllment of constraints by the execution trace ω.
The resulting set of security domains describe the permutted information ﬂows
during the execution.
Deﬁnition 7. For a security policy G and a execution trace ω an observer at V
can observe the localities
∇(V, G, ω) = V ∪{v1 | v2 ∈V ∧ω ∈[[λ(v1, v2)]]}
A less restrictive policy, ∇will never reduce the observable part of the net. This
allows us to establish the following fact.
Fact 1. If G ≤G′ then ∇(V, G, ω) ⊆∇(V, G′, ω).
We consider a program secure if in no execution trace, neither a single step
nor a series of steps, will an attacker observing the system at the level of a set
of security domains V be able to observe a diﬀerence at any locality, when all
resources permitted to interfere with the locality is observably equivalent before
the evaluation. We formalize the condition as a bisimulation over execution traces
on nets.
Deﬁnition 8. (Bisimulation) A (G, V)-bisimulation is a symmetric relation R
on (the process part of) nets whenever
L1 ⊢N1
ω↣∗L′
1 ⊢N ′
1 ∧
L1 ⊢N1|∅R L2 ⊢N2|∅∧
N1|∇(V,G,ω) ∼N2|∇(V,G,ω)

Locality-Based Security Policies
193
then there exists N ′
2, L′
2 and ω′ such that
L2 ⊢N2
ω′
↣∗L′
2 ⊢N ′
2 ∧
L′
1 ⊢N ′
1|∅R L′
2 ⊢N ′
2|∅∧
N ′
1|V ∼N ′
2|V
We use the fact that the observable part of a net projected on an empty set of
security domains L ⊢N|∅gives the process part of the net. The reason why we
deﬁne the bisimulation in this way is to focus on the executable part and not
the memory part.
The bisimulation follows the approach of Sabelfeld and Sands [22] in utiliz-
ing a resetting of the state between subtraces. This follows from modelling the
attacker’s ability to modify tuple spaces concurrently with the execution. Fur-
thermore it accomodates the dynamically changing nature of the security policies
due to the fulﬁllment of constraints, as seen in [13]. The deﬁnition is transitive
but not reﬂexive. That the deﬁnition is not reﬂexive follows from observing that
the net l :: in(!x)@lH.out(x)@lL is not self-similar whenever information is not
permitted to ﬂow from lH to lL.
Fact 2. If G ≤G′ and R is a (G, V)-bisimulation then R is also a (G′, V)-
bisimulation.
Deﬁnition 9. A G-bisimulation is a relation R such that for all V, R is a
(G, V)-bisimulation. Denote the largest G-bisimulation ≈G.
Now we can deﬁne the security condition as a net being bisimilar to itself.
Deﬁnition 10. (History-based Release) A net N is secure wrt. the security pol-
icy G if and only if we have N ≈G N.
Example 2. In the following we will consider a number of example programs that
illustrate the strength of History-based Release. First consider the program
l :: in(!x)@l1.out(x)@l2
which reads a tuple from l1 and writes it to l2. With the policy l1
l

l2 , the
program is secure, while changing the policy to l1
l′

l2 makes the program
insecure. The reason that the second program is insecure is because the bisimu-
lation forces us to consider all possible traces, so even if the above program was
modiﬁed to execute a process on l′ concurrently with the one on l, the result
would be the same. This corresponds to intransitive non-interference [14] (or
lexically scoped ﬂows according to [2]).
History-based Release goes beyond lexically scoped ﬂows as the policy might
constrain the history of a process. This is illustrated by the following example.
Consider the security policy in Fig. 5(c) and assume that l1 = v1, l2 = v2 and
l3 = v3, for which the program
l :: in(!x)@l1.out(x)@l2
∥
l′ :: in(!y)@l2.out(y)@l3

194
T.K. Tolstrup, F. Nielson, and R.R. Hansen
is secure. On the other hand the program
l :: in(!x)@l1.out(x)@l2
∥
l′ :: in(!y)@l1.out(y)@l3
is insecure because the process at l′ might evaluate prior to the process at l.
Example 3. Another concern is the handling of indirect ﬂows. Consider the
program
l :: in(a)@l1.in(b)@l2
and an attacker observing whether the tuple b is removed from location l2 or
not. Based on this the attacker will know if the process was capable of removing
the tuple a from location l1. Therefore History-based Release allows the program
for the policy l1
l

l2 , but not for l1
false 
l2 . This is due to the fact that
information can be observed by the attacker through the absence of a tuple in a
tuple space.
Example 4. Finally we wish to look at an example program that is insecure in
the traditional setting where lattices are used as security policies. Consider the
program
l :: in(!x)@l2.out(x)@l3.read(!y)@l1.out(y)@l2
which is secure for the policy in Fig. 5(a) when l1 = v1, l2 = v2 and l3 = v3.
This is because evaluating the program does not result in information ﬂowing
from l1 to l3.
4.2
Consistency of History-Based Release
In this subsection we argue the consistency of the deﬁnition of History-based Re-
lease. In particular we will discuss two of the principles presented by Sabelfeld
and Sands in [21]. In the following we consider declassiﬁcation to refer to con-
straints that are not trivially evaluated to true or false.
Conservativity: Security for programs with no declassiﬁcation is equivalent to
non-interference.
Limiting all constraints on edges in the ﬂow graphs to only being of the simple
form true, false or v gives us intransitive non-interference. Removing all non-
trivial constraints (i.e. only having the constraints true and false) results in
traditional non-interference.
Monotonicity of release: Adding further declassiﬁcations to a secure program
cannot render it insecure.
Adding declassiﬁcations to a program coresponds to making our security poli-
cies less restrictive. Hence we aim to show that a program will be secure for any
policy at least as restrictive as the original policy, for which it can be shown secure.
Lemma 2. (Monotonicity) If G ≤G′ then N1 ≈G N2 ⇒N1 ≈G′ N2.
Proof: It follows from Fact 1 that (N1|∇(V,G′,ω) ∼N2|∇(V,G′,ω)) ⇒(N1|∇(V,G,ω)
∼N2|∇(V,G,ω)). The Lemma follows from Fact 2 and observing that to show
N1 ≈G′ N2 we have either N1|∇(V,G′,ω) ∼N2|∇(V,G′,ω), in which case we can
reuse the proof for N1 ≈G N2, or otherwise the result holds trivially.

Locality-Based Security Policies
195
E(nil) = ∅
E(P1 | P2) = E(P1) ∪E(P2)
E(A) = E(P)
if A ≜P
E(outι(t)@ℓ.P) = {ι}
E(inι(T)@ℓ.P) = {ι}
E(readι(T)@ℓ.P) = {ι}
E(evalι(Q)@ℓ.P) = {ι}
E(newlocι(u).P) = {ι}
(a)
ˆσ[[V ]] = {V }
ˆσ[[x]] = ˆσ(x)
ˆσ[[l]] = {l}
ˆσ[[u]] = ˆσ(u)
ˆσ[[f, t]] = ˆσ[[f]] × ˆσ[[t]]
(b)
Fig. 7. (a) Exposed labels in a process. (b) Extension of ˆσ to templates.
5
Security Analysis
In this section we present an approach for verifying systems fulﬁllment of conﬁ-
dentiality wrt. History-based Release speciﬁed in a security policy. The analyses
are given in the Flow Logic framework [18]. Hence the security guarantee is static
and performed prior to the deployment of the system considered.
The analyses are based on the approach of Kemmerer [12]. Thus we analyse a
system in two steps. First in Section 5.1 we identify the local dependencies; this
is done by modifying a control ﬂow analysis by introducing a novel component
for the synchronization of events allowing it to track implicit ﬂows. Second in
Section 5.2 we describe a closure condition of the local dependencies to ﬁnd
the global dependencies. These two steps are independent of the security policy
given for the considered system, and only related to the program analysed. The
ﬁnal step is the comparison of the security policy and the ﬂow graph extracted
from the program and in Section 5.3 we argue that the security enforced by our
approach is History-based Release.
5.1
Local Dependencies
The local dependencies are identiﬁed by a control ﬂow analysis. In fact we mod-
ify the analysis presented in [10] by introducing a novel component for capturing
synchronizations performed in Klaim processes. Hence we will only brieﬂy de-
scribe the other components, before focusing on the extension. The analysis for
the net N is handled by judgements of the form
( ˆT , ˆσ, ˆC) ⊨N : ˆG
The component ˆT : L →P(t) is an abstract mapping that associates a location
or location variable with the set of tuples that might be present at the tuple
space. The component ˆσ : T →P(t) is an abstract mapping holding all possible
bindings of a variable or locality variable (or a pattern of these) that might be
introduced during execution. Furthermore we introduce the abstract mapping
ˆC : Lab →P(L) that associates the label of an action to the set of localities
that the process has previously synchronized with during its execution. The
labels of actions are introduced below. Finally we collect an abstract ﬂow graph
in the component ˆG : L →P(L×L) for describing the ﬂow between tuple spaces

196
T.K. Tolstrup, F. Nielson, and R.R. Hansen
ˆσ ⊨i ϵ : ˆV◦▷ˆV•
iﬀ{ ˆet ∈ˆV◦| | ˆet| = i −1} ⊑ˆV•
ˆσ ⊨i V, T : ˆV◦▷ˆ
W•
iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| prji( ˆet) = V } ⊑ˆV•
ˆσ ⊨i l, T : ˆV◦▷ˆ
W•
iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| prji( ˆet) = l} ⊑ˆV•
ˆσ ⊨i x, T : ˆV◦▷ˆ
W•
iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| prji( ˆet) = ˆσ(x)} ⊑ˆV•
ˆσ ⊨i u, T : ˆV◦▷ˆ
W•
iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧{ ˆet ∈ˆV◦| prji( ˆet) = ˆσ(u)} ⊑ˆV•
ˆσ ⊨i !x, T : ˆV◦▷ˆ
W• iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧ˆV◦⊑ˆV• ∧prji( ˆ
W•) ⊑ˆσ(x)
ˆσ ⊨i !u, T : ˆV◦▷ˆ
W• iﬀˆσ ⊨i+1 T : ˆV• ▷ˆ
W• ∧ˆV◦⊑ˆV• ∧prji( ˆ
W•) ⊑ˆσ(u)
Fig. 8. Abstract tuple matching
and the location at which the process was executed. We write l
l′′
→l′ when a ﬂow
from l to l′ occurs at l′′.
An indirect ﬂow can occur by synchronizing with a tuple space before syn-
chronizing with another, as the attacker might observe the absence of the second
synchronization. For tracking these ﬂows we label the actions in a program. We
deﬁne the function E : P →P(Lab) as the ﬁxpoint of the exposed set of la-
bels. This allows us to track which actions have been executed prior to the one
considered at present. The function is presented in Fig. 7(a).
When analysing an action we must use ˆC to ﬁnd the locations that it syn-
chronizes with. The reason is that these might block further execution, if their
templates can not be matched anything available at the location. Hence if the
attacker can observe the result of an action that follows the one considered he
will learn of the existence (or absence) of the tuples matched. Therefore for all
input or read actions the condition ∀ιP ∈E(P) : ˆC(ι) ∪ˆσ(ℓ) ⊆ˆC(ιP ) must be
fulﬁlled, where ι is the label of the considered action, ˆσ(ℓ) is the set of locations
synchronized with and P is the remaining part of the process.
All local information ﬂows must be found in ˆG. Hence whenever an action
results in a ﬂow of information we check that it is in ˆG. There are two actions
that result in ﬂow of information. Clearly the out action will output information
to the speciﬁed location, and whether or not this happens will give away whether
previous synchronizations were successful or not. Similarly the in action will
remove a tuple from the speciﬁed location, hence we ensure that a ﬂow is recorded
in ˆG. We do so by emposing the condition [ ˆC(ι)
l→ˆσ(ℓ)] ⊆ˆG, where ι is the
label of the action considered and ˆσ(ℓ) is the set of inﬂuenced locations.
The rest of the components of the analysis are the same as in [10]. The analysis
is speciﬁed in Fig. 9. In Fig. 7(b) we extend the component ˆσ for application on
templates and in Fig. 8 the analysis of abstract tuple matching is presented.
5.2
Global Dependencies
The analysis of global dependencies are inspired by the approach of Kemmerer
[12]. This approach utilizes a transitive closure of the local dependencies. In
our setting the local dependencies were identiﬁed by the control ﬂow analysis
presented above. However as we wish to take execution traces into account, we
need to extend the closure, so that the edges are labelled with regular expres-
sions. The goal of the closure will be to guarantee that the language of regular

Locality-Based Security Policies
197
( ˆT, ˆσ, ˆC) ⊨l :: P : ˆG
iﬀ
( ˆT, ˆσ, ˆC) ⊨⌊l⌋P : ˆG
( ˆT, ˆσ, ˆC) ⊨l :: ⟨et⟩: ˆG
iﬀ
⟨et⟩∈ˆT(⌊l⌋)
( ˆT, ˆσ, ˆC) ⊨(N1 ∥N2) : ˆG
iﬀ
( ˆT, ˆσ, ˆC) ⊨N1 : ˆG ∧
( ˆT, ˆσ, ˆC) ⊨N2 : ˆG
( ˆT, ˆσ, ˆC) ⊨l nil : ˆG
iﬀ
true
( ˆT, ˆσ, ˆC) ⊨l P1 | P2 : ˆG
iﬀ
( ˆT, ˆσ, ˆC) ⊨l P1 : ˆG ∧
( ˆT, ˆσ, ˆC) ⊨l P2 : ˆG
( ˆT, ˆσ, ˆC) ⊨l A : ˆG
iﬀ
( ˆT, ˆσ, ˆC) ⊨l P : ˆG ∧
A ≜P
( ˆT, ˆσ, ˆC) ⊨l outι(t)@ℓ.P : ˆG
iﬀ
∀l′ ∈ˆσ(ℓ) : ˆσ[[t]] ⊆ˆT(l′) ∧
[ ˆC(ι)
l→ˆσ(ℓ)] ⊆ˆG ∧
∀ιP ∈E(P) : ˆC(ι) ⊆ˆC(ιP ) ∧
( ˆT, ˆσ, ˆC) ⊨l P : ˆG
( ˆT, ˆσ, ˆC) ⊨l inι(T)@ℓ.P : ˆG
iﬀ
∀l′ ∈ˆσ(ℓ) : ˆσ ⊢1 T : ˆT(l′) ▷ˆ
W• ∧
[ ˆC(ι)
l→ˆσ(ℓ)] ⊆ˆG ∧
∀ιP ∈E(P) : ˆC(ι)∪ˆσ(ℓ) ⊆ˆC(ιP ) ∧
( ˆT, ˆσ, ˆC) ⊨l P : ˆG
( ˆT, ˆσ, ˆC) ⊨l readι(T)@ℓ.P : ˆG
iﬀ
∀l′ ∈ˆσ(ℓ) : ˆσ ⊢1 T : ˆT(l′) ▷ˆ
W• ∧
∀ιP ∈E(P) : ˆC(ι)∪ˆσ(ℓ) ⊆ˆC(ιP ) ∧
( ˆT, ˆσ, ˆC) ⊨l P : ˆG
( ˆT, ˆσ, ˆC) ⊨l evalι(Q)@ℓ.P : ˆG
iﬀ
∀ιQ ∈E(Q) : ˆC(ι) ⊆ˆC(ιQ) ∧
∀l′ ∈ˆσ(ℓ) : ( ˆT, ˆσ, ˆC) ⊨l′ Q : ˆG ∧
∀ιP ∈E(P) : ˆC(ι) ⊆ˆC(ιP ) ∧
( ˆT, ˆσ, ˆC) ⊨l P : ˆG
( ˆT, ˆσ, ˆC) ⊨l newlocι(u).P : ˆG
iﬀ
{⌊u⌋} ⊆ˆσ(⌊u⌋) ∧
∀ιP ∈E(P) : ˆC(ι) ⊆ˆC(ιP ) ∧
( ˆT, ˆσ, ˆC) ⊨l P : ˆG
Fig. 9. Flow analysis
expressions connected to an edge does indeed accept all the executions traces in
which the information ﬂow happens. Therefore a correct closure must guarantee
that the resulting labelled graph has an edge from a node n0 to another node
nk, whenever there exists a path possibly through other nodes in the local ﬂow
graph. Furthermore the labels in the resulting graph must accept a language
that is a superset of all the languages in the local ﬂow graphs.
Deﬁnition 11. A correct closure ˆH of the ﬂow graph ˆG, written ˆG  ˆH, is
deﬁned as ˆG  ˆH iﬀ
∀n0
l1→n1
l2→· · ·
lk→nk ∈ˆG : ∃δ : l1 · · · lk ∈[[δ]] ∧n0
δ

nk ∈ˆH
where n0
l1→n1
l2→· · ·
lk→nk ∈ˆG means ∀i : ni−1
li→ni ∈ˆG.
One algorithm that can be used to compute the least ˆH such that ˆG  ˆH is the
Pigeonhole Principle presented in [11].

198
T.K. Tolstrup, F. Nielson, and R.R. Hansen
5.3
Static Security
We are conﬁdent that the static analyses presented above compute a ﬂow graph
ˆH for which the analyzed net comply with History-based Release.
Conjecture 1. If ( ˆT, ˆσ, ˆC) ⊨N : ˆG and ˆG  ˆH then N ≈ˆ
H N.
In fact the analyses ensure that the analyzed net comply with History-based
Release for any policy that is at least as restrictive as ˆH.
Corollary 1. If ( ˆT, ˆσ, ˆC) ⊨N : ˆG, ˆG  ˆH and ˆH ≤G then N ≈G N.
Proof: Follows from Lemma 2 and Conjecture 1.
6
Related Work
Traditionally policies for information ﬂow security have been of the form of secu-
rity lattices [1,6] where an underlying hierarchical structure on the principals in
the system is assumed and reﬂected in the security lattice. Hence the principals
are tied to security levels and an ordering of security levels indicate what infor-
mation is observable to a principal. Security lattices have found a widespread
usage in language-based approaches to information ﬂow security, see e.g. [24,20].
In this paper we base our security policies on labelled graphs, i.e. without
assigning an underlying ordering. Due to the lack of underlying ordering the
expressiveness of the policies is increased, allowing for simpliﬁed speciﬁcation of
security policies for systems. One example is systems that let a principal act on
resources in diﬀerent security domains without causing a ﬂow of information in
between. The expressiveness gained is due to the transitive nature of the ordering
in a lattice. Graphs have previously been used as information ﬂow policies in [23].
Furthermore these policies relate back to resource matrices applied for e.g. covert
channel identiﬁcation [12,15].
Clearly the translation of a policy speciﬁed as a lattice to a labelled graph is
straightforward. For each security level a security domain is introduced. Edges
(labelled true) are added between security domains according to the ordering of
the corresponding security levels.
The Decentralized Label Model by Myers and Liskov [17,16] is a framework for
security policies that allows owners of information to specify who is permitted
to read that information. The basis model is still a lattice and does not provide
expressiveness similar to what is presented in this paper.
6.1
Semantical Security Conditions
The goal of specifying whether a system complies with what is stated in an
information ﬂow policy has been formally stated as non-interference [5,7]. Infor-
mally non-interference states that for all input to a system, that varies only on
information not observable to the attacker, the resulting output will only vary
on information not observable to the attacker. We showed in Section 4.2 that
History-based Release generalises non-interference.

Locality-Based Security Policies
199
Non-Disclosure by Matos and Boudol [14] proposes extending the syntax of a
ML-like calculus with speciﬁc constructs for loosening the security policy. These
constructs have the form
ﬂow A ≺B in M
where M is an expression and A and B are security levels. The construct ex-
tends the security relation to permit information to ﬂow from A to B in M and
thereby permits disclosure of conﬁdential information in lexically scoped parts
of programs. The policies presented in this paper allow for ﬂows to be scoped
within a speciﬁed location, i.e. locations tied with a security domain. Clearly by
introducing a security domain tied to a fresh location for each ﬂow construct and
constraining the information ﬂow to only happen in execution traces containing
the security domain we get scoped ﬂows. Finally due to the transitive nature
of underlying lattice structure in [14], we need to perform a transitive closure
on the resulting graph to achieve the same eﬀect in our policies. In this manner
the Non-Disclosure property can be seen as a specialisation of the History-based
Release property. Obviously the Non-Disclosure property does not have the ex-
pressiveness to handle constraints on execution traces.
Intransitive non-interference. Goguen and Meseguer [7] generalised non-interfer-
ence to intransitive non-interference while observing that information ﬂows
might be permitted when properly ﬁltered at speciﬁed security levels. The prop-
erty was further investigated in [9,19] and adapted to a language-based setting
by Mantel and Sands [13]. Mantel and Sands [13] formalise intransitive non-
interference so that two goals are achieved. First the place in the program where
information ﬂow is limited through a syntactical extension of the language. Sec-
ond the security level where information ﬂows through is speciﬁed through an
extension of the security lattice by an intransitive component.
History-based Release incorporates these concerns. The place in the program
where information ﬂow is guaranteed in the same way as described above for
the non-disclosure property. Furthermore the locality-based security policies are
intransitive due to being based on graphs rather than lattices.
Non-interference until by Chong and Myers [3,4] propose adding conditions to
security policies based on lattices. This is done by introducing a special anno-
tated ﬂow into the security policies of the form ℓ0
c1
⇝· · ·
ck
⇝ℓk which states that
information can be gradually downgraded along with the fulﬁlment of the con-
ditions c1, . . . , ck. It is straightforward to represent the downgrading condition
with History-based Release.
However, observe that once the conditions are fulﬁlled, information can ﬂow
directly from ℓ0 to any of the security levels ℓ1, . . . , ℓk. Therefore non-interference
until does not provide the intransitive guarantees of History-based Release. An-
other point is the temporal constraints that History-based Release enforce on
execution traces. Non-interference until provides simple temporal guarantees,
namely that conditions are fulﬁlled prior to downgrading, however neither the
order of the fulﬁlment nor the conditions allow for temporal constraints.

200
T.K. Tolstrup, F. Nielson, and R.R. Hansen
Flow Locks. Recently Broberg and Sands [2] introduced the novel concept of
ﬂow locks as a framework for dynamic information ﬂow policies. The policies
are speciﬁed in syntactical constructs introduced in an ML-like calculus. The
constructs are utilised in the opening and closing of ﬂow locks, these locks are
used in constraining the information ﬂows in the policies. The policies have the
form {σ1 ⇒A1; . . . ; σn ⇒An} and are annotated to declarations of variables
in the programs. These policies correspond to ours, where a policy needs to
specify where information might ﬂow globally during execution and is hence not
transitively closed. The major diﬀerence is that our policies can include temporal
constraints which cannot be expressed in the ﬂow locks policies.
Another major diﬀerence between [2] and the present paper is the intuition
behind the security condition. In the ﬂow lock setting information can ﬂow to
security levels as speciﬁed in the policies, as long as necessary locks are opened
beforehand. This diﬀers from our deﬁnition in not being tied to the actual ﬂow
of information. E.g. once a lock is open information can ﬂow from several levels
and several times. Furthermore ﬂow locks have no way of observing if a lock has
been closed and opened again between two ﬂows. In our setting the constraints
on the execution trace must be fulﬁlled for every single ﬂow, hence it is not
suﬃcient that another process is executed at the right location, just before or
after considered ﬂow.
7
Conclusion
We have presented a novel concept of locality-based security policies for infor-
mation ﬂow security. These policies are based on labelled graphs and we have
illustrated how this allows for a simpler speciﬁcation of certain policies. Fur-
thermore we have presented the History-based Release condition that formalise
how temporal conditions and intransitive information ﬂow are captured in the
security policies.
A static analysis is presented as a mechanism for veriﬁcation of systems.
The analysis is divided into three parts. Since the ﬁrst part is the only syntax-
directed part and as it is independent of the security policy for the given system
it can freely be exchanged. Hence allowing us to analyse other process calculi
or even programming languages. Future investigation might consider possibili-
ties of adapting History-based Release to hardware description languages where
locations could be mapped to blocks in structural speciﬁcations.
References
1. Bell, D.E., LaPadula, L.J.: Secure computer systems: Mathematical foundations.
Technical Report MTR-2547, vol. 1, MITRE Corp., Bedford, MA (1973)
2. Broberg, N., Sands, D.: Flow locks: Towards a core calculus for dynamic ﬂow
policies. In: Proc. European Symposium on Programming, pp. 180–196 (2006)
3. Chong, S., Myers, A.C.: Security policies for downgrading. In: CCS ’04. Proceedings
of the 11th ACM conference on Computer and communications security, New York,
USA, pp. 198–209. ACM Press, New York (2004)

Locality-Based Security Policies
201
4. Chong, S., Myers, A.C.: Language-based information erasure. In: CSFW, pp. 241–
254 (2005)
5. Cohen, E.S.: Information transmission in computational systems. ACM SIGOPS
Operating Systems Review 11(5), 133–139 (1977)
6. Denning, D.E.: A lattice model of secure information ﬂow. Comm. of the
ACM 19(5), 236–243 (1976)
7. Goguen, J.A., Meseguer, J.: Security policies and security models. In: Proc. IEEE
Symp. on Security and Privacy, pp. 11–20 (April 1982)
8. Gorla, D., Pugliese, R.: Resource access and mobility control with dynamic priv-
ileges acquisition. In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J.
(eds.) ICALP 2003. LNCS, vol. 2719, pp. 119–132. Springer, Heidelberg (2003)
9. Haigh, J.T., Young, W.D.: Extending the Non-Interference Version of MLS for
SAT. In: IEEE Symposium on Security and Privacy, pp. 232–239. IEEE Computer
Society Press, Los Alamitos (1986)
10. Hansen, R.R., Probst, C.W., Nielson, F.: Sandboxing in myklaim. In: Proc.
ARES’06 (2006)
11. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to automata theory, lan-
guages, and computation, 2nd edn. Addison-Wesley, London (2001)
12. Kemmerer, R.A.: A practical approach to identifying storage and timing channels.
In: IEEE Symposium on Security and Privacy, pp. 66–73 (1982)
13. Mantel, H., Sands, D.: Controlled declassiﬁcation based on intransitive noninterfer-
ence. In: Chin, W.-N. (ed.) APLAS 2004. LNCS, vol. 3302, pp. 129–145. Springer,
Heidelberg (2004)
14. Matos, A.A., Boudol, G.: On declassiﬁcation and the non-disclosure policy. In:
Proc. IEEE Computer Security Foundations Workshop, pp. 226–240 (2005)
15. McHugh, J.: Covert Channel Analysis. Handbook for the Computer Security Cer-
tiﬁcation of Trusted Systems (1995)
16. Myers, A.C.: Jﬂow: Practical mostly-static information ﬂow control. In: POPL, pp.
228–241 (1999)
17. Myers, A.C., Liskov, B.: A decentralized model for information ﬂow control. In:
SOSP, pp. 129–142 (1997)
18. Nielson, H.R., Nielson, F.: Flow logic: A multi-paradigmatic approach to static
analysis. In: The Essence of Computation, pp. 223–244 (2002)
19. Rushby, J.: Noninterference, Transitivity, and Channel-Control Security Policies.
Technical Report CSL-92-02, SRI International (December 1992)
20. Sabelfeld, A., Myers, A.C.: Language-based information-ﬂow security. IEEE J. Se-
lected Areas in Communications 21(1), 5–19 (2003)
21. Sabelfeld, A., Sands, D.: Dimensions and principles of declassiﬁcation. In: Proc.
IEEE Computer Security Foundations Workshop. IEEE Computer Society Press,
Los Alamitos (2005)
22. Sabelfeld, A., Sands, D.: Probabilistic noninterference for multi-threaded programs.
In: CSFW, pp. 200–214 (2000)
23. Tolstrup, T.K., Nielson, F., Riis, H.: Information Flow Analysis for VHDL. In:
Proc. Eighth International Conference on Parallel Computing Technologies. LNCS,
Springer, Heidelberg (2005)
24. Volpano, D., Smith, G., Irvine, C.: A sound type system for secure ﬂow analysis.
J. Computer Security 4(3), 167–187 (1996)
25. Volpano, D.M., Smith, G.: Probabilistic noninterference in a concurrent language.
Journal of Computer Security 7(1) (1999)

A Theorem-Proving Approach to Veriﬁcation of
Fair Non-repudiation Protocols
Kun Wei and James Heather
Department of Computing, University of Surrey, Guildford, Surrey GU2 7XH, UK
{k.wei,j.heather}@surrey.ac.uk
Abstract. We use a PVS embedding of the stable failures model of
CSP to verify non-repudiation protocols, allowing us to prove the cor-
rectness of properties that are diﬃcult to analyze in full generality with a
model checker. The PVS formalization comprises a semantic embedding
of CSP and a collection of theorems and proof rules for reasoning about
non-repudiation properties. The well-known Zhou-Gollmann protocol is
analyzed within this framework.
1
Introduction
Over the past decade, formal methods have been remarkably successful in their
application to the analysis of security protocols. For example, the combination
of CSP and FDR [10,8,12] has proved to be an excellent tool for modelling and
verifying safety properties such as authentication and conﬁdentiality. However,
non-repudiation properties have not yet been mastered to the same degree since
they must often be expressed as liveness properties and the vast bulk of work to
date has been concerned only with safety properties.
Schneider has shown in [13] how to extend the CSP approach to analyze non-
repudiation protocols. His proofs of correctness, based on the traces and the
stable failures models of CSP as well as on rank functions, are constructed by
hand. For safety properties, one usually assumes that one honest party wishes
to communicate with another honest party, and one asks whether a dishon-
est intruder can disrupt the communications so as to eﬀect breach of security.
When considering non-repudiation, however, we are concerned with protecting
one honest party against possible cheating by his or her interlocutor. Thus a
non-repudiation protocol enables parties such as a sender Alice and a responder
Bob to send and receive messages, and provides them with evidence so that nei-
ther of them can deny having sent or received these messages when they later
resort to a judge for resolving a dispute.
There are two basic types of non-repudiation: Non-repudiation of Origin
(NRO) provides Bob with evidence of origin that unambiguously shows that
Alice has previously sent a particular message, and Non-repudiation of Receipt
(NRR) provides Alice with evidence of receipt that unambiguously shows that
Bob has received the message. Unforgeable digital signatures are usually the
mechanism by which NRO and NRR can be obtained.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 202–219, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

A Theorem-Proving Approach to Veriﬁcation
203
However, a major problem often arises: there may come a point during the
run at which either Alice or Bob reaches an advantageous position; for example,
Alice may have collected all the evidence she needs before Bob has collected his,
and Alice may then deliberately abandon the protocol to keep her advantageous
position. Usually we will want to ensure that the protocol is fair.
– Fairness guarantees that neither Alice nor Bob can reach a point where he
or she has obtained non-repudiation evidence, but where the other party is
prevented from retrieving any required evidence that has not already been
obtained.
Obviously fairness is the most diﬃcult property to achieve in the design of such
protocols, and several diﬀerent solutions have been proposed. Two kinds of ap-
proach are discussed in [6], classiﬁed according to whether or not the protocol
uses a third trusted party (TTP). The ﬁrst kind of approach providing fairness
in exchange protocols is based on either a gradual exchange [17] or probabilistic
protocol [9]. Without the involvement of a TTP, a sender Alice gradually releases
messages to a responder Bob over many rounds of a protocol, with the number
of rounds chosen by Alice and unknown to Bob. Bob is supposed to respond for
every message, and any failure to respond may cause Alice to stop the protocol.
However, such protocols require that all parties have the same computational
power, and a large number of messages must be exchanged. The other kind of
approach uses a TTP to handle some of the evidence. Many fair non-repudiation
protocols use the TTP as a delivery authority to establish and transmit some
key evidence. The eﬃciency of such protocols depends on how much a TTP is
involved in the communication, since heavy involvement of the TTP may become
a bottleneck of communication and computation.
In the CSP model, fairness is naturally described as a liveness property. It
is impossible for fairness to guarantee that both Alice and Bob can collect the
required evidence simultaneously, since we are dealing with an asynchronous
network, but it does guarantee that either of them must be able to access the
evidence as long as the other party has obtained it. In this paper, we will verify
the correctness of fairness of the Zhou-Gollmann protocol [20] using the process
algebra CSP and its semantics embedding in a PVS theorem prover.
We have shown how to go some way towards verifying fairness of the ZG
protocol using CSP and the FDR model checker in a recent paper [19]. However,
in order to keep the problem tractable, it was necessary there to model a system
with very small numbers of parties and messages; it was not possible to use the
model-checking approach to prove fairness of the protocol in its full generality,
or even for a system of a moderate size.
Evans and Schneider [4,13] give a useful start on this issue by using rank
functions and the PVS embedding of the traces model of CSP to verify safety
properties such as NRO and NRR. Our model, to some degree extending their
work, can verify not only safety speciﬁcations, but also liveness speciﬁcations
by means of embedding the stable failures model of CSP into the PVS theorem
prover; in consequence, we can prove fairness, which cannot be tackled within
their model.

204
K. Wei and J. Heather
In the remainder of this paper, we give a brief introduction to the CSP no-
tation and to the embedding of CSP in PVS. We present the ZG protocol and
its analysis, and then show the formalization of the protocol modelling and of
its veriﬁcation in PVS. Finally, we discuss and compare with other veriﬁcation
approaches for non-repudiation protocols.
2
CSP Notation
In CSP, a system can be considered as a process that might be hierarchically
composed of many smaller processes. An individual process can be combined
with events or other processes by operators such as preﬁxing, choice, parallel
composition, and so on. There are four semantic models available—traces, sta-
ble failures, failures/divergences, and failures/divergences/inﬁnite traces—and
which one is chosen depends on what properties of the system one is trying to
analyze. For safety properties, the traces model of CSP is enough. In this paper,
we use the stable failures model of CSP to verify fairness in the ZG protocol.
We will brieﬂy illustrate the CSP language and the semantic models; for a fuller
introduction, the reader is referred to [11,15].
Stop is a stable deadlocked process that never performs any events. The pro-
cess c →P behaves like P after performing the event c. An event like c may
be compounded; for example, one often used patten of events is c.i.j.m con-
sisting of a channel c, a sender i, a receiver j and a message m. The output
c!x →P describes a process which is initially willing to output x along channel
c, and then behave as P. The input c?x : T →P(x) denotes that it is initially
ready to accept any value x of type T along channel c, and subsequently behave
as P(x).
The external choice P1 2 P2 may behave either like P1 or like P2, depending
on what events its environment initially oﬀers it. The traces of internal choice
P1 ⊓P2 are the same as those of P1 2 P2, but the choice in this case is non-
deterministic.
The interface parallel P1 ∥
A
P2 is the process where all events in the set A must
be synchronized, and other events can be performed independently by P1 and P2
respectively. An interleaving P1 ||| P2 executes each part entirely independently
and is equivalent with P1 ∥
∅
P2.
A trace is deﬁned to be a ﬁnite sequence of events. A refusal set is a set of
events from which a process can fail to accept anything no matter how long it
is oﬀered; refusals(P/t) is the set of P’s refusals after the trace t; then (t, X ) is
a failure in which X denotes refusals(P/t). If the trace t can make no internal
progress, this failure is called a stable failure.
Liveness is concerned with behaviour that a process is guaranteed to make
available, and can be inferred from stable failures; for example, if, for a ﬁxed
trace t, we have a ̸∈X for all stable failures of P of the form (t, X ), then a
must be available after P has performed t.
Veriﬁcation of property speciﬁcations is done by means of determining
whether one process satisﬁes a speciﬁcation. In the stable failures model, this

A Theorem-Proving Approach to Veriﬁcation
205
equates to checking whether the traces and failures of one process are subsets of
the traces and failures of the other:
P sat S ⇔∀tr ∈traces(P) • S(tr) ∧∀(tr, X ) ∈failures(P) • S(tr, X )
where traces(P) collects all traces of the process P and failures(P) denotes the
set of stable failures of P.
For the properties we are considering, if S satisﬁes the property speciﬁcation
we are verifying, then P also holds such a property.
3
Embedding CSP Semantics in PVS
Full details of the embedding of the stable failures model of CSP in PVS for
mechanizing proofs is presented in [18].
In the analysis of security protocols, we usually do not require modelling of
successful termination; therefore, we here use a simpliﬁed version of the embed-
ding of CSP in order to reduce the complexity of veriﬁcation to some extent.
The stable failures model is represented by pairs (T, F) in which T is a set
of traces that forms the semantics of a process in the traces model. The classic
formalization of traces is to simply consider traces as lists of events. PVS has
provided a predeﬁned abstract datatype list. Thus, the type trace is deﬁned
as follows:
trace: TYPE = list[E]
where E is a parameter to denote the events appearing in the lists.
Table 1. CSP syntax
Operation
CSP
PVS
Stop
Stop
Stop
Preﬁx
a →P
a >> P
External choice
P1 2 P2
P1 \/ P2
Internal choice
P1 ⊓P2
P1 /\ P2
Interface parallel
P1 ∥
A P2
Par(A)(P1,P2)
Interleave
P1 ||| P2
P1 // P2
Processes in the stable failures model consist of pairs (T, F) that satisfy var-
ious conditions, which can be found in [18], or in [11,15]. Some of CSP’s main
operators are listed in Table 1, with the standard CSP syntax and PVS’s syn-
tax. We also use the relation ‘<=’ to denote satisfaction; for example, P sat S is
represented as P<=S in PVS.
Recursive processes in CSP are deﬁned in terms of equational deﬁnitions. We
here formalize such processes by using the ‘μ-calculus’ theory to compute the

206
K. Wei and J. Heather
least ﬁxed point of a monotonic function1. We also have proved a general ﬁxed
point induction theorem to verify recursive processes. In order for ﬁxed points
to be useful, we have extended the least ﬁxed point theory to represent mutually
recursive processes and to prove whether a function has a unique ﬁxed point.
In addition, we have proved a number of algebraic laws which are essential in
the veriﬁcation of properties of processes; these laws can also help us to verify
the consistency of the CSP semantics. For more detailed explanations of the
embedding of CSP in PVS, readers are advised to consult [18].
4
The Zhou-Gollmann Protocol
Zhou and Gollmann present a basic fair non-repudiation protocol using a
lightweight TTP in [20], which supports non-repudiation of origin and non-
repudiation of receipt as well as fairness. The main idea of the Zhou-Gollmann
protocol is that a sender Alice delivers the ciphertext and the message key to
Bob separately; the ciphertext is sent from the originator Alice to the recipient
Bob, and Alice then sends the message key encrypted with her secret key to
the TTP. Finally Alice and Bob may get their evidence from the TTP to es-
tablish the required non-repudiation. The notation below is used in the protocol
description.
– M : message to be sent from A to B.
– K: symmetric key chosen by A.
– C: commitment (ciphertext) for message M encrypted with K.
– L: a unique label used to identify a particular protocol run.
– fEOO, fEOR, fSUB, fCON : ﬂags indicating the purpose of a signed message.
– si: an asymmetric key used to generate i’s digital signature.
After cutting down the plaintext part, the simpliﬁed protocol can be described
as follows:
1. A →B : sA(fEOO, B, L, C)
2. B →A : sB(fEOR, A, L, C)
3. A →TTP : sA(fSUB, B, L, K)
4. B ↔TTP : sT(fCON , A, B, L, K)
5. A ↔TTP : sT(fCON , A, B, L, K)
We brieﬂy examine the protocol step by step to see how it works. Firstly,
Alice composes a message including a ﬂag, a unique label L, the receiver’s name
B and a ciphertext C = K(M ); Alice then signs the message with her private
key and sends it to Bob. Secondly, Bob collects the message as one piece of
evidence in which the label L identiﬁes the run of the protocol, and then Bob
responds with his signed message to provide A with evidence that B really has
1 A monotonic function in this context is a function F such that if Q ⊑P then
F(Q) ⊑F(P).

A Theorem-Proving Approach to Veriﬁcation
207
received C in this run. After she has got a response, Alice submits the encrypted
message key K to the TTP. The TTP then decrypts it to get the K, generates
the associated evidence encrypted with its private key and makes it available to
Alice and Bob. Finally, Alice and Bob can fetch the evidence respectively.
The guarantee of fairness of such a protocol comes from an assumption that
the channels between TTP and the parties are resilient; that is, messages may
be delayed, but will eventually arrive in a ﬁnite amount of time. However, the
channels between Alice and Bob can be unreliable; that is, the medium may
delay, lose or misdirect messages.
5
CSP Modelling
Schneider in [13] gives an excellent overview of how to extend the CSP approach
to analyze non-repudiation protocols. Because of the absence of mechanizing
support, however, his proof of correctness has to be constructed by hand. Our
version, whilst based on his analysis, is machine-assisted and makes various
changes to the model in order to bring it closer a real-world implementation
of the protocol.
Fairness says that if either A or B has managed to retrieve full evidence, the
other party cannot be prevented indeﬁnitely from retrieving the evidence. We
cannot assert for verifying fairness that once A has obtained the evidence then B
must have obtained the evidence as well, because there may be a delay between
A’s reception and B’s reception. However, we can ensure that the evidence must
be available to B, or that a speciﬁc action must be about to happen to enable
B to get the evidence in the future.
To check a protocol like this one with CSP, we have to build models of the
parties, the TTP and the medium and see how they can interfere with each other.
Since the protocol is used to protect parties that do not trust each other, we do
not adopt the traditional Dolev-Yao model [2], which provides a special intruder;
instead, in our model, one of two communicating parties is an intruder. Fairness
is only guaranteed to the party who runs in accordance with the protocol; for
example, if A releases the symmetric key K before B responds, A will certainly
place herself in a disadvantageous position.
It is also important, for a fully general proof, to allow other agents to partici-
pate on the network, since the protocol is expected to be correct no matter how
many parties there are. We use a similar structure to that given by Schneider;
the structure is shown in Figure 1.
The transmission of messages between parties is modelled by a CSP channel
trans: the event trans.i.j.m denotes that party i transmits a message m to
party j. Similarly, the receipt of messages is modelled as a channel rec: the event
rec.i.j.m means that i receives a message m from j. The medium plays a role of
the unreliable channel, whereas the the resilient channel is removed out of the
medium and is used to directly link parties and the TTP. The communication in
the resilient channel is modelled as send and get. In addition, the parties have
an evidence channel which they use to announce the evidence.

208
K. Wei and J. Heather
A
TTP
B
MEDIUM
trans
rec
send
get
resilient channel
unreliable channel
evidence.A
evidence.B
trans
rec
Fig. 1. Network for the ZG protocol
The entire network is generally represented as the parallel composition of
these components:
NETWORK =((|||i∈USER Partyi)
∥
{send,get}
TTP)
∥
{trans,rec}
Medium
In our scenario, we will treat A as a dishonest party, or a spy, and B as
an honest party who always performs in accordance with the protocol; A and
B may behave either as a sender or as a responder; A and B may run the
protocol consecutively, and A may make use of the information deduced from
B’s messages to initiate a new run and may change her role at will. We also
assume that more parities are able to communicate with A and B and consider
the worst case that all parties except B are dishonest parties in the network.
Therefore, such a network is further described as following:
NETWORK =(PartyB ||| (|||i∈USER\{B} Spyi))
∥
{send,get}
TTP
∥
{trans,rec}
Medium
5.1
Deﬁning Honest Parties
The basic assumption underlying the deﬁnition is that the party B is not able
to release his private key, reuse a label, or lose evidence he has already got. In
our model, the party B can act either as a sender or as a responder.
PartyB = SEND 2 RESP

A Theorem-Proving Approach to Veriﬁcation
209
When acting as a sender, the party B running the protocol will then be
described as follows:
SEND = 2i∈User trans.B!i!sB(fEOO.i.L.C)
→rec.B.i.si(fEOR.B.L.C)
→send.B.TTP.sB(fSUB.i.L.K)
→get.B.TTP.sT(fCON .i.B.L.K) →PartyB
Here we deﬁne that B is not included in the set USER. The responder process
performs the protocol from the opposite perspective. B acting as a responder is
described as follows:
RESP = 2j∈User rec.B?j!sj (fEOO.B.L.C)
→trans.B.j.sB(fEOR.j.L.C)
→get.B.TTP.sT(fCON .j.B.L.K) →PartyB
Obtaining the evidence channel to announce the evidence for an honest party
is somewhat unnecessary since the PartyB can get the evidence only through his
interlocutor and the TTP. However, a dishonest party may obtain the evidence
from someone else.
5.2
Creating a Spy
In our modelling of the non-repudiation protocol, we do not deﬁne a special
party, a spy, as diﬀerent from the legitimate parties. We assume that one of two
communicating parties is a spy who may be able to deduce something of value
from the messages it has received. The non-repudiation protocol is supposed to
provide fairness for an honest party even if the other party is a spy.
The behaviour of a spy on the network is therefore described by the CSP
process Spy:
Spyi(S) = (2m∈S trans.i.!j!m →Spyi(S))
2 rec.i.?j?m →Spyi(Close(S ∪{m}))
2 (2m∈S send.i.TTP!m →Spyi(S))
2 get.i.TTP?m →Spyi(Close(S ∪{m}))
2 (2m∈S evidence.i?m →Spyi(S))
The spy is able to transmit anything over the network that can be deduced
from the messages she has already learnt. She is also able to receive anything
transmitted over the network.

210
K. Wei and J. Heather
The spy has an initial basic knowledge, such as public keys, labels and so on,
and can build a number of legitimate messages before the start of the protocol.
The Close(S) function returns the set S closed up under these deduction rules.
We here allow the spy to have three types of deduction based on constructing
and extracting sequences, symmetric-key encryption and public-key encryption.
For example, if she knows {K(M ), K}, the spy can deduce M . However, the spy
is supposed not to know other parties’ private keys since they will never transmit
these keys on the network.
5.3
Medium and TTP
The medium provides two types of message delivery service: one is an unreliable
channel where messages might be lost, delayed and sent to any address; another
one is a resilient channel where messages might be delayed, but will eventually
arrive, and also be guaranteed not to arrive at the wrong address. The medium
here is deﬁned only for the unreliable channel, since the resilient channel will be
integrated into the deﬁnition of the TTP.
Medium(S) = ( trans!i!j!m →Medium(S ∪{m})
2 (2m∈S rec?i?j?m →Medium(S\{m}) ) ⊓idle →Medium(S)
Note that the medium can deliver a message to the wrong destination, which
means that it may lose messages in some sense. The idle channel may cause
messages to be delayed at random.
The trusted third party is expected to act in accordance with its role in the
protocol; that is, the TTP accepts signed messages, generates new evidence and
makes them available to associated parties. It is therefore modelled as follows:
TTP(S) = ( send!i!TTP!m →TTP(S ∪Evi(i, m)))
2 (2e∈S get?e →TTP(S) )) ⊓idle →TTP(S)
The TTP also plays the role of the resilient channel, along with the idle chan-
nel that causes delays to message delivery. The Evi({(i, m)})) generates two
copies of the evidence, for example, in this case they are A.TTP.sT
(fCON .A.B.L.K) and B.TTP.sT(fCON .A.B.L.K), so that only involved parties
are able to have access to the evidence. It is important to note the underly-
ing assumption hidden in this deﬁnition: the TTP always stores evidence it has
generated and never discards it.
5.4
Speciﬁcation
We here concentrate on fairness of the ZG protocol since Evans and Schnei-
der [4,13] have provided rigorous veriﬁcation of Non-repudiation of Origin and
Non-repudiation of receipt in PVS by using the embedding of the traces model
of CSP and rank functions.

A Theorem-Proving Approach to Veriﬁcation
211
Fairness is naturally expressed in the stable failures model of CSP. The essence
of the idea is that if one of the two parties has obtained full evidence, then the
other party either is already in possession of it or is able to access it. Since
fairness is guaranteed only to a party who performs completely in accordance
with the protocol, we here give only two speciﬁcations according to the diﬀerent
roles of B.
First, we deal with the case where B acts as a responder; that is, if A has
proof of receipt, then B must be in a position to obtain proof of origin. Thus
the formal speciﬁcation is given as follows:
FAIR1(tr, X ) = evidence.A.sT (fCON .A.B.L.K) ∈tr
∧evidence.A.sB(fEOR.A.L.C) ∈tr
⇒
rec.B.A.sA(fEOO.B.L.C) ∈tr
∧(get.B.TTP.sT(fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X )
and the requirement on the system is that
NETWORK
sat FAIR1(tr, X )
The above speciﬁcation states that if A holds full evidence, then B must either
be able to get the evidence or have already obtained the evidence.
Secondly, we deal with the case in which B acts as a sender; that is, if A has
proof of origin, then B must be in a position to obtain proof of receipt. It is
therefore modelled as follows:
FAIR2(tr, X ) = evidence.A.sT(fCON .A.B.L.K) ∈tr
∧evidence.A.sB(fEOO.A.L.C) ∈tr
⇒
rec.B.sA(fEOR.B.L.C) ∈tr
∧(get.B.TTP.sT (fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X )
and the speciﬁcation is:
NETWORK
sat FAIR2(tr, X )
We now need to verify the two assertions above by translating the speciﬁca-
tions into PVS notation.
6
The Fairness Model in PVS
The fairness property requires that if A has obtained full evidence, then B ei-
ther is already in possession of it or is able to access it; for example, in the

212
K. Wei and J. Heather
FAIR1 speciﬁcation, if A has got sB(fEOR.A.L.C) and sT(fCON .A.B.L.K), then
either both messages sA(fEOO.B.L.C) and sT(fCON .A.B.L.K) must have been
appeared in the trace of B or the event get.B.TTP.sT(fCON .A.B.L.K) cannot
be prevented from appearing in the trace of B.
In order to establish such properties, it is useful to construct some general
properties. We here use the assertion that the network satisﬁes the FAIR1 spec-
iﬁcation as an example to show how to prove the fairness property in PVS.
According to the description of FAIR1, we split it into two lemmas:
Lemma 1
NETWORK sat evidence.A.sB(fEOR.A.L.C) ∈tr
⇒rec.B.A.sA(fEOO.B.L.C) ∈tr
and
Lemma 2
NETWORK sat evidence.A.sB(fEOR.A.L.C) ∈tr
∧evidence.A.sT(fCON .A.B.L.K) ∈tr
⇒get.B.TTP.sT(fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X
Obviously, the FAIR1 is proved if Lemma 1 and Lemma 2 hold. We will prove
the above two lemmas respectively.
The fairness property is concerned with the fact that certain events should
occur only under particular circumstances. We here ﬁrst introduce an important
speciﬁcation derived from Schneider’s rank function theory as follows:
R precedes T = tr ↾R = ⟨⟩⇒tr ↾T = ⟨⟩
no R = tr ↾R = ⟨⟩
which states that if some events from T occur in a trace, then some events from
R must have been appear earlier in the trace. The Lemma 1 can be naturally
represented by such a property.
Schneider [14] has provided a well-developed approach using the rank function
theory and the CSP traces model to establish such properties for authentication
properties. Evan [4] has also constructed the proof of NRO and NRR of the ZG
protocol in PVS. Since the proof of the Lemma 1 has nearly been done in their
previous work, we here just migrate it to our stable failures model and rewrite
all proofs. For example, the deﬁnition pcd(R,T) and some rules are given in
Figure 2 where proj corresponding to ↾is the projection operation.
The basic idea in the rank function theory is that we use a function ρ to map
the network’s message space to two values: 0 and 1. For example, we assume
that every message in T has rank 0 and other messages have rank 1; if every
component of the network maintains2 positive rank when prevented from out-
putting R, we may assert that nothing in T can occur unless something in R
2 If only messages of positive rank are input, then any output message must have
positive rank.

A Theorem-Proving Approach to Veriﬁcation
213
property_pcd[E:TYPE]:THEORY
...
a: VAR E
t: VAR trace[E]
R,T,X,A: VAR set[E]
P,Q: VAR process[E]
pcd(R,T):[set[trace[E],set[[trace[E],set[E]]]]
=( {t| proj(t,T) = null IMPLIES proj(t,R)= null},
{(t,X)|true})
no(R): [set[trace[E]],set[[trace[E],set[E]]]]
= ( { t | proj(t,R)=null }, {(t,X) | true } )
...
pcd_parallel: LEMMA P <= pcd(R,T) AND subset?(T,A)
IMPLIES Par(A)(P,Q) <= pcd(R,T)
pcd_interleave: LEMMA P <= pcd(R,T) AND Q <= pcd(R,T)
IMPLIES P//Q <= pcd(R,T)
pcd_interleave1: LEMMA P <= pcd(R,T) AND Q <= no(T)
IMPLIES P//Q <= pcd(R,T)
...
END property_pcd
Fig. 2. Proof rules for precedes
occurs previously. The rank function approach is discussed more fully in [14] and
the proof is detailedly introduced in [3,4], and no further discussion is given in
the paper.
Using the rank function approach, we have proved the following key property:
Lemma 3
NETWORK sat evidence.A.sB(fEOR.A.L.C) ∈tr
⇒trans.B.A.sB(fEOR.A.L.C) ∈tr
The key property is that if A has obtained the evidence sB(fEOR.A.L.C) then
B must have sent it; in other words, a message signed by sB must have rank 0
in this case. Such a rank function is established in PVS as Figure 3.
The network also satisﬁes another property:
Lemma 4
NETWORK sat rec.B.A.sA(fEOO.B.L.C) ∈tr
⇒trans.B.A.sB(fEOR.A.L.C) ∈tr

214
K. Wei and J. Heather
rho(m) : RECURSIVE int =
CASES m OF
text(z)
: 1,
label(z)
: 1,
user(z)
: 1,
flag(z)
: 1,
public(z)
: 1,
symm(z)
: 1,
secret(z)
: IF z = b THEN 0 ELSE 1 ENDIF,
conc(z1, z2) : min(rho(z1), rho(z2)),
code(k, z)
: IF k = secret(b) AND
z = conc4(feor,a,l,m) THEN 0 ELSE rho(z) ENDIF
ENDCASES
MEASURE m BY <<
Fig. 3. Rank function for Lemma 3
Proof. In the behaviour of PartyB, the event rec.B.A.sA(fEOO.B.L.C) always
precedes trans.B.A.sB(fEOR.A.L.C) since he is an honest party. Therefore the
network inherits the property from the PartyB
according to the rules
pcd_parallel and pcd_interleave1 listed in Figure 2.
In addition, the ‘precedes’ can be transitive; therefore, the Lemma 1 is proved
by the Lemma 3 and Lemma 4.
To ﬁnish the proof of the fairness property, we then introduce the second
property as follows:
T is unpreventable after R = σ(tr ↾R) = R
⇒(X ∩T) ‵ σ(tr ↾T) = ∅
which ‵ is a diﬀerent operation. The property shows that if all events from the
set R occur in a trace, then the event from T either has appeared earlier in the
trace or is not included in the refusal set X after performing this trace. Such
a property corresponding to the second part of the FAIR1 speciﬁcation is here
used to prove that get.B.TTP.sT(fCON .A.B.L.K) is unpreventable after A has
got the full evidence. The deﬁnition of upt(T,R) and some important rules are
given in Figure 4. We also deﬁne s_pcd(R,T) as a more strict deﬁnition of the
‘precedes’ property which requires if all elements of T occur in a trace then all
elements of R must have occurred in the trace.
The Lemma 2 is too tricky to be reached simply by applying the rules listed
in Figure 4. The solution is to rewrite the NETWORK process as follows:
NETWORK = (PartyA ||| PartyB)
∥
{trans,rec,send,get}
(Medium ||| TTP)

A Theorem-Proving Approach to Veriﬁcation
215
property_upt[E:TYPE]:THEORY
...
a: VAR E
t: VAR trace[E]
R,T,X,A: VAR set[E]
P,Q: VAR process[E]
upt(T,R):[set[trace[E],set[[trace[E],set[E]]]]
=( {t|true},
{(t,X)| sigma(proj(t,R))=R IMPLIES
difference(intersection(X,T), sigma(proj(t,T)))
= emptyset })
s_pcd(R,T):[set[trace[E],set[[trace[E],set[E]]]]
=( {t| sigma(proj(t,T))=T IMPLIES sigma(proj(t,R))=R},
{(t,X) | true } )
...
upt_parallel: LEMMA P <= upt(T,R) AND Q <= upt(T,X) AND
subset?(union(R,X),A) AND subset?(T,A)
IMPLIES Par(A)(P,Q) <= upt(T,union(R,X))
upt_interleave: LEMMA P <= upt(T,R) AND Q <= no(R)
IMPLIES P//Q <= upt(T,R)
upt_pcd_transitive: LEMMA
P <= upt(T,X) AND P <= s_pcd(X,R)
IMPLIES P <= upt(T,R)
...
END property_upt
Fig. 4. Proof rules for unpreventable
Obviously, this new deﬁnition is equivalent to the original one, but makes the
proof become rather easier when combined with the following lemmas.
Lemma 5
Medium ||| TTP sat get.A.TTP.sT(fCON .A.B.L.K) ∈tr
⇒get.B.TTP.sT(fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X
Proof. Once the TTP has received a legitimate submission, the evidence is al-
ways available to the parties involved. Hence, get.B.TTP.sT(fCON .A.B.L.K)
is always unpreventable after get.A.TTP.sT(fCON .A.B.L.K) has occurred. The
assertion that the TTP satisﬁes the property has been proved by directly using
the general induction theorem. The interleaving of the medium and TTP holds
it as well by means of the rule upt_interleave listed in Figure 4.

216
K. Wei and J. Heather
Lemma 6
PartyA ||| PartyB sat trans.B.A.sB(fEOR.A.L.C) ∈tr
⇒get.B.TTP.sT(fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X
Proof. The PartyB has to perform get.B.TTP.sT(fCON .A.B.L.K) after he re-
sponds to A with sB(fEOO.A.L.C), so that get.B.TTP.sT(fCON .A.B.L.K) is
unpreventable.
Lemma 7
NETWORK sat trans.B.A.sB(fEOR.A.L.C) ∈tr
∧get.A.TTP.sT(fCON .A.B.L.K) ∈tr
⇒get.B.TTP.sT(fCON .A.B.L.K) ∈tr
∨get.B.TTP.sT(fCON .A.B.L.K) ̸∈X
Proof. This is proved relying on the rule upt_parallel with the Lemma 5 and
Lemma 6 since the union of two ‘preceding’ events respectively in two diﬀerent
processes is a subset of the interface of their parallel then the unpreventable
property still holds in the parallel composition.
Furthermore, the Lemma 2 is proved in terms of the Lemma 7 and two prop-
erties that trans.B.A.sB(fEOR.A.L.C) precedes evidence.A.sB(fEOR.A.L.C) and
get.A.TTP.sT(fCON .A.B.L.K) precedes evidence.A.sT(fCON .A.B.L.K) in the
network. The rule upt_pcd_transitive plays an important role in the above
proof that if the process P satisﬁes the fact that a set of events T are unpre-
ventable after a set of events X , whereas X precedes a set of events R, then P
satisﬁes that T is unpreventable after R.
This completes the proof; we have formalized all this in PVS. The proof of the
FAIR2 speciﬁcation has also been formally completed in a similar way, making
use of the above lemmas.
It is also important, for a fully general proof, to allow other agents to par-
ticipate on the network, since the protocol is expected to be correct no matter
how many parties there are. We have also extended our model to more general
situations; for example, the case where B may engage in concurrent runs.
7
Discussion
In this paper, we have modelled and veriﬁed the Zhou-Gollmann non-repudiation
protocol with respect to correctness of fairness, which requires that neither of
two parties can establish evidence of origin or evidence of receipt while still
preventing the other party from obtaining such evidence. Proving fairness in a
theorem prover can help us to extract some general understanding of how to
design such a kind of protocol. For instance, from the Lemma 5 and Lemma 6,

A Theorem-Proving Approach to Veriﬁcation
217
we can clearly see that least two factors should be considered: one is that the
TTP should always make the evidence available to the parties involved when it
has received a legitimate submission; the other is that B cannot be prevented
from accessing the evidence when he has responded to the ﬁrst part of evidence
from A.
Although the Zhou-Gollmann protocol is rather simple, our formal veriﬁcation
shows that it does provide strong fairness under the assumptions described in
this paper. However, there is an attack [5] if we slightly change our assumptions.
Suppose that we allow the TTP and B to lose the evidence, and A ﬁrst completes
a protocol run with B and possesses sB(fEOR.A.L.C) and sT(fCON .A.B.L.K); a
couple of weeks later, A then uses the same symmetric key and label but sends
a new plaintext message to initiate a new run; A ends the run by obtaining the
new EOR and presenting it with sT(fCON .A.B.L.K) to a judge; A might be
lucky enough to discover that TTP and B have discarded their old evidence.
Incorporating this assumption into our model would mean that the correctness
of the fairness property cannot be proved because of the presence of the hidden
attacks. For example, Lemma 5 would not hold any more if B could lose the
evidence. In investigating why these lemmas can no longer be proven, it is likely
that one would uncover the attack.
We here give a suggestion that all evidence should have incorporated into it
a tag provided by the TTP, such as timestamp, so as to make it clear when the
two parts of the evidence match each other.
Some related work can be found in the literature concerning veriﬁcation of
non-repudiation protocols using diﬀerent approaches. Zhou et al. in [21] ﬁrstly
use ‘BAN-like’ belief logic to check only safety properties of the non-repudiation
protocols. Schneider [13] gives an excellent overview of the CSP modelling and
proves the correctness of properties using stable failures and rank functions; how-
ever, the proofs are constructed by hand. Evans [4] extends Schneider’s work to
prove safety properties such as NRO and NRR in the PVS theorem prover.
Shmatikov and Mitchell in [16] verify fairness as a monotonic property using
Murϕ; that is, if fairness is broken at one point of the protocol, the proto-
col will remain unfair. This approach also cannot deal with liveness properties.
Kremer and Raskin [7] use the ﬁnite state model checker MOCHA to verify non-
repudiation and fair exchange protocols. This approach, which is rather diﬀerent
from ours here, can also cope with liveness properties as well as safety properties.
However, they have modelled networks in which A and B can engage in only
one run of the protocol. In addition, Abadi and Blanchet [1] formalize and verify
the key security properties of a cryptographic protocol for certiﬁed email that
has many commonalities with the Zhou-Gollmann protocol. Most of veriﬁcation
work is done with an automatic protocol veriﬁer, however such a tool has not
been veriﬁed and ﬁnished.
We have proved the fairness property of the Zhou-Gollmann protocol in its
full generality in the case of two parties that are able to perform multiple runs
or concurrent runs, along with an unbounded number of other dishonest parties.
Admittedly, verifying a system like this requires considerable work. However,

218
K. Wei and J. Heather
PVS is a deductive system in which all completed proofs can be used in later
proofs. In the course of constructing this proof, we have amassed many lemmas
and theorems that will make proving properties of similar systems substantially
less time-consuming, both for us and for others.
We aim to extend our model to cover a larger size of network running the Zhou-
Gollmann protocol; for example, we will deal with the network allowing multiple
concurrent runs. We also wish to challenge the analysis of certain sophisticated
systems or security protocols.
References
1. Abadi, M., Blanchet, B.: Computer-Assisted Veriﬁcation of a Protocol for Certiﬁed
Email. In: Cousot, R. (ed.) SAS 2003. LNCS, vol. 2694, pp. 316–335. Springer,
Heidelberg (2003)
2. Dolev, D., Yao, A.C.: On the security of public key protocols. IEEE Transactions
on Information Theory 29(2) (1983)
3. Dutertre, B., Schneider, S.A.: Embedding CSP in PVS: an application to au-
thentication protocols. In: Gunter, E.L., Felty, A.P. (eds.) TPHOLs 1997. LNCS,
vol. 1275, Springer, Heidelberg (1997)
4. Evans, N.: Investigating Security through Proof. PhD thesis, Royal Holloway, Uni-
versity of London (2003)
5. G¨urgens, S., Rudolph, C.: Security analysis of (un-) fair non-repudiation protocols.
In: Abdallah, A.E., Ryan, P.Y A, Schneider, S. (eds.) FASec 2002. LNCS, vol. 2629,
pp. 97–114. Springer, Heidelberg (2003)
6. Kremer, S., Markowitch, O., Zhou, J.: An intensive survey of non-repudiation pro-
tocols. Technical Report 473 (2002)
7. Kremer, S., Raskin, J.-F.: A game-based veriﬁcation of non-repudiation and fair
exchange protocols. In: Larsen, K.G., Nielsen, M. (eds.) CONCUR 2001. LNCS,
vol. 2154, Springer, Heidelberg (2001)
8. Lowe, G.: Breaking and ﬁxing the Needham-Schroeder public-key protocol using
FDR. In: Margaria, T., Steﬀen, B. (eds.) TACAS 1996. LNCS, vol. 1055, pp.
147–166. Springer, Heidelberg (1996)
9. Markowitch, O., Roggeman, Y.: Probabilistic non-repudiation without trusted
third party. In: Second Workshop on Security in Communication Network 99 (1999)
10. Roscoe, A.W.: Modelling and verifying key-exchange protocols using CSP and
FDR. In: Proceedings of 8th IEEE Computer Security Foundations Workshop
(1995)
11. Roscoe, A.W.: The Theory and Practice of Concurrency. Prentice-Hall Interna-
tional, Englewood Cliﬀs (1998)
12. Ryan, P., Schneider, S., Goldsmith, M., Lowe, G., Roscoe, B.: The Modelling And
Analysis Of Security Protocols. Addison Wesley, London (2000)
13. Schneider, S.A.: Formal analysis of a non-repudiation protocol. In: Proceedings of
the 11th IEEE Computer Security Foundations Workshop (1998)
14. Schneider, S.A.: Verifying authentication protocols in CSP. IEEE TSE, 24(9)
(September 1998)
15. Schneider, S.A.: Concurrent and real-time systems: the CSP approach. John Wiley
& Sons, West Sussex, England (1999)
16. Shmatikov, V., Mitchell, J.C.: Analysis of abuse-free contract signing. In: Frankel,
Y. (ed.) FC 2000. LNCS, vol. 1962, pp. 174–191. Springer, Heidelberg (2001)

A Theorem-Proving Approach to Veriﬁcation
219
17. Tedrick, T.: How to exchange half a bit. In: CRYPTO, pp. 147–151 (1983)
18. Wei, K., Heather, J.: Embedding the stable failures model of CSP in PVS. In:
accepted by Fifth International Conference on Integrated Formal Methods, Eind-
hoven, The Netherlands (2005)
19. Wei, K., Heather, J.: Towards veriﬁcation of timed non-repudiation protocols. In:
Dimitrakos, T., Martinelli, F., Ryan, P.Y A, Schneider, S. (eds.) FAST 2005. LNCS,
vol. 3866, Springer, Heidelberg (2006)
20. Zhou, J., Gollmann, D.: A fair non-repudiation protocol. In: Proceedings of the
IEEE Symposium on Research in Security and Privacy, Oakland, CA, pp. 55–61.
IEEE Computer Society Press, Los Alamitos (1996)
21. Zhou, J., Gollmann, D.: Towards veriﬁcation of non-repudiation protocols. In: Pro-
ceedings of 1998 International Reﬁnement Workshop and Formal Methods Paciﬁc,
pp. 370–380, Canberra, Australia (September 1998)

A Formal Speciﬁcation of the MIDP 2.0
Security Model⋆
Santiago Zanella B´eguelin1, Gustavo Betarte2, and Carlos Luna2
1 INRIA Sophia Antipolis, 06902 Sophia Antipolis Cedex, France
Santiago.Zanella@sophia.inria.fr
2 InCo, Facultad de Ingenier´ıa, Universidad de la Rep´ublica, Montevideo, Uruguay
{gustun,cluna}@fing.edu.uy
Abstract. This paper presents, to the best of our knowledge, the ﬁrst
formal speciﬁcation of the application security model deﬁned by the
Mobile Information Device Proﬁle 2.0 for Java 2 Micro Edition. The
speciﬁcation, which has been formalized in Coq, provides an abstract
representation of the state of a device and the security-related events
that allows to reason about the security properties of the platform where
the model is deployed. We state and sketch the proof of some desirable
properties of the security model. Although the abstract speciﬁcation is
not executable, we describe a reﬁnement methodology that leads to an
executable prototype.
1
Introduction
Mobile devices (e.g. cell phones) often have access to sensitive personal data,
are subscribed to paid services and are capable of establishing connections with
external entities. Users of such devices may, in addition, download and install
applications from untrusted sites at their will. Since any security breach may
expose sensitive data, prevent the use of the device, or allow applications to
perform actions that incur a charge for the user, it is essential to provide an
application security model that can be relied upon – the slightest vulnerability
may imply huge losses due to the scale the technology has been deployed.
Java 2 Micro Edition (J2ME) is a version of the Java platform targeted at
resource-constrained devices which comprises two kinds of components: conﬁgu-
rations and proﬁles. A conﬁguration is composed of a virtual machine and a set
of APIs that provide the basic functionality for a particular category of devices.
Proﬁles further determine the target technology by deﬁning a set of higher level
APIs built on top of an underlying conﬁguration. This two-level architecture en-
hances portability and enables developers to deliver applications that run on a
wide range of devices with similar capabilities. This work concerns the topmost
level of the architecture which corresponds to the proﬁle that deﬁnes the security
model we formalize.
⋆This work is partially funded by the IST program of the European Commission,
FET under the MOBIUS project, the INRIA-Microsoft Joint Research Laboratory,
and STIC-AmSud under the ReSeCo project.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 220–234, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

A Formal Speciﬁcation of the MIDP 2.0 Security Model
221
The Connected Limited Device Conﬁguration (CLDC) is a J2ME conﬁgura-
tion designed for devices with slow processors, limited memory and intermit-
tent connectivity. CLDC together with the Mobile Information Device Proﬁle
(MIDP) provides a complete J2ME runtime environment tailored for devices
like cell phones and personal data assistants. MIDP deﬁnes an application life
cycle, a security model and APIs that oﬀer the functionality required by mobile
applications, including networking, user interface, push activation and persistent
local storage. Many mobile device manufacturers have adopted MIDP since the
speciﬁcation was made available. Nowadays, literally millions of MIDP enabled
devices are deployed worldwide and the market acceptance of the speciﬁcation
is expected to continue to grow steadily.
In the original MIDP 1.0 speciﬁcation [1], any application not installed by
the device manufacturer or a service provider runs in a sandbox that prohibits
access to security sensitive APIs or functions of the device (e.g. push activation).
Although this sandbox security model eﬀectively prevents any rogue application
from jeopardizing the security of the device, it is excessively restrictive and does
not allow many useful applications to be deployed after issuance.
MIDP 2.0 [2] introduces a new security model based on the concept of protec-
tion domain. Each API or function on the device may deﬁne permissions in order
to prevent it from being used without authorization. Every installed application
is bound to a unique protection domain that deﬁnes a set of permissions granted
either unconditionally or with explicit user authorization. Untrusted applica-
tions are bound to a protection domain with permissions equivalent to those in
a MIDP 1.0 sandbox. Trusted applications may be identiﬁed by means of cryp-
tographic signatures and bound to more permissive protection domains. This
model enables applications developed by trusted third parties to be downloaded
and installed after issuance of the device without compromising its security.
Some eﬀort has been put into the evaluation of the security model for MIDP
2.0; Kolsi and Virtanen [3] and Debbabi et al. [4] analyse the application security
model, spot vulnerabilities in various implementations and suggest improvements
to the speciﬁcation. Although these works report on the detection of security
holes, they do not intend to prove their absence. The formalization we overview
here, however, provides a formal basis for the veriﬁcation of the model and the
understanding of its intricacies.
We developed our speciﬁcation using the Coq proof assistant [5,6]. A de-
tailed description of the speciﬁcation is presented in Spanish in [7]; a shorter,
preliminary version of this paper appeared in InCo’s technical report series [8].
Both documents, along with the full formalization in Coq may be obtained from
http://www-sop.inria.fr/everest/personnel/Santiago.Zanella/MIDP.
The rest of the paper is organized as follows, Section 2 describes some of
the notation used in this document, Section 3 overviews the formalization of
the MIDP 2.0 security model, Section 4 presents some of its veriﬁed properties
along with outlines of their proofs, Section 5 proposes a methodology to reﬁne the
speciﬁcation and obtain an executable prototype and ﬁnally, Section 6 concludes
with a summary of our contributions and directions for future work.

222
S. Zanella B´eguelin, G. Betarte, and C. Luna
2
Notation
We use standard notation for equality and logical connectives (∧, ∨, ¬, →, ∀, ∃).
Implication and universal quantiﬁcation may be encoded in Coq using dependent
product, while equality and the other connectives can be deﬁned inductively.
Anonymous predicates are introduced using lambda notation, e.g. (λ n . n = 0)
is a predicate that when applied to n, is true iﬀn is zero.
We extensively use record types; a record type deﬁnition
R
def
= {field1 : A1, . . . , fieldn : An}
(1)
generates a non-recursive inductive type with just one constructor, namely mkR,
and projections functions fieldi : R →Ai. We write ⟨a1, . . . , an⟩instead of
mkR a1 . . . an when the type R is obvious from the context. Application of
projections functions is abbreviated using dot notation (i.e. fieldi r = r.fieldi).
For each ﬁeld fieldi in a record type we deﬁne a binary relation ≡fieldi over
objects of the type as
r1 ≡fieldi r2
def
= ∀j, j ̸= i →r1.fieldj = r2.fieldj
(2)
We deﬁne an inductive relation I by giving introduction rules of the form
P1 · · · Pm
I x1 . . . xn rule
(3)
where free occurrences of variables are implicitly universally quantiﬁed.
We assume as predeﬁned inductive types the parametric type option T with
constructors None : option T and Some : T →option T , and the type seq T of
ﬁnite sequences over T . We denote the empty sequence by [ ] and the constructor
that appends an element a to a sequence s in inﬁx form as in s ⌢a. The symbol
⊕stands for the concatenation operator on sequences.
3
Formalization of the MIDP 2.0 Security Model
In this section we present and discuss the formal speciﬁcation of the security
model. We introduce ﬁrst some types and constants used in the remainder of the
formalization, then we deﬁne the set of valid device states and security-related
events, give a transition semantics for events based on pre- and postconditions
and deﬁne the concept of a session.
3.1
Sets and Constants
In MIDP, applications (usually called MIDlets) are packaged and distributed
as suites. A suite may contain one or more MIDlets and is distributed as two
ﬁles, an application descriptor ﬁle and an archive ﬁle that contains the actual
Java classes and resources. A suite that uses protected APIs or functions should

A Formal Speciﬁcation of the MIDP 2.0 Security Model
223
declare the corresponding permissions in its descriptor either as required for its
correct functioning or as optional.
Let Permission be the total set of permissions deﬁned by every protected API
or function on the device and Domain the set of all protection domains. Let us
introduce, as a way of referring to individual MIDlet suites, the set SuiteID of
valid suite identiﬁers. We will represent a descriptor as a record composed of two
predicates, required and optional, that identify respectively the set of permis-
sions declared as required and those declared as optional by the corresponding
suite,
Descriptor
def
= {required, optional : Permission →Prop}
(4)
A record type is used to represent an installed suite, with ﬁelds for its identiﬁer,
associated protection domain and descriptor,
Suite
def
= {id : SuiteID, domain : Domain, descriptor : Descriptor}
(5)
Permissions may be granted by the user to an active MIDlet suite in either of
three modes, for a single use (oneshot), as long as the suite is running (session), or
as long as the suite remains installed (blanket). Let Mode be the enumerated set
of user interaction modes {oneshot, session, blanket} and ≤m an order relation
such that
oneshot ≤m session ≤m blanket
(6)
We will assume for the rest of the formalization that the security policy of the
protection domains on the device is an anonymous constant of type
Policy
def
= { allow : Domain →Permission →Prop,
user : Domain →Permission →Mode →Prop }
(7)
which for each domain speciﬁes at most one mode for a given permission,
(∀d p, allow d p →∀m, ¬user d p m) ∧
(∀d p m, user d p m →¬allow d p ∧∀m′, user d p m′ →m = m′)
(8)
and such that allow d p holds when domain d unconditionally grants the per-
mission p and user d p m holds when domain d grants permission p with explicit
user authorization and maximum allowable mode m (w.r.t. ≤m). The permis-
sions eﬀectively granted to a MIDlet suite are the intersection of the permissions
requested in its descriptor with the union of the permissions given uncondition-
ally by its domain and those given explicitly by the user.
3.2
Device State
To reason about the MIDP 2.0 security model most details of the device state
may be abstracted; it is suﬃcient to specify the set of installed suites, the per-
missions granted or revoked to them and the currently active suite in case there

224
S. Zanella B´eguelin, G. Betarte, and C. Luna
is one. The active suite, and the permissions granted or revoked to it for the
session are grouped into a record structure
SessionInfo
def
= { id
: SuiteID,
granted, revoked : Permission →Prop }
(9)
The abstract device state is described as a record of type
State
def
= { suite
: Suite →Prop,
session
: option SessionInfo,
granted, revoked : SuiteID →Permission →Prop }
(10)
where suite is the characteristic predicate of the set of installed suites.
Example 1. Consider a MIDlet that periodically connects to a webmail service
using HTTPS when possible or HTTP otherwise, and alerts the user when-
ever they have new mail. The suite containing this MIDlet should declare in
its descriptor as required permissions p push, for accessing the PushRegistry
(for timer-based activation), and p http, for using the HTTP protocol API. It
should also declare as optional the permission p https for accessing the HTTPS
protocol API. Suppose that upon installation, the suite (whose identiﬁer is id)
is recognized as trusted and is thus bound to a protection domain dom that
allows access to the PushRegistry API unconditionally but by default requests
user authorization for opening every HTTP or HTTPS connection. Suppose also
that the domain allows the user to grant the MIDlet the permission for opening
further connections as long as the suite remains installed. Then, the security
policy satisﬁes:
allow dom p push ∧user dom p http blanket ∧user dom p https blanket (11)
If st is the state of the device, the suite is represented by some ms : Suite such
that st.suite ms and ms.id = id hold. Its descriptor ms.descriptor satisﬁes
ms.descriptor.required p push ∧ms.descriptor.required p http ∧
ms.descriptor.optional p https
(12)
The MIDlet will have unlimited access to the PushRegistry applet, but will have
to request user authorization every time it makes a new connection. The user may
chose at any time to authorize further connections by granting the corresponding
permission in blanket mode, thus avoiding being asked for authorization each
time the applet communicates with the webmail service.
⊓⊔
The remainder of this subsection enumerates the conditions that must hold
for an element s : State in order to represent a valid state for a device.
1. A MIDlet suite can be installed and bound to a protection domain only if
the set of permissions declared as required in its descriptor are a subset of
the permissions the domain oﬀers (with or without user authorization). This

A Formal Speciﬁcation of the MIDP 2.0 Security Model
225
compatibility relation between des : Descriptor and dom : Domain can be
stated formally as follows,
des ≀dom
def
= ∀p : Permission,
des.required p →allow dom p ∨∃m : Mode, user dom p m
(13)
Every installed suite must be compatible with its associated protection do-
main,
SuiteCompatible
def
=
∀ms : Suite, s.suite ms →ms.descriptor ≀ms.domain
(14)
2. Whenever there exists a running session, the suite identiﬁer in s.session
must correspond to an installed suite,
CurrentInstalled
def
= ∀ses : SessionInfo, s.session = Some ses →
∃ms : Suite, s.suite ms ∧ms.id = ses.id
(15)
3. The set of permissions granted for the session must be a subset of the permis-
sions requested in the application descriptor of the active suite. In addition,
the associated protection domain policy must allow those permissions to be
granted at least in session mode,
V alidSessionGranted
def
= ∀ses : SessionInfo, s.session = Some ses →
∀p : Permission, ses.granted p →
∀ms : Suite, s.suite ms →ms.id = ses.id →
(ms.descriptor.required p ∨ms.descriptor.optional p) ∧
(∃m : Mode, user ms.domain p m ∧session ≤m m)
(16)
4. Every installed suite shall have a unique identiﬁer,
UniqueSuiteID
def
= ∀ms1 ms2 : Suite,
s.suite ms1 →s.suite ms2 →ms1.id = ms2.id →ms1 = ms2
(17)
5. For every installed suite with identiﬁer id, the predicate s.granted id should
be valid with respect to its descriptor and associated protection domain
(V alidGranted s). We omit the detailed formalization of this condition.
6. A granted permission shall not be revoked at the same time and viceversa
(V alidGrantedRevoked s). We omit the detailed formalization.
3.3
Events
We deﬁne a set Event for those events that are relevant to our abstraction of
the device state (Table 1). The user may be presented with the choice between
accepting or refusing an authorization request, specifying the period of time their

226
S. Zanella B´eguelin, G. Betarte, and C. Luna
Table 1. Events
Name
Description
Type
start
Start of session
SuiteID →Event
terminate End of session
Event
request
Permission request
Permission →option UserAnswer →Event
install
MIDlet suite installation SuiteID →Descriptor →Domain →Event
remove
MIDlet suite removal
SuiteID →Event
choice remains valid. The outcome of a user interaction is represented using the
type UserAnswer with constructors
ua allow, ua deny : Mode →UserAnswer
(18)
The behaviour of the events is speciﬁed by their pre- and postconditions
given by the predicates Pre and Pos respectively. Preconditions (Table 2) are
deﬁned in terms of the device state while postconditions (Table 3) are deﬁned
in terms of the before and after states and an optional response which is only
meaningful for the request event and indicates whether the requested operation
is authorized,
Pre : State →Event →Prop
Pos : State →State →option Response →Event →Prop
(19)
Table 2. Event preconditions. The precondition of the request event is omitted for
reasons of space
Pre s (start id)
def
=
s.session = None ∧∃ms : Suite, s.suite ms ∧ms.id = id
Pre s terminate
def
= s.session ̸= None
Pre s (install id des dom)
def
=
des ≀dom ∧∀ms : Suite, s.suite ms →ms.id ̸= id.
Pre s (remove id)
def
=
(∀ses : SessionInfo, s.session = Some ses →ses.id ̸= id) ∧
∃ms : Suite, s.suite ms ∧ms.id = id
Example 2. Consider an event representing a permission request for which the
user denies the authorization. Such an event can only occur when the active
suite has declared the requested permission in its descriptor and is bound to
a protection domain that speciﬁes a user interaction mode for that permission
(otherwise, the request would be immediately accepted or rejected). Further-
more, the requested permission must not have been revoked or granted for the

A Formal Speciﬁcation of the MIDP 2.0 Security Model
227
Table 3. Event postconditions. The postcondition for the request event is omitted for
reasons of space.
Pos s s′ r (start id)
def
=
r = None ∧s ≡session s′ ∧∃ses′, s′.session = Some ses′ ∧ses′.id = id ∧
∀p : Permission, ¬ses′.granted p ∧¬ses′.revoked p
Pos s s′ r terminate
def
= r = None ∧s ≡session s′ ∧s′.session = None
Pos s s′ r (install id des dom)
def
=
r = None ∧(∀ms : Suite, s.suite ms →s′.suite ms) ∧
(∀ms : Suite, s′.suite ms →s.suite ms ∨ms = ⟨id, dom, des⟩) ∧
s′.suite ⟨id, dom, des⟩∧s′.session = s.session ∧
(∀p : Permission, ¬s′.granted id p ∧¬s′.revoked id p) ∧
(∀id1 : SuiteID, id1 ̸= id →
s′.granted id1 = s.granted id1 ∧s′.revoked id1 = s.revoked id1)
Pos s s′ r (remove id)
def
= r = None ∧s ≡suite s′ ∧
(∀ms : Suite, s.suite ms →ms.id ̸= id →s′.suite ms) ∧
(∀ms : Suite, s′.suite ms →s.suite ms ∧ms.id ̸= id)
rest of the session or the rest of the suite’s life,
Pre s (request p (Some (ua deny m)))
def
=
∃ses : SessionInfo, s.session = Some ses ∧
∀ms : Suite, s.suite ms →ms.id = ses.id →
(ms.descriptor.required p ∨ms.descriptor.optional p) ∧
(∃m1 : Mode, user ms.domain p m1) ∧
¬ses.granted p ∧¬ses.revoked p ∧
¬s.granted ses.id p ∧¬s.revoked ses.id p
(20)
When m = session, the user revokes the permission for the whole session, there-
fore, the response denies the permission and the state is updated accordingly,
Pos s s′ r (request p (Some (ua deny session)))
def
=
r = Some denied ∧s ≡session s′ ∧
∀ses : SessionInfo, s.session = Some ses →
∃ses′ : SessionInfo,
s′.session = Some ses′ ∧ses′ ≡revoked ses ∧ses′.revoked p ∧
(∀q : Permission, q ̸= p →ses′.revoked q = ses.revoked q)
(21)
3.4
One-Step Execution
The behavioural speciﬁcation of the execution of an event is given by the →
relation with the following introduction rules:
¬Pre s e
s 
e/None
−−−−−→s
npre
Pre s e
Pos s s′ r e
s 
e/r
−−→s′
pre
(22)

228
S. Zanella B´eguelin, G. Betarte, and C. Luna
Whenever an event occurs for which the precondition does not hold, the state
must remain unchanged. Otherwise, the state may change in such a way that
the event postcondition is established. The notation s 
e/r
−−→s′ may be read as
“the execution of the event e in state s results in a new state s′ and produces a
response r”.
3.5
Sessions
A session is the period of time spanning from a successful start event to a
terminate event, in which a single suite remains active. A session for a suite
with identiﬁer id (Fig. 1) is determined by an initial state s0 and a sequence of
steps ⟨ei, si, ri⟩(i = 1, . . . , n) such that the following conditions hold,
– e1 = start id ;
– Pre s0 e1 ;
– ∀i ∈{2, . . ., n −1}, ei ̸= terminate ;
– en = terminate ;
– ∀i ∈{1, . . ., n}, si−1 
ei/ri
−−−→si .
s0 
start id/r1
−−−−−−−→s1 
e2/r2
−−−→s2 
e3/r3
−−−→· · · 
en−1/rn−1
−−−−−−−→sn−1 
terminate/rn
−−−−−−−−−→sn
Fig. 1. A session for a suite with identiﬁer id
To deﬁne the session concept we introduce before the concept of partial ses-
sion. A partial session is a session for which the terminate event has not yet
been elicited; it is deﬁned inductively by the following rules,
Pre s0 (start id)
s0 
start id/r1
−−−−−−−→s1
PSession s0 ([ ] ⌢⟨start id, s1, r1⟩)
psession start
(23)
PSession s0 (ss ⌢last)
e ̸= terminate
last.s 
e/r
−−→s′
PSession s0 (ss ⌢last ⌢⟨e, s′, r⟩)
psession app
(24)
Now, sessions can be easily deﬁned as follows,
PSession s0 (ss ⌢last)
last.s 
terminate/r
−−−−−−−−→s′
Session s0 (ss ⌢last ⌢⟨terminate, s′, r⟩)
session terminate
(25)
4
Veriﬁcation of Security Properties
This section is devoted to establishing relevant security properties of the model.
Due to space constraints, proofs are merely outlined; however, all proofs have
been formalized in Coq and are available as part of the full speciﬁcation.

A Formal Speciﬁcation of the MIDP 2.0 Security Model
229
4.1
An Invariant of One-Step Execution
We call one-step invariant a property that remains true after the execution of
every event if it is true before. We show next that the validity of the device state,
as deﬁned in Section 3.2, is a one-step invariant of our speciﬁcation.
Theorem 1. Let V alid be a predicate over State deﬁned as the conjunction of
the validity conditions in Sect. 3.2. For any s s′ : State, r : option Response
and e : Event, if V alid s and s 
e/r
−−→s′ hold, then V alid s′ also holds.
Proof. By case analysis on s
e/r
−−→s′. When Pre s e does not hold, s = s′ and s′ is
valid because s is valid. Otherwise, Pos s s′ r e must hold and we proceed by case
analysis on e. We will only show the case request p (Some (ua deny session)),
obtained after further case analysis on a when e = request p (Some a).
The postcondition (21) entails that s ≡session s′, that the session remains
active, and that ses′ ≡revoked ses. Therefore, the set of installed suites re-
mains unchanged (s′.suite = s.suite), the set of permissions granted for the
session does not change (ses′.granted = ses.granted) and neither does the set
of permissions granted or revoked in blanket mode (s′.granted = s.granted,
s′.revoked = s.revoked). From these equalities, every validity condition of the
state s′ except V alidGrantedRevoked s′ follows immediately from the validity
of s. We next prove V alidGrantedRevoked s′.
We know from the postcondition of the event that
∀q, q ̸= p →ses′.revoked q = ses.revoked q
(26)
Let q be any permission. If q ̸= p, then from (26) follows ses′.revoked q =
ses.revoked q and because q was not granted and revoked simultaneously before
the event, neither it is afterwards. If q = p, then we know from the precondi-
tion (20) that p were not granted before and thus it is not granted afterwards.
This proves V alidGrantedRevoked s′ and together with the previous results,
V alid s′.
⊓⊔
4.2
Session Invariants
We call session invariant a property of a step that holds for the rest of a session
once it is established in any step. Let P be a predicate over T , we deﬁne all P
as an inductive predicate over seq T by the following rules:
all P [ ] all nil
all P ss
P s
all P (ss ⌢s) all snoc
(27)
Theorem 2. Let s0 be a valid state and ss a partial session starting from s0,
then every state in ss is valid,
all (λ step . V alid step.s) ss
(28)

230
S. Zanella B´eguelin, G. Betarte, and C. Luna
Proof. By induction on the structure of PSession s0 ss.
– When constructed using psession start, ss has the form [ ]⌢⟨start id, s1, r1⟩
and s0 
start id/r1
−−−−−−−→s1 holds. We must prove
all (λ step . V alid step.s) ([ ] ⌢⟨start id, s1, r1⟩)
(29)
By applying all app and then all nil the goal is simpliﬁed to V alid s1 and
is proved from s0 
start id/r1
−−−−−−−→s1 and V alid s0 by applying Theorem 1.
– When it is constructed using psession app, ss has the form ss1 ⌢last ⌢
⟨e, s′, r⟩and last.s 
e/r
−−→s′ holds. The induction hypothesis is
all (λ step . V alid step.s) (ss1 ⌢last)
(30)
and we must prove all (λ step . V alid step.s) (ss1 ⌢last ⌢⟨e, s′, r⟩). By
applying all app and then (30) the goal is simpliﬁed to V alid s′. From (30)
we know that last.s is a valid state. The goal is proved from last.s 
e/r
−−→s′
and V alid last.s by applying Theorem 1.
⊓⊔
The above theorem may be easily extended from partial sessions to sessions using
Theorem 1 one more time. State validity is just a particular property that is true
for a partial session once it is established, the result can be generalized for other
properties as shown in the following lemma.
Lemma 1. For any property P of a step satisfying
∀(s s′ : State)(r r′ : option Response)(e e′ : Event),
e′ ̸= terminate →s 
e′/r′
−−−→s′ →P ⟨e, s, r⟩→P ⟨e′, s′, r′⟩,
(31)
if PSession s0 (ss ⌢step ⊕ss1) and P step, then all P ss1 holds.
Perhaps a more interesting property is a guarantee of the proper enforcement of
revocation. We prove that once a permission is revoked by the user for the rest
of a session, any further request for the same permission in the same session is
refused.
Lemma 2. The following property satisﬁes (31),
(λ step . ∃ses, step.s.session = Some ses ∧ses.revoked p)
(32)
Theorem 3. For any permission p, if PSession s0 (ss ⌢step ⌢step1 ⊕ss1),
step1.e = request p (Some (ua deny session)) and Pre step.s step1.e, then
all (λ step . ∀o, step.e = request p o →step.r ̸= Some allowed) ss1
(33)
Proof. Since Pos step.s step1.s step1.r step1.e must hold, p is revoked for the
session in step1.s. From Lemmas 1 and 2, p remains revoked for the rest of the
session. Let e = request p o be an event in a step step2 in ss1. We know that
p is revoked for the session in the state before step2.s. If the precondition for e
does not hold in the state before1, then step2.r = None. Otherwise, e must be
request p None and its postcondition entails step2.r = Some denied.
⊓⊔
1 Actually, it holds only when o = None.

A Formal Speciﬁcation of the MIDP 2.0 Security Model
231
5
Reﬁnement
In the formalization described in the previous sections we have speciﬁed the
behaviour of events implicitly as a binary relation on states instead of explic-
itly as a state transformer. Moreover, the described formalization is higher-order
because, for instance, predicates are used to represent part of the device state
and the transition semantics of events is given as a relation on states. The most
evident consequence of this choice is that the resulting speciﬁcation is not ex-
ecutable. What is more, the program extraction mechanism provided by Coq
to extract programs from speciﬁcations cannot be used in this case. However,
had we constructed a more concrete speciﬁcation at ﬁrst, we would have had to
take arbitrary design decisions from the beginning, unnecessarily restricting the
allowable implementations and complicating the veriﬁcation of properties of the
security model.
We will show in the rest of this section that it is feasible to obtain an exe-
cutable speciﬁcation from our abstract speciﬁcation. The methodology we pro-
pose produces also a proof that the former is a reﬁnement of the latter, thus
guaranteeing soundness of the entire process. The methodology is inspired by
the work of Spivey [9] on operation and data reﬁnement, and the more com-
prehensive works of Back and von Wright [10] and Morgan [11] on reﬁnement
calculus.
5.1
Executable Speciﬁcation
In order to construct an executable speciﬁcation it is ﬁrst necessary to choose a
concrete representation for every object in the original speciﬁcation not directly
implementable in a functional language. In particular, the transition relation
that deﬁnes the behaviour of events implicitly by means of their pre- and post-
conditions must be reﬁned to a function that deterministically computes the
outcome of an event. At this point, it is unavoidable to take some arbitrary deci-
sions about the exact representation to use. For example, a decidable predicate
P on A might be represented as a function from A to a type isomorphic to bool,
as an exhaustive list of the elements of A that satisﬁes the predicate (when P
has ﬁnite support), or in some other equally expressive way. For every type T in
the abstract speciﬁcation, we will denote its concrete model as T. Let a : A and
a : A, we will indicate that a is a reﬁnement of a as a ⊑a.
In our case, every predicate to be reﬁned is decidable and is satisﬁed only by
a ﬁnite subset of elements in its domain (they are all characteristic predicates
of ﬁnite sets). Let P be one of such predicates on a set A and let l be a list of
elements of A, we will say that l reﬁnes P whenever
(∀a, P a →∃a, a ∈l ∧a ⊑a) ∧
(∀a, a ∈l →∃a, P a ∧a ⊑a)
(34)
where x ∈l means that there exists at least one occurrence of x in l. When A
and A coincide, ⊑is the equality relation on A, and the condition (34) simpliﬁes

232
S. Zanella B´eguelin, G. Betarte, and C. Luna
to ∀a, P a ↔a ∈l. Let a : A and a : A be such that a ⊑a, we deﬁne
(None : option A) ⊑(None : option A)
Some a
⊑Some a
(35)
The above concrete representations can be used to obtain a concrete model
for the device state and the security-related events:
State := { suite
: list Suite,
session
: option SessionInfo,
granted, revoked : SuiteID →list Permission }
(36)
start
: SuiteID →Event
terminate : Event
request
: Permission →option UserAnswer →Event
install
: SuiteID →Descriptor →Domain →Event
remove
: SuiteID →Event
(37)
The reﬁnement relation ⊑can be naturally extended to states, events and the
rest of the types used in the formalization.
5.2
Soundness
Having chosen a concrete representation for the objects in the speciﬁcation,
everything is set for specifying the behaviour of events as a function
interp : State →Event →State × (option Response)
(38)
The soundness of the interp function w.r.t. the transition relation →is given
by the following simulation condition, illustrated in Fig. 2.
∀(s : State)

s : State

(e : Event)

e : Event

(r : option Response),
s ⊑s →e ⊑e →
let (s′, r) := interp s e in ∃s′ : State, s′ ⊑s′ ∧s 
e/r
−−→s′
(39)
It can be shown that the reﬁnement relation ⊑on states satisﬁes
∀s : State, ∃s : State, s ⊑s
(40)
Thus, the existential quantiﬁer in (39) may be replaced by a universal quantiﬁer
to obtain the stronger (but sometimes easier to prove) condition:
∀(s s′ : State)

s : State

(e : Event)

e : Event

(r : option Response),
s ⊑s →e ⊑e →
let (s′, r) := interp s e in s′ ⊑s′ →s 
e/r
−−→s′
(41)
With a function interp satisfying either (39) or (41) and a concrete initial state s0
that reﬁnes an initial abstract state s0, the Coq program extraction mechanism
can be used to produce an executable prototype of the MIDP 2.0 security model
in a functional language such as OCaml, Haskell or Scheme.

A Formal Speciﬁcation of the MIDP 2.0 Security Model
233
s
s′
s
s′
interp
→
⊑
⊑
State
State
Fig. 2. Simulation relation between interp and the relation →. Given states s, s and
events e, e such that s ⊑s and e ⊑e, for every state s′ and response r computed by
interp there must exist a corresponding abstract state s′ reﬁned by s′ reachable from
s by the →relation with the same response.
6
Conclusions
The informal speciﬁcation in [2] puts forward an application security model
that any MIDP 2.0 enabled device must satisfy. Although analyses of particular
implementations have been proved useful for discovering vulnerabilities, so far
the problem of the veriﬁcation of the security model has not been addressed. We
believe our contribution constitutes an excellent starting point for a thorough
veriﬁcation of the model, which would give a higher assurance level than the
techniques applied so far.
We have produced, to the best of our knowledge, an unprecedented veriﬁ-
able formalization of the MIDP 2.0 security model and have also constructed
the proofs of several important properties that should be satisﬁed by any im-
plementation that fulﬁls its speciﬁcation. It is unclear from the MIDP 2.0 spec-
iﬁcation exactly how other mechanisms interact with the security model. Our
formalization is precise and detailed enough to study, for instance, the interfer-
ence between the security rules that control access to the device resources and
mechanisms such as application installation. These issues have not been treated
anywhere else.
We have also proposed a reﬁnement methodology that might be used to obtain
a sound executable prototype of the security model. Although we do not show it
here, we have followed this methodology with a restricted set of events. Judging
by the success of this experience, we strongly believe it is feasible to obtain a
prototype for the whole set.
The speciﬁcation has been completed in 4 man-months and comprises around
2200 lines, about 1000 of which are dedicated to proofs. In its construction, two
simplifying assumptions have been made:
1. the security policy is static;
2. up to one suite may be active at a time.

234
S. Zanella B´eguelin, G. Betarte, and C. Luna
Actually, most existing implementations (if not all) enforce these assumptions.
However, the MIDP 2.0 speciﬁcation does not and therefore, it would be interest-
ing to explore the consequences of relaxing them by extending the formalization.
An orthogonal direction is to divert from the MIDP 2.0 speciﬁcation, enriching
the model to allow more expressive policies. We imagine two possibilities:
1. abandon the unstructured model of permissions in favour of a hierarchical
one;
2. generalize user interaction modes.
The former would allow group of permissions to be revoked or granted accord-
ing to a tree structure, perhaps exploiting the already hierarchical naming of
permissions in MIDP. The latter would allow richer policies such as granting
a permission for a given number of uses or sessions. In a recent, as yet unpub-
lished work, Besson, Dufay and Jensen [12] describe a generalized model of access
control for mobile devices that follows these directions to some extent.
References
1. JSR 37 Expert Group: Mobile Information Device Proﬁle for Java 2 Micro Edition.
Version 1.0. Sun Microsystems, Inc. (2000)
2. JSR 118 Expert Group: Mobile Information Device Proﬁle for Java 2 Micro Edition.
Version 2.0. Sun Microsystems, Inc. and Motorola, Inc. (2002)
3. Kolsi, O., Virtanen, T.: MIDP 2.0 security enhancements. In: Proceedings of the
37th Annual Hawaii International Conference on System Sciences, Washington,
DC, USA. IEEE Computer Society, Los Alamitos (2004) 90287.3
4. Debbabi, M., Saleh, M., Talhi, C., Zhioua, S.: Security analysis of wireless Java. In:
Proceedings of the 3rd Annual Conference on Privacy, Security and Trust (2005)
5. The Coq Development Team: The Coq Proof Assistant Reference Manual – Version
V8.0. (2004)
6. Bertot, Y., Cast´eran, P.: Interactive Theorem Proving and Program Development.
Coq’Art: The Calculus of Inductive Constructions. In: Texts in Theoretical Com-
puter Science, Springer, Heidelberg (2004)
7. Zanella B´eguelin, S.: Especiﬁcaci´on formal del modelo de seguridad de MIDP 2.0
en el C´alculo de Construcciones Inductivas. Master’s thesis, Universidad Nacional
de Rosario (2006)
8. Zanella B´eguelin, S., Betarte, G., Luna, C.: A formal speciﬁcation of the MIDP
2.0 security model. Technical Report 06-09, Instituto de Computaci´on, Facultad
de Ingenier´ıa, Universidad de la Rep´ublica, Uruguay (2006)
9. Spivey, J.M.: The Z Notation: A Reference Manual. In: International Series in
Computer Science, Prentice Hall, Hemel Hempstead, Hertfordshire, UK (1989)
10. Back, R.J., von Wright, J.: Reﬁnement Calculus: A Systematic Introduction. In:
Graduate Texts in Computer Science, Springer, Heidelberg (1998)
11. Morgan, C.: Programming from speciﬁcations. Prentice-Hall, Inc, Upper Saddle
River, NJ, USA (1990)
12. Besson, F., Dufay, G., Jensen, T.: A formal model of access control for mobile
interactive devices. In: Gollmann, D., Meier, J., Sabelfeld, A. (eds.) ESORICS
2006. LNCS, vol. 4189, Springer, Heidelberg (2006) (to appear)

A Comparison of Semantic Models for
Noninterference⋆
Ron van der Meyden and Chenyi Zhang
University of New South Wales and
National ICT Australia, Sydney, Australia
Abstract. The literature on deﬁnitions of security based on causality-
like notions such as noninterference has used several distinct semantic
models for systems. Early work was based on state-machine and trace-
set deﬁnitions; more recent work has dealt with deﬁnitions of security
in two distinct process algebraic settings. Comparisons between the def-
initions has been carried out mainly within semantic frameworks. This
paper studies the relationship between semantic frameworks, by deﬁn-
ing mappings between a number of semantic models and studying the
relationship between notions of noninterference under these mappings.
1
Introduction
“Noninterference” is a term loosely applied in the literature to a class of formal
security properties motivated from considerations of information ﬂow and causal-
ity. Since it was invented in [1], several distinct schools have produced a variety of
generalizations of the original notion, each based on their own approach to mod-
elling systems. Existing deﬁnitions of noninterference can be roughly classiﬁed by
whether they are framed in the semantic context of state-based automaton mod-
els [1,2,3,4,5,6], trace-based models [7,8,9,10], or process algebraic based models
(further divisible into CSP [11] and CCS [12] based variants) [13,14,15].
There have been a number of survey works and studies of the relationships
between these deﬁnitions in the individual schools [15,13] but, on the whole, com-
parisons have been carried out within rather than across semantic frameworks.
So far the only cross-domain contribution to our knowledge is [16], in which
language-based security has been connected with a particular process algebraic
property by a one-way translation. In this paper, we attempt to bridge some of
the gaps by considering the relationships between the various classical semantic
models and some of the proposed notions of noninterference. We consider three
types of models: two automaton-like models (introduced in Sect. 2) and a process
algebraic framework (discussed in Sect. 5). The semantic intuitions underlying
these frameworks are somewhat diﬀerent. The automaton models have notions
of “action” and of “observation”, the latter being a function of state in one
case and associated to actions in the other. The process algebraic framework
⋆National ICT Australia is funded through the Australian Government’s Backing
Australia’s Ability initiative, in part through the Australian Research Council.
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 235–249, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

236
R. van der Meyden and C. Zhang
is seemingly more general, but diverges from the intuitions of the automaton
models in that it treats both actions (outputs) and observations (inputs) uni-
formly as “active”. However, the state-observed automaton models rather than
the process algebraic models are what is used in current work in operating sys-
tems veriﬁcation [17,18] — the application originally motivating the literature
on noninterference — so it is desirable to precisely understand the relationship
between these frameworks. We address this question by deﬁning formal map-
pings (see Sects. 4 and 5) between the semantic frameworks. We study whether
a variety of deﬁnitions of noninterference (introduced for the automaton models
in Sect. 3 ) in the diﬀerent frameworks correspond under these mappings.
2
State-Based Models
The original system models used in the literature on noninterference modelled
systems as a type of deterministic or nondeterministic automaton, with outputs
for each of the security domains. Similarly to the Moore-Mealy distinction for
ﬁnite state automata, we ﬁnd two types of models, depending on whether outputs
are associated to states [5,6] or actions [1,4]. The original deﬁnitions assumed
deterministic systems, but the focus of subsequent work has been on how to
generalize the deﬁnitions to nondeterministic systems. In general, these systems
are input-enabled, in the sense that any action can be taken at any time.
A nondeterministic action-observed state machine is a tuple of the form M =
⟨S, s0, next, dom, A⟩, where S is a set of states, s0 ∈S is the initial state, A is a
set of actions, dom : A →D associates with each action a security domain from
the set of security domains D, and next : S × A →P(O × S) is a transition
function. Here O is a set of observations that can be made when performing an
action. Given a state s ∈S, and an action a ∈A, the set next(s, a) is required to
be non-empty. A tuple (o, t) ∈next(s, a) intuitively represents that on action a it
is possible to make a transition from state s to state t and produce output o. Such
a machine is deterministic if next(s, a) is a singleton for all states s and actions a.
In this case, the function next may be replaced by two functions step : S×A →S
and out : S×A →O such that next(s, a) = {(out(s, a), step(s, a))} to obtain the
state machine deﬁnition one ﬁnds, e.g., in [4]. A run of an action-observed system
is a sequence r = s0(a1, o1)s1(a2, o2)s2 . . . (an, on)sn ∈S((A × O)S)∗such that
for all 1 ≤i ≤n, (oi, si) ∈next(si−1, ai). A state s ∈S is said to be reachable
if it occurs in some run. We write
 na for the set of all nondeterministic action-
observed state machines and
 a for the set of deterministic action-observed state
machines, where, in both cases, all states are reachable1.
A nondeterministic state-observed state machine is a tuple of the form M =
⟨S, s0, next, obs, dom, A⟩where S is a set of states; s0 ∈S is the initial state; the
function next : S × A →P(S) \ {∅} is a transition function, such that next(s, a)
1 This restriction is of signiﬁcance because the deﬁnitions and results below that con-
cern unfolding relations are sensitive to unreachable states. A system may always
be restricted to its reachable component, and this operation should, intuitively, not
have an impact on its security. Thus this restriction is without loss of generality.

A Comparison of Semantic Models for Noninterference
237
deﬁnes the set of states to which it is possible to make a transition when action
a ∈A is performed at a state s ∈S; the function dom : A →D associates a
security domain with each action, and the function obs : S × D →O describes
the observation made in each state by each security domain. For readability,
we ‘curry’ the function obs by writing obsu(s) for obs(s, u) for u ∈D and
s ∈S. Such a state machine is deterministic if next(s, a) is a singleton for all
states s and actions a. In this case we may deﬁne a function step : S × A →S
by next(s, a) = {step(s, a)}. A run of a state-observed system is a sequence
r = s0a1s1a2s2 . . . ansn ∈S(AS)∗such that for all 1 ≤i ≤n, si ∈next(si−1, ai).
(Here we omit representation of the observations since these may be recovered
using the function obs.) A state s ∈S is said to be reachable if it occurs in some
run. We write
 ns for the set of all nondeterministic state-observed machines,
and
 s for the set of all deterministic state-observed machines where, in both
cases, all states are reachable.
The most signiﬁcant apparent diﬀerence between state and action observed ma-
chines is that, in the former, all agents make an observation when an action is per-
formed, whereas in the latter, only the agent performing the action does so. Since
the execution model is asynchronous, this means that whereas in action observed
systems, other agents would, unless they themselves act, have no knowledge that
any agent has performed an action, they may come to have this information in
state observed systems even without acting. However, such a situation would of-
ten be a reason for the system to be declared insecure. The action-observed setting
somewhat resembles the process algebraic setting of [19] where agents have to per-
form actions to synchronise with the system to achieve the eﬀect of ‘observation’,
but diﬀers from it in that it bundles actions together with observations whereas
[19] has separate notions of ‘input’ and ‘output’ actions.
3
Security Properties on State-Based Models
We now recall from the literature a number of security properties in the two types
of state-based systems. We study the relationships between these properties in
Sect. 4.
Historically, one of the ﬁrst information ﬂow properties was (transitive) non-
interference [1,20], deﬁned with respect to deterministic machines. We base our
discussion on the presentation of Rushby [4], which has been followed in many
other works. Rushby deﬁnes both state-observed and action-observed systems,
but treats them independently and does not consider any direct relations be-
tween the two. The classical deﬁnitions were cast in terms of security policies
describing permitted information ﬂows between an arbitrary collection of agents.
Much of the subsequent literature restricts attention to the policy L ≤H with
two agents High (H) and Low (L), with information permitted to ﬂow from Low
to High but not from High to Low. For uniformity, we also make this restriction
here, and let AH = {a ∈A | dom(a) = H} and AL = {a ∈A | dom(a) = L}.
As noted above, in both state-observed and action-observed deterministic sys-
tems, we have a function step : S × A →S to represent the deterministic
state evolution as a result of actions. To represent the result of executing a

238
R. van der Meyden and C. Zhang
sequence of actions, deﬁne the operation ◦: S × A∗→S, by s ◦ϵ = s and
s ◦(α · a) = step(s ◦α, a) for s ∈S, α ∈A∗and a ∈A.
With respect to the simple policy L ≤H, the deﬁnition of noninterference
can be described in terms of the operation purgeL : A∗→A∗on sequences of
actions that restricts the sequence to the subsequence of actions of L. Intuitively,
the purged High actions are not allowed to lead to any eﬀects observable to L.
This is formalised as follows in the deﬁnitions of noninterference following [4],
one for each type of system.
Deﬁnition 1
1. A system in
 na satisﬁes noninterference if it is deterministic and for all
α ∈A∗and a ∈AL, we have out(s0 ◦α, a) = out(s0 ◦purgeL(α), a). We
write NIa for the set of such systems.
2. A system in
 ns satisﬁes noninterference if it is deterministic and for all
α ∈A∗, we have obsL(s0 ◦α) = obsL(s0 ◦purgeL(α)). We write NIs for the
set of such systems.
The deﬁnitions of noninterference in the two types of system are very similar.
We show below that they can be seen to be equivalent in a precise sense.
One way of understanding the statement that H does not interfere with L in a
deterministic system is as stating that every sequence of H actions is compatible
with the actions and observations of L. This leads to the proposal to take a
similar notion as the formulation of noninterference in nondeterministic systems:
an approach known as nondeducibility [21]. Nondeducibility is deﬁned in a quite
general way, in terms of a pair of views of runs. We focus here on a commonly
used special case: Low’s nondeducibility of High’s actions.
We take an agent u’s view view u(r) of a run r to be the maximal state of
information that it can have in an asynchronous system: its sequence of actions
and observations reduced modulo stuttering. We begin by extending the agent’s
observations to runs. In action-observed systems we deﬁne the extended obser-
vation function Obsa
u : S((A × O)S)∗→(AO)∗for u ∈D by Obsa
u(s) = ϵ
and
Obsa
u(r · (a, o) · s′) =
Obsa
u(r) · a · o if dom(a) = u
Obsa
u(r)
otherwise.
Here, taking the stance that an agent is aware of each action that it performs
(so that if it performs an action twice, obtaining the same output, it knows
that it has performed the action twice) we do not need to apply a stuttering
reduction, and take view u(r) = Obsa
u(r). In state-observed systems, the agent
makes an observation at each state, and we deﬁne Obss
u : S(AS)∗→O+(AO+)∗
by Obss
u(s) = obsu(s), and
Obss
u(δ · a · s) =
Obss
u(δ) · a · obsu(s) if dom(a) = u
Obss
u(δ) · obsu(s)
otherwise.
Here the agent may make the same observation several times in a row, with-
out an intervening action by that agent. This indicates that another agent has

A Comparison of Semantic Models for Noninterference
239
acted. To eliminate this timing-based reasoning, in order to make the deﬁni-
tion compatible with the assumption of asynchrony, we may take the view to
be view u(r) = Cond(Obss
u(r)) where Cond is the function on sequences that
removes consecutive repetitions.
To state the deﬁnition of nondeducibility, we also require a function to extract
the sequence of actions performed by an agent. We write Actu(r) for the sequence
of actions performed by agent u in run r, and Act(r) the sequence of all actions
in r. We say that a sequence β is a possible view for agent u in a system M if
there exists a run r of M such that view u(r) = β.
Deﬁnition 2. A system M satisﬁes Nondeducibility on Inputs if for every α ∈
A∗
H, and every possible L view β in M , there exists a run r of M with ActH(r) =
α and view L(r) = β. Write NDIs and NDIa for the set of systems in
 ns and
 na (respectively) satisfying nondeducibility on inputs.
Wittbold and Johnson [2] argued that systems classiﬁed as secure by nonde-
ducibility on inputs may nevertheless permit ﬂows of information ﬂow from High
to Low. They present a system in which by selecting its actions according to a
particular strategy, High may directly control Low’s observations. They propose
an alternate deﬁnition they call “nondeducibility on strategies” which behaves
more satisfactorily on the example.
The framework in which they work is synchronous state machines with si-
multaneous actions. Nevertheless, it is possible to formulate a similar deﬁnition
in the asynchronous models deﬁned above. In state-observed systems, we deﬁne
an asynchronous High strategy to be a function π : O+(AHO+)∗→AH ∪{ϵ}
mapping each possible high view to a choice of High action or the “noop” ac-
tion ϵ. We say that a run s0a1s1 . . . ansn is consistent with π if dom(ai) = H
implies ai = π(viewH(s0a1s1 . . . ai−1si−1)), for each i = 1 . . . n. Similarly, in
action-observed systems, we deﬁne an asynchronous High strategy to be a func-
tion π : (AHO)∗→AH ∪{ϵ}. A run s0(a1, o1)s1 . . . (an, on)sn is consistent with
π if dom(ai) = H implies ai = π(viewH(s0(a1, o1)s1 . . . (ai−1, oi−1)si−1)). Given
a system M ∈
 na or M ∈
 ns and a strategy π of the appropriate type, deﬁne
AviewL(M , π) = {viewL(r) | r is a run of M consistent with π}.
Deﬁnition 3. M is secure wrt Nondeducibility on Strategies (written M ∈
NDSa or M ∈NDSs, according as M ∈
 na or M ∈
 ns) if for all High
strategies π, π′, AviewL(M , π) = AviewL(M , π′).
It is evident that every possible low level observation arises from some high level
action sequence, and hence from some high level strategy (perform that sequence
of actions). Hence if every high level strategy is compatible with every low level
observation, then every high level input sequence is compatible with every low
level observation. That is, nondeducibility on strategies implies nondeducibility
on inputs. In fact the converse holds as well.
Theorem 1. NDSa = NDIa and NDSs = NDIs.

240
R. van der Meyden and C. Zhang
A very similar result has previously been noted in a process algebraic setting by
Focardi and Gorrieri2. Theorem. 1 could in fact be obtained as a consequence
of their results and translation results from state and action observed systems
that we present in Sect. 5. Note that, by Wittbold and Johnson’s example [2],
the equivalence does not hold in synchronous systems.
A number of the deﬁnitions in the literature on noninterference for nondeter-
ministic systems are closely related to the following notion, that was originally
motivated as a way of facilitating proofs of noninterference for deterministic
systems.
Deﬁnition 4. An unwinding relation for a system M ∈
 a is an equivalence
relation ∼L on the states of M satisfying the following conditions, for all states
s, t and actions a:3
– Output Consistencya: if a ∈AL and s ∼L t then out(s, a) = out(t, a);
– Locally Respects: if a ∈AH then s ∼L step(s, a);
– Step Consistency: if a ∈AL and s ∼L t then step(s, a) ∼L step(t, a).
An unwinding relation for a system M ∈
 s is an equivalence relation satis-
fying Locally Respects, Step Consistency, and the following variant of Output
Consistency.
– Output Consistencys: if s ∼L t then obsL(s) = obsL(t).
The relationship between unwinding conditions and noninterference is given by
the following classical results:
Theorem 2. [20,4]
1. If there exists an unwinding relation for M ∈
 na (M ∈
 ns), then M ∈NIa
(M ∈NIs).
2. If M ∈NIa (M ∈NIs) then there exists an unwinding relation for M .
The following is a natural generalization of Deﬁnition 4 to nondeterministic
systems. (Note that Output Consistency has been incorporated into SC in the
unwinding relation for
 na.)
Deﬁnition 5. An unwinding relation for a system M ∈
 na is an equivalence
relation ∼L on the states of M such that for all states s, s′, t, actions a, and
outputs o,
– LRa: if a ∈AH and (o, t) ∈next(s, a) then s ∼L t,
2 See [19] on p.20-21: Theorem 3.27 states NDCIT = NNIIT, and Corollary 3.29
states NNIIT = TNDI ∩IT. The deﬁnition of TNDI resembles that of NDI, and
the deﬁnition of NDCIT resembles that of NDS.
3 We present a slight modiﬁcation of the usual deﬁnition, which would have an equiva-
lence relation ∼u for each agent u, satisfying a similar set of conditions for each u. For
the policy L ≤H we can take ∼H to be the universal relation, which automatically
satisﬁes the necessary conditions.

A Comparison of Semantic Models for Noninterference
241
– SCa: if a ∈AL and s ∼L s′ and (o, t) ∈next(s, a), then there exists a state
t′ such that (o, t′) ∈next(s′, a) and t ∼L t′.
An unwinding relation for a system M ∈
 ns is an equivalence relation satisfying
– OCs: if s ∼L t then obsL(s) = obsL(t).
– LRs: if a ∈AH and t ∈next(s, a) then s ∼L t,
– SCs: if a ∈AL and s ∼L s′ and t ∈next(s, a), then there exists t′ ∈
next(s′, a) such that t ∼L t′.
Several deﬁnitions of noninterference can be expressed in terms of this gener-
alized notion of unwinding. The following is essentially from [5], and a similar
deﬁnition is given in [6].
Deﬁnition 6. M ∈
 ns satisﬁes Behavioral Nondeterministic Security (M ∈
BNSs) if the relation ∼L on the states of M deﬁned by s ∼L t if obsL(s) =
obsL(t) is an unwinding relation.
Intuitively, this deﬁnition says that L’s future observations depend only on L’s
current observation and L’s future actions. This is particularly appropriate when
we interpret L’s observation as L’s complete state, and wish to express that H
is unable to interfere with this state. A related intuition in action observed
systems is that L’s future observations should depend only on L’s most recent
observation. The literature does not appear to contain any such deﬁnition for
action observed systems, perhaps because states do not necessarily encode the
most recent observation. However, by means of a transformation of the system
we may obtain a behaviourally equivalent4 system in which states do encode the
information required.
Deﬁnition 7. Let UF :
 na →
 na be the unfolding function such that for each
M = ⟨S, s0, next, A, dom⟩the system UF(M ) is the restriction of the system
⟨S′, s′
0, next′, A, dom⟩to its set of reachable states, where
– S′ = S × (D →O ∪{ε0});
– s′
0 = (s0, f0) where f0 is the function with f0(u) = ε0 for all u ∈D
– next′ : S′ × A →P(O × S′) is deﬁned as next′((s, f), a) = {(o, (s′, f[dom(a)
→o])) | (o, s′) ∈next(s, a)}.
Here ε0 is a special output denoting no ‘real’ output has been observed to this
moment. We use the notation f[u →o] for the function g that is identical to f
except that g(u) = o.
The intuition of this mapping is that introduces an extra component in the
state that remembers the most recent output for each agent. (The price is to
blow up the state space for all these observational possibilities.) This information
is extractable by the functions lastobsu : S′ →O deﬁned by lastobsu((s, f)) =
f(u) for (s, f) ∈S′ and u ∈D. We may now give a deﬁnition of Behavioural
Nondeterministic Security on action observed systems that captures the intuition
that L behaviour should depend only on L’s most recent observation.
4 This can be made precise by means of an appropriate notion of bisimulation on
action observed systems.

242
R. van der Meyden and C. Zhang
Deﬁnition 8. M ∈
 na satisﬁes Behavioral Nondeterministic Security (M ∈
BNSa) if on UF(M ) the relation ∼L deﬁned by s ∼L t if lastobsL(s) = lastobsL(t)
is an unwinding relation.
Whereas Deﬁnition 6 and 8 obtain a deﬁnition of noninterference by asserting
that a particular relation is unwinding, the following does so by requiring the
existence of an unwinding relation.
Deﬁnition 9. M ∈
 na (M ∈
 ns) satisﬁes restrictiveness, written M ∈RESa
(M ∈RESs), if there exists an unwinding relation for M .
The use of McCullough’s [7] term “restrictiveness” in this deﬁnition is non-
obvious. We justify it later when we discuss McCullough’s work in the context
of the process algebraic deﬁnitions treated in Sect. 5.
Proposition 1. The following inclusions are proper: BNSa ⊂RESa ⊂NDIa
and BNSs ⊂RESs ⊂NDIs.
4
Transformations Between State-Based Models
We now turn to our main interest in this paper, which is to study the relationship
between security properties deﬁned over diﬀerent semantic models. For this, we
require translations between the two types of models. The intuition underlying
the two models introduced above, that agents can both act on and observe their
environment is the same, and the modelling of the dynamics of actions is very
closely related. Thus, the major issue in translation is how to deal with the obser-
vations. To transform action-observed systems into state-observed systems is not
too diﬃcult: the essence has already been introduced in the unfolding construc-
tion used for BNSa (Deﬁnition 7), and we need only modify this construction
by erasing observations from the transitions.
Deﬁnition 10. Let Fas :
 na →
 ns be the translation function such that
for each M = ⟨S, s0, next, A, dom⟩, if UF(M ) = ⟨S′, s′
0, next′, A, dom⟩and
lastobsu : S′ →O are the associated mappings to observations O, we have
Fas(M ) = ⟨S′, s′
0, next′′, obs, A, dom⟩, where
next′′(s, a) = {t | (o, t) ∈next′(s, a)}
and obs(s, u) = lastobsu(s).
The range of Fas is a proper subset of
 ns, because in any Fas(M ), for any
u, v ∈D, u can not modify v’s observation before v gives any input. It is plain
that if M is deterministic, then so is Fas(M ), so also Fas :
 a →
 s.
It is also possible to translate state observed systems to action observed sys-
tems. An apparent obstacle, however, is that whereas in action-observed systems,
an action gives a new observation only to the agent performing the action, an
action in a state-observed system may also give a new observation to others.

A Comparison of Semantic Models for Noninterference
243
In the following deﬁnition, we handle the need to model these additional eﬀects
by mapping the state observations to potential observations, that would be ob-
tained if the agent were to look at the state. Thus, we deﬁne a translation that
equips each agent u with a new action looku that enables the agent to obtain its
observation from the current state, without changing that state.
Deﬁnition 11. Let Fsa :
 ns →
 na be the function such that for each M =
⟨S, s0, next, obs, A, dom⟩, we have Fsa(M ) = ⟨S, s0, next′, A′, dom′⟩, where:
1. A′ = A ∪{looku | u ∈D},
2. next′ : S × A′ →P(O × S) is deﬁned by
(a) next′(s, a) = {(o, t) | t ∈next(s, a) ∧o = obsdom(a)(t)} for a ∈A,
(b) next′(s, looku) = {(obsu(s), s)} for u ∈D,
3. dom′ = dom ∪{⟨looku, u⟩| u ∈D}.
We note that this translation produces a system with signiﬁcantly more runs and
views than the original state-observed system. This comes about because agents
may, by failing to perform a look action, omit to make an observation they would
have made in the state-observed system, or may perform a look action multiple
times in the same state. The former, in particular, means that there exist runs
in which agents have a “state of information” that would not have occurred in
the state observed system. We would not expect, therefore, that all ‘information
theoretic’ properties will be preserved by these translations. However, we may
prove that the properties discussed above correspond under the translations:
Theorem 3. Let P be any of the properties NI, NDI, NDS, BNS, RES. Then
1. for all M ∈
 na, we have M ∈Pa iﬀFas(M ) ∈Ps, and
2. for all M ∈
 ns, we have M ∈Ps iﬀFsa(M ) ∈Pa.
This result can be understood as conﬁrming the following key intuition concern-
ing security properties and observations: a system is insecure if an agent is able
to obtain prohibited information. Thus, modifying a system by permitting ad-
ditional runs in which agents make fewer observations and uninformative (e.g.
repeat) observations does not change the satisfaction of the security property.
5
Transformations to a Process Algebraic Model
Since the development of the original noninterference deﬁnitions, research has
moved to how these deﬁnitions may be generalised to systems deﬁned in process
algebra. Work in this area has been conducted within the framework of the
process algebra CSP [11], surveyed in [15], as well as the framework of a variant
called SPA of the process algebra CCS [12], surveyed in [13]. We focus here on the
latter, which it is closer to the models considered above in that it distinguishes
inputs and outputs (corresponding loosely to actions and observations). It is also
cast in terms of a common semantic underpinning for both the CSP and CCS
approaches, viz., labelled transition systems.

244
R. van der Meyden and C. Zhang
Deﬁnition 12. A labelled transition system (LTS) is a quadruple M = ⟨P, p0,
→, L⟩where L is the set of event labels, P is the set of processes (or states),
p0 is the initial process (or state), and →⊆P × (L ∪{τ}) × P is the transition
relation.
A run of M is a sequence p0
l1→p1
l2→p2 . . . pn−1
ln
→pn, and the states that occur
in a run are said to be reachable. The corresponding trace of L is the sequence
of labels l1 . . . ln with any occurrences of τ deleted. We write T (M ) for the set of
traces of M . We write
 for the set of all LT Ss in which all states are reachable.
In CCS, there is also a self-inverse bijection · : L →L and the set of events L
is partitioned into a set I of input events and the set O = {a|a ∈I} of output
events. Intuitively, the input event a may synchronise with the output event a
when composing processes. We write
IO(I) for the set of all LT S’s with inputs
I and corresponding set of outputs O = {a|a ∈I}, or simply
IO when I is clear.
In order to study security deﬁnitions, Focardi and Gorrieri [19] enhance CCS
by an orthogonal partitioning of the space of events into High and Low events.
Combining the two distinctions, the set L of all events is thereby partitioned
into High inputs (denoted HI), High outputs (HO), Low inputs (LI) and Low
outputs (LO). They call the resulting process calculus SPA.
Apparently, labelled transition systems are more general than the state ma-
chine models discussed above, in that inputs are not always enabled. Super-
ﬁcially, SPA’s labelled transition systems seem closest to action-observed state
machines, inasmuch as both inputs (actions) and outputs (observations) are asso-
ciated to transitions. Given the equivalences discussed above, we therefore focus
on translating action-observed machines into SPA. However, whereas action-
observed machines combine an action and an observation into a single state
transition, SPA separates the two notions. This leaves open several plausible
translations from
 na to
IO. One follows an approach like that used above
for the translation from
 na to
 ns, and treats the observations as optional
events which do not change the state. We assume in the following that the sets
of possible H and L observations in an action observed system are disjoint. This
is without loss of generality, since we may always rename the H observations,
which does not aﬀect any of the notions of security, since these do not refer to
H observations. Similarly, we assume the sets of actions and observations are
disjoint. (Note the H and L actions are already separated by the function dom.)
Deﬁnition 13. Deﬁne F 1
al :
 na →
IO, such that if M = ⟨S, s0, next, dom, A⟩,
then F 1
al(M ) is the restriction to its reachable states of ⟨P, p0, →, L⟩where
1. P = S × (O ∪{εH, εL})D,
2. p0 = (s0, f0) where f0 is the function with f0(L) = εL and f0(H) = εH,
3. L = I ∪O with I = A,
4. (s, f)
l
−→(t, g) iﬀeither l = a ∈A and for some o ∈O we have (o, t) ∈
next(s, a) and g = f[dom(a) →o], or (t, g) = (s, f) and l = f(u) for some
u ∈D and f(u) ∈O.
Another approach to the translation, which keeps observations obligatory, is to
introduce for each state s and action a a new state (s, a) to represent that the

A Comparison of Semantic Models for Noninterference
245
action a has been taken from state s, but the corresponding observation has not
yet been made.
Deﬁnition 14. Let F 2
al :
 na →
IO such that for M = ⟨S, s0, next, dom, A⟩,
we have Fal(M ) = ⟨P, p0, →, L⟩where
1. P = S ∪(S × A),
2. p0 = s0,
3. L = I ∪O with I = A,
4. →= {(s, a, (s, a)) | s ∈S, a ∈A} ∪{((s, a), o, t) | (o, t) ∈next(s, a)}.
Focardi and Gorrieri discuss the condition of input-totality in the context of
relating their deﬁnitions of security on SPA processes to classical deﬁnitions. An
LTS M ∈
IO(I) is input total if for all s ∈P and for all a ∈I, there exists
t ∈P such that s
a
−→t. It is apparent that for all M ∈
 na, the LTS F 1
al(M ) is
input total, but the LTS F 2
al(M ) is not input total, since inputs are not accepted
in the intermediate states (s, a). We will discuss below the impact this diﬀerence
has on the relationship between deﬁnitions of security in
 na and
IO.
We now state a number of the deﬁnitions of security discussed by Focardi and
Gorrieri. Given a trace t of an LTS in
IO, we write low(t) for the subsequence
of labels in LI ∪LO, high(t) for the subsequence of labels in HI ∪HO, and
highinput(t) for the subsequence of labels in HI. We extend these functions to
apply pointwise to sets of traces. We call a sequence in low(T (M)) a possible
low view of M.
Deﬁnition 15. M ∈
IO is secure wrt Nondeterministic Noninterference (M ∈
NNIl) if for every possible low view α ∈low(T (M )), there exists a trace t ∈
T (M ) such that low(t) = α and highinput(t) = ϵ is the null sequence.
This deﬁnition permits the trace t to contain high outputs. The following
stronger deﬁnition prohibits this.
Deﬁnition 16. M ∈
IO is secure wrt Strong Nondeterministic Noninterfer-
ence (M ∈SNNIl) if for every possible low observation α ∈low(T (M )), there
exists a trace t ∈T (M ) such that low(t) = α and high(t) = ϵ is the null sequence.
The following is a formulation of nondeducibility on inputs in
IO.
Deﬁnition 17. M ∈
IO is secure wrt Nondeducibility on Inputs (M ∈NDIl)
if for every α ∈HI∗, for every possible low view β ∈low(T (M )), there exists a
trace t ∈T (M ) such that low(t) = β and highinput(t) = α.
Finally, we have a deﬁnition that is motivated as a generalization of nonde-
ducibility on strategies. This can be phrased5 in terms of a process composi-
tion with synchronization on High events, which we formulate as follows. Given
LTSs M1 = ⟨P1, p1, →1, L1⟩and M2 = ⟨P2, p2, →2, L2⟩, deﬁne the composition
M1||HM2 = ⟨P, p0, →, L⟩with states P = P1 × P2, initial state p0 = (p1, p2),
5 We simplify the presentation of Focardi and Gorrieri to minimize the amount of
process algebraic notation that we need to introduce.

246
R. van der Meyden and C. Zhang
labels L = L1 ∪L2, and transitions deﬁned by (s, t)
l
−→(s′, t′) if either l ∈
LI ∪LO and one of s
l
−→s′ and t = t′ or s = s′ and t
l
−→t′, or else l = τ and
there exists events l1, l2 in HI ∪HO such that l1 = l2 and s
l1
−→s′ and t
l2
−→t′.
Deﬁnition 18. M ∈
IO is secure wrt Nondeducibility on Compositions (M ∈
NDCl) if for every M ′ ∈
IO that has labels in HI ∪HO only, we have
low(T (M )) = low(T (M ||HM ′)).
Focardi and Gorrieri also consider the following variant NDCIT (or NDCl∩IT ),
which constrains the LTS’s in question to be input-enabled. We deﬁne this in
terms of a looser notion NDC(IT )l, to separate input-totality of the system
itself from input-totality of the composed systems.
Deﬁnition 19. M ∈
IO(HI ∪LI) is secure wrt Nondeducibility on Com-
positions with Input Total systems (M ∈NDC(IT )l) if for every input-total
M ′ ∈
IO(HI), we have low(T (M )) = low(T (M ||HM ′)).
Intuitively, restricting M ′ to be input-total ensures M ′ cannot block any H
output events from M in the composed system M ||HM ′. Investigating the rela-
tionship between the deﬁnitions of security in action-observed systems and
IO,
under the transformations deﬁned above, we obtain the following.
Theorem 4. For all systems M ∈
 na we have M ∈NDIa iﬀF 1
al(M ) ∈NDIl
iﬀF 2
al(M ) ∈NDIl.
Thus, both transformations produce LTS representations of the system that
are equivalent with respect to the property of nondeducibility on input. Since
nondeducibility on strategies is equivalent to nondeducibility on input on
 na,
this result gives us a way of checking the former property through a map-
ping to
IO. However, it remains of interest to check whether the notion of
nondeducibility deﬁned on
IO corresponds to that on
 na. This is particu-
larly so as Focardi and Gorrieri show that the placement of nondeducibility
on composition with respect to the other properties is somewhat sensitive to
the class of systems to which it is applied, and the class of systems used in
the compositions. Focardi and Gorrieri prove the following relationships: [19]
NDCl = SNNIl ⊂NNIl, and NDIl ⊂NNIl, and NDIl ̸⊆NDCl and
NDCl ̸⊆NDIl and NDCl ∩IT = NNIl ∩IT = NDIl ∩IT . We add to
this the following result about input total systems:
Proposition 2. NDCl ∩IT = NDC(IT )l ∩IT .
That is, on input-total systems, input-total High processes have the same dis-
criminative powers as all processes. Using the fact that F 1
al produces input-total
LTS’s, the equivalence of NDSa and NDIa and the facts from the previous
two propositions, we obtain a direct correspondence between nondeducibility on
strategies and several notions of nondeducibility on composition.
Corollary 1. For M ∈
 na, we have M ∈NDSa iﬀF 1
al(M ) ∈NDCITl iﬀ
F 1
al(M ) ∈NDC(IT )l iﬀF 1
al(M ) ∈NDCl.

A Comparison of Semantic Models for Noninterference
247
This means that on input-total systems, and hence on the range of F 1
al, the
distinct notions NDCl, NDC(IT )l, NDCITl, NDIl and NNIl collapse. We
ﬁnd a similar correspondence for F 2
al (except that NDCITl is excluded here
since F 2
al(M ) is not input-total.)
Theorem 5. For M ∈
 na, we have F 2
al(M ) ∈NDIl iﬀF 2
al(M ) ∈NDCl iﬀ
F 2
al(M ) ∈NDC(IT )l.
We now turn to McCullough’s notion of ‘restrictiveness’, already mentioned
above. There are two versions of ‘restrictiveness’ introduced in McCullough’s
early works. The former [22] is a trace-based deﬁnition, while the latter is essen-
tially deﬁned on labelled transition systems [7,23]. In [23] McCullough mentions
both deﬁnitions and concludes that the one on labelled transition systems is
a stronger notion. The cleanest presentation of the LTS version occurs in [23].
Here we present this deﬁnition in the pattern used for unwinding properties for
the automaton models above.
Deﬁnition 20. Deﬁne a McCullough unwinding relation for an LTS M without
τ transitions to be an equivalence relation ∼on the states of M such that
– for all states s, s′, t and input sequences α and α′ such that α|LI = α′|LI,
s
α
−→s′ and s ∼t there exists a state t′ such that s′ ∼t′ and t
α′
−→t′;
– for all states s, s′, t and output sequences α such that s
α
−→s′ and s ∼t,
there exists a state t′ and an output sequence α′ such that α|LO = α′|LO,
t
α′
−→t′, and s′ ∼t′.
Using this notion, the following is equivalent to McCullough’s deﬁnition.
Deﬁnition 21. An LTS M is restrictive (M ∈RESl) if it is input-total, it has
no τ transitions, and there exists a McCullough unwinding relation for M .
Focardi and Gorrieri [19] have proposed a deﬁnition of restrictiveness in the
context of all LTS’s, but in addition to dealing with τ transitions, their deﬁnition
requires that a distinction be made between high and low level τ transitions, for
reasons that are not made clear. Since our translations do not produce LTSs with
τ transitions, the above restricted deﬁnition suﬃces for our present purposes.
Further, Focardi and Gorrieri classify their deﬁnition of restrictiveness with
the other trace-based properties they consider. We point out that a better com-
parison is with the separate hierarchy of bisimulation based deﬁnitions of security
they deﬁne. The following is one of the notions in this hierarchy.
Deﬁnition 22. M ∈
IO satisﬁes strong bisimulation non-deducibility on com-
positions (SBNDC) if for every p ∈P reachable from p0, if p
h
−→p′ for some
h ∈H then (p\H) ≈B (p′\H).
Here, ≈B is the weak bisimulation, and ‘\’ is the restriction operator, with the
usual deﬁnitions in CCS [12]. We may show the following, which justiﬁes the use
of the term restrictiveness in Deﬁnition 9.

248
R. van der Meyden and C. Zhang
Theorem 6. If M ∈
 na then M ∈RESa iﬀF 1
al(M ) ∈RESl iﬀF 1
al(M ) ∈
SBNDC.
For completeness, we also characterise the notion BNS [5] discussed above,
within LTS. The intuition for BNS is that that Low’s future pattern of obser-
vations depends only on the current Low state. To obtain a notion of Low state
in LTS’s, we take the perspective that an agent is able to perceive the set of
observations being oﬀered it in each state. (This makes the most intuitive sense
when these transitions are self-transitions. It is also quite reasonable if diﬀer-
ent observations represent, e.g., the values of diﬀerent variables that the agent
may read.) For a state p of an LTS ⟨P, p0, →, L⟩, deﬁne obsL(p) to be the set of
o ∈LO such that there exists p′ ∈P with p
o
−→p′.
Deﬁnition 23. An LTS M is in BNSl if the relation ∼L, deﬁned on M by
p ∼L q if obsL(p) = obsL(q), is a McCullough unwinding.
The intuition for the relation ∼L on LTS’s (equivalence of the set of possible
observations) is somewhat diﬀerent from that used in the deﬁnition of BNSa
(equivalence of the most recent L observation). However, in F 1
al(M), the (unique)
next possible observation is in fact that which would have been obtained from the
most recent L action. Thus, it is not surprising to ﬁnd the following equivalence.
Theorem 7. If M ∈
 na then M ∈BNSa iﬀF 1
al(M ) ∈BNSl.
6
Conclusion
We have studied the relationships between a variety of deﬁnitions of noninterfer-
ence under a number of mappings between diﬀerent semantic frameworks. Our
results show that similar properties in diﬀerent models do correspond in a precise
sense, but highlight some subtleties: e.g., more properties are preserved when the
obligatory observations in the state-observed model are treated as optional when
mapped to the other models. Of particular interest, given our motivation from
operating systems veriﬁcation, is that the strongest process algebraic notion,
SBNDC, is still weaker on the automaton models than the notion BNSs which
seems closest to the models and properties used in the operating systems veriﬁca-
tion literature [17,18]. However, this literature involves issues such as separation
policy, scheduling and synchrony that go beyond asynchronous models and the
speciﬁc policy L ≤H we have treated in this paper. We intend to address these
issues in future work.
References
1. Goguen, J., Meseguer, J.: Security policies and security models. In: IEEE Symp.
on Security and Privacy, pp. 11–20 (1982)
2. Wittbold, J.T., Johnson, D.M.: Information ﬂow in nondeterministic systems. In:
Proc. IEEE Symp. on Security and Privacy, pp. 144–161 (1990)
3. Millen, J.K.: Hookup security for synchronous machine. In: Proc. CSFW’90, pp.
84–90 (1990)

A Comparison of Semantic Models for Noninterference
249
4. Rushby, J.: Noninterference, transitivity, and channel-control security policies.
Technical report, SRI international (1992)
5. Bevier, W.R., Young, W.D.: A state-based approach to noninterference. In: Proc.
CSFW’94, pp. 11–21 (1994)
6. Oheimb, D.: Information ﬂow control revisited: Noninﬂuence = Noninterference
+ Nonleakage. In: Samarati, P., Ryan, P.Y A, Gollmann, D., Molva, R. (eds.)
ESORICS 2004. LNCS, vol. 3193, pp. 225–243. Springer, Heidelberg (2004)
7. McCullough, D.: Noninterference and the composability of security properties. In:
Proc. IEEE Symp. on Security and Privacy, pp. 177–186 (1988)
8. McLean, J.: A general theory of composition for trace sets closed under selective
interleaving functions. In: Proc. IEEE Symp. on Security and Privacy, pp. 79–93
(1994)
9. Zakinthinos, A., Lee, E.: A general theory of security properties. In: Proc. IEEE
Symp. on Security and Privacy, pp. 94–102 (1997)
10. Mantel, H.: Possibilistic deﬁnitions of security – an assembly kit. In: Proc.
CSFW’00, pp. 185–199 (2000)
11. Hoare, C.: Communicating Sequential Processes. Prentice Hall, Englewood Cliﬀs
(1985)
12. Milner, R.: Communication and Concurrency. Prentice-Hall, Englewood Cliﬀs
(1989)
13. Focardi, R., Gorrieri, R.: Classiﬁcation of security properties. In: Focardi, R., Gor-
rieri, R. (eds.) Foundations of Security Analysis and Design. LNCS, vol. 2171, pp.
331–396. Springer, Heidelberg (2001)
14. Roscoe, A.: CSP and determinism in security modelling. In: Proc. IEEE Symp. on
Security and Privacy, pp. 114–221 (1995)
15. Ryan, P.Y.A.: Mathematical models of computer security. In: Focardi, R., Gorrieri,
R. (eds.) Foundations of Security Analysis and Design. LNCS, vol. 2171, pp. 1–62.
Springer, Heidelberg (2001)
16. Focardi, R., Rossi, S., Sabelfeld, A.: Bridging language-based and process cal-
culi security. In: Sassone, V. (ed.) FOSSACS 2005. LNCS, vol. 3441, pp. 299–315.
Springer, Heidelberg (2005)
17. Greve, D., Wilding, M., van Fleet, W.: A separation kernel formal security policy.
In: ACL2 Workshop (2003)
18. Martin, W., White, P., Taylor, F., Goldberg, A.: Formal construction of the mathe-
matically analyzed separation kernel. In: Proc. 15th IEEE Int. Conf. on Automated
Software Engineering (ASE’00) (2000)
19. Focardi, R., Gorrieri, R.: A classiﬁcation of security properties for process algebras.
Journal of Computer Security 1, 5–33 (1995)
20. Goguen, J., Meseguer, J.: Unwinding and inference control. In: IEEE Symp. on
Security and Privacy (1984)
21. Sutherland, D.: A model of information. In: Proc. 9th National Computer Security
Conference, pp. 175–183 (1986)
22. McCullough, D.: Speciﬁcations for multi-level security and a hook-up property. In:
Proc. IEEE Symp. on Security and Privacy, pp. 161–166 (1987)
23. McCullough, D.: A hookup theorem for multi-level security. IEEE Transactions on
Software Engineering 16(6), 563–568 (1990)

Hiding Information in Multi Level Security Systems
Dani`ele Beauquier1 and Ruggero Lanotte2
1 LACL CNRS FRE 2673, Universit´e Paris 12 Val de Marne
2 Dipartimento di Scienze della Cultura, Politiche e dell’Informazione
Universit`a dell’Insubria
Abstract. In this paper we analyze the possibility for malicious agents to trans-
mit an information possibly hidden in a Multi Level Security System via a covert
channel. We give a framework for which we get two decidability results. Firstly,
given a code and a system one can decide whether the system allows a covert
channel for this code. Secondly, one can decide whether there exists a code to
transmit one bit of information, the code is computable as well as the strategies
of the two partners.
1
Introduction
Security in systems (see [2], [3] and [4]) must ensure a state of inviolability from hostile
actions. Hence, unauthorized agents should not be able to have access to secret infor-
mation. Detecting and preventing illegal information leak is particularly important for
multi-level security (MLS) systems. In these systems the access is controlled in such a
way that no high level information is allowed to pass to lower level users, but low level
information is available to high level users. The challenge here is to check whether
agents having classiﬁed information may leak information to agents with lower classi-
ﬁcation via the use of shared resources.
One way to violate a security policy is the use of covert channels. The notion of
covert channel was ﬁrst introduced by Lampson (see [12]). A covert channel is a trans-
mission channel that is used to transfer data with the purpose of violating security pol-
icy. The idea is that an agent uses the resources and functionalities of a system changing
its performance with the purpose of sending a (secrete) information to another agent.
Analyzing covert channels is one branch of current information ﬂow security research.
A new approach to covert channel analysis and capacity estimation and can be found
in [25].
A communication channel can be used also to transmit hidden information, it is usu-
ally referred as a steganographic channel ([23]). In [29] there was a ﬁrst formalization
of steganographic channel by modelling the prisoners problem. In this problem, two
prisoners in the jail plan to escape together. All communications between them are
monitored, hence, they must hide the escape plan messages in innocuous looking data.
This landmark begun the studies on subliminal channels. Subliminal since the in-
formation is transmitted by a channel used to send information in a cryptosystem. The
history of subliminal channel can be found in [30], further studies can be found in [28]
and [32].
T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 250–269, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

Hiding Information in Multi Level Security Systems
251
These studies have found applications in several ﬁelds like military and intelligence,
digital cash and copyright marking (for a survey see [23]).
There are two main differences between ordinary covert channels and steganographic
covert channels. Opposite to steganographic channels, when studying covert channels
no attention is paid to hiding their existence. Secondly, a covert channel is tacitly as-
sumed to emit data for an unlimited time, which is not the case for steganographic
channels : a message hidden in an image for example has a size which is obviously
limited by the size of the image.
The covert channels we study in this paper refer to both of these properties, namely
an encoding is used (hence in some way we can model also hiding information) and
the size of the transmitted message is bounded. The systems we consider are Multi
Level Systems. We use the classical partition of agents in high agents (having access to
secrete information) and low agents (having access only to public information). Each
agent can perform actions classiﬁed as high or low according to the class of the agent
which performs these actions. Depending on the performed action, the system changes
its state in a non deterministic way.
The high agent wants to communicate to the low agent an information (sometimes
hidden in the messages allowed by the protocol) by using a channel allowed by the
system under consideration. High and low agents agree on a certain code built from
what the low agent can see about the behavior of the system. Hence each information
is associated with a certain sequence of symbols that the low agent can see. When the
high agent wants to send a certain information, he performs the strategy associated with
that information. The goal of this strategy is to get a sequence visible by the low agent
which corresponds to one of the possible encodings of this information, whatever is the
choice of the system.
We prove that given a code and a system, it is decidable to check whether this code
generates a channel and, if it is the case, it is possible to compute the strategies of
both agents to send and receive information. Moreover, we give an algorithm which
decides given a system whether there exists a code to transmit one bit of information.
The algorithm provides also the code and the strategies when the answer is positive.
After some notations given in the next section, we describe our model in section 3.
Section 4 is devoted to the decidability result. We illustrate our model with an example
in section 5 and describe in the last section our future work.
2
Related Work
Several studies have been done on covert channels. As an example, in [6] and [8]
covert channels are studied from the point of view of non interference. Non interfer-
ence means that a high level agent cannot inﬂuence the system in such a way that a low
level agent can see the effects. Other techniques to detect covert channels have been
proposed, which are based on typing, process algebra, axiomatic approaches, etc (see
[24]),[13],[1] and [21]). Covert timing channels have also been studied. For example, in
[7] results on information ﬂow shown in [6] are extended to a timed setting and in [22]
timing covert channels for real time data base systems are studied. Timed Z-channel are
studied in [18]. In [16] covert channels are described as ﬁnite state machines, noiseless

252
D. Beauquier and R. Lanotte
and memoryless covert channels are studied in [17]. Other examples can be found in
[20] and [15].
Unlike these works, our paper focuses on the code that can be chosen by an attacker
to send information in a timed setting, the approach is based on the deﬁnition of strate-
gies and the problem is solved by using game theory.
In [10] the authors have considered a coding and transmission strategy for a covert
channel using a single choice node (namely, a conﬁguration of the system sensible to
the actions of the attacker). In [11] an algorithm based on game theory is proposed for
this approach. Differently from our study, these papers give a sufﬁcient condition for
the existence of a covert channel.
Other papers consider notions of security properties based on strategies. As an ex-
ample, in [34] a security property called Nondeducibility on Strategies is introduced.
This property is satisﬁed if the low level agent cannot exclude any strategy of the high
level agent by reading his visible part of the execution. Similar properties are deﬁned
in [14] (where a system appears the same to the low level agent, for every possible high
level users interacting with him) and also in [33] (where a stronger version of NS is
considered.).
These deﬁnitions assume that the low level agent is passive. Here we consider a
cooperation between the low and the high level agents where the low level agent chooses
a strategy which permits to the high level agent to send information. Moreover, in the
papers mentioned a security property that a system can satisfy is deﬁned. In our paper,
instead, we focus on the deﬁnition of the code of the transmission and on the capacity
on the channel.
3
Basic Notations
Let Σ be a ﬁnite set of symbols. The set of ﬁnite (resp. inﬁnite) strings over the alphabet
Σ is denoted by Σ∗(resp. Σω). The empty word is denoted ϵ. Let Σ∞= Σ∗∪Σω and
Σ+ = Σ∗\ {ϵ}.
For every α ∈Σ∞, with |α| we denote the length of α. If α is an inﬁnite string, then
|α| = ∞. Moreover, if |α| ≥1 and |α| ̸= ∞, then with last(α) we denote the last
symbol appearing in α, more precisely, last(α) = a if α = βa for some β and a ∈Σ.
Given a string α and a natural number n such that n ≤|α|, then with α|n we denote
the string β such that |β| = n and α = βγ, for some string γ.
Given two strings α1, α2 ∈Σ∞, we say that α1 is a proper preﬁx of α2 (written
α1 < α2) if α2 = α1β for some β ∈Σ+. Moreover, we say that α1 is a preﬁx of α2
(denoted α1 ≤α2) if either α1 = α2 or α1 < α2.
Let Σ be an alphabet; with Pair(Σ) we denote the alphabet 2Σ × Σ.
Given a string α ∈Σ∞and a set of symbols A ⊆Σ, with ΠA(α) we denote the
subsequence of α one gets by erasing symbols which are not in A.
Let Δ ⊆Σ∞be a set of strings. With Pref(Δ) (resp. PPref(Δ)) we denote the
set of ﬁnite preﬁxes of Δ: {α ∈Σ∗| α ≤β for some β ∈Δ} (resp. the set of proper
preﬁxes of Δ: {α ∈Σ∗| α < β for some β ∈Δ}). The set Δ is preﬁx free if
Δ ∩PPref(Δ) = ∅.
The cardinality of a set X is denoted |X|.

Hiding Information in Multi Level Security Systems
253
4
The Model
In this section we describe the model we use for the system and the covert channel.
4.1
The System
Following [26], we model non deterministic systems using action (or event) systems.
The system is modelled as a ﬁnite non deterministic transition system where the
actions of the low and high agents modify the state of the system. We model the elapsing
of time with a special action denoted tick. The agents know the entire speciﬁcation of
the system but during an execution they have a partial information, in particular they do
not know what is the current state of the system and the low agents see only the actions
of their level.
Deﬁnition 1. A timed system S is a tuple (Q, q0, Act, δ) where
– Q is a ﬁnite set of states partitioned into two sets QLow and QHigh that are disjoint
ﬁnite sets of low and high states respectively.
– q0 ∈Q is the initial state.
– Act is a ﬁnite set of symbols partitioned into two sets Low and High that are two
disjoint ﬁnite sets of low and high actions, respectively. We assume that the special
symbol tick representing the elapsing of one time unit without any low or high
action being performed is not in Act.
– δ is the set of transitions that is partitioned into two sets δLow and δHigh such that
δLow ⊆QLow × (Low ∪{tick}) × Q is the set of low transitions and δHigh ⊆
QHigh × (High ∪{tick}) × Q is the set of high transitions. Moreover, one cannot
avoid time elapsing, for this reason we assume that for every state q ∈Q there is at
least one transition (q, a, q′) in δ, a ∈Act ∪{tick}. It means that even if no action
in Act is possible, the tick action is available in that case.
The timed system S is deterministic if, given a state q and an action a ∈Act ∪{tick},
there is at most one transition from q labelled with a, more precisely, for any q ∈Q and
a ∈Act ∪{tick}, it holds that |{(q, a, q′) | (q, a, q′) ∈δ}| ≤1.
Given a state q, with Act(q) we denote the set of actions performable from q, more
precisely, Act(q) = {a ∈Act ∪{tick} | (q, a, q′) ∈δ for some q′ ∈Q}.
A path for a timed system S = (Q, q0, Act, δ) starting from q ∈Q is a (possibly
inﬁnite) sequence of transitions δ1δ2 . . . δn . . . where for any i, δi = (qi, ai, qi+1) ∈δ.
The path is also written q1
a1
−→q2 . . . qn
an
−→qn+1 . . . .
The set of ﬁnite paths of S is denoted with Path and the set of ﬁnite paths starting
from q is denoted with Path(q). We denote with last(ω) (resp. first(ω)) the last (resp.
ﬁrst) state of a ﬁnite path ω, i.e. if w = q1
a1
−→q2 . . . qn
an
−→qn+1 then last(ω) = qn+1
and first(ω) = q1.
With PathLow (resp. PathHigh) we denote the set of paths ω in Path such that
last(ω) ∈QLow (resp. QHigh).
Given two ﬁnite paths ω = δ1δ2 . . . δn and ω′ = δ′
1δ′
2 . . . δ′
p such that last(ω) =
first(ω′), the sequence δ1δ2 . . . δnδ′
1 . . . δ′
p is a path denoted with ωω′.

254
D. Beauquier and R. Lanotte
4.2
Covert Channels
In this subsection we explain how the low and high users can collaborate to create
a hidden covert channel inside the system. The interpretation of the behavior of the
system is as follows. At each step the choice of the action to perform is made by the
high (resp. low) agent if the state is a high (resp.) low one. After that, the system resolves
the non determinism. In this way, there is a possibility of cooperation between a low
and a high agent if they agree by advance on some code. The two agents can decide
in advance of some partition (the code) of the set of visible sequences seen by the
low agent into 2n classes. In this way the low agent is able to receive the encoding
of a message of length n from the high agent. This message corresponds to the class
containing the sequence of actions he observes. The problem is to know whether for a
given code, the high agent is able to force the system to produce a sequence of actions
whose visible part belongs to the class chosen by the high agent.
As usual we assume that the low agent cannot see high actions but he sees the tick
steps. Actually, it is reasonable that when time elapses for an agent, then it elapses for
all the agents. Since we consider instantaneous actions, obviously, the low agent cannot
distinguish whether the high agent performs one or more actions in a given time slot
(it is the designer of the system who decides the number of actions that can occur in a
given time slot).
This semantics is a classical one considered in several papers deﬁning discrete time
models, see as an example [7] where the same semantics is adopted in a process algebra
framework.
One way to get a covert channel in such a situation is to decide that the low agent will
apply a strategy ﬁxed by advance, and that the high agent will apply a strategy which
depends on the message he intends to send. The problem can be interpreted as a game
of the two partners against the non deterministic choices of the system.
Example 1. Fig. 1 and Fig 2 are two timed systems. In Fig. 1, low states are states
0, 2, 3, 6, 7 and in Fig 2 low states are states 0, 3, 4, 5, 6.
We will see below that the ﬁrst system does not contain a covert channel for any code,
because whatever is the choice of the high agent the low level will observe the same
behavior, and then the low agent cannot infer any choice of the high agent. The second
system has a covert channel of one bit at least because depending on the choice of the
high agent the low agent will observe different behaviors.
From now on with Lowt, Hight and Actt we respectively denote the sets Low ∪
{tick}, High ∪{tick} and Act ∪{tick}.
Let ω = q1
a1
−→q2 . . . qn
an
−→qn+1 be a path. The sequence of choices in ω w.r.t.
Actt is the sequence (A1, a1) . . . (An, an) in Pair(Actt)∗
such that for i ∈[1, n],
Ai = Act(qi). This sequence represents what was seen by the high agent, namely, what
were the choices made by the high and the low agents. In particular the high agents does
not know the current state of the system.
The situation for the low agent is different since he sees only the low events.
Let ω = q1
a1
−→q2 . . . qn
an
−→qn+1 be a path. Let i1 < i2 < · · · < ik be the
sequence of indices j in [1, n] such that aj ∈Lowt. The sequence of choices in ω w.r.t.
Lowt is the sequence (B1, ai1) . . . (Bk, aik) in Pair(Lowt)∗such that:

Hiding Information in Multi Level Security Systems
255
- m
0 


l
@
@
R
l
m
1
-
h1
QQQ
Q
s
h2
m
2
-
l
m
3
m
4
6tick
W
tick
m
5
-
h2
m
6
-
l
m
7 W
tick
Fig. 1.
- m
0
-
l
ZZZ
Z
~
l
m
2 W
h1



tick
@
@
R
h2
m
1

tick, h2
6
h1
m
4
-
l1
m
6 W
tick
m
3
-
l2
m
5

tick

l

l
Fig. 2.
for j ∈[1, k], if qij ∈QLow, then Bj = Act(qij) otherwise Bj = ∅.
Notice that in this last case aij = tick, because if qij is a high state, the low agent
can see the choice of the high agent only if it is a tick.
The sequence of choices in a path ω w.r.t. Lowt will be denoted by ΠLowt(ω).
Let ω1, ω2 ∈Path; we write ω1 ≈Low ω2 if ω1 and ω2 have the same sequence of
choices w.r.t. Lowt and Act(last(ω1)) = Act(last(ω2)). This equivalence relation
≈Low expresses the fact that two paths in the same equivalence class are undistin-
guishable from the point of view of the low agent. With [ω]Low we denote the class
{ω′ ∈Path | ω ≈Low ω′}. On the same way, we write ω1 ≈High ω2 if ω1 and ω2 have
the same sequence of choices w.r.t. Actt and Act(last(ω1)) = Act(last(ω2)).
A Low-strategy is a function f : PathLow →Lowt satisfying:
- f(ω) ∈Act(last(ω))
- if ω ≈Low ω′, then f(ω) = f(ω′).
The idea is that the low agent performs the same choice for the paths with the same
visible part w.r.t. Lowt because his choice can depend only on what he can see.
Symmetrically, a High-strategy is a function r : PathHigh →Hight satisfying:
- r(ω) ∈Act(last(ω))
- if ω ≈High ω′, then r(ω) = r(ω′).
With StrLowt and StrHight we denote, respectively, the set of Low-strategies and
High-strategies.
A strategy of S is a partial function g : Path × Actt →Q such that, for any path
ﬁnite ω and any a ∈Act(last(ω)), g(ω, a) is deﬁned and (last(ω), a, g(ω, a)) ∈δ. It
means that the system S resolves the non determinism, by choosing the transition to use
at some point performing some action. With StrS we denote the set of strategies of S.
For any f ∈StrLowt, r ∈StrHight and any ﬁnite path ω, with next(ω, f, r) we
denote the symbol f(ω) if last(ω) ∈QLow and r(ω) if last(ω) ∈QHigh. It represents
the next action which will occur after the execution of path ω according to the respective
low and high strategies f and r.
Moreover, given g ∈StrS, with path(f, r, g) we denote the inﬁnite path ω = q1
a1
−→
. . .
an
−→qn+1 . . . such that, for any n, it holds that qn+1 = g(ω|n−1, an) and an =

256
D. Beauquier and R. Lanotte
next(ω|n−1, f, r). Actually, if strategies of the low agent, the high agent and the system
are ﬁxed, then the systems becomes deterministic and has a unique behavior which
corresponds to the path path(f, r, g).
The sequence ΠLowt(path(f, r, g)) will be abbreviated into ΠLowt(f, r, g).
Deﬁnition 2. Let ψ be a subset of Pair(Lowt)+, and f ∈StrLowt. A winning strategy
with respect to f and ψ is a strategy r ∈StrHight such that for any g ∈StrS, it holds
that ΠLowt(f, r, g), has a preﬁx in ψ.
A strategy for the high agent is a winning strategy with respect to f and ψ if whatever
is the strategy of the system, if the low agent applies the strategy f and the high agent
applies the strategy r then the behavior of the system will have a projection on the low
level which has a preﬁx in ψ.
Deﬁnition 3. A n-code is a family {ψ1, . . . , ψ2n} of disjoint non empty ﬁnite subsets
of Pair(Lowt)+ such that
2n
i=1 ψi

is preﬁx free.
Deﬁnition 4. Let S be a timed system. There exists a covert channel of n bits for S
if there exists an n-code ψ1, . . . , ψ2n and a strategy f ∈StrLowt such that for any
i ∈[1, 2n] there exists a winning strategy with respect to f and ψi.
Actually, if the low agent applies strategy f and the high agent applies strategy ri then
whatever is the strategy of the system, the behavior of the system will have a projection
on the low level which has a preﬁx in ψi. And since
2n
i=1 ψi

is preﬁx free, as soon as
this preﬁx occurs, the low agent knows that no longest preﬁx can belong to another ψi′.
We note that this permits therefore to send hidden information. Let M = {m1, . . . ,
mk} be a set of messages expressing the possible value of a certain information. As an
example, M can be the set {Y es, Not} or the range of possible values of some secrete
stored data. The idea is that the high agent during his execution wants to send hiddenly
to the low agent some message of M. Let ψ1, . . . , ψ2n be a n-code with n ≥log(k).
The high and low agents agree that ψ1, . . . , ψk correspond respectively to the messages
m1, . . . , mk. Hence, if the high agent wants to send hiddenly the value mj, he will
apply a strategy which forces the low agent to realize a sequence in ψj.
Example 2. The system of Fig. 1 does not allow an n-code for any n ≥1 for the
reason invoked above. Actually, the only manner for the high agent to send information
to the low agent is to perform the tick action. But ψ0 = {({l}, l)(∅, tick)}, ψ1 =
{({l}, l)({l}, l)} is not a 1-code, since the state reached from state 0 after the action l
is chosen by the system and not by the high agent. Thus if the system chooses to go in
state 5, then the observation of the low level is always ({l},l)({l},l) and the high level
cannot send the bit 0.
Nevertheless this system is usually considered insecure. Actually, if the low agent
observes the ({l}, l)(∅, tick) surely h2 is performed by the high agent. But we are not
interested in this paper to this type of information ﬂow.
The second example, namely the system of Fig. 2 allows an 1-code. The pair ψ0 =
{({l}, l)(∅, tick)}, ψ1 = {({l}, l)({l2}, l2)} is a 1-code. Actually, the strategy f of the
low level is trivial. The winning strategy r0 with respect to f and ψ0 is as follows:

Hiding Information in Multi Level Security Systems
257
r0(q0
l
−→q2) = r0(q0
l
−→q1) = tick (the two paths are ≈High-equivalent so the
value for r0 must be the same).
So whatever is the strategy g of the system, ({l}, l)(∅, tick) will be a preﬁx of
ΠLowt(f, r0, g), i.e. the low agent will observe this string.
The winning strategy r1 with respect to f and ψ0 is as follows:
- r1(q0
l
−→q2) = r1(q0
l
−→q1) = h1
- r1(q0
l
−→q2
h1
−→q2 = r1(q0
l
−→q1
h1
−→q2) = h2
Whatever is the strategy g of the system, ({l}, l)({l2}, l2) will be a preﬁx of
ΠLowt(f, r1, g), i.e. the low agent will observe this string.
One can notice that we have deﬁned r0 and r1 not everywhere but only on the paths
which are ”realized” when these strategies are executed. The value of r0 and r1 on other
paths is not important.
4.3
Deadlocking the Timed System
In the deﬁnition given before, we do not give the possibility to the high agent to dead-
lock the system to communicate information. We note that we consider a system to be
deadlocked if the low agent can no longer perform any action in Low.
In the following deﬁnition we show how to modify the system S in order to consider
also information ﬂow by mean of deadlock.
Deﬁnition 5. Given a timed system S = (Q, q0, Act, δ) with Sdead we denote the timed
system (Q′, q′
0, Act′, δ′)
– Q′ = Q ∪{dl}. The state dl is a low state; So we have Q′
Low = QLow ∪{dl} and
Q′
High = QHigh. The state dl is reached when the high agent decides to deadlock
the system.
– q′
0 = q0.
– Act′ = Act ∪{⊤}}. The sets of low and high actions are respectively Low and
High ∪{⊤}}. The action ⊤represents the fact that the high system stops its com-
munication with the system S with the purpose of deadlocking S.
– δ′ = δ ∪δ1 ∪{(dl, tick, dl)} such that:
δ1 = {(q, ⊤, dl) | q ∈QHigh}.
- k
0 

l
@@
R
l
k
1
-
h1
QQQ
s
h2
k
2
-
l
k
3Ntick
k
46
tick
k
5
-
h2
k
6
-
l
k
7Ntick

⊤
6
⊤
k
dlNtick
Fig. 3.

258
D. Beauquier and R. Lanotte
Example 3. The system of Fig. 1 allowing deadlock is represented in Fig. 3. The system
now allows a covert channel of one bit. As an example ψ0 = {({l}, l)({tick}, tick)},
ψ1 = {({l}, l)({l}, l)} is a 1-code. Actually after the choice of the system in the initial
state the high agent can block the system in order to realize ψ0, and in order to realize
ψ1 he chooses h1 if this action is available, otherwise he chooses h2.
5
Two Algorithms for Detecting a Covert Channel
In this section we prove that for a given code one can decide whether a system has a
covert channel. Moreover, we show how to ﬁnd a 1-code.
5.1
Checking an n-Code
From now on, we use the system of Figure 2 as a running example. Hence, each example
of this section refers to that system.
First of all we observe that the value assumed by a high strategy is not important
when considering a path whose sequence of choices is not a proper preﬁx of a sequence
in ψ. Hence, we formalize this fact before proving the main theorem stating the decid-
ability of checking n-codes.
Deﬁnition 6. Let f ∈StrLowt and r ∈StrHight be a winning strategy with respect
to some ﬁnite preﬁx free set ψ ⊂Pair(Lowt)+. We deﬁne Domain(r) as the set of
ω ∈PathHigh such that ω ∈Path(f, r) and ΠLowt(ω) ∈PPref(ψ).
Clearly, the value of r outside its domain does not inﬂuence the fact that r is a winning
strategy. More precisely:
Lemma 1. Let f ∈StrLowt and r ∈StrHight be a winning strategy with respect to f
and some ﬁnite preﬁx free set ψ ⊂Pair(Lowt)+.
If r′ ∈StrHight is such that r′
|Domain(r) = r|Domain(r) then r′ is a winning strategy
with respect to f and ψ and Domain(r) = Domain(r′).
Let f ∈StrLowt and consider some ﬁnite preﬁx free set ψ ⊂Pair(Lowt)+. In order
to decide whether there exists a winning strategy with respect to f and ψ, we translate
the problem into a 2-player game.
We recall brieﬂy the main idea of a two player game. A two player game is composed
by two players P0 and P1 and a game graph that is a tuple (V0, V1, Act, E) where
V0 and V1 are two disjoint sets of vertices, Act is a ﬁnite set of symbols, and, E ⊆
(V0 ∪V1) × Act × (V0 ∪V1) is a ﬁnite set of edges. For i = 0, 1, when a vertex in
Vi is reached the next edge to follow is chosen by the player Pi. A player chooses the
next edge with respect to a strategy that is a function from paths to edges. A strategy
is memoryless if it depends only on the last vertex of the path. A player wins when
the other player is in a vertex without exiting edges. A player has a winning strategy
starting from a certain vertex when he wins the game for any strategy adopted by the
other player.
The following Lemma is a well known result [9].

Hiding Information in Multi Level Security Systems
259
Lemma 2. Let G be a game graph. One can decide whether a player has a winning
strategy from some vertex in linear time w.r.t. the size of the game graph G. Moreover
if there exists a winning strategy there exists a memoryless one and it can be computed
also in linear time.
Let us describe the two player game associated to our problem.
Given a timed system S = (Q, q0, Act, δ), with post(q, a) we denote the set

(q,a,q′){q′}. We can easily extend this deﬁnition to sets D ⊆Q as post(D, a) =

q∈D{post(q, a)}. Moreover, with Act(D) we denote the set 
q∈D{Act(q)}, and,
with split(D) we denote the partition {D1, . . . , Dm} of D such that, for any q, q′ ∈D,
it holds that Act(q) = Act(q′) iff there exists j such that q, q′ ∈Dj.
We consider the game graph G(S, f, ψ) = (V0, V1, Act ∪{ϵ}, E) where
– V0 is the set of triples (D, α, 0) such that:
• either D ⊆QLow or D ⊆QHigh and q, q′ ∈D =⇒Act(q) = Act(q′);
• α is in PPref(ψ);
– V1 is the set of triples (D, α, 1) such that:
• D ⊆Q;
• α is in Pref(ψ);
– the set of edges E is deﬁned as follows:
• For any vertex (D, α, 0) ∈V0 such that α ∈PPref(ψ) and D ⊆QHigh if
h ∈Act(D), there exists an edge with label h and target (post(D, h), α, 1).
• For any vertex (D, α, 0) ∈V0 such that α ∈PPref(ψ) and D ⊆QHigh,
if tick ∈Act(D) there exists an edge with label tick and target
(post(D, tick), α(∅, tick), 1).
• For any vertex (D, α, 0) ∈V0 such that α ∈PPref(ψ) and D ⊆QLow, there
exists an edge with label l and target (post(D, l), α(Act(D), l), 1) iff l is the
symbol chosen by f when considering a path with sequence of choices equal
to α(Act(D)).
• for any vertex (D, α, 1) ∈V1 and D′ ∈split(D), there exists an edge with
label ϵ and target (D′, α, 0);
We explain below how this graph simulates the behavior of the system when the low
level applies the strategy f.
A vertex s = (D, α, 0) in V0 stores in its ﬁrst component D all the possible reached
states in S due to the non determinism of S for a ﬁxed set of actions in Actt, and in its
second component α the sequence of choices w.r.t. Lowt. If D ⊆QLow there is only
one transition from s which corresponds to the application of strategy f. If D ⊆QHigh
there are several transitions from s which correspond to the different possible strategies
of the high level. Transitions from states in V0 simulate the choices of the low and the
high agents. So player P0 corresponds to the low and high agents.
Transitions (D, α, 1)
ϵ
−→(D′, α, 0) where D′ ∈split(D) are used to simulate
the non determinism of the system with the control that the decisions of the high (or
low) level are the same for two states q and q′ such that Act(q) = Act(q′). Therefore
transitions from states of V1 simulate the strategy of the system S and correspond to the
choices of player P1.

260
D. Beauquier and R. Lanotte
There are two types of vertices without exiting edges. Those which are in V0 corre-
spond to the situation when no more action is possible which permits to keep a sequence
of choices w.r.t. Lowt which is in Pref(ψ). In that case the player P1 wins the game.
Exiting edges which are in V1 correspond to the situation when the system has realized
a sequence of choices w.r.t. Lowt which is in ψ. In that case the player P0 wins the
game.
As a matter of fact, our problem is reducible to a two player game in the following
sense:
Theorem 1. Let S be a timed system, f ∈StrLowt and ψ be a ﬁnite preﬁx free set
included in Pair(Lowt)+. There exists a winning strategy with respect to f and ψ in S
iff player P0 has a winning strategy in the game graph G(S, f, ψ). Moreover, to check
whether there exists a winning strategy with respect to f and ψ in S is decidable in
exponential time w.r.t. the size of S. If the system is deterministic, then it is decidable in
polynomial time w.r.t. the size of |S| and the size of |ψ|.
Moreover we can estimate the complexity of the problem:
Theorem 2. Let S be a timed system. To check whether there exists a covert channel in
S for a given n-code is decidable in exponential time w.r.t. the size of S and the size of
the n-code. If the system is deterministic, then it is decidable in exponential time w.r.t.
the size of the n-code.
In order to prove these two theorems we need auxiliary lemmas.
In the following Lemmas we associate to each path in S which is compatible with
the set ψ, some path in G and we prove that P1 wins the game iff there is a winning
strategy with respect to f and ψ.
We say that a path ω in S starting from q0 is compatible with the set ψ if ΠLowt(ω) ∈
PPref(ψ) or ΠLowt(ω) ∈ψ and the last transition of ω is labeled by an action in
Lowt. Notice that this action can be a tick and can be performed in that case by the
high level. Let us associate to each path ω compatible with the set ψ a path θ(ω) starting
from the initial state of G as follows:
if ω has a length equal to zero, the same is true for θ(ω), otherwise
let ω = q0
a0
−→q1 . . . qn
an
−→qn+1 and let (D, α, 0) be the last state of θ(ω′) where
ω′ = q0
a0
−→q1 . . . qn−1
an−1
−→qn,
1. if ΠLowt(ω)
∈
PPref(ψ) then θ(ω) is obtained by adding to θ(ω′) two
transitions:
(D, α, 0)
an
−→(post(D, an), α′, 1)
ϵ
−→(Dj, α′, 0)
where Dj and α′ are deﬁned by:
– Dj ∈split(post(D, an) and qn+1 ∈Dj
– α′ = α if qn ∈QHigh and an ̸= tick
– α′ = α(∅, tick) if qn ∈QHigh and an = tick
– α′ = α(Act(D), an) if qn ∈QLow.
2. if ΠLowt(ω) ∈ψ and an ∈Lowt then θ(ω) is obtained by adding to θ(ω′) the
transition (D, α, 0)
an
−→(post(D, an), α′, 1) where α′ satisﬁes:

Hiding Information in Multi Level Security Systems
261
– α′ = α(∅, tick) if qn ∈QHigh and an = tick
– α′ = α(Act(D), an) if qn ∈QLow.
Notice that in this latter case, the last state of θ(ω) has no exiting edge. So we have the
property:
Lemma 3. Let r be a winning strategy with respect to f and ψ. For every strategy g of
S and every preﬁx ω of path(f, r, g) such that ΠLowt(ω) ∈ψ then θ(ω) is a winning
path for player P0.
And as a consequence we have:
Lemma 4. If there exists a winning strategy with respect to f and ψ in S then there
exists a winning strategy γ for player P0 in the game graph G(S, f, ψ).
Conversely,
Lemma 5. If there exists a winning strategy for player P0 in the game graph
G(S, f, ψ), then there exists a winning strategy with respect to f and ψ for S.
Proof. Let γ be a winning strategy for player P0. We use γ to deﬁne a winning strategy
r with respect to f and ψ for S. Due to Lemma 1, it is useless to deﬁne r on paths
such that ΠLowt(ω) ̸∈PPref(ψ). Let r(ω) = γ(θ(ω)) for every ω ∈PPref(ψ). By
construction if ω1 ≈High ω2 then r(ω1 = r(ω2), so r is a high strategy. Moreover since
γ is a winning strategy for player P0, clearly for every strategy g ∈StrS, ΠLow(f, r, g)
has a preﬁx in ψ and so r is a winning strategy.
Proof of Theorem 1
Using Lemmas 4 and 5 there exists a winning strategy with respect to f and ψ iff the
game G(S, f, ψ) has a winning strategy for player P0.
By Lemma 2 in linear time with respect to the size of G one can decide whether
player P0 has a winning strategy and if it is the case, compute a memoryless winning
strategy for this player.
Since the size of Pref(ψ) is polynomial in |ψ|, the number of vertices of G is
exponential only w.r.t. Q. Therefore, one can decide in exponential time w.r.t. Q and
in polynomial time with respect to |ψ| whether there exists a winning strategy with
respect to f and ψ. If the system is deterministic obviously the size of the game graph
is polynomial in |Q|, |Act| and |ψ|.
⊓⊔
Proof of Theorem 2
Let {ψ1, . . . , ψ2n} be a n-code. For each strategy f ∈StrLowt, one can decide for each
i ∈[1..2n] whether there is a winning strategy with respect to f and ψi using Theo-
rem 1. Moreover the ”useful” part of f is its restriction to the set F = {[ω]Low | ω ∈
PathLow, ΠLowt(ω) ∈Pref(2n
i=1 ψi)}.
That is if two strategies f and f ′ coincide on F then there is a winning strategy with
respect to f and ψi for every i ∈[1, 2n] iff there is a winning strategy with respect to f ′
and ψi for every i ∈[1, 2n]. The set F is ﬁnite and the number of possible values of f is
computable and is exponential in the size of the n-code, and therefore using Theorem 1
to check whether there exists a covert channel in S for the n-code {ψ1, . . . , ψ2n} is
decidable in exponential time w.r.t. the size of S and the n-code.

262
D. Beauquier and R. Lanotte
If the system is deterministic, by Theorem 1, it is decidable in exponential time w.r.t.
the size of the n-code.
⊓⊔
Example 4. In Fig. 4 we show the game graph of the system in Fig. 2 where ψ is equal
to {({l}, l)(∅, tick)}. In this case f is unique. Circles represent the cases in which
the mark is 0 and hence represent vertices in V0 (played by low/high agents). Boxes
represent the cases in which the mark is 1 and hence represent vertices in V1 (played
by the system). States ({4}, ({l}, l)(∅, tick), 1) and ({1, 4}, ({l}, l)(∅, tick), 1) are the
winning states for the low/high agents. State ({3}, ({l}, l), 0) is the winning state for
the system.
{0}, ϵ
{1, 2}, ({l}, l)
{1, 2}, ({l}, l)
{1, 4}, ({l}, l)(∅, tick)
{1, 3}, ({l}, l)
{1}, ({l}, l)
{2}, ({l}, l)
{2}, ({l}, l)
{4}, ({l}, l)(∅, tick)
{3}, ({l}, l)
{3}, ({l}, l)
l
ϵ
h2
ϵ
h1
h1
ϵ
h2
ϵ
tick
tick
ϵ
Fig. 4. Game graph for system in Fig. 2
Note that even for deterministic systems we have an exponential complexity bound.
But, if each set composing the n-code has exactly one element the problem complex-
ity is exponential only in the size of S. If S is deterministic the problem complexity
becomes polynomial.
Corollary 1. Let S be a timed system and {ψ1, . . . , ψ2n} be an n-code such that |ψi| =
1, for any i. To check whether {ψ1, . . . , ψ2n} is a covert channel in S is decidable in
exponential time w.r.t. the size of S. If the system is deterministic, then it is decidable in
polynomial time w.r.t. the size of S and the n-code.
Proof. The strategy f is ﬁxed by the n-code in the set F = {[ω]Low | ΠLowt(ω) ∈
Pref(2n
i=1 ψi)}.
⊓⊔
It can be of interest to consider for which class it is possible to have a n-code where each
set has exactly one element. The following proposition states that it is always possible
for deterministic systems, implying that the decidability is potentially polynomial for
deterministic systems.
Proposition 1. If S is deterministic and {ψ1, . . . , ψ2n} is an n-code for S, there exists
an n-code {ψ′
1, . . . , ψ′
2n} for S such that ψ′
i ⊆ψi and |ψ′
i| = 1, for any i.
Proof. Derives directly from the fact that |StrS| = 1, since S is deterministic. Indeed,
there exists only one string αi in each ψi which is a preﬁx of ΠLowt(path(f, ri, g))
where g is the unique strategy of the system. Thus one can take ψ′
i = {αi}.
⊓⊔

Hiding Information in Multi Level Security Systems
263
5.2
Finding a 1-Code
An interesting question is to decide whether a given system has a n-code for a ﬁxed n.
We give here a positive answer to this problem for n = 1.
Let S be a system and {ψ0, ψ1} be a 1-code for S. It means that the high agent can
send one bit of information to the low agent. We show that if such a code exists there
exists one with a size bounded by some function of the size of S.
Lemma 6. Let S be a system and {ψ0, ψ1} be a 1-code for S. Let l ∈Pair(Lowt) and
u ∈Pair(Lowt)+ such that ul ∈ψ0 and u ̸∈PPref(ψ1) then {ψ0 ∪{u} \ {ul}, ψ1}
is a 1-code for S.
Proof. Let f be a low strategy and r0, r1 be winning high strategies for f and the code
{ψ0, ψ1}. Clearly, without changing f, r0 and r1 remain winning strategies for the new
code.
⊓⊔
Lemma 7. Let S be a system and {ψ0, ψ1} be a 1-code for S. Let u ∈PPref(ψ0 ∪
ψ1). If there does not exist two different strings v, w such that uv ∈ψ0 and uw ∈ψ1
then there exists a smaller 1-code for S.
Proof. Assume that for every v such that uv ∈ψ0 ∪ψ1, actually uv belongs to ψ0 (the
other case is treated in a similar way). Then all these uv can be removed from ψ0 and
replaced with u. Let ψ′
0 be the new set we get in this way. Clearly, if f is a low strategy
and r0, r1 are winning high strategies w.r.t. f for ψ0 and ψ1 respectively, then r0, r1 are
also winning high strategies w.r.t. f for ψ′
0 and ψ1 respectively.
⊓⊔
Lemma 8. Let S be a system which admits a 1-code. There exists a 1-code {ψ0, ψ1}
such that every proper preﬁx of ψ0 ∪ψ1 is a proper preﬁx of ψ0 and a proper preﬁx
of ψ1.
Proof. It is a direct consequence of the previous Lemma (Recall that ψ0 and ψ1 are
preﬁx-free).
⊓⊔
The following Lemma expresses the fact that one can take a code where all strings are
”useful”.
Lemma 9. Let S be a system and {ψ0, ψ1} be a 1-code for S. There exist a pair
{ψ′
0, ψ′
1} and a low strategy f ∈StrLowt such that there exist winning strategies r0, r1
w.r.t. f for ψ′
0 and ψ′
1 respectively satisfying the following property: for every u ∈ψ′
i
there exists a strategy g ∈StrS such that u is a preﬁx of ΠLowt(path(f, ri, g))
Proof. If for i = 0, 1 we remove from ψi all the strings u for which there does not
exist a strategy g ∈StrS such that u is a preﬁx of ΠLowt(path(f, ri, g)) the new pair
{ψ′
0, ψ′
1} is still a 1-code for S.
⊓⊔
Let ω = q1
a1
−→. . . in Path. A strategy g ∈StrS is compatible with ω if for every
n ≥0, g(ω|n, an+1) = last(ωn+1).
Let us introduce now some deﬁnitions in order to prove that if a system admits a 1-
code, there exists one with a size bounded by a function of the size of S, which implies,
using theorem 2 the decidability of the existence and the computation of a 1-code.

264
D. Beauquier and R. Lanotte
The idea is to attach to each preﬁx of ψ0 ∪ψ1 where {ψ0, ψ1} is a 1-code a ﬁnite
information about the runs which contain this preﬁx.
Let f ∈StrLow and r0, r1 ∈StrHigh. Let P(f, r0, r1) be the set PathLow ∩
Pref(path(f, r0) ∪path(f, r1))
For all ω ∈P(f, r0, r1), let us deﬁne
C(ω) = {(q, l) ∈QLow × Lowt | ∃ω′ ∈P(f, r0, r1) s.t. ω′ ≈High ω f(ω′) =
l and last(ω′) = q}.
The ﬁnite set C(ω) represents all the possible pairs (q, l) associated to paths ω′ ∈
PathLow which are ≈High equivalent to ω, where q is the last state of ω′ and l is the
action chosen by f for ω′. Thus C(ω) collects all the possible values (q, l) for paths in
PathLow which have the same observable sequence for a high level agent.
D(ω) = {C(ω′) | ω′ ∈P(f, r0, r1) and ω′ ≈Low ω}
Let u ∈Pair(Lowt)∗. We attach to u some information given by:
State(u) = {D(ω) | ω ∈P(f, r0, r1) and ΠLowt(ω) = u}.
As for LowPair, the idea is that, if State(uv) = State(u), then the strings uvz of
the 1-code can be substituted by uz.
Example 5. Let α = (∅, tick)({l1}, l1)({l}, l). We note that ψ0 = {({l}, l)α(∅, tick)}
and ψ1 = {({l}, l)α({l2}, l2)} is a 1-code for the system of Fig. 2. Moreover, we have
that State(({l}, l)α) = State(({l}, l)) = {{(q3, l2)}}. Hence, α can be cut. Actually,
we have already shown that ψ0 = {({l}, l)(∅, tick)} and ψ1 = {({l}, l)({l2}, l2)} is a
1-code for the system.
Let N = |Q|×(|Low|+1). Then State(u) is bounded by the constant 22N, if S is non
deterministic, and 2N if S is deterministic.
We deﬁne the size of a ﬁnite subset of Pair(Lowt)∗as the maximal length of its
strings.
Lemma 10. Let S be a system and {ψ0, ψ1} be a 1-code for S. There exists a 1-code
of size at most 22N . If S is deterministic, then there exists a 1-code of size at most 2N.
Proof. Let r0 and r1 be winning strategies with respect to some low strategy f for
ψ0 and ψ1 respectively. Using Lemmas 6, 7, 8 and 9 one can assume that the 1-code
{ψ0, ψ1} satisﬁes the following properties :
1. if u ∈PPref(ψi) then u ∈PPref(ψ¯i) (if i = 0,¯i = 1 and if i = 1,¯i = 0)
2. for every u ∈ψi there exists a strategy g ∈StrS such that u is a preﬁx of
ΠLowt(path(f, ri, g)).
Let M = 22N. If the size of the 1-code {ψ0, ψ1} is greater than M there exists a string
uvw of Pref(ψ0)∩Pref(ψ1) where v is non empty and State(u) = State(uv) (note
that if S is deterministic is sufﬁcient 2N). We now build smaller sets ψ′
0, ψ′
1 a new low
strategy f ′ and new high strategies r′
0 and r′
1.
– Sets ψ′
0 and ψ′
1 are deﬁned as follows:
ψ′
0 = (ψ0 \ uvPair(Lowt)∗) ∪{uz | uvz ∈ψ0}
ψ′
1 = (ψ1 \ uvPair(Lowt)∗) ∪{uz | uvz ∈ψ1}.
Actually in ψ0 and in ψ1, the strings which have uv as a preﬁx are replaced by
strings obtained by ”cutting” the factor v.

Hiding Information in Multi Level Security Systems
265
– The new strategy f ′ is derived from f as follows:
For every ﬁnite path ω ∈P(f, r0, r1):
If u is not a preﬁx of ΠLowt(ω) or if ΠLowt(ω) = u then f ′(ω) = f(ω).
Otherwise let ω1 ∈PathLow the unique path such that ω = ω1ω2 and
ΠLowt(ω1) = u. Let q = last(ω1). Since State(u) = State(uv) there exists
a path θ(ω1) such that ΠLowt(θ(ω1)) = uv, last(θ(ω1)) = q, f(θ(ω1)) = f(ω1),
C(ω1) = C(θ(ω1)) and D(ω1) = D(θ(ω1)). The choice of θ(ω1) is not arbi-
trary, that is we respect both equivalences ≈High (note that ≈High is a reﬁnement
of ≈Low). More precisely if C(ω′
1) = C(ω1), then we choose a θ(ω′
1) such that
C(θ(ω′
1)) = C(θ(ω1)). Then deﬁne f ′(ω) = f(θ(ω1)ω2).
If ω ̸∈P(f, r0, r1), deﬁne f ′ in any way regular for relation ≈Low.
We prove that f ′ deﬁnes a low strategy, i.e. if ω′ ≈Low ω then f ′(ω′) = f ′(ω).
We have just to prove it in the case when u is a proper preﬁx of ΠLowt(ω).
If u is a proper preﬁx of ΠLowt(ω) then ω′ and ω can be written in a unique
way:
ω = ω1ω2 with ΠLowt(ω1) = u and ω1 ∈PathLow,
ω′ = ω′
1ω′
2 with ΠLowt(ω′
1) = u and ω′
1 ∈PathLow,
with ω1 ≈Low ω′
1.
So there exists a path θ(ω1) such that f ′(ω) = f(θ(ω1)ω2) and f ′(ω′) =
f(θ(ω1)ω′
2). Since θ(ω1)ω′
2 and θ(ω1)ω2 are ≈Low-equivalent we have f ′(ω) =
f(ω).
– Let us now deﬁne the new high strategies r′
i.
* Let ω ∈PathHigh ∩(Pref(path(f, ri))).
If u is not a preﬁx of ΠLowt(ω) or if ΠLowt(ω) = u then r′
i(ω) = ri(ω).
Otherwise we take the same decomposition ω = ω1ω2 as above and the same
θ(ω). Then deﬁne r′
i(ω) = ri(θ(ω)ω2).
* Let ω ̸∈PathHigh∩Pref(path(f, ri)), deﬁne r′
i in any way regular for relation
≈High.
We prove that r′
i deﬁnes a high strategy, i.e. if ω′ ≈High ω, then r′
i(ω′) = r′
i(ω). We
have just to prove it in the case when u is a proper preﬁx of ΠLowt(ω).
If u is a proper preﬁx of ΠLowt(ω) then ω′ and ω can be written in a unique way:
ω = ω1ω2 with ΠLowt(ω1) = u and ω1 ∈PathLow
ω′ = ω′
1ω′
2 with ΠLowt(ω′
1) = u and ω′
1 ∈PathLow
in such a way that ω1 ≈High ω′
1. So the path θ(ω1) is ≈High equivalent to θ(ω1), thus
θ(ω1)ω2 is ≈High equivalent to θ(ω1)ω′
2 and r′
i(ω) = ri(θ(ω1)ω2) = ri(θ(ω′
1)ω′
2)
= ri(ω′).
We have to prove now that r′
i is a winning strategy for f ′ with respect to ψ′
i.
Let g ∈StrS and ω = path(f ′, r′
i, g). Assume that Pref(ΠLowt(ω))∩ψ′
i = ∅. Let
u0 be the longest preﬁx of ΠLowt(ω) which is a preﬁx of ψ′
i.
There are two cases:
1. u is a preﬁx of u0
2. u0 is a proper preﬁx of u or u and u0 are incomparable for the preﬁx order
Case 1. We have to discuss according to ΠLowt(ω) = u0 or not.

266
D. Beauquier and R. Lanotte
If ΠLowt(ω) = u0, we write ω = ω0ω1ω′ with ΠLowt(ω0) = u, last(ω0) ∈QLow
and ΠLowt(ω0ω1) = u0 = uu′. The path ω0 is deﬁned in a unique way, and we choose
ω1 as short as possible.
Then ω0 is a preﬁx of path(f, ri, g). Moreover for every ω2 such that ω2 ≤ω1,
if ω0ω2 ∈PathLow then f ′(ω0ω2) = f(θ(ω0)ω2), and
if ω0ω2 ∈PathHigh then r′
i(ω0ω2) = ri(θ(ω0)ω2)
with ΠLowt(θ(ω0)) = uv.
Thus take a strategy g′ ∈StrS which is compatible with θ(ω0)ω1ω′. We have
that θ(ω′
0)ω1ω′ = path(f, ri, g′). Since uu′ is a proper preﬁx of ψ′
i then uvu′ is
a proper preﬁx of ψi. There exists a ﬁnite path θ(ω0)ω1ω2 in PathLow such that
f(θ(ω0)ω1ω2) = l ∈Lowt, B = Act(last(θ(ω0)ω1ω2)), ΠLowt(ω0ω1ω2) = uvu′,
and uvu′(l, B) is a preﬁx of ΠLow(θ(ω0)ω1ω′). But f(ω0ω1ω2)=l and g(ω0ω1ω2, l)=
g′(θ(ω0)ω1ω2, l). So uu′(l, B) must be a preﬁx of ΠLow(ω0ω1ω′). Contradiction.
If ΠLowt(ω) ̸= u0.
We write ω = ω0ω1ω′ with ΠLowt(ω0) = u, last(ω0) ∈QLow,ΠLowt(ω0ω1) =
u0 = uu′, and last(ω0ω1) ∈QLow. Paths ω0 and ω1 are deﬁned in a unique way. Let
B = Act(last(ω0ω1)), l = f ′(ω0ω1). Then uu′(B, l) is a preﬁx of ΠLowt(ω) and
uu′(B, l) is not a preﬁx of ψ′
i.
As before, take a strategy g′ ∈StrS which is compatible with θ(ω0)ω1ω′. We have
that θ(ω0)ω1ω′ = path(f, ri, g′). Since uu′(B, l) is not a preﬁx of ψ′
i then uvu′(B, l)
is not a preﬁx of ψi. But uvu′(B, l) is a preﬁx of ΠLowt(path(f, ri, g′)). Contradiction.
Case 2. We have u0 ∈Pref(ψi). Moreover for every ﬁnite preﬁx ωl of ω which is
in PathLow one has f ′(ωl) = f(ωl) and for every ﬁnite preﬁx ωh of ω which is in
PathHigh one has r′
i(ωh) = ri(ωh). So ω = path(f, ri, g). But since ri is a winning
strategy for f with respect to ψi, ΠLowt(ω) has a preﬁx u1 in ψi. So u0 ≤u1 and
u1 ∈ψ′
i. Contradiction with Pref(ΠLowt(ω)) ∩ψ′
i = ∅.
By lemma 10, to ﬁnd a 1-code, it is sufﬁcient to enumerate all the 1-codes. Hence we
have the following theorem.
Theorem 3. Let S be a system; it is decidable in triple exponential time to ﬁnd a 1-code
for S. If S is deterministic, then it is decidable in double exponential time.
6
A Case Study
Here is a classical example in which shared ﬁles stored in a cache can create a covert
timed channel ([12]).
Let a system with a disk of space M and a cache of space N. Obviously, we assume
that N ≤M. The security policy implemented by the system is that low and high
agents can only read ﬁles. To read a ﬁle from the cache costs tc time units. If the ﬁle is
not in the cache, beforehand it must be read from the disk and stored in the cache. This
operation costs td, and hence, to read a ﬁle not in the cache costs tc + td.
Consider the following policy of management of the cache: when a request for a ﬁle
not in the cache occurs, the ﬁle less recently refereed in the cache is overwritten.
With ReadLi (resp. ReadHwi) we denote the request of the low agent (resp. high
agent) for the ﬁle i. With NopH we denote the fact that the high agent requires no ﬁle.

Hiding Information in Multi Level Security Systems
267
- n

:
tick
X
X
X
X
X
X
X
y







9
ReadL1
ReadH1
n
1H
W
tick
n
1L

tick
?
NopH
n
-
ReadL2
-
tick3

*
tick3
H
H
H
H
H
H
H
Y
ReadL1
n

tick3








tick3

ReadH1
HHHHHHH
j
ReadH2
n
2H
W
tick
n
2L

tick
?
NopH
n
X
X
X
X
X
X
X
y
tick

:
XXXXXXX
z
ReadL2
ReadH2
Fig. 5. A cache model
Symbol NopH is necessary to allow the system to pass from high to low states, and
hence, to accept high/low requests.
In ﬁgure 5 we model the cache where M = 2, N = 1 tc = 1 and td = 2. With tickn
we denote the sequence of n ticks. State 1H (resp. 1L) is a high state (resp. low state)
representing the fact that ﬁle 1 is stored in the cache, and states 2H (resp. 2L) is a high
state (resp. low state) representing the fact that ﬁle 2 is stored in the cache. Initially the
cache contains the ﬁle 1 and to start the system needs 1 time unit.
Obviously, the high agent can transmit information by storing ﬁles in the cache and
hence by changing the answer of the system to a request of the low agent.
Let [tick] = (∅, tick) and [ReadL1] = ({tick, ReadL1, ReadL2}, ReadL1).
As an example, a 1-code is:
– ψ0 = {[tick][ReadL1]}
– ψ1 = {[tick][tick][tick][tick][ReadL1]}
Actually to realize ψ0 (resp. ψ1) the high agent must choose the request NopH (resp.
the request ReadH2) when the set of choices is {tick, NopH, ReadH1, ReadH2}. On
the other hand the low agent chooses always the request ReadL1.
We note that the system is deterministic and hence to check that {ψ0, ψ1} is a 1-code
can be done in polynomial time.
In this manner, we can we easily deﬁne a n-code, for any n ≥2. Actually, we have
shown how the high agent can communicate 1 bit to the low agent. This is possible
since the low agent can understand whether the ﬁle 1 is or is not in the cache. Hence,
an n-code can be written by considering a sequence of n requests of the low agent
for the ﬁle 1. Each set composing the n-code is a possible sequence of length n of
presence/absence of the ﬁle 1 in the cache. So, we have 2n possible cases. Hence,
to communicate a certain sequence of n bits, the high agent performs a sequences of
requests that respects the chosen sequence of length n of presence/absence of the ﬁle 1.
7
Conclusion
In this paper we have described how one can exploit and get round a multi level secu-
rity system to get a hidden covert channel. We have proved that it is decidable to check
whether a system allows a covert channel for a given n-code on one hand and on the

268
D. Beauquier and R. Lanotte
other hand to to check whether a system allows a covert channel for one bit of infor-
mation. This latter algorithm provides in case of a positive answer the 1-code and the
strategies to apply for the low and high user respectively.
As a future work we want to extend our second algorithm to the case of a n-code
which permits a hidden covert channel for a ﬁxed n. We conjecture also that this code
can be expressed as a regular expression which is a function of the number n of bits one
wants to transmit.
Instead of of a hierarchical concept of classes of users, a distributed one is also
relevant in some situations. In that case the rules are symmetric for high and low users,
and the high agent observes only high actions. The questions which are solved in this
paper are open under these hypothesis.
References
1. Andrews, G.R., Reitmans, R.P.: An axiomatic approach to information ﬂows in programs.
ACM transactions on Programming languages and Systems 2, 56–76 (1980)
2. Bell, D.E., La Padula, J.J.: Secure computer systems: mathematical foundations, Mitre tech-
nical report 2547, MITRE, vol. I (1973)
3. Bell, D.E., La Padula, J.J.: Secure computer systems: a mathematical model, MITRE techni-
cal report 2547, MITRE, vol. II (1973)
4. Bell, D.E., La Padula, J.J.: Secure computer systems: uniﬁed exposition and multics inter-
pretation, Mitre technical report 2997, MITRE, vol. I (1975)
5. Fisky, G., Fisk, M., Papadopoulos, C., Neil, J.: Eliminating Steganography in Internet Traf-
ﬁc with Active Wardens. In: Petitcolas, F.A.P. (ed.) IH 2002. LNCS, vol. 2578, pp. 18–35.
Springer, Heidelberg (2003)
6. Focardi, R., Gorrieri, R.: Classiﬁcation of Security Properties (Part I: Information Flow).
In: Focardi, R., Gorrieri, R. (eds.) Foundations of Security Analysis and Design. LNCS,
vol. 2171, pp. 331–396. Springer, Heidelberg (2001)
7. Focardi, R., Gorrieri, R., Martinelli, F.: Real Time information Flow Analysis. IEEE
JSAC 21, 20–35 (2003)
8. Goguen, J., Meseguer, J.: Security policies and security models. In: Proc. IEEE Symposium
on Security and Privacy Proceedings, pp. 11–20 (1982)
9. Gr¨adel, E.: Finite model theory and descriptive complexity. In: Finite Model Theory and Its
Applications, Springer, Heidelberg (2003) (to appear)
10. H´elou¨et, L., Jard, C., Zeitoun, M.: Covert channels detection in protocols using scenarios.
In: SPV 2003 (2003)
11. H´elou¨et, L., Zeitoun, M., Degorre, A.: Scenarios and Covert channels, another game. In:
Proc. of Games in Design and Veriﬁcation, ENTCS, pp. 93–116 (2005)
12. Lampson, B.: A note on the conﬁnement problem. Communication of the ACM 16, 613–615
13. Lowe, G.: Quantifying information ﬂow. In: Gollmann, D., Karjoth, G., Waidner, M. (eds.)
ESORICS 2002. LNCS, vol. 2502, pp. 18–31. Springer, Heidelberg (2002)
14. Martinelli, F.: Partial Model Checking and Theorem Proving for Ensuring Security Proper-
ties. In: Proc. of IEEE CSFW, pp. 44–52 (1998)
15. McHugh, J.: Covert Channel Analysis: A Chapter of the Handbook for the Computer Secu-
rity Certiﬁcation of Trusted Systems (1995), available at
http://chacs.nrl.navy.mil/publications/handbook/
16. Millen, J.K.: Finite-State Noiseless Covert Channels. In: Proc. of IEEE CSFW, pp. 81–86
(1989)

Hiding Information in Multi Level Security Systems
269
17. Moskowitz, I.S., Miller, A.R.: Simple Timing Channels. In: Proc. of IEEE Computer Sym-
posium on Research in Security and Privacy, pp. 56–64 (1994)
18. Moskowitz, I.S., Greenwald, S.J., Kang, M.H.: An Analysis of the Timed-Z Channel. In:
Proc. of IEEE Computer Symposium on Security and Privacy, pp. 2–11 (1996)
19. Murdoch, S.J., Zielinski, P.: Covert Channels for Collusion in Online Computer Games. In:
Fridrich, J. (ed.) IH 2004. LNCS, vol. 3200, pp. 355–369. Springer, Heidelberg (2004)
20. National Computer Security Center.: A Guide to Understanding Covert Channel Analysis of
Trusted Systems NCSC-TG-30 (1993), available at
http://www.radium.ncsc.mil/tpep/library/rainbow/
21. Sabelfeld, A., Myers, A.C.: Language-based information ﬂow security. IEEE Journal on se-
lected areas in communications, 21 (2003)
22. Son, S.H., Mukkamala, R., David, R.: Integrating Security and Real-Time Requirements
using Covert Channel Capacity. IEEE Trans. Knowledge and Data Eng. 12, 865–879 (2000)
23. Petitcolas, F.A.P., Anderson, R.J., Kuhn, M.G.: Information Hiding-A Survey. In: Proc. of
the IEEE Special issue on protection of multimedia content, vol. 87, pp. 1062–1078 (1999)
24. Volpano, D., Smith, G.: Eliminating covert ﬂows with minimum typings. In: Proc. IEEE CSF,
pp. 156–168 (1997)
25. Wang, Z., Lee, R.: New Constructive Approach to Covert Channel Modeling and Channel
Capacity Estimation. In: Zhou, J., Lopez, J., Deng, R.H., Bao, F. (eds.) ISC 2005. LNCS,
vol. 3650, pp. 498–505. Springer, Heidelberg (2005)
26. Zakinthinos, A., Lee, E.S.: A general theory of security properties. In: Proc. of IEEE Sym-
posium on Security and Privacy, pp. 74–102 (1997)
27. Shannon, C.E.: Communication theory of secrecy systems. Bell System Technical Journal 28,
656–715 (1949)
28. Simmons, G.J.: Contemporary Cryptology. In: The Science of Information Integrity, IEEE
Press, New York (1992)
29. Simmons, G.J.: The prisoners’ problem and the subliminal channel. In: Prod. of Workshop
on Communications Security, pp. 51–67. IEEE Press, Orlando, Florida (1984)
30. Simmons, G.J.: The history of subliminal channels. IEEE Journal of Selected Areas in Com-
munications 16, 452–462 (1998)
31. Anderson, R.J., Vaudenay, S., Preneel, B., Nyberg, K.: The Newton channel, Proc. of Work-
shop on Information Hiding. In: Anderson, R. (ed.) Information Hiding. LNCS, vol. 1174,
pp. 151–156. Springer, Heidelberg (1996)
32. Simmons, G.J.: Subliminal channels: Past and present. European Transaction on Telecom-
munications 5, 459–473 (1994)
33. van der Meyden, R., Wilke, T.: Synthesis of Distributed Systems from Knowledge-Based
Speciﬁcations. In: Abadi, M., de Alfaro, L. (eds.) CONCUR 2005. LNCS, vol. 3653, pp.
562–576. Springer, Heidelberg (2005)
34. Wittbold, J.T., Johnson, D.M.: Information ﬂow in nondeterministic systems. In: Proc. of
IEEE Symposium on Security and Privacy, pp. 144–161 (1990)
35. Wittbold, J.T., Johnson, D.M.: Results concerning the bandwidth of subliminal channels.
IEEE Journal of Selected Areas in Communications 16, 463–473 (1998)

T. Dimitrakos et al. (Eds.): FAST 2006, LNCS 4691, pp. 270–284, 2007. 
© Springer-Verlag Berlin Heidelberg 2007 
A New Trust Model Based on Advanced D-S Evidence 
Theory for P2P Networks* 
Chunqi Tian, Shihong Zou, Wendong Wang, and Shiduan Cheng 
State Key Laboratory of Networking and Switching 
Beijing University of Posts and Telecommunications 
Beijing, P.R. China, 100876 
tianchunqi@163.com, {zoush, wdwang, chsd}@bupt.edu.cn 
Abstract. Building trust relationship between peers in a large-scale distributed 
P2P file-sharing system is a fundamental and challenging research topic. 
Recommendation based trust mechanism is widely employed to establish the 
trust relationship. However, most existing approaches can not efficiently deal 
with inconsistent or conflicting recommendation information, and uncertainty of 
information. Dempster-Shafer (D-S) evidence theory is preponderant in tackling 
uncertainty of information, but classical combination rule always results in 
unreasonable results especially when evidences severely conflict each other. In 
this paper, we improve the combination rule for D-S evidence theory and develop 
a novel trust model based on it. For the problem of security, some measures are 
also proposed to defense against several malicious attacks. Experimental results 
show that the proposed model can significantly improve the successful 
transaction rate of P2P networks and effectively detect malicious behaviors in 
P2P networks.  
1   Introduction 
Peer-to-peer file-sharing networks have many benefits over standard client-server 
approaches to distributing data, including increased robustness, scalability. However, 
the open and anonymous nature of these networks leads to a complete lack of 
responsibility for the content a peer puts on the network, opening the door to abuses of 
these networks by malicious peers. Therefore, it is necessary to build trust relationship 
between peers in the distributed large-scale P2P networks. 
Trust represents an integrated evaluation of behavior and capacity of a peer in the 
viewpoint of another peer. Current researches on trust mostly focus on setting up the 
reliable trust management model for P2P networks. There exist a large number of trust 
models that commit themselves to computing a trust value for a source provider. Such 
systems typically assign each node a trust value based on the transactions it has 
performed with others. For example, XRep [1,2] provides a protocol complementing 
                                                           
*  This work was supported by the National Basic Research Program of China (Grant No. 
2003CB314806 and 2006CB701306), the National Natural Science Foundation of China (No. 
90204003 and 60472067). 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
271 
current Gnutella protocol by allowing peers to keep track of and share information 
about the reputation of other peers and resources. EigenRep proposed by Kamvar et al 
[3] is another reputation management system for P2P networks. Their trust evaluation 
stems from the concept of transitive trust. They assume the existence of a small number 
of trustworthy peers whose opinions on other peers are known to be trusted. The 
reputation-based trust mechanisms in P2P systems [5-7] are similar with social 
networks in which a peer issues a query for another peer’s reputation to its friends 
before they will transact and depends on its friends’ referrals to build trust relationship. 
However, there are some disadvantages in these schemes and we sum up as follows: 
first, though some methods [6,7] differentiate referrals’ reliability, they do not 
incorporate multi-characteristic to rate these referrals or only give some experiential 
evaluation without any theoretic bases; second, most existing reputation mechanisms 
need clear positive or negative rating ([1-3,5,7]) ,but this condition is difficult to 
achieve in actual networks; third, the certainty of trust relationship is only considered in 
these mechanisms, but in fact trust relationship is very complex and dynamic, so 
uncertainty of trust ought to be considered [15].    
With these research problems in mind, we develop a trust management framework 
for unstructured peer-to-peer systems, with emphasis on efficiently aggregating 
referrals which include conflicts and inconsistency, as well controlling such possible 
attacks and threats as denigration, collusion, and strategic attacks. Our contribution can 
be summarized in the following four aspects: 
First, we present an advanced D-S combination rule that can efficiently combine 
various evidences even though these evidences badly conflict each other. 
Second, we propose a decentralized trust model—AET2M, an Advanced D-S 
Evidence Theory based Trust Model. Furthermore, we address a feedback credibility 
based algorithm to effectively build a basic probability assignment function (BPA) of a 
peer and give a evaluation evidence of uncertainty. Moreover, some methods for 
resisting such malicious attacks as denigration, collusion and behavior oscillating are 
presented. 
Third, we present a set of experiments that show the effectiveness of our trust model 
in detecting malicious behavior of service providers. 
This paper is organized as follows. In section 2, we review some existing works. 
Section 3 presents the advanced D-S combination rule and AET2M.  Some experiment 
results are illustrated in section 4. In section 5, we conclude our work. 
2   Related Works 
XRep [1,2] proposed by Cornelli et al. adopts a binary rating system and is based on the 
Gnutella query broadcasting method using TTL limit. Their focus is to provide a 
protocol complementing existing P2P protocols, as demonstrated on top of Gnutella. 
However, there are no formalized trust metric and no experimental results in the paper 
validating their approach.  
Another work is EigenRep [3]. Their algorithm again focuses on a Gnutella like P2P 
file sharing network. They addressed the collusion problem by assuming there are peers 
in the network that can be pre-trusted. While the algorithm showed promising results 

272 
C. Tian et al. 
against a variety of threat models, we argue that the implementation of the algorithm is 
very complex and requires strong coordination and synchronization of peers.  
Wang and Vassileva [4] propose a Bayesian network-based trust model that uses 
reputation built on recommendations. They differentiate two types of trust: trust in the 
host’s capability to provide the service and trust in the host’s reliability in providing 
recommendations. 
Xiong and Liu [5] present a reputation-based trust supporting framework. They 
introduce three basic parameters and two adaptive parameters. They incorporate the 
concepts of a trust value and the similarity with oneself to compute credibility and 
satisfaction. 
Song [7] proposes a trust model that used fuzzy logic inference to compute the local 
trust value of a peer and aggregate the recommendation information. Fuzzy logic 
provides rules for reasoning with linguistic trust metrics. 
Yu and Singh present in [6] a mathematical theory of evidence to evaluate and 
spread the trustworthiness ratings of agents. Main contribution is to introduce evidence 
to express the local trust value of a given peer. However，there are a few disadvantages 
in this paper, for example, directly applying D-S evidence theory that can give rise to 
unreasonable results when evidences are not independent each other to combine 
referrals, not considering evidence reliability when the local trust values are aggregated 
and so on. 
3   Advanced D-S Evidence Theory Based Trust Model for P2P 
Networks 
D-S theory of evidence [8,9] is a framework for such purposes that have found 
applications in diverse areas such as expert systems, information retrieval, computer 
vision, pattern matching, and automatic target recognition and so on. The D-S theory 
could narrow down a hypothesis set with the accumulation of evidence and it allows for 
a representation of the “ignorance” due to the uncertainty in the evidence. When the 
ignorance reaches the value zero, the D-S model reduces to the standard Bayesian 
model. Thus, the D-S theory could be considered as a generalization of the theory of 
probability. Here for brief purpose we will not state D-S theory of evidence and its 
combination rule in detail. For expositions on the D-S theory, see the seminal 
monograph [9], also [10] and [11]. 
3.1   Advanced Combination Rule for D-S Theory of Evidence 
There exist many problems when directly using D-S theory of evidence to fuse data, as 
can be found by researches. The main reasons are as below: 
1. 
The combination condition is strict and require that evidences be independent 
each other. In P2P networks, there are always collusive peers who collude and 
depend on each other to disturb the P2P system and referrals from collusive peers 
are interrelated. Directly combining these evidences will easily induce 
unreasonable results. 
2. 
Focal element’s exploration arises easily and computation complexity increases in 
exponential means. 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
273 
3. 
Unable to deal with conflicting evidences and distinguish the scale of subset in 
which evidence is. 
Bearing those in mind, we take such steps as de-correlation, as can be seen in [16], to 
weaken these conditions and further apply them to model trust relationship between 
peers in P2P networks.  
The basic D-S combination rule can fuse evidences from different evidence sources. 
However, there exist some shortcomings especially when evidences conflict each other 
badly and the combination results may be unacceptable. Therefore, Yager [10] 
presented a new combination rule. In Yager’s combination rule, the probability of 
conflicting evidences is completely distributed to the unknown proposition. Yager’s 
combination rule is better than that of D-S evidence theory in capability. However, the 
combination results are not desirable in case of three or more evidences.  
A new combination formula was presented in [11], by defining evidence credibility 
and distributing conflicting information partly to unknown region, partly to weighted 
combination. Though it averted a conflict, the result was not all-sided since it discarded 
the conflicting information which was sometimes very important. Absorbing method 
was proposed in [12], distributing conflicting probability to maximal focal element of 
BPA and not considering the effect of conflict on other focal elements. Therefore, it 
could produce an illogical result. 
We point out that in the combination rule conflicting evidences are partial useful 
even though they badly conflict each other, and further conflicting information can not 
be discarded simply. Based on the opinion, we put forward an efficient combination 
rule depicted as formula (1), that is, the conflicting probability that evidences support is 
distributed to every proposition according to its average supported degree. The new 
combination rule improves the reliability and rationality of the combination results. 
Even though evidences conflict one another highly, good combination results are also 
obtained. 
⎪⎩
⎪⎨
⎧
Φ
≠
∀
⋅
+
=
=
Φ
∑∏
=
≤
≤
iA
A
n
i
i
i
A
A
f
k
A
m
A
m
m
I
1
),
(
)
(
)
(
0
)
(
. 
(1) 
Where 
∑∏
∑∏
Φ
≠
≤
≤
Φ
=
≤
≤
−
=
=
i
i
A
n
i
i
i
A
n
i
i
i
A
m
A
m
k
I
I
0
0
)
(
1
)
(
, 
e
n
i
i
S
A
m
A
f
∑
≤
≤
= 1
)
(
)
(
, 
∑
≤
≤
=
n
i
i
e
X
m
S
1
)
(
, X is 
the collect of all the focal element of n evidences. 
Aiming at the advanced combination rule, we have done a large number of 
experiments. The results shows that in comparison with some existing rules our 
combination rule not only converges at consistent information quickly but also yields a 
more reasonable consequence, no matter how much the number of evidences is or the 
number of focal elements in the evidences is.  
3.2   Description of Downloading for P2P Networks 
Before depicting our model, we list three principles for the design. First, Peer will 
always trust itself. Second, bad behavior makes the trustworthiness value drop faster 
and good behavior increases the value slower. Third, it will be bad peer prone if a peer 
continually behaves badly. 

274 
C. Tian et al. 
P2P networks are overlaying networks that consist of a large number of nodes. For 
convenience, we use file-sharing systems as example in the whole paper. In the 
following, we call the peer to request file and rate other peers rater, the peer to response 
and be evaluated ratee, and the peer that sends the trustworthiness value of the known 
peers to others the recommender. 
According to the quality of services provided by cooperating peers (ratee), we 
classify services into four categories, as shown in Table 1. We formalize the quality set 
as Q = {G, C, I, M}. This coarse-grain classification is flexible enough to apply to any 
resource sharing. More subclasses can be introduced if necessary. Both I and M 
services are bad services, and will cause the rater to decrease the ratee’s score. 
Table 1. Description of file downloading 
File Quality 
Description 
G（Good） 
The file is as good as expected. 
C（Common） 
The file is correct, but with some degradation 
I（Inauthentic） 
The file is inauthentic 
M（Malicious） 
The file is malicious (e.g. virus or Trojan Horse) 
In a reputation system, a peer makes decisions based on its experience and other 
peers’ referrals. The peer i rate another peer j after they directly transact with each 
other. Local trust value of j is evaluated by i in the light of the quality of files that j have 
provided. But to evaluate the trustworthiness of j comprehensively, i can not rely on 
only direct experience. In a referral process, i issues a query for j’s reputation, other 
peers who have interacted with j— termed recommenders—may response to query and 
give their feedbacks that are interaction experience with j to i. I then can incorporate the 
knowledge of other peers according to its acquaintanceship degree to them. 
3.3   Credibility 
Trustworthiness of a peer is a temporal value, because the behavior of the peer will 
change dynamically. The old trustworthiness value may totally misrepresent one peer’s 
quality after some time. To solve this problem, an adaptive time window is used in this 
paper to depict peer’s transaction behavior. Only the behaviors of the ratee inside the 
window are taken into consideration. With the window shifting forward, (we suppose 
that the forward window end-time is backward window start-time), the value reflects 
the fresh statistics of the ratee’s recent behaviors. The window size plays an important 
role in the credibility calculation. The smaller the window size is, the more the 
shorter-term assessment is favorite by the trustworthiness calculation. 
In our model a peer locally stores the proportion lists of four types of files (G, C, I, 
M) downloaded from the peers that it has interacted with. For instance, if peer i has a 
number of interactions with peer j during the period
]
,
[
]
,
[
2
1
n
end
start
W
W
W
t
t
L
=
, where 
k
W (
n
k ≤
≤
1
) denote the kth time window, the first transaction must happen inside 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
275 
1
W and current transaction lies in
n
W . Suppose the transaction results of peer i with peer 
j inside
k
W is described as
)}
(
),
(
),
(
),
(
{
M
r
I
r
C
r
G
r
k
ij
k
ij
k
ij
k
ij
. 
It is indispensable to differentiate the effect of transaction period on computing the 
trust value of a peer in current trust models since individual behavior changes over 
time. To solve the problem, a time based evaluation method that fresher interactions are 
more important than old ones is adopted widely, namely, assigning more weights to 
recent interactions and less weight to previous interactions. We present a decay 
function to achieve the same goal, furthermore, decay function is more operable, more 
easy controllable, more flexible than weight application.  
Decay function f is in fact a timing discount function and is depicted as 
n
k
f
k
f
k
n
k
≤
≤
<
<
=
=
−
0,1
0,
)
(
ρ
ρ
, where 
kf  is a function value, also is decay factor for 
the kth time window. As can be seen from the definition of decay function, the weight 
for the first interaction is
1
1
−
=
n
f
ρ
, that is, decay degree is maximal; the current 
interaction’s weight is
1
=
nf
, namely, decay degree is 0. 
Suppose the interaction result of peer i and peer j for the current time window  
n
W  is 
)}
(
),
(
),
(
),
(
{
M
r
I
r
C
r
G
r
n
ij
n
ij
n
ij
n
ij
, if for the time window 
1
−
n
W
, the one is 
)}
(
),
(
),
(
),
(
{
1
1
1
1
M
r
I
r
C
r
G
r
n
ij
n
ij
n
ij
n
ij
−
−
−
−
, incorporating both the results using decay function f is  
(
)
(
)
⎭⎬⎫
⎩⎨⎧
⎟⎠
⎞
⎜⎝
⎛
+
⎟⎠
⎞
⎜⎝
⎛
+
+
+
−
−
−
−
−
−
−
−
)
(
1
),
(
1
),
(
),
(
1
1
1
1
1
1
1
1
M
r
f
r
I
r
f
r
C
r
f
r
G
r
f
r
n
ij
n
n
ij
n
ij
n
n
ij
n
ij
n
n
ij
n
ij
n
n
ij
. Note that 
decay function act as penalty function which is represented by the reciprocal of decay 
factor 
1
1
−
nf
)1
(>
when I and M happen. In the same way, incorporating all interactions 
results is
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
∑
∑
∑
=
=
=
=
)
(
1
),
(
1
),
(
),
(
1
1
1
1
M
r
f
I
r
f
C
r
f
G
r
f
n
k
k
ij
k
n
k
k
ij
k
k
ij
n
k
k
k
ij
n
k
k
. 
Credibility is the percentage of the rate of diverse files to overall rate. As to above 
example, credibility in
M
I
C
G
,
,
,
is respectively 
GCIM
k
ij
n
k
k
S
G
r
f
)
(
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
=
, 
GCIM
k
ij
n
k
k
S
C
r
f
)
(
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
=
, 
GCIM
n
k
k
ij
k
S
I
r
f
)
(
1
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
=
,
GCIM
n
k
k
ij
k
S
M
r
f
)
(
1
1
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
=
 where 
)
(
1
)
(
,
1
,
1
T
r
f
T
r
f
S
n
M
I
T
k
k
ij
k
k
ij
n
C
G
T
k
k
GCIM
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
+
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
=
∑
∑
=
=
=
=
. 
In our model, credibility is computed when a peer receives a query for referral about 
another peer, furthermore, in the current time window if other peers request the 
referrals about the same peer, the computed credibility is valid. Upon acquiring the 
credibility in
M
I
C
G
,
,
,
, we may build BPA function of a peer using it. 
 
3.4   Modeling BPA Function of a Peer  
We have classified file quality into four categories
M
I
C
G
,
,
,
 and so the frame of 
discernment is
}
,
,
,
{
M
I
C
G
=
Θ
. When peer i has transacted with peer j, we assume that 
its credibility in 
Θ
∈
j
θ
（
4,3,2,1
=
j
）is 
ij
α , satisfying 
1
4
1
=
∑
=
j
ij
α
. 

276 
C. Tian et al. 
Using the credibility, we can model the support degree of peer i to
j
θ , moreover, the 
latter should be a single increasing function of the former. We simplify the relationship 
into a linear function: 
ij
i
j R
m
λα
θ
=
)
}
({
,
1
0
≤
< λ
. 
(2) 
Because 
ij
α
 satisfies the condition
1
4
1
=
∑
=
j
ij
α
, the constraint
1
)
}
({
4
1
≤
∑
=
j
i
j R
m θ
is 
tenable.   
In order to turn m into a BPA function, we must complement a definition: 
∑
=
−
=
Θ
4
1
)
}
({
1
)
(
j
i
j
i
R
m
R
m
θ
. 
(3) 
Therefore, 
)
(
iR
m •
 function defined by formulae (2) and (3) is a basic probability 
assignment function whose focal elements include at most 
}
{
j
θ
(
4,3,2,1
=
j
) and 
discernment frame Θ . 
We will simplify the BPA function of a recommender. Suppose 
)
}
({
i
j
j
i
R
m
m
θ
=
, 
)
(
0
i
i
R
m
m
Θ
=
for a recommender
iR , 
r
i
L
,2,1
=
,
Θ
∈
j
θ
, 
4,3,2,1
=
j
, so the BPA 
functions of such r recommenders can be described by the following matrix: 
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
4
1
0
4
2
1
2
0
2
4
1
1
1
0
1
r
r
r
m
m
m
m
m
m
m
m
m
M
L
M
L
L
. 
(4) 
Where the ith row denote the ith recommender’s BPA function, 
iR , 
r
i
L
,2,1
=
. 
3.5   Evidence Inference Algorithm and Combination Method 
Now we will combine such r BPA functions into an integrated function. First of all, 
de-correlation of evidences must be executed so as to be independent of each other. 
Being characteristic of a common property, these BPA functions include at most such 
focal elements as 
}
{
j
θ
，
Θ
∈
j
θ
（
4,3,2,1
=
j
）and discernment frame Θ . Therefore, 
our combination algorithm is a lineal increasing computation, which efficiently 
decreases computation complexity and also preferably solves the problem of the 
explosion of focal elements. 
Suppose the collection of all the recommenders is
r
i
R
R
R
i
I
i
L
L
,2,1
},
,
,
{
)
(
2
1
=
=
, so 
the former i rows in the recommendation matrix can be depicted as below:  
))
(
}
({
)
(
i
I
m
m
j
j
i
I
θ
=
,
Θ
∈
j
θ
,
4,3,2,1
=
j
. 
(5) 
))
(
(
0
)
(
i
I
m
m
i
I
Θ
=
,
4,3,2,1
=
j
. 
(6) 
Combining these BPA functions by the advanced combination rule, we can educe 
the following formula:  

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
277 
)
(
)
(
)
(
}
{
1
)
(
j
i
I
i
I
A
i
t
A
t
j
i
I
f
k
m
m
j
t
t
θ
θ
⋅
+
= ∑∏
=
≤
≤
I
,
Θ
∈
j
θ
,
4,3,2,1
=
j
 . 
(7) 
)
(
)
(
)
(
1
0
0
)
(
Θ
⋅
+
= ∏
≤
≤
i
I
i
I
i
t
t
i
I
f
k
m
m
 . 
(8) 
Where 
∑∏
Φ
=
≤
≤
=
t
t
A
i
t
A
t
i
I
m
k
I
1
)
(
, 
∑
∑
+
=
=
=
1
1
1
)
(
)
(
i
t
X
t
i
t
j
t
j
i
I
m
m
f
θ
, X is the collection of all focal 
elements of 
}
,
,
{
)
(
2
1
iR
R
R
i
I
L
=
. When 
r
i =
 comes into existence, we will obtain a 
combined BPA function denoted as m, so 
j
r
I
j
m
m
)
(
})
({
=
θ
, 
0
)
(
)
(
r
I
m
m
=
Θ
, 
Θ
∈
j
θ
, 
4,3,2,1
=
j
. This m function is just the combined BPA function of all the 
recommendation evidences. 
Because the focal elements of referral evidences are only the single element and 
discernment frame in our P2P trust model, the complexity of referral information is 
)
(
)1
(
ri
O
i
r
=
+
⋅
 (i denote the cardinality of Θ ) if there exist r evidences .That is to say, 
information complexity is a linear relationship with the number of evidences. 
The computation complexity of combining r evidences is
)
(
)
(
2
2
ri
O
i
O
r
=
⋅
, that is, 
computation complexity is also a linear relationship with the number of evidences. 
3.6   Local Rating  
The recommendation based P2P trust models always include two steps: one is local 
rating, the other is aggregating ratings. A peer’s local rating about another peer is based 
on direct interactions each other. The local rating is generated every time when an 
interaction takes place. We describe both the ratings in detail in the following sections. 
In AET2M, the local rating of peer j in the viewpoint of peer i is expressed in the 
form of an evidence that i supports in term of quality of the files downloaded from peer 
j. For example, the local rating is
)}
(
),
(
),
(
),
(
),
(
{
Θ
=
=
ij
ij
ij
ij
ij
ij
ij
m
M
m
I
m
C
m
G
m
m
R
, where 
m is a BPA function over
}
,
,
,
{
M
I
C
G
=
Θ
.  
3.7   Aggregating Ratings and Trust Value Evaluating Method 
Upon receiving overall referrals, rater will fuse them into a specific evidence by 
advanced combination rule. However, main challenge is that recommenders’ reputation 
is different and the referral from the peer with high reputation is more reliable than that 
with low reputation. Therefore, we should differentiate these referrals and assign 
different weights to them. We think that the first few peers to join a network are often 
known to be trustworthy, since the designers and early users of a P2P network are likely 
to have less motivation to destroy the networks they build. (These peers form a 
collective named pre-trusted peers). If the referrals come from these peers, they are 
regarded reliable. For convenience, suppose the weights of the referrals from 
pre-trusted peers are
k
w  and other referrals’ weight is 
iw (
k
i
w
w <
), moreover, the 
lower the reputation value of a peer is, the less the weight of its referral is. We define 
the peers’ BPA function as: 

278 
C. Tian et al. 
,
)
}
({
ij
k
i
i
j
w
w
R
m
λα
θ
=
∑
=
−
=
Θ
4
1
)
}
({
1
)
(
j
i
j
i
R
m
R
m
θ
,
4,3,2,1
=
j
 . 
(9) 
On obtaining the BPAs of all referrals, peer i (rater) will combine these evidences in 
terms of formula (7). 
The combined evidence is not a specific value but a joint support list 
for
Θ
,
,
,
,
M
I
C
G
and further we have found through a large number of experiments that 
combined evidence offers very little support to Θ . In our model, the evaluation rule for 
the trust value of peer j (ratee) in the viewpoint of peer i (rater) is in the light of relative 
degree of support which combined evidence offers to
Θ
,
,
,
,
M
I
C
G
. The details are as 
follows: (suppose the degree to which combined evidence supports 
Θ
,
,
,
,
M
I
C
G
is 
d
c
b
a
,
,
,
respectively). 
1. If the degree of support that combined evidence offers to I and M is not beyond a 
threshold (
M
I
Threshold
Threshold
>
), the peer is thought of as a trustworthy node; 
otherwise an untrustworthy one.  
2. If there are two credible peers, their trust value is judged by the support degree of 
G and C according to
η
b
a +
（
1
>
η
）. The larger 
η
b
a +
 is, the higher the trust 
value of the peer is. 
3. When both the peers are credible and further the support degree of G and C is 
same, the following result is obvious: the lager 
d
c
γ
+
(
1
>
γ
）is, the less credible 
the peer is.  
As can be seen from above, we can conclude that the way in which the trust value of 
a peer is rated is flexible and multi-scale in comparison with some existing trust model 
in which the trust value of a peer is defined as a specific, single value. 
3.8   The Approach to Resisting Denigration, Collusion and Oscillating  
Denigration will happen when this type of malicious peers are asked for the trustworthy 
of these peers with which they have transacted and they always provide a untrue, 
negative ratings for the peers.  
Malicious peers form a malicious collective by assigning a high trust value to 
another malicious peer in the network. Collusive peers provide inauthentic files to peers 
outside when selected as download source and provide denigrated ratings for these 
peers. 
Compared with some existing models, trust value of a peer is not a specific value but 
a combined evidence that peers, ever having transacted with it, support its transaction 
behavior in AET2M. Furthermore, the evaluation of trust value is elastic and 
multi-scale. We have found that a small quantity of denigrations or exaggerations have 
a slight effect on an established result through a large number of experiments. This fact 
also embodies the predominance of D-S theory of evidence in soft evaluation aspect. 
We have taken several measures to defense against denigration and collusion in the 
following. 
Firstly, we de-correlate the evidences from all the recommenders before combining 
them. The reasons lie in two aspects: one is that combination rule require that evidences 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
279 
should be independent of one another, the other is that we aim at prevent peers from 
colluding.  
Secondly, we pre-treat all evidences before combining them in accordance with their 
similarity. If similarity degree of an evidence with the combined evidence is less than a 
given threshold, the evidence is regarded as an invalid one and is discarded. 
Thirdly, we differentiate the referral according to a recommender’s reputation. As 
shown in formula (9), if a recommender is a discredited one, its referral is discounted, 
that is to say, the lower the recommender’s reputation is, the more the referral is at a 
discount. Therefore, the low-reputation peer denigrates its competitor or exaggerates its 
complice very difficultly.  
Finally, we prescribe that if there are some evidences which support I or M type of 
files beyond a given threshold, rater may ask ratee for transaction records. By checking 
the records, rater will ignore the referral if it finds I or M rating is given to ratee 
frequently. In addition, if rater has performed a large number of transactions with rate, 
and furthermore referral information is different from direct transaction records, rater 
may neglect the referral and think much of direct trust. 
We can efficiently restrain malicious peers from denigrating and colluding by means 
of these measures and favorable effect is found in the consequent simulations. 
There are more realistic malicious peers that are knowledgeable of the rules of the 
game and may adaptively decide on a strategy in order to maximize their expected 
payoff in P2P networks. There are a number of ways in which such peers can attempt to 
fool the system and obtain higher payoffs. For example, they can build a good 
reputation and then start cheating occasionally at a rate that gives them a higher payoff, 
but still allows them to maintain an acceptable reputation. Or, they could oscillate 
between periods of building and then milking their reputation. 
To cope with such potential dynamic behaviors of peers, our adaptive time window 
based algorithm to better react to the oscillating behaviors. The basic idea is to 
adaptively use a smaller time window to reflect the peer’s most recent behavior when 
the peer is dropping its performance over a threshold. Concrete steps are depicted as 
below (suppose peer i, rater, is transacting with peer j, ratee). 
Firstly, peer i must check whether I and M types of files uploaded by peer j exceed a 
given threshold in the current time window win or not.  
Secondly, if the rate 
M
I r
r ,
of I and M files uploaded by peer j in the current time 
window win exceed each threshold (both thresholds are different and in general  
M
I
r
r
Threshold
Threshold
>
). Rater will computes another rate 
'
', M
I r
r
 using a recent subset  
of the feedback taken by a small time window 
s
W  smaller than W . The second value, 
'
Ir  or 
'
M
r , will be returned as the final value if it is larger than the first value by a certain 
threshold, which likely indicates the peer is dropping its performance recently. 
Otherwise, the first value,
M
I r
r ,
will be returned.  
By choosing a proper adaptive small window, this method can detect the abnormity 
of peer behavior in time, and make the credibility of the peer drop quickly as long as 
few fake transactions happen. Simultaneity, it makes the credibility of a peer hard to 
build, easy to drop, namely, the credibility cannot be quickly increased by a small 
number of good transactions, but it will quickly drop if the peer starts cheating.  

280 
C. Tian et al. 
4   Experiment Results  
In this section, we will assess the performance of our scheme as compared to a P2P 
network where YuBin scheme and EigenRep is implemented. We shall demonstrate the 
scheme’s performance under a variety of threat models. All three schemes are 
implemented based on Query Cycle Simulator [13,14]. There are 100 query cycles in 
one experiment and the results are averaged over 3 runs. 
4.1   Simulation Environment 
In each query cycle, a peer i in the network may be actively issuing a query, inactive, or 
even down and not responding to queries passing by. Upon issuing a query, a peer waits 
for incoming responses, selects a download source among those peers that responded 
and starts downloading the file until gets the authentic file or tries all the download 
sources. Then the query cycle finishes and the data is collected. 
In simulation we assume that there are 1000 peers in the network among which 
malicious peers vary between 100 and 500 and the query message is flooded with 
TTL=5. Normal peers are in the uptime with the uniform random distribution over  
[0%, 100%] and issue queries in the uptime with the uniform random distribution over 
[0%, 50%], while malicious peers are always up and always issue queries. In addition, 
different types of peers also vary in their behavior of responding queries and providing 
files. For good peers, the probability to provide authentic files is 96%，while simple 
malicious peers will respond to all queries they have received and provide inauthentic 
files with the probability of 70% for all download requests. To tackle node overload 
problem, we prescribe that ratio of request download from a peer with high trust value 
is 80%. 
The content distribution model is the same as that in [14]. In this model each file is 
characterized by the content category c and the popularity rank r within this category. 
c and r both follow the Zipf distribution. Distributions used in the model are based on 
the measurement of real-world P2P networks. Normal peers issue queries in 
accordance with their interests while malicious peers issue queries randomly just to 
harm other peers or disturb the system. In our simulation environment, there are 5000 
numbers of files in all and 50 content categories are hold in the network. 
In our experiments, we consider different threat models, where a threat model 
describes the behavior of a malicious peer in the network. Malicious peers’ behavior 
will be described as follows: 
z 
Simple malicious peers. This type of malicious peers always provide an 
inauthentic file when selected as download source, we call these SM. 
z 
Denigrating peers. This type of malicious peers always denigrates the peers 
with which they have transacted and provide a denigrated ratings when 
asked for the trustworthy of these peers .we call these DM. 
z 
Collusive peers. Malicious peers are explained in Section 3.8. Collusive 
camouflage is a fact that collusive peers provide an authentic file to good 
peers outside the collective in a probability when selected as download 
source in an attempt to hide their malicious intention, we call these CC. 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
281 
z 
Strategic peers. This type of malicious peers can build a good reputation and 
then start cheating occasionally at a variable rate, but still allows them to 
maintain an acceptable reputation. Or, they could oscillate between periods 
of building and then milking their reputation.  
4.2   Successful Transaction Rate (STR) 
We compare the successful transaction rate (STR) of our scheme with YuBin scheme 
and EigenRep under SM, DM, Collusive, CC and strategy. The metrics, successful 
transaction rate, is ratio of the number of successful transaction over overall transaction 
numbers, is used to evaluate the efficiency of trust model. 
0.0
0.1
0.2
0.3
0.4
0.5
0.5
0.6
0.7
0.8
0.9
1.0
Successful Transaction Rate
The Fraction of Malicious Peers
 AET
2M SM
 AET
2MDM
 YuBin SM
 YuBin DM
0.0
0.1
0.2
0.3
0.4
0.5
0.7
0.8
0.9
1.0
AET
2M SM
 AET
2M DM
 EigenRep SM
 EigenRep DM
Successful Transaction Rate
The Fraction of Malicious Peers
 
Fig. 1. STR under SM and DM                Fig. 2. STR under SM and DM 
Simple Malicious Peers and Denigrating Peer. The successful transaction rates of 
three models under SM and DM are shown in Fig.1 and Fig.2. When there is no 
malicious peer in the system, STR of three schemes are all 96%. With the fraction of 
malicious peers increasing, STRs of three schemes descend, but our model descends 
most slightly. YuBin scheme drop rapidly and STR is only about 55% when the 
fraction of malicious peers is 50%. This is because on the one hand it directly uses 
original D-S combination rule to combine evidences, on the other hand it does not 
distinguish transaction date and does not punish malicious behavior, so its computation 
of trust value is rough. It can possibly regard bad peers from which to download file as 
good peers by error, which get invalid transaction numbers increase inevitably. Not 
taking into consideration under SM that malicious peers provide authentic file in 
certain probability, EigenRep can not punish these peers and therefore the successful 
transaction rates fall more heavily. In comparison with YuBin scheme and EigenRep, 
the successful transaction rates of our model remain high all long when the fraction of 
malicious peers is 50%. 
Collusive Malicious Peers. The successful transaction rates of three models under 
Collusive are shown in Figure 3, and total number of malicious peers performing 
collusion was increased in steps of 10%. The successful transaction rate in EigenRep, 
which does not take into account collusive attacks, descends evidently with malicious 
peers increasing. YuBin scheme’s STR is falling almost linearly and STR is only 20%  
 

282 
C. Tian et al. 
when the rate of collusive peers arrives at 50%. Compared to both schemes, our trust 
model is designed to tackle collusive attacks, therefore to large extend is proved robust 
against collusive cheating. 
0.0
0.1
0.2
0.3
0.4
0.5
0.2
0.4
0.6
0.8
1.0
Successful Transaction Rate
The Fraction of Collusive Peers
 AET
2M
 EigenRep
 YuBin
0.0
0.2
0.4
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0
 AET
2M
 EigenRep
 YuBin
Successful Transaction Rate
Malicious Rate of Malicious Peers
 
Fig. 3. STR under Collusive                         Fig. 4. STR under CC 
Figure 4 shows an interesting result. For this experiment, fraction of malicious 
providers in the system was fixed to 25%. The fraction of transactions in which 
malicious providers offer inauthentic services was varied, starting from 0%, increasing 
in steps of 10%. However, in a CC system we can see that their attack will be more 
effective when they provide inauthentic services intermittently. When the services are 
always inauthentic, reports on malicious providers’ behavior between good providers 
and malicious providers will be entirely polarized. If malicious providers offer good 
services frequently, reports issued by good peers and malicious peers will have less 
difference compared to the previous case. We believe this may increase the effectiveness 
of their attack. As we can see in Figure 4, YuBin system suffers the most under CC. As 
malicious providers offer inauthentic services more frequently, however, EigenRep and 
our system tend to perform better. This is because when malicious peers provide 
inauthentic files in a low probability, they can gather high trust even from good peers, 
namely, trust mechanism can not distinguish whether the peers are good or not. From 
this experiment we can find that our scheme is not sensitive to the fraction of inauthentic 
services that collusive peers provide in the system and so is influenced mildly.  
Strategic Peers. We also compare the successful transaction rate of the system with the 
complex strategy peers with EigenRep and YuBin scheme. Considering the operability 
of the experiment, we simplify this type of strategic peers, whereas the case is 
representative. In simulation, suppose the peer whose trust value is less than 0.5 is 
untrustworthy, and a strategic peer provides true files in a probability of 20% when  
its trust value is beyond 0.6 and in 60% probability when its trust value is less than 0.6.   
4.3   The Efficiency of Resistance Against Oscillating Peers 
To address dynamic behaviors of peers, we propose a simple adaptive small-window 
based algorithm to better react to the behaviors. The goal of this experiment is to show 
how adaptive small-window algorithm works against strategic dynamic personality of 
peers. We simulated a community with 200 good peers, but a malicious peer with 

 
A New Trust Model Based on Advanced D-S Evidence Theory for P2P Networks 
283 
oscillating behavior. We simulated a changing pattern in which the peer first builds its 
reputation, then performs fake transactions with others peers and so its reputation is 
milked, finally it again tries to retrieve its reputation as much as before.  
The experiment proceeds as peers randomly perform transactions with each other 
and a good peer is selected to compute the trust value of the malicious peer periodically. 
We compare adaptive small window algorithm to basic approach that uses a fixed time 
window. 
0.0
0.1
0.2
0.3
0.4
0.5
0.5
0.6
0.7
0.8
0.9
1.0
 AET
2M
 EigenRep
 YuBin
Successful Transaction Rate
The Fraction of Strategy Peers
0
100
200
300
400
500
0.2
0.4
0.6
0.8
1.0
Computed Trust Value
Number of Transaction
 Basic
 AET
2Ml Adaptive
 
                    Fig. 5. STR under strategy                  Fig. 6. The trust value of an oscillating peer 
Fig.6 shows the computed trust value of peer u by adaptive metric and basic metric 
against the type of changing pattern. In the basic model, the peer gains by milking the 
reputation, but it also has to pay the same amount of cost to build it back. In our model 
adaptive, a peer is quickly detected for its bad behavior, but it cannot simply increase its 
trust value quickly by acting well for a short period so the cost of rebuilding reputation 
is actually higher than the gain of milking it.  
5   Result 
The paper presents a framework for trust management in distributed P2P networks. By 
virtue of an advanced D-S combination rule, we efficiently enhance the aggregating 
performance especially when referrals include conflicting or inconsistent information. 
Further, we address some methods for resisting such malicious attacks as denigration, 
collusion and behavior oscillating. The simulation results show that the proposed trust 
model can improve the successful transaction rate as well as help malicious peer 
detection and can be applied to a large-scale distributed system.   
References 
1. Damiani, E., di Vimercati, S.D.C., Paraboschi, S., Samarati, P.: A reputation-based approach 
for choosing reliable resources in peer-to-peer networks. In: Proceedings of the 9th ACM 
conference on Computer and communications security, pp. 207–216. ACM Press, NewYork 
(2002) 
2. Cornelli, F., Damiani, E., di Vimercati, S.D.C., Paraboschi, S.: Choosing reputable servents 
in a P2P network. In: Proceedings of the 11th Int’l World Wide Web Conf., pp. 441–449. 
ACM Press, Hawaii (2002) 

284 
C. Tian et al. 
3. Kamvar, S.D., Schlosser, M., Garcia-Molina, H.: The EigenTrust Algorithm for Reputation 
Management in P2P Networks. In: Proceedings of the 12th International conference of 
WWW, Budapest, Hungary (2003) 
4. Wang, Y., Vassileva, J.: Trust and Reputation Model in Peer-to-Peer Networks. In: 
Proceedings of the Third International Conference on Peer-to-Peer Computing(P2P’03) 
(2003) 
5. Xiong, L., Liu, L.: PeerTrust: Supporting Reputation-Based Trust for Peer-to-Peer 
Electronic Communities. IEEE Transactions on Knowledge and Data Engineering 16, 
843–857 (2004) 
6. Yu, B., Singh, M.P.: An Evidential Model of Distributed Reputation Management. In: 
Falcone, R., Barber, S., Korba, L., Singh, M.P. (eds.) AAMAS 2002. LNCS (LNAI), 
vol. 2631, Springer, Heidelberg (2003) 
7. Song, S.S., Hwang, K., Zhou, R.F.: Trusted P2PTransactions with Fuzzy Reputation 
Aggregation. IEEE Internet Computing 2, 18–28 (2005) 
8. Dempster, A.P.: Upper and lower probabilities induced by a multi—valued mapping. Ann 
Mathematical Statistics (1967) 
9. Shafer, G.: A mathematical theory of evidence, pp. 10–28. Princeton University Press, 
Princeton, NJ (1976) 
10. Yager.: On the Dempster-Shafer framework and new combination rules. Information 
Science 41, 93–137 (1989) 
11. Sun, Q., Ye, X.Q., Gu, W.K.: A new composition method for evidence theory. Chinese 
Journal of Electronics 28, II7–119 (2002) 
12. Xu, H., Semt, P.: Some strategies for explanations in evidence reasoning. IEEE Trans.on 
Syst.,Man and Cybern Part A: Systems Human 28, 599–607 (1996) 
13. http://p2p.stanford.edu/www/demos.htm  
14. Schlosser, M., Condie, T., Kamvar, S.: Simulating a File-Sharing P2P Network. In: First 
Workshop on Semantics in P2P and Grid Computing (December 2002) 
15. Sun, Y., Han, Z., Yu, W., Ray, K.J.: A trust evaluation framework in distributed network: 
vulnerability analysis and defense against attacks. In: IEEE Infocom 2006, Barcelona, Spain 
(2006) 
16. Zhang, D.G., Xu, G.Y., Shi, Y.C.: Extended method of evidence theory for pervasive 
computing. Chinese Journal of Computers 27, 918–927 (2004) 

Author Index
Basin, David
80
Beauquier, Dani`ele
250
Betarte, Gustavo
220
Bistarelli, Stefano
1
Borgstr¨om, Johannes
16
Brændeland, Gyrd
31
Cheng, Shiduan
270
Dall’Aglio, Marco
1
Daubignard, M.
47
Deng, Yuxin
65
Grinchtein, Olga
16
Hankes Drielsma, Paul
80
Hansen, Ren´e Rydhof
127, 170, 185
Heather, James
202
Janvier, R.
47
Jensen, Christian Damsgaard
170
Kramer, Simon
16
Krukow, Karl
96
Lakhnech, Y.
47
Lanotte, Ruggero
250
Luna, Carlos
220
Matteucci, Ilaria
112
Mazar´e, L.
47
M¨odersheim, Sebastian
80
Nielsen, Mogens
96
Nielson, Flemming
127, 185
Pang, Jun
65
Peretti, Pamela
1
Probst, Christian W.
127, 170
Rissanen, Erik
158
Sadighi, Babak
158
Seehusen, Fredrik
143
Seitz, Ludwig
158
Søndergaard, Dan
170
Stølen, Ketil
31, 143
Tian, Chunqi
270
Tolstrup, Terkel K.
185
van der Meyden, Ron
235
Vigan`o, Luca
80
Wang, Wendong
270
Wei, Kun
202
Wu, Peng
65
Zanella B´eguelin, Santiago
220
Zhang, Chenyi
235
Zou, Shihong
270


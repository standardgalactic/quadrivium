DATABASE MANAGEMENT
SYSTEMS
SOLUTIONS MANUAL
THIRD EDITION
Raghu Ramakrishnan
University of Wisconsin
Madison, WI, USA
Johannes Gehrke
Cornell University
Ithaca, NY, USA
JeﬀDerstadt, Scott Selikoﬀ, and Lin Zhu
Cornell University
Ithaca, NY, USA

CONTENTS
PREFACE
iii
1
INTRODUCTION TO DATABASE SYSTEMS
1
2
INTRODUCTION TO DATABASE DESIGN
6
3
THE RELATIONAL MODEL
16
4
RELATIONAL ALGEBRA AND CALCULUS
28
5
SQL: QUERIES, CONSTRAINTS, TRIGGERS
45
6
DATABASE APPLICATION DEVELOPMENT
63
7
INTERNET APPLICATIONS
66
8
OVERVIEW OF STORAGE AND INDEXING
73
9
STORING DATA: DISKS AND FILES
81
10
TREE-STRUCTURED INDEXING
88
11
HASH-BASED INDEXING
100
12
OVERVIEW OF QUERY EVALUATION
119
13
EXTERNAL SORTING
126
14
EVALUATION OF RELATIONAL OPERATORS
131
i

iiDatabase Management Systems Solutions Manual Third Edition
15
A TYPICAL QUERY OPTIMIZER
144
16
OVERVIEW OF TRANSACTION MANAGEMENT
159
17
CONCURRENCY CONTROL
167
18
CRASH RECOVERY
179
19
SCHEMA REFINEMENT AND NORMAL FORMS
189
20
PHYSICAL DATABASE DESIGN AND TUNING
204
21
SECURITY
215

PREFACE
It is not every question that deserves an answer.
Publius Syrus, 42 B.C.
I hope that most of the questions in this book deserve an answer. The set of questions
is unusually extensive, and is designed to reinforce and deepen students’ understanding
of the concepts covered in each chapter. There is a strong emphasis on quantitative
and problem-solving type exercises.
While I wrote some of the solutions myself, most were written originally by students
in the database classes at Wisconsin. I’d like to thank the many students who helped
in developing and checking the solutions to the exercises; this manual would not be
available without their contributions.
In alphabetical order: X. Bao, S. Biao, M.
Chakrabarti, C. Chan, W. Chen, N. Cheung, D. Colwell, J. Derstadt, C. Fritz, V.
Ganti, J. Gehrke, G. Glass, V. Gopalakrishnan, M. Higgins, T. Jasmin, M. Krish-
naprasad, Y. Lin, C. Liu, M. Lusignan, H. Modi, S. Narayanan, D. Randolph, A.
Ranganathan, J. Reminga, A. Therber, M. Thomas, Q. Wang, R. Wang, Z. Wang and
J. Yuan. In addition, James Harrington and Martin Reames at Wisconsin and Nina
Tang at Berkeley provided especially detailed feedback.
Several students contributed to each chapter’s solutions, and answers were subse-
quently checked by me and by other students. This manual has been in use for several
semesters. I hope that it is now mostly accurate, but I’m sure it still contains er-
rors and omissions. If you are a student and you do not understand a particular
solution, contact your instructor; it may be that you are missing something, but it
may also be that the solution is incorrect! If you discover a bug, please send me mail
(raghu@cs.wisc.edu) and I will update the manual promptly.
The latest version of this solutions manual is distributed freely through the Web; go
to the home page mentioned below to obtain a copy.
For More Information
The home page for this book is at URL:
iii

Database Management Systems Solutions Manual Third Edition
http://www.cs.wisc.edu/~dbbook
This page is frequently updated and contains information about the book, past and
current users, and the software. This page also contains a link to all known errors in
the book, the accompanying slides, and the software. Since the solutions manual is
distributed electronically, all known errors are immediately ﬁxed and no list of errors is
maintained. Instructors are advised to visit this site periodically; they can also register
at this site to be notiﬁed of important changes by email.

1
INTRODUCTION TO DATABASE
SYSTEMS
Exercise 1.1 Why would you choose a database system instead of simply storing data
in operating system ﬁles? When would it make sense not to use a database system?
Answer 1.1 A database is an integrated collection of data, usually so large that it
has to be stored on secondary storage devices such as disks or tapes. This data can
be maintained as a collection of operating system ﬁles, or stored in a DBMS (database
management system). The advantages of using a DBMS are:
Data independence and eﬃcient access. Database application programs are in-
dependent of the details of data representation and storage. The conceptual and
external schemas provide independence from physical storage decisions and logical
design decisions respectively. In addition, a DBMS provides eﬃcient storage and
retrieval mechanisms, including support for very large ﬁles, index structures and
query optimization.
Reduced application development time. Since the DBMS provides several impor-
tant functions required by applications, such as concurrency control and crash
recovery, high level query facilities, etc., only application-speciﬁc code needs to
be written.
Even this is facilitated by suites of application development tools
available from vendors for many database management systems.
Data integrity and security. The view mechanism and the authorization facilities
of a DBMS provide a powerful access control mechanism. Further, updates to the
data that violate the semantics of the data can be detected and rejected by the
DBMS if users specify the appropriate integrity constraints.
Data administration. By providing a common umbrella for a large collection of
data that is shared by several users, a DBMS facilitates maintenance and data
administration tasks. A good DBA can eﬀectively shield end-users from the chores
of ﬁne-tuning the data representation, periodic back-ups etc.
1

2
Chapter 1
Concurrent access and crash recovery. A DBMS supports the notion of a trans-
action, which is conceptually a single user’s sequential program. Users can write
transactions as if their programs were running in isolation against the database.
The DBMS executes the actions of transactions in an interleaved fashion to obtain
good performance, but schedules them in such a way as to ensure that conﬂicting
operations are not permitted to proceed concurrently. Further, the DBMS main-
tains a continuous log of the changes to the data, and if there is a system crash,
it can restore the database to a transaction-consistent state. That is, the actions
of incomplete transactions are undone, so that the database state reﬂects only the
actions of completed transactions. Thus, if each complete transaction, executing
alone, maintains the consistency criteria, then the database state after recovery
from a crash is consistent.
If these advantages are not important for the application at hand, using a collection of
ﬁles may be a better solution because of the increased cost and overhead of purchasing
and maintaining a DBMS.
Exercise 1.2 What is logical data independence and why is it important?
Answer 1.2 Answer omitted.
Exercise 1.3 Explain the diﬀerence between logical and physical data independence.
Answer 1.3 Logical data independence means that users are shielded from changes
in the logical structure of the data, while physical data independence insulates users
from changes in the physical storage of the data. We saw an example of logical data
independence in the answer to Exercise 1.2. Consider the Students relation from that
example (and now assume that it is not replaced by the two smaller relations). We
could choose to store Students tuples in a heap ﬁle, with a clustered index on the
sname ﬁeld. Alternatively, we could choose to store it with an index on the gpa ﬁeld,
or to create indexes on both ﬁelds, or to store it as a ﬁle sorted by gpa. These storage
alternatives are not visible to users, except in terms of improved performance, since
they simply see a relation as a set of tuples. This is what is meant by physical data
independence.
Exercise 1.4 Explain the diﬀerence between external, internal, and conceptual sche-
mas.
How are these diﬀerent schema layers related to the concepts of logical and
physical data independence?
Answer 1.4 Answer omitted.
Exercise 1.5 What are the responsibilities of a DBA? If we assume that the DBA
is never interested in running his or her own queries, does the DBA still need to
understand query optimization? Why?

Introduction to Database Systems
3
Answer 1.5 The DBA is responsible for:
Designing the logical and physical schemas, as well as widely-used portions of the
external schema.
Security and authorization.
Data availability and recovery from failures.
Database tuning: The DBA is responsible for evolving the database, in particular
the conceptual and physical schemas, to ensure adequate performance as user
requirements change.
A DBA needs to understand query optimization even if s/he is not interested in run-
ning his or her own queries because some of these
responsibilities (database design
and tuning) are related to query optimization. Unless the DBA understands the per-
formance needs of widely used queries, and how the DBMS will optimize and execute
these queries, good design and tuning decisions cannot be made.
Exercise 1.6 Scrooge McNugget wants to store information (names, addresses, de-
scriptions of embarrassing moments, etc.) about the many ducks on his payroll. Not
surprisingly, the volume of data compels him to buy a database system.
To save
money, he wants to buy one with the fewest possible features, and he plans to run it as
a stand-alone application on his PC clone. Of course, Scrooge does not plan to share
his list with anyone. Indicate which of the following DBMS features Scrooge should
pay for; in each case, also indicate why Scrooge should (or should not) pay for that
feature in the system he buys.
1. A security facility.
2. Concurrency control.
3. Crash recovery.
4. A view mechanism.
5. A query language.
Answer 1.6 Answer omitted.
Exercise 1.7 Which of the following plays an important role in representing informa-
tion about the real world in a database? Explain brieﬂy.
1. The data deﬁnition language.

4
Chapter 1
2. The data manipulation language.
3. The buﬀer manager.
4. The data model.
Answer 1.7 Let us discuss the choices in turn.
The data deﬁnition language is important in representing information because it
is used to describe external and logical schemas.
The data manipulation language is used to access and update data; it is not
important for representing the data. (Of course, the data manipulation language
must be aware of how data is represented, and reﬂects this in the constructs that
it supports.)
The buﬀer manager is not very important for representation because it brings
arbitrary disk pages into main memory, independent of any data representation.
The data model is fundamental to representing information.
The data model
determines what data representation mechanisms are supported by the DBMS.
The data deﬁnition language is just the speciﬁc set of language constructs available
to describe an actual application’s data in terms of the data model.
Exercise 1.8 Describe the structure of a DBMS. If your operating system is upgraded
to support some new functions on OS ﬁles (e.g., the ability to force some sequence of
bytes to disk), which layer(s) of the DBMS would you have to rewrite to take advantage
of these new functions?
Answer 1.8 Answer omitted.
Exercise 1.9 Answer the following questions:
1. What is a transaction?
2. Why does a DBMS interleave the actions of diﬀerent transactions instead of exe-
cuting transactions one after the other?
3. What must a user guarantee with respect to a transaction and database consis-
tency? What should a DBMS guarantee with respect to concurrent execution of
several transactions and database consistency?
4. Explain the strict two-phase locking protocol.
5. What is the WAL property, and why is it important?

Introduction to Database Systems
5
Answer 1.9 Let us answer each question in turn:
1. A transaction is any one execution of a user program in a DBMS. This is the basic
unit of change in a DBMS.
2. A DBMS is typically shared among many users. Transactions from these users
can be interleaved to improve the execution time of users’ queries. By interleav-
ing queries, users do not have to wait for other user’s transactions to complete
fully before their own transaction begins. Without interleaving, if user A begins
a transaction that will take 10 seconds to complete, and user B wants to be-
gin a transaction, user B would have to wait an additional 10 seconds for user
A’s transaction to complete before the database would begin processing user B’s
request.
3. A user must guarantee that his or her transaction does not corrupt data or insert
nonsense in the database. For example, in a banking database, a user must guar-
antee that a cash withdraw transaction accurately models the amount a person
removes from his or her account. A database application would be worthless if
a person removed 20 dollars from an ATM but the transaction set their balance
to zero! A DBMS must guarantee that transactions are executed fully and in-
dependently of other transactions. An essential property of a DBMS is that a
transaction should execute atomically, or as if it is the only transaction running.
Also, transactions will either complete fully, or will be aborted and the database
returned to it’s initial state. This ensures that the database remains consistent.
4. Strict two-phase locking uses shared and exclusive locks to protect data. A trans-
action must hold all the required locks before executing, and does not release any
lock until the transaction has completely ﬁnished.
5. The WAL property aﬀects the logging strategy in a DBMS. The WAL, Write-
Ahead Log, property states that each write action must be recorded in the log
(on disk) before the corresponding change is reﬂected in the database itself. This
protects the database from system crashes that happen during a transaction’s
execution. By recording the change in a log before the change is truly made, the
database knows to undo the changes to recover from a system crash. Otherwise,
if the system crashes just after making the change in the database but before
the database logs the change, then the database would not be able to detect his
change during crash recovery.

2
INTRODUCTION TO DATABASE
DESIGN
Exercise 2.1 Explain the following terms brieﬂy: attribute, domain, entity, relation-
ship, entity set, relationship set, one-to-many relationship, many-to-many relationship,
participation constraint, overlap constraint, covering constraint, weak entity set, aggre-
gation, and role indicator.
Answer 2.1 Term explanations:
Attribute - a property or description of an entity. A toy department employee
entity could have attributes describing the employee’s name, salary, and years of
service.
Domain - a set of possible values for an attribute.
Entity - an object in the real world that is distinguishable from other objects such
as the green dragon toy.
Relationship - an association among two or more entities.
Entity set - a collection of similar entities such as all of the toys in the toy depart-
ment.
Relationship set - a collection of similar relationships
One-to-many relationship - a key constraint that indicates that one entity can be
associated with many of another entity. An example of a one-to-many relationship
is when an employee can work for only one department, and a department can
have many employees.
Many-to-many relationship - a key constraint that indicates that many of one
entity can be associated with many of another entity. An example of a many-
to-many relationship is employees and their hobbies: a person can have many
diﬀerent hobbies, and many people can have the same hobby.
6

Introduction to Database Design
7
Participation constraint - a participation constraint determines whether relation-
ships must involve certain entities. An example is if every department entity has
a manager entity. Participation constraints can either be total or partial. A total
participation constraint says that every department has a manager. A partial
participation constraint says that every employee does not have to be a manager.
Overlap constraint - within an ISA hierarchy, an overlap constraint determines
whether or not two subclasses can contain the same entity.
Covering constraint - within an ISA hierarchy, a covering constraint determines
where the entities in the subclasses collectively include all entities in the superclass.
For example, with an Employees entity set with subclasses HourlyEmployee and
SalaryEmployee, does every Employee entity necessarily have to be within either
HourlyEmployee or SalaryEmployee?
Weak entity set - an entity that cannot be identiﬁed uniquely without considering
some primary key attributes of another identifying owner entity. An example is
including Dependent information for employees for insurance purposes.
Aggregation - a feature of the entity relationship model that allows a relationship
set to participate in another relationship set. This is indicated on an ER diagram
by drawing a dashed box around the aggregation.
Role indicator - If an entity set plays more than one role, role indicators describe
the diﬀerent purpose in the relationship. An example is a single Employee entity
set with a relation Reports-To that relates supervisors and subordinates.
Exercise 2.2 A university database contains information about professors (identiﬁed
by social security number, or SSN) and courses (identiﬁed by courseid). Professors
teach courses; each of the following situations concerns the Teaches relationship set. For
each situation, draw an ER diagram that describes it (assuming no further constraints
hold).
1. Professors can teach the same course in several semesters, and each oﬀering must
be recorded.
2. Professors can teach the same course in several semesters, and only the most
recent such oﬀering needs to be recorded. (Assume this condition applies in all
subsequent questions.)
3. Every professor must teach some course.
4. Every professor teaches exactly one course (no more, no less).
5. Every professor teaches exactly one course (no more, no less), and every course
must be taught by some professor.

8
Chapter 2
6. Now suppose that certain courses can be taught by a team of professors jointly,
but it is possible that no one professor in a team can teach the course. Model this
situation, introducing additional entity sets and relationship sets if necessary.
Answer 2.2 Answer omitted.
Exercise 2.3 Consider the following information about a university database:
Professors have an SSN, a name, an age, a rank, and a research specialty.
Projects have a project number, a sponsor name (e.g., NSF), a starting date, an
ending date, and a budget.
Graduate students have an SSN, a name, an age, and a degree program (e.g., M.S.
or Ph.D.).
Each project is managed by one professor (known as the project’s principal inves-
tigator).
Each project is worked on by one or more professors (known as the project’s
co-investigators).
Professors can manage and/or work on multiple projects.
Each project is worked on by one or more graduate students (known as the
project’s research assistants).
When graduate students work on a project, a professor must supervise their work
on the project. Graduate students can work on multiple projects, in which case
they will have a (potentially diﬀerent) supervisor for each one.
Departments have a department number, a department name, and a main oﬃce.
Departments have a professor (known as the chairman) who runs the department.
Professors work in one or more departments, and for each department that they
work in, a time percentage is associated with their job.
Graduate students have one major department in which they are working on their
degree.
Each graduate student has another, more senior graduate student (known as a
student advisor) who advises him or her on what courses to take.
Design and draw an ER diagram that captures the information about the university.
Use only the basic ER model here; that is, entities, relationships, and attributes. Be
sure to indicate any key and participation constraints.

Introduction to Database Design
9
work_in
Manages
project
pid
sponsor
start_date
end_date
budget
Dept
Runs
Work_dept
office
dname
dno
Professor
ssn
age
rank
speciality
Major
Work_proj
Advisor
Graduate
senior
grad
ssn
pc_time
age
ssn
deg_prog
name
Supervises
Figure 2.1
ER Diagram for Exercise 2.3

10
Chapter 2
Answer 2.3 The ER diagram is shown in Figure 2.1.
Exercise 2.4 A company database needs to store information about employees (iden-
tiﬁed by ssn, with salary and phone as attributes), departments (identiﬁed by dno,
with dname and budget as attributes), and children of employees (with name and age
as attributes). Employees work in departments; each department is managed by an
employee; a child must be identiﬁed uniquely by name when the parent (who is an
employee; assume that only one parent works for the company) is known. We are not
interested in information about a child once the parent leaves the company.
Draw an ER diagram that captures this information.
Answer 2.4 Answer omitted.
Exercise 2.5 Notown Records has decided to store information about musicians who
perform on its albums (as well as other company data) in a database. The company
has wisely chosen to hire you as a database designer (at your usual consulting fee of
$2500/day).
Each musician that records at Notown has an SSN, a name, an address, and
a phone number. Poorly paid musicians often share the same address, and no
address has more than one phone.
Each instrument used in songs recorded at Notown has a unique identiﬁcation
number, a name (e.g., guitar, synthesizer, ﬂute) and a musical key (e.g., C, B-ﬂat,
E-ﬂat).
Each album recorded on the Notown label has a unique identiﬁcation number, a
title, a copyright date, a format (e.g., CD or MC), and an album identiﬁer.
Each song recorded at Notown has a title and an author.
Each musician may play several instruments, and a given instrument may be
played by several musicians.
Each album has a number of songs on it, but no song may appear on more than
one album.
Each song is performed by one or more musicians, and a musician may perform a
number of songs.
Each album has exactly one musician who acts as its producer. A musician may
produce several albums, of course.

Introduction to Database Design
11
ssn
Musicians
name
Album
copyrightDate
speed
albumIdentifier
dname
Instrument
instrId
key
songId
Songs
title
suthor
Plays
Appears
Perform
Producer
title
address
Home
Lives
Place
Telephone
phone_no
Figure 2.2
ER Diagram for Exercise 2.5
Design a conceptual schema for Notown and draw an ER diagram for your schema.
The preceding information describes the situation that the Notown database must
model. Be sure to indicate all key and cardinality constraints and any assumptions
you make. Identify any constraints you are unable to capture in the ER diagram and
brieﬂy explain why you could not express them.
Answer 2.5 The ER diagram is shown in Figure 2.2.

12
Chapter 2
Exercise 2.6 Computer Sciences Department frequent ﬂiers have been complaining to
Dane County Airport oﬃcials about the poor organization at the airport. As a result,
the oﬃcials decided that all information related to the airport should be organized
using a DBMS, and you have been hired to design the database. Your ﬁrst task is
to organize the information about all the airplanes stationed and maintained at the
airport. The relevant information is as follows:
Every airplane has a registration number, and each airplane is of a speciﬁc model.
The airport accommodates a number of airplane models, and each model is iden-
tiﬁed by a model number (e.g., DC-10) and has a capacity and a weight.
A number of technicians work at the airport. You need to store the name, SSN,
address, phone number, and salary of each technician.
Each technician is an expert on one or more plane model(s), and his or her exper-
tise may overlap with that of other technicians. This information about technicians
must also be recorded.
Traﬃc controllers must have an annual medical examination.
For each traﬃc
controller, you must store the date of the most recent exam.
All airport employees (including technicians) belong to a union. You must store
the union membership number of each employee.
You can assume that each
employee is uniquely identiﬁed by a social security number.
The airport has a number of tests that are used periodically to ensure that air-
planes are still airworthy. Each test has a Federal Aviation Administration (FAA)
test number, a name, and a maximum possible score.
The FAA requires the airport to keep track of each time a given airplane is tested
by a given technician using a given test. For each testing event, the information
needed is the date, the number of hours the technician spent doing the test, and
the score the airplane received on the test.
1. Draw an ER diagram for the airport database. Be sure to indicate the various
attributes of each entity and relationship set; also specify the key and participation
constraints for each relationship set. Specify any necessary overlap and covering
constraints as well (in English).
2. The FAA passes a regulation that tests on a plane must be conducted by a tech-
nician who is an expert on that model. How would you express this constraint in
the ER diagram? If you cannot express it, explain brieﬂy.
Answer 2.6 Answer omitted.

Introduction to Database Design
13
Exercise 2.7 The Prescriptions-R-X chain of pharmacies has oﬀered to give you a
free lifetime supply of medicine if you design its database. Given the rising cost of
health care, you agree. Here’s the information that you gather:
Patients are identiﬁed by an SSN, and their names, addresses, and ages must be
recorded.
Doctors are identiﬁed by an SSN. For each doctor, the name, specialty, and years
of experience must be recorded.
Each pharmaceutical company is identiﬁed by name and has a phone number.
For each drug, the trade name and formula must be recorded.
Each drug is
sold by a given pharmaceutical company, and the trade name identiﬁes a drug
uniquely from among the products of that company. If a pharmaceutical company
is deleted, you need not keep track of its products any longer.
Each pharmacy has a name, address, and phone number.
Every patient has a primary physician. Every doctor has at least one patient.
Each pharmacy sells several drugs and has a price for each. A drug could be sold
at several pharmacies, and the price could vary from one pharmacy to another.
Doctors prescribe drugs for patients. A doctor could prescribe one or more drugs
for several patients, and a patient could obtain prescriptions from several doctors.
Each prescription has a date and a quantity associated with it. You can assume
that, if a doctor prescribes the same drug for the same patient more than once,
only the last such prescription needs to be stored.
Pharmaceutical companies have long-term contracts with pharmacies. A phar-
maceutical company can contract with several pharmacies, and a pharmacy can
contract with several pharmaceutical companies. For each contract, you have to
store a start date, an end date, and the text of the contract.
Pharmacies appoint a supervisor for each contract. There must always be a super-
visor for each contract, but the contract supervisor can change over the lifetime
of the contract.
1. Draw an ER diagram that captures the preceding information. Identify any con-
straints not captured by the ER diagram.
2. How would your design change if each drug must be sold at a ﬁxed price by all
pharmacies?
3. How would your design change if the design requirements change as follows: If a
doctor prescribes the same drug for the same patient more than once, several such
prescriptions may have to be stored.

14
Chapter 2
ssn
age
Patient
      
   
address
   
name
Pri_physician
Doctor
name
phy_ssn
speciality
exp_years
Prescription
quentity
Sell
address
phone_num
Pharmacy
Pharm_co
Make
   Drug
formula
trade_name
   date
phone_num
name
price
start_date
end_date
text
Contract
supervisor
name
Figure 2.3
ER Diagram for Exercise 2.7

Introduction to Database Design
15
Answer 2.7
1. The ER diagram is shown in Figure 2.3.
2. If the drug is to be sold at a ﬁxed price we can add the price attribute to the Drug
entity set and eliminate the price from the Sell relationship set.
3. The date information can no longer be modeled as an attribute of Prescription.
We have to create a new entity set called Prescription date and make Prescription
a 4-way relationship set that involves this additional entity set.
Exercise 2.8 Although you always wanted to be an artist, you ended up being an ex-
pert on databases because you love to cook data and you somehow confused database
with data baste. Your old love is still there, however, so you set up a database company,
ArtBase, that builds a product for art galleries. The core of this product is a database
with a schema that captures all the information that galleries need to maintain. Gal-
leries keep information about artists, their names (which are unique), birthplaces, age,
and style of art. For each piece of artwork, the artist, the year it was made, its unique
title, its type of art (e.g., painting, lithograph, sculpture, photograph), and its price
must be stored. Pieces of artwork are also classiﬁed into groups of various kinds, for
example, portraits, still lifes, works by Picasso, or works of the 19th century; a given
piece may belong to more than one group. Each group is identiﬁed by a name (like
those just given) that describes the group. Finally, galleries keep information about
customers. For each customer, galleries keep that person’s unique name, address, total
amount of dollars spent in the gallery (very important!), and the artists and groups of
art that the customer tends to like.
Draw the ER diagram for the database.
Answer 2.8 Answer omitted.
Exercise 2.9 Answer the following questions.
Explain the following terms brieﬂy: UML, use case diagrams, statechart dia-
grams, class diagrams, database diagrams, component diagrams, and deployment
diagrams.
Explain the relationship between ER diagrams and UML.
Answer 2.9 Not yet done.

3
THE RELATIONAL MODEL
Exercise 3.1 Deﬁne the following terms: relation schema, relational database schema,
domain, attribute, attribute domain, relation instance, relation cardinality, and relation
degree.
Answer 3.1 A relation schema can be thought of as the basic information describing
a table or relation. This includes a set of column names, the data types associated
with each column, and the name associated with the entire table. For example, a
relation schema for the relation called Students could be expressed using the following
representation:
Students(sid: string, name: string, login: string,
age: integer, gpa: real)
There are ﬁve ﬁelds or columns, with names and types as shown above.
A relational database schema is a collection of relation schemas, describing one or more
relations.
Domain is synonymous with data type. Attributes can be thought of as columns in a
table. Therefore, an attribute domain refers to the data type associated with a column.
A relation instance is a set of tuples (also known as rows or records) that each conform
to the schema of the relation.
The relation cardinality is the number of tuples in the relation.
The relation degree is the number of ﬁelds (or columns) in the relation.
Exercise 3.2 How many distinct tuples are in a relation instance with cardinality 22?
Answer 3.2 Answer omitted.
16

The Relational Model
17
Exercise 3.3 Does the relational model, as seen by an SQL query writer, provide
physical and logical data independence? Explain.
Answer 3.3 The user of SQL has no idea how the data is physically represented in the
machine. He or she relies entirely on the relation abstraction for querying. Physical
data independence is therefore assured. Since a user can deﬁne views, logical data
independence can also be achieved by using view deﬁnitions to hide changes in the
conceptual schema.
Exercise 3.4 What is the diﬀerence between a candidate key and the primary key for
a given relation? What is a superkey?
Answer 3.4 Answer omitted.
53831
53832
53650
53688
53666
50000
3.3
3.4
3.2
3.8
1.8
2.0
19
18
18
19
11
12
madayan@music
guldu@music
smith@math
smith@ee
jones@cs
dave@cs
Madayan
Guldu
Smith
Smith
Jones
Dave
  sid
age
gpa
login
name
TUPLES
(RECORDS, ROWS)
FIELDS (ATTRIBUTES, COLUMNS)
Field names
Figure 3.1
An Instance S1 of the Students Relation
Exercise 3.5 Consider the instance of the Students relation shown in Figure 3.1.
1. Give an example of an attribute (or set of attributes) that you can deduce is not
a candidate key, based on this instance being legal.
2. Is there any example of an attribute (or set of attributes) that you can deduce is
a candidate key, based on this instance being legal?
Answer 3.5 Examples of non-candidate keys include the following: {name}, {age}.
(Note that {gpa} can not be declared as a non-candidate key from this evidence alone
even though common sense tells us that clearly more than one student could have the
same grade point average.)
You cannot determine a key of a relation given only one instance of the relation. The
fact that the instance is “legal” is immaterial. A candidate key, as deﬁned here, is a

18
Chapter 3
key, not something that only might be a key. The instance shown is just one possible
“snapshot” of the relation. At other times, the same relation may have an instance (or
snapshot) that contains a totally diﬀerent set of tuples, and we cannot make predictions
about those instances based only upon the instance that we are given.
Exercise 3.6 What is a foreign key constraint? Why are such constraints important?
What is referential integrity?
Answer 3.6 Answer omitted.
Exercise 3.7 Consider the relations Students, Faculty, Courses, Rooms, Enrolled,
Teaches, and Meets In deﬁned in Section 1.5.2.
1. List all the foreign key constraints among these relations.
2. Give an example of a (plausible) constraint involving one or more of these relations
that is not a primary key or foreign key constraint.
Answer 3.7 There is no reason for a foreign key constraint (FKC) on the Students,
Faculty, Courses, or Rooms relations. These are the most basic relations and must be
free-standing. Special care must be given to entering data into these base relations.
In the Enrolled relation, sid and cid should both have FKCs placed on them. (Real
students must be enrolled in real courses.) Also, since real teachers must teach real
courses, both the fid and the cid ﬁelds in the Teaches relation should have FKCs.
Finally, Meets In should place FKCs on both the cid and rno ﬁelds.
It would probably be wise to enforce a few other constraints on this DBMS: the length
of sid, cid, and fid could be standardized; checksums could be added to these iden-
tiﬁcation numbers; limits could be placed on the size of the numbers entered into the
credits, capacity, and salary ﬁelds; an enumerated type should be assigned to the grade
ﬁeld (preventing a student from receiving a grade of G, among other things); etc.
Exercise 3.8 Answer each of the following questions brieﬂy. The questions are based
on the following relational schema:
Emp(eid: integer, ename: string, age: integer, salary: real)
Works(eid: integer, did: integer, pcttime: integer)
Dept(did: integer, dname: string, budget: real, managerid: integer)
1. Give an example of a foreign key constraint that involves the Dept relation. What
are the options for enforcing this constraint when a user attempts to delete a Dept
tuple?

The Relational Model
19
2. Write the SQL statements required to create the preceding relations, including
appropriate versions of all primary and foreign key integrity constraints.
3. Deﬁne the Dept relation in SQL so that every department is guaranteed to have
a manager.
4. Write an SQL statement to add John Doe as an employee with eid = 101, age = 32
and salary = 15, 000.
5. Write an SQL statement to give every employee a 10 percent raise.
6. Write an SQL statement to delete the Toy department.
Given the referential
integrity constraints you chose for this schema, explain what happens when this
statement is executed.
Answer 3.8 Answer omitted.
sid
name
login
age
gpa
53831
Madayan
madayan@music
11
1.8
53832
Guldu
guldu@music
12
2.0
Figure 3.2
Students with age < 18 on Instance S
Exercise 3.9 Consider the SQL query whose answer is shown in Figure 3.2.
1. Modify this query so that only the login column is included in the answer.
2. If the clause WHERE S.gpa >= 2 is added to the original query, what is the set of
tuples in the answer?
Answer 3.9 The answers are as follows:
1. Only login is included in the answer:
SELECT S.login
FROM
Students S
WHERE
S.age < 18
2. The answer tuple for Madayan is omitted then.
Exercise 3.10 Explain why the addition of NOT NULL constraints to the SQL deﬁ-
nition of the Manages relation (in Section 3.5.3) does not enforce the constraint that
each department must have a manager. What, if anything, is achieved by requiring
that the ssn ﬁeld of Manages be non-null?

20
Chapter 3
Answer 3.10 Answer omitted.
Exercise 3.11 Suppose that we have a ternary relationship R between entity sets A,
B, and C such that A has a key constraint and total participation and B has a key
constraint; these are the only constraints. A has attributes a1 and a2, with a1 being
the key; B and C are similar. R has no descriptive attributes. Write SQL statements
that create tables corresponding to this information so as to capture as many of the
constraints as possible. If you cannot capture some constraint, explain why.
Answer 3.11 The following SQL statements create the corresponding relations.
CREATE TABLE A (
a1
CHAR(10),
a2
CHAR(10),
b1
CHAR(10),
c1
CHAR(10),
PRIMARY KEY (a1),
UNIQUE (b1),
FOREIGN KEY (b1) REFERENCES B,
FOREIGN KEY (c1) REFERENCES C )
CREATE TABLE B (
b1
CHAR(10),
b2
CHAR(10),
PRIMARY KEY (b1) )
CREATE TABLE C (
b1
CHAR(10),
c2
CHAR(10),
PRIMARY KEY (c1) )
The ﬁrst SQL statement folds the relationship R into table A and thereby guarantees
the participation constraint.
Exercise 3.12 Consider the scenario from Exercise 2.2, where you designed an ER
diagram for a university database. Write SQL statements to create the corresponding
relations and capture as many of the constraints as possible. If you cannot capture
some constraints, explain why.
Answer 3.12 Answer omitted.
Exercise 3.13 Consider the university database from Exercise 2.3 and the ER dia-
gram you designed. Write SQL statements to create the corresponding relations and
capture as many of the constraints as possible. If you cannot capture some constraints,
explain why.

The Relational Model
21
Answer 3.13 The following SQL statements create the corresponding relations.
1. CREATE TABLE Professors (
prof ssn
CHAR(10),
name
CHAR(64),
age
INTEGER,
rank
INTEGER,
speciality CHAR(64),
PRIMARY KEY (prof ssn) )
2. CREATE TABLE Depts (
dno
INTEGER,
dname
CHAR(64),
oﬃce
CHAR(10),
PRIMARY KEY (dno) )
3. CREATE TABLE Runs (
dno
INTEGER,
prof ssn
CHAR(10),
PRIMARY KEY ( dno, prof ssn),
FOREIGN KEY (prof ssn) REFERENCES Professors,
FOREIGN KEY (dno) REFERENCES Depts )
4. CREATE TABLE Work Dept (
dno
INTEGER,
prof ssn
CHAR(10),
pc time
INTEGER,
PRIMARY KEY (dno, prof ssn),
FOREIGN KEY (prof ssn) REFERENCES Professors,
FOREIGN KEY (dno) REFERENCES Depts )
Observe that we would need check constraints or assertions in SQL to enforce the
rule that Professors work in at least one department.
5. CREATE TABLE Project (
pid
INTEGER,
sponsor
CHAR(32),
start dateDATE,
end date DATE,
budget
FLOAT,
PRIMARY KEY (pid) )
6. CREATE TABLE Graduates (
grad ssn CHAR(10),
age
INTEGER,
name
CHAR(64),
deg prog CHAR(32),

22
Chapter 3
major
INTEGER,
PRIMARY KEY (grad ssn),
FOREIGN KEY (major) REFERENCES Depts )
Note that the Major table is not necessary since each Graduate has only one major
and so this can be an attribute in the Graduates table.
7. CREATE TABLE Advisor (
senior ssn CHAR(10),
grad ssn CHAR(10),
PRIMARY KEY (senior ssn, grad ssn),
FOREIGN KEY (senior ssn)
REFERENCES Graduates (grad ssn),
FOREIGN KEY (grad ssn) REFERENCES Graduates )
8. CREATE TABLE Manages (
pid
INTEGER,
prof ssn
CHAR(10),
PRIMARY KEY (pid, prof ssn),
FOREIGN KEY (prof ssn) REFERENCES Professors,
FOREIGN KEY (pid) REFERENCES Projects )
9. CREATE TABLE Work In (
pid
INTEGER,
prof ssn
CHAR(10),
PRIMARY KEY (pid, prof ssn),
FOREIGN KEY (prof ssn) REFERENCES Professors,
FOREIGN KEY (pid) REFERENCES Projects )
Observe that we cannot enforce the participation constraint for Projects in the
Work In table without check constraints or assertions in SQL.
10. CREATE TABLE Supervises (
prof ssn
CHAR(10),
grad ssn CHAR(10),
pid
INTEGER,
PRIMARY KEY (prof ssn, grad ssn, pid),
FOREIGN KEY (prof ssn) REFERENCES Professors,
FOREIGN KEY (grad ssn) REFERENCES Graduates,
FOREIGN KEY (pid) REFERENCES Projects )
Note that we do not need an explicit table for the Work Proj relation since every
time a Graduate works on a Project, he or she must have a Supervisor.
Exercise 3.14 Consider the scenario from Exercise 2.4, where you designed an ER
diagram for a company database. Write SQL statements to create the corresponding

The Relational Model
23
relations and capture as many of the constraints as possible. If you cannot capture
some constraints, explain why.
Answer 3.14 Answer omitted.
Exercise 3.15 Consider the Notown database from Exercise 2.5. You have decided
to recommend that Notown use a relational database system to store company data.
Show the SQL statements for creating relations corresponding to the entity sets and
relationship sets in your design. Identify any constraints in the ER diagram that you
are unable to capture in the SQL statements and brieﬂy explain why you could not
express them.
Answer 3.15 The following SQL statements create the corresponding relations.
1. CREATE TABLE Musicians ( ssn
CHAR(10),
name
CHAR(30),
PRIMARY KEY (ssn))
2. CREATE TABLE Instruments (
instrId
CHAR(10),
dname
CHAR(30),
key
CHAR(5),
PRIMARY KEY (instrId))
3. CREATE TABLE Plays (
ssn
CHAR(10),
instrId
INTEGER,
PRIMARY KEY (ssn, instrId),
FOREIGN KEY (ssn) REFERENCES Musicians,
FOREIGN KEY (instrId) REFERENCES Instruments )
4. CREATE TABLE Songs Appears ( songId
INTEGER,
author
CHAR(30),
title
CHAR(30),
albumIdentiﬁer INTEGER NOT NULL,
PRIMARY KEY (songId),
FOREIGN KEY (albumIdentiﬁer)
References Album Producer,
5. CREATE TABLE Telephone Home ( phone
CHAR(11),
address
CHAR(30),
PRIMARY KEY (phone),
FOREIGN KEY (address) REFERENCES Place,

24
Chapter 3
6. CREATE TABLE Lives (
ssn
CHAR(10),
phone
CHAR(11),
address CHAR(30),
PRIMARY KEY (ssn, address),
FOREIGN KEY (phone, address)
References Telephone Home,
FOREIGN KEY (ssn) REFERENCES Musicians )
7. CREATE TABLE Place (
address CHAR(30) )
8. CREATE TABLE Perform (
songId
INTEGER,
ssn
CHAR(10),
PRIMARY KEY (ssn, songId),
FOREIGN KEY (songId) REFERENCES Songs,
FOREIGN KEY (ssn) REFERENCES Musicians )
9. CREATE TABLE Album Producer ( albumIdentiﬁer INTEGER,
ssn
CHAR(10),
copyrightDate DATE,
speed
INTEGER,
title
CHAR(30),
PRIMARY KEY (albumIdentiﬁer),
FOREIGN KEY (ssn) REFERENCES Musicians )
Exercise 3.16 Translate your ER diagram from Exercise 2.6 into a relational schema,
and show the SQL statements needed to create the relations, using only key and null
constraints. If your translation cannot capture any constraints in the ER diagram,
explain why.
In Exercise 2.6, you also modiﬁed the ER diagram to include the constraint that tests
on a plane must be conducted by a technician who is an expert on that model. Can
you modify the SQL statements deﬁning the relations obtained by mapping the ER
diagram to check this constraint?
Answer 3.16 Answer omitted.
Exercise 3.17 Consider the ER diagram that you designed for the Prescriptions-R-X
chain of pharmacies in Exercise 2.7. Deﬁne relations corresponding to the entity sets
and relationship sets in your design using SQL.

The Relational Model
25
Answer 3.17 The statements to create tables corresponding to entity sets Doctor,
Pharmacy, and Pharm co are straightforward and omitted. The other required tables
can be created as follows:
1. CREATE TABLE Pri Phy Patient ( ssn
CHAR(11),
name
CHAR(20),
age
INTEGER,
address
CHAR(20),
phy ssn
CHAR(11),
PRIMARY KEY (ssn),
FOREIGN KEY (phy ssn) REFERENCES Doctor )
2. CREATE TABLE Prescription ( ssn
CHAR(11),
phy ssn
CHAR(11),
date
CHAR(11),
quantity
INTEGER,
trade name CHAR(20),
pharm id
CHAR(11),
PRIMARY KEY (ssn, phy ssn),
FOREIGN KEY (ssn) REFERENCES Patient,
FOREIGN KEY (phy ssn) REFERENCES Doctor,
FOREIGN KEY (trade name, pharm id)
References Make Drug)
3. CREATE TABLE Make Drug (trade name
CHAR(20),
pharm id
CHAR(11),
PRIMARY KEY (trade name, pharm id),
FOREIGN KEY (trade name) REFERENCES Drug,
FOREIGN KEY (pharm id) REFERENCES Pharm co)
4. CREATE TABLE Sell (
price
INTEGER,
name
CHAR(10),
trade name
CHAR(10),
PRIMARY KEY (name, trade name),
FOREIGN KEY (name) REFERENCES Pharmacy,
FOREIGN KEY (trade name) REFERENCES Drug)
5. CREATE TABLE Contract (
name
CHAR(20),
pharm id
CHAR(11),
start date
CHAR(11),
end date
CHAR(11),

26
Chapter 3
text
CHAR(10000),
supervisor
CHAR(20),
PRIMARY KEY (name, pharm id),
FOREIGN KEY (name) REFERENCES Pharmacy,
FOREIGN KEY (pharm id) REFERENCES Pharm co)
Exercise 3.18 Write SQL statements to create the corresponding relations to the
ER diagram you designed for Exercise 2.8. If your translation cannot capture any
constraints in the ER diagram, explain why.
Answer 3.18 Answer omitted.
Exercise 3.19 Brieﬂy answer the following questions based on this schema:
Emp(eid: integer, ename: string, age: integer, salary: real)
Works(eid: integer, did: integer, pct time: integer)
Dept(did: integer, budget: real, managerid: integer)
1. Suppose you have a view SeniorEmp deﬁned as follows:
CREATE VIEW SeniorEmp (sname, sage, salary)
AS SELECT E.ename, E.age, E.salary
FROM
Emp E
WHERE
E.age > 50
Explain what the system will do to process the following query:
SELECT S.sname
FROM
SeniorEmp S
WHERE
S.salary > 100,000
2. Give an example of a view on Emp that could be automatically updated by up-
dating Emp.
3. Give an example of a view on Emp that would be impossible to update (auto-
matically) and explain why your example presents the update problem that it
does.
Answer 3.19 The answer to each question is given below.
1. The system will do the following:

The Relational Model
27
SELECT
S.name
FROM
( SELECT E.ename AS name, E.age, E.salary
FROM
Emp E
WHERE
E.age > 50 ) AS S
WHERE
S.salary > 100000
2. The following view on Emp can be updated automatically by updating Emp:
CREATE VIEW SeniorEmp (eid, name, age, salary)
AS SELECT E.eid, E.ename, E.age, E.salary
FROM
Emp E
WHERE
E.age > 50
3. The following view cannot be updated automatically because it is not clear which
employee records will be aﬀected by a given update:
CREATE VIEW AvgSalaryByAge (age, avgSalary)
AS SELECT
E.eid, AVG (E.salary)
FROM
Emp E
GROUP BY
E.age
Exercise 3.20 Consider the following schema:
Suppliers(sid: integer, sname: string, address: string)
Parts(pid: integer, pname: string, color: string)
Catalog(sid: integer, pid: integer, cost: real)
The Catalog relation lists the prices charged for parts by Suppliers. Answer the fol-
lowing questions:
Give an example of an updatable view involving one relation.
Give an example of an updatable view involving two relations.
Give an example of an insertable-into view that is updatable.
Give an example of an insertable-into view that is not updatable.
Answer 3.20 Answer omitted.

4
RELATIONAL ALGEBRA AND
CALCULUS
Exercise 4.1 Explain the statement that relational algebra operators can be com-
posed. Why is the ability to compose operators important?
Answer 4.1 Every operator in relational algebra accepts one or more relation in-
stances as arguments and the result is always an relation instance. So the argument
of one operator could be the result of another operator. This is important because,
this makes it easy to write complex queries by simply composing the relational algebra
operators.
Exercise 4.2 Given two relations R1 and R2, where R1 contains N1 tuples, R2 con-
tains N2 tuples, and N2 > N1 > 0, give the minimum and maximum possible sizes (in
tuples) for the resulting relation produced by each of the following relational algebra
expressions. In each case, state any assumptions about the schemas for R1 and R2
needed to make the expression meaningful:
(1) R1∪R2, (2) R1∩R2, (3) R1−R2, (4) R1×R2, (5) σa=5(R1), (6) πa(R1),
and (7) R1/R2
Answer 4.2 Answer omitted.
Exercise 4.3 Consider the following schema:
Suppliers(sid: integer, sname: string, address: string)
Parts(pid: integer, pname: string, color: string)
Catalog(sid: integer, pid: integer, cost: real)
The key ﬁelds are underlined, and the domain of each ﬁeld is listed after the ﬁeld
name. Therefore sid is the key for Suppliers, pid is the key for Parts, and sid and pid
together form the key for Catalog. The Catalog relation lists the prices charged for
parts by Suppliers. Write the following queries in relational algebra, tuple relational
calculus, and domain relational calculus:
28

Relational Algebra and Calculus
29
1. Find the names of suppliers who supply some red part.
2. Find the sids of suppliers who supply some red or green part.
3. Find the sids of suppliers who supply some red part or are at 221 Packer Street.
4. Find the sids of suppliers who supply some red part and some green part.
5. Find the sids of suppliers who supply every part.
6. Find the sids of suppliers who supply every red part.
7. Find the sids of suppliers who supply every red or green part.
8. Find the sids of suppliers who supply every red part or supply every green part.
9. Find pairs of sids such that the supplier with the ﬁrst sid charges more for some
part than the supplier with the second sid.
10. Find the pids of parts supplied by at least two diﬀerent suppliers.
11. Find the pids of the most expensive parts supplied by suppliers named Yosemite
Sham.
12. Find the pids of parts supplied by every supplier at less than $200. (If any supplier
either does not supply the part or charges more than $200 for it, the part is not
selected.)
Answer 4.3 In the answers below RA refers to Relational Algebra, TRC refers to
Tuple Relational Calculus and DRC refers to Domain Relational Calculus.
1.
RA
πsname(πsid((πpidσcolor=′red′Parts) ▷◁Catalog) ▷◁Suppliers)
TRC
{T | ∃T 1 ∈Suppliers(∃X ∈Parts(X.color =′ red′ ∧∃Y ∈Catalog
(Y.pid = X.pid ∧Y.sid = T 1.sid)) ∧T.sname = T 1.sname)}
DRC
{⟨Y ⟩| ⟨X, Y, Z⟩∈Suppliers ∧∃P, Q, R(⟨P, Q, R⟩∈Parts
∧R =′ red′ ∧∃I, J, K(⟨I, J, K⟩∈Catalog ∧J = P ∧I = X))}
SQL

30
Chapter 4
SELECT S.sname
FROM
Suppliers S, Parts P, Catalog C
WHERE
P.color=’red’ AND C.pid=P.pid AND C.sid=S.sid
2.
RA
πsid(πpid(σcolor=′red′∨color=′green′Parts) ▷◁catalog)
TRC
{T | ∃T 1 ∈Catalog(∃X ∈Parts((X.color = ‘red′ ∨X.color = ‘green′)
∧X.pid = T 1.pid) ∧T.sid = T 1.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∃A, B, C(⟨A, B, C⟩∈Parts
∧(C =′ red′ ∨C =′ green′) ∧A = Y )}
SQL
SELECT C.sid
FROM
Catalog C, Parts P
WHERE
(P.color = ‘red’ OR P.color = ‘green’)
AND P.pid = C.pid
3.
RA
ρ(R1, πsid((πpidσcolor=′red′Parts) ▷◁Catalog))
ρ(R2, πsidσaddress=′221P ackerStreet′Suppliers)
R1 ∪R2
TRC
{T | ∃T 1 ∈Catalog(∃X ∈Parts(X.color = ‘red′ ∧X.pid = T 1.pid)
∧T.sid = T 1.sid)
∨∃T 2 ∈Suppliers(T 2.address =′ 221PackerStreet′ ∧T.sid = T 2.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∃A, B, C(⟨A, B, C⟩∈Parts
∧C =′ red′ ∧A = Y )
∨∃P, Q(⟨X, P, Q⟩∈Suppliers ∧Q =′ 221PackerStreet′)}
SQL

Relational Algebra and Calculus
31
SELECT S.sid
FROM
Suppliers S
WHERE
S.address = ‘221 Packer street’
OR S.sid IN ( SELECT C.sid
FROM
Parts P, Catalog C
WHERE
P.color=’red’ AND P.pid = C.pid )
4.
RA
ρ(R1, πsid((πpidσcolor=′red′Parts) ▷◁Catalog))
ρ(R2, πsid((πpidσcolor=′green′Parts) ▷◁Catalog))
R1 ∩R2
TRC
{T | ∃T 1 ∈Catalog(∃X ∈Parts(X.color = ‘red′ ∧X.pid = T 1.pid)
∧∃T 2 ∈Catalog(∃Y ∈Parts(Y.color =′ green′ ∧Y.pid = T 2.pid)
∧T 2.sid = T 1.sid) ∧T.sid = T 1.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∃A, B, C(⟨A, B, C⟩∈Parts
∧C =′ red′ ∧A = Y )
∧∃P, Q, R(⟨P, Q, R⟩∈Catalog ∧∃E, F, G(⟨E, F, G⟩∈Parts
∧G =′ green′ ∧E = Q) ∧P = X)}
SQL
SELECT C.sid
FROM
Parts P, Catalog C
WHERE
P.color = ‘red’ AND P.pid = C.pid
AND EXISTS ( SELECT P2.pid
FROM
Parts P2, Catalog C2
WHERE
P2.color = ‘green’ AND C2.sid = C.sid
AND P2.pid = C2.pid )
5.
RA
(πsid,pidCatalog)/(πpidParts)
TRC
{T | ∃T 1 ∈Catalog(∀X ∈Parts(∃T 2 ∈Catalog
(T 2.pid = X.pid ∧T 2.sid = T 1.sid)) ∧T.sid = T 1.sid)}

32
Chapter 4
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∀⟨A, B, C⟩∈Parts
(∃⟨P, Q, R⟩∈Catalog(Q = A ∧P = X))}
SQL
SELECT C.sid
FROM
Catalog C
WHERE
NOT EXISTS (SELECT P.pid
FROM
Parts P
WHERE
NOT EXISTS (SELECT C1.sid
FROM
Catalog C1
WHERE
C1.sid = C.sid
AND C1.pid = P.pid))
6.
RA
(πsid,pidCatalog)/(πpidσcolor=′red′Parts)
TRC
{T | ∃T 1 ∈Catalog(∀X ∈Parts(X.color ̸= ‘red′
∨∃T 2 ∈Catalog(T 2.pid = X.pid ∧T 2.sid = T 1.sid))
∧T.sid = T 1.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∀⟨A, B, C⟩∈Parts
(C ̸= ‘red′ ∨∃⟨P, Q, R⟩∈Catalog(Q = A ∧P = X))}
SQL
SELECT C.sid
FROM
Catalog C
WHERE
NOT EXISTS (SELECT P.pid
FROM
Parts P
WHERE
P.color = ‘red’
AND (NOT EXISTS (SELECT C1.sid
FROM
Catalog C1
WHERE
C1.sid = C.sid AND
C1.pid = P.pid)))
7.
RA
(πsid,pidCatalog)/(πpidσcolor=′red′∨color=′green′Parts)

Relational Algebra and Calculus
33
TRC
{T | ∃T 1 ∈Catalog(∀X ∈Parts((X.color ̸= ‘red′
∧X.color ̸= ‘green′) ∨∃T 2 ∈Catalog
(T 2.pid = X.pid ∧T 2.sid = T 1.sid)) ∧T.sid = T 1.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∀⟨A, B, C⟩∈Parts
((C ̸= ‘red′ ∧C ̸= ‘green′) ∨∃⟨P, Q, R⟩∈Catalog
(Q = A ∧P = X))}
SQL
SELECT C.sid
FROM
Catalog C
WHERE
NOT EXISTS (SELECT P.pid
FROM
Parts P
WHERE
(P.color = ‘red’ OR P.color = ‘green’)
AND (NOT EXISTS (SELECT C1.sid
FROM
Catalog C1
WHERE
C1.sid = C.sid AND
C1.pid = P.pid)))
8.
RA
ρ(R1, ((πsid,pidCatalog)/(πpidσcolor=′red′Parts)))
ρ(R2, ((πsid,pidCatalog)/(πpidσcolor=′green′Parts)))
R1 ∪R2
TRC
{T | ∃T 1 ∈Catalog((∀X ∈Parts
(X.color ̸= ‘red′ ∨∃Y ∈Catalog(Y.pid = X.pid ∧Y.sid = T 1.sid))
∨∀Z ∈Parts(Z.color ̸= ‘green′ ∨∃P ∈Catalog
(P.pid = Z.pid ∧P.sid = T 1.sid))) ∧T.sid = T 1.sid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧(∀⟨A, B, C⟩∈Parts
(C ̸= ‘red′ ∨∃⟨P, Q, R⟩∈Catalog(Q = A ∧P = X))
∨∀⟨U, V, W⟩∈Parts(W ̸= ‘green′ ∨⟨M, N, L⟩∈Catalog
(N = U ∧M = X)))}

34
Chapter 4
SQL
SELECT C.sid
FROM
Catalog C
WHERE
(NOT EXISTS (SELECT P.pid
FROM
Parts P
WHERE
P.color = ‘red’ AND
(NOT EXISTS (SELECT C1.sid
FROM
Catalog C1
WHERE
C1.sid = C.sid AND
C1.pid = P.pid))))
OR ( NOT EXISTS (SELECT P1.pid
FROM
Parts P1
WHERE
P1.color = ‘green’ AND
(NOT EXISTS (SELECT C2.sid
FROM
Catalog C2
WHERE
C2.sid = C.sid AND
C2.pid = P1.pid))))
9.
RA
ρ(R1, Catalog)
ρ(R2, Catalog)
πR1.sid,R2.sid(σR1.pid=R2.pid∧R1.sid̸=R2.sid∧R1.cost>R2.cost(R1 × R2))
TRC
{T
|
∃T 1 ∈Catalog(∃T 2 ∈Catalog
(T 2.pid = T 1.pid ∧T 2.sid ̸= T 1.sid
∧T 2.cost < T1.cost ∧T.sid2 = T 2.sid)
∧T.sid1 = T 1.sid)}
DRC
{⟨X, P⟩| ⟨X, Y, Z⟩∈Catalog ∧∃P, Q, R
(⟨P, Q, R⟩∈Catalog ∧Q = Y ∧P ̸= X ∧R < Z)}
SQL
SELECT C1.sid, C2.sid
FROM
Catalog C1, Catalog C2
WHERE
C1.pid = C2.pid AND C1.sid ̸= C2.sid
AND C1.cost > C2.cost

Relational Algebra and Calculus
35
10.
RA
ρ(R1, Catalog)
ρ(R2, Catalog)
πR1.pidσR1.pid=R2.pid∧R1.sid̸=R2.sid(R1 × R2)
TRC
{T | ∃T 1 ∈Catalog(∃T 2 ∈Catalog
(T 2.pid = T 1.pid ∧T 2.sid ̸= T 1.sid)
∧T.pid = T 1.pid)}
DRC
{⟨X⟩| ⟨X, Y, Z⟩∈Catalog ∧∃A, B, C
(⟨A, B, C⟩∈Catalog ∧B = Y ∧A ̸= X)}
SQL
SELECT C.pid
FROM
Catalog C
WHERE
EXISTS (SELECT C1.sid
FROM
Catalog C1
WHERE
C1.pid = C.pid AND C1.sid ̸= C.sid )
11.
RA
ρ(R1, πsidσsname=′Y osemiteSham′Suppliers)
ρ(R2, R1 ▷◁Catalog)
ρ(R3, R2)
ρ(R4(1 →sid, 2 →pid, 3 →cost), σR3.cost<R2.cost(R3 × R2))
πpid(R2 −πsid,pid,costR4)
TRC
{T | ∃T 1 ∈Catalog(∃X ∈Suppliers
(X.sname =′ Y osemiteSham′ ∧X.sid = T 1.sid) ∧¬(∃S ∈Suppliers
(S.sname =′ Y osemiteSham′ ∧∃Z ∈Catalog
(Z.sid = S.sid ∧Z.cost > T1.cost))) ∧T.pid = T 1.pid)
DRC
{⟨Y ⟩| ⟨X, Y, Z⟩∈Catalog ∧∃A, B, C
(⟨A, B, C⟩∈Suppliers ∧C =′ Y osemiteSham′ ∧A = X)
∧¬(∃P, Q, R(⟨P, Q, R⟩∈Suppliers ∧R =′ Y osemiteSham′
∧∃I, J, K(⟨I, J, K⟩∈Catalog(I = P ∧K > Z))))}

36
Chapter 4
SQL
SELECT C.pid
FROM
Catalog C, Suppliers S
WHERE
S.sname = ‘Yosemite Sham’ AND C.sid = S.sid
AND C.cost ≥ALL (Select C2.cost
FROM Catalog C2, Suppliers S2
WHERE S2.sname = ‘Yosemite Sham’
AND C2.sid = S2.sid)
Exercise 4.4 Consider the Supplier-Parts-Catalog schema from the previous ques-
tion. State what the following queries compute:
1. πsname(πsid((σcolor=′red′Parts) ▷◁(σcost<100Catalog)) ▷◁Suppliers)
2. πsname(πsid((σcolor=′red′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers))
3. (πsname((σcolor=′red′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers)) ∩
(πsname((σcolor=′green′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers))
4. (πsid((σcolor=′red′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers)) ∩
(πsid((σcolor=′green′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers))
5. πsname((πsid,sname((σcolor=′red′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers)) ∩
(πsid,sname((σcolor=′green′Parts) ▷◁(σcost<100Catalog) ▷◁Suppliers)))
Answer 4.4 The statements can be interpreted as:
1. Find the Supplier names of the suppliers who supply a red part that costs less
than 100 dollars.
2. This Relational Algebra statement does not return anything because of the se-
quence of projection operators. Once the sid is projected, it is the only ﬁeld in
the set. Therefore, projecting on sname will not return anything.
3. Find the Supplier names of the suppliers who supply a red part that costs less
than 100 dollars and a green part that costs less than 100 dollars.
4. Find the Supplier ids of the suppliers who supply a red part that costs less than
100 dollars and a green part that costs less than 100 dollars.
5. Find the Supplier names of the suppliers who supply a red part that costs less
than 100 dollars and a green part that costs less than 100 dollars.

Relational Algebra and Calculus
37
Exercise 4.5 Consider the following relations containing airline ﬂight information:
Flights(ﬂno: integer, from: string, to: string,
distance: integer, departs: time, arrives: time)
Aircraft(aid: integer, aname: string, cruisingrange: integer)
Certiﬁed(eid: integer, aid: integer)
Employees(eid: integer, ename: string, salary: integer)
Note that the Employees relation describes pilots and other kinds of employees as well;
every pilot is certiﬁed for some aircraft (otherwise, he or she would not qualify as a
pilot), and only pilots are certiﬁed to ﬂy.
Write the following queries in relational algebra, tuple relational calculus, and domain
relational calculus. Note that some of these queries may not be expressible in relational
algebra (and, therefore, also not expressible in tuple and domain relational calculus)!
For such queries, informally explain why they cannot be expressed. (See the exercises
at the end of Chapter 5 for additional queries over the airline schema.)
1. Find the eids of pilots certiﬁed for some Boeing aircraft.
2. Find the names of pilots certiﬁed for some Boeing aircraft.
3. Find the aids of all aircraft that can be used on non-stop ﬂights from Bonn to
Madras.
4. Identify the ﬂights that can be piloted by every pilot whose salary is more than
$100,000.
5. Find the names of pilots who can operate planes with a range greater than 3,000
miles but are not certiﬁed on any Boeing aircraft.
6. Find the eids of employees who make the highest salary.
7. Find the eids of employees who make the second highest salary.
8. Find the eids of employees who are certiﬁed for the largest number of aircraft.
9. Find the eids of employees who are certiﬁed for exactly three aircraft.
10. Find the total amount paid to employees as salaries.
11. Is there a sequence of ﬂights from Madison to Timbuktu?
Each ﬂight in the
sequence is required to depart from the city that is the destination of the previous
ﬂight; the ﬁrst ﬂight must leave Madison, the last ﬂight must reach Timbuktu,
and there is no restriction on the number of intermediate ﬂights. Your query must
determine whether a sequence of ﬂights from Madison to Timbuktu exists for any
input Flights relation instance.

38
Chapter 4
Answer 4.5 In the answers below RA refers to Relational Algebra, TRC refers to
Tuple Relational Calculus and DRC refers to Domain Relational Calculus.
1.
RA
πeid(σaname=‘Boeing′(Aircraft ▷◁Certified))
TRC
{C.eid
|
C ∈Certified ∧
∃A ∈Aircraft(A.aid = C.aid ∧A.aname = ‘Boeing′)}
DRC
{⟨Ceid⟩
|
⟨Ceid, Caid⟩∈Certified ∧
∃Aid, AN, AR(⟨Aid, AN, AR⟩∈Aircraft
∧Aid = Caid ∧AN = ‘Boeing′)}
SQL
SELECT C.eid
FROM
Aircraft A, Certiﬁed C
WHERE
A.aid = C.aid AND A.aname = ‘Boeing’
2.
RA
πename(σaname=‘Boeing′(Aircraft ▷◁Certified ▷◁Employees))
TRC
{E.ename | E ∈Employees ∧∃C ∈Certified
(∃A ∈Aircraft(A.aid = C.aid ∧A.aname = ‘Boeing′ ∧E.eid = C.eid))}
DRC
{⟨EN⟩| ⟨Eid, EN, ES⟩∈Employees∧
∃Ceid, Caid(⟨Ceid, Caid⟩∈Certified∧
∃Aid, AN, AR(⟨Aid, AN, AR⟩∈Aircraft∧
Aid = Caid ∧AN = ‘Boeing′ ∧Eid = Ceid)}
SQL
SELECT E.ename
FROM
Aircraft A, Certiﬁed C, Employees E
WHERE
A.aid = C.aid AND A.aname = ‘Boeing’ AND E.eid = C.eid

Relational Algebra and Calculus
39
3.
RA
ρ(BonnT oMadrid, σfrom=‘Bonn′∧to=‘Madrid′(Flights))
πaid(σcruisingrange>distance(Aircraft × BonnT oMadrid))
TRC
{A.aid | A ∈Aircraft ∧∃F ∈Flights
(F.from = ‘Bonn′ ∧F.to = ‘Madrid′ ∧A.cruisingrange > F.distance)}
DRC
{Aid | ⟨Aid, AN, AR⟩∈Aircraft∧
(∃FN, FF, FT, FDi, FDe, FA(⟨FN, FF, FT, FDi, FDe, FA⟩∈Flights∧
FF = ‘Bonn′ ∧FT = ‘Madrid′ ∧FDi < AR))}
SQL
SELECT A.aid
FROM
Aircraft A, Flights F
WHERE
F.from = ‘Bonn’ AND F.to = ‘Madrid’ AND
A.cruisingrange > F.distance
4.
RA
πflno(σdistance<cruisingrange∧salary>100,000(Flights ▷◁Aircraft ▷◁
Certified ▷◁Employees)))
TRC {F.flno | F ∈Flights ∧∃A ∈Aircraft∃C ∈Certified
∃E ∈Employees(A.cruisingrange > F.distance ∧E.salary > 100, 000∧
A.aid = C.aid ∧E.eid = C.eid)}
DRC
{FN | ⟨FN, FF, FT, FDi, FDe, FA⟩∈Flights∧
∃Ceid, Caid(⟨Ceid, Caid⟩∈Certified∧
∃Aid, AN, AR(⟨Aid, AN, AR⟩∈Aircraft∧
∃Eid, EN, ES(⟨Eid, EN, ES⟩∈Employees
(AR > FDi ∧ES > 100, 000 ∧Aid = Caid ∧Eid = Ceid)}
SQL
SELECT E.ename
FROM
Aircraft A, Certiﬁed C, Employees E, Flights F
WHERE
A.aid = C.aid AND E.eid = C.eid AND
distance < cruisingrange AND salary > 100,000

40
Chapter 4
5.
RA ρ(R1, πeid(σcruisingrange>3000(Aircraft ▷◁Certified)))
πename(Employees ▷◁(R1 −πeid(σaname=‘Boeing′(Aircraft ▷◁Certified))))
TRC
{E.ename | E ∈Employees ∧∃C ∈Certified(∃A ∈Aircraft
(A.aid = C.aid ∧E.eid = C.eid ∧A.cruisingrange > 3000))∧
¬(∃C2 ∈Certified(∃A2 ∈Aircraft(A2.aname = ‘Boeing′ ∧C2.aid =
A2.aid ∧C2.eid = E.eid)))}
DRC
{⟨EN⟩| ⟨Eid, EN, ES⟩∈Employees∧
∃Ceid, Caid(⟨Ceid, Caid⟩∈Certified∧
∃Aid, AN, AR(⟨Aid, AN, AR⟩∈Aircraft∧
Aid = Caid ∧Eid = Ceid ∧AR > 3000))∧
¬(∃Aid2, AN2, AR2(⟨Aid2, AN2, AR2⟩∈Aircraft∧
∃Ceid2, Caid2(⟨Ceid2, Caid2⟩∈Certified
∧Aid2 = Caid2 ∧Eid = Ceid2 ∧AN2 = ‘Boeing′)))}
SQL
SELECT E.ename
FROM
Certiﬁed C, Employees E, Aircraft A
WHERE
A.aid = C.aid AND E.eid = C.eid AND A.cruisingrange > 3000
AND E.eid NOT IN ( SELECT C2.eid
FROM Certiﬁed C2, Aircraft A2
WHERE C2.aid = A2.aid AND A2.aname = ‘Boeing’ )
6.
RA
The approach to take is ﬁrst ﬁnd all the employees who do not have the
highest salary. Subtract these from the original list of employees and what
is left is the highest paid employees.
ρ(E1, Employees)
ρ(E2, Employees)
ρ(E3, πE2.eid(E1 ▷◁E1.salary>E2.salary E2)
(πeidE1) −E3
TRC
{E1.eid | E1 ∈Employees∧¬(∃E2 ∈Employees(E2.salary > E1.salary))}
DRC

Relational Algebra and Calculus
41
{⟨Eid1⟩| ⟨Eid1, EN1, ES1⟩∈Employees∧
¬(∃Eid2, EN2, ES2(⟨Eid2, EN2, ES2⟩∈Employees ∧ES2 > ES1))}
SQL
SELECT E.eid
FROM
Employees E
WHERE
E.salary = ( Select MAX (E2.salary)
FROM Employees E2 )
7.
RA
The approach taken is similar to the solution for the previous exercise. First
ﬁnd all the employees who do not have the highest salary. Remove these from
the original list of employees and what is left is the highest paid employees.
Remove the highest paid employees from the original list. What is left is the
second highest paid employees together with the rest of the employees. Then
ﬁnd the highest paid employees of this new list. This is the list of the second
highest paid employees.
ρ(E1, Employees)
ρ(E2, Employees)
ρ(E3, πE2.eid(E1 ▷◁E1.salary>E2.salary E2)
ρ(E4, E2 ▷◁E3)
ρ(E5, E2 ▷◁E3)
ρ(E6, πE5.eid(E4 ▷◁E1.salary>E5.salary E5)
(πeidE3) −E6
TRC
{E1.eid | E1 ∈Employees ∧∃E2 ∈Employees(E2.salary > E1.salary
∧¬(∃E3 ∈Employees(E3.salary > E2.salary)))}
DRC
{⟨Eid1⟩| ⟨Eid1, EN1, ES1⟩∈Employees∧
∃Eid2, EN2, ES2(⟨Eid2, EN2, ES2⟩∈Employees(ES2 > ES1)
∧¬(∃Eid3, EN3, ES3(⟨Eid3, EN3, ES3⟩∈Employees(ES3 > ES2))))}
SQL
SELECT E.eid
FROM
Employees E
WHERE
E.salary = (SELECT MAX (E2.salary)
FROM
Employees E2
WHERE
E2.salary ̸= (SELECT MAX (E3.salary)
FROM
Employees E3 ))

42
Chapter 4
8. This cannot be expressed in relational algebra (or calculus) because there is no
operator to count, and this query requires the ability to count up to a number
that depends on the data. The query can however be expressed in SQL as follows:
SELECT Temp.eid
FROM
( SELECT
C.eid AS eid, COUNT (C.aid) AS cnt,
FROM
Certiﬁed C
GROUP BY C.eid) AS Temp
WHERE
Temp.cnt = ( SELECT
MAX (Temp.cnt)
FROM
Temp)
9.
RA
The approach behind this query is to ﬁrst ﬁnd the employees who are certiﬁed
for at least three aircraft (they appear at least three times in the Certiﬁed
relation). Then ﬁnd the employees who are certiﬁed for at least four aircraft.
Subtract the second from the ﬁrst and what is left is the employees who are
certiﬁed for exactly three aircraft.
ρ(R1, Certified)
ρ(R2, Certified)
ρ(R3, Certified)
ρ(R4, Certified)
ρ(R5, πeid(σ(R1.eid=R2.eid=R3.eid)∧(R1.aid̸=R2.aid̸=R3.aid)(R1 × R2 × R3)))
ρ(R6, πeid(σ(R1.eid=R2.eid=R3.eid=R4.eid)∧(R1.aid̸=R2.aid̸=R3.aid̸=R4.aid)
(R1 × R2 × R3 × R4)))
R5 −R6
TRC
{C1.eid | C1 ∈Certified ∧∃C2 ∈Certified(∃C3 ∈Certified
(C1.eid = C2.eid ∧C2.eid = C3.eid∧
C1.aid ̸= C2.aid ∧C2.aid ̸= C3.aid ∧C3.aid ̸= C1.aid∧
¬(∃C4 ∈Certified
(C3.eid = C4.eid ∧C1.aid ̸= C4.aid∧
C2.aid ̸= C4.aid ∧C3.aid ̸= C4.aid))))}
DRC
{⟨CE1⟩| ⟨CE1, CA1⟩∈Certified∧
∃CE2, CA2(⟨CE2, CA2⟩∈Certified∧
∃CE3, CA3(⟨CE3, CA3⟩∈Certified∧
(CE1 = CE2 ∧CE2 = CE3∧
CA1 ̸= CA2 ∧CA2 ̸= CA3 ∧CA3 ̸= CA1∧
¬(∃CE4, CA4(⟨CE4, CA4⟩∈Certified∧

Relational Algebra and Calculus
43
(CE3 = CE4 ∧CA1 ̸= CA4∧
CA2 ̸= CA4 ∧CA3 ̸= CA4))))}
SQL
SELECT C1.eid
FROM
Certiﬁed C1, Certiﬁed C2, Certiﬁed C3
WHERE
(C1.eid = C2.eid AND C2.eid = C3.eid AND
C1.aid ̸= C2.aid AND C2.aid ̸= C3.aid AND C3.aid ̸= C1.aid)
EXCEPT
SELECT C4.eid
FROM
Certiﬁed C4, Certiﬁed C5, Certiﬁed C6, Certiﬁed C7,
WHERE
(C4.eid = C5.eid AND C5.eid = C6.eid AND C6.eid = C7.eid AND
C4.aid ̸= C5.aid AND C4.aid ̸= C6.aid AND C4.aid ̸= C7.aid AND
C5.aid ̸= C6.aid AND C5.aid ̸= C7.aid AND C6.aid ̸= C7.aid )
This could also be done in SQL using COUNT.
10. This cannot be expressed in relational algebra (or calculus) because there is no
operator to sum values. The query can however be expressed in SQL as follows:
SELECT SUM (E.salaries)
FROM
Employees E
11. This cannot be expressed in relational algebra or relational calculus or SQL. The
problem is that there is no restriction on the number of intermediate ﬂights. All
of the query methods could ﬁnd if there was a ﬂight directly from Madison to
Timbuktu and if there was a sequence of two ﬂights that started in Madison and
ended in Timbuktu. They could even ﬁnd a sequence of n ﬂights that started in
Madison and ended in Timbuktu as long as there is a static (i.e., data-independent)
upper bound on the number of intermediate ﬂights. (For large n, this would of
course be long and impractical, but at least possible.) In this query, however, the
upper bound is not static but dynamic (based upon the set of tuples in the Flights
relation).
In summary, if we had a static upper bound (say k), we could write an algebra
or SQL query that repeatedly computes (upto k) joins on the Flights relation. If
the upper bound is dynamic, then we cannot write such a query because k is not
known when writing the query.
Exercise 4.6 What is relational completeness?
If a query language is relationally
complete, can you write any desired query in that language?
Answer 4.6 Answer omitted.

44
Chapter 4
Exercise 4.7 What is an unsafe query? Give an example and explain why it is im-
portant to disallow such queries.
Answer 4.7 An unsafe query is a query in relational calculus that has an inﬁnite
number of results. An example of such a query is:
{S | ¬(S ∈Sailors)}
The query is for all things that are not sailors which of course is everything else. Clearly
there is an inﬁnite number of answers, and this query is unsafe. It is important to
disallow unsafe queries because we want to be able to get back to users with a list of
all the answers to a query after a ﬁnite amount of time.

5
SQL: QUERIES, CONSTRAINTS,
TRIGGERS
Online material is available for all exercises in this chapter on the book’s webpage at
http://www.cs.wisc.edu/~dbbook
This includes scripts to create tables for each exercise for use with Oracle, IBM DB2,
Microsoft SQL Server, Microsoft Access and MySQL.
Exercise 5.1 Consider the following relations:
Student(snum: integer, sname: string, major: string, level: string, age: integer)
Class(name: string, meets at: string, room: string, ﬁd: integer)
Enrolled(snum: integer, cname: string)
Faculty(ﬁd: integer, fname: string, deptid: integer)
The meaning of these relations is straightforward; for example, Enrolled has one record
per student-class pair such that the student is enrolled in the class.
Write the following queries in SQL. No duplicates should be printed in any of the
answers.
1. Find the names of all Juniors (level = JR) who are enrolled in a class taught by
I. Teach.
2. Find the age of the oldest student who is either a History major or enrolled in a
course taught by I. Teach.
3. Find the names of all classes that either meet in room R128 or have ﬁve or more
students enrolled.
4. Find the names of all students who are enrolled in two classes that meet at the
same time.
45

46
Chapter 5
5. Find the names of faculty members who teach in every room in which some class
is taught.
6. Find the names of faculty members for whom the combined enrollment of the
courses that they teach is less than ﬁve.
7. For each level, print the level and the average age of students for that level.
8. For all levels except JR, print the level and the average age of students for that
level.
9. For each faculty member that has taught classes only in room R128, print the
faculty member’s name and the total number of classes she or he has taught.
10. Find the names of students enrolled in the maximum number of classes.
11. Find the names of students not enrolled in any class.
12. For each age value that appears in Students, ﬁnd the level value that appears most
often. For example, if there are more FR level students aged 18 than SR, JR, or
SO students aged 18, you should print the pair (18, FR).
Answer 5.1 The answers are given below:
1.
SELECT DISTINCT S.Sname
FROM
Student S, Class C, Enrolled E, Faculty F
WHERE
S.snum = E.snum AND E.cname = C.name AND C.ﬁd = F.ﬁd AND
F.fname = ‘I.Teach’ AND S.level = ‘JR’
2.
SELECT MAX(S.age)
FROM
Student S
WHERE
(S.major = ‘History’)
OR S.snum IN (SELECT E.snum
FROM
Class C, Enrolled E, Faculty F
WHERE
E.cname = C.name AND C.ﬁd = F.ﬁd
AND F.fname = ‘I.Teach’ )
3.
SELECT
C.name
FROM
Class C
WHERE
C.room = ‘R128’
OR C.name IN (SELECT
E.cname
FROM
Enrolled E
GROUP BY E.cname
HAVING
COUNT (*) >= 5)

SQL: Queries, Constraints, Triggers
47
4.
SELECT DISTINCT S.sname
FROM
Student S
WHERE
S.snum IN (SELECT E1.snum
FROM
Enrolled E1, Enrolled E2, Class C1, Class C2
WHERE
E1.snum = E2.snum AND E1.cname <> E2.cname
AND E1.cname = C1.name
AND E2.cname = C2.name AND C1.meets at = C2.meets at)
5.
SELECT DISTINCT F.fname
FROM
Faculty F
WHERE
NOT EXISTS (( SELECT *
FROM
Class C )
EXCEPT
(SELECTC1.room
FROM
Class C1
WHERE
C1.ﬁd = F.ﬁd ))
6.
SELECT
DISTINCT F.fname
FROM
Faculty F
WHERE
5 > (SELECT COUNT (E.snum)
FROM
Class C, Enrolled E
WHERE
C.name = E.cname
AND
C.ﬁd = F.ﬁd)
7.
SELECT
S.level, AVG(S.age)
FROM
Student S
GROUP BY S.level
8.
SELECT
S.level, AVG(S.age)
FROM
Student S
WHERE
S.level <> ‘JR’
GROUP BY S.level
9.
SELECT
F.fname, COUNT(*) AS CourseCount
FROM
Faculty F, Class C
WHERE
F.ﬁd = C.ﬁd
GROUP BY F.ﬁd, F.fname
HAVING
EVERY ( C.room = ‘R128’ )
10.
SELECT
DISTINCT S.sname
FROM
Student S
WHERE
S.snum IN (SELECT
E.snum
FROM
Enrolled E
GROUP BY E.snum

48
Chapter 5
HAVING
COUNT (*) >= ALL (SELECT
COUNT (*)
FROM
Enrolled E2
GROUP BY E2.snum ))
11.
SELECT DISTINCT S.sname
FROM
Student S
WHERE
S.snum NOT IN (SELECT E.snum
FROM
Enrolled E )
12.
SELECT
S.age, S.level
FROM
Student S
GROUP BY S.age, S.level,
HAVING
S.level IN (SELECT
S1.level
FROM
Student S1
WHERE
S1.age = S.age
GROUP BY S1.level, S1.age
HAVING
COUNT (*) >= ALL (SELECT
COUNT (*)
FROM
Student S2
WHERE s1.age = S2.age
GROUP BY S2.level, S2.age))
Exercise 5.2 Consider the following schema:
Suppliers(sid: integer, sname: string, address: string)
Parts(pid: integer, pname: string, color: string)
Catalog(sid: integer, pid: integer, cost: real)
The Catalog relation lists the prices charged for parts by Suppliers. Write the following
queries in SQL:
1. Find the pnames of parts for which there is some supplier.
2. Find the snames of suppliers who supply every part.
3. Find the snames of suppliers who supply every red part.
4. Find the pnames of parts supplied by Acme Widget Suppliers and no one else.
5. Find the sids of suppliers who charge more for some part than the average cost of
that part (averaged over all the suppliers who supply that part).
6. For each part, ﬁnd the sname of the supplier who charges the most for that part.
7. Find the sids of suppliers who supply only red parts.
8. Find the sids of suppliers who supply a red part and a green part.

SQL: Queries, Constraints, Triggers
49
9. Find the sids of suppliers who supply a red part or a green part.
10. For every supplier that only supplies green parts, print the name of the supplier
and the total number of parts that she supplies.
11. For every supplier that supplies a green part and a red part, print the name and
price of the most expensive part that she supplies.
Answer 5.2 Answer omitted.
Exercise 5.3 The following relations keep track of airline ﬂight information:
Flights(ﬂno: integer, from: string, to: string, distance: integer,
departs: time, arrives: time, price: real)
Aircraft(aid: integer, aname: string, cruisingrange: integer)
Certiﬁed(eid: integer, aid: integer)
Employees(eid: integer, ename: string, salary: integer)
Note that the Employees relation describes pilots and other kinds of employees as well;
every pilot is certiﬁed for some aircraft, and only pilots are certiﬁed to ﬂy. Write each
of the following queries in SQL. (Additional queries using the same schema are listed
in the exercises for Chapter 4.)
1. Find the names of aircraft such that all pilots certiﬁed to operate them have
salaries more than $80,000.
2. For each pilot who is certiﬁed for more than three aircraft, ﬁnd the eid and the
maximum cruisingrange of the aircraft for which she or he is certiﬁed.
3. Find the names of pilots whose salary is less than the price of the cheapest route
from Los Angeles to Honolulu.
4. For all aircraft with cruisingrange over 1000 miles, ﬁnd the name of the aircraft
and the average salary of all pilots certiﬁed for this aircraft.
5. Find the names of pilots certiﬁed for some Boeing aircraft.
6. Find the aids of all aircraft that can be used on routes from Los Angeles to
Chicago.
7. Identify the routes that can be piloted by every pilot who makes more than
$100,000.
8. Print the enames of pilots who can operate planes with cruisingrange greater than
3000 miles but are not certiﬁed on any Boeing aircraft.

50
Chapter 5
9. A customer wants to travel from Madison to New York with no more than two
changes of ﬂight. List the choice of departure times from Madison if the customer
wants to arrive in New York by 6 p.m.
10. Compute the diﬀerence between the average salary of a pilot and the average
salary of all employees (including pilots).
11. Print the name and salary of every nonpilot whose salary is more than the average
salary for pilots.
12. Print the names of employees who are certiﬁed only on aircrafts with cruising
range longer than 1000 miles.
13. Print the names of employees who are certiﬁed only on aircrafts with cruising
range longer than 1000 miles, but on at least two such aircrafts.
14. Print the names of employees who are certiﬁed only on aircrafts with cruising
range longer than 1000 miles and who are certiﬁed on some Boeing aircraft.
Answer 5.3 The answers are given below:
1.
SELECT DISTINCT A.aname
FROM
Aircraft A
WHERE
A.Aid IN (SELECT C.aid
FROM
Certiﬁed C, Employees E
WHERE
C.eid = E.eid AND
NOT EXISTS ( SELECT *
FROM
Employees E1
WHERE
E1.eid = E.eid AND E1.salary < 80000 ))
2.
SELECT
C.eid, MAX (A.cruisingrange)
FROM
Certiﬁed C, Aircraft A
WHERE
C.aid = A.aid
GROUP BY C.eid
HAVING
COUNT (*) > 3
3.
SELECT DISTINCT E.ename
FROM
Employees E
WHERE
E.salary < ( SELECT MIN (F.price)
FROM
Flights F
WHERE
F.from = ‘Los Angeles’ AND F.to = ‘Honolulu’ )
4. Observe that aid is the key for Aircraft, but the question asks for aircraft names;
we deal with this complication by using an intermediate relation Temp:

SQL: Queries, Constraints, Triggers
51
SELECT Temp.name, Temp.AvgSalary
FROM
( SELECT
A.aid, A.aname AS name,
AVG (E.salary) AS AvgSalary
FROM
Aircraft A, Certiﬁed C, Employees E
WHERE
A.aid = C.aid AND
C.eid = E.eid AND A.cruisingrange > 1000
GROUP BY A.aid, A.aname ) AS Temp
5.
SELECT DISTINCT E.ename
FROM
Employees E, Certiﬁed C, Aircraft A
WHERE
E.eid = C.eid AND
C.aid = A.aid AND
A.aname LIKE ‘Boeing%’
6.
SELECT A.aid
FROM
Aircraft A
WHERE
A.cruisingrange > ( SELECT MIN (F.distance)
FROM
Flights F
WHERE
F.from = ‘Los Angeles’ AND F.to = ‘Chicago’ )
7.
SELECT DISTINCT F.from, F.to
FROM
Flights F
WHERE
NOT EXISTS ( SELECT *
FROM
Employees E
WHERE
E.salary > 100000
AND
NOT EXISTS (SELECT *
FROM
Aircraft A, Certiﬁed C
WHERE
A.cruisingrange > F.distance
AND E.eid = C.eid
AND A.aid = C.aid) )
8.
SELECT DISTINCT E.ename
FROM
Employees E
WHERE
E.eid IN ( ( SELECT C.eid
FROM
Certiﬁed C
WHERE
EXISTS ( SELECT A.aid
FROM
Aircraft A
WHERE
A.aid = C.aid
AND
A.cruisingrange > 3000 )
AND
NOT EXISTS ( SELECT A1.aid

52
Chapter 5
FROM
Aircraft A1
WHERE
A1.aid = C.aid
AND
A1.aname LIKE ‘Boeing%’ ))
9.
SELECT F.departs
FROM
Flights F
WHERE
F.ﬂno IN ( ( SELECT F0.ﬂno
FROM
Flights F0
WHERE
F0.from = ‘Madison’ AND F0.to = ‘New York’
AND F0.arrives < ‘18:00’ )
UNION
( SELECT F0.ﬂno
FROM
Flights F0, Flights F1
WHERE
F0.from = ‘Madison’ AND F0.to <> ‘New York’
AND F0.to = F1.from AND F1.to = ‘New York’
AND F1.departs > F0.arrives
AND F1.arrives < ‘18:00’ )
UNION
( SELECT F0.ﬂno
FROM
Flights F0, Flights F1, Flights F2
WHERE
F0.from = ‘Madison’
AND F0.to = F1.from
AND F1.to = F2.from
AND F2.to = ‘New York’
AND F0.to <> ‘New York’
AND F1.to <> ‘New York’
AND F1.departs > F0.arrives
AND F2.departs > F1.arrives
AND F2.arrives < ‘18:00’ ))
10.
SELECT Temp1.avg - Temp2.avg
FROM
(SELECT AVG (E.salary) AS avg
FROM
Employees E
WHERE
E.eid IN (SELECT DISTINCT C.eid
FROM Certiﬁed C )) AS Temp1,
(SELECT AVG (E1.salary) AS avg
FROM
Employees E1 ) AS Temp2
11.
SELECT E.ename, E.salary
FROM
Employees E
WHERE
E.eid NOT IN ( SELECT DISTINCT C.eid
FROM
Certiﬁed C )

SQL: Queries, Constraints, Triggers
53
AND E.salary > ( SELECT AVG (E1.salary)
FROM
Employees E1
WHERE
E1.eid IN
( SELECT DISTINCT C1.eid
FROM
Certiﬁed C1 ) )
12.
SELECT
E.ename
FROM
Employees E, Certiﬁed C, Aircraft A
WHERE
C.aid = A.aid AND E.eid = C.eid
GROUP BY E.eid, E.ename
HAVING
EVERY (A.cruisingrange > 1000)
13.
SELECT
E.ename
FROM
Employees E, Certiﬁed C, Aircraft A
WHERE
C.aid = A.aid AND E.eid = C.eid
GROUP BY E.eid, E.ename
HAVING
EVERY (A.cruisingrange > 1000) AND COUNT (*) > 1
14.
SELECT
E.ename
FROM
Employees E, Certiﬁed C, Aircraft A
WHERE
C.aid = A.aid AND E.eid = C.eid
GROUP BY E.eid, E.ename
HAVING
EVERY (A.cruisingrange > 1000) AND ANY (A.aname = ’Boeing’)
Exercise 5.4 Consider the following relational schema.
An employee can work in
more than one department; the pct time ﬁeld of the Works relation shows the percent-
age of time that a given employee works in a given department.
Emp(eid: integer, ename: string, age: integer, salary: real)
Works(eid: integer, did: integer, pct time: integer)
Dept(did: integer, dname: string, budget: real, managerid: integer)
Write the following queries in SQL:
1. Print the names and ages of each employee who works in both the Hardware
department and the Software department.
2. For each department with more than 20 full-time-equivalent employees (i.e., where
the part-time and full-time employees add up to at least that many full-time
employees), print the did together with the number of employees that work in
that department.
3. Print the name of each employee whose salary exceeds the budget of all of the
departments that he or she works in.

54
Chapter 5
sid
sname
rating
age
18
jones
3
30.0
41
jonah
6
56.0
22
ahab
7
44.0
63
moby
null
15.0
Figure 5.1
An Instance of Sailors
4. Find the managerids of managers who manage only departments with budgets
greater than $1 million.
5. Find the enames of managers who manage the departments with the largest bud-
gets.
6. If a manager manages more than one department, he or she controls the sum of all
the budgets for those departments. Find the managerids of managers who control
more than $5 million.
7. Find the managerids of managers who control the largest amounts.
8. Find the enames of managers who manage only departments with budgets larger
than $1 million, but at least one department with budget less than $5 million.
Answer 5.4 Answer omitted.
Exercise 5.5 Consider the instance of the Sailors relation shown in Figure 5.1.
1. Write SQL queries to compute the average rating, using AVG; the sum of the
ratings, using SUM; and the number of ratings, using COUNT.
2. If you divide the sum just computed by the count, would the result be the same
as the average? How would your answer change if these steps were carried out
with respect to the age ﬁeld instead of rating?
3. Consider the following query: Find the names of sailors with a higher rating than
all sailors with age < 21. The following two SQL queries attempt to obtain the
answer to this question. Do they both compute the result? If not, explain why.
Under what conditions would they compute the same result?
SELECT S.sname
FROM
Sailors S
WHERE
NOT EXISTS ( SELECT *
FROM
Sailors S2
WHERE
S2.age < 21
AND S.rating <= S2.rating )

SQL: Queries, Constraints, Triggers
55
SELECT *
FROM
Sailors S
WHERE
S.rating > ANY ( SELECT S2.rating
FROM
Sailors S2
WHERE
S2.age < 21 )
4. Consider the instance of Sailors shown in Figure 5.1. Let us deﬁne instance S1 of
Sailors to consist of the ﬁrst two tuples, instance S2 to be the last two tuples, and
S to be the given instance.
(a) Show the left outer join of S with itself, with the join condition being sid=sid.
(b) Show the right outer join of S with itself, with the join condition being
sid=sid.
(c) Show the full outer join of S with itself, with the join condition being sid=sid.
(d) Show the left outer join of S1 with S2, with the join condition being sid=sid.
(e) Show the right outer join of S1 with S2, with the join condition being sid=sid.
(f) Show the full outer join of S1 with S2, with the join condition being sid=sid.
Answer 5.5 The answers are shown below:
1.
SELECT AVG (S.rating) AS AVERAGE
FROM
Sailors S
SELECT SUM (S.rating)
FROM
Sailors S
SELECT COUNT (S.rating)
FROM
Sailors S
2. The result using SUM and COUNT would be smaller than the result using AV-
ERAGE if there are tuples with rating = NULL. This is because all the aggregate
operators, except for COUNT, ignore NULL values. So the ﬁrst approach would
compute the average over all tuples while the second approach would compute the
average over all tuples with non-NULL rating values. However, if the aggregation
is done on the age ﬁeld, the answers using both approaches would be the same
since the age ﬁeld does not take NULL values.
3. Only the ﬁrst query is correct. The second query returns the names of sailors with
a higher rating than at least one sailor with age < 21. Note that the answer to
the second query does not necessarily contain the answer to the ﬁrst query. In
particular, if all the sailors are at least 21 years old, the second query will return an
empty set while the ﬁrst query will return all the sailors. This is because the NOT
EXISTS predicate in the ﬁrst query will evaluate to true if its subquery evaluates

56
Chapter 5
4.
(a)
sid
sname
rating
age
sid
sname
rating
age
18
jones
3
30.0
18
jones
3
30.0
41
jonah
6
56.0
41
jonah
6
56.0
22
ahab
7
44.0
22
ahab
7
44.0
63
moby
null
15.0
63
moby
null
15.0
(b)
sid
sname
rating
age
sid
sname
rating
age
18
jones
3
30.0
18
jones
3
30.0
41
jonah
6
56.0
41
jonah
6
56.0
22
ahab
7
44.0
22
ahab
7
44.0
63
moby
null
15.0
63
moby
null
15.0
(c)
sid
sname
rating
age
sid
sname
rating
age
18
jones
3
30.0
18
jones
3
30.0
41
jonah
6
56.0
41
jonah
6
56.0
22
ahab
7
44.0
22
ahab
7
44.0
63
moby
null
15.0
63
moby
null
15.0
to an empty set, while the ANY predicate in the second query will evaluate to
false if its subquery evaluates to an empty set. The two queries give the same
results if and only if one of the following two conditions hold:
The Sailors relation is empty, or
There is at least one sailor with age > 21 in the Sailors relation, and for
every sailor s, either s has a higher rating than all sailors under 21 or s has
a rating no higher than all sailors under 21.
Exercise 5.6 Answer the following questions:
1. Explain the term impedance mismatch in the context of embedding SQL com-
mands in a host language such as C.
(d)
sid
sname
rating
age
sid
sname
rating
age
18
jones
3
30.0
null
null
null
null
41
jonah
6
56.0
null
null
null
null
(e)
sid
sname
rating
age
sid
sname
rating
age
null
null
null
null
22
ahab
7
44.0
null
null
null
null
63
moby
null
15.0

SQL: Queries, Constraints, Triggers
57
(f)
sid
sname
rating
age
sid
sname
rating
age
18
jones
3
30.0
null
null
null
null
41
jonah
6
56.0
null
null
null
null
null
null
null
null
22
ahab
7
44.0
null
null
null
null
63
moby
null
15.0
2. How can the value of a host language variable be passed to an embedded SQL
command?
3. Explain the WHENEVER command’s use in error and exception handling.
4. Explain the need for cursors.
5. Give an example of a situation that calls for the use of embedded SQL; that is, in-
teractive use of SQL commands is not enough, and some host language capabilities
are needed.
6. Write a C program with embedded SQL commands to address your example in
the previous answer.
7. Write a C program with embedded SQL commands to ﬁnd the standard deviation
of sailors’ ages.
8. Extend the previous program to ﬁnd all sailors whose age is within one standard
deviation of the average age of all sailors.
9. Explain how you would write a C program to compute the transitive closure of
a graph, represented as an SQL relation Edges(from, to), using embedded SQL
commands. (You need not write the program, just explain the main points to be
dealt with.)
10. Explain the following terms with respect to cursors: updatability, sensitivity, and
scrollability.
11. Deﬁne a cursor on the Sailors relation that is updatable, scrollable, and returns
answers sorted by age.
Which ﬁelds of Sailors can such a cursor not update?
Why?
12. Give an example of a situation that calls for dynamic SQL; that is, even embedded
SQL is not suﬃcient.
Answer 5.6 Answer omitted.
Exercise 5.7 Consider the following relational schema and brieﬂy answer the ques-
tions that follow:

58
Chapter 5
Emp(eid: integer, ename: string, age: integer, salary: real)
Works(eid: integer, did: integer, pct time: integer)
Dept(did: integer, budget: real, managerid: integer)
1. Deﬁne a table constraint on Emp that will ensure that every employee makes at
least $10,000.
2. Deﬁne a table constraint on Dept that will ensure that all managers have age > 30.
3. Deﬁne an assertion on Dept that will ensure that all managers have age > 30.
Compare this assertion with the equivalent table constraint. Explain which is
better.
4. Write SQL statements to delete all information about employees whose salaries
exceed that of the manager of one or more departments that they work in. Be
sure to ensure that all the relevant integrity constraints are satisﬁed after your
updates.
Answer 5.7 The answers are given below:
1. Deﬁne a table constraint on Emp that will ensure that every employee makes at
least $10,000
CREATE TABLE Emp ( eid
INTEGER,
ename
CHAR(10),
age
INTEGER ,
salary
REAL,
PRIMARY KEY (eid),
CHECK ( salary >= 10000 ))
2. Deﬁne a table constraint on Dept that will ensure that all managers have age > 30
CREATE TABLE Dept ( did
INTEGER,
buget
REAL,
managerid
INTEGER ,
PRIMARY KEY (did),
FOREIGN KEY (managerid) REFERENCES Emp,
CHECK (
( SELECT E.age FROM Emp E, Dept D)
WHERE E.eid = D.managerid ) > 30 )
3. Deﬁne an assertion on Dept that will ensure that all managers have age > 30
CREATE TABLE Dept ( did
INTEGER,
budget
REAL,
managerid
INTEGER ,
PRIMARY KEY (did) )

SQL: Queries, Constraints, Triggers
59
CREATE ASSERTION managerAge
CHECK ((SELECT E.age
FROM
Emp E, Dept D
WHERE
E.eid = D.managerid ) > 30 )
Since the constraint involves two relations, it is better to deﬁne it as an assertion,
independent of any one relation, rather than as a check condition on the Dept
relation. The limitation of the latter approach is that the condition is checked
only when the Dept relation is being updated. However, since age is an attribute
of the Emp relation, it is possible to update the age of a manager which violates the
constraint. So the former approach is better since it checks for potential violation
of the assertion whenever one of the relations is updated.
4. To write such statements, it is necessary to consider the constraints deﬁned over
the tables. We will assume the following:
CREATE TABLE Emp (
eid
INTEGER,
ename
CHAR(10),
age
INTEGER,
salary
REAL,
PRIMARY KEY (eid) )
CREATE TABLE Works ( eid
INTEGER,
did
INTEGER,
pcttime
INTEGER,
PRIMARY KEY (eid, did),
FOREIGN KEY (did) REFERENCES Dept,
FOREIGN KEY (eid) REFERENCES Emp,
ON DELETE CASCADE)
CREATE TABLE Dept (
did
INTEGER,
buget
REAL,
managerid
INTEGER ,
PRIMARY KEY (did),
FOREIGN KEY (managerid) REFERENCES Emp,
ON DELETE SET NULL)
Now, we can deﬁne statements to delete employees who make more than one of
their managers:
DELETE
FROM
Emp E
WHERE
E.eid IN (
SELECT W.eid
FROM
Work W, Emp E2, Dept D
WHERE
W.did = D.did

60
Chapter 5
AND
D.managerid = E2.eid
AND
E.salary > E2.salary )
Exercise 5.8 Consider the following relations:
Student(snum: integer, sname: string, major: string,
level: string, age: integer)
Class(name: string, meets at: time, room: string, ﬁd: integer)
Enrolled(snum: integer, cname: string)
Faculty(ﬁd: integer, fname: string, deptid: integer)
The meaning of these relations is straightforward; for example, Enrolled has one record
per student-class pair such that the student is enrolled in the class.
1. Write the SQL statements required to create these relations, including appropriate
versions of all primary and foreign key integrity constraints.
2. Express each of the following integrity constraints in SQL unless it is implied by
the primary and foreign key constraint; if so, explain how it is implied. If the
constraint cannot be expressed in SQL, say so. For each constraint, state what
operations (inserts, deletes, and updates on speciﬁc relations) must be monitored
to enforce the constraint.
(a) Every class has a minimum enrollment of 5 students and a maximum enroll-
ment of 30 students.
(b) At least one class meets in each room.
(c) Every faculty member must teach at least two courses.
(d) Only faculty in the department with deptid=33 teach more than three courses.
(e) Every student must be enrolled in the course called Math101.
(f) The room in which the earliest scheduled class (i.e., the class with the smallest
meets at value) meets should not be the same as the room in which the latest
scheduled class meets.
(g) Two classes cannot meet in the same room at the same time.
(h) The department with the most faculty members must have fewer than twice
the number of faculty members in the department with the fewest faculty
members.
(i) No department can have more than 10 faculty members.
(j) A student cannot add more than two courses at a time (i.e., in a single
update).
(k) The number of CS majors must be more than the number of Math majors.

SQL: Queries, Constraints, Triggers
61
(l) The number of distinct courses in which CS majors are enrolled is greater
than the number of distinct courses in which Math majors are enrolled.
(m) The total enrollment in courses taught by faculty in the department with
deptid=33 is greater than the number of Math majors.
(n) There must be at least one CS major if there are any students whatsoever.
(o) Faculty members from diﬀerent departments cannot teach in the same room.
Answer 5.8 Answer omitted.
Exercise 5.9 Discuss the strengths and weaknesses of the trigger mechanism. Con-
trast triggers with other integrity constraints supported by SQL.
Answer 5.9 A trigger is a procedure that is automatically invoked in response to a
speciﬁed change to the database. The advantages of the trigger mechanism include
the ability to perform an action based on the result of a query condition. The set of
actions that can be taken is a superset of the actions that integrity constraints can
take (i.e. report an error). Actions can include invoking new update, delete, or insert
queries, perform data deﬁnition statements to create new tables or views, or alter
security policies. Triggers can also be executed before or after a change is made to the
database (that is, use old or new data).
There are also disadvantages to triggers. These include the added complexity when
trying to match database modiﬁcations to trigger events. Also, integrity constraints
are incorporated into database performance optimization; it is more diﬃcult for a
database to perform automatic optimization with triggers. If database consistency is
the primary goal, then integrity constraints oﬀer the same power as triggers. Integrity
constraints are often easier to understand than triggers.
Exercise 5.10 Consider the following relational schema. An employee can work in
more than one department; the pct time ﬁeld of the Works relation shows the percent-
age of time that a given employee works in a given department.
Emp(eid: integer, ename: string, age: integer, salary: real)
Works(eid: integer, did: integer, pct time: integer)
Dept(did: integer, budget: real, managerid: integer)
Write SQL-92 integrity constraints (domain, key, foreign key, or CHECK constraints; or
assertions) or SQL:1999 triggers to ensure each of the following requirements, consid-
ered independently.
1. Employees must make a minimum salary of $1000.

62
Chapter 5
2. Every manager must be also be an employee.
3. The total percentage of all appointments for an employee must be under 100%.
4. A manager must always have a higher salary than any employee that he or she
manages.
5. Whenever an employee is given a raise, the manager’s salary must be increased to
be at least as much.
6. Whenever an employee is given a raise, the manager’s salary must be increased
to be at least as much.
Further, whenever an employee is given a raise, the
department’s budget must be increased to be greater than the sum of salaries of
all employees in the department.
Answer 5.10 Answer omitted.

6
DATABASE APPLICATION
DEVELOPMENT
Exercise 6.1 Brieﬂy answer the following questions.
1. Explain the following terms: Cursor, Embedded SQL, JDBC, SQLJ, stored pro-
cedure.
2. What are the diﬀerences between JDBC and SQLJ? Why do they both exist?
3. Explain the term stored procedure, and give examples why stored procedures are
useful.
Answer 6.1 The answers are given below:
1. A cursor enables individual row access of a relation by positioning itself at a row
and reading its contents. Embedded SQL refers to the usage of SQL commands
within a host program. JDBC stands for Java DataBase Connectivity and is an
interface that allows a Java program to easily connect to any database system.
SQLJ is a tool that allows SQL to be embedded directly into a Java program. A
stored procedure is program that runs on the database server and can be called
with a single SQL statement.
2. SQLJ provides embedded SQL statements. These SQL statements are static in
nature and thus are preprocessed and precompiled. For instance, syntax checking
and schema checking are done at compile time. JDBC allows dynamic queries
that are checked at runtime. SQLJ is easier to use than JDBC and is often a
better option for static queries. For dynamic queries, JDBC must still be used.
3. Stored procedures are programs that run on the database server and can be
called with a single SQL statement.
They are useful in situations where the
processing should be done on the server side rather than the client side. Also,
since the procedures are centralized to the server, code writing and maintenance
is simpliﬁed, because the client programs do not have to duplicate the application
logic. Stored procedures can also be used to reduce network communication; the
results of a stored procedure can be analyzed and kept on the database server.
63

64
Chapter 6
Exercise 6.2 Explain how the following steps are performed in JDBC:
1. Connect to a data source.
2. Start, commit, and abort transactions.
3. Call a stored procedure.
How are these steps performed in SQLJ?
Answer 6.2 The answers are given below:
1. Connecting to a data source in JDBC involves the creation of a Connection object.
Parameters for the connection are speciﬁed using a JDBC URL that contains
things like the network address of the database server and the username and
password for connecting.
SQLJ makes calls to the same JDBC drver for connecting to a data source and
uses the same type of JDBC URL.
2. Each connection can specify how to handle transactions. If the autocommit ﬂag is
set, each SQL statement is treated as a separate transaction. If the ﬂag is turned
oﬀ, there is a commit() function call that will actually commit the transaction.
The autocommit ﬂag can also be set in SQLJ. If the ﬂag is not set, transactions
are committed by passing a COMMIT SQL statement to the DBMS.
3. Stored procedures are called from JDBC using the CallableStatement class with
the SQL command {CALL StoredProcedureName}.
SQLJ also uses CALL StoredProcedureName to execute stored prodecures at the
DBMS.
Exercise 6.3 Compare exception handling and handling of warnings in embedded
SQL, dynamic SQL, JDBC, and SQLJ.
Answer 6.3 The answers are given below:
Embedded SQL: The SQLSTATE variable is used to check for errors after each
Embedded SQL statement is executed. If an error has occurred, program control
is transferred to a separate statement. This is done during the precompilation
step for static queries.
Dynamic SQL: For dynamic SQL, the SQL statement can change at runtime and
thus the error handling must also occur at runtime.
JDBC: In JDBC, programmers can use the try ... catch syntax to handle excep-
tions of type SQLException. The SQLWarning class is used for problems not as
severe as errors. They are not caught in the try ... catch statement and must be
checked independently with a getWarnings() function call.

Database Application Development
65
SQLJ: SQLJ uses the same mechanisms as JDBC to catch error and warnings.
Exercise 6.4 Answer the following questions.
1. Why do we need a precompiler to translate embedded SQL and SQLJ? Why do
we not need a precompiler for JDBC?
2. SQLJ and embedded SQL use variables in the host language to pass parameters
to SQL queries, whereas JDBC uses placeholders marked with a ‘?’. Explain the
diﬀerence, and why the diﬀerent mechanisms are needed.
Answer 6.4 Answer omitted.
Exercise 6.5 A dynamic web site generates HTML pages from information stored in
a database. Whenever a page is requested, is it dynamically assembled from static data
and data in a database, resulting in a database access. Connecting to the database
is usually a time-consuming process, since resources need to be allocated, and the
user needs to be authenticated. Therefore, connection pooling—setting up a pool
of persistent database connections and then reusing them for diﬀerent requests can
signiﬁcantly improve the performance of database-backed websites. Since servlets can
keep information beyond single requests, we can create a connection pool, and allocate
resources from it to new requests.
Write a connection pool class that provides the following methods:
Create the pool with a speciﬁed number of open connections to the database
system.
Obtain an open connection from the pool.
Release a connection to the pool.
Destroy the pool and close all connections.
Answer 6.5 The answer for this exercise is available online for instructors. To ﬁnd
out how to get access to instructor’s material, visit the book homepage at
http://www.cs.wisc.edu/~dbbook.

7
INTERNET APPLICATIONS
Exercise 7.1 Brieﬂy answer the following questions:
1. Explain the following terms and describe what they are used for: HTML, URL,
XML, Java, JSP, XSL, XSLT, servlet, cookie, HTTP, CSS, DTD.
2. What is CGI? Why was CGI introduced?
What are the disadvantages of an
architecture using CGI scripts?
3. What is the diﬀerence between a webserver and an application server?
What
funcionality do typical application servers provide?
4. When is an XML document well-formed? When is an XML document valid?
Answer 7.1 The answers are as follows.
1. HTTP (HyperText Transfer Protocol) is the communication protocol used to con-
nect clients with servers over the Internet. URL (Universal Resource Locator) is
a string that uniquely identiﬁes an internet address. HTML (HyperText Markup
Language) is a simple language used to enhance regular text by including special
tags.
CSS (Cascading Style Sheets) are used to deﬁne how to display HTML
documents. XML (Extensible Markup Language) allows users to deﬁne their own
markup tags in a document. XSL (Extensible Style Language) can be used to de-
scribe how an XML document should be displayed. XSLT (XML Transformation
Language) is a language that can transform input XML into diﬀerently struc-
tured XML. A DTD (Document Type Declaration) is a grammar that describes
how to use new tags in an XML document. Java is cross-platform interpreted
programming language. Servlets are pieces of Java code that run on the middle
tier or server layers and be used for any functionality that Java provides. JSP
(JavaServer Pages) are HTML pages with embedded servlet code.. Cookies are a
simple way to store persistent data at the client level.
66

Internet Applications
67
Figure 7.1
Solution to Exercise 7.2 (d)
.
2. CGI (Common Gateway Interface) speciﬁes how the web server communicates
other programs on the server. CGI programs are used to pass HTML form data
to other programs that process that data. Each page request will create a new
process on the server, which is a performance issue when requests are scaled up.
3. A web server handles the interaction with the client’s web browser. Application
servers are used to maintain a pool of processes for handling requests. Typically,
they are the middleware tier between the web server and the data sources such
as database systems. Application servers eliminate the problems with process-
creation overload and can also provide extra functionality like abstracting away
heterogeneous data sources and maintaining session state information.
4. An XML document is valid if it has an associated DTD and the document follows
the rules of the DTD. An XML document is well-formed if it follows three guide-
lines: (1) it starts with an XML declaration, (2) it contains a root element that
contains all other elements and (3) all elements are properly nested.
Exercise 7.2 Brieﬂy answer the following questions about the HTTP protocol:
1. What is a communication protocol?
2. What is the structure of an HTTP request message? What is the structure of an
HTTP response message? Why do HTTP messages carry a version ﬁeld?
3. What is a stateless protocol? Why was HTTP designed to be stateless?
4. Show the HTTP request message generated when you request the home page
of this book (http://www.cs.wisc.edu/~dbbook). Show the HTTP response
message that the server generates for that page.
Answer 7.2 Answer omitted.
Exercise 7.3 In this exercise, you are asked to write the functionality of a generic
shopping basket; you will use this in several subsequent project exercises. Write a set
of JSP pages that displays a shopping basket of items and allows users to add, remove,
and change the quantity of items. To do this, use a cookie storage scheme that stores
the following information:
The UserId of the user who owns the shopping basket.
The number of products stored in the shopping basket.

68
Chapter 7
A product id and a quantity for each product.
When manipulating cookies, remember to set the Expires property such that the
cookie can persist for a session or indeﬁnitely. Experiment with cookies using JSP and
make sure you know how to retrieve, set values, and delete the cookie.
You need to create ﬁve JSP pages to make your prototype complete:
Index Page (index.jsp): This is the main entry point.
It has a link that
directs the user to the Products page so they can start shopping.
Products Page (products.jsp): Shows a listing of all products in the database
with their descriptions and prices. This is the main page where the user ﬁlls out
the shopping basket. Each listed product should have a button next to it, which
adds it to the shopping basket. (If the item is already in the shopping basket,
it increments the quantity by one.) There should also be a counter to show the
total number of items currently in the shopping basket. Note that if a user has a
quantity of ﬁve of a single item in the shopping basket, the counter should indicate
a total quantity of ﬁve. The page also contains a button that directs the user to
the Cart page.
Cart Page (cart.jsp): Shows a listing of all items in the shopping basket
cookie. The listing for each item should include the product name, price, a text
box for the quantity (the user can change the quantity of items here), and a button
to remove the item from the shopping basket. This page has three other buttons:
one button to continue shopping (which returns the user to the Products page),
a second button to update the cookie with the altered quantities from the text
boxes, and a third button to place or conﬁrm the order, which directs the user to
the Conﬁrm page.
Conﬁrm Page (confirm.jsp): Lists the ﬁnal order. There are two buttons
on this page. One button cancels the order and the other submits the completed
order. The cancel button just deletes the cookie and returns the user to the Index
page. The submit button updates the database with the new order, deletes the
cookie, and returns the user to the Index page.
Exercise 7.4 In the previous exercise, replace the page products.jsp with the fol-
lowing search page search.jsp. This page allows users to search products by name or
description. There should be both a text box for the search text and radio buttons to
allow the user to choose between search-by-name and search-by-description (as well as
a submit button to retrieve the results). The page that handles search results should
be modeled after products.jsp (as described in the previous exercise) and be called
products.jsp. It should retrieve all records where the search text is a substring of
the name or description (as chosen by the user). To integrate this with the previous
exercise, simply replace all the links to products.jsp with search.jsp.

Internet Applications
69
Exercise 7.5 Write a simple authentication mechanism (without using encrypted
transfer of passwords, for simplicity). We say a user is authenticated if she has pro-
vided a valid username-password combination to the system; otherwise, we say the
user is not authenticated. Assume for simplicity that you have a database schema that
stores only a customer id and a password:
Passwords(cid: integer, username: string, password: string)
1. How and where are you going to track when a user is ‘logged on’ to the system?
2. Design a page that allows a registered user to log on to the system.
3. Design a page header that checks whether the user visiting this page is logged in.
Exercise 7.6 (Due to JeﬀDerstadt) TechnoBooks.com is in the process of reorganiz-
ing its website. A major issue is how to eﬃciently handle a large number of search
results. In a human interaction study, it found that modem users typically like to view
20 search results at a time, and it would like to program this logic into the system.
Queries that return batches of sorted results are called top N queries. (See Section 23
for a discussion of database support for top N queries.) For example, results 1-20 are
returned, then results 21-40, then 41-60, and so on. Diﬀerent techniques are used for
performing top N queries and TechnoBooks.com would like you to implement two of
them.
Infrastructure:
Create a database with a table called Books and populate it with
some books, using the format that follows. This gives you 111 books in your database
with a title of AAA, BBB, CCC, DDD, or EEE, but the keys are not sequential for
books with the same title.
Books(bookid: INTEGER, title: CHAR(80), author: CHAR(80), price: REAL)
For i = 1 to 111 {
Insert the tuple (i, “AAA”, “AAA Author”, 5.99)
i = i + 1
Insert the tuple (i, “BBB”, “BBB Author”, 5.99)
i = i + 1
Insert the tuple (i, “CCC”, “CCC Author”, 5.99)
i = i + 1
Insert the tuple (i, “DDD”, “DDD Author”, 5.99)
i = i + 1
Insert the tuple (i, “EEE”, “EEE Author”, 5.99)
}

70
Chapter 7
Placeholder Technique:
The simplest approach to top N queries is to store a
placeholder for the ﬁrst and last result tuples, and then perform the same query. When
the new query results are returned, you can iterate to the placeholders and return the
previous or next 20 results.
Tuples Shown
Lower Placeholder
Previous Set
Upper Placeholder
Next Set
1-20
1
None
20
21-40
21-40
21
1-20
40
41-60
41-60
41
21-40
60
61-80
Write a webpage in JSP that displays the contents of the Books table, sorted by the
Title and BookId, and showing the results 20 at a time. There should be a link (where
appropriate) to get the previous 20 results or the next 20 results. To do this, you can
encode the placeholders in the Previous or Next Links as follows. Assume that you
are displaying records 21–40. Then the previous link is display.jsp?lower=21 and
the next link is display.jsp?upper=40.
You should not display a previous link when there are no previous results; nor should
you show a Next link if there are no more results. When your page is called again to
get another batch of results, you can perform the same query to get all the records,
iterate through the result set until you are at the proper starting point, then display
20 more results.
What are the advantages and disadvantages of this technique?
Query Constraints Technique:
A second technique for performing top N queries
is to push boundary constraints into the query (in the WHERE clause) so that the query
returns only results that have not yet been displayed. Although this changes the query,
fewer results are returned and it saves the cost of iterating up to the boundary. For
example, consider the following table, sorted by (title, primary key).

Internet Applications
71
Batch
Result Number
Title
Primary Key
1
1
AAA
105
1
2
BBB
13
1
3
CCC
48
1
4
DDD
52
1
5
DDD
101
2
6
DDD
121
2
7
EEE
19
2
8
EEE
68
2
9
FFF
2
2
10
FFF
33
3
11
FFF
58
3
12
FFF
59
3
13
GGG
93
3
14
HHH
132
3
15
HHH
135
In batch 1, rows 1 through 5 are displayed, in batch 2 rows 6 through 10 are displayed,
and so on. Using the placeholder technique, all 15 results would be returned for each
batch. Using the constraint technique, batch 1 displays results 1-5 but returns results
1-15, batch 2 will display results 6-10 but returns only results 6-15, and batch 3 will
display results 11-15 but return only results 11-15.
The constraint can be pushed into the query because of the sorting of this table.
Consider the following query for batch 2 (displaying results 6-10):
EXEC SQL SELECT B.Title
FROM
Books B
WHERE
(B.Title = ’DDD’ AND B.BookId > 101) OR (B.Title > ’DDD’)
ORDER BY B.Title, B.BookId
This query ﬁrst selects all books with the title ‘DDD,’ but with a primary key that is
greater than that of record 5 (record 5 has a primary key of 101). This returns record
6. Also, any book that has a title after ‘DDD’ alphabetically is returned. You can
then display the ﬁrst ﬁve results.
The following information needs to be retained to have Previous and Next buttons
that return more results:
Previous: The title of the ﬁrst record in the previous set, and the primary key
of the ﬁrst record in the previous set.
Next: The title of the ﬁrst record in the next set; the primary key of the ﬁrst
record in the next set.

72
Chapter 7
These four pieces of information can be encoded into the Previous and Next buttons as
in the previous part. Using your database table from the ﬁrst part, write a JavaServer
Page that displays the book information 20 records at a time. The page should include
Previous and Next buttons to show the previous or next record set if there is one. Use
the constraint query to get the Previous and Next record sets.

8
OVERVIEW OF STORAGE AND
INDEXING
Exercise 8.1 Answer the following questions about data on external storage in a
DBMS:
1. Why does a DBMS store data on external storage?
2. Why are I/O costs important in a DBMS?
3. What is a record id? Given a record’s id, how many I/Os are needed to fetch it
into main memory?
4. What is the role of the buﬀer manager in a DBMS? What is the role of the disk
space manager? How do these layers interact with the ﬁle and access methods
layer?
Answer 8.1 The answer to each question is given below.
1. A DBMS stores data on external storage because the quantity of data is vast, and
must persist across program executions.
2. I/O costs are of primary important to a DBMS because these costs typically
dominate the time it takes to run most database operations.
Optimizing the
amount of I/O’s for an operation can result in a substantial increase in speed in
the time it takes to run that operation.
3. A record id, or rid for short, is a unique identiﬁer for a particular record in a set
of records. An rid has the property that we can identify the disk address of the
page containing the record by using the rid. The number of I/O’s required to read
a record, given a rid, is therefore 1 I/O.
4. In a DBMS, the buﬀer manager reads data from persistent storage into memory as
well as writes data from memory into persistent storage. The disk space manager
manages the available physical storage space of data for the DBMS. When the ﬁle
73

74
Chapter 8
and access methods layer needs to process a page, it asks the buﬀer manager to
fetch the page and put it into memory if it is not all ready in memory. When the
ﬁles and access methods layer needs additional space to hold new records in a ﬁle,
it asks the disk space manager to allocate an additional disk page.
Exercise 8.2 Answer the following questions about ﬁles and indexes:
1. What operations are supported by the ﬁle of records abstraction?
2. What is an index on a ﬁle of records? What is a search key for an index? Why
do we need indexes?
3. What alternatives are available for the data entries in an index?
4. What is the diﬀerence between a primary index and a secondary index? What is
a duplicate data entry in an index? Can a primary index contain duplicates?
5. What is the diﬀerence between a clustered index and an unclustered index? If an
index contains data records as ‘data entries,’ can it be unclustered?
6. How many clustered indexes can you create on a ﬁle? Would you always create
at least one clustered index for a ﬁle?
7. Consider Alternatives (1), (2) and (3) for ‘data entries’ in an index, as discussed
in Section 8.2. Are all of them suitable for secondary indexes? Explain.
Answer 8.2 Answer omitted.
Exercise 8.3 Consider a relation stored as a randomly ordered ﬁle for which the only
index is an unclustered index on a ﬁeld called sal. If you want to retrieve all records
with sal > 20, is using the index always the best alternative? Explain.
Answer 8.3 No. In this case, the index is unclustered, each qualifying data entry
could contain an rid that points to a distinct data page, leading to as many data page
I/Os as the number of data entries that match the range query. In this situation, using
index is actually worse than ﬁle scan.
Exercise 8.4 Consider the instance of the Students relation shown in Figure 8.1,
sorted by age: For the purposes of this question, assume that these tuples are stored
in a sorted ﬁle in the order shown; the ﬁrst tuple is on page 1 the second tuple is also
on page 1; and so on. Each page can store up to three data records; so the fourth tuple
is on page 2.
Explain what the data entries in each of the following indexes contain. If the order of
entries is signiﬁcant, say so and explain why. If such an index cannot be constructed,
say so and explain why.

Overview of Storage and Indexing
75
sid
name
login
age
gpa
53831
Madayan
madayan@music
11
1.8
53832
Guldu
guldu@music
12
2.0
53666
Jones
jones@cs
18
3.4
53688
Smith
smith@ee
19
3.2
53650
Smith
smith@math
19
3.8
Figure 8.1
An Instance of the Students Relation, Sorted by age
1. An unclustered index on age using Alternative (1).
2. An unclustered index on age using Alternative (2).
3. An unclustered index on age using Alternative (3).
4. A clustered index on age using Alternative (1).
5. A clustered index on age using Alternative (2).
6. A clustered index on age using Alternative (3).
7. An unclustered index on gpa using Alternative (1).
8. An unclustered index on gpa using Alternative (2).
9. An unclustered index on gpa using Alternative (3).
10. A clustered index on gpa using Alternative (1).
11. A clustered index on gpa using Alternative (2).
12. A clustered index on gpa using Alternative (3).
Answer 8.4 Answer omitted.
Exercise 8.5 Explain the diﬀerence between Hash indexes and B+-tree indexes. In
particular, discuss how equality and range searches work, using an example.
Answer 8.5 A Hash index is constructed by using a hashing function that quickly
maps an search key value to a speciﬁc location in an array-like list of elements called
buckets. The buckets are often constructed such that there are more bucket locations
than there are possible search key values, and the hashing function is chosen so that
it is not often that two search key values hash to the same bucket. A B+-tree index
is constructed by sorting the data on the search key and maintaining a hierarchical
search data structure that directs searches to the correct page of data entries.

76
Chapter 8
Insertions and deletions in a hash based index are relatively simple.
If two search
values hash to the same bucket, called a collision, a linked list is formed connecting
multiple records in a single bucket. In the case that too many of these collisions occur,
the number of buckets is increased. Alternatively, maintaining a B+-tree’s hierarchical
search data structure is considered more costly since it must be updated whenever there
are insertions and deletions in the data set. In general, most insertions and deletions
will not modify the data structure severely, but every once in awhile large portions of
the tree may need to be rewritten when they become over-ﬁlled or under-ﬁlled with
data entries.
Hash indexes are especially good at equality searches because they allow a record look
up very quickly with an average cost of 1.2 I/Os. B+-tree indexes, on the other hand,
have a cost of 3-4 I/Os per individual record lookup. Assume we have the employee
relation with primary key eid and 10,000 records total. Looking up all the records
individually would cost 12,000 I/Os for Hash indexes, but 30,000-40,000 I/Os for B+-
tree indexes.
For range queries, hash indexes perform terribly since they could conceivably read as
many pages as there are records since the data is not sorted in any clear grouping or
set. On the other hand, B+-tree indexes have a cost of 3-4 I/Os plus the number of
qualifying pages or tuples, for clustered or unclustered B+-trees respectively. Assume
we have the employees example again with 10,000 records and 10 records per page.
Also assume that there is an index on sal and query of age ¿ 20,000, such that there
are 5,000 qualifying tuples. The hash index could cost as much as 100,000 I/Os since
every page could be read for every record. It is not clear with a hash index how we
even go about searching for every possible number greater than 20,000 since decimals
could be used. An unclustered B+-tree index would have a cost of 5,004 I/Os, while
a clustered B+-tree index would have a cost of 504 I/Os. It helps to have the index
clustered whenever possible.
Exercise 8.6 Fill in the I/O costs in Figure 8.2.
File
Scan
Equality
Range
Insert
Delete
T ype
Search
Search
Heap ﬁle
Sorted ﬁle
Clustered ﬁle
Unclustered tree index
Unclustered hash index
Figure 8.2
I/O Cost Comparison

Overview of Storage and Indexing
77
Answer 8.6 Answer omitted.
Exercise 8.7 If you were about to create an index on a relation, what considerations
would guide your choice? Discuss:
1. The choice of primary index.
2. Clustered versus unclustered indexes.
3. Hash versus tree indexes.
4. The use of a sorted ﬁle rather than a tree-based index.
5. Choice of search key for the index. What is a composite search key, and what
considerations are made in choosing composite search keys? What are index-only
plans, and what is the inﬂuence of potential index-only evaluation plans on the
choice of search key for an index?
Answer 8.7 The answer to each question is given below.
1. The choice of the primary key is made based on the semantics of the data. If we
need to retrieve records based on the value of the primary key, as is likely, we
should build an index using this as the search key. If we need to retrieve records
based on the values of ﬁelds that do not constitute the primary key, we build (by
deﬁnition) a secondary index using (the combination of) these ﬁelds as the search
key.
2. A clustered index oﬀers much better range query performance, but essentially the
same equality search performance (modulo duplicates) as an unclustered index.
Further, a clustered index is typically more expensive to maintain than an unclus-
tered index. Therefore, we should make an index be clustered only if range queries
are important on its search key. At most one of the indexes on a relation can be
clustered, and if range queries are anticipated on more than one combination of
ﬁelds, we have to choose the combination that is most important and make that
be the search key of the clustered index.
3. If it is likely that ranged queries are going to be performed often, then we should
use a B+-tree on the index for the relation since hash indexes cannot perform
range queries.
If it is more likely that we are only going to perform equality
queries, for example the case of social security numbers, than hash indexes are
the best choice since they allow for the faster retrieval than B+-trees by 2-3 I/Os
per request.
4. First of all, both sorted ﬁles and tree-based indexes oﬀer fast searches. Insertions
and deletions, though, are much faster for tree-based indexes than sorted ﬁles. On

78
Chapter 8
the other hand scans and range searches with many matches are much faster for
sorted ﬁles than tree-based indexes. Therefore, if we have read-only data that is
not going to be modiﬁed often, it is better to go with a sorted ﬁle, whereas if we
have data that we intend to modify often, then we should go with a tree-based
index.
5. A composite search key is a key that contains several ﬁelds. A composite search
key can support a broader range as well as increase the possibility for an index-
only plan, but are more costly to maintain and store. An index-only plan is query
evaluation plan where we only need to access the indexes for the data records, and
not the data records themselves, in order to answer the query. Obviously, index-
only plans are much faster than regular plans since it does not require reading of
the data records. If it is likely that we are going to performing certain operations
repeatly that only require accessing one ﬁeld, for example the average value of a
ﬁeld, it would be an advantage to create a search key on this ﬁeld since we could
then accomplish it with an index-only plan.
Exercise 8.8 Consider a delete speciﬁed using an equality condition. For each of the
ﬁve ﬁle organizations, what is the cost if no record qualiﬁes? What is the cost if the
condition is not on a key?
Answer 8.8 Answer omitted.
Exercise 8.9 What main conclusions can you draw from the discussion of the ﬁve
basic ﬁle organizations discussed in Section
8.4?
Which of the ﬁve organizations
would you choose for a ﬁle where the most frequent operations are as follows?
1. Search for records based on a range of ﬁeld values.
2. Perform inserts and scans, where the order of records does not matter.
3. Search for a record based on a particular ﬁeld value.
Answer 8.9 The main conclusion about the ﬁve ﬁle organizations is that all ﬁve have
their own advantages and disadvantages. No one ﬁle organization is uniformly superior
in all situations. The choice of appropriate structures for a given data set can have a
signiﬁcant impact upon performance. An unordered ﬁle is best if only full ﬁle scans
are desired. A hash indexed ﬁle is best if the most common operation is an equality
selection. A sorted ﬁle is best if range selections are desired and the data is static; a
clustered B+ tree is best if range selections are important and the data is dynamic.
An unclustered B+ tree index is useful for selections over small ranges, especially if
we need to cluster on another search key to support some common query.
1. Using these ﬁelds as the search key, we would choose a sorted ﬁle organization or
a clustered B+ tree depending on whether the data is static or not.

Overview of Storage and Indexing
79
2. Heap ﬁle would be the best ﬁt in this situation.
3. Using this particular ﬁeld as the search key, choosing a hash indexed ﬁle would
be the best.
Exercise 8.10 Consider the following relation:
Emp(eid: integer, sal: integer, age: real, did: integer)
There is a clustered index on eid and an unclustered index on age.
1. How would you use the indexes to enforce the constraint that eid is a key?
2. Give an example of an update that is deﬁnitely speeded up because of the available
indexes. (English description is suﬃcient.)
3. Give an example of an update that is deﬁnitely slowed down because of the indexes.
(English description is suﬃcient.)
4. Can you give an example of an update that is neither speeded up nor slowed down
by the indexes?
Answer 8.10 Answer omitted.
Exercise 8.11 Consider the following relations:
Emp(eid: integer, ename: varchar, sal: integer, age: integer, did: integer)
Dept(did: integer, budget: integer, ﬂoor: integer, mgr eid: integer)
Salaries range from $10,000 to $100,000, ages vary from 20 to 80, each department has
about ﬁve employees on average, there are 10 ﬂoors, and budgets vary from $10,000
to $1 million. You can assume uniform distributions of values.
For each of the following queries, which of the listed index choices would you choose to
speed up the query? If your database system does not consider index-only plans (i.e.,
data records are always retrieved even if enough information is available in the index
entry), how would your answer change? Explain brieﬂy.
1. Query: Print ename, age, and sal for all employees.
(a) Clustered hash index on ⟨ename, age, sal⟩ﬁelds of Emp.
(b) Unclustered hash index on ⟨ename, age, sal⟩ﬁelds of Emp.
(c) Clustered B+ tree index on ⟨ename, age, sal⟩ﬁelds of Emp.

80
Chapter 8
(d) Unclustered hash index on ⟨eid, did⟩ﬁelds of Emp.
(e) No index.
2. Query: Find the dids of departments that are on the 10th ﬂoor and have a budget
of less than $15,000.
(a) Clustered hash index on the floor ﬁeld of Dept.
(b) Unclustered hash index on the floor ﬁeld of Dept.
(c) Clustered B+ tree index on ⟨floor, budget⟩ﬁelds of Dept.
(d) Clustered B+ tree index on the budget ﬁeld of Dept.
(e) No index.
Answer 8.11 The answer to each question is given below.
1. We should create an unclustered hash index on ⟨ename, age, sal⟩ﬁelds of Emp
(b) since then we could do an index only scan. If our system does not include
index only plans then we shouldn’t create an index for this query (e). Since this
query requires us to access all the Emp records, an index won’t help us any, and
so should we access the records using a ﬁlescan.
2. We should create a clustered dense B+ tree index (c) on ⟨floor, budget⟩ﬁelds of
Dept, since the records would be ordered on these ﬁelds then. So when executing
this query, the ﬁrst record with floor = 10 must be retrieved, and then the other
records with floor = 10 can be read in order of budget. Note that this plan,
which is the best for this query, is not an index-only plan (must look up dids).

9
STORING DATA: DISKS AND FILES
Exercise 9.1 What is the most important diﬀerence between a disk and a tape?
Answer 9.1 Tapes are sequential devices that do not support direct access to a desired
page. We must essentially step through all pages in order. Disks support direct access
to a desired page.
Exercise 9.2 Explain the terms seek time, rotational delay, and transfer time.
Answer 9.2 Answer omitted.
Exercise 9.3 Both disks and main memory support direct access to any desired lo-
cation (page). On average, main memory accesses are faster, of course. What is the
other important diﬀerence between the two (from the perspective of the time required
to access a desired page)?
Answer 9.3 The time to access a disk page is not constant. It depends on the location
of the data. Accessing to some data might be much faster than to others. It is diﬀerent
for memory. The time to access memory is uniform for most computer systems.
Exercise 9.4 If you have a large ﬁle that is frequently scanned sequentially, explain
how you would store the pages in the ﬁle on a disk.
Answer 9.4 Answer omitted.
Exercise 9.5 Consider a disk with a sector size of 512 bytes, 2000 tracks per surface,
50 sectors per track, ﬁve double-sided platters, and average seek time of 10 msec.
1. What is the capacity of a track in bytes? What is the capacity of each surface?
What is the capacity of the disk?
81

82
Chapter 9
2. How many cylinders does the disk have?
3. Give examples of valid block sizes. Is 256 bytes a valid block size? 2048? 51200?
4. If the disk platters rotate at 5400 rpm (revolutions per minute), what is the
maximum rotational delay?
5. If one track of data can be transferred per revolution, what is the transfer rate?
Answer 9.5
1.
bytes/track = bytes/sector × sectors/track = 512 × 50 = 25K
bytes/surface = bytes/track × tracks/surface = 25K × 2000 = 50, 000K
bytes/disk = bytes/surface × surfaces/disk = 50, 000K × 5 × 2 = 500, 000K
2. The number of cylinders is the same as the number of tracks on each platter,
which is 2000.
3. The block size should be a multiple of the sector size. We can see that 256 is not
a valid block size while 2048 is. 51200 is not a valid block size in this case because
block size cannot exceed the size of a track, which is 25600 bytes.
4. If the disk platters rotate at 5400rpm, the time required for one complete rotation,
which is the maximum rotational delay, is
1
5400 × 60 = 0.011seconds
. The average rotational delay is half of the rotation time, 0.006 seconds.
5. The capacity of a track is 25K bytes. Since one track of data can be transferred
per revolution, the data transfer rate is
25K
0.011 = 2, 250Kbytes/second
Exercise 9.6 Consider again the disk speciﬁcations from Exercise 9.5, and suppose
that a block size of 1024 bytes is chosen. Suppose that a ﬁle containing 100,000 records
of 100 bytes each is to be stored on such a disk and that no record is allowed to span
two blocks.
1. How many records ﬁt onto a block?
2. How many blocks are required to store the entire ﬁle?
If the ﬁle is arranged
sequentially on the disk, how many surfaces are needed?
3. How many records of 100 bytes each can be stored using this disk?

Storing Data: Disks and Files
83
4. If pages are stored sequentially on disk, with page 1 on block 1 of track 1, what
page is stored on block 1 of track 1 on the next disk surface? How would your
answer change if the disk were capable of reading and writing from all heads in
parallel?
5. What time is required to read a ﬁle containing 100,000 records of 100 bytes each
sequentially? Again, how would your answer change if the disk were capable of
reading/writing from all heads in parallel (and the data was arranged optimally)?
6. What is the time required to read a ﬁle containing 100,000 records of 100 bytes
each in a random order? To read a record, the block containing the record has
to be fetched from disk. Assume that each block request incurs the average seek
time and rotational delay.
Answer 9.6 Answer omitted.
Exercise 9.7 Explain what the buﬀer manager must do to process a read request for
a page. What happens if the requested page is in the pool but not pinned?
Answer 9.7 When a page is requested the buﬀer manager does the following:
1. The buﬀer pool is checked to see if it contains the requested page. If the page is in
the pool, skip to step 2. If the page is not in the pool, it is brought in as follows:
(a) A frame is chosen for replacement, using the replacement policy.
(b) If the frame chosen for replacement is dirty, it is ﬂushed (the page it contains
is written out to disk).
(c) The requested page is read into the frame chosen for replacement.
2. The requested page is pinned (the pin count of the chosen frame is incremented)
and its address is returned to the requester.
Note that if the page is not pinned, it could be removed from buﬀer pool even if it is
actually needed in main memory. Pinning a page prevents it from being removed from
the pool.
Exercise 9.8 When does a buﬀer manager write a page to disk?
Answer 9.8 Answer omitted.
Exercise 9.9 What does it mean to say that a page is pinned in the buﬀer pool? Who
is responsible for pinning pages? Who is responsible for unpinning pages?

84
Chapter 9
Answer 9.9
1. Pinning a page means the pin count of its frame is incremented.
Pinning a page guarantees higher-level DBMS software that the page will not be
removed from the buﬀer pool by the buﬀer manager. That is, another ﬁle page
will not be read into the frame containing this page until it is unpinned by this
requestor.
2. It is the buﬀer manager’s responsibility to pin a page.
3. It is the responsibility of the requestor of that page to tell the buﬀer manager to
unpin a page.
Exercise 9.10 When a page in the buﬀer pool is modiﬁed, how does the DBMS ensure
that this change is propagated to the disk? (Explain the role of the buﬀer manager as
well as the modiﬁer of the page.)
Answer 9.10 Answer omitted.
Exercise 9.11 What happens if a page is requested when all pages in the buﬀer pool
are dirty?
Answer 9.11 If there are some unpinned pages, the buﬀer manager chooses one by
using a replacement policy, ﬂushes this page, and then replaces it with the requested
page.
If there are no unpinned pages, the buﬀer manager has to wait until an unpinned page
is available (or signal an error condition to the page requestor).
Exercise 9.12 What is sequential ﬂooding of the buﬀer pool?
Answer 9.12 Answer omitted.
Exercise 9.13 Name an important capability of a DBMS buﬀer manager that is not
supported by a typical operating system’s buﬀer manager.
Answer 9.13
1. Pinning a page to prevent it from being replaced.
2. Ability to explicitly force a single page to disk.
Exercise 9.14 Explain the term prefetching. Why is it important?
Answer 9.14 Answer omitted.

Storing Data: Disks and Files
85
Exercise 9.15 Modern disks often have their own main memory caches, typically
about 1 MB, and use this to prefetch pages. The rationale for this technique is the em-
pirical observation that, if a disk page is requested by some (not necessarily database!)
application, 80% of the time the next page is requested as well. So the disk gambles
by reading ahead.
1. Give a nontechnical reason that a DBMS may not want to rely on prefetching
controlled by the disk.
2. Explain the impact on the disk’s cache of several queries running concurrently,
each scanning a diﬀerent ﬁle.
3. Is this problem addressed by the DBMS buﬀer manager prefetching pages? Ex-
plain.
4. Modern disks support segmented caches, with about four to six segments, each of
which is used to cache pages from a diﬀerent ﬁle. Does this technique help, with
respect to the preceding problem? Given this technique, does it matter whether
the DBMS buﬀer manager also does prefetching?
Answer 9.15
1. The pre-fetching done at the disk level varies widely across diﬀerent
drives and manufacturers, and pre-fetching is suﬃciently important to a DBMS
that one would like it to be independent of speciﬁc hardware support.
2. If there are many queries running concurrently, the request of a page from diﬀerent
queries can be interleaved. In the worst case, it cause the cache miss on every
page request, even with disk pre-fetching.
3. If we have pre-fetching oﬀered by DBMS buﬀer manager, the buﬀer manager can
predict the reference pattern more accurately. In particular, a certain number
of buﬀer frames can be allocated per active scan for pre-fetching purposes, and
interleaved requests would not compete for the same frames.
4. Segmented caches can work in a similar fashion to allocating buﬀer frames for each
active scan (as in the above answer). This helps to solve some of the concurrency
problem, but will not be useful at all if more ﬁles are being accessed than the
number of segments. In this case, the DBMS buﬀer manager should still prefer to
do pre-fetching on its own to handle a larger number of ﬁles, and to predict more
complicated access patterns.
Exercise 9.16 Describe two possible record formats. What are the trade-oﬀs between
them?
Answer 9.16 Answer omitted.
Exercise 9.17 Describe two possible page formats. What are the trade-oﬀs between
them?

86
Chapter 9
Answer 9.17 Two possible page formats are: consecutive slots and slot directory
The consecutive slots organization is mostly used for ﬁxed length record formats. It
handles the deletion by using bitmaps or linked lists.
The slot directory organization maintains a directory of slots for each page, with a
⟨record oﬀset, record length⟩pair per slot.
The slot directory is an indirect way to get the oﬀset of an entry. Because of this indi-
rection, deletion is easy. It is accomplished by setting the length ﬁeld to 0. And records
can easily be moved around on the page without changing their external identiﬁer.
Exercise 9.18 Consider the page format for variable-length records that uses a slot
directory.
1. One approach to managing the slot directory is to use a maximum size (i.e., a
maximum number of slots) and allocate the directory array when the page is
created. Discuss the pros and cons of this approach with respect to the approach
discussed in the text.
2. Suggest a modiﬁcation to this page format that would allow us to sort records (ac-
cording to the value in some ﬁeld) without moving records and without changing
the record ids.
Answer 9.18 Answer omitted.
Exercise 9.19 Consider the two internal organizations for heap ﬁles (using lists of
pages and a directory of pages) discussed in the text.
1. Describe them brieﬂy and explain the trade-oﬀs. Which organization would you
choose if records are variable in length?
2. Can you suggest a single page format to implement both internal ﬁle organiza-
tions?
Answer 9.19
1. The linked-list approach is a little simpler, but ﬁnding a page with
suﬃcient free space for a new record (especially with variable length records) is
harder. We have to essentially scan the list of pages until we ﬁnd one with enough
space, whereas the directory organization allows us to ﬁnd such a page by simply
scanning the directory, which is much smaller than the entire ﬁle. The directory
organization is therefore better, especially with variable length records.
2. A page format with previous and next page pointers would help in both cases.
Obviously, such a page format allows us to build the linked list organization; it is
also useful for implementing the directory in the directory organization.

Storing Data: Disks and Files
87
Exercise 9.20 Consider a list-based organization of the pages in a heap ﬁle in which
two lists are maintained: a list of all pages in the ﬁle and a list of all pages with free
space. In contrast, the list-based organization discussed in the text maintains a list of
full pages and a list of pages with free space.
1. What are the trade-oﬀs, if any? Is one of them clearly superior?
2. For each of these organizations, describe a suitable page format.
Answer 9.20 Answer omitted.
Exercise 9.21 Modern disk drives store more sectors on the outer tracks than the
inner tracks. Since the rotation speed is constant, the sequential data transfer rate is
also higher on the outer tracks. The seek time and rotational delay are unchanged.
Considering this information, explain good strategies for placing ﬁles with the following
kinds of access patterns:
1. Frequent, random accesses to a small ﬁle (e.g., catalog relations).
2. Sequential scans of a large ﬁle (e.g., selection from a relation with no index).
3. Random accesses to a large ﬁle via an index (e.g., selection from a relation via the
index).
4. Sequential scans of a small ﬁle.
Answer 9.21
1. Place the ﬁle in the middle tracks. Sequential speed is not an issue
due to the small size of the ﬁle, and the seek time is minimized by placing ﬁles in
the center.
2. Place the ﬁle in the outer tracks. Sequential speed is most important and outer
tracks maximize it.
3. Place the ﬁle and index on the inner tracks. The DBMS will alternately access
pages of the index and of the ﬁle, and so the two should reside in close proximity
to reduce seek times. By placing the ﬁle and the index on the inner tracks we also
save valuable space on the faster (outer) tracks for other ﬁles that are accessed
sequentially.
4. Place small ﬁles in the inner half of the disk. A scan of a small ﬁle is eﬀectively
random I/O because the cost is dominated by the cost of the initial seek to the
beginning of the ﬁle.
Exercise 9.22 Why do frames in the buﬀer pool have a pin count instead of a pin
ﬂag?
Answer 9.22 Answer omitted.

10
TREE-STRUCTURED INDEXING
Exercise 10.1 Consider the B+ tree index of order d = 2 shown in Figure 10.1.
1. Show the tree that would result from inserting a data entry with key 9 into this
tree.
2. Show the B+ tree that would result from inserting a data entry with key 3 into the
original tree. How many page reads and page writes does the insertion require?
3. Show the B+ tree that would result from deleting the data entry with key 8 from
the original tree, assuming that the left sibling is checked for possible redistribu-
tion.
4. Show the B+ tree that would result from deleting the data entry with key 8
from the original tree, assuming that the right sibling is checked for possible
redistribution.
5. Show the B+ tree that would result from starting with the original tree, inserting
a data entry with key 46 and then deleting the data entry with key 52.
6. Show the B+ tree that would result from deleting the data entry with key 91 from
the original tree.
Root
32* 39*
41*45*
52* 58*
73* 80*
91* 99*
85
73
50
27*
18*
10*
8*
6*
5*
2*
1*
8
18
32
40
Figure 10.1
Tree for Exercise 10.1
88

Tree-Structured Indexing
89
73
85
Root
50
8
18
32
40
32* 39*
18* 27*
41* 45*
52* 58*
73* 80*
91* 99*
2*
1*
5*
6*
8*
10*
9*
Figure 10.2
73
85
2*
1*
99*
91*
80*
73*
58*
52*
45*
41*
39*
32*
27*
18*
32
40
Root
50
18
8
10*
8*
6*
5*
3*
5
Figure 10.3
7. Show the B+ tree that would result from starting with the original tree, inserting
a data entry with key 59, and then deleting the data entry with key 91.
8. Show the B+ tree that would result from successively deleting the data entries
with keys 32, 39, 41, 45, and 73 from the original tree.
Answer 10.1
1. The data entry with key 9 is inserted on the second leaf page. The
resulting tree is shown in ﬁgure 10.2.
2. The data entry with key 3 goes on the ﬁrst leaf page F. Since F can accommodate
at most four data entries (d = 2), F splits. The lowest data entry of the new leaf
is given up to the ancestor which also splits. The result can be seen in ﬁgure 10.3.
The insertion will require 5 page writes, 4 page reads and allocation of 2 new
pages.
3. The data entry with key 8 is deleted, resulting in a leaf page N with less than
two data entries. The left sibling L is checked for redistribution. Since L has
more than two data entries, the remaining keys are redistributed between L and
N, resulting in the tree in ﬁgure 10.4.
4. As is part 3, the data entry with key 8 is deleted from the leaf page N. N’s
right sibling R is checked for redistribution, but R has the minimum number of

90
Chapter 10
73
85
Root
50
18
32
40
32* 39*
18* 27*
41* 45*
52* 58*
73* 80*
91* 99*
2*
1*
5*
10*
6*
6
Figure 10.4
73
85
Root
50
8
32*
39*
41*
45*
52*
58*
73*
80*
91*
99*
27*
18*
10*
6*
5*
2*
1*
32
40
Figure 10.5
keys. Therefore the two siblings merge. The key in the ancestor which distin-
guished between the newly merged leaves is deleted. The resulting tree is shown
in ﬁgure 10.5.
5. The data entry with key 46 can be inserted without any structural changes in
the tree. But the removal of the data entry with key 52 causes its leaf page L to
merge with a sibling (we chose the right sibling). This results in the removal of a
key in the ancestor A of L and thereby lowering the number of keys on A below
the minimum number of keys. Since the left sibling B of A has more than the
minimum number of keys, redistribution between A and B takes place. The ﬁnal
tree is depicted in ﬁgure 10.6.
6. Deleting the data entry with key 91 causes a scenario similar to part 5. The result
can be seen in ﬁgure 10.7.
7. The data entry with key 59 can be inserted without any structural changes in the
tree. No sibling of the leaf page with the data entry with key 91 is aﬀected by the
insert. Therefore deleting the data entry with key 91 changes the tree in a way
very similar to part 6. The result is depicted in ﬁgure 10.8.

Tree-Structured Indexing
91
Root
8
91*
99*
6*
5*
2*
1*
80*
73*
58*
46*
45*
41*
39*
32*
27*
18*
10*
8*
18
32
40
85
50
Figure 10.6
Root
8
6*
5*
2*
1*
45*
41*
39*
32*
27*
18*
10*
8*
18
32
40
50
99*
80*
73*
58*
52*
73
Figure 10.7
Root
8
6*
5*
2*
1*
45*
41*
39*
32*
27*
18*
10*
8*
18
32
40
50
99*
80*
73*
58*
52*
73
59*
Figure 10.8

92
Chapter 10
6*
5*
2*
1*
27*
18*
10*
8*
52*
58*
80*
99*
91*
Root
8
18
50
73
Figure 10.9
10
20
30
80
35
42
50
65
90
98
A
B
C
30* 31*
36* 38*
42* 43*
51* 52* 56* 60*
68* 69* 70* 79*
81* 82*
94* 95* 96* 97*
98* 99*
105*
L1
L2
L3
L4
L5
L6
L7
L8
I1
I2
I3
100*
Figure 10.10
Tree for Exercise 10.2
8. Considering checking the right sibling for possible merging ﬁrst, the successive
deletion of the data entries with keys 32, 39, 41, 45 and 73 results in the tree
shown in ﬁgure 10.9.
Exercise 10.2 Consider the B+ tree index shown in Figure 10.10, which uses Alter-
native (1) for data entries. Each intermediate node can hold up to ﬁve pointers and
four key values. Each leaf can hold up to four records, and leaf nodes are doubly
linked as usual, although these links are not shown in the ﬁgure. Answer the following
questions.
1. Name all the tree nodes that must be fetched to answer the following query: “Get
all records with search key greater than 38.”

Tree-Structured Indexing
93
2. Show the B+ tree that would result from inserting a record with search key 109
into the tree.
3. Show the B+ tree that would result from deleting the record with search key 81
from the original tree.
4. Name a search key value such that inserting it into the (original) tree would cause
an increase in the height of the tree.
5. Note that subtrees A, B, and C are not fully speciﬁed. Nonetheless, what can you
infer about the contents and the shape of these trees?
6. How would your answers to the preceding questions change if this were an ISAM
index?
7. Suppose that this is an ISAM index. What is the minimum number of insertions
needed to create a chain of three overﬂow pages?
Answer 10.2 Answer omitted.
Exercise 10.3 Answer the following questions:
1. What is the minimum space utilization for a B+ tree index?
2. What is the minimum space utilization for an ISAM index?
3. If your database system supported both a static and a dynamic tree index (say,
ISAM and B+ trees), would you ever consider using the static index in preference
to the dynamic index?
Answer 10.3 The answer to each question is given below.
1. By the deﬁnition of a B+ tree, each index page, except for the root, has at least
d and at most 2d key entries. Therefore—with the exception of the root—the
minimum space utilization guaranteed by a B+ tree index is 50 percent.
2. The minimum space utilization by an ISAM index depends on the design of the
index and the data distribution over the lifetime of ISAM index. Since an ISAM
index is static, empty spaces in index pages are never ﬁlled (in contrast to a
B+ tree index, which is a dynamic index).
Therefore the space utilization of
ISAM index pages is usually close to 100 percent by design. However, there is no
guarantee for leaf pages’ utilization.
3. A static index without overﬂow pages is faster than a dynamic index on inserts
and deletes, since index pages are only read and never written. If the set of keys
that will be inserted into the tree is known in advance, then it is possible to build

94
Chapter 10
Root
17
24
30
2*
3*
5*
7*
14* 16*
19* 20* 22*
24*
27* 29*
33* 34* 38* 39*
13
Figure 10.11
Tree for Exercise 10.5
a static index which reserves enough space for all possible future inserts. Also if
the system goes periodically oﬀline, static indices can be rebuilt and scaled to
the current occupancy of the index. Infrequent or scheduled updates are ﬂags for
when to consider a static index structure.
Exercise 10.4 Suppose that a page can contain at most four data values and that all
data values are integers. Using only B+ trees of order 2, give examples of each of the
following:
1. A B+ tree whose height changes from 2 to 3 when the value 25 is inserted. Show
your structure before and after the insertion.
2. A B+ tree in which the deletion of the value 25 leads to a redistribution. Show
your structure before and after the deletion.
3. A B+ tree in which the deletion of the value 25 causes a merge of two nodes but
without altering the height of the tree.
4. An ISAM structure with four buckets, none of which has an overﬂow page. Fur-
ther, every bucket has space for exactly one more entry. Show your structure
before and after inserting two additional values, chosen so that an overﬂow page
is created.
Answer 10.4 Answer omitted.
Exercise 10.5 Consider the B+ tree shown in Figure 10.11.
1. Identify a list of ﬁve data entries such that:
(a) Inserting the entries in the order shown and then deleting them in the op-
posite order (e.g., insert a, insert b, delete b, delete a) results in the original
tree.

Tree-Structured Indexing
95
(b) Inserting the entries in the order shown and then deleting them in the op-
posite order (e.g., insert a, insert b, delete b, delete a) results in a diﬀerent
tree.
2. What is the minimum number of insertions of data entries with distinct keys that
will cause the height of the (original) tree to change from its current value (of 1)
to 3?
3. Would the minimum number of insertions that will cause the original tree to
increase to height 3 change if you were allowed to insert duplicates (multiple data
entries with the same key), assuming that overﬂow pages are not used for handling
duplicates?
Answer 10.5 The answer to each question is given below.
1. The answer to each part is given below.
(a) One example is the set of ﬁve data entries with keys 17, 18, 13, 15, and 25.
Inserting 17 and 18 will cause the tree to split and gain a level. Inserting 13,
15, and 25 does change the tree structure any further, so deleting them in
reverse order causes no structure change. When 18 is deleted, redistribution
will be possible from an adjacent node since one node will contain only the
value 17, and its right neighbor will contain 19, 20, and 22. Finally, when 17
is deleted, no redistribution will be possible so the tree will loose a level and
will return to the original tree.
(b) Inserting and deleting the set 13, 15, 18, 25, and 4 will cause a change in the
tree structure. When 4 is inserted, the right most leave will split causing the
tree to gain a level. When it is deleted, the tree will not shrink in size. Since
inserts 13, 15, 18, and 25 did not aﬀect the right most node, their deletion
will not change the altered structure either.
2. Let us call the current tree depicted in Figure 10.11 T . T has 16 data entries.
The smallest tree S of height 3 which is created exclusively through inserts has
(1∗2∗3∗3)∗2+1 = 37 data entries in its leaf pages. S has 18 leaf pages with two
data entries each and one leaf page with three data entries. T has already four
leaf pages which have more than two data entries; they can be ﬁlled and made
to split, but after each spilt, one of the two pages will still has three data entries
remaining. Therefore the smallest tree of height 3 which can possibly be created
from T only through inserts has (1 ∗2 ∗3 ∗3) ∗2 + 4 = 40 data entries. Therefore
the minimum number of entries that will cause the height of T to change to 3 is
40 −16=24.
3. The argument in part 2 does not assume anything about the data entries to be
inserted; it is valid if duplicates can be inserted as well. Therefore the solution
does not change.

96
Chapter 10
Exercise 10.6 Answer Exercise 10.5 assuming that the tree is an ISAM tree! (Some
of the examples asked for may not exist—if so, explain brieﬂy.)
Answer 10.6 Answer omitted.
Exercise 10.7 Suppose that you have a sorted ﬁle and want to construct a dense
primary B+ tree index on this ﬁle.
1. One way to accomplish this task is to scan the ﬁle, record by record, inserting
each one using the B+ tree insertion procedure. What performance and storage
utilization problems are there with this approach?
2. Explain how the bulk-loading algorithm described in the text improves upon this
scheme.
Answer 10.7
1. This approach is likely to be quite expensive, since each entry
requires us to start from the root and go down to the appropriate leaf page.
Even though the index level pages are likely to stay in the buﬀer pool between
successive requests, the overhead is still considerable.
Also, according to the
insertion algorithm, each time a node splits, the data entries are redistributed
evenly to both nodes. This leads to a ﬁxed page utilization of 50%
2. The bulk loading algorithm has good performance and space utilization compared
with the repeated inserts approach. Since the B+ tree is grown from the bottom
up, the bulk loading algorithm allows the administrator to pre-set the amount
each index and data page should be ﬁlled.
This allows good performance for
future inserts, and supports some desired space utilization.
Exercise 10.8 Assume that you have just built a dense B+ tree index using Alterna-
tive (2) on a heap ﬁle containing 20,000 records. The key ﬁeld for this B+ tree index
is a 40-byte string, and it is a candidate key. Pointers (i.e., record ids and page ids)
are (at most) 10-byte values. The size of one disk page is 1000 bytes. The index was
built in a bottom-up fashion using the bulk-loading algorithm, and the nodes at each
level were ﬁlled up as much as possible.
1. How many levels does the resulting tree have?
2. For each level of the tree, how many nodes are at that level?
3. How many levels would the resulting tree have if key compression is used and it
reduces the average size of each key in an entry to 10 bytes?
4. How many levels would the resulting tree have without key compression but with
all pages 70 percent full?

Tree-Structured Indexing
97
Answer 10.8 Answer omitted.
Exercise 10.9 The algorithms for insertion and deletion into a B+ tree are presented
as recursive algorithms. In the code for insert, for instance, a call is made at the
parent of a node N to insert into (the subtree rooted at) node N, and when this call
returns, the current node is the parent of N. Thus, we do not maintain any ‘parent
pointers’ in nodes of B+ tree. Such pointers are not part of the B+ tree structure
for a good reason, as this exercise demonstrates. An alternative approach that uses
parent pointers—again, remember that such pointers are not part of the standard B+
tree structure!—in each node appears to be simpler:
Search to the appropriate leaf using the search algorithm; then insert the
entry and split if necessary, with splits propagated to parents if necessary
(using the parent pointers to ﬁnd the parents).
Consider this (unsatisfactory) alternative approach:
1. Suppose that an internal node N is split into nodes N and N2. What can you
say about the parent pointers in the children of the original node N?
2. Suggest two ways of dealing with the inconsistent parent pointers in the children
of node N.
3. For each of these suggestions, identify a potential (major) disadvantage.
4. What conclusions can you draw from this exercise?
Answer 10.9 The answer to each question is given below.
1. The parent pointers in either d or d + 1 of the children of the original node N are
not valid any more: they still point to N, but they should point to N2.
2. One solution is to adjust all parent pointers in the children of the original node
N which became children of N2. Another solution is to leave the pointers during
the insert operation and to adjust them later when the page is actually needed
and read into memory anyway.
3. The ﬁrst solution requires at least d+1 additional page reads (and sometime later,
page writes) on an insert, which would result in a remarkable slowdown. In the
second solution mentioned above, a child M, which has a parent pointer to be
adjusted, is updated if an operation is performed which actually reads M into
memory (maybe on a down path from the root to a leaf page). But this solution
modiﬁes M and therefore requires sometime later a write of M, which might not
have been necessary if there were no parent pointers.

98
Chapter 10
sid
name
login
age
gpa
53831
Madayan
madayan@music
11
1.8
53832
Guldu
guldu@music
12
3.8
53666
Jones
jones@cs
18
3.4
53901
Jones
jones@toy
18
3.4
53902
Jones
jones@physics
18
3.4
53903
Jones
jones@english
18
3.4
53904
Jones
jones@genetics
18
3.4
53905
Jones
jones@astro
18
3.4
53906
Jones
jones@chem
18
3.4
53902
Jones
jones@sanitation
18
3.8
53688
Smith
smith@ee
19
3.2
53650
Smith
smith@math
19
3.8
54001
Smith
smith@ee
19
3.5
54005
Smith
smith@cs
19
3.8
54009
Smith
smith@astro
19
2.2
Figure 10.12
An Instance of the Students Relation
4. In conclusion, to add parent pointers to the B+ tree data structure is not a good
modiﬁcation. Parent pointers cause unnecessary page updates and so lead to a
decrease in performance.
Exercise 10.10 Consider the instance of the Students relation shown in Figure 10.12.
Show a B+ tree of order 2 in each of these cases below, assuming that duplicates are
handled using overﬂow pages. Clearly indicate what the data entries are (i.e., do not
use the k∗convention).
1. A B+ tree index on age using Alternative (1) for data entries.
2. A dense B+ tree index on gpa using Alternative (2) for data entries. For this
question, assume that these tuples are stored in a sorted ﬁle in the order shown
in Figure 10.12: The ﬁrst tuple is in page 1, slot 1; the second tuple is in page
1, slot 2; and so on. Each page can store up to three data records. You can use
⟨page-id, slot⟩to identify a tuple.
Answer 10.10 Answer omitted.
Exercise 10.11 Suppose that duplicates are handled using the approach without
overﬂow pages discussed in Section
10.7. Describe an algorithm to search for the
left-most occurrence of a data entry with search key value K.

Tree-Structured Indexing
99
Answer 10.11 The key to understanding this problem is to observe that when a
leaf splits due to inserted duplicates, then of the two resulting leaves, it may happen
that the left leaf contains other search key values less than the duplicated search key
value. Furthermore, it could happen that the least element on the right leaf could be
the duplicated value. (This scenario could arise, for example, when the majority of
data entries on the original leaf were for search keys of the duplicated value.) The
parent index node (assuming the tree is of at least height 2) will have an entry for the
duplicated value with a pointer to the rightmost leaf.
If this leaf continues to be ﬁlled with entries having the same duplicated key value, it
could split again causing another entry with the same key value to be inserted in the
parent node. Thus, the same key value could appear many times in the index nodes
as well. While searching for entries with a given key value, the search should proceed
by using the left-most of the entries on an index page such that the key value is less
than or equal to the given key value. Moreover, on reaching the leaf level, it is possible
that there are entries with the given key value (call it k) on the page to the left of the
current leaf page, unless some entry with a smaller key value is present on this leaf
page. Thus, we must scan to the left using the neighbor pointers at the leaf level until
we ﬁnd an entry with a key value less than k (or come to the beginning of the leaf
pages). Then, we must scan forward along the leaf level until we ﬁnd an entry with a
key value greater than k.
Exercise 10.12 Answer Exercise 10.10 assuming that duplicates are handled without
using overﬂow pages, using the alternative approach suggested in Section 9.7.
Answer 10.12 Answer omitted.

11
HASH-BASED INDEXING
Exercise 11.1 Consider the Extendible Hashing index shown in Figure 11.1. Answer
the following questions about this index:
1. What can you say about the last entry that was inserted into the index?
2. What can you say about the last entry that was inserted into the index if you
know that there have been no deletions from this index so far?
3. Suppose you are told that there have been no deletions from this index so far.
What can you say about the last entry whose insertion into the index caused a
split?
4. Show the index after inserting an entry with hash value 68.
2
2
2
1
5
21
10
15
7
16
4
12
20
000
001
010
011
100
101
110
111
3
3
3
DIRECTORY
Bucket A
Bucket B
Bucket C
Bucket D
Bucket A2
64
51
36
Figure 11.1
Figure for Exercise 11.1
100

Hash-Based Indexing
101
5. Show the index after inserting entries with hash values 17 and 69 into the original
tree.
6. Show the index after deleting the entry with hash value 21 into the original tree.
(Assume that the full deletion algorithm is used.)
7. Show the index after deleting the entry with hash value 10 into the original tree.
Is a merge triggered by this deletion? If not, explain why. (Assume that the full
deletion algorithm is used.)
Answer 11.1 The answer to each question is given below.
1. It could be any one of the data entries in the index. We can always ﬁnd a sequence
of insertions and deletions with a particular key value, among the key values shown
in the index as the last insertion. For example, consider the data entry 16 and
the following sequence:
1 5 21 10 15 7 51 4 12 36 64 8 24 56 16 56D 24D 8D
The last insertion is the data entry 16 and it also causes a split. But the sequence
of deletions following this insertion cause a merge leading to the index structure
shown in Fig 11.1.
2. The last insertion could not have caused a split because the total number of data
entries in the buckets A and A2 is 6. If the last entry caused a split the total
would have been 5.
3. The last insertion which caused a split cannot be in bucket C. Buckets B and C or
C and D could have made a possible bucket-split image combination but the total
number of data entries in these combinations is 4 and the absence of deletions
demands a sum of at least 5 data entries for such combinations. Buckets B and D
can form a possible bucket-split image combination because they have a total of
6 data entries between themselves. So do A and A2. But for the B and D to be
split images the starting global depth should have been 1. If the starting global
depth is 2, then the last insertion causing a split would be in A or A2.
4. See Fig 11.2.
5. See Fig 11.3.
6. See Fig 11.4.
7. The deletion of the data entry 10 which is the only data entry in bucket C doesn’t
trigger a merge because bucket C is a primary page and it is left as a place holder.
Right now, directory element 010 and its split image 110 already point to the same
bucket C. We can’t do a further merge.
See Fig 11.5.

102
Chapter 11
4
0000
0001
0010
0011
0100
0101
0110
0111
1000
1001
1010
1011
1100
1101
1110
1111
64
16
3
2
1
5
21
2
10
2
4
4
15
7
51
12
4
20
36
68
DIRECTORY
BUCKET A
BUCKET
BUCKET
BUCKET
BUCKET
BUCKET
B
C
D
A2
A3
Figure 11.2

Hash-Based Indexing
103
64
16
3
1
2
10
2
15
7
51
4
BUCKET A
BUCKET
BUCKET
BUCKET
BUCKET
BUCKET
B
C
D
A2
000
001
010
011
100
101
110
111
17
3
3
3
5
21
69
3
B2
36
20
12
DIRECTORY
Figure 11.3

104
Chapter 11
64
16
3
1
2
10
2
15
7
51
4
BUCKET A
BUCKET
BUCKET
BUCKET
BUCKET
B
C
D
A2
000
001
010
011
100
101
110
111
3
3
36
20
12
DIRECTORY
5
2
Figure 11.4
64
16
3
1
2
2
15
7
51
4
BUCKET A
BUCKET
BUCKET
BUCKET
BUCKET
B
C
D
A2
000
001
010
011
100
101
110
111
3
3
36
20
12
DIRECTORY
5
2
21
Figure 11.5

Hash-Based Indexing
105
h(0)
h(1)
00
01
10
11
000
001
010
011
00
100
Next=1
PRIMARY
PAGES
OVERFLOW
PAGES
Level=0
32
8
24
9
25
17
10 30
18
14
31
35
7
11
36
44
41
Figure 11.6
Figure for Exercise 11.2
Exercise 11.2 Consider the Linear Hashing index shown in Figure 11.6. Assume that
we split whenever an overﬂow page is created. Answer the following questions about
this index:
1. What can you say about the last entry that was inserted into the index?
2. What can you say about the last entry that was inserted into the index if you
know that there have been no deletions from this index so far?
3. Suppose you know that there have been no deletions from this index so far. What
can you say about the last entry whose insertion into the index caused a split?
4. Show the index after inserting an entry with hash value 4.
5. Show the index after inserting an entry with hash value 15 into the original tree.
6. Show the index after deleting the entries with hash values 36 and 44 into the
original tree. (Assume that the full deletion algorithm is used.)
7. Find a list of entries whose insertion into the original index would lead to a bucket
with two overﬂow pages. Use as few entries as possible to accomplish this. What
is the maximum number of entries that can be inserted into this bucket before a
split occurs that reduces the length of this overﬂow chain?
Answer 11.2 Answer omitted.
Exercise 11.3 Answer the following questions about Extendible Hashing:

106
Chapter 11
1. Explain why local depth and global depth are needed.
2. After an insertion that causes the directory size to double, how many buckets have
exactly one directory entry pointing to them? If an entry is then deleted from
one of these buckets, what happens to the directory size? Explain your answers
brieﬂy.
3. Does Extendible Hashing guarantee at most one disk access to retrieve a record
with a given key value?
4. If the hash function distributes data entries over the space of bucket numbers in a
very skewed (non-uniform) way, what can you say about the size of the directory?
What can you say about the space utilization in data pages (i.e., non-directory
pages)?
5. Does doubling the directory require us to examine all buckets with local depth
equal to global depth?
6. Why is handling duplicate key values in Extendible Hashing harder than in ISAM?
Answer 11.3 The answer to each question is given below.
1. Extendible hashing allows the size of the directory to increase and decrease de-
pending on the number and variety of inserts and deletes. Once the directory size
changes, the hash function applied to the search key value should also change. So
there should be some information in the index as to which hash function is to be
applied. This information is provided by the global depth.
An increase in the directory size doesn’t cause the creation of new buckets for
each new directory entry. All the new directory entries except one share buckets
with the old directory entries. Whenever a bucket which is being shared by two
or more directory entries is to be split the directory size need not be doubled.
This means for each bucket we need to know whether it is being shared by two
or more directory entries. This information is provided by the local depth of the
bucket. The same information can be obtained by a scan of the directory, but this
is costlier.
2. Exactly two directory entries have only one directory entry pointing to them after
a doubling of the directory size. This is because when the directory is doubled,
one of the buckets must have split causing a directory entry to point to each of
these two new buckets.
If an entry is then deleted from one of these buckets, a merge may occur, but this
depends on the deletion algorithm. If we try to merge two buckets only when a
bucket becomes empty, then it is not necessary that the directory size decrease
after the deletion that was considered in the question. However, if we try to merge

Hash-Based Indexing
107
two buckets whenever it is possible to do so then the directory size decreases after
the deletion.
3. No ”minimum disk access” guarantee is provided by extendible hashing. If the
directory is not already in memory it needs to be fetched from the disk which may
require more than one disk access depending upon the size of the directory. Then
the required bucket has to be brought into the memory. Also, if alternatives 2 and
3 are followed for storing the data entries in the index then another disk access is
possibly required for fetching the actual data record.
4. Consider the index in Fig 11.1. Let us consider a list of data entries with search
key values of the form 2i where i > k. By an appropriate choice of k, we can get
all these elements mapped into the Bucket A. This creates 2k elements in the
directory which point to just k + 3 diﬀerent buckets. Also, we note there are k
buckets (data pages), but just one bucket is used. So the utilization of data pages
= 1/k.
5. No. Since we are using extendible hashing, only the local depth of the bucket
being split needs be examined.
6. Extendible hashing is not supposed to have overﬂow pages (overﬂow pages are
supposed to be dealt with using redistribution and splitting). When there are
many duplicate entries in the index, overﬂow pages may be created that can never
be redistributed (they will always map to the same bucket). Whenever a ”split”
occurs on a bucket containing only duplicate entries, an empty bucket will be
created since all of the duplicates remain in the same bucket. The overﬂow chains
will never be split, which makes inserts and searches more costly.
Exercise 11.4 Answer the following questions about Linear Hashing:
1. How does Linear Hashing provide an average-case search cost of only slightly more
than one disk I/O, given that overﬂow buckets are part of its data structure?
2. Does Linear Hashing guarantee at most one disk access to retrieve a record with
a given key value?
3. If a Linear Hashing index using Alternative (1) for data entries contains N records,
with P records per page and an average storage utilization of 80 percent, what is
the worst-case cost for an equality search? Under what conditions would this cost
be the actual search cost?
4. If the hash function distributes data entries over the space of bucket numbers in
a very skewed (non-uniform) way, what can you say about the space utilization in
data pages?
Answer 11.4 Answer omitted.

108
Chapter 11
Exercise 11.5 Give an example of when you would use each element (A or B) for
each of the following ‘A versus B’ pairs:
1. A hashed index using Alternative (1) versus heap ﬁle organization.
2. Extendible Hashing versus Linear Hashing.
3. Static Hashing versus Linear Hashing.
4. Static Hashing versus ISAM.
5. Linear Hashing versus B+ trees.
Answer 11.5 The answer to each question is given below.
1. Example 1: Consider a situation in which most of the queries are equality queries
based on the search key ﬁeld. It pays to build a hashed index on this ﬁeld in
which case we can get the required record in one or two disk accesses. A heap ﬁle
organisation may require a full scan of the ﬁle to access a particular record.
Example 2: Consider a ﬁle on which only sequential scans are done. It may fare
better if it is organised as a heap ﬁle. A hashed index built on it may require more
disk accesses because the occupancy of the pages may not be 100%.
2. Example 1: Consider a set of data entries with search keys which lead to a skewed
distribution of hash key values.
In this case, extendible hashing causes splits
of buckets at the necessary bucket whereas linear hashing goes about splitting
buckets in a round-robin fashion which is useless. Here extendible hashing has
a better occupancy and shorter overﬂow chains than linear hashing. So equality
search is cheaper for extendible hashing.
Example 2: Consider a very large ﬁle which requires a directory spanning several
pages. In this case extendible hashing requires d + 1 disk accesses for equality
selections where d is the number of directory pages. Linear hashing is cheaper.
3. Example 1: Consider a situation in which the number of records in the ﬁle is
constant. Let all the search key values be of the form 2n + k for various values of
n and a few values of k. The traditional hash functions used in linear hashing like
taking the last d bits of the search key lead to a skewed distribution of the hash
key values. This leads to long overﬂow chains. A static hashing index can use the
hash function deﬁned as
h(2n + k) = n
A family of hash functions can’t be built based on this hash function as k takes
only a few values. In this case static hashing is better.
Example 2: Consider a situation in which the number of records in the ﬁle varies
a lot and the hash key values have a uniform distribution. Here linear hashing is
clearly better than static hashing which might lead to long overﬂow chains thus
considerably increasing the cost of equality search.

Hash-Based Indexing
109
4. Example 1: Consider a situation in which the number of records in the ﬁle is
constant and only equality selections are performed. Static hashing requires one
or two disk accesses to get to the data entry. ISAM may require more than one
depending on the height of the ISAM tree.
Example 2: Consider a situation in which the search key values of data entries
can be used to build a clustered index and most of the queries are range queries
on this ﬁeld. Then ISAM deﬁnitely wins over static hashing.
5. Example 1: Again consider a situation in which only equality selections are
performed on the index. Linear hashing is better than B+ tree in this case.
Example 2: When an index which is clustered and most of the queries are range
searches, B+ indexes are better.
Exercise 11.6 Give examples of the following:
1. A Linear Hashing index and an Extendible Hashing index with the same data
entries, such that the Linear Hashing index has more pages.
2. A Linear Hashing index and an Extendible Hashing index with the same data
entries, such that the Extendible Hashing index has more pages.
Answer 11.6 Answer omitted.
Exercise 11.7 Consider a relation R(a, b, c, d) containing 1 million records, where
each page of the relation holds 10 records. R is organized as a heap ﬁle with unclustered
indexes, and the records in R are randomly ordered. Assume that attribute a is a
candidate key for R, with values lying in the range 0 to 999,999. For each of the
following queries, name the approach that would most likely require the fewest I/Os
for processing the query. The approaches to consider follow:
Scanning through the whole heap ﬁle for R.
Using a B+ tree index on attribute R.a.
Using a hash index on attribute R.a.
The queries are:
1. Find all R tuples.
2. Find all R tuples such that a < 50.
3. Find all R tuples such that a = 50.
4. Find all R tuples such that a > 50 and a < 100.

110
Chapter 11
Answer 11.7 Let h be the height of the B+ tree (usually 2 or 3 ) and M be the
number of data entries per page (M > 10). Let us assume that after accessing the
data entry it takes one more disk access to get the actual record. Let c be the occupancy
factor in hash indexing.
Consider the table shown below (disk accesses):
Problem
Heap File
B+ Tree
Hash Index
1. All tuples
105
h + 106
M + 106
106
cM + 106
2. a < 50
105
h + 50
M + 50
100
3. a = 50
105
h + 1
2
4. a > 50 and a < 100
105
h + 50
M + 49
98
1. From the ﬁrst row of the table, we see that heap ﬁle organization is the best (has
the fewest disk accesses).
2. From the second row of the table, with typical values for h and M, the B+ Tree
has the fewest disk accesses.
3. From the third row of the table, hash indexing is the best.
4. From the fourth row or the table, again we see that B+ Tree is the best.
Exercise 11.8 How would your answers to Exercise 11.7 change if a is not a candidate
key for R? How would they change if we assume that records in R are sorted on a?
Answer 11.8 Answer omitted.
Exercise 11.9 Consider the snapshot of the Linear Hashing index shown in Figure
11.7. Assume that a bucket split occurs whenever an overﬂow page is created.
1. What is the maximum number of data entries that can be inserted (given the
best possible distribution of keys) before you have to split a bucket? Explain very
brieﬂy.
2. Show the ﬁle after inserting a single record whose insertion causes a bucket split.
3.
(a) What is the minimum number of record insertions that will cause a split of
all four buckets? Explain very brieﬂy.
(b) What is the value of Next after making these insertions?
(c) What can you say about the number of pages in the fourth bucket shown
after this series of record insertions?

Hash-Based Indexing
111
1
0
h
h
Level=0, N=4
00
01
10
11
000
001
010
011
Next=0
PRIMARY
PAGES
44
64
25
9
5
10
31
15
3
7
Figure 11.7
Figure for Exercise 11.9
Answer 11.9 The answer to each question is given below.
1. The maximum number of entries that can be inserted without causing a split is 6
because there is space for a total of 6 records in all the pages. A split is caused
whenever an entry is inserted into a full page.
2. See Fig 11.8
3.
(a) Consider the list of insertions 63, 41, 73, 137 followed by 4 more entries which
go into the same bucket, say 18, 34, 66, 130 which go into the 3rd bucket. The
insertion of 63 causes the ﬁrst bucket to be split. Insertion of 41, 73 causes
the second bucket split leaving a full second bucket. Inserting 137 into it
causes third bucket-split. At this point at least 4 more entries are required
to split the fourth bucket. A minimum of 8 entries are required to cause the
4 splits.
(b) Since all four buckets would have been split, that particular round comes to
an end and the next round begins. So Next = 0 again.
(c) There can be either one data page or two data pages in the fourth bucket
after these insertions. If the 4 more elements inserted into the 2nd bucket
after 3rd bucket-splitting, then 4th bucket has 1 data page.
If the new 4 more elements inserted into the 4th bucket after 3rd bucket-
spliting and all of them have 011 as its last three bits, then 4th bucket has 2
data pages. Otherwise, if not all have 011 as its last three bits, then the 4th
bucket has 1 data page.
Exercise 11.10 Consider the data entries in the Linear Hashing index for Exercise
11.9.

112
Chapter 11
10
00
01
10
11
000
001
010
011
Level = 1
h
h
0
1
,    N = 4
64
9
25
5
Next = 1
31
15
7
3
63
00
100
44
Figure 11.8

Hash-Based Indexing
113
1. Show an Extendible Hashing index with the same data entries.
2. Answer the questions in Exercise 11.9 with respect to this index.
Answer 11.10 Answer omitted.
Exercise 11.11 In answering the following questions, assume that the full deletion
algorithm is used. Assume that merging is done when a bucket becomes empty.
1. Give an example of Extendible Hashing where deleting an entry reduces global
depth.
2. Give an example of Linear Hashing in which deleting an entry decrements Next
but leaves Level unchanged. Show the ﬁle before and after the deletion.
3. Give an example of Linear Hashing in which deleting an entry decrements Level.
Show the ﬁle before and after the deletion.
4. Give an example of Extendible Hashing and a list of entries e1, e2, e3 such that
inserting the entries in order leads to three splits and deleting them in the reverse
order yields the original index. If such an example does not exist, explain.
5. Give an example of a Linear Hashing index and a list of entries e1, e2, e3 such
that inserting the entries in order leads to three splits and deleting them in the
reverse order yields the original index. If such an example does not exist, explain.
Answer 11.11 The answers are as follows.
1. See Fig 11.9
2. See Fig 11.10
3. See Fig 11.11
4. Let us take the transition shown in Fig 11.12. Here we insert the data entries
4, 5 and 7. Each one of these insertions causes a split with the initial split also
causing a directory split. But none of these insertions redistribute the already
existing data entries into the new buckets. So when we delete these data entries
in the reverse order (actually the order doesn’t matter) and follow the full deletion
algorithm we get back the original index.
5. This example is shown in Fig 11.13.
Here the idea is similar to that used in
the previous answer except that the bucket being split is the one into which the
insertion being made. So bucket 2 has to be split and not bucket 3. Also the order
of deletions should be exactly reversed because in the deletion algorithm Next is
decremented only if the last bucket becomes empty.

114
Chapter 11
2
10
2
64
2
44
5
9
25
3
11
A
B
C
D
27
19
19
27
Delete  63
2
2
2
10
000
001
010
011
100
101
110
111
3
3
DIRECTORY
2
64
2
3
44
5
9
25
3
11
63
A
B
C
D
D2
00
01
10
11
Figure 11.9

Hash-Based Indexing
115
10
00
01
10
11
000
001
010
011
h
h
0
1
64
9
25
5
Next = 1
31
15
7
3
63
00
100
44
Level = 1 ,    N = 4
Delete 44
10
00
01
10
11
000
001
010
011
h
h
0
1
64
9
25
5
31
15
7
3
63
Level = 1 ,    N = 4
Next = 0
Figure 11.10

116
Chapter 11
Level = 2 ,    N = 4
10
64
9
25
5
h1
00
01
10
h0
0
0
1
Level = 1 , N = 2
Delete   31
19
h2
000
001
010
011
10
64
9
25
5
31
Next = 0
h1
00
01
10
11
19
4
8
16
4
8
16
Next = 1
Figure 11.11

Hash-Based Indexing
117
3
3
3
2
00
01
10
11
2
2
2
10
64
2
9
25
3
A
B
C
D
2
10
000
001
010
011
100
101
110
111
3
DIRECTORY
64
3
9
25
8
16
32
41
73
11
19
35
Insert  4 , 5,  7.  
32
8
16
41
73
35
19
3
11
4
5
7
3
3
Figure 11.12

118
Chapter 11
10
64
9
25
A
B
C
D
10
64
9
25
8
16
32
41
73
11
19
32
8
16
41
73
19
11
4
5
h
0
h
1
00
01
10
11
000
001
010
011
Next = 0
h
0
h
1
00
01
10
11
00
01
10
000
001
010
011
100
101
110
18
34
66
66
34
18
6
Insert  4, 5, 6
Next = 3
Figure 11.13

12
OVERVIEW OF QUERY EVALUATION
Exercise 12.1 Brieﬂy answer the following questions:
1. Describe three techniques commonly used when developing algorithms for rela-
tional operators. Explain how these techniques can be used to design algorithms
for the selection, projection, and join operators.
2. What is an access path? When does an index match an access path? What is a
primary conjunct, and why is it important?
3. What information is stored in the system catalogs?
4. What are the beneﬁts of storing the system catalogs as relations?
5. What is the goal of query optimization? Why is optimization important?
6. Describe pipelining and its advantages.
7. Give an example query and plan in which pipelining cannot be used.
8. Describe the iterator interface and explain its advantages.
9. What role do statistics gathered from the database play in query optimization?
10. What were the important design decisions made in the System R optimizer?
11. Why do query optimizers consider only left-deep join trees? Give an example of
a query and a plan that would not be considered because of this restriction.
Answer 12.1 The answer to each question is given below.
1. The three techniques commonly used are indexing, iteration, and partitioning:
Indexing: If a selection or join condition is speciﬁed, use an index to examine
just the tuples that satisfy the condition.
119

120
Chapter 12
Iteration: Examine all tuples in an input table, one after the other.
If
we need only a few ﬁelds from each tuple and there is an index whose key
contains all these ﬁelds, instead of examining data tuples, we can scan all
index data entries.
Partitioning: By partitioing tuples on a sort key, we can often decompose an
operation into a less expensive collection of operations on partitions. Sorting
and hashing are two commonly used partitioning techniques.
They can be used to design algorithms for selection, projection, and join operators
as follows:
Selection: For a selection with more than one tuple matching the query (in
general, at least 5%), indexing like B+ Trees are very useful. This comes into
play often with range queries. It allows us to not only ﬁnd the ﬁrst qualifying
tuple quickly, but also the other qualifying tuples soon after (especially if the
index is clustered). For a selection condition with an equality query where
there are only a few (usually 1) matching tuple, partitioning using hash
indexing is often appropriate. It allows us to ﬁnd the exact tuple we are
searching for with a cost of only a few (typically one or two) I/Os.
Projection: The projection operation requires us to drop certain ﬁelds of
the input, which can result in duplicates appearing in the result set. If we
do not need to remove these duplicates, then the iteration technique can
eﬃciently handle this problem. On ther other hand, if we do need to elim-
inate duplicates, partitioning the data and applying a sort key is typically
performed.
In the case that there are appropriate indexes available, this can lead to less
expensive plans for sorting the tuples during duplicate elimination since the
data may all ready be sorted on the index (in that case we simply compare
adjacent entries to check for duplicates)
Join: The most expensive database operation, joins, can combinations of
all three techniques. A join operation typically has multiple selection and
projection elements built into it, so the importance of having appropriate
indexes or of partitioning the data is just as above, if not more so. When
possible, the individual selections and projections are applied to two relations
before they are joined, so as to decrease the size of the intermediate table.
As an example consider joining two relations with 100,000 tuples each and
only 5 % of qualifying tuples in each table.
Joining before applying the
selection conditions, would result in a huge intermediate table size that would
then have to be searched for matching selections.
Alternatively, consider
applying parts of the selection ﬁrst. We can then perform a join of the 5,000
qualifying tuples found after applying the selection to each table, that can
then be searched and handled signiﬁcantly faster.

Overview of Query Evaluation
121
2. An access path is a way of retrieving tuples from a table and consists of either
a ﬁle scan or an index plus a matching selection condition. An index matches a
selection condition if the index can be used to retrieve just the tuples that satisfy
the condition. An index can match some subset of conjunts in a selection condition
even though it does not match the entire condition and we refer to the conjuct that
the index matches as the primary conjuncts in the selection. Primary conjuncts
are important because they allow us to quickly discard information we do not
need and only focus in on searching/sorting the data that more closely matches
the selection conditions.
3. Information about relations, indexes, and views is stored in the system catalogs.
This includes ﬁle names, ﬁle sizes, and ﬁle structure, the attribute names and data
types, lists of keys, and constraints.
Some commonly stored statistical information includes:
(a) Cardinality - the number of tuples for each relation
(b) Size - the number of pages in each relation
(c) Index Cardinality - the number of distinct key values for each index
(d) Index Size - the number of pages for each index (or number of leaf pages)
(e) Index Height - the number of nonleaf levels for each tree index
(f) Index Range - the minimum present key value and the maximum present key
value for each index.
4. There are several advantages to storing the system catatlogs as relations. Rela-
tional system catalogs take advantage of all of the implementation and manage-
ment beneﬁts of relational tables: eﬀective information storage and rich querying
capabilities. The choice of what system catalogs to maintain is left to the DBMS
implementor.
5. The goal of query optimization is to avoid the worst plans and ﬁnd a good plan.
The goal is usually not to ﬁnd the optimal plan. The diﬀerence in cost between a
good plan and a bad plan can be several orders of magnitude: a good query plan
can evaluate the query in seconds, whereas a bad query plan might take days!
6. Pipelining allows us to avoid creating and reading temporary relations; the I/O
savings can be substantial.
7. Bushy query plans often cannot take advantage of pipelining because of limited
buﬀer or CPU resources. Consider a bushy plan in which we are doing a selection
on two relations, followed by a join.
We cannot always use pipelining in this
strategy becuase the result of the selection on the ﬁrst selection may not ﬁt in
memory, and we must wait for the second relation’s selection to complete before
we can begin the join.

122
Chapter 12
8. The iterator interface for an operator includes the functions open, get next, and
close; it hides the details of how the operator is implemented, and allows us to
view all operator nodes in a query plan uniformly.
9. The query optimizer uses statistics to improve the chances of selecting an optimum
query plan. The statistics are used to calculate reduction factors which determine
the results the optimizer may expect given diﬀerent indexes and inputs.
10. Some important design decisions in the System R optimizer are:
(a) Using statistics about a database instance to estimate the cost of a query
evaluation plan.
(b) A decision to consider only plans with binary joins in which the inner plan
is a base relation.
This heuristic reduces the often signiﬁcant number of
alternative plans that must be considered.
(c) A decision to focus optimization on the class of SQL queries without nesting
and to treat nested queries in a relatively ad hoc way.
(d) A decision not to perform duplicate elimination for projections (except as a
ﬁnal step in the query evaluation when required by a DISTINCT clause).
(e) A model of cost that accounted for CPU costs as wellas I/O costs.
11. There are two main reasons for the decision to concentrate on left-deep plains
only:
(a) As the number of joins increases, the number of alternative plans increases
rapidly and it becomes neccessary to prune the space of the alternative plans.
(b) Left-deep trees allow us to generate all fully piplined plans; that is, plans in
which all joins are evaluated using pipelining.
Consider the join A ▷◁B ▷◁C ▷◁D. The query plan (A ▷◁B) ▷◁(C ▷◁D) would
never be considered because it is a bushy tree.
Exercise 12.2 Consider a relation R(a,b,c,d,e) containing 5,000,000 records, where
each data page of the relation holds 10 records. R is organized as a sorted ﬁle with
secondary indexes. Assume that R.a is a candidate key for R, with values lying in
the range 0 to 4,999,999, and that R is stored in R.a order. For each of the following
relational algebra queries, state which of the following three approaches is most likely
to be the cheapest:
Access the sorted ﬁle for R directly.
Use a (clustered) B+ tree index on attribute R.a.
Use a linear hashed index on attribute R.a.

Overview of Query Evaluation
123
1. σa<50,000(R)
2. σa=50,000(R)
3. σa>50,000∧a<50,010(R)
4. σa̸=50,000(R)
Answer 12.2 Answer omitted.
Exercise 12.3 For each of the following SQL queries, for each relation involved, list
the attributes that must be examined to compute the answer. All queries refer to the
following relations:
Emp(eid: integer, did: integer, sal: integer, hobby: char(20))
Dept(did: integer, dname: char(20), ﬂoor: integer, budget: real)
1. SELECT * FROM Emp E
2. SELECT * FROM Emp E, Dept D
3. SELECT * FROM Emp E, Dept D WHERE E.did = D.did
4. SELECT E.eid, D.dname FROM Emp E, Dept D WHERE E.did = D.did
Answer 12.3 The answer to each question is given below.
1. E.eid, E.did, E.sal, E.hobby
2. E.eid, E.did, E.sal, E.hobby, D.did, D.dname, D.ﬂoor, D.budget
3. E.eid, E.did, E.sal, E.hobby, D.did, D.dname, D.ﬂoor, D.budget
4. E.eid, D.dname, E.did, D.did
Exercise 12.4 Consider the following schema with the Sailors relation:
Sailors(sid: integer, sname: string, rating: integer, age: real)
For each of the following indexes, list whether the index matches the given selection
conditions. If there is a match, list the primary conjuncts.
1. A B+-tree index on the search key ⟨Sailors.sid ⟩.

124
Chapter 12
(a) σSailors.sid<50,000(Sailors)
(b) σSailors.sid=50,000(Sailors)
2. A hash index on the search key ⟨Sailors.sid ⟩.
(a) σSailors.sid<50,000(Sailors)
(b) σSailors.sid=50,000(Sailors)
3. A B+-tree index on the search key ⟨Sailors.sid, Sailors.age ⟩.
(a) σSailors.sid<50,000∧Sailors.age=21(Sailors)
(b) σSailors.sid=50,000∧Sailors.age>21(Sailors)
(c) σSailors.sid=50,000(Sailors)
(d) σSailors.age=21(Sailors)
4. A hash-tree index on the search key ⟨Sailors.sid, Sailors.age ⟩.
(a) σSailors.sid=50,000∧Sailors.age=21(Sailors)
(b) σSailors.sid=50,000∧Sailors.age>21(Sailors)
(c) σSailors.sid=50,000(Sailors)
(d) σSailors.age=21(Sailors)
Answer 12.4 Answer omitted.
Exercise 12.5 Consider again the schema with the Sailors relation:
Sailors(sid: integer, sname: string, rating: integer, age: real)
Assume that each tuple of Sailors is 50 bytes long, that a page can hold 80 Sailors
tuples, and that we have 500 pages of such tuples. For each of the following selection
conditions, estimate the number of pages retrieved, given the catalog information in
the question.
1. Assume that we have a B+-tree index T on the search key ⟨Sailors.sid ⟩, and as-
sume that IHeight(T ) = 4, INPages(T ) = 50, Low(T ) = 1, and High(T ) = 100,000.
(a) σSailors.sid<50,000(Sailors)
(b) σSailors.sid=50,000(Sailors)
2. Assume that we have a hash index T on the search key ⟨Sailors.sid ⟩, and assume
that IHeight(T ) = 2, INPages(T ) = 50, Low(T ) = 1, and High(T ) = 100,000.
(a) σSailors.sid<50,000(Sailors)

Overview of Query Evaluation
125
(b) σSailors.sid=50,000(Sailors)
Answer 12.5 The answer to each question is given below.
1.
(a) Assuming uniform distribution, around half of the 40,000 tuples will match
the selection condition. The total cost is then the cost of ﬁnding the ﬁrst leaf
node (4 I/O’s) plus the cost of retrieving the matching tuples. If the index
is clustered, then the total cost is 4 + 250 I/O’s = 254 I/O’s. If the index is
unclustered, then the cost of retrieving each matching tuple could be as high
as 20,000 I/O’s (one I/O for each matching tuple), leading to a total cost of
20,004 I/O’s. If the index is unclustered, then doing a ﬁle scan is most likely
the preferred method with a cost of 500 I/O’s.
(b) Since sid is a primary key for the relation we expect only one matching tuple
for the hash index, therefore the cost is just the height of the tree (4 I/O’s)
plus the cost of reading the qualifying tuple’s page (1 I/O) which adds up to
be 5 I/O’s.
2.
(a) Since a hash index on sid cannot help us for range queries, the index is useless,
and therefore we must do a ﬁle scan at a cost of 500 pages I/O’s (the cost of
reading the entire relation).
(b) Since sid is a primary key for the relation we expect only one matching tuple
for the hash index, therefore the cost is just the height of the tree (2 I/O’s)
plus the cost of reading the qualifying tuple’s page (1 I/O) which adds up to
be 3 I/O’s.
Exercise 12.6 Consider the two join methods described in Section 12.3.3. Assume
that we join two relations R and S, and that the systems catalog contains appropriate
statistics about R and S. Write formulas for the cost estimates of the index nested
loops join and sort-merge join using the appropriate variables from the systems catalog
in Section 12.1. For index nested loops join, consider both a B+ tree index and a hash
index.
(For the hash index, you can assume that you can retrieve the index page
containing the rid of the matching tuple with 1.2 I/Os on average.)
Answer 12.6 Answer omitted.

13
EXTERNAL SORTING
Exercise 13.1 Suppose you have a ﬁle with 10,000 pages and you have three buﬀer
pages. Answer the following questions for each of these scenarios, assuming that our
most general external sorting algorithm is used:
(a) A ﬁle with 10,000 pages and three available buﬀer pages.
(b) A ﬁle with 20,000 pages and ﬁve available buﬀer pages.
(c) A ﬁle with 2,000,000 pages and 17 available buﬀer pages.
1. How many runs will you produce in the ﬁrst pass?
2. How many passes will it take to sort the ﬁle completely?
3. What is the total I/O cost of sorting the ﬁle?
4. How many buﬀer pages do you need to sort the ﬁle completely in just two passes?
Answer 13.1 The answer to each question is given below.
1. In the ﬁrst pass (Pass 0), ⌈N/B⌉runs of B pages each are produced, where N is
the number of ﬁle pages and B is the number of available buﬀer pages:
(a) ⌈10000/3⌉= 3334 sorted runs.
(b) ⌈20000/5⌉= 4000 sorted runs.
(c) ⌈2000000/17⌉= 117648 sorted runs.
2. The number of passes required to sort the ﬁle completely, including the initial
sorting pass, is ⌈logB−1N1⌉+ 1, where N1
=
⌈N/B⌉is the number of runs
produced by Pass 0:
(a) ⌈log23334⌉+ 1 = 13 passes.
(b) ⌈log44000⌉+ 1 = 7 passes.
(c) ⌈log16117648⌉+ 1 = 6 passes.
126

External Sorting
127
3. Since each page is read and written once per pass, the total number of page I/Os
for sorting the ﬁle is 2 ∗N ∗(#passes):
(a) 2*10000*13 = 260000.
(b) 2*20000*7 = 280000.
(c) 2*2000000*6 = 24000000.
4. In Pass 0, ⌈N/B⌉runs are produced. In Pass 1, we must be able to merge this
many runs; i.e., B −1 ≥⌈N/B⌉. This implies that B must at least be large
enough to satisfy B ∗(B −1) ≥N; this can be used to guess at B, and the guess
must be validated by checking the ﬁrst inequality. Thus:
(a) With 10000 pages in the ﬁle, B = 101 satisﬁes both inequalities, B = 100 does
not, so we need 101 buﬀer pages.
(b) With 20000 pages in the ﬁle, B = 142 satisﬁes both inequalities, B = 141 does
not, so we need 142 buﬀer pages.
(c) With 2000000 pages in the ﬁle, B = 1415 satisﬁes both inequalities, B = 1414
does not, so we need 1415 buﬀer pages.
Exercise 13.2 Answer Exercise 13.1 assuming that a two-way external sort is used.
Answer 13.2 Answer omitted.
Exercise 13.3 Suppose that you just ﬁnished inserting several records into a heap
ﬁle and now want to sort those records. Assume that the DBMS uses external sort
and makes eﬃcient use of the available buﬀer space when it sorts a ﬁle. Here is some
potentially useful information about the newly loaded ﬁle and the DBMS software
available to operate on it:
The number of records in the ﬁle is 4500. The sort key for the ﬁle is 4 bytes
long. You can assume that rids are 8 bytes long and page ids are 4 bytes long.
Each record is a total of 48 bytes long. The page size is 512 bytes. Each page
has 12 bytes of control information on it. Four buﬀer pages are available.
1. How many sorted subﬁles will there be after the initial pass of the sort, and how
long will each subﬁle be?
2. How many passes (including the initial pass just considered) are required to sort
this ﬁle?
3. What is the total I/O cost for sorting this ﬁle?
4. What is the largest ﬁle, in terms of the number of records, you can sort with just
four buﬀer pages in two passes? How would your answer change if you had 257
buﬀer pages?

128
Chapter 13
5. Suppose that you have a B+ tree index with the search key being the same as the
desired sort key. Find the cost of using the index to retrieve the records in sorted
order for each of the following cases:
The index uses Alternative (1) for data entries.
The index uses Alternative (2) and is unclustered. (You can compute the
worst-case cost in this case.)
How would the costs of using the index change if the ﬁle is the largest that
you can sort in two passes of external sort with 257 buﬀer pages? Give your
answer for both clustered and unclustered indexes.
Answer 13.3 The answer to each question is given below.
1. Assuming that the general external merge-sort algorithm is used, and that the
available space for storing records in each page is 512 −12 = 500 bytes, each page
can store up to 10 records of 48 bytes each. So 450 pages are needed in order to
store all 4500 records, assuming that a record is not allowed to span more than
one page.
Given that 4 buﬀer pages are available, there will be ⌈450/4⌉= 113 sorted runs
(sub-ﬁles) of 4 pages each, except the last run, which is only 2 pages long.
2. The total number of passes will be equal to log3113 + 1 = 6 passes.
3. The total I/O cost for sorting this ﬁle is 2 ∗450 ∗6 = 5400 I/Os.
4. As we saw in the previous exercise, in Pass 0, ⌈N/B⌉runs are produced. In Pass
1, we must be able to merge this many runs; i.e., B −1 ≥⌈N/B⌉. When B is
given to be 4, we get N = 12. The maximum number of records on 12 pages is
12 ∗10 = 120. When B = 257, we get N = 65792, and the number of records is
65792 ∗10 = 657920.
5.
(a) If the index uses Alternative (1) for data entries, and it is clustered, the
cost will be equal to the cost of traversing the tree from the root to the left-
most leaf plus the cost of retrieving the pages in the sequence set. Assuming
67% occupancy, the number of leaf pages in the tree (the sequence set) is
450/0.67 = 600.
(b) If the index uses Alternative (2), and is not clustered, in the worst case, ﬁrst
we scan B+ tree’s leaf pages, also each data entry will require fetching a data
page. The number of data entries is equal to the number of data records,
which is 4500. Since there is one data entry per record, each data entry
requires 12 bytes, and each page holds 512 bytes, the number of B+ tree leaf
pages is about (4500 ∗12)/(512 ∗0.67)), assuming 67% occupancy, which is
about 150. Thus, about 4650 I/Os are required in a worst-case scenario.

External Sorting
129
(c) The B+ tree in this case has 65792/0.67 = 98197 leaf pages if Alternative
(1) is used, assuming 67% occupancy. This is the number of I/Os required
(plus the relatively minor cost of going from the root to the left-most leaf).
If Alternative (2) is used, and the index is not clustered, the number of I/Os
is approximately equal to the number of data entries in the worst case, that
is 657920,plus the number of B+ tree leaf pages 2224. Thus, number of I/Os
is 660144.
Exercise 13.4 Consider a disk with an average seek time of 10ms, average rotational
delay of 5ms, and a transfer time of 1ms for a 4K page. Assume that the cost of
reading/writing a page is the sum of these values (i.e., 16ms) unless a sequence of
pages is read/written. In this case, the cost is the average seek time plus the average
rotational delay (to ﬁnd the ﬁrst page in the sequence) plus 1ms per page (to transfer
data). You are given 320 buﬀer pages and asked to sort a ﬁle with 10,000,000 pages.
1. Why is it a bad idea to use the 320 pages to support virtual memory, that is, to
‘new’ 10,000,000 · 4K bytes of memory, and to use an in-memory sorting algorithm
such as Quicksort?
2. Assume that you begin by creating sorted runs of 320 pages each in the ﬁrst pass.
Evaluate the cost of the following approaches for the subsequent merging passes:
(a) Do 319-way merges.
(b) Create 256 ‘input’ buﬀers of 1 page each, create an ‘output’ buﬀer of 64
pages, and do 256-way merges.
(c) Create 16 ‘input’ buﬀers of 16 pages each, create an ‘output’ buﬀer of 64
pages, and do 16-way merges.
(d) Create eight ‘input’ buﬀers of 32 pages each, create an ‘output’ buﬀer of 64
pages, and do eight-way merges.
(e) Create four ‘input’ buﬀers of 64 pages each, create an ‘output’ buﬀer of 64
pages, and do four-way merges.
Answer 13.4 Answer omitted.
Exercise 13.5 Consider the reﬁnement to the external sort algorithm that produces
runs of length 2B on average, where B is the number of buﬀer pages. This reﬁnement
was described in Section 11.2.1 under the assumption that all records are the same
size. Explain why this assumption is required and extend the idea to cover the case of
variable-length records.
Answer 13.5 The assumption that all records are of the same size is used when the
algorithm moves the smallest entry with a key value large than k to the output buﬀer

130
Chapter 13
and replaces it with a value from the input buﬀer. This ”replacement” will only work
if the records of the same size.
If the entries are of variable size, then we must also keep track of the size of each
entry, and replace the moved entry with a new entry that ﬁts in the available memory
location. Dynamic programming algorithms have been adapted to decide an optimal
replacement strategy in these cases.

14
EVALUATION OF RELATIONAL
OPERATORS
Exercise 14.1 Brieﬂy answer the following questions:
1. Consider the three basic techniques, iteration, indexing, and partitioning, and the
relational algebra operators selection, projection, and join. For each technique-
operator pair, describe an algorithm based on the technique for evaluating the
operator.
2. Deﬁne the term most selective access path for a query.
3. Describe conjunctive normal form, and explain why it is important in the context
of relational query evaluation.
4. When does a general selection condition match an index? What is a primary term
in a selection condition with respect to a given index?
5. How does hybrid hash join improve on the basic hash join algorithm?
6. Discuss the pros and cons of hash join, sort-merge join, and block nested loops
join.
7. If the join condition is not equality, can you use sort-merge join? Can you use
hash join? Can you use index nested loops join? Can you use block nested loops
join?
8. Describe how to evaluate a grouping query with aggregation operator MAX using a
sorting-based approach.
9. Suppose that you are building a DBMS and want to add a new aggregate operator
called SECOND LARGEST, which is a variation of the MAX operator. Describe how
you would implement it.
10. Give an example of how buﬀer replacement policies can aﬀect the performance of
a join algorithm.
Answer 14.1 The answer to each question is given below.
131

132
Chapter 14
1.
(a) iteration–selection Scan the entire collection, checking the condition on each
tuple, and adding the tuple to the result if the condition is satisﬁed.
(b) indexing–selection If the selection is equality and a B+ or hash index exists
on the ﬁeld condition, we can retrieve relevant tuples by ﬁnding them in the
index and then locating them on disk.
(c) partitioning–selection Do a binary search on sorted data to ﬁnd the ﬁrst tuple
that mathes the condition. To retireve the remaining entries, we simple scan
the collection starting at the ﬁrst tuple we found.
(d) iteration–projection Scan the entire relation, and eliminate unwanted at-
tributes in the result.
(e) indexing–projection If a multiattribute B+ tree index exists for all of the
projection attributes, then one needs to only look at the leaves of the B+.
(f) partitioning–projection To elimiate duplicates when doing a projection, one
can simply project out the unwanted attributes and hash a combination of
the remaining attributes so duplicates can be easily detected.
(g) iteration–join To join two relations, one takes the ﬁrst attribute in the ﬁrst
relation and scans the entire second relation to ﬁnd tuples that match the
join condition. Once the ﬁrst attribute has compared to all tuples in the
second relation, the second attribute from the ﬁrst relation is compared to
all tuples in the second relation, and so on.
(h) indexing–join When an index is available, joining two relations can be more
eﬃcient. Say there are two relations A and B, and there is a secondary index
on the join condition over relation A. The join works as follows: for each
tuple in B, we lookup the join attribute in the index over relation A to see if
there is a match. If there is a match, we store the tuple, otherwise we move
to the next tuple in relation B.
(i) partitioning–join One can join using partitioning by using hash join variant
or a sort-merge join. For example, if there is a sort merge join, we sort both
relations on the the join condition. Next, we scan both relations and identify
matches. After sorting, this requires only a single scan over each relation.
2. The most selective access path is the query access path that retrieves the fewest
pages during query evaluation. This is the most eﬃcient way to gather the query’s
results.
3. Conjuntive normal form is important in query evaluation because often indexes
exist over some subset of conjucts in a CNF expression.
Since conjuct order
does not matter in CNF expressions, often indexes can be used to increase the
selectivity of operators by doing a selection over two, three, or more conjucts using
a single multiattribute index.
4. An index matches a selection condition if the index can be used to retrieve just
the tuples that satisfy the condition. A primary term in a selection condition is
a conjuct that matches an index (i.e. can be used by the index).

Evaluation of Relational Operators
133
5. Hybrid hash join improves performance by comparing the ﬁrst hash buckets during
the partitioning phase rather than saving it for the probing phase. This saves us
teh cost of writing and reading the ﬁrst partition to disk.
6. Hash join provides excellent performance for equality joins, and can be tuned to
require very few extra disk accesses beyond a one-time scan (provided enough
memory is available). However, hash join is worthless for non-equality joins.
Sort-merge joins are suitable when there is either an equality or non-equality based
join condition. Sort-merge also leaves the results sorted which is often a desired
property. Sort-merge join has extra costs when you have to use external sorting
(there is not enough memory to do the sort in-memory).
Block nested loops is eﬃcient when one of the relations will ﬁt in memory and you
are using an MRU replacement strategy. However, if an index is available, there
are better strategies available (but often indexes are not available).
7. If the join condition is not equality, you can use sort-merge join, index nested
loops (if you have a range style index such as a B+ tree index or ISAM index),
or block nested loops join. Hash joining works best for equality joins and is not
suitable otherwise.
8. First we sort all of the tuples based on the GROUP BY attribute. Next we re-sort
each group by sorting all elements on the MAX attribute, taking case not to re-sort
beyond the group boundaries.
9. The operator SECOND LARGEST can be implemented using sorting. For each group
(if there is a GROUP BY clause), we sort the tuples and return the second largest
value for the desired attribute. The cost here is the cost of sorting.
10. One example where the buﬀer replacement stratagy eﬀects join performance is the
use of LRU and MRU in an simple nested loops join. If the relations don’t ﬁt in
main memory, then the buﬀer strategy is critical. Say there are M buﬀer pages
and N are ﬁlled by the ﬁrst relation, and the second relation is of size M-N+P,
meaning all of the second relation will ﬁt in the the buﬀer except P pages. Since
we must do repeated scans of the second relation, the replacement policy comes
into play. With LRU, whenever we need to ﬁnd a page it will have been paged
out so every page request requires a disk IO. On the other hand, with MRU, we
will only need to reread P-1 of the pages in the second relation, since the others
will remain in memory.
Exercise 14.2 Consider a relation R(a,b,c,d,e) containing 5,000,000 records, where
each data page of the relation holds 10 records. R is organized as a sorted ﬁle with
secondary indexes. Assume that R.a is a candidate key for R, with values lying in
the range 0 to 4,999,999, and that R is stored in R.a order. For each of the following
relational algebra queries, state which of the following approaches (or combination
thereof) is most likely to be the cheapest:

134
Chapter 14
Access the sorted ﬁle for R directly.
Use a clustered B+ tree index on attribute R.a.
Use a linear hashed index on attribute R.a.
Use a clustered B+ tree index on attributes (R.a, R.b).
Use a linear hashed index on attributes (R.a, R.b).
Use an unclustered B+ tree index on attribute R.b.
1. σa<50,000∧b<50,000(R)
2. σa=50,000∧b<50,000(R)
3. σa>50,000∧b=50,000(R)
4. σa=50,000∧a=50,010(R)
5. σa̸=50,000∧b=50,000(R)
6. σa<50,000∨b=50,000(R)
Answer 14.2 Answer omitted.
Exercise 14.3 Consider processing the following SQL projection query:
SELECT DISTINCT E.title, E.ename FROM Executives E
You are given the following information:
Executives has attributes ename, title, dname, and address; all are string
ﬁelds of the same length.
The ename attribute is a candidate key.
The relation contains 10,000 pages.
There are 10 buﬀer pages.
Consider the optimized version of the sorting-based projection algorithm: The ini-
tial sorting pass reads the input relation and creates sorted runs of tuples containing
only attributes ename and title. Subsequent merging passes eliminate duplicates while
merging the initial runs to obtain a single sorted result (as opposed to doing a separate
pass to eliminate duplicates from a sorted result containing duplicates).
1. How many sorted runs are produced in the ﬁrst pass? What is the average length of
these runs? (Assume that memory is utilized well and any available optimization
to increase run size is used.) What is the I/O cost of this sorting pass?

Evaluation of Relational Operators
135
2. How many additional merge passes are required to compute the ﬁnal result of the
projection query? What is the I/O cost of these additional passes?
3.
(a) Suppose that a clustered B+ tree index on title is available. Is this index
likely to oﬀer a cheaper alternative to sorting? Would your answer change if
the index were unclustered? Would your answer change if the index were a
hash index?
(b) Suppose that a clustered B+ tree index on ename is available. Is this index
likely to oﬀer a cheaper alternative to sorting? Would your answer change if
the index were unclustered? Would your answer change if the index were a
hash index?
(c) Suppose that a clustered B+ tree index on ⟨ename, title⟩is available. Is
this index likely to oﬀer a cheaper alternative to sorting? Would your answer
change if the index were unclustered? Would your answer change if the index
were a hash index?
4. Suppose that the query is as follows:
SELECT E.title, E.ename FROM Executives E
That is, you are not required to do duplicate elimination. How would your answers
to the previous questions change?
Answer 14.3 The answer to each question is given below.
1. The ﬁrst pass will produce 250 sorted runs of 20 pages each, costing 15000 I/Os.
2. Using the ten buﬀer pages provided, on average we can write 2*10 internally sorted
pages per pass, instead of 10. Then, three more passes are required to merge the
5000/20 runs, costing 2*3*5000 = 30000 I/Os.
3.
(a) Using a clustered B+ tree index on title would reduce the cost to single
scan, or 12,500 I/Os. An unclustered index could potentially cost more than
2500+100,000 (2500 from scanning the B+ tree, and 10000 * tuples per page,
which I just assumed to be 10). Thus, an unclustered index would not be
cheaper. Whether or not to use a hash index would depend on whether the
index is clustered. If so, the hash index would probably be cheaper.
(b) Using the clustered B+ tree on ename would be cheaper than sorting, in
that the cost of using the B+ tree would be 12,500 I/Os. Since ename is
a candidate key, no duplicate checking need be done for < title, ename >
pairs. An unclustered index would require 2500 (scan of index) + 10000 *
tuples per page I/Os and thus probably be more expensive than sorting.
(c) Using a clustered B+ tree index on < ename, title > would also be more
cost-eﬀective than sorting. An unclustered B+ tree over the same attributes

136
Chapter 14
would allow an index-only scan, and would thus be just as economical as the
clustered index. This method (both by clustered and unclustered ) would
cost around 5000 I/O’s.
4. Knowing that duplicate elimination is not required, we can simply scan the relation
and discard unwanted ﬁelds for each tuple. This is the best strategy except in the
case that an index (clustered or unclustered) on < ename, title > is available; in
this case, we can do an index-only scan. (Note that even with DISTINCT speciﬁed,
no duplicates are actually present int he answer because ename is a candidate key.
However, a typical optimizer is not likely to recognize this and omit the duplicate
elimination step.)
Exercise 14.4 Consider the join R▷◁R.a=S.bS, given the following information about
the relations to be joined. The cost metric is the number of page I/Os unless otherwise
noted, and the cost of writing out the result should be uniformly ignored.
Relation R contains 10,000 tuples and has 10 tuples per page.
Relation S contains 2000 tuples and also has 10 tuples per page.
Attribute b of relation S is the primary key for S.
Both relations are stored as simple heap ﬁles.
Neither relation has any indexes built on it.
52 buﬀer pages are available.
1. What is the cost of joining R and S using a page-oriented simple nested loops
join?
What is the minimum number of buﬀer pages required for this cost to
remain unchanged?
2. What is the cost of joining R and S using a block nested loops join? What is the
minimum number of buﬀer pages required for this cost to remain unchanged?
3. What is the cost of joining R and S using a sort-merge join? What is the minimum
number of buﬀer pages required for this cost to remain unchanged?
4. What is the cost of joining R and S using a hash join? What is the minimum
number of buﬀer pages required for this cost to remain unchanged?
5. What would be the lowest possible I/O cost for joining R and S using any join
algorithm, and how much buﬀer space would be needed to achieve this cost?
Explain brieﬂy.
6. How many tuples does the join of R and S produce, at most, and how many pages
are required to store the result of the join back on disk?
7. Would your answers to any of the previous questions in this exercise change if you
were told that R.a is a foreign key that refers to S.b?

Evaluation of Relational Operators
137
Answer 14.4 Answer omitted.
Exercise 14.5 Consider the join of R and S described in Exercise 14.1.
1. With 52 buﬀer pages, if unclustered B+ indexes existed on R.a and S.b, would
either provide a cheaper alternative for performing the join (using an index nested
loops join) than a block nested loops join? Explain.
(a) Would your answer change if only ﬁve buﬀer pages were available?
(b) Would your answer change if S contained only 10 tuples instead of 2000
tuples?
2. With 52 buﬀer pages, if clustered B+ indexes existed on R.a and S.b, would either
provide a cheaper alternative for performing the join (using the index nested loops
algorithm) than a block nested loops join? Explain.
(a) Would your answer change if only ﬁve buﬀer pages were available?
(b) Would your answer change if S contained only 10 tuples instead of 2000
tuples?
3. If only 15 buﬀers were available, what would be the cost of a sort-merge join?
What would be the cost of a hash join?
4. If the size of S were increased to also be 10,000 tuples, but only 15 buﬀer pages
were available, what would be the cost of a sort-merge join? What would be the
cost of a hash join?
5. If the size of S were increased to also be 10,000 tuples, and 52 buﬀer pages were
available, what would be the cost of sort-merge join? What would be the cost of
hash join?
Answer 14.5 Assume that it takes 3 I/Os to access a leaf in R, and 2 I/Os to access a
leaf in S. And since S.b is a primary key, we will assume that every tuple in S matches
5 tuples in R.
1. The Index Nested Loops join involves probing an index on the inner relation for
each tuple in the outer relation. The cost of the probe is the cost of accessing
a leaf page plus the cost of retrieving any matching data records. The cost of
retrieving data records could be as high as one I/O per record for an unclustered
index.
With R as the outer relation, the cost of the Index Nested Loops join will be the
cost of reading R plus the cost of 10,000 probes on S.
T otalCost = 1, 000 + 10, 000 ∗(2 + 1) = 31, 000

138
Chapter 14
With S as the outer relation, the cost of the Index Nested Loops join will be the
cost of reading S plus the cost of 2000 probes on R.
T otalCost = 200 + 2, 000 ∗(3 + 5) = 16, 200
Neither of these solutions is cheaper than Block Nested Loops join which required
4,200 I/Os.
(a) With 5 buﬀer pages, the cost of the Index Nested Loops joins remains the
same, but the cost of the Block Nested Loops join will increase. The new
cost of the Block Nested Loops join is
T otalCost = N + M ∗⌈
N
B −2⌉= 67, 200
And now the cheapest solution is the Index Nested Loops join with S as the
outer relation.
(b) If S contains 10 tuples then we’ll need to change some of our initial assump-
tions. Now all of the S tuples ﬁt on a single page, and it will only require a
single I/O to access the (single) leaf in the index. Also, each tuple in S will
match 1,000 tuples in R.
Block Nested Loops:
T otalCost = N + M ∗⌈
N
B −2⌉= 1, 001
Index Nested Loops with R as the outer relation:
T otalCost = 1, 000 + 10, 000 ∗(1 + 1) = 21, 000
Index Nested Loops with S as the outer relation:
T otalCost = 1 + 10 ∗(3 + 1, 000) = 10, 031
Block Nested Loops is still the best solution.
2. With a clustered index the cost of accessing data records becomes one I/O for
every 10 data records.
With R as the outer relation, the cost of the Index Nested Loops join will be the
cost of reading R plus the cost of 10,000 probes on S.
T otalCost = 1, 000 + 10, 000 ∗(2 + 1) = 31, 000
With S as the outer relation, the cost of the Index Nested Loops join will be the
cost of reading S plus the cost of 2000 probes on R.
T otalCost = 200 + 2, 000 ∗(3 + 1) = 8, 200
Neither of these solutions is cheaper than Block Nested Loops join which required
4,200 I/Os.

Evaluation of Relational Operators
139
(a) With 5 buﬀer pages, the cost of the Index Nested Loops joins remains the
same, but the cost of the Block Nested Loops join will increase. The new
cost of the Block Nested Loops join is
T otalCost = N + M ∗⌈
N
B −2⌉= 67, 200
And now the cheapest solution is the Index Nested Loops join with S as the
outer relation.
(b) If S contains 10 tuples then we’ll need to change some of our initial assump-
tions. Now all of the S tuples ﬁt on a single page, and it will only require a
single I/O to access the (single) leaf in the index. Also, each tuple in S will
match 1,000 tuples in R.
Block Nested Loops:
T otalCost = N + M ∗⌈
N
B −2⌉= 1, 001
Index Nested Loops with R as the outer relation:
T otalCost = 1, 000 + 10, 000 ∗(1 + 1) = 21, 000
Index Nested Loops with S as the outer relation:
T otalCost = 1 + 10 ∗(3 + 100) = 1, 031
Block Nested Loops is still the best solution.
3. SORT-MERGE: With 15 buﬀer pages we can sort R in three passes and S in
two passes. The cost of sorting R is 2 ∗3 ∗M = 6, 000, the cost of sorting S is
2 ∗2 ∗N = 800, and the cost of the merging phase is M + N = 1, 200.
T otalCost = 6, 000 + 800 + 1, 200 = 8, 000
HASH JOIN: With 15 buﬀer pages the ﬁrst scan of S (the smaller relation) splits
it into 14 buckets, each containing about 15 pages. To store one of these buckets
(and its hash table) in memory will require f ∗15 pages, which is more than we
have available. We must apply the Hash Join technique again to all partitions of
R and S that were created by the ﬁrst partitioning phase. Then we can ﬁt an
entire partition of S in memory. The total cost will be the cost of two partioning
phases plus the cost of one matching phase.
T otalCost = 2 ∗(2 ∗(M + N)) + (M + N) = 6, 000
4. SORT-MERGE: With 15 buﬀer pages we can sort R in three passes and S in
three passes. The cost of sorting R is 2 ∗3 ∗M = 6, 000, the cost of sorting S is
2 ∗3 ∗N = 6, 000, and the cost of the merging phase is M + N = 2, 000.
T otalCost = 6, 000 + 6, 000 + 2, 000 = 14, 000

140
Chapter 14
HASH JOIN: Now both relations are the same size, so we can treat either one
as the smaller relation. With 15 buﬀer pages the ﬁrst scan of S splits it into 14
buckets, each containing about 72 pages, so again we have to deal with partition
overﬂow. We must apply the Hash Join technique again to all partitions of R
and S that were created by the ﬁrst partitioning phase. Then we can ﬁt an entire
partition of S in memory. The total cost will be the cost of two partioning phases
plus the cost of one matching phase.
T otalCost = 2 ∗(2 ∗(M + N)) + (M + N) = 10, 000
5. SORT-MERGE: With 52 buﬀer pages we have B >
√
M so we can use the ”merge-
on-the-ﬂy” reﬁnement which costs 3 ∗(M + N).
T otalCost = 3 ∗(1, 000 + 1, 000) = 6, 000
HASH JOIN: Now both relations are the same size, so we can treat either one
as the smaller relation. With 52 buﬀer pages the ﬁrst scan of S splits it into 51
buckets, each containing about 20 pages. This time we do not have to deal with
partition overﬂow. The total cost will be the cost of one partioning phase plus the
cost of one matching phase.
T otalCost = (2 ∗(M + N)) + (M + N) = 6, 000
Exercise 14.6 Answer each of the questions—if some question is inapplicable, explain
why—in Exercise 14.4 again but using the following information about R and S:
Relation R contains 200,000 tuples and has 20 tuples per page.
Relation S contains 4,000,000 tuples and also has 20 tuples per page.
Attribute a of relation R is the primary key for R.
Each tuple of R joins with exactly 20 tuples of S.
1,002 buﬀer pages are available.
Answer 14.6 Answer omitted.
Exercise 14.7 We described variations of the join operation called outer joins in Sec-
tion 5.6.4. One approach to implementing an outer join operation is to ﬁrst evaluate
the corresponding (inner) join and then add additional tuples padded with null values
to the result in accordance with the semantics of the given outer join operator. How-
ever, this requires us to compare the result of the inner join with the input relations
to determine the additional tuples to be added. The cost of this comparison can be
avoided by modifying the join algorithm to add these extra tuples to the result while in-
put tuples are processed during the join. Consider the following join algorithms: block
nested loops join, index nested loops join, sort-merge join, and hash join. Describe
how you would modify each of these algorithms to compute the following operations
on the Sailors and Reserves tables discussed in this chapter:

Evaluation of Relational Operators
141
1. Sailors NATURAL LEFT OUTER JOIN Reserves
2. Sailors NATURAL RIGHT OUTER JOIN Reserves
3. Sailors NATURAL FULL OUTER JOIN Reserves
Answer 14.7 Each join method is considered in turn.
1. Sailors (S) NATURAL LEFT OUTER JOIN Reserves (R)
In this LEFT OUTER JOIN, Sailor rows without a matching Reserves row will appear
in the result with a null value for the Reserves value.
(a) block nested loops join
In the block nested loops join algorithm, we place as large a partition of the
Sailors relation in memory as possibly, leaving 2 extra buﬀer pages (one for
input pages of R, the other for output pages plus enough pages for a single
bit for each record of the block of S. These ’bit pages’ are initially set to
zero; when a tuple of R matches a tuple in S, the bit is set to 1 meaning
that this page has already met the join condition. Once all of R has been
compared to the block of S, any tuple with its bit still set to zero is added
to the ouput with a null value for the R tuple. This process is then repeated
for the remaining blocks of S.
(b) index nested loops join
An index nested loops join requires an index for Reserves on all attributes
that Sailors and Reserves have in common. For each tuple in Sailors, if it
matches a tuple in the R index, it is added to the output, otherwise the S
tuple is added to the output with a null value.
(c) sort-merge join
When the two relations are merged, Sailors is scanned in sorted order and if
there is no match in Reserves, the Sailors tuple is added to the output with
a null value.
(d) hash join
We hash so that partitions of Reserves will ﬁt in memory with enough leftover
space to hold a page of the corresponding Sailors partition. When we compare
a Sailors tuple to all of the tuples in the Reserves partition, if there is a match
it is added to the output, otherwise we add the S tuple and a null value to
the output.
2. Sailors NATURAL RIGHT OUTER JOIN Reserves
In this RIGHT OUTER JOIN, Reserves rows without a matching Sailors row will
appear in the result with a null value for the Sailors value.
(a) block nested loops join
In the block nested loops join algorithm, we place as large a partition of the

142
Chapter 14
Reserves relation in memory as possibly, leaving 2 extra buﬀer pages (one
for input pages of Sailors, the other for output pages plus enough pages for a
single bit for each record of the block of R. These ’bit pages’ are initially set
to zero; when a tuple of S matches a tuple in R, the bit is set to 1 meaning
that this page has already met the join condition. Once all of S has been
compared to the block of R, any tuple with its bit still set to zero is added
to the ouput with a null value for the S tuple. This process is then repeated
for the remaining blocks of R.
(b) index nested loops join
An index nested loops join requires an index for Sailors on all attributes
that Reserves and Sailors have in common. For each tuple in Reserves, if it
matches a tuple in the S index, it is added to the output, otherwise the R
tuple is added to the output with a null value.
(c) sort-merge join
When the two relations are merged, Reserves is scanned in sorted order and
if there is no match in Sailors, the Reserves tuple is added to the output with
a null value.
(d) hash join
We hash so that partitions of Sailors will ﬁt in memory with enough leftover
space to hold a page of the corresponding Reserves partition.
When we
compare a Reserves tuple to all of the tuples in the Sailors partition, if there
is a match it is added to the output, otherwise we add the Reserves tuple
and a null value to the output.
3. Sailors NATURAL FULL OUTER JOIN Reserves
In this FULL OUTER JOIN, Sailor rows without a matching Reserves row will appear
in the result with a null value for the Reserves value, and Reserves rows without
a matching Sailors row will appear in teh result with a null value.
(a) block nested loops join
For this algorithm to work properly, we need a bit for each tuple in both
relations. If after completing the join there are any bits still set to zero,
these tuples are joined with null values.
(b) index nested loops join
If there is only an index on one relation, we can use that index to ﬁnd half
of the full outer join in a similar fashion as in the LEFT and RIGHT OUTER
joins. To ﬁnd the non-matches of the relation with the index, we can use the
same trick as in the block nested loops join and keep bit ﬂags for each block
of scans.
(c) sort-merge join
During the merge phase, we scan both relations alternating to the relation
with the lower value. If that tuple has no match, it is added to the output
with a null value.

Evaluation of Relational Operators
143
(d) hash join
When we hash both relations, we should choose a hash function that will
hash the larger relation into partitions that will ﬁt in half of memory. This
way we can ﬁt both relations’ partitions into main memory and we can scan
both relations for matches. If no match is found (we must scan for both
relations), then we add that tuple to the output with a null value.

15
A TYPICAL QUERY OPTIMIZER
Exercise 15.1 Brieﬂy answer the following questions:
1. In the context of query optimization, what is an SQL query block?
2. Deﬁne the term reduction factor.
3. Describe a situation in which projection should precede selection in processing a
project-select query, and describe a situation where the opposite processing order
is better. (Assume that duplicate elimination for projection is done via sorting.)
4. If there are unclustered (secondary) B+ tree indexes on both R.a and S.b, the join
R ▷◁a=bS could be processed by doing a sort-merge type of join—without doing
any sorting—by using these indexes.
(a) Would this be a good idea if R and S each has only one tuple per page or
would it be better to ignore the indexes and sort R and S? Explain.
(b) What if R and S each have many tuples per page? Again, explain.
5. Explain the role of interesting orders in the System R optimizer.
Answer 15.1 The answer to each question is given below.
1. An SQL query block is an SQL query without nesting, and serves as a unit of
optimization. Blocks have one SELECT statement, one FROM statement, and at
most one WHERE, one GROUP BY, and one HAVING statements. Queries with nesting
can be broken up into a collection of query blocks whose evaluation must be
coordinated at runtime.
2. The reduction factor for a term, is the ratio between the expected result size to
the input size, considering only the slection represented by the term.
144

A Typical Query Optimizer
145
3. If the selection is to be done on the inner relation of a simple nested loop, and
the projection will reduce the number of pages occupied signiﬁcantly, then the
projection should be done ﬁrst.
The opposite is true in the case of an index-only join. The projections should be
done on the ﬂy after the join.
4.
(a) Using the indexes is a good idea when R and S each have only one tuple per
page. Each data page is read exactly once and the cost of scanning the B+
tree is likely to be very small.
(b) Doing an actual data sort on appropriate keys may be a good idea when R
and S have many tuples per page. Given that the indexes are unclustered,
without sorting there is potential for many reads of a single page.
After
sorting, there will only be one read per matching page. The choice may be
determined by number of potential matches and number of tuples per page.
5. The System R optimizer implements a multiple pass algorithm.
In each pass,
it must consider adding a join to those retained in previous passes. Each level
retains the cheapest plan for each interesting order for result tuples. An ordering
of tuples is interesting if it is sorted on some combination of ﬁelds.
Exercise 15.2 Consider a relation with this schema:
Employees(eid: integer, ename: string, sal: integer, title: string, age: integer)
Suppose that the following indexes, all using Alternative (2) for data entries, exist: a
hash index on eid, a B+ tree index on sal, a hash index on age, and a clustered B+
tree index on ⟨age, sal⟩. Each Employees record is 100 bytes long, and you can assume
that each index data entry is 20 bytes long. The Employees relation contains 10,000
pages.
1. Consider each of the following selection conditions and, assuming that the reduc-
tion factor (RF) for each term that matches an index is 0.1, compute the cost of
the most selective access path for retrieving all Employees tuples that satisfy the
condition:
(a) sal > 100
(b) age = 25
(c) age > 20
(d) eid = 1, 000
(e) sal > 200 ∧age > 30
(f) sal > 200 ∧age = 20
(g) sal > 200 ∧title =′CFO′

146
Chapter 15
(h) sal > 200 ∧age > 30 ∧title =′CFO′
2. Suppose that, for each of the preceding selection conditions, you want to retrieve
the average salary of qualifying tuples. For each selection condition, describe the
least expensive evaluation method and state its cost.
3. Suppose that, for each of the preceding selection conditions, you want to compute
the average salary for each age group. For each selection condition, describe the
least expensive evaluation method and state its cost.
4. Suppose that, for each of the preceding selection conditions, you want to compute
the average age for each sal level (i.e., group by sal). For each selection condition,
describe the least expensive evaluation method and state its cost.
5. For each of the following selection conditions, describe the best evaluation method:
(a) sal > 200 ∨age = 20
(b) sal > 200 ∨title =′CFO′
(c) title =′CFO′ ∧ename =′Joe′
Answer 15.2 Answer omitted.
Exercise 15.3 For each of the following SQL queries, for each relation involved, list
the attributes that must be examined to compute the answer. All queries refer to the
following relations:
Emp(eid: integer, did: integer, sal: integer, hobby: char(20))
Dept(did: integer, dname: char(20), ﬂoor: integer, budget: real)
1. SELECT COUNT(*) FROM Emp E, Dept D WHERE E.did = D.did
2. SELECT MAX(E.sal) FROM Emp E, Dept D WHERE E.did = D.did
3. SELECT MAX(E.sal) FROM Emp E, Dept D WHERE E.did = D.did AND D.ﬂoor = 5
4. SELECT E.did, COUNT(*) FROM Emp E, Dept D WHERE E.did = D.did GROUP BY
D.did
5. SELECT D.ﬂoor, AVG(D.budget) FROM Dept D GROUP BY D.ﬂoor HAVING COUNT(*)
> 2
6. SELECT D.ﬂoor, AVG(D.budget) FROM Dept D GROUP BY D.ﬂoor ORDER BY D.ﬂoor
Answer 15.3 The answer to each question is given below.
1. E.did, D.did

A Typical Query Optimizer
147
2. E.sal, E.did, D.did
3. E.sal, E.did, D.did, D.ﬂoor
4. E.did, D.did
5. D.ﬂoor, D.budget
6. D.ﬂoor, D.budget
Exercise 15.4 You are given the following information:
Executives has attributes ename, title, dname, and address; all are string
ﬁelds of the same length.
The ename attribute is a candidate key.
The relation contains 10,000 pages.
There are 10 buﬀer pages.
1. Consider the following query:
SELECT E.title, E.ename FROM Executives E WHERE E.title=‘CFO’
Assume that only 10% of Executives tuples meet the selection condition.
(a) Suppose that a clustered B+ tree index on title is (the only index) available.
What is the cost of the best plan? (In this and subsequent questions, be sure
to describe the plan you have in mind.)
(b) Suppose that an unclustered B+ tree index on title is (the only index) avail-
able. What is the cost of the best plan?
(c) Suppose that a clustered B+ tree index on ename is (the only index) available.
What is the cost of the best plan?
(d) Suppose that a clustered B+ tree index on address is (the only index) avail-
able. What is the cost of the best plan?
(e) Suppose that a clustered B+ tree index on ⟨ename, title⟩is (the only index)
available. What is the cost of the best plan?
2. Suppose that the query is as follows:
SELECT E.ename FROM Executives E WHERE E.title=‘CFO’ AND E.dname=‘Toy’
Assume that only 10% of Executives tuples meet the condition E.title =′CFO′,
only 10% meet E.dname =′T oy′, and that only 5% meet both conditions.
(a) Suppose that a clustered B+ tree index on title is (the only index) available.
What is the cost of the best plan?

148
Chapter 15
(b) Suppose that a clustered B+ tree index on dname is (the only index) avail-
able. What is the cost of the best plan?
(c) Suppose that a clustered B+ tree index on ⟨title, dname⟩is (the only index)
available. What is the cost of the best plan?
(d) Suppose that a clustered B+ tree index on ⟨title, ename⟩is (the only index)
available. What is the cost of the best plan?
(e) Suppose that a clustered B+ tree index on ⟨dname, title, ename⟩is (the only
index) available. What is the cost of the best plan?
(f) Suppose that a clustered B+ tree index on ⟨ename, title, dname⟩is (the only
index) available. What is the cost of the best plan?
3. Suppose that the query is as follows:
SELECT E.title, COUNT(*) FROM Executives E GROUP BY E.title
(a) Suppose that a clustered B+ tree index on title is (the only index) available.
What is the cost of the best plan?
(b) Suppose that an unclustered B+ tree index on title is (the only index) avail-
able. What is the cost of the best plan?
(c) Suppose that a clustered B+ tree index on ename is (the only index) available.
What is the cost of the best plan?
(d) Suppose that a clustered B+ tree index on ⟨ename, title⟩is (the only index)
available. What is the cost of the best plan?
(e) Suppose that a clustered B+ tree index on ⟨title, ename⟩is (the only index)
available. What is the cost of the best plan?
4. Suppose that the query is as follows:
SELECT E.title, COUNT(*) FROM Executives E
WHERE E.dname > ‘W%’ GROUP BY E.title
Assume that only 10% of Executives tuples meet the selection condition.
(a) Suppose that a clustered B+ tree index on title is (the only index) available.
What is the cost of the best plan? If an additional index (on any search key
you want) is available, would it help produce a better plan?
(b) Suppose that an unclustered B+ tree index on title is (the only index) avail-
able. What is the cost of the best plan?
(c) Suppose that a clustered B+ tree index on dname is (the only index) avail-
able. What is the cost of the best plan? If an additional index (on any search
key you want) is available, would it help to produce a better plan?
(d) Suppose that a clustered B+ tree index on ⟨dname, title⟩is (the only index)
available. What is the cost of the best plan?

A Typical Query Optimizer
149
(e) Suppose that a clustered B+ tree index on ⟨title, dname⟩is (the only index)
available. What is the cost of the best plan?
Answer 15.4 Answer omitted.
Exercise 15.5 Consider the query πA,B,C,D(R ▷◁A=CS). Suppose that the projec-
tion routine is based on sorting and is smart enough to eliminate all but the desired
attributes during the initial pass of the sort and also to toss out duplicate tuples on-
the-ﬂy while sorting, thus eliminating two potential extra passes. Finally, assume that
you know the following:
R is 10 pages long, and R tuples are 300 bytes long.
S is 100 pages long, and S tuples are 500 bytes long.
C is a key for S, and A is a key for R.
The page size is 1024 bytes.
Each S tuple joins with exactly one R tuple.
The combined size of attributes A, B, C, and D is 450 bytes.
A and B are in R and have a combined size of 200 bytes; C and D are in S.
1. What is the cost of writing out the ﬁnal result? (As usual, you should ignore this
cost in answering subsequent questions.)
2. Suppose that three buﬀer pages are available, and the only join method that is
implemented is simple (page-oriented) nested loops.
(a) Compute the cost of doing the projection followed by the join.
(b) Compute the cost of doing the join followed by the projection.
(c) Compute the cost of doing the join ﬁrst and then the projection on-the-ﬂy.
(d) Would your answers change if 11 buﬀer pages were available?
Answer 15.5 The answer to each question is given below.
1. From the given information, we know that R has 30 tuples (10 pages of 3 records
each), and S has 200 tuples (100 pages of 2 records each). Since every S tuple
joins with exaclty one R tuple, there can be at most 200 tuples after the join.
Since the size of the result is 450 bytes/record, 2 records will ﬁt on a page. This
means 200 / 2 = 100 page writes are needed to write the result to disk.
2.
(a) Cost of projection followed by join: The projection is sort-based, so we must
sort relation S, which contains attributes C and D. Relation S has 100 pages,
and we have 3 buﬀer pages, so the sort cost is 200 ∗⌈log2(100)⌉= 200 ∗7 =
1400.

150
Chapter 15
Assume that 1/10 of the tuples are removed as duplicates, so that there are
180 remaining tuples of S, each of size 150 bytes (combined size of attributes
C, D). Therefore, 6 tuples ﬁt on a page, so the resulting size of the inner
relation is 30 pages.
The projection on R is calculated similarly: R has 10 pages, so the sort will
cost 30 ∗⌈log2(10)⌉= 30 ∗4 = 120. If 1/10 are not duplicates, then there are
27 tuples remaining, each of size 200 bytes. Therefore, 5 tuples ﬁt on a page
so the resulting size of the outer relation is 6 pages.
The cost using SNL is (6 + 6*30) = 186 I/Os, for a total cost of 1586 I/Os.
(b) Cost of join followed by projection:
SNL join is (10 + 10*100) = 1010 I/Os, and results in 200 tuples, each of
size 800 bytes. Thus, only one result tuple ﬁts on a page, and we have 200
pages.
The projection is a sort using 3 buﬀer pages, and in the ﬁrst pass unwanted
attributes are eliminated on-the-ﬂy to produce tuples of size 450 bytes, i.e.,
2 tuples per page.
Thus, 200 pages are scanned and 100 pages written
in the ﬁrst pass in 33 runs of 3 pages each and 1 run of a page.
These
runs are merged pairwise in 6 additional passes for a total projection cost
of 200+100+2*6*100=1500 I/Os. This includes the cost of writing out the
result of 100 pages; removing this cost and adding the cost of the join step,
we obtain a total cost of 2410 I/Os.
(c) Cost of join and projection on the ﬂy:
This means that the projection cost is 0, so the only cost is the join, which
we know from above is 1010 I/Os.
(d) If we had 11 buﬀer pages, then the projection sort could be done log10 instead
of log2.
Exercise 15.6 Brieﬂy answer the following questions:
1. Explain the role of relational algebra equivalences in the System R optimizer.
2. Consider a relational algebra expression of the form σc(πl(R × S)).
Suppose
that the equivalent expression with selections and projections pushed as much
as possible, taking into account only relational algebra equivalences, is in one of
the following forms.
In each case give an illustrative example of the selection
conditions and the projection lists (c, l, c1, l1, etc.).
(a) Equivalent maximally pushed form: πl1(σc1(R) × S).
(b) Equivalent maximally pushed form: πl1(σc1(R) × σc2(S)).
(c) Equivalent maximally pushed form: σc(πl1(πl2(R) × S)).
(d) Equivalent maximally pushed form: σc1(πl1(σc2(πl2(R)) × S)).

A Typical Query Optimizer
151
(e) Equivalent maximally pushed form: σc1(πl1(πl2(σc2(R)) × S)).
(f) Equivalent maximally pushed form: πl(σc1(πl1(πl2(σc2(R)) × S))).
Answer 15.6 Answer omitted.
Exercise 15.7 Consider the following relational schema and SQL query. The schema
captures information about employees, departments, and company ﬁnances (organized
on a per department basis).
Emp(eid: integer, did: integer, sal: integer, hobby: char(20))
Dept(did: integer, dname: char(20), ﬂoor: integer, phone: char(10))
Finance(did: integer, budget: real, sales: real, expenses: real)
Consider the following query:
SELECT D.dname, F.budget
FROM
Emp E, Dept D, Finance F
WHERE
E.did=D.did AND D.did=F.did AND D.ﬂoor=1
AND E.sal ≥59000 AND E.hobby = ‘yodeling’
1. Identify a relational algebra tree (or a relational algebra expression if you prefer)
that reﬂects the order of operations a decent query optimizer would choose.
2. List the join orders (i.e., orders in which pairs of relations can be joined to compute
the query result) that a relational query optimizer will consider. (Assume that
the optimizer follows the heuristic of never considering plans that require the
computation of cross-products.) Brieﬂy explain how you arrived at your list.
3. Suppose that the following additional information is available: Unclustered B+
tree indexes exist on Emp.did, Emp.sal, Dept.ﬂoor, Dept.did, and Finance.did.
The system’s statistics indicate that employee salaries range from 10,000 to 60,000,
employees enjoy 200 diﬀerent hobbies, and the company owns two ﬂoors in the
building. There are a total of 50,000 employees and 5,000 departments (each with
corresponding ﬁnancial information) in the database. The DBMS used by the
company has just one join method available, index nested loops.
(a) For each of the query’s base relations (Emp, Dept, and Finance) estimate
the number of tuples that would be initially selected from that relation if all
of the non-join predicates on that relation were applied to it before any join
processing begins.
(b) Given your answer to the preceding question, which of the join orders con-
sidered by the optimizer has the lowest estimated cost?
Answer 15.7 The answer to each question is given below.

152
Chapter 15
1.
πD.dname,F.budget(((πE.did(σE.sal>=59000,E.hobby=”yodelling”(E))
▷◁πD.did,D.dname(σD.floor=1(D))) ▷◁πF.budget,F.did(F))
2. There are 2 join orders considered, assuming that the optimizer only consider
left-deep joins and ignores cross-products: (D,E,F) and (D,F,E)
3.
(a) The answer to each relation is given below.
Emp: card = 50,000, E.sal ≥59,000, E.hobby = ”yodelling” resulting
card = 50000 * 1/50 * 1/200 = 5
Dept: card = 5000, D.ﬂoor = 1
resulting card = 5000 * 1/2 = 2500
Finance: card = 5000, there are no non-join predicates
resulting card = 5000
(b) Consider the following join methods on the following left-deep tree: (E ▷◁
D) ▷◁F).
The tuples from E will be pipelined, no temporary relations are created.
First, retrieve the tuples from E with salary ≥59,000 using the B-tree in-
dex on salary; we estimate 1000 such tuples will be found, with a cost of
1 tree traversal + the cost of retrieving the 1000 tuples (since the index is
unclustered) = 3+1000 = 1003. Note, we ignore the cost of scanning the
leaves.
Of these 1000 retrieved tuples, on the ﬂy select only those that have hobby
= ”yodelling”, we estimate there will be 5 such tuples.
Pipeline these 5 tuples one at a time to D, and using the B-tree index on
D.did and the fact the D.did is a key, we can ﬁnd the matching tuples for
the join by searching the Btree and retrieving at most 1 matching tuple, for
a total cost of 5(3 + 1) = 20. The resulting cardinality of this join is at most
5.
Pipeline the estimated 3 tuples of these 5 that have D.ﬂoor=1 1 up to F, and
use the Btree index on F.did and the fact that F.did is a key to retrieve at
most 1 F tuple for each of the 3 pipelined tuples. This costs at most 3(3+1)
= 12.
Ignoring the cost of writing out the ﬁnal result, we get a total cost of
1003+20+12 = 1035.
Exercise 15.8 Consider the following relational schema and SQL query:
Suppliers(sid: integer, sname: char(20), city: char(20))
Supply(sid: integer, pid: integer)
Parts(pid: integer, pname: char(20), price: real)

A Typical Query Optimizer
153
SELECT S.sname, P.pname
FROM
Suppliers S, Parts P, Supply Y
WHERE
S.sid = Y.sid AND Y.pid = P.pid AND
S.city = ‘Madison’ AND P.price ≤1,000
1. What information about these relations does the query optimizer need to select a
good query execution plan for the given query?
2. How many diﬀerent join orders, assuming that cross-products are disallowed, does
a System R style query optimizer consider when deciding how to process the given
query? List each of these join orders.
3. What indexes might be of help in processing this query? Explain brieﬂy.
4. How does adding DISTINCT to the SELECT clause aﬀect the plans produced?
5. How does adding ORDER BY sname to the query aﬀect the plans produced?
6. How does adding GROUP BY sname to the query aﬀect the plans produced?
Answer 15.8 Answer omitted.
Exercise 15.9 Consider the following scenario:
Emp(eid: integer, sal: integer, age: real, did: integer)
Dept(did: integer, projid: integer, budget: real, status: char(10))
Proj(projid: integer, code: integer, report: varchar)
Assume that each Emp record is 20 bytes long, each Dept record is 40 bytes long, and
each Proj record is 2000 bytes long on average. There are 20,000 tuples in Emp, 5000
tuples in Dept (note that did is not a key), and 1000 tuples in Proj. Each department,
identiﬁed by did, has 10 projects on average. The ﬁle system supports 4000 byte pages,
and 12 buﬀer pages are available. All following questions are based on this information.
You can assume uniform distribution of values. State any additional assumptions. The
cost metric to use is the number of page I/Os. Ignore the cost of writing out the ﬁnal
result.
1. Consider the following two queries: “Find all employees with age = 30” and
“Find all projects with code = 20.” Assume that the number of qualifying tuples
is the same in each case. If you are building indexes on the selected attributes to
speed up these queries, for which query is a clustered index (in comparison to an
unclustered index) more important?
2. Consider the following query: “Find all employees with age > 30.” Assume that
there is an unclustered index on age. Let the number of qualifying tuples be N.
For what values of N is a sequential scan cheaper than using the index?

154
Chapter 15
3. Consider the following query:
SELECT *
FROM
Emp E, Dept D
WHERE
E.did=D.did
(a) Suppose that there is a clustered hash index on did on Emp. List all the
plans that are considered and identify the plan with the lowest estimated
cost.
(b) Assume that both relations are sorted on the join column. List all the plans
that are considered and show the plan with the lowest estimated cost.
(c) Suppose that there is a clustered B+ tree index on did on Emp and Dept is
sorted on did. List all the plans that are considered and identify the plan
with the lowest estimated cost.
4. Consider the following query:
SELECT
D.did, COUNT(*)
FROM
Dept D, Proj P
WHERE
D.projid=P.projid
GROUP BY
D.did
(a) Suppose that no indexes are available. Show the plan with the lowest esti-
mated cost.
(b) If there is a hash index on P.projid what is the plan with lowest estimated
cost?
(c) If there is a hash index on D.projid what is the plan with lowest estimated
cost?
(d) If there is a hash index on D.projid and P.projid what is the plan with lowest
estimated cost?
(e) Suppose that there is a clustered B+ tree index on D.did and a hash index
on P.projid. Show the plan with the lowest estimated cost.
(f) Suppose that there is a clustered B+ tree index on D.did, a hash index
on D.projid, and a hash index on P.projid. Show the plan with the lowest
estimated cost.
(g) Suppose that there is a clustered B+ tree index on ⟨D.did, D.projid⟩and a
hash index on P.projid. Show the plan with the lowest estimated cost.
(h) Suppose that there is a clustered B+ tree index on ⟨D.projid, D.did⟩and a
hash index on P.projid. Show the plan with the lowest estimated cost.
5. Consider the following query:

A Typical Query Optimizer
155
SELECT
D.did, COUNT(*)
FROM
Dept D, Proj P
WHERE
D.projid=P.projid AND D.budget>99000
GROUP BY
D.did
Assume that department budgets are uniformly distributed in the range 0 to
100,000.
(a) Show the plan with lowest estimated cost if no indexes are available.
(b) If there is a hash index on P.projid show the plan with lowest estimated cost.
(c) If there is a hash index on D.budget show the plan with lowest estimated
cost.
(d) If there is a hash index on D.projid and D.budget show the plan with lowest
estimated cost.
(e) Suppose that there is a clustered B+ tree index on ⟨D.did,D.budget⟩and a
hash index on P.projid. Show the plan with the lowest estimated cost.
(f) Suppose there is a clustered B+ tree index on D.did, a hash index on D.budget,
and a hash index on P.projid. Show the plan with the lowest estimated cost.
(g) Suppose there is a clustered B+ tree index on ⟨D.did, D.budget, D.projid⟩
and a hash index on P.projid. Show the plan with the lowest estimated cost.
(h) Suppose there is a clustered B+ tree index on ⟨D.did, D.projid, D.budget⟩
and a hash index on P.projid. Show the plan with the lowest estimated cost.
6. Consider the following query:
SELECT E.eid, D.did, P.projid
FROM
Emp E, Dept D, Proj P
WHERE
E.sal=50,000 AND D.budget>20,000
E.did=D.did AND D.projid=P.projid
Assume that employee salaries are uniformly distributed in the range 10,009 to
110,008 and that project budgets are uniformly distributed in the range 10,000 to
30,000. There is a clustered index on sal for Emp, a clustered index on did for
Dept, and a clustered index on projid for Proj.
(a) List all the one-relation, two-relation, and three-relation subplans considered
in optimizing this query.
(b) Show the plan with the lowest estimated cost for this query.
(c) If the index on Proj were unclustered, would the cost of the preceding plan
change substantially? What if the index on Emp or on Dept were unclus-
tered?

156
Chapter 15
Answer 15.9 The reader should calculate actual costs of all alternative plans; in the
answers below, we just outline the best plans without detailed cost calculations to
prove that these are indeed the best plans.
1. The question speciﬁes that the number, rather than the fraction, of qualifying
tuples is identical for the two queries. Since Emp tuples are small, many will ﬁt
on a single page; conversely, few (just 2) of the large Proj tuples will ﬁt on a page.
Since we wish to minimize the number of page I/Os, it will be an advantage if the
Emp tuples are clustered with respect to the age index (all matching tuples will be
retrieved in a few page I/Os). Clustering is not as important for the Proj tuples
since almost every matching tuple will require a page I/O, even with clustering.
2. The Emp relation occupies 100 pages.
For an unclustered index retrieving N
tuples requires N page I/Os. If more than 100 tuples match, the cost of fetching
Emp tuples by following pointers in the index data entries exceeds the cost of
sequential scan. Using the index also involves about 2 I/Os to get to the right leaf
page, and the cost of fetching leaf pages that contain qualifying data entries; this
makes scan better than the index with fewer than 100 matches.)
3.
(a) One plan is to use (simple or blocked) NL join with E as the outer. Another
plan is SM or Hash join. A third plan is to use D as the outer and to use INL;
given the clustered hash index on E, this plan will likely be the cheapest.
(b) The same plans are considered as before, but now, SM join is the best strategy
because both relations are sorted on the join column (and all tuples of Emp
are likely to join with some tuple of Dept, and must therefore be fetched at
least once, even if INL is used).
(c) The same plans are considered as before. As in the previous case, SM join
is the best: the clustered B+ tree index on Emp can be used to eﬃciently
retrieve Emp tuples in sorted order.
4.
(a) BNL with Proj as the outer, followed by sorting on did to implement the
aggregation. All attributes except did can be eliminated during the join but
duplicates should not be eliminated!
(b) Sort Dept on did ﬁrst (all other attributes except projid can be projected
out), then scan while probing Proj and counting tuples in each did group
on-the-ﬂy.
(c) INL with Dept as inner, followed by sorting on did to implement the aggre-
gation. Again, all attributes except did can be eliminated during the join
but duplicates should not be eliminated!
(d) As in the previous case, INL with Dept as inner, followed by sorting on
did to implement the aggregation. Again, all attributes except did can be
eliminated during the join but duplicates should not be eliminated!

A Typical Query Optimizer
157
(e) Scan Dept in did order using the clustered B+ tree index while probing Proj
and counting tuples in each did group on-the-ﬂy.
(f) Same as above.
(g) Scan the clustered B+ tree index using an index-only scan while probing
Proj and counting tuples in each did group on-the-ﬂy.
(h) Sort the data entries in the clustered B+ tree index on Dept, then scan while
probing Proj and counting tuples in each did group on-the-ﬂy.
5.
(a) BNL with Proj as the outer with the selection applied on-the-ﬂy, followed
by sorting on did to implement the aggregation. All attributes except did
can be eliminated during the join but duplicates should not be eliminated!
(b) Sort Dept on did ﬁrst (while applying the selection and projecting out all
other attributes except projid in the initial scan), then scan while probing
Proj and counting tuples in each did group on-the-ﬂy.
(c) Select Dept tuples using the index on budget, join using INL with Proj as
inner, projecting out all attributes except did. Then sort to implement the
aggregation.
(d) Same as the case with no index; this index does not help.
(e) Retrieve Dept tuples that satisfy the condition on budget in did order by
using the clustered B+ tree index while probing Proj and counting tuples in
each did group on-the-ﬂy.
(f) Since the condition on budget is very selective, even though the index on
budget is unclustered we retrieve Dept tuples using this index, project out
the did and projid ﬁelds and sort them by did. Then we scan while probing
Proj and counting tuple sin each did gorup on-the-ﬂy.
(g) Use an index-only scan on the B+ tree and apply the condition on budget,
while probing Proj and counting tuples in each did group on-the-ﬂy. Notice
that this plan is applicable even if the B+ tree index is not clustered. (Within
each did group, can optimize search for data entries in the index that satisfy
the budget condition, but this is a minor gain.)
(h) Use an index-only scan on the B+ tree and apply the condition on budget,
while probing Proj and counting tuples in each did group on-the-ﬂy.
6.
(a) 1-relation subplans: Clustered index on E.sal; Scan Dept; and Scan Proj.
2-relation subplans: (i) Clustered index on E.sal, probe Dept using the index
on did, apply predicate on D.budget and join. (ii) Scan Dept, apply predi-
cate on D.budget and probe Proj. (iii) Scan Proj, probe Dept and apply
predicate on D.budget and join.
3-relation subplans: Join Emp and Dept and probe Proj; Join Dept and
Proj and probe Emp.

158
Chapter 15
(b) The least cost plan is to use the index on E.sal to eliminate most tuples, probe
Dept using the index on D.did, apply the predicate on D.budget, probe and
join on Proj.projid.
(c) Unclustering the index on Proj would increase the number of I/Os but not
substantially since the total number of matching Proj tuples to be retrieved
is small.

16
OVERVIEW OF TRANSACTION
MANAGEMENT
Exercise 16.1 Give brief answers to the following questions:
1. What is a transaction? In what ways is it diﬀerent from an ordinary program (in
a language such as C)?
2. Deﬁne these terms: atomicity, consistency, isolation, durability, schedule, blind
write, dirty read, unrepeatable read, serializable schedule, recoverable schedule,
avoids-cascading-aborts schedule.
3. Describe Strict 2PL.
4. What is the phantom problem?
Can it occur in a database where the set of
database objects is ﬁxed and only the values of objects can be changed?
Answer 16.1 The answer to each question is given below.
1. A transaction is an execution of a user program, and is seen by the DBMS as a
series or list of actions. The actions that can be executed by a transaction include
reads and writes of database objects, whereas actions in an ordinary program
could involve user input, access to network devices, user interface drawing, etc.
2. Each term is described below.
(a) Atomicity means a transaction executes when all actions of the transaction
are completed fully, or none are. This means there are no partial transactions
(such as when half the actions complete and the other half do not).
(b) Consistency involves beginning a transaction with a ’consistent’ database,
and ﬁnishing with a ’consistent’ database. For example, in a bank database,
money should never be ”created” or ”deleted” without an appropriate deposit
or withdrawal. Every transaction should see a consistent database.
159

160
Chapter 16
(c) Isolation ensures that a transaction can run independently, without consider-
ing any side eﬀects that other concurrently running transactions might have.
When a database interleaves transaction actions for performance reasons, the
database protects each transaction from the eﬀects of other transactions.
(d) Durability deﬁnes the persistence of committed data: once a transaction
commits, the data should persist in the database even if the system crashes
before the data is written to non-volatile storage.
(e) A schedule is a series of (possibly overlapping) transactions.
(f) A blind write is when a transaction writes to an object without ever reading
the object.
(g) A dirty read occurs when a transaction reads a database object that has been
modiﬁed by another not-yet-committed transaction.
(h) An unrepeatable read occurs when a transaction is unable to read the same
object value more than once, even though the transaction has not modiﬁed
the value. Suppose a transaction T2 changes the value of an object A that
has been read by a transaction T1 while T1 is still in progress. If T1 tries
to read the value of A again, it will get a diﬀerent result, even though it has
not modiﬁed A.
(i) A serializable schedule over a set S of transactions is a schedule whose eﬀect
on any consistent database instance is identical to that of some complete
serial schedule over the set of committed transactions in S.
(j) A recoverable schedule is one in which a transaction can commit only after
all other transactions whose changes it has read have committed.
(k) A schedule that avoids-cascading-aborts is one in which transactions only
read the changes of committed transactions. Such a schedule is not only
recoverable, aborting a transaction can be accomplished without cascading
the abort to other transactions.
3. Strict 2PL is the most widely used locking protocol where 1) A transaction requests
a shared/exclusive lock on the object before it reads/modiﬁes the object. 2) All
locks held by a transaction are released when the transaction is completed.
4. The phantom problem is a situation where a transaction retrieves a collection of
objects twice but sees diﬀerent results, even though it does not modify any of these
objects itself and follows the strict 2PL protocol. This problem usually arises in
dynamic databases where a transaction cannot assume it has locked all objects of
a given type (such as all sailors with rank 1; new sailors of rank 1 can be added
by a second transaction after one transaction has locked all of the original ones).
If the set of database objects is ﬁxed and only the values of objects can be changed,
the phantom problem cannot occur since one cannot insert new objects into the
database.

Overview of Transaction Management
161
Exercise 16.2 Consider the following actions taken by transaction T 1 on database
objects X and Y :
R(X), W(X), R(Y), W(Y)
1. Give an example of another transaction T 2 that, if run concurrently to transaction
T without some form of concurrency control, could interfere with T 1.
2. Explain how the use of Strict 2PL would prevent interference between the two
transactions.
3. Strict 2PL is used in many database systems. Give two reasons for its popularity.
Answer 16.2 Answer omitted.
Exercise 16.3 Consider a database with objects X and Y and assume that there are
two transactions T 1 and T 2. Transaction T 1 reads objects X and Y and then writes
object X. Transaction T 2 reads objects X and Y and then writes objects X and Y .
1. Give an example schedule with actions of transactions T 1 and T 2 on objects X
and Y that results in a write-read conﬂict.
2. Give an example schedule with actions of transactions T 1 and T 2 on objects X
and Y that results in a read-write conﬂict.
3. Give an example schedule with actions of transactions T 1 and T 2 on objects X
and Y that results in a write-write conﬂict.
4. For each of the three schedules, show that Strict 2PL disallows the schedule.
Answer 16.3 The answer to each question is given below.
1. The following schedule results in a write-read conﬂict:
T2:R(X), T2:R(Y), T2:W(X), T1:R(X) ...
T1:R(X) is a dirty read here.
2. The following schedule results in a read-write conﬂict:
T2:R(X), T2:R(Y), T1:R(X), T1:R(Y), T1:W(X) ...
Now, T2 will get an unrepeatable read on X.
3. The following schedule results in a write-write conﬂict:
T2:R(X), T2:R(Y), T1:R(X), T1:R(Y), T1:W(X), T2:W(X) ...
Now, T2 has overwritten uncommitted data.
4. Strict 2PL resolves these conﬂicts as follows:

162
Chapter 16
(a) In S2PL, T1 could not get a shared lock on X because T2 would be holding
an exclusive lock on X. Thus, T1 would have to wait until T2 was ﬁnished.
(b) Here T1 could not get an exclusive lock on X because T2 would already be
holding a shared or exclusive lock on X.
(c) Same as above.
Exercise 16.4 We call a transaction that only reads database object a read-only
transaction, otherwise the transaction is called a read-write transaction. Give brief
answers to the following questions:
1. What is lock thrashing and when does it occur?
2. What happens to the database system throughput if the number of read-write
transactions is increased?
3. What happens to the database system throughput if the number of read-only
transactions is increased?
4. Describe three ways of tuning your system to increase transaction throughput.
Answer 16.4 Answer omitted.
Exercise 16.5 Suppose that a DBMS recognizes increment, which increments an in-
teger-valued object by 1, and decrement as actions, in addition to reads and writes.
A transaction that increments an object need not know the value of the object; incre-
ment and decrement are versions of blind writes. In addition to shared and exclusive
locks, two special locks are supported: An object must be locked in I mode before
incrementing it and locked in D mode before decrementing it. An I lock is compatible
with another I or D lock on the same object, but not with S and X locks.
1. Illustrate how the use of I and D locks can increase concurrency. (Show a schedule
allowed by Strict 2PL that only uses S and X locks. Explain how the use of I
and D locks can allow more actions to be interleaved, while continuing to follow
Strict 2PL.)
2. Informally explain how Strict 2PL guarantees serializability even in the presence
of I and D locks. (Identify which pairs of actions conﬂict, in the sense that their
relative order can aﬀect the result, and show that the use of S, X, I, and D locks
according to Strict 2PL orders all conﬂicting pairs of actions to be the same as
the order in some serial schedule.)
Answer 16.5 The answer to each question is given below.

Overview of Transaction Management
163
1. Take the following two transactions as example:
T1: Increment A, Decrement B, Read C;
T2: Increment B, Decrement A, Read C
If using only strict 2PL, all actions are versions of blind writes, they have to obtain
exclusive locks on objects. Following strict 2PL, T1 gets an exclusive lock on A,
if T2 now gets an exclusive lock on B, there will be a deadlock. Even if T1 is fast
enough to have grabbed an exclusive lock on B ﬁrst, T2 will now be blocked until
T1 ﬁnishes. This has little concurrency. If I and D locks are used, since I and
D are compatible, T1 obtains an I-Lock on A, and a D-Lock on B; T2 can still
obtain an I-Lock on B, a D-Lock on A; both transactions can be interleaved to
allow maximum concurrency.
2. The pairs of actions which conﬂicts are:
RW, WW, WR, IR, IW, DR, DW
We know that strict 2PL orders the ﬁrst 3 conﬂicts pairs of actions to be the same
as the order in some serial schedule. We can also show that even in the presence
of I and D locks, strict 2PL also orders the latter 4 pairs of actions to be the
same as the order in some serial schedule. Think of an I (or D)lock under these
circumstances as an exclusive lock, since an I(D) lock is not compatible with S
and X locks anyway (ie. can’t get a S or X lock if another transaction has an I or
D lock). So serializability is guaranteed.
Exercise 16.6 Answer the following questions: SQL supports four isolation-levels and
two access-modes, for a total of eight combinations of isolation-level and access-mode.
Each combination implicitly deﬁnes a class of transactions; the following questions
refer to these eight classes:
1. Consider the four SQL isolation levels. Describe which of the phenomena can
occur at each of these isolation levels: dirty read, unrepeatable read, phantom
problem.
2. For each of the four isolation levels, give examples of transactions that could be
run safely at that level.
3. Why does the access mode of a transaction matter?
Answer 16.6 Answer omitted.
Exercise 16.7 Consider the university enrollment database schema:

164
Chapter 16
Student(snum: integer, sname: string, major: string, level: string, age: integer)
Class(name: string, meets at: time, room: string, ﬁd: integer)
Enrolled(snum: integer, cname: string)
Faculty(ﬁd: integer, fname: string, deptid: integer)
The meaning of these relations is straightforward; for example, Enrolled has one record
per student-class pair such that the student is enrolled in the class.
For each of the following transactions, state the SQL isolation level you would use and
explain why you chose it.
1. Enroll a student identiﬁed by her snum into the class named ’Introduction to
Database Systems’.
2. Change enrollment for a student identiﬁed by her snum from one class to another
class.
3. Assign a new faculty member identiﬁed by his ﬁd to the class with the least number
of students.
4. For each class, show the number of students enrolled in the class.
Answer 16.7 The answer to each question is given below.
1. Because we are inserting a new row in the table Enrolled, we do not need any
lock on the existing rows. So we would use READ UNCOMMITTED.
2. Because we are updating one existing row in the table Enrolled, we need an
exclusive lock on the row which we are updating. So we would use READ COM-
MITTED.
3. To prevent other transactions from inserting or updating the table Enrolled while
we are reading from it (known as the phantom problem), we would need to use
SERIALIZABLE.
4. same as above.
Exercise 16.8 Consider the following schema:
Suppliers(sid: integer, sname: string, address: string)
Parts(pid: integer, pname: string, color: string)
Catalog(sid: integer, pid: integer, cost: real)
The Catalog relation lists the prices charged for parts by Suppliers.
For each of the following transactions, state the SQL isolation level that you would use
and explain why you chose it.

Overview of Transaction Management
165
1. A transaction that adds a new part to a supplier’s catalog.
2. A transaction that increases the price that a supplier charges for a part.
3. A transaction that determines the total number of items for a given supplier.
4. A transaction that shows, for each part, the supplier that supplies the part at the
lowest price.
Answer 16.8 Answer omitted.
Exercise 16.9 Consider a database with the following schema:
Suppliers(sid: integer, sname: string, address: string)
Parts(pid: integer, pname: string, color: string)
Catalog(sid: integer, pid: integer, cost: real)
The Catalog relation lists the prices charged for parts by Suppliers.
Consider the transactions T 1 and T 2. T 1 always has SQL isolation level SERIALIZABLE.
We ﬁrst run T 1 concurrently with T 2 and then we run T 1 concurrently with T 2 but we
change the isolation level of T 2 as speciﬁed below. Give a database instance and SQL
statements for T 1 and T 2 such that result of running T 2 with the ﬁrst SQL isolation
level is diﬀerent from running T 2 with the second SQL isolation level. Also specify the
common schedule of T 1 and T 2 and explain why the results are diﬀerent.
1. SERIALIZABLE versus REPEATABLE READ.
2. REPEATABLE READ versus READ COMMITTED.
3. READ COMMITTED versus READ UNCOMMITTED.
Answer 16.9 The answer to each question is given below.
1. Suppose a database instance of table Catalog and SQL statements shown below:
sid
pid
cost
18
45
$7.05
22
98
$89.35
31
52
$357.65
31
53
$26.22
58
15
$37.50
58
94
$26.22

166
Chapter 16
SELECT *
FROM
Catalog C
WHERE
C.cost < 100
EXCEPT
(SELECT*
FROM
Catalog C
WHERE
C.cost < 100 )
INSERT INTO catalog (sid, pid, cost)
VALUES (99, 38, 75.25)
When we use SERIALIZABLE, we would expect that the ﬁrst SQL statement
return nothing. But if we instead use REPEATABLE READ, then it is possible
that the phantom problem could occur from inserting a new tuple into the table,
resulting in the ﬁrst SQL statement incorrectly returning a tuple (in this case the
one inserted by the second SQL statement).
2. Suppose the same database instance as above and SQL statements shown below:
UPDATE Catalog
SET
cost = cost * 0.95
WHERE
sid = 31
UPDATE Catalog
SET
cost = cost * 0.95
WHERE
sid = 31
When we use READ COMMITTED on the SQL statements above, an unrepeat-
able read could occur resulting in an incorrect value being assigned to cost. But
this problem cannot occur when we use REPEATABLE READ.
3. Suppose the same database instance as above and SQL statements shown below
(assuming READ UNCOMMITTED can write to the database):
UPDATE Catalog
SET
cost = cost * 0.95
SELECT C.sid, C.pid
FROM
Catalog C
WHERE
C.cost = 36.22
When we use READ UNCOMMITTED on the SQL statements above, dirty read
of the value of cost could occur because the ﬁrst SQL statement might not be
ﬁnished while the second one is reading. But this problem cannot occur when we
use READ UNCOMMITTED.

17
CONCURRENCY CONTROL
Exercise 17.1 Answer the following questions:
1. Describe how a typical lock manager is implemented. Why must lock and unlock
be atomic operations? What is the diﬀerence between a lock and a latch? What
are convoys and how should a lock manager handle them?
2. Compare lock downgrades with upgrades. Explain why downgrades violate 2PL
but are nonetheless acceptable. Discuss the use of update locks in conjunction
with lock downgrades.
3. Contrast the timestamps assigned to restarted transactions when timestamps are
used for deadlock prevention versus when timestamps are used for concurrency
control.
4. State and justify the Thomas Write Rule.
5. Show that, if two schedules are conﬂict equivalent, then they are view equivalent.
6. Give an example of a serializable schedule that is not strict.
7. Give an example of a strict schedule that is not serializable.
8. Motivate and describe the use of locks for improved conﬂict resolution in Opti-
mistic Concurrency Control.
Answer 17.1 The answer to each question is given below.
1. A typical lock manager is implemented with a hash table, also called lock table,
with the data object identiﬁer as the key. A lock table entry contains the following
information: the number of transactions currently holding a lock on the object,
the nature of the lock, and a pointer to a queue of lock requests.
167

168
Chapter 17
Lock and unlock must be atomic operations because otherwise it may be possi-
ble for two transactions to obtain an exclusive lock on the same object, thereby
destroying the principles of 2PL.
A lock is held over a long duration, and a latch is released immediately after the
physical read or write operation is completed.
Convoy is a queue of waiting transactions. It occurs when a transaction hold-
ing a heavily used lock is suspended by the operating system, and every other
transactions that needs this lock is queued.
2. A lock upgrade is to grant a transaction an exclusive lock of an object for which
it already holds a shared lock. A lock downgrade happens when an exclusive lock
is obtained by a transaction initially, but downgrades to a shared lock once it’s
clear that this is suﬃcient.
Lock downgrade violates the 2PL requirement because it reduces the locking priv-
ileges held by a transaction, and the transaction may go on to acquire other locks.
But the transaction did nothing but read the object that it downgraded. So it is
nonetheless acceptable, provided that the transaction has not modiﬁed the object.
The downgrade approach reduces concurrency, so we introduce a new kind of lock,
called an update lock, that is compatible with shared locks but not other updates
and exclusive lock. By setting an update lock initially, rather than a exclusive lock
as in the case of lock downgrade, we prevent conﬂicts with other read operations
and increase concurrency.
3. When timestamps are used for deadlock prevention, a transaction that is aborted
and re-started it is given the same timestamp that it had originally. When times-
tamps are used for concurrency control, a transaction that is aborted and restarted
is given a new, larger timestamp.
4. The Thomas Write Rule says that if an transaction T with timestamp TS(T) acts
on a database object O with a write timestamp of WTS(O) such that TS(T) <
WTS(O), we can safely ignore writes by T and continue.
To understand and justify the Thomas Write Rule fully, we need to give the
complete context when it arises.
To implement timestamp-based concurrency control scheme, the following regula-
tions are made when transaction T wants to write object O:
(a) If T S(T ) < RT S(O), the write action conﬂicts with the most recent
read action of O, and T is therefore aborted and restarted.
(b) If T S(T ) < WT S(O), a naive approach would be to abort T as well
because its write action conﬂicts with the most recent write of O,
and is out of timestamp order. But it turns out that we can safely
ignore such previous write and process with this new write; this is
called T homas′WriteRule.

Concurrency Control
169
(c) Otherwise, T writes O and WTS(O) is set to TS(T).
The justiﬁcation is as follows: had T S(T ) < RT S(O), T would have been aborted
and we would not have bothered to check the WTS(O). So to decide whether to
abort T based on WTS(O), we can assume that T S(T ) >= RT S(O). If T S(T ) >=
RT S(O) and T S(T ) < WT S(O), then RT S(O) < WT S(O), which means the
previous write occurred immediately before this planned-new-write of O and was
never read by anyone, therefore the previous write can be safely ignored.
5. If two schedules over the same set of actions of the transactions are conﬂict equiv-
alent, they must order every pair of conﬂicting actions of two committed trans-
actions in the same way. Let’s assume that two schedules are conﬂict equivalent,
but they are not view equivalent, then one of the three conditions held under view
equivalency must be violated. But as we can see if every pair of conﬂicting actions
is ordered in the same way, this cannot happen. Thus we can conclude that if two
schedules are conﬂict equivalent, they are also view equivalent.
6. The following example is a serializable schedule, but it’s not strict.
T1:R(X), T2:R(X), T2:W(X), T1:W(X), T2:Commit, T1:Commit
7. The following example is a strict schedule, but it’s not serializable.
T1:R(X), T2:R(X), T1:W(X), T1:Commit, T2:W(X), T2:Commit
8. In Optimistic Concurrency Control, we have no way to tell when T i wrote the
object at the time we validate T j, since all we have is the list of objects written
by T i and the list read by T j. To solve such conﬂict, we use mechanisms very
similar to locking. The basic idea is that each transaction in the Read phase tells
the DBMS about items it is reading, and when a transaction T i is committed and
its writes are accepted, the DBMS checks whether any of the items written by T i
are being rad by any (yet to be validated) transaction T j. If so, we know that
T j’s validation must eventually fail. Then we can pick either the die or kill policy
to resolve the conﬂict.
Exercise 17.2 Consider the following classes of schedules: serializable, conﬂict-se-
rializable, view-serializable, recoverable, avoids-cascading-aborts, and strict. For each
of the following schedules, state which of the preceding classes it belongs to. If you
cannot decide whether a schedule belongs in a certain class based on the listed actions,
explain brieﬂy.
The actions are listed in the order they are scheduled and preﬁxed with the transaction
name. If a commit or abort is not shown, the schedule is incomplete; assume that abort
or commit must follow all the listed actions.
1. T1:R(X), T2:R(X), T1:W(X), T2:W(X)

170
Chapter 17
2. T1:W(X), T2:R(Y), T1:R(Y), T2:R(X)
3. T1:R(X), T2:R(Y), T3:W(X), T2:R(X), T1:R(Y)
4. T1:R(X), T1:R(Y), T1:W(X), T2:R(Y), T3:W(Y), T1:W(X), T2:R(Y)
5. T1:R(X), T2:W(X), T1:W(X), T2:Abort, T1:Commit
6. T1:R(X), T2:W(X), T1:W(X), T2:Commit, T1:Commit
7. T1:W(X), T2:R(X), T1:W(X), T2:Abort, T1:Commit
8. T1:W(X), T2:R(X), T1:W(X), T2:Commit, T1:Commit
9. T1:W(X), T2:R(X), T1:W(X), T2:Commit, T1:Abort
10. T2: R(X), T3:W(X), T3:Commit, T1:W(Y), T1:Commit, T2:R(Y),
T2:W(Z), T2:Commit
11. T1:R(X), T2:W(X), T2:Commit, T1:W(X), T1:Commit, T3:R(X), T3:Commit
12. T1:R(X), T2:W(X), T1:W(X), T3:R(X), T1:Commit, T2:Commit, T3:Commit
Answer 17.2 Answer omitted.
Exercise 17.3 Consider the following concurrency control protocols: 2PL, Strict 2PL,
Conservative 2PL, Optimistic, Timestamp without the Thomas Write Rule, Times-
tamp with the Thomas Write Rule, and Multiversion. For each of the schedules in
Exercise 17.2, state which of these protocols allows it, that is, allows the actions to
occur in exactly the order shown.
For the timestamp-based protocols, assume that the timestamp for transaction T i is i
and that a version of the protocol that ensures recoverability is used. Further, if the
Thomas Write Rule is used, show the equivalent serial schedule.
Answer 17.3 See the table 17.1.
Note the following abbreviations.
S-2PL: Strict 2PL; C-2PL: Conservative 2PL; Opt cc: Optimistic; TS W/O THR:
Timestamp without Thomas Write Rule; TS With THR: Timestamp without Thomas
Write Rule.
Thomas Write Rule is used in the following schedules, and the equivalent serial sched-
ules are shown below:
5. T1:R(X), T1:W(X), T2:Abort, T1:Commit
6. T1:R(X), T1:W(X), T2:Commit, T1:Commit
11. T1:R(X), T2:Commit, T1:W(X), T2:Commit, T3:R(X), T3:Commit

Concurrency Control
171
2PL
S-2PL
C-2PL
Opt CC
TS w/o TWR
TS w/ TWR
Multiv.
1
N
N
N
N
N
N
N
2
Y
N
N
Y
Y
Y
Y
3
N
N
N
Y
N
N
Y
4
N
N
N
Y
N
N
Y
5
N
N
N
Y
N
Y
Y
6
N
N
N
N
N
Y
Y
7
N
N
N
Y
N
N
N
8
N
N
N
N
N
N
N
9
N
N
N
Y
N
N
N
10
N
N
N
N
Y
Y
Y
11
N
N
N
N
N
Y
N
12
N
N
N
N
N
Y
Y
Table 17.1
Exercise 17.4 Consider the following sequences of actions, listed in the order they
are submitted to the DBMS:
Sequence S1: T1:R(X), T2:W(X), T2:W(Y), T3:W(Y), T1:W(Y),
T1:Commit, T2:Commit, T3:Commit
Sequence S2: T1:R(X), T2:W(Y), T2:W(X), T3:W(Y), T1:W(Y),
T1:Commit, T2:Commit, T3:Commit
For each sequence and for each of the following concurrency control mechanisms, de-
scribe how the concurrency control mechanism handles the sequence.
Assume that the timestamp of transaction T i is i. For lock-based concurrency control
mechanisms, add lock and unlock requests to the previous sequence of actions as per the
locking protocol. The DBMS processes actions in the order shown. If a transaction is
blocked, assume that all its actions are queued until it is resumed; the DBMS continues
with the next action (according to the listed sequence) of an unblocked transaction.
1. Strict 2PL with timestamps used for deadlock prevention.
2. Strict 2PL with deadlock detection. (Show the waits-for graph in case of deadlock.)
3. Conservative (and Strict, i.e., with locks held until end-of-transaction) 2PL.
4. Optimistic concurrency control.
5. Timestamp concurrency control with buﬀering of reads and writes (to ensure re-
coverability) and the Thomas Write Rule.

172
Chapter 17
Serializable
Conﬂict-serializable
Recoverable
Avoid cascading aborts
1
No
No
No
No
2
No
No
Yes
Yes
3
Yes
Yes
Yes
Yes
4
Yes
Yes
Yes
Yes
Table 17.2
6. Multiversion concurrency control.
Answer 17.4 Answer omitted.
Exercise 17.5 For each of the following locking protocols, assuming that every trans-
action follows that locking protocol, state which of these desirable properties are
ensured: serializability, conﬂict-serializability, recoverability, avoidance of cascading
aborts.
1. Always obtain an exclusive lock before writing; hold exclusive locks until end-of-
transaction. No shared locks are ever obtained.
2. In addition to (1), obtain a shared lock before reading; shared locks can be released
at any time.
3. As in (2), and in addition, locking is two-phase.
4. As in (2), and in addition, all locks held until end-of-transaction.
Answer 17.5 See the table 17.2.
Exercise 17.6 The Venn diagram (from [76]) in Figure 17.1 shows the inclusions
between several classes of schedules. Give one example schedule for each of the regions
S1 through S12 in the diagram.
Answer 17.6 Answer omitted.
Exercise 17.7 Brieﬂy answer the following questions:
1. Draw a Venn diagram that shows the inclusions between the classes of schedules
permitted by the following concurrency control protocols: 2PL, Strict 2PL, Con-
servative 2PL, Optimistic, Timestamp without the Thomas Write Rule, Times-
tamp with the Thomas Write Rule, and Multiversion.

Concurrency Control
173
S5
S11
S12
All Schedules
View Serializable
Conflict Serializable
Recoverable
Avoid Cascading Abort
Strict
Serial
S10
S8
S9
S6
S3
S2
S7
S4
S1
Figure 17.1
Venn Diagram for Classes of Schedules
Conservative 2PL
Strict 2PL
2PL
Timestamp W/O TWR
Timestamp With TWR
Multiversion
Optimistic
S4
S5
S6
S7
S8
S9
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S20
S21
S22
S23
S24
S25
S26
S29
S30
S31
S32
S27
S28
S1
S2
S3
Figure 17.2
2. Give one example schedule for each region in the diagram.
3. Extend the Venn diagram to include serializable and conﬂict-serializable schedules.
Answer 17.7 The answer to each question is given below.
1. See ﬁgure 17.2.
2.
(a) Here we deﬁne the following schedule ﬁrst:

174
Chapter 17
i. C1: T0:R(O),T0:Commit.
ii. C2: T1:Begin,T2:Begin,T1:W(A),T1:Commit,T2:R(A),T2:Commit.
iii. C3: T4:Begin,T3:Begin,T3:W(B),T3:Commit,T4:W(B),T4:Abort.
iv. C4: T4:Begin,T3:Begin,T3:W(B),T3:Commit,T4:R(B),T4:Abort.
v. C5: T3:Begin,T4:Begin,T4:R(B),T4:Commit,T3:W(B),T3:Commit.
vi. C6: T5:Begin,T6:Begin,T6:R(D),T5:R(C),T5:Commit,
T6:W(C),T6:Commit.
vii. C7: T5:Begin,T6:Begin,T6:R(D),T5:R(C),T6:W(C),
T5:Commit,T6:Commit.
viii. C8: T5:Begin,T6:Begin,T5:R(C),T6:W(C),T5:R(D),
T5:Commit,T6:Commit.
Then we have the following schedule for each region in the diagram.(Please
note, S1: C2,C5,C8 means that S1 is the combination of schedule C2,C5,C8.)
i. S1: C2,C5,C8
ii. S2: C2,C4,C8
iii. S3: C2,C3,C8
iv. S4: C2,C8
v. S5: C2,C5,C7
vi. S6: C2,C4,C7
vii. S7: C2,C3,C7
viii. S8: C2,C7
ix. S9: C2,C5,C6
x. S10: C2,C4,C6
xi. S11: C2,C3,C6
xii. S12: C2,C6
xiii. S13: C2,C5
xiv. S14: C2,C4
xv. S15: C2,C3
xvi. S16: C2,C1
And for the rest of 16 schedules, just remove the C2 from the corresponding
schedule.(eg, S17: C5,C8, which is made by removing C2 from S1.)
3. See ﬁgure 17.3.
Exercise 17.8 Answer each of the following questions brieﬂy. The questions are based
on the following relational schema:
Emp(eid: integer, ename: string, age: integer, salary: real, did: integer)
Dept(did: integer, dname: string, ﬂoor: integer)

Concurrency Control
175
Serializable
Conservative 2PL
Strict 2PL
2PL
Timestamp W/O TWR
Timestamp With TWR
Optimistic
Multiversion
Conflict-serializable
Figure 17.3
and on the following update command:
replace (salary = 1.1 * EMP.salary) where EMP.ename = ‘Santa’
1. Give an example of a query that would conﬂict with this command (in a concur-
rency control sense) if both were run at the same time. Explain what could go
wrong, and how locking tuples would solve the problem.
2. Give an example of a query or a command that would conﬂict with this command,
such that the conﬂict could not be resolved by just locking individual tuples or
pages but requires index locking.
3. Explain what index locking is and how it resolves the preceding conﬂict.
Answer 17.8 Answer omitted.
Exercise 17.9 SQL supports four isolation-levels and two access-modes, for a total
of eight combinations of isolation-level and access-mode. Each combination implicitly
deﬁnes a class of transactions; the following questions refer to these eight classes:
1. For each of the eight classes, describe a locking protocol that allows only transac-
tions in this class. Does the locking protocol for a given class make any assump-
tions about the locking protocols used for other classes? Explain brieﬂy.

176
Chapter 17
2. Consider a schedule generated by the execution of several SQL transactions. Is it
guaranteed to be conﬂict-serializable? to be serializable? to be recoverable?
3. Consider a schedule generated by the execution of several SQL transactions, each
of which has READ ONLY access-mode. Is it guaranteed to be conﬂict-serializable?
to be serializable? to be recoverable?
4. Consider a schedule generated by the execution of several SQL transactions,
each of which has SERIALIZABLE isolation-level. Is it guaranteed to be conﬂict-
serializable? to be serializable? to be recoverable?
5. Can you think of a timestamp-based concurrency control scheme that can support
the eight classes of SQL transactions?
Answer 17.9
1. The classes SERIALIZABLE, REPEATABLE READ and READ COMMITTED
rely on the assumption that other classes obtain exclusive locks before writing ob-
jects and hold exclusive locks until the end of the transaction.
(a) SERIALIZABLE + READ ONLY: Strict 2PL including locks on a set of objects
that it requires to be unchanged. No exclusive locks are granted.
(b) SERIALIZABLE + READ WRITE: Strict 2PL including locks on a set of objects
that it requires to be unchanged.
(c) REPEATABLE READ + READ ONLY: Strict 2PL, only locks individual objects,
not sets of objects. No exclusive locks are granted.
(d) REPEATABLE READ + READ WRITE: Strict 2PL, only locks individual objects,
not sets of objects.
(e) READ COMMITTED + READ ONLY: Obtains shared locks before reading objects,
but these locks are released immediately.
(f) READ COMMITTED + READ WRITE: Obtains exclusive locks before writing ob-
jects, and hold these locks until the end. Obtains shared locks before reading
objects, but these locks are released immediately.
(g) READ UNCOMMITTED + READ ONLY: Do not obtain shared locks before reading
objects.
(h) READ UNCOMMITTED + READ WRITE: Obtains exclusive locks before writing
objects, and hold these locks until the end. Does not obtain shared locks
before reading objects.
2. Suppose we do not have any requirements for the access-mode and isolation-level
of the transaction, then they are not guaranteed to be conﬂict-serializable, serial-
izable, or recoverable.
3. A schedule generated by the execution of several SQL transactions, each of which
having READ ONLY access-mode, would be guaranteed to be conﬂict-serializable,
serializable, and recoverable. This is because the only actions are reads so there
are no WW, RW, or WR conﬂicts.

Concurrency Control
177
4. A schedule generated by the execution of several SQL transactions, each of which
having SERIALIZABLE isolation-level, would be guaranteed to be conﬂict-serializable,
serializable, and recoverable. This is because SERIALIZABLE isolation level follows
strict 2PL.
5. Timestamp locking with wait-die or would wait would be suitable for any SERIALIZABLE
or REPEATABLE READ transaction because these follow strict 2PL. This could be
modiﬁed to allow READ COMMITTED by allowing other transactions with a higher
priority to read values changed by this transaction, as long as they didn’t need to
overwrite the changes. READ UNCOMMITTED transactions can only be in the READ
ONLY access mode, so they can read from any timestamp.
Exercise 17.10 Consider the tree shown in Figure 19.5. Describe the steps involved
in executing each of the following operations according to the tree-index concurrency
control algorithm discussed in Section 19.3.2, in terms of the order in which nodes are
locked, unlocked, read, and written. Be speciﬁc about the kind of lock obtained and
answer each part independently of the others, always starting with the tree shown in
Figure 19.5.
1. Search for data entry 40*.
2. Search for all data entries k∗with k ≤40.
3. Insert data entry 62*.
4. Insert data entry 40*.
5. Insert data entries 62* and 75*.
Answer 17.10 Answer omitted.
Exercise 17.11 Consider a database organized in terms of the following hierarchy of
objects: The database itself is an object (D), and it contains two ﬁles (F1 and F2),
each of which contains 1000 pages (P1 . . . P1000 and P1001 . . .P2000, respectively).
Each page contains 100 records, and records are identiﬁed as p : i, where p is the page
identiﬁer and i is the slot of the record on that page.
Multiple-granularity locking is used, with S, X, IS, IX and SIX locks, and database-
level, ﬁle-level, page-level and record-level locking. For each of the following operations,
indicate the sequence of lock requests that must be generated by a transaction that
wants to carry out (just) these operations:
1. Read record P1200 : 5.
2. Read records P1200 : 98 through P1205 : 2.

178
Chapter 17
3. Read all (records on all) pages in ﬁle F1.
4. Read pages P500 through P520.
5. Read pages P10 through P980.
6. Read all pages in F1 and (based on the values read) modify 10 pages.
7. Delete record P1200 : 98. (This is a blind write.)
8. Delete the ﬁrst record from each page. (Again, these are blind writes.)
9. Delete all records.
Answer 17.11 The answer to each question is given below.
1. IS on D; IS on F2; IS on P1200; S on P1200:5.
2. IS on D; IS on F2; IS on P1200, S on 1201 through 1204, IS on P1205; S on
P1200:98/99/100, S on P1205:1/2.
3. IS on D; S on F1
4. IS on D; IS on F1; S on P500 through P520.
5. IS on D; S on F1 (performance hit of locking 970 pages is likely to be higher than
other blocked transactions).
6. IS and IX on D; SIX on F1.
7. IX on D; IX on F2; X on P1200.
(Locking the whole page is not necessary, but it would require some reorganization
or compaction.)
8. IX on D; X on F1 and F2.
(There are many ways to do this, there is a tradeoﬀbetween overhead and con-
currency.)
9. IX on D; X on F1 and F2.
Exercise 17.12 Suppose that we have only two types of transactions, T 1 and T 2.
Transactions preserve database consistency when run individually. We have deﬁned
several integrity constraints such that the DBMS never executes any SQL statement
that brings the database into an inconsistent state. Assume that the DBMS does not
perform any concurrency control. Give an example schedule of two transactions T 1
and T 2 that satisﬁes all these conditions, yet produces a database instance that is not
the result of any serial execution of T 1 and T 2.
Answer 17.12 Answer omitted.

18
CRASH RECOVERY
Exercise 18.1 Brieﬂy answer the following questions:
1. How does the recovery manager ensure atomicity of transactions? How does it
ensure durability?
2. What is the diﬀerence between stable storage and disk?
3. What is the diﬀerence between a system crash and a media failure?
4. Explain the WAL protocol.
5. Describe the steal and no-force policies.
Answer 18.1 The answer to each question is given below.
1. The Recovery Manager ensures atomicity of transactions by undoing the actions
of transactions that do not commit. It ensures durability by making sure that all
actions of committed transactions survive system crashes and media failures.
2. Stable storage is guaranteed (with very high probability) to survive crashes and
media failures. A disk might get corrupted or fail but the stable storage is still
expected to retain whatever is stored in it. One of the ways of achieving stable
storage is to store the information in a set of disks rather than in a single disk
with some information duplicated so that the information is available even if one
or two of the disks fail.
3. A system crash happens when the system stops functioning in a normal way
or stops altogether. The Recovery Manager and other parts of the DBMS stop
functioning (e.g. a core dump caused by a bus error) as opposed to media failure.
In a media failure, the system is up and running but a particular entity of the
system is not functioning. In this case, the Recovery Manager is still functioning
and can start recovering from the failure while the system is still running (e.g., a
disk is corrupted).
179

180
Chapter 18
4. WAL Protocol: Whenever a change is made to a database object, the change is
ﬁrst recorded in the log and the log is written to stable storage before the change
is written to disk.
5. If a steal policy is in eﬀect, the changes made to an object in the buﬀer pool by a
transaction can be written to disk before the transaction commits. This might be
because some other transaction might ”steal” the buﬀer page presently occupied
by an uncommitted transaction.
A no-force policy is in eﬀect if, when a transaction commits, we need not ensure
that all the changes it has made to objects in the buﬀer pool are immediately
forced to disk.
Exercise 18.2 Brieﬂy answer the following questions:
1. What are the properties required of LSNs?
2. What are the ﬁelds in an update log record? Explain the use of each ﬁeld.
3. What are redoable log records?
4. What are the diﬀerences between update log records and CLRs?
Answer 18.2 Answer omitted.
Exercise 18.3 Brieﬂy answer the following questions:
1. What are the roles of the Analysis, Redo, and Undo phases in ARIES?
2. Consider the execution shown in Figure 18.1.
(a) What is done during Analysis? (Be precise about the points at which Analysis
begins and ends and describe the contents of any tables constructed in this
phase.)
(b) What is done during Redo? (Be precise about the points at which Redo
begins and ends.)
(c) What is done during Undo? (Be precise about the points at which Undo
begins and ends.)
Answer 18.3 The answer to each question is given below.
1. The Analysis phase starts with the most recent begin checkpoint record and pro-
ceeds forward in the log until the last log record. It determines
(a) The point in the log at which to start the Redo pass

Crash Recovery
181
10
20
30
40
50
60
00
end_checkpoint
begin_checkpoint
LOG 
LSN
update: T1 writes P5
update: T2 writes P3
update: T3 writes P3
CRASH, RESTART
T2 end
T2 commit
T1 abort
70
Figure 18.1
Execution with a Crash
(b) The dirty pages in the buﬀer pool at the time of the crash.
(c) Transactions that were active at the time of the crash which need to be
undone.
The Redo phase follows Analysis and redoes all changes to any page that might
have been dirty at the time of the crash. The Undo phase follows Redo and undoes
the changes of all transactions that were active at the time of the crash.
2. (a) For this example, we will assume that the Dirty Page Table and Transaction
Table were empty before the start of the log. Analysis determines that the last
begin checkpoint was at LSN 00 and starts at the corresponding end checkpoint
(LSN 10).
We will denote Transaction Table records as (transID, lastLSN) and Dirty Page
Table records as (pageID, recLSN) sets.
Then Analysis phase runs until LSN 70, and does the following:
LSN 20
Adds (T1, 20) to TT and (P5, 20) to DPT
LSN 30
Adds (T2, 30) to TT and (P3, 30) to DPT
LSN 40
Changes status of T2 to ”C” from ”U”
LSN 50
Deletes entry for T2 from Transaction Table
LSN 60
Adds (T3, 60) to TT. Does not change P3 entry in DPT
LSN 70
Changes (T1, 20) to (T1, 70)
The ﬁnal Transaction Table has two entries: (T1, 70), and (T3, 60). The ﬁnal
Dirty Page Table has two entries: (P5, 20), and (P3, 30).
(b) Redo Phase: Redo starts at LSN 20 (smallest recLSN in DPT).

182
Chapter 18
10
20
30
40
50
60
LSN
LOG 
00
update: T1 writes P1
update: T3 writes P3
70
update: T1 writes P2
update: T2 writes P3
update: T2 writes P5
update: T2 writes P5
T2 abort
T3 commit
Figure 18.2
Aborting a Transaction
LSN 20
Changes to P5 are redone.
LSN 30
P3 is retrieved and its pageLSN is checked. If the page had been
written to disk before the crash (i.e. if pageLSN >= 30), nothing
is re-done otherwise the changes are re-done.
LSN 40,50
No action
LSN 60
Changes to P3 are redone
LSN 70
No action
(c) Undo Phase: Undo starts at LSN 70 (highest lastLSN in TT). The Loser Set
consists of LSNs 70 and 60. LSN 70: Adds LSN 20 to the Loser Set. Loser Set
= (60, 20). LSN 60: Undoes the change on P3 and adds a CLR indicating this
Undo. Loser Set = (20). LSN 20: Undoes the change on P5 and adds a CLR
indicating this Undo.
Exercise 18.4 Consider the execution shown in Figure 18.2.
1. Extend the ﬁgure to show prevLSN and undonextLSN values.
2. Describe the actions taken to rollback transaction T 2.
3. Show the log after T 2 is rolled back, including all prevLSN and undonextLSN
values in log records.
Answer 18.4 Answer omitted.
Exercise 18.5 Consider the execution shown in Figure 18.3. In addition, the system

Crash Recovery
183
10
20
30
40
50
60
LSN
LOG 
00
70
CRASH, RESTART
T3 abort
update: T1 writes P5
T2 end
update: T3 writes P2
T2 commit
update: T3 writes P3
update: T2 writes P2
update: T1 writes P1
begin_checkpoint
end_checkpoint
80
90
Figure 18.3
Execution with Multiple Crashes
crashes during recovery after writing two log records to stable storage and again after
writing another two log records.
1. What is the value of the LSN stored in the master log record?
2. What is done during Analysis?
3. What is done during Redo?
4. What is done during Undo?
5. Show the log when recovery is complete, including all non-null prevLSN and un-
donextLSN values in log records.
Answer 18.5 The answer to each question is given below.
1. LSN 00 is stored in the master log record as it is the LSN of the begin checkpoint
record.
2. During analysis the following happens:

184
Chapter 18
LSN 20
Add (T1,20) to TT and (P1,20) to DPT
LSN 30
Add (T2,30) to TT and (P2,30) to DPT
LSN 40
Add (T3,40) to TT and (P3,40) to DPT
LSN 50
Change status of T2 to C
LSN 60
Change (T3,40) to (T3,60)
LSN 70
Remove T2 from TT
LSN 80
Change (T1,20) to (T1,70) and add (P5,70) to DPT
LSN 90
No action
At the end of analysis, the transaction table contains the following entries: (T1,80),
and (T3,60). The Dirty Page Table has the following entries: (P1,20), (P2,30),
(P3,40), and (P5,80).
3. Redo starts from LSN20 (minimum recLSN in DPT).
LSN 20
Check whether P1 has pageLSN more than 10 or not. Since it is a
committed transaction, we probably need not redo this update.
LSN 30
Redo the change in P2
LSN 40
Redo the change in P3
LSN 50
No action
LSN 60
Redo the changes on P2
LSN 70
No action
LSN 80
Redo the changes on P5
LSN 90
No action
4. ToUndo consists of (80, 60).
LSN 80
Undo the changes in P5. Append a CLR: Undo T1 LSN 80, set
undonextLSN = 20. Add 20 to ToUndo.
ToUndo consists of (60, 20).
LSN 60
Undo the changes on P2. Append a CLR: Undo T3 LSN 60, set
undonextLSN = 40. Add 40 to ToUndo.
ToUndo consists of (40, 20).
LSN 40
Undo the changes on P3. Append a CLR: Undo T3 LSN 40, T3
end
ToUndo consists of (20).
LSN 20
Undo the changes on P1. Append a CLR: Undo T1 LSN 20, T1
end

Crash Recovery
185
5. The log looks like the following after recovery:
LSN 00
begin checkpoint
LSN 10
end checkpoint
LSN 20
update: T1 writes P1
LSN 30
update: T2 writes P2
LSN 40
update: T3 writes P3
LSN 50
T2 commit
prevLSN = 30
LSN 60
update: T3 writes P2
prevLSN = 40
LSN 70
T2 end
prevLSN = 50
LSN 80
update: T1 writes P5
prevLSN = 20
LSN 90
T3 abort
prevLSN = 60
LSN 100
CLR: Undo T1 LSN 80
undonextLSN= 20
LSN 110
CLR: Undo T3 LSN 60
undonextLSN= 40
LSN 120,125
CLR: Undo T3 LSN 40
T3 end.
LSN 130,135
CLR: Undo T1 LSN 20
T1 end.
Exercise 18.6 Brieﬂy answer the following questions:
1. How is checkpointing done in ARIES?
2. Checkpointing can also be done as follows: Quiesce the system so that only check-
pointing activity can be in progress, write out copies of all dirty pages, and include
the dirty page table and transaction table in the checkpoint record. What are the
pros and cons of this approach versus the checkpointing approach of ARIES?
3. What happens if a second begin checkpoint record is encountered during the Anal-
ysis phase?
4. Can a second end checkpoint record be encountered during the Analysis phase?
5. Why is the use of CLRs important for the use of undo actions that are not the
physical inverse of the original update?
6. Give an example that illustrates how the paradigm of repeating history and the
use of CLRs allow ARIES to support locks of ﬁner granularity than a page.
Answer 18.6 Answer omitted.
Exercise 18.7 Brieﬂy answer the following questions:
1. If the system fails repeatedly during recovery, what is the maximum number of
log records that can be written (as a function of the number of update and other
log records written before the crash) before restart completes successfully?

186
Chapter 18
2. What is the oldest log record we need to retain?
3. If a bounded amount of stable storage is used for the log, how can we always
ensure enough stable storage to hold all log records written during restart?
Answer 18.7 The answer to each question is given below.
1. Let us take the case where each log record is an update record of an uncommitted
transaction and each record belongs to a diﬀerent transaction. This means there
are n records of n diﬀerent transactions, each of which has to be undone. During
recovery, we have to add a CLR record of the undone action and an end transaction
record after each CLR. Thus, we can write a maximum of 2n log records before
restart completes.
2. The oldest begin checkpoint referenced in any fuzzy dump or master log record.
3. One needs to ensure that there is enough to hold twice as many records as the
current number of log records. If necessary, do a fuzzy dump to free up some log
records whenever the number of log records goes above one third of the available
space.
Exercise 18.8 Consider the three conditions under which a redo is unnecessary (Sec-
tion 18.6.2).
1. Why is it cheaper to test the ﬁrst two conditions?
2. Describe an execution that illustrates the use of the ﬁrst condition.
3. Describe an execution that illustrates the use of the second condition.
Answer 18.8 Answer omitted.
Exercise 18.9 The description in Section 18.6.1 of the Analysis phase made the sim-
plifying assumption that no log records appeared between the begin checkpoint and
end checkpoint records for the most recent complete checkpoint. The following ques-
tions explore how such records should be handled.
1. Explain why log records could be written between the begin checkpoint and end checkpoint
records.
2. Describe how the Analysis phase could be modiﬁed to handle such records.
3. Consider the execution shown in Figure 18.4. Show the contents of the end checkpoint
record.
4. Illustrate your modiﬁed Analysis phase on the execution shown in Figure 18.4.

Crash Recovery
187
10
20
30
40
LSN
LOG 
00
begin_checkpoint
update: T1 writes P1
update: T2 writes P2
50
60
70
80
T1 commit
CRASH, RESTART
T3 commit
end_checkpoint
update: T3 writes P3
T2 abort
T1 end
Figure 18.4
Log Records between Checkpoint Records
Answer 18.9 The answer to each question is given below.
1. In ARIES, ﬁrst a begin checkpoint record is written and then, after some time,
an end checkpoint record is written. While the end checkpoint record is being
constructed, the DBMS continues executing transactions and writing other log
records.
So, we could have log records between the begin checkpoint and the
end checkpoint records. The only guarantee we have is that the transaction table
and the dirty page table are accurate as of the time of the begin checkpoint record.
2. The Analysis phase begins by examining the most recent begin checkpoint log
record and then searches for the next end checkpoint record.
Then the Dirty
Page Table and the Transaction Table are initialized to the copies of those struc-
tures in the end checkpoint. Our new Analysis phase remains the same untill
here. In the old algorithm, Analysis scans the log in the forward direction until it
reaches the end of the log. In the modiﬁed algorithm, Analysis goes back to the
begin checkpoint and scans the log in the forward direction.
3. The end checkpoint record contains the transaction table and the dirty page table
as of the time of the begin checkpoint (LSN 00 in this case). Since we are assuming
these tables to be empty before LSN 00, the end checkpoint record will indicate
an empty transaction table and an empty dirty page table.
4. Instead of starting from LSN 80, Analysis goes back to LSN 10 and executes as
follows:

188
Chapter 18
LSN 10
Add (T1,10) to TT and (P1, 10) to DPT
LSN 20
Change status of T1 from U to C.
LSN 30
Add (T2,30) to TT and (P2, 30) to DPT
LSN 40
Remove (T1,10) from TT
LSN 50
No action
LSN 60
Add (T3,60) to TT and (P3, 60) to DPT
LSN 70
No action
LSN 80
Change status of T3 from U to C.
Exercise 18.10 Answer the following questions brieﬂy:
1. Explain how media recovery is handled in ARIES.
2. What are the pros and cons of using fuzzy dumps for media recovery?
3. What are the similarities and diﬀerences between checkpoints and fuzzy dumps?
4. Contrast ARIES with other WAL-based recovery schemes.
5. Contrast ARIES with shadow-page-based recovery.
Answer 18.10 Answer omitted.

19
SCHEMA REFINEMENT AND
NORMAL FORMS
Exercise 19.1 Brieﬂy answer the following questions:
1. Deﬁne the term functional dependency.
2. Why are some functional dependencies called trivial?
3. Give a set of FDs for the relation schema R(A,B,C,D) with primary key AB under
which R is in 1NF but not in 2NF.
4. Give a set of FDs for the relation schema R(A,B,C,D) with primary key AB under
which R is in 2NF but not in 3NF.
5. Consider the relation schema R(A,B,C), which has the FD B →C. If A is a can-
didate key for R, is it possible for R to be in BCNF? If so, under what conditions?
If not, explain why not.
6. Suppose we have a relation schema R(A,B,C) representing a relationship between
two entity sets with keys A and B, respectively, and suppose that R has (among
others) the FDs A →B and B →A. Explain what such a pair of dependencies
means (i.e., what they imply about the relationship that the relation models).
Answer 19.1
1. Let R be a relational schema and let X and Y be two subsets of the set of all
attributes of R. We say Y is functionally dependent on X, written X →Y, if the
Y-values are determined by the X-values. More precisely, for any two tuples r1
and r2 in (any instance of) R
πX(r1) = πX(r2)
⇒
πY (r1) = πY (r2)
2. Some functional dependencies are considered trivial because they contain super-
ﬂuous attributes that do not need to be listed. Consider the FD: A →AB. By
reﬂexivity, A always implies A, so that the A on the right hand side is not neces-
sary and can be dropped. The proper form, without the trivial dependency would
then be A →B.
189

190
Chapter 19
3. Consider the set of FD: AB →CD and B →C. AB is obviously a key for this
relation since AB →CD implies AB →ABCD. It is a primary key since there are
no smaller subsets of keys that hold over R(A,B,C,D). The FD: B →C violates
2NF since:
C ∈B is false; that is, it is not a trivial FD
B is not a superkey
C is not part of some key for R
B is a proper subset of the key AB (transitive dependency)
4. Consider the set of FD: AB →CD and C →D. AB is obviously a key for this
relation since AB →CD implies AB →ABCD. It is a primary key since there are
no smaller subsets of keys that hold over R(A,B,C,D). The FD: C →D violates
3NF but not 2NF since:
D ∈C is false; that is, it is not a trivial FD
C is not a superkey
D is not part of some key for R
5. The only way R could be in BCNF is if B includes a key, i.e. B is a key for R.
6. It means that the relationship is one to one. That is, each A entity corresponds
to at most one B entity and vice-versa. (In addition, we have the dependency AB
→C, from the semantics of a relationship set.)
Exercise 19.2 Consider a relation R with ﬁve attributes ABCDE. You are given the
following dependencies: A →B, BC →E, and ED →A.
1. List all keys for R.
2. Is R in 3NF?
3. Is R in BCNF?
Answer 19.2 Answer omitted.
Exercise 19.3 Consider the relation shown in Figure 19.1.
1. List all the functional dependencies that this relation instance satisﬁes.
2. Assume that the value of attribute Z of the last record in the relation is changed
from z3 to z2. Now list all the functional dependencies that this relation instance
satisﬁes.

Schema Reﬁnement and Normal Forms
191
X
Y
Z
x1
y1
z1
x1
y1
z2
x2
y1
z1
x2
y1
z3
Figure 19.1
Relation for Exercise 19.3.
Answer 19.3
1. The following functional dependencies hold over R: Z →Y, X →Y, and XZ →Y
2. Same as part 1. Functional dependency set is unchanged.
Exercise 19.4 Assume that you are given a relation with attributes ABCD.
1. Assume that no record has NULL values. Write an SQL query that checks whether
the functional dependency A →B holds.
2. Assume again that no record has NULL values.
Write an SQL assertion that
enforces the functional dependency A →B.
3. Let us now assume that records could have NULL values. Repeat the previous
two questions under this assumption.
Answer 19.4 Answer omitted.
Exercise 19.5 Consider the following collection of relations and dependencies. As-
sume that each relation is obtained through decomposition from a relation with at-
tributes ABCDEFGHI and that all the known dependencies over relation ABCDEFGHI
are listed for each question. (The questions are independent of each other, obviously,
since the given dependencies over ABCDEFGHI are diﬀerent.) For each (sub)relation:
(a) State the strongest normal form that the relation is in. (b) If it is not in BCNF,
decompose it into a collection of BCNF relations.
1. R1(A,C,B,D,E), A →B, C →D
2. R2(A,B,F), AC →E, B →F
3. R3(A,D), D →G, G →H
4. R4(D,C,H,G), A →I, I →A

192
Chapter 19
5. R5(A,I,C,E)
Answer 19.5
1. 1NF. BCNF decomposition: AB, CD, ACE.
2. 1NF. BCNF decomposition: AB, BF
3. BCNF.
4. BCNF.
5. BCNF.
Exercise 19.6 Suppose that we have the following three tuples in a legal instance of
a relation schema S with three attributes ABC (listed in order): (1,2,3), (4,2,3), and
(5,3,3).
1. Which of the following dependencies can you infer does not hold over schema S?
(a) A →B, (b) BC →A, (c) B →C
2. Can you identify any dependencies that hold over S?
Answer 19.6 Answer omitted.
Exercise 19.7 Suppose you are given a relation R with four attributes ABCD. For
each of the following sets of FDs, assuming those are the only dependencies that hold
for R, do the following: (a) Identify the candidate key(s) for R. (b) Identify the best
normal form that R satisﬁes (1NF, 2NF, 3NF, or BCNF). (c) If R is not in BCNF,
decompose it into a set of BCNF relations that preserve the dependencies.
1. C →D, C →A, B →C
2. B →C, D →A
3. ABC →D, D →A
4. A →B, BC →D, A →C
5. AB →C, AB →D, C →A, D →B
Answer 19.7
1.
(a) Candidate keys: B
(b) R is in 2NF but not 3NF.

Schema Reﬁnement and Normal Forms
193
(c) C →D and C →A both cause violations of BCNF. One way to obtain a
(lossless) join preserving decomposition is to decompose R into AC, BC, and
CD.
2.
(a) Candidate keys: BD
(b) R is in 1NF but not 2NF.
(c) Both B →C and D →A cause BCNF violations. The decomposition: AD,
BC, BD (obtained by ﬁrst decomposing to AD, BCD) is BCNF and lossless
and join-preserving.
3.
(a) Candidate keys: ABC, BCD
(b) R is in 3NF but not BCNF.
(c) ABCD is not in BCNF since D →A and D is not a key. However if we split
up R as AD, BCD we cannot preserve the dependency ABC →D. So there
is no BCNF decomposition.
4.
(a) Candidate keys: A
(b) R is in 2NF but not 3NF (because of the FD: BC →D).
(c) BC →D violates BCNF since BC does not contain a key. So we split up R
as in: BCD, ABC.
5.
(a) Candidate keys: AB, BC, CD, AD
(b) R is in 3NF but not BCNF (because of the FD: C →A).
(c) C →A and D →B both cause violations. So decompose into: AC, BCD
but this does not preserve AB →C and AB →D, and BCD is still not
BCNF because D →B. So we need to decompose further into: AC, BD,
CD. However, when we attempt to revive the lost functioanl dependencies
by adding ABC and ABD, we that these relations are not in BCNF form.
Therefore, there is no BCNF decomposition.
Exercise 19.8 Consider the attribute set R = ABCDEGH and the FD set F = {AB →
C, AC →B, AD →E, B →D, BC →A, E →G}.
1. For each of the following attribute sets, do the following: (i) Compute the set of
dependencies that hold over the set and write down a minimal cover. (ii) Name
the strongest normal form that is not violated by the relation containing these
attributes. (iii) Decompose it into a collection of BCNF relations if it is not in
BCNF.
(a) ABC, (b) ABCD, (c) ABCEG, (d) DCEGH, (e) ACEH
2. Which of the following decompositions of R = ABCDEG, with the same set of
dependencies F, is (a) dependency-preserving? (b) lossless-join?

194
Chapter 19
(a) {AB, BC, ABDE, EG }
(b) {ABC, ACDE, ADG }
Answer 19.8 Answer omitted.
Exercise 19.9 Let R be decomposed into R1, R2, . . ., Rn. Let F be a set of FDs on
R.
1. Deﬁne what it means for F to be preserved in the set of decomposed relations.
2. Describe a polynomial-time algorithm to test dependency-preservation.
3. Projecting the FDs stated over a set of attributes X onto a subset of attributes
Y requires that we consider the closure of the FDs.
Give an example where
considering the closure is important in testing dependency-preservation, that is,
considering just the given FDs gives incorrect results.
Answer 19.9
1. Let Fi denote the projection of F on Ri. F is preserved if the closure of the (union
of) the Fi’s equals F (note that F is always a superset of this closure.)
2. We shall describe an algorithm for testing dependency preservation which is poly-
nomial in the cardinality of F. For each dependency X →Y ∈F check if it is in F
as follows: start with the set S (of attributes in) X. For each relation Ri, compute
the closure of S ∩Ri relative to F and project this closure to the attributes of Ri.
If this results in additional attributes, add them to S. Do this repeatedly until
there is no change to S.
3. There is an example in the text in Section 19.5.2.
Exercise 19.10 Suppose you are given a relation R(A,B,C,D). For each of the fol-
lowing sets of FDs, assuming they are the only dependencies that hold for R, do the
following: (a) Identify the candidate key(s) for R. (b) State whether or not the pro-
posed decomposition of R into smaller relations is a good decomposition and brieﬂy
explain why or why not.
1. B →C, D →A; decompose into BC and AD.
2. AB →C, C →A, C →D; decompose into ACD and BC.
3. A →BC, C →AD; decompose into ABC and AD.
4. A →B, B →C, C →D; decompose into AB and ACD.
5. A →B, B →C, C →D; decompose into AB, AD and CD.

Schema Reﬁnement and Normal Forms
195
Answer 19.10 Answer omitted.
Exercise 19.11 Consider a relation R that has three attributes ABC. It is decom-
posed into relations R1 with attributes AB and R2 with attributes BC.
1. State the deﬁnition of a lossless-join decomposition with respect to this example.
Answer this question concisely by writing a relational algebra equation involving
R, R1, and R2.
2. Suppose that B →C. Is the decomposition of R into R1 and R2 lossless-join?
Reconcile your answer with the observation that neither of the FDs R1 ∩R2 →
R1 nor R1 ∩R2 →R2 hold, in light of the simple test oﬀering a necessary and
suﬃcient condition for lossless-join decomposition into two relations in Section
15.6.1.
3. If you are given the following instances of R1 and R2, what can you say about the
instance of R from which these were obtained? Answer this question by listing
tuples that are deﬁnitely in R and tuples that are possibly in R.
Instance of R1 = {(5,1), (6,1)}
Instance of R2 = {(1,8), (1,9)}
Can you say that attribute B deﬁnitely is or is not a key for R?
Answer 19.11
1. The decomposition of R into R1 and R2 is lossless if and only if:
R1 ▷◁R1.B=R2.B R2
=
R
Note that this is a statement about relation schemas, not some speciﬁc instances
of them.
2. Answer Omitted.
3. All we can say is that the instance of R from which the given instances of R1 and
R2 were obtained, must be a subset of the set of ABC tuples: {(5,1,8), (5,1,9),
(6,1,8), (6,1,9)} which is also, at the same time, a superset of {(5,1,8), (6,1,9)} or
a superset of {(5,1,9), (6,1,8)}. In particular, R contains at least two tuples but
no more than 4. This also implies the attribute B is not a key for R (because R
has at least 2 distinct tuples but each tuple in R has the same B value.)
Exercise 19.12 Suppose that we have the following four tuples in a relation S with
three attributes ABC: (1,2,3), (4,2,3), (5,3,3), (5,3,4). Which of the following functional
(→) and multivalued (→→) dependencies can you infer does not hold over relation S?

196
Chapter 19
1. A →B
2. A →→B
3. BC →A
4. BC →→A
5. B →C
6. B →→C
Answer 19.12 Answer omitted.
Exercise 19.13 Consider a relation R with ﬁve attributes ABCDE.
1. For each of the following instances of R, state whether it violates (a) the FD BC
→D and (b) the MVD BC →→D:
(a) { } (i.e., empty relation)
(b) {(a,2,3,4,5), (2,a,3,5,5)}
(c) {(a,2,3,4,5), (2,a,3,5,5), (a,2,3,4,6)}
(d) {(a,2,3,4,5), (2,a,3,4,5), (a,2,3,6,5)}
(e) {(a,2,3,4,5), (2,a,3,7,5), (a,2,3,4,6)}
(f) {(a,2,3,4,5), (2,a,3,4,5), (a,2,3,6,5), (a,2,3,6,6)}
(g) {(a,2,3,4,5), (a,2,3,6,5), (a,2,3,6,6), (a,2,3,4,6)}
2. If each instance for R listed above is legal, what can you say about the FD A →
B?
Answer 19.13
1. Note: The answer sometimes depends on the value of a. Unless otherwise men-
tioned, the answer applies to all values of a.
(a) { } (i.e., empty relation):
does not violate either dependency.
(b) {(a,2,3,4,5), (2,a,3,5,5)}:
If a = 2, then BC →D is violated (otherwise it is not).
BC →→D is not violated (for any value of a)
(c) {(a,2,3,4,5), (2,a,3,5,5), (a,2,3,4,6)}:
BC →D is violated if a = 2 (otherwise not).
If a = 2 then BC →→D is violated (consider the tuples (2,a,3,5,5) and
(a,2,3,4,6); if a equals 2 must also have (2,a,3,5,6)
)

Schema Reﬁnement and Normal Forms
197
(d) {(a,2,3,4,5), (2,a,3,4,5), (a,2,3,6,5)}:
BC →D is violated (consider the ﬁrst and the third tuples (a,2,3,4,5) and
(a,2,3,6,5) ).
BC →→D is not violated.
(e) {(a,2,3,4,5), (2,a,3,7,5), (a,2,3,4,6)}:
If a = 2, then BC →D is violated (otherwise it is not).
If a = 2, then BC →→D is violated (otherwise it is not). To prove this look
at the last two tuples; there must also be a tuple (2,a,3,7,6) for BC →→to
hold.
(f) {(a,2,3,4,5), (2,a,3,4,5), (a,2,3,6,5), (a,2,3,6,6)}:
BC →D does not hold. (Consider the ﬁrst and the third tuple).
BC →→C is violated. Consider the 1st and the 4th tuple. For this depen-
dency to hold there should be a tuple (a,2,3,4,6).
(g) {(a,2,3,4,5), (a,2,3,6,5), (a,2,3,6,6), (a,2,3,4,6)}:
BC →D does not hold. (Consider the ﬁrst and the third tuple).
BC →→C is not violated.
2. We can not say anything about the functional dependency A →B.
Exercise 19.14 JDs are motivated by the fact that sometimes a relation that cannot
be decomposed into two smaller relations in a lossless-join manner can be so decom-
posed into three or more relations. An example is a relation with attributes supplier,
part, and project, denoted SPJ, with no FDs or MVDs. The JD ▷◁{SP, PJ, JS}
holds.
From the JD, the set of relation schemes SP, PJ, and JS is a lossless-join decomposition
of SPJ. Construct an instance of SPJ to illustrate that no two of these schemes suﬃce.
Answer 19.14 Answer omitted.
Exercise 19.15 Answer the following questions
1. Prove that the algorithm shown in Figure 19.4 correctly computes the attribute
closure of the input attribute set X.
2. Describe a linear-time (in the size of the set of FDs, where the size of each FD is
the number of attributes involved) algorithm for ﬁnding the attribute closure of a
set of attributes with respect to a set of FDs. Prove that your algorithm correctly
computes the attribute closure of the input attribute set.
Answer 19.15 The answer to each question is given below.

198
Chapter 19
1. Proof Omitted.
2. Recall that the attribute closure of (attribute) X relative to a set of FD’s Σ is the
set of attributes A such that Σ satisﬁes X →A.
// Initialize
X+
:=
X;
FdSet :=
Σ;
do
{
for each FD Y →Z in FdSet such that X+ ⊇Y
{
X+ :=
X+ union Z;
Remove Y →Z from FdSet;
}
}
until ( X+ does not change) ;
Let n = | Σ | denote the cardinality of Σ. Then the loop repeats at most n times
since for each iteration we either permanently remove a functional dependency
from the set Σ, or stop all together. With the proper choice of data structures it
can be show that this algorithm is linear in the size of Σ.
As for correctness, we claim that this algorithm is equivalent to the standard
attribute closure algorithm. If we throw away a functional dependency Y →Z at
a given step, then it must be the case that X →Y since Y ∈X+ at that step,
therefore by transitivity X →Z. Since Z was added to X+ we no longer need
the functional dependency Y →Z for ﬁnding the attribute closure of X, since it
is implied by X →X+.
The rest of the proof of correctness follows from part 1 of this exercise.
Exercise 19.16 Let us say that an FD X →Y is simple if Y is a single attribute.
1. Replace the FD AB →CD by the smallest equivalent collection of simple FDs.
2. Prove that every FD X →Y in a set of FDs F can be replaced by a set of simple
FDs such that F + is equal to the closure of the new set of FDs.
Answer 19.16 Answer omitted.
Exercise 19.17 Prove that Armstrong’s Axioms are sound and complete for FD in-
ference. That is, show that repeated application of these axioms on a set F of FDs
produces exactly the dependencies in F +.

Schema Reﬁnement and Normal Forms
199
Answer 19.17 Answer omitted.
Exercise 19.18 Consider a relation R with attributes ABCDE. Let the following FDs
be given: A →BC, BC →E, and E →DA. Similarly, let S be a relation with attributes
ABCDE and let the following FDs be given: A →BC, B →E, and E →DA. (Only
the second dependency diﬀers from those that hold over R.) You do not know whether
or which other (join) dependencies hold.
1. Is R in BCNF?
2. Is R in 4NF?
3. Is R in 5NF?
4. Is S in BCNF?
5. Is S in 4NF?
6. Is S in 5NF?
Answer 19.18 Answer omitted.
Exercise 19.19 Let R be a relation schema with a set F of FDs.
Prove that the
decomposition of R into R1 and R2 is lossless-join if and only if F + contains R1 ∩
R2 →R1 or R1 ∩R2 →R2.
Answer 19.19
For both directions (if and only-if) we use the notation
C = R1∩R2,
X = R1−C,
Y = R2−C, so thatR1 = XC,
R2 = CY , and R = XCY .
(⇐): For this direction, assume we are given the dependency C →X. (The other case
C →Y is similar.)
So let r be an instance of schema R and let (x1, c, y1) and (x2, c, y2) be two tuples in r.
The FD, C →X implies that x1 = x2. Thus, (x1, c, y2) is the same as (x2, c, y2) and
(x2, c, y1) is the same as (x1, c, y1), so that both these tuples (x1, c, y2) and (x2, c, y1)
are in r. Thus r satisﬁes the JD: R = R1 ▷◁R2. Since r is an arbitrary instance, we
have proved that the decomposition is lossless.
(⇒): Now for the other direction, assume that neither C →X nor C →Y holds. We
shall prove that the join is lossy by exhibiting a relation instance that violates the JD:
R1 ▷◁R2. Actually we will prove a slightly more general result. Suppose we are given
some set of FD’s Σ, such that R has a lossless join w.r.t. Σ. This means that for any
instance r satisfying Σ, we have
r = r1 ▷◁r2 where r1 = πR1(r), r2 = πR2(r).

200
Chapter 19
Then we prove that
{C →X, C →Y } ∩Σ+ ̸= ∅.
The proof is by contradiction. Assume that the intersection { C →X,C →Y } ∩
Σ+ is empty. Suppose r1 is an instance of the schema that does not satisfy the FD:
C →X and r2 is an instance that does not satisfy the FD: C →Y . Choose c such
that there are tuples (x1, c, y1), (x2, c, y2) ∈r1 for which x1 ̸= x2 and c′ such that
there are tuples (x′
1, c′, y′
1), (x′
2, c′, y′
2) ∈r2 for which y1′ ̸= y2′.
Use selection to replace r1 by πR.C=c(r1) and r2 by πR.C=c′(r2). Since r1 and r2 are
ﬁnite and the domain sets are inﬁnite, we can assume without loss of generality (by
modifying some of the values of the tuples in r1 and r2, if necessary) so that
c
=
c′
πA(r1) ∩πA(r2)
=
∅
for each attribute A ∈X
πB(r1) ∩πB(r2)
=
∅
for each attribute B ∈Y.
Now consider the relation r1 ∪r2. This is an instance of the schema R that satisﬁes Σ.
However, (x1, c, y′
1) ̸∈r1 ∪r2, so the instance r1 ∪r2 does not satisfy the JD: R1 ▷◁R2.
Exercise 19.20 Consider a scheme R with FDs F that is decomposed into schemes
with attributes X and Y. Show that this is dependency-preserving if F ⊆(FX ∪FY )+.
Answer 19.20 Answer omitted.
Exercise 19.21 Prove that the optimization of the algorithm for lossless-join, dependency-
preserving decomposition into 3NF relations (Section 19.6.2) is correct.
Answer 19.21 Answer Omitted.
Exercise 19.22 Prove that the 3NF synthesis algorithm produces a lossless-join de-
composition of the relation containing all the original attributes.
Answer 19.22 Answer omitted.
Exercise 19.23 Prove that an MVD X →→Y over a relation R can be expressed as
the join dependency
▷◁{XY, X(R −Y )}.
Answer 19.23
Write Z
= R −Y . Thus, R
=
Y XZ. X →→Y says that if
(y1, x, z1), (y2, x, z2) ∈R then (y1, x, z2), (y2, x, z1) also ∈R. But this is precisely the
same as saying R =
▷◁{ XY, X(R −Y ) }.

Schema Reﬁnement and Normal Forms
201
Exercise 19.24 Prove that, if R has only one key, it is in BCNF if and only if it is in
3NF.
Answer 19.24 Answer omitted.
Exercise 19.25 Prove that, if R is in 3NF and every key is simple, then R is in BCNF.
Answer 19.25 Since every key is simple, then we know that for any FD that satisﬁes
X →A, where A is part of some key implies that A is a key. By the deﬁnition of an
FD, if X is known, then A is known. This means that if X is known, we know a key
for the relation, so X must be a superkey. This satisﬁes all of the properties of BCNF.
Exercise 19.26 Prove these statements:
1. If a relation scheme is in BCNF and at least one of its keys consists of a single
attribute, it is also in 4NF.
2. If a relation scheme is in 3NF and each key has a single attribute, it is also in
5NF.
Answer 19.26 Answer omitted.
Exercise 19.27 Give an algorithm for testing whether a relation scheme is in BCNF.
The algorithm should be polynomial in the size of the set of given FDs. (The size is
the sum over all FDs of the number of attributes that appear in the FD.) Is there a
polynomial algorithm for testing whether a relation scheme is in 3NF?
Answer 19.27 Let |F| denote the size of the representation of the schema i.e.,
set
of all the FD’s of the schema. Also, let |f| denote the number of FD’s in the schema.
By exercise 19.15 we know that for each attribute in the schema, we can compute the
attribute closure of its left hand side in time O(|F|).
The algorithm to test if R is in BCNF consists of computing the attribute closure of
the left hand side of each FD. If one of them doesn not equal U where U is the set of
all attributes, then R is not in BCNF. Otherwise conclude that R is in BCNF.
Clearly the worst case complexity of this algorithm is O(|f|·|F|). Since |f| is bounded
by |F| this yields a polynomial time algorithm in |F|.
On the other hand, a priori, there is no polynomial time algorithm for testing for 3NF.
This is because to test whether or not a given FD violates 3NF we may need to check if
the right hand side is prime i.e., is a subset of some key of the schema. But identifying
(all) the keys of the schema involves checking all subsets of U, and there are 2|U| many
of them. This last prime attribute problem is known to be NP-complete and the 3NF
problem is clearly as hard (in fact polynomially reducible to the other) and, hence is
also NP-complete.

202
Chapter 19
Exercise 19.28 Give an algorithm for testing whether a relation scheme is in BCNF.
The algorithm should be polynomial in the size of the set of given FDs. (The ‘size’ is
the sum over all FDs of the number of attributes that appear in the FD.) Is there a
polynomial algorithm for testing whether a relation scheme is in 3NF?
Answer 19.28 Answer omitted.
Exercise 19.29 Prove that the algorithm for decomposing a relation schema with a
set of FDs into a collection of BCNF relation schemas as described in Section 19.6.1
is correct (i.e., it produces a collection of BCNF relations, and is lossless-join) and
terminates.
Answer 19.29 First, we will repeat the algorithm so as to keep consistent notation:
1. Let X ⊂R, A be a single atribute in R and X →A be a FD that causes a violation
of BCNF. Decompose into R −A and XA.
2. If either R −A or XA is not in BCNF, decompose them further by a recursive
application of this algorithm.
Proving the correctness of the algorithm is divided into 3 parts:
Proof that every Decomposition is Lossless:
For any decomposition of a relation R into R −A and XA that the algorithm
takes, it is trivially loseless by Thoerem 3 of this chapter. First, we claim that
(R −A)  (XA) = X since: X ⊂R by construction, and A is not in X (else
it would be a trivially functional dependency and not violate BCNF, which is
a contradiction). The given functional dependency X →A then implies X →
XA by the Union Rule, therefore (R −A)  (XA) →XA and by Thoerem 3,
this decomposition is loseless. Note however, that this decomposition may not be
dependency preserving.
Proof that Algorithm Terminates:
Every decomposition of a Relation R that the algorithm performs produces rela-
tions R −A and XA with strictly fewer attributes then R. R −A has strictly
fewer attributes than R since by construction A is not null, and since the func-
tional dependency violates BCNF also by construction, A must be contained in
R, else the functional dependency would not be applicable to R. Further, XA has
strictly fewer attributes than R since by construction X ⊂R and XA ̸= R. This is
clear since if we assume that XA = R, then we can conclude that X is a superkey
because XA →R trivially and X →A, so that X →R from Transivitivty. This
would contradict the assumption that the functional dependency violates BCNF,

Schema Reﬁnement and Normal Forms
203
leaving us to conclude that XA ̸= R.
If we let n denote the number of attributes in the original relation R, then there
are at most (2n −1) decompositions the algorithm will perform before it termi-
nates. Once a relation contains just a single attribute, it is in BCNF and cannot
be decomposed further since there are no non-trivial functional dependencies we
can apply to it that would violate BCNF.
Proof that every Relation in Final Set is in BCNF:
As discussed in the previous part of the problem, in the worst case the algorithm
decomposes R into a set of n unique single attribute relations where n is the number
of attributes in the original relation R. As also discussed above, each relation
is clearly in BCNF. The decomposition process may, and in most cases should,
terminate before we are down to all single attribute relations but irregardless, the
algorithm will only stop when all subsets of R are in BCNF.

20
PHYSICAL DATABASE DESIGN AND
TUNING
Exercise 20.1 Consider the following BCNF schema for a portion of a simple cor-
porate database (type information is not relevant to this question and is omitted):
Emp (eid, ename, addr, sal, age, yrs, deptid)
Dept (did, dname, ﬂoor, budget)
Suppose you know that the following queries are the six most common queries in the
workload for this corporation and that all six are roughly equivalent in frequency and
importance:
List the id, name, and address of employees in a user-speciﬁed age range.
List the id, name, and address of employees who work in the department with a
user-speciﬁed department name.
List the id and address of employees with a user-speciﬁed employee name.
List the overall average salary for employees.
List the average salary for employees of each age; that is, for each age in the
database, list the age and the corresponding average salary.
List all the department information, ordered by department ﬂoor numbers.
1. Given this information, and assuming that these queries are more important than
any updates, design a physical schema for the corporate database that will give
good performance for the expected workload.
In particular, decide which at-
tributes will be indexed and whether each index will be a clustered index or an
unclustered index. Assume that B+ tree indexes are the only index type sup-
ported by the DBMS and that both single- and multiple-attribute keys are per-
mitted. Specify your physical design by identifying the attributes you recommend
indexing on via clustered or unclustered B+ trees.
204

Physical Database Design and Tuning
205
2. Redesign the physical schema assuming that the set of important queries is changed
to be the following:
List the id and address of employees with a user-speciﬁed employee name.
List the overall maximum salary for employees.
List the average salary for employees by department; that is, for each deptid
value, list the deptid value and the average salary of employees in that de-
partment.
List the sum of the budgets of all departments by ﬂoor; that is, for each ﬂoor,
list the ﬂoor and the sum.
Assume that this workload is to be tuned with an automatic index tuning
wizard. Outline the main steps in the execution of the index tuning algorithm
and the set of candidate conﬁgurations that would be considered.
Answer 20.1 The answer to each question is given below.
1.
If we create a dense unclustered B+ tree index on ⟨age, sal⟩of the Emp
relation we will be able to do an index-only scan to answer the 5th query. A
hash index would not serve our purpose here, since the data entries will not
be ordered by age! If index only scans are not allowed create a clustered B+
tree index on just the age ﬁeld of Emp.
We should create an unclustered B+Tree index on deptid of the Emp relation
and another unclustered index on ⟨dname, did⟩in the Dept relation. Then,
we can do an index only search on Dept and then get the Emp records with
the proper deptid’s for the second query.
We should create an unclustered index on ename of the Emp relation for the
third query.
We want a clustered sparse B+ tree index on floor of the Dept index so we
can get the department on each ﬂoor in floor order for the sixth query.
Finally, a dense unclustered index on sal will allow us to average the salaries
of all employees using an index only-scan. However, the dense unclustered
B+ tree index on ⟨age, sal⟩that we created to support Query (5) can also be
used to compute the average salary of all employees, and is almost as good
for this query as an index on just sal. So we should not create a separate
index on just sal.
2.
We should create an unclustered B+Tree index on ename for the Emp rela-
tion so we can eﬃciently ﬁnd employees with a particular name for the ﬁrst
query. This is not an index-only plan.
An unclustered B+ tree index on sal for the Emp relation will help ﬁnd the
maximum salary for the second query. (This is better than a hash index
because the aggregate operation involved is MAX—we can simply go down to
the rightmost leaf page in the B+ tree index.) This is not an index-only plan.

206
Chapter 20
We should create a dense unclustered B+ tree index on ⟨deptid, sal⟩of the
Emp relation so we can do an index-only scan on all of a department’s em-
ployees. If index only plans are not supported, a sparse, clustered B+ tree
index on deptid would be best. It would allow us to retrieve tuples by deptid.
We should create a dense, unclustered index on ⟨floor, budget⟩for Dept.
This would allow us to sum budgets by ﬂoor using an index only plan. If
index-only plans are not supported, we should create a sparse clustered B+
tree index on floor for the Dept relation, so we can ﬁnd the departments on
each ﬂoor in order by ﬂoor.
Exercise 20.2 Consider the following BCNF relational schema for a portion of a
university database (type information is not relevant to this question and is omitted):
Prof(ssno, pname, oﬃce, age, sex, specialty, dept did)
Dept(did, dname, budget, num majors, chair ssno)
Suppose you know that the following queries are the ﬁve most common queries in the
workload for this university and that all ﬁve are roughly equivalent in frequency and
importance:
List the names, ages, and oﬃces of professors of a user-speciﬁed sex (male or
female) who have a user-speciﬁed research specialty (e.g., recursive query process-
ing). Assume that the university has a diverse set of faculty members, making it
very uncommon for more than a few professors to have the same research specialty.
List all the department information for departments with professors in a user-
speciﬁed age range.
List the department id, department name, and chairperson name for departments
with a user-speciﬁed number of majors.
List the lowest budget for a department in the university.
List all the information about professors who are department chairpersons.
These queries occur much more frequently than updates, so you should build whatever
indexes you need to speed up these queries. However, you should not build any un-
necessary indexes, as updates will occur (and would be slowed down by unnecessary
indexes). Given this information, design a physical schema for the university database
that will give good performance for the expected workload. In particular, decide which
attributes should be indexed and whether each index should be a clustered index or
an unclustered index. Assume that both B+ trees and hashed indexes are supported
by the DBMS and that both single- and multiple-attribute index search keys are per-
mitted.

Physical Database Design and Tuning
207
1. Specify your physical design by identifying the attributes you recommend indexing
on, indicating whether each index should be clustered or unclustered and whether
it should be a B+ tree or a hashed index.
2. Assume that this workload is to be tuned with an automatic index tuning wizard.
Outline the main steps in the algorithm and the set of candidate conﬁgurations
considered.
3. Redesign the physical schema, assuming that the set of important queries is
changed to be the following:
List the number of diﬀerent specialties covered by professors in each depart-
ment, by department.
Find the department with the fewest majors.
Find the youngest professor who is a department chairperson.
Answer 20.2 Answer omitted.
Exercise 20.3 Consider the following BCNF relational schema for a portion of a
company database (type information is not relevant to this question and is omitted):
Project(pno, proj name, proj base dept, proj mgr, topic, budget)
Manager(mid, mgr name, mgr dept, salary, age, sex)
Note that each project is based in some department, each manager is employed in
some department, and the manager of a project need not be employed in the same
department (in which the project is based).
Suppose you know that the following
queries are the ﬁve most common queries in the workload for this university and all
ﬁve are roughly equivalent in frequency and importance:
List the names, ages, and salaries of managers of a user-speciﬁed sex (male or
female) working in a given department. You can assume that, while there are
many departments, each department contains very few project managers.
List the names of all projects with managers whose ages are in a user-speciﬁed
range (e.g., younger than 30).
List the names of all departments such that a manager in this department manages
a project based in this department.
List the name of the project with the lowest budget.
List the names of all managers in the same department as a given project.

208
Chapter 20
These queries occur much more frequently than updates, so you should build whatever
indexes you need to speed up these queries. However, you should not build any un-
necessary indexes, as updates will occur (and would be slowed down by unnecessary
indexes). Given this information, design a physical schema for the company database
that will give good performance for the expected workload. In particular, decide which
attributes should be indexed and whether each index should be a clustered index or
an unclustered index. Assume that both B+ trees and hashed indexes are supported
by the DBMS, and that both single- and multiple-attribute index keys are permitted.
1. Specify your physical design by identifying the attributes you recommend indexing
on, indicating whether each index should be clustered or unclustered and whether
it should be a B+ tree or a hashed index.
2. Assume that this workload is to be tuned with an automatic index tuning wizard.
Outline the main steps in the algorithm and the set of candidate conﬁgurations
considered.
3. Redesign the physical schema assuming the set of important queries is changed to
be the following:
Find the total of the budgets for projects managed by each manager; that
is, list proj mgr and the total of the budgets of projects managed by that
manager, for all values of proj mgr.
Find the total of the budgets for projects managed by each manager but only
for managers who are in a user-speciﬁed age range.
Find the number of male managers.
Find the average age of managers.
Answer 20.3 The answer to each question is given below.
1.
For the ﬁrst query, we should create a dense unclustered hash index on
mgr dept for the Manager relation. We omit sex from the key in this in-
dex since it is not very selective; however, including it is probably not very
expensive since this ﬁeld is unlikely to be updated.
We should create a unclustered B+ tree index on ⟨age, mgr dept, mid⟩for the
Manager relation, and an unclustered hash index on ⟨proj base dept, proj mgr⟩
for the Project relation. We can do an index only scan to ﬁnd managers whose
age is in the speciﬁed range, and then hash into the Project relation to get the
project names. If index only scans are not supported, the index on manager
should be a clustered index on age.
For the third query we don’t need a new index. We can scan all managers
and use the hash index on ⟨proj base dept, proj mgr⟩on the Project relation
to check if mgr dept = proj base dept.

Physical Database Design and Tuning
209
We can create an unclustered B+ tree index on budget in the Project relation
and then go down the tree to ﬁnd the lowest budget for the fourth query.
For the ﬁfth query, we should create dense unclustered hash index on pno
for the Project relation. We can can get the proj base dept of the project
by using this index, and then use the hash index on mgr dept to get the
managers in this department. Note that an index on ⟨pno, proj base dept for
Project would allow us to do an index only scan on Project. However, since
there is exactly one base department for each project (pno is the key) this is
not likely to be signiﬁcantly faster. (It does save us one I/O per project.)
2.
For the ﬁrst query, we should create an unclustered B+Tree index on ⟨proj mgr, budget⟩
for the Project relation. An index only scan can then be used to solve the
query. If index only scans are not supported, a clustered index on proj mgr
would be best.
If we create a sparse clustered B+ tree index on ⟨age, mid⟩for Manager, we
can do an index only scan on this index to ﬁnd the ids of managers in the
given range. Then, we can use an index only scan of the B+Tree index on
⟨proj mgr, budget⟩to compute the total of the budgets of the projects that
each of these managers manages. If index only scans are not supported, the
index on Manager should be a clustered B+ tree index on age.
An unclustered hash index on sex will divide the managers by sex and allow
us to count the number that are male using an index only scan. If index only
scans are not allowed, then no index will help us for the third query.
We should create an unclustered hash index on age for the fourth query. All
we need to do is average the ages using an index-only scan. If index-only
plans are not allowed no index will help us.
Exercise 20.4 The Globetrotters Club is organized into chapters. The president of a
chapter can never serve as the president of any other chapter, and each chapter gives
its president some salary. Chapters keep moving to new locations, and a new president
is elected when (and only when) a chapter moves. This data is stored in a relation
G(C,S,L,P), where the attributes are chapters (C), salaries (S), locations (L), and
presidents (P). Queries of the following form are frequently asked, and you must be
able to answer them without computing a join: “Who was the president of chapter X
when it was in location Y ?”
1. List the FDs that are given to hold over G.
2. What are the candidate keys for relation G?
3. What normal form is the schema G in?
4. Design a good database schema for the club. (Remember that your design must
satisfy the stated query requirement!)

210
Chapter 20
5. What normal form is your good schema in? Give an example of a query that is
likely to run slower on this schema than on the relation G.
6. Is there a lossless-join, dependency-preserving decomposition of G into BCNF?
7. Is there ever a good reason to accept something less than 3NF when designing a
schema for a relational database? Use this example, if necessary adding further
constraints, to illustrate your answer.
Answer 20.4 Answer omitted.
Exercise 20.5 Consider the following BCNF relation, which lists the ids, types (e.g.,
nuts or bolts), and costs of various parts, along with the number available or in stock:
Parts (pid, pname, cost, num avail)
You are told that the following two queries are extremely important:
Find the total number available by part type, for all types. (That is, the sum of
the num avail value of all nuts, the sum of the num avail value of all bolts, and
so forth)
List the pids of parts with the highest cost.
1. Describe the physical design that you would choose for this relation. That is, what
kind of a ﬁle structure would you choose for the set of Parts records, and what
indexes would you create?
2. Suppose your customers subsequently complain that performance is still not satis-
factory (given the indexes and ﬁle organization you chose for the Parts relation in
response to the previous question). Since you cannot aﬀord to buy new hardware
or software, you have to consider a schema redesign. Explain how you would try
to obtain better performance by describing the schema for the relation(s) that you
would use and your choice of ﬁle organizations and indexes on these relations.
3. How would your answers to the two questions change, if at all, if your system did
not support indexes with multiple-attribute search keys?
Answer 20.5 The answer to each question is given below.
1. A heap ﬁle structure could be used for the relation Parts. A dense unclustered
B+Tree index on ⟨pname, num avail⟩and a dense unclustered B+ Tree index on
⟨cost, pid⟩can be created to eﬃciently answers the queries.

Physical Database Design and Tuning
211
2. The problem could be that the optimizer may not be considering the index only
plans that could be obtained using the previously described schema. So we can
instead create clustered indexes on ⟨pid, cost⟩and ⟨pname, num avail⟩. To do
this we have to vertically partition the relation into two relations like Parts1( pid,
cost ) and Parts2( pid, pname, num avail). (If the indexes themselves have not
been implemented properly, then we can instead use sorted ﬁle organizations for
these two split relations).
3. If the multi attribute keys are not allowed then we can have a clustered B+ Tree
indexes on cost and on pname on the two relations.
Exercise 20.6 Consider the following BCNF relations, which describe employees and
the departments they work in:
Emp (eid, sal, did)
Dept (did, location, budget)
You are told that the following queries are extremely important:
Find the location where a user-speciﬁed employee works.
Check whether the budget of a department is greater than the salary of each
employee in that department.
1. Describe the physical design you would choose for this relation. That is, what
kind of a ﬁle structure would you choose for these relations, and what indexes
would you create?
2. Suppose that your customers subsequently complain that performance is still not
satisfactory (given the indexes and ﬁle organization that you chose for the rela-
tions in response to the previous question). Since you cannot aﬀord to buy new
hardware or software, you have to consider a schema redesign. Explain how you
would try to obtain better performance by describing the schema for the rela-
tion(s) that you would use and your choice of ﬁle organizations and indexes on
these relations.
3. Suppose that your database system has very ineﬃcient implementations of index
structures. What kind of a design would you try in this case?
Answer 20.6 Answer omitted.
Exercise 20.7 Consider the following BCNF relations, which describe departments
in a company and employees:

212
Chapter 20
Dept(did, dname, location, managerid)
Emp(eid, sal)
You are told that the following queries are extremely important:
List the names and ids of managers for each department in a user-speciﬁed loca-
tion, in alphabetical order by department name.
Find the average salary of employees who manage departments in a user-speciﬁed
location. You can assume that no one manages more than one department.
1. Describe the ﬁle structures and indexes that you would choose.
2. You subsequently realize that updates to these relations are frequent. Because
indexes incur a high overhead, can you think of a way to improve performance on
these queries without using indexes?
Answer 20.7 The answer to each question is given below.
1. A heap ﬁle organization for the two relations is suﬃcient if we create the following
indexes.
For the ﬁrst, a clustered B+ tree index on ⟨location, dname⟩would
improve performance (we cannot list the names of the managers because there is
no name attribute present). We can also have a hash index on eid on the Emp
relation to speed up the second query: we ﬁnd all of the managerids from the
B+ tree index, and then use the hash index to ﬁnd their salaries.
2. Without indexes, we can use horizontal decompostion of the Dept relation based
on the location. We can also try sorted ﬁle organizations, with the relation Dept
sorted on dname and Emp on eid.
Exercise 20.8 For each of the following queries, identify one possible reason why an
optimizer might not ﬁnd a good plan. Rewrite the query so that a good plan is likely
to be found. Any available indexes or known constraints are listed before each query;
assume that the relation schemas are consistent with the attributes referred to in the
query.
1. An index is available on the age attribute:
SELECT E.dno
FROM
Employee E
WHERE
E.age=20 OR E.age=10
2. A B+ tree index is available on the age attribute:

Physical Database Design and Tuning
213
SELECT E.dno
FROM
Employee E
WHERE
E.age<20 AND E.age>10
3. An index is available on the age attribute:
SELECT E.dno
FROM
Employee E
WHERE
2*E.age<20
4. No index is available:
SELECT DISTINCT *
FROM
Employee E
5. No index is available:
SELECT
AVG (E.sal)
FROM
Employee E
GROUP BY E.dno
HAVING
E.dno=22
6. The sid in Reserves is a foreign key that refers to Sailors:
SELECT
S.sid
FROM
Sailors S, Reserves R
WHERE
S.sid=R.sid
Answer 20.8 Answer omitted.
Exercise 20.9 Consider two ways to compute the names of employees who earn more
than $100,000 and whose age is equal to their manager’s age. First, a nested query:
SELECT
E1.ename
FROM
Emp E1
WHERE
E1.sal > 100 AND E1.age = ( SELECT E2.age
FROM
Emp E2, Dept D2
WHERE
E1.dname = D2.dname
AND D2.mgr = E2.ename )
Second, a query that uses a view deﬁnition:
SELECT
E1.ename
FROM
Emp E1, MgrAge A
WHERE
E1.dname = A.dname AND E1.sal > 100 AND E1.age = A.age

214
Chapter 20
CREATE VIEW MgrAge (dname, age)
AS SELECT D.dname, E.age
FROM
Emp E, Dept D
WHERE
D.mgr = E.ename
1. Describe a situation in which the ﬁrst query is likely to outperform the second
query.
2. Describe a situation in which the second query is likely to outperform the ﬁrst
query.
3. Can you construct an equivalent query that is likely to beat both these queries
when every employee who earns more than $100,000 is either 35 or 40 years old?
Explain brieﬂy.
Answer 20.9
1. Consider the case when there are very few or no employees having
salary more than 100K. Then in the ﬁrst query the nested part would not be
computed (due to short circuit evaluation) whereas in the second query the join
of Emp and MgrAge would be computed irrespective of the number of Employees
with sal > 100K.
Also, if there is an index on dname, then the nested portion of of the ﬁrst query
will be eﬃcient. However, the index does not aﬀect the view in the second query
since it is used from a view.
2. In the case when there are a large number of employees with sal > 100K and
the Dept relation is large, in the ﬁrst query the join of Dept and Emp would
be computed for each tuple in Emp that satisﬁes the condition E1.sal > 100K,
whereas in the latter the join is computed only once.
3. In this case the selectivity of age may be very high. So if we have a B+ Tree index
on ⟨age, sal⟩, then the following query may perform better.
SELECT
E1.ename
FROM
Emp E1
WHERE
E1.age=35 AND E1.sal > 100 AND E1.age =
( SELECT E2.age
FROM
Emp E2, Dept D2
WHERE
E1.dname = D2.dname AND D2.mgr = E2.ename)
UNION
SELECT
E1.ename
FROM
Emp E1
WHERE
E1.age = 40 AND E1.sal > 100 AND E1.age =
( SELECT E2.age
FROM
Emp E2, Dept D2
WHERE
E1.dname = D2.dname AND D2.mgr = E2.ename)

21
SECURITY
Exercise 21.1 Brieﬂy answer the following questions:
1. Explain the intuition behind the two rules in the Bell-LaPadula model for manda-
tory access control.
2. Give an example of how covert channels can be used to defeat the Bell-LaPadula
model.
3. Give an example of polyinstantiation.
4. Describe a scenario in which mandatory access controls prevent a breach of security
that cannot be prevented through discretionary controls.
5. Describe a scenario in which discretionary access controls are required to enforce
a security policy that cannot be enforced using only mandatory controls.
6. If a DBMS already supports discretionary and mandatory access controls, is there
a need for encryption?
7. Explain the need for each of the following limits in a statistical database system:
(a) A maximum on the number of queries a user can pose.
(b) A minimum on the number of tuples involved in answering a query.
(c) A maximum on the intersection of two queries (i.e., on the number of tuples
that both queries examine).
8. Explain the use of an audit trail, with special reference to a statistical database
system.
9. What is the role of the DBA with respect to security?
10. Describe AES and its relationship to DES.
215

216
Chapter 21
11. What is public-key encryption? How does it diﬀer from the encryption approach
taken in the Data Encryption Standard (DES), and in what ways is it better than
DES?
12. Explain how a company oﬀering services on the Internet could use encryption-
based techniques to make its order-entry process secure. Discuss the role of DES,
AES, SSL, SET, and digital signatures. Search the Web to ﬁnd out more about
related techniques such as electronic cash.
Answer 21.1
The answer to each question is given below.
1. The Simple Security Property states that subjects can only interact with objects
with a lesser or equal security class. This ensures subjects with low security classes
from accessing high security objects. The *-Property states that subjects can only
create objects with a greater or equal security class. This prevents a high security
subject from mistakenly creating an object with a low security class (which low
security subjects could then access!).
2. One example of a covert channel is in statistical databases. If a malicious subject
wants to ﬁnd the salary of a new employee, and can issue queries to ﬁnd the
average salary in a department, and the total number of current employees in
the depatment, then the malicious subject can calculate the new employees salary
based on the increase in average salary and number of employees.
3. Say relation R contains the following values:
cid
carname
Security Class
1
Honda
U
1
Porsche
C
2
Toyota
C
3
Mazda
C
3
Ferrari
TS
Then subjects with security class U will see R as:
cid
carname
Security Class
1
Honda
U
Subjects with security class C will see R as:
cid
carname
Security Class
1
Honda
U
1
Porsche
C
2
Toyota
C
3
Mazda
C

Security
217
Subjects with security class TS will see R as:
cid
carname
Security Class
1
Honda
U
1
Porsche
C
2
Toyota
C
3
Mazda
C
3
Ferrari
TS
4. Trojan horse tables are an example where discretionary access controls are not
suﬃcient. If a malicious user creates a table and has access to the source code of
some other user with privileges to other tables, then the malicious user can modify
the source code to copy tuples from privileged tables to his or her non-privileged
table.
5. Manditory access controls do not distinguish between people in the same clearance
level so it is not possible to limit permissions to certain users within the same
clearance level. Also, it is not possible to give only insert or select privileges to
diﬀerent users in the same level: all users in the same clearance level have select,
insert, delete and update privileges.
6. Yes, especially if the data is transmitted over a network in a distributed environ-
ment. In these cases it is important to encrypt the data so people ’listening’ on
the wire cannot directly access the information.
7.
(a) If a user can issue an unlimited number of queries, he or she can repeatedly
decompose statistical information by gathering the statistics at each level
(for example, at age ¿ 20, age ¿ 21, etc.).
(b) If a malicious subject can query a database and retrieve single rows of statis-
tical information, he or she may be able to isolate sensitive information such
as maximum and minimum values.
(c) Often the information from two queries can be combined to deduce or infer
speciﬁc values. This is often the case with average and total aggregates. This
can be prevented by restricting the tuple overlap between queries.
8. The audit trail is a log of updates with the authorization id of the user who issued
the update. Since it is possible to infer information from statistical databases
using repeated queries, or queries that target a common set of tuples, the DBA
can use an audit trail to see which people issued these security-breaking queries.
9. The DBA creates new accounts, ensures that passwords are safe and changed
often, assigns mandatory access control levels, and can analyze the audit trail
to look for security breaches. They can also assist users with their discretionary
permissions.

218
Chapter 21
10. Public-key encryption is an encryption scheme that uses a public encryption key
and a private decryption key. These keys are part of one-way functions whose in-
verse is very diﬃcult to determine (which is why large prime numbers are involved
in encryption algorithms...factoring is diﬃcult!). The public key and private key
are inverses which allow a user to encrypt any information, but only the person
with the private key can decode the messages. DES has only one key and a speciﬁc
decrypting algorithm. DES decoding can be more diﬃcult and relies on only one
key so both the sender and the receiver must know it.
11. A one-way function is a mathematical function whose inversese is very diﬃcult to
determine. These are used to determine the public and private keys, and to do the
actual decoding: a message is encoding using the function and is decoded using
the inverse of the function. Since the inverse is diﬃcult to ﬁnd, the code can not
be broken easily.
12. An internet server could issue each user a public key with which to encrypt his or
her data and send it back to the server (which holds all of the private keys). This
way users cannot decode other users’ messages, and even knowledge of the public
key is not suﬃcient to decode the message. With DES, the encryption key is used
both in encryption and decryption so sending keys to users is risky (anyone who
intercepts the key can potentially decode the message).
Exercise 21.2 You are the DBA for the VeryFine Toy Company and create a relation
called Employees with ﬁelds ename, dept, and salary. For authorization reasons, you
also deﬁne views EmployeeNames (with ename as the only attribute) and DeptInfo
with ﬁelds dept and avgsalary. The latter lists the average salary for each department.
1. Show the view deﬁnition statements for EmployeeNames and DeptInfo.
2. What privileges should be granted to a user who needs to know only average
department salaries for the Toy and CS departments?
3. You want to authorize your secretary to ﬁre people (you will probably tell him
whom to ﬁre, but you want to be able to delegate this task), to check on who is an
employee, and to check on average department salaries. What privileges should
you grant?
4. Continuing with the preceding scenario, you do not want your secretary to be able
to look at the salaries of individuals. Does your answer to the previous question
ensure this? Be speciﬁc: Can your secretary possibly ﬁnd out salaries of some
individuals (depending on the actual set of tuples), or can your secretary always
ﬁnd out the salary of any individual he wants to?
5. You want to give your secretary the authority to allow other people to read the
EmployeeNames view. Show the appropriate command.

Security
219
6. Your secretary deﬁnes two new views using the EmployeeNames view. The ﬁrst is
called AtoRNames and simply selects names that begin with a letter in the range
A to R. The second is called HowManyNames and counts the number of names.
You are so pleased with this achievement that you decide to give your secretary
the right to insert tuples into the EmployeeNames view. Show the appropriate
command and describe what privileges your secretary has after this command is
executed.
7. Your secretary allows Todd to read the EmployeeNames relation and later quits.
You then revoke the secretary’s privileges. What happens to Todd’s privileges?
8. Give an example of a view update on the preceding schema that cannot be imple-
mented through updates to Employees.
9. You decide to go on an extended vacation, and to make sure that emergencies
can be handled, you want to authorize your boss Joe to read and modify the
Employees relation and the EmployeeNames relation (and Joe must be able to
delegate authority, of course, since he is too far up the management hierarchy to
actually do any work). Show the appropriate SQL statements. Can Joe read the
DeptInfo view?
10. After returning from your (wonderful) vacation, you see a note from Joe, indicating
that he authorized his secretary Mike to read the Employees relation. You want
to revoke Mike’s SELECT privilege on Employees, but you do not want to revoke
the rights you gave to Joe, even temporarily. Can you do this in SQL?
11. Later you realize that Joe has been quite busy. He has deﬁned a view called All-
Names using the view EmployeeNames, deﬁned another relation called StaﬀNames
that he has access to (but you cannot access), and given his secretary Mike the
right to read from the AllNames view. Mike has passed this right on to his friend
Susan. You decide that, even at the cost of annoying Joe by revoking some of
his privileges, you simply have to take away Mike and Susan’s rights to see your
data. What REVOKE statement would you execute? What rights does Joe have
on Employees after this statement is executed? What views are dropped as a
consequence?
Answer 21.2 Answer omitted.
Exercise 21.3 You are a painter and have an Internet store where you sell your
paintings directly to the public. You would like customers to pay for their purchases
with credit cards, and wish to ensure that these electronic transactions are secure.
Assume that Mary wants to purchase your recent painting of the Cornell Uris Library.
Answer the following questions.
1. How can you ensure that the user who is purchasing the painting is really Mary?

220
Chapter 21
2. Explain how SSL ensures that the communication of the credit card number is
secure. What is the role of a certiﬁcation authority in this case?
3. Assume that you would like Mary to be able to verify that all your email mes-
sages are really sent from you. How can you authenticate your messages without
encrypting the actual text?
4. Assume that your customers can also negotiate the price of certain paintings and
assume that Mary wants to negotiate the price of your painting of the Madison
Terrace. You would like the text of this communication to be private between
you and Mary. Explain the advantages and disadvantages of diﬀerent methods of
encrypting your communication with Mary.
Answer 21.3
The answer to each question is given below.
1. In order to determine whether the user who is purchasing the painting is really
Mary, we need some level of veriﬁcation when Mary ﬁrst registers with the system.
On the lowest level, we can simply ask the user to conﬁrm things like Mary’s ad-
dress or social security number. To increase the level of security, we could also ask
the user to verify Mary’s credit card number. Since these numbers are deemed
diﬃcult to obtain, most merchant websites consider this suﬃcient evidence for
proof of identity.
For an even higher level of security, we can take external steps to verify Mary’s
information such as calling her up with the phone number provided, sending a let-
ter to Mary’s mailing address, or sending her an e-mail with instructions to reply
back. In each instance, we attempt to validate the information the user provided
so that the element of uncertainty in the provided information is decreased.
2. SSL Encryption is a form of public-key encryption where a third party certiﬁcation
authority acts to validate public keys between two clients. In a general public-key
encryption system, data is sent to a user encrypted with a publicly known key for
that user, such that only the user’s private key, known only to that user, can be
used to decrypt the data. Attempts to decrypt the information using other keys
will produce garbage data, and the ability to decipher the private key is considered
computationally expensive even for the most modern computing systems.
In SSL Encryption, the client transmitting the data asks the certiﬁcation au-
thority for a certiﬁcate containing public key information about the other client.
The ﬁrst client then validates this information by decrypting the certiﬁcate to get
the second client’s public key. If the decrypted certiﬁcate matches up with the
certiﬁcation authority’s information, the ﬁrst client then uses this public key to
encrypt a randomly generated session key and sends it to the second client. The
ﬁrst client and second client now have a randomly generated public-key system

Security
221
that they can use to communicate privately without fear that anyone else can
decode their information.
Once complete, SSL encryption ensures that data such as credit card informa-
tion transmitted between the two clients cannot be easily decrypted by others
intercepting packets because the certiﬁcation authority helped to generate a ran-
domly created public-key that only the two clients involved can understand.
3. A message to Mary can be sent unencrypted with a message signature attached
to the message. A signature is obtained by applying a one-way function to the
message and is considerably smaller than the message itself. Mary can then apply
the one-way function and if the results of it match the signature, she’ll know it
was authentic.
4. One method of sending Mary a message is to create a digital signature for the
message by encrypting it twice. First, we encrypt the message using our private
key, then we encrypt the results using Mary’s public key. The ﬁrst encryption
ensures that the message did indeed come from us, since only we know our private
key while the second encryption ensures that only Mary can read the message,
since only Mary knows her private key.
This system is very safe from tampering since it is hard to send a message pre-
tending to be someone else as well as diﬃcult to properly decode an intercepted
message. The only disadvantages are that it requires that we have copies of each
person’s public key as well as spend the time to encrypt/decrypt the messages.
For example, if Mary receives the message on her laptop or PDA while traveling,
she may not have the resources or public keys to decrypt it and respond, and
might need to wait until she returns to the oﬃce.
Another method of communicating with Mary, discussed in the previous question,
is to use message signatures. This allows Mary to be able to read the message
from almost any system since it is not encrypted and ensure that the message is
authentic. The only disadvantage is that it does not safely prevent someone else
from reading the message as well.
Exercise 21.4 Consider Exercises 6.6 to 6.9 from Chapter 6. For each exercise, iden-
tify what data should be accessible to diﬀerent groups of users, and write the SQL
statements to enforce these access control policies.
Answer 21.4 Answer omitted.

222
Chapter 21
Exercise 21.5 Consider Exercises 7.7 to 7.9 from Chapter 7. For each exercise, dis-
cuss where encryption, SSL, and digital signatures are appropriate.
Answer 21.5
The answer to each question is given below.
Exercise 7.7
For the Notown Records website, encryption plays an important part in ensuring
that customers are able to safely interact and order records over the Internet.
Before discussing what should be encrypted, it is important to also note what
should not be encrypted. Many of operations including searching the database
and browsing record catalogs do not require any encryption. These operations
are performed often and encrypting every communication from the client to the
website would severely drain the server’s resources. As a result, it is better for
the server to focus on encrypting only information that is of a more serious nature.
There are three places where we can apply an encryption scheme to this system:
user registration, user login, and user checkout. The purpose of encrypting data
at registration and checkout is obvious, the user is transmitting sensitive personal
information like address and credit card numbers, and it is of utmost importance
to protect this information. We also encrypt the password transmitted during
user login since that ensures that future communications with the user are safe
after login.
In practice, we would use SSL encryption via a certiﬁcation authority, e.g., Verisign.
Upon an encryption request, the client’s web browser requests the Verisign cer-
tiﬁcate, validates it by decrypting it, and uses the public key from the decrypted
certiﬁcate to encrypt a radomly generated session key that it then sends to the
server. Using this session key, the certiﬁcation authority is no longer needed, and
the server/client then transmit information just as they would in a normal public
key system.
The Notown website could also use digital signatures to verify a user’s registration
information via e-mail, as well as send an order conﬁrmation e-mail after an order
has been placed. By checking the digital signature of e-mails sent, the website
and user can help to double check each other’s identities after a new registration
has been processed or a transaction has been placed.
Exercise 7.8
For this question, security is much more important than in the previous question,
since medical records are considered very serious. As such, we would want to
encrypt most of the information transmitted during a doctor or patient’s visit to
the website. The only information we do not need to transit encrypted would be
pages with read-only data, e.g., company information or help pages. Although

Security
223
this puts a higher strain on the system, the demand for security from users in the
medical ﬁeld is much higher than that of users in the record sales industry.
As before, we would use an SSL encryption via a certiﬁcation authorization to
handle the actual encryption.
When a new user registers, it would especially
important to verify their identify as to prevent someone else from ordering pre-
scriptions in their name.
To this end, digital signatures would be much more
important to verify the validity of the e-mail used to sign up with. In addition,
external authorization including faxes, phone calls, and written mail should also
be used to verify registration information when a user signs up.
Exercise 7.9
For the course enrollment system, security is important for many of the processes,
but the system does not need to encrypt as much as it did in the online pharmacy
system.
For faculty members, their passwords should be encrypted when they login as
well as their registration information when they sign up for the ﬁrst time. When
they create/delete existing courses, it is probably not necessary to encrypt their
data so long as the system is sure the user is properly logged in. To this end, the
system could ask the faculty to re-enter their passwords when they are about to
submit a change to the system.
For students, their login and registration should also be encrypted.
Like the
faculty system, it is probably not important to encrypt any of the information
about what classes a student signs up for as long as the login information is accu-
rate. Students would then be able to freely modify their schedule after logging in
to the website and only be asked their password again when they were ready to
submit their changes to the system.
In both cases, SSL encryption would again be the method of choice for the ac-
tual encryption process. Digital signatures could be used in e-mails sent to the
students conﬁrming their course registration information.
Exercise 7.10
For the airline reservation website, encryption would again be important in user
login and user registration information for both customers and employees. Other
information like searching for ﬂights should be left unencrypted as a customer
may search for dozens of ﬂights. When a customer purchases a ﬂight, their order,
including their ﬂight information and credit card number, should be encrypted to
prevent others from learning which ﬂight they have selected, as well as protecting
the customer’s money. For airline employees, it is preferable to encrypt all of the
transmitted data so as not to give hackers the opportunity to see any backend
part of the system. Since it is likely there will be few employees per number of

224
Chapter 21
clients, the drain on the system from the extra level of encryption for employees
should be negligible.
Note that before when we considered the prescription website, we recommended
encrypting all transmitted data, whereas when we considered the course enroll-
ment system we recommended a looser form of encryption on both classes of users.
By looser form of encryption, we refer to the fact that some of the transmitted
data is encrypted while some data is not. Contrast this with the airline reservation
system, where we suggest loose encryption for customers and total encryption for
employees. For each, keep in mind that the true level of encryption can change
depending on the available resources and sensitivity of the data.
As before, SSL encryption is the preferred method of data encryption. For the
airline reservation system, digital signatures can be applied to conﬁrm when cus-
tomers have places orders via e-mail.



A SCIENCE PUBLISHERS BOOK
p,
AI and SwArm 
Evolutionary Approach  
to Emergent Intelligence
Hitoshi Iba
Information and Communication Engineering  
School of Information Science and Technology  
The University of Tokyo, Japan

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2020 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group,  an Informa business
No claim to original U.S. Government works
Printed on acid-free paper
Version Date: 20190725
International Standard Book Number-13: 978-0-367-13631-4  (Hardback)
Th is book contains information obtained from authentic and highly regarded sources. Reasonable eﬀ orts have been
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the
validity of all materials or the consequences of their use. Th e authors and publishers have attempted to trace the
copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to
publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let
us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, 
or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, includ-
ing photocopying, microﬁ lming, and recording, or in any information storage or retrieval system, without written
permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com
(http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, 
MA 01923, 978-750-8400. CCC is a not-for-proﬁ t organization that provides licenses and registration for a variety
of users. For organizations that have been granted a photocopy license by the CCC, a separate system of payment
has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for
identiﬁ cation and explanation without intent to infringe.
Library of Congress Cataloging-in-Publication Data
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com
Names: Iba, Hitoshi, author.  
Title: AI and swarm : evolutionary approach to emergent intelligence / 
   Hitoshi Iba, Information and Communication Engineering, School of 
   Information Science and Technology, The University of Tokyo, Tokyo, 
   Japan.  
Description: First edition. | Boca Raton, FL : CRC Press/Taylor & Francis 
   Group, 2019. | Includes bibliographical references and indexes. | 
   Summary: “The book provides an overview of swarm intelligence and 
   adaptive complex systems for AI, organized in seven chapters. Each 
   chapter begins with a background of the problem and of the current state 
   of the art of the field, and ends with detailed discussion about the 
   complex system frameworks. In addition, the simulators based on 
   optimizers such as PSO, ABC complex adaptive system simulation are 
   described in detail. These simulators, along with the source codes that 
   are available online, will be useful to readers for practical 
   application tasks”-- Provided by publisher.  
Identifiers: LCCN 2019029264 | ISBN 9780367136314 (hardback ; acid-free 
   paper)  
Subjects: LCSH: Artificial intelligence. | Swarm intelligence. | 
   Evolutionary computation. | Self-organizing systems. 
Classification: LCC Q335 .I333 2019 | DDC 006.3--dc23 
LC record available at https://lccn.loc.gov/2019029264

Preface
We used to joke that AI means “Almost Implemented.”
—Rodney Brooks
This book is an explanatory text about Artificial Intelligence (AI) and swarm, 
that is, “emergence” for AI. These days, we can say that we are in the middle of 
the third AI boom. One of the factors responsible for this is deep learning, which 
resulted in the evolution of neural networks as well as machine learning based on 
statistical theory. However, there remains the question of whether it is possible 
to create a true AI (a human-like general-purpose intelligence, the so-called 
“strong” AI). Many research results indicate that there is clearly more depth in 
human intelligence and cognition than is possible to express by AI techniques 
(“weak” AI).
For about 30 years, the author has studied evolution and emergent 
computation. The swarm mechanism gives us a glimpse of the principles of 
intelligence in humans and living beings. In this book, such topics related to 
artificial intelligence are explained in detail, from the theoretical background to 
the most recent progress, as well as future topics.
Some readers may wonder if some of the topics related to swarm do really 
belong to the realm of AI technology. Indeed, the essence of AI is to find problems. 
According to this line of thought, a problem is no longer considered to be AI as 
soon as it is solved. Brooks’ joke in the beginning of this text symbolizes this 
point. Brooks is a researcher specializing in AI at MIT (Massachusetts Institute 
of Technology) since the 1980’s. He is the author of numerous innovative ideas 
in the field of AI. He is also famous for having created the robot vacuum cleaner 
Roomba (he is the founder of iRobot in the US). From this perspective, is it 
possible to say that, because the vacuum cleaner robot is already in the market, 
it is no longer AI?
Current, AI seems to be surrounded by an excessively flamboyant 
advertisement of its technical applications. Of course, it is worth noting that 

iv < AI and Swarm: Evolutionary Approach to Emergent Intelligence
the intention here is not to deny all this. The author himself is involved in such 
projects. However, being satisfied with the above is a different issue. Given that 
it is possible to link any type of intelligence to AI, we should not ignore basic 
research that focuses on the essence of cognition and life.
The objective of the author’s research is to build a model of human cognitive 
functions from the perspective of “emergence” in order to achieve strong AI. 
In other words, the objective is to understand how intelligence appeared and 
interacted with the real world, as well as the causal relationships in terms of actual 
physical and chemical mechanisms. In that sense, this book intends not only 
to explain intelligent behavior itself, but also understand its causes and effects. 
In order to do that, I have explained the emergence of several types of human 
cognitive errors, cognitive dissonances, irrational behaviors, and cooperative/
betrayal actions.
Unfortunately, it is difficult to explain such phenomena using machine 
learning based on statistical models and deep learning based on big data, which 
constitute the core of current AI. On the contrary, a few hypotheses that contradict 
such approaches may be needed. In this book, research results related to brain 
mechanism are described, which is a factor that directly triggers intelligent 
behavior (called “proximate cause” or “physiological cause” in biological 
jargon), and the reasons for the evolution of living beings’ behavior (“ultimate 
cause,” or “biological/evolutional cause”). A few emergence simulation models 
are also introduced for better understanding. These approaches are important 
research efforts to clarify the essence of intelligence, in order to achieve true AI.
Most photographs depicting nature (modes of life and shapes of fish, patterns 
found in animals, images of Galapagos, Papua New Guinea, etc.) were shot by 
the author himself. Unfortunately, it is difficult for us to witness the evolution of 
nature on our own. However, it is possible to feel the driving force of emergence 
and the significance of diversity. In order to do that, it is suggested that you leave 
behind your PC or smartphone and see real things with your own eyes. The 
author has indulged in water life observation for about 30 years as a hobby, and 
still feels moved by it, reinforcing his conviction that nothing compares to seeing 
the real thing. Even though it is now possible to obtain almost any kind of image 
from the Internet, there are lots of things to be learned from real nature, and they 
are worth seeing. 
Numerous things can be learned from actual living beings and from nature 
when studying swarms and AI. Therefore, in this book, the author has tried as 
much as possible to explain topics related to living beings and real life based on 
actual examples. It is the intent of the author that the readers will expand those 
topics and tackle new issues related to AI and swarm.

Acknowledgments
This book is based on my class notes on “Artificial Intelligence” and “Simulation 
Theory”. The assignments given to the students in class were often unusual and 
considered difficult or unanswerable. However, the reports that were submitted in 
return often contained impressive descriptions and deep insights which I always 
appreciated. In this book, some examples of answers and opinions contained 
in the submitted reports are used after necessary corrections and refinement. 
Although it is impossible to acknowledge everyone by their names, I would like 
to express his deepest gratitude to the students who made the efforts to write 
interesting reports.
Undoubtedly, those philosophical and interesting discussions with colleagues 
from the laboratory and institute where I belonged as a student, constitute the 
core of this book. I takes this opportunity to express my deepest gratitude to 
teachers, and senior and junior colleagues.
I also wish to express my gratitude to SmartRams Co., Ltd. for cover image 
design.
And last, but not least, I would like to thank his wife Yumiko and sons and 
daughter Kohki, Hirono and Hiroto, for their patience and assistance.
H. Iba
April 2019 
Amami isl. (Galapagos of the East), Japan 

Contents
Preface  
iii
Acknowledgments 
 
v
Abbreviations 
 
xi
1. Introduction 
1
1.1 What is AI? – Strong AI vs Weak AI 
1
1.2 What is Emergence? 
6
1.3 Cellular Automaton and Edge of Chaos 
7
2. AI, Alife and Emergent Computation 
15
2.1 Evolutionary Computation 
15
2.1.1 What is Evolutionary Computation? 
15
2.1.2 Evolution Strategy 
22
2.1.3 Multi-objective Optimization 
24
2.2 How to Make a Bit? – Exploration vs Exploitation 
29
2.3 Wireworld: A Computer Implemented as a Cellular 
38
 
Automaton
2.4 Langton’s Ant 
43
3. meta-heuristics 
49
3.1 Ant Colony Optimization (ACO) 
49
3.1.1 Collective Behaviors of Ants 
49
3.1.2 Simulating the Pheromone Trails of Ants 
51
3.1.3 Generating a Death Spiral 
52
3.1.4 ACO using a Pheromone Trail Model 
53
3.2 Particle Swarm Optimization (PSO) 
56
3.2.1 Collective Behavior of Boids 
56
3.2.2 PSO Algorithm 
60

viii < AI and Swarm: Evolutionary Approach to Emergent Intelligence
3.2.3 Comparison with GA 
63
3.2.4 Collective Memory and Spatial Sorting 
66
3.2.5 Boids Attacked by an Enemies 
71
3.3 Artificial Bee Colony Optimization (ABC) 
76
3.4 Firefly Algorithms 
84
3.5 Cuckoo Search 
86
3.6 Harmony Search (HS) 
89
3.7 Cat Swarm Optimization (CSO) 
91
3.8 Meta-heuristics Revisited 
94
4. Emergent Properties and Swarm Intelligence 
97
4.1 Reaction-Diffusion Computing 
97
4.1.1 Voronoi Diagram Generation 
98
4.1.2 Thinning and Skeletonization for Image 
109
 
Understanding
4.2 Queuing Theory and Traffic Jams 
112
4.2.1 Most Random Customers and Poisson Arrival 
112
4.2.2 Poisson Distribution and Cognitive Errors 
115
4.2.3 Queue Management and Scheduling 
119
4.3 Silicon Traffic and Rule 184 
125
4.4 Segregation and Immigration: What is Right? 
131
5. Complex Adaptive Systems 
137
5.1 Diffusion-Limited Aggregation (DLA) 
137
5.2 How do Snowflakes Form? 
138
5.3 Why do Fish Patterns Change? 
149
5.3.1 Turing Model and Morphogenesis 
149
5.4 BZ Reaction and its Oscillation 
153
5.5 Why do We have Mottled Snakes? Theory of Murray 
155
6. Emergence of Intelligence 
161
6.1 Evolution of Cooperation and Defection 
161
6.1.1 How to Clean a Fish 
161
6.1.2 The Prisoner’s Dilemma 
163
6.1.3 Iterated Prisoner’s Dilemma 
166
6.1.4 ESS: Evolutionarily Stable Strategy 
173
6.1.5 IPD using GA 
175
6.1.6 IPD as Spatial Games 
179
6.1.7 Can the Dilemma be Eliminated with 
183
 
Quantum Games?
6.1.8 The Ultimatum Game: Are Humans Selfish or  
185 
 
Cooperative? 
6.2 Evolutionary Psychology and Mind Theory 
188

Contents < ix
6.3 How does Slime Solve a Maze Problem? Slime Intelligence 
190
6.4 Swarm Robots 
197
6.4.1 Evolutionary Robotics and Swarm 
197
6.4.2 Transportation Task for Swarm 
199
6.4.3 Occlusion-based Pushing (OBP) 
199
6.4.4 Guide-based Approach to OBP 
203
6.4.5 Let us See How they Cooperate with each other 
205
7. Conclusion 
211
7.1 Summary and Concluding Remarks 
211
References 
 
213
Index  
223
Color Section 
 
235


Abbreviations
ABC 
: artificial bee colony
ACO 
: ant colony optimization
AI 
: artificial intelligence
AL 
: artificial life
ANN 
: artificial neural network
ASEP 
: asymmetric simple exclusion process
BCA 
: Burgers cellular automaton automata
CA 
: cellular automata
CS 
: cuckoo search
CSO 
: cat swarm optimization
DLA 
: diffusion-limited aggregation
EA 
: evolutionary algorithms
EANN 
: evolutionary ANN
EC 
: evolutionary computation
EDA 
: estimation of distribution algorithm
EDD 
: earliest due date
EP 
: evolutionary programming
ES 
: evolution strategy
ESS 
: evolutionarily stable strategy
FA 
: firefly algorithm
FIFO 
: first-in, first-out
GA 
: genetic algorithms
GP 
: genetic programming
HS 
: harmony search
IPD 
: iterated prisoner’s dilemma
JSSP 
: job scheduling problem
OBP 
: occlusion-based pushing
PSO 
: particle swarm optimization

xii < AI and Swarm: Evolutionary Approach to Emergent Intelligence
SA 
: simulated annealing
SPT 
: shortest processing time
TFT 
: tit for tat
TSP 
: traveling salesman problem
UCB 
: upper confidence bound
WSLS 
: Win-Stay-Lose-Shift
ZD 
: zero-determinant strategy

Chapter 1
Introduction
A sonnet written by a machine would be better appreciated by another machine.
—Alan Turing
1.1
What is AI? – Strong AI vs Weak AI
AI (artiﬁcial intelligence) refers to the implementation of intelligence on a com-
puter and is divided into two hypotheses.
Strong AI The viewpoint that true intelligence can be implemented on a
computer, also known as general AI.
Weak AI The viewpoint that computers can merely give the impression of
intelligence, also known as narrow AI.
In the same manner, artiﬁcial life (AL), which is mentioned later, can be deﬁned
in terms of strong AL and weak AL.
“Weak AI” is an application of AI technologies to enable a high-functioning
system that replicates human intelligence for a speciﬁc purpose. In fact, there are
breakthroughs in weak AI.
In this book we consider simulation in the sense of “strong AI”. More pre-
cisely, the rationale behind this approach is that “the appropriately programed
computer really is a mind, in the sense that computers, given the right programs,
can be literally said to understand and have other cognitive states.”

2
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
AI
Text-based conversation 
Judge:  Human
A ‘s reply
B ‘s reply
Human
A is human？
How about B?
Hello.
-- Hello.
Colorless green?
Figure 1.1: Turing test.
Realizing such a computer is nontrivial, since the deﬁnition of “intelligence”
is difﬁcult in the ﬁrst place. Therefore, if a claim is made that AI (in the strong
sense) has been created, what would be the most appropriate way to evaluate it?
To this end, Alan Turing1 proposed a test of a machine’s capacity to exhibit
intelligent behavior, now called the “Turing test”, which, despite being powerful,
is the subject of numerous disputes (Fig. 1.1). The question of whether machines
can think was considered in great depth by Turing, and his ﬁnal opinion was
afﬁrmative. The Turing test can be translated into modern terms in the form of a
game involving the exchanging of messages via a discussion board:
■
One day, two new users, A and B, join the discussion board.
■
When a message is sent to A and B, they both return apt responses.
■
Of A and B, one is human and the other is a computer.
■
However, it is impossible to determine which is which, regardless of the
questions asked.
If a program passes this test (in other words, the computer cannot be identi-
ﬁed), the program can be said to simulate intelligence (as long as the questions
are valid). A similar contest, named the “The Loebner prize” after its patron,
the American philanthropist Hugh Loebner, is held online2. Although an award
of 100,000 US dollars and a solid gold medal has been offered since 1990, so
far, not a single machine participating in the contest has satisﬁed the criteria for
winning.
Nevertheless, a number of problems with the Turing test have been pointed
out, and various critical remarks have been issued about potential implementation
1Alan Turing (1912–1954): British mathematician. He worked on deciphering the Enigma encryption
used by the German Army during World War II. He is considered the “father of computer science.” The
Turing machine for computing theory and the Turing model for morphogenesis are some of his pioneering
achievements. Apple’s bitten apple logo is allegedly an homage to Turing.
2http://www.loebner.net/Prizef/loebner-prize.html

Introduction
■
3
Figure 1.2: A Chinese room.
of AI. A notable example is the challenge to the very “deﬁnition of intelligence”
by John Searle3, who questioned the foundations of the Turing test by creating a
counter thought experiment. Searle’s experiment, known as the “Chinese room,”
can be summarized as follows.
A person is conﬁned to a room with a large amount of written material on
the Chinese language (Fig. 1.2). Looking inside the room is impossible, and
there are only input and output boxes for submitting sentences and obtaining
responses. Having no understanding of Chinese, the person cannot distinguish
between different Chinese characters (for this purpose, we assume that the per-
son is British and not Japanese). Furthermore, the person is equipped with a
comprehensive manual (written in English) containing rules for connecting sets
of Chinese characters. Let us consider that a person who understands Chinese is
leading a conversation by inserting questions written in Chinese into the input
box and retrieving answers from the output box. Searle provides the following
argument.
Suppose that the person quickly becomes truly proﬁcient at manipulating
Chinese characters in accordance to the instructions, and that the person outside
the room also becomes proﬁcient at providing instructions. Then, the answers
prepared by the person inside the room would become indistinguishable from
answers provided by a Chinese person. Nobody would consider, simply by look-
ing at the provided answers, that the person inside the room does not understand
Chinese. However, in contrast to English, in the case of Chinese, the person in
the room prepares the answers by formally manipulating characters, without any
understanding whatsoever.
It cannot be said that true understanding is achieved simply by looking at
the typeface while performing manipulations in accordance with a formal set
of rules. However, as demonstrated by the “Chinese room” thought experiment,
3John Rogers Searle (1932–): American philosopher. He is known for his criticism of artiﬁcial intelli-
gence. In 1980, he proposed the “strong and weak AI” classiﬁcation.

4
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
under speciﬁc conditions, human-like behavior can be fabricated by both humans
and machines if appropriate formal rules are provided. Searle, therefore, argues
that strong AI is impossible to realize.
Various counterarguments have been considered in response to Searle, and
questions that would probably occur to most people include
■
Can conversion rules be written for all possible inputs?
■
Can such an immense database actually be searched?
However, these counterarguments are devoid of meaning. The former rejects the
realization of AI in the ﬁrst place, and the latter cannot be refuted in light of the
possibility that ultra-high-speed parallel computing or quantum computing may
exist in the future. Thus, neither one can serve as the basis of an argument.
One powerful counterargument is based on system theory. Although the per-
son in the room certainly lacks understanding, he constitutes no more than a sin-
gle part of a larger system incorporating other elements, such as the paper and the
database, and this system as a whole does possess understanding. It is the equiv-
alent to the fact that, even for Chinese, cranial nerve cells do not understand the
language by themselves. This point is integral to the complex systems regarded in
this book. The level at which intelligence is sought depends on the observed phe-
nomenon, and if the phenomenon is considered as being an emergent property,
the validity of the above system theory can be recognized. Moreover, a debate is
ongoing about whether intelligence should be thought of as an integrated concept
or as a phenomenon that is co-evolving as a result of evolution.
On the other hand, Searle reargues that “if the manual is fully memorized
and the answers are given without external help, it is still possible not to under-
stand the Chinese language”. Recently, Levesque et al. [67] counterpoint against
Searle’s criticism has been made based on a theory of computation. This is de-
scribed in the “addition room argument” as follows:
■
Consider the addition room, where twenty ten-digit numbers are to be
added.
■
Suppose there is a person in the room who does not know how to add and
a manual on addition is placed in the middle of the room.
■
At this time, if the human perfectly grasps the content of the manual and
does all the operations in their head, then is it still possible to create a
manual without understanding addition?
For the “addition room,” the following manual immediately comes to mind:
■
Memorize one-digit additions.
■
Reduce two or more-digit numbers to one-digit and add.

Introduction
■
5
However, this is exactly the way we learned in elementary school. Therefore,
knowing this manual means being acquainted with the addition algorithm, thus,
it is possible to say that addition is understood.
Let us think about a little bit more primitive manual described below:
■
Go to the same chapter as the ﬁrst number.
■
Go to the section with the same number as the second number within that
chapter.
■
Furthermore, go to the subsection with the same number as the third num-
ber within that section.
■
Repeat that for all twenty numbers.
■
When everything is over, there is a number with up to twelve-digits, so
return that number and exit.
This manual merely lists the calculation results in the same way as a dictionary.
Just like in the Chinese room argument, a human following this manual does not
add and additionally does not understand the calculation at all. So, was Searle’s
claim correct?
Let us consider here the computational complexity of this manual. The chap-
ter corresponding to the ﬁrst number requires ten to the tenth power. Each chapter
consists of ten to tenth power of sections. Since this is repeated twenty times, it
becomes 10 to 10 power to 20 power = 10200 of items. It is said that the number
of molecules in the universe is around 10100. Therefore, such a large manual can
never be made. From this, it can be understood that Searle’s claim is mistaken,
from a computational theory. This is true as far as this manual is concerned, but
Searle himself did not specify the composition method of the manual, so there
may be recurring objections.
Recently, machine translation is accomplished using a huge database and sta-
tistical processing. The famous one is Google’s machine translation. Past ma-
chine translation was a classical AI based on natural language understanding.
Unfortunately, it was not always effective. On the other hand, Google’s machine
translation does not understand natural language at all. This software statistically
joins translated words using a huge database (human translations, e.g., minutes
of the United Nations). As a result, it won the 2006 machine learning contest with
an overwhelming difference. This kind of translation method is possible due to
the power of modern computers and internet connection. This approach may be
a solution to the Chinese room argument.

6
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
1.2
What is Emergence?
Originally, emergence (emergent property) was a term in biology. A good exam-
ple of that property is seen in ant social life. Each ant has a simple mechanical
behavior. However, the ant colony as a whole acts in a highly intelligent collec-
tive manner (collective behavior), according to the distribution pattern of food
and enemies, therefore increasing the survival rate of the colony. As a result,
a caste system emerges, consisting of individuals specialized in doing different
jobs, where a social division of labor and cooperation will be seen (see Fig. 1.3).
There are no rules (programs) that govern the behavior of the whole colony.
Nonetheless, an emergence of intellectual global behavior, brought about by col-
lective action of simple programs (individual ants), is observed.
Worker ant
Soldier ant (Colobopsis), 
whose heads work like a hole cap, 
close the entrance of the ant nest 
like a living door.
Infertility caste 
(Nasutitermes. Takasagoensis), 
whose heads look like water guns, 
blow toxic substances 
on approaching enemies.
Satiated working ant
(Myrmecocystus) 
lives all their lives 
inside the nest 
as a living deposit.
Queen ant
Figure 1.3: Job seperation emerged [123].
In emergence, arising of the macroscopic phenomena from micro interactions
has become a central idea. This phenomenon is observed in various ﬁelds, such
as morphological formation (pattern generation, see sec. 5.3) or group forma-
tion (see sec. 3.2) by animal populations in ecology. Emergent phenomenon can
be seen even in economy and ﬁnance. For example, in a ﬁnancial market, each
individual (participants, investors) pursues their own proﬁt based on local infor-
mation, thus, interacts with other individuals (speculative behavior). As a result,
when we look at the entire ﬁnancial market, phenomena such as “price move-
ment of sidelines” and “nervous movement” are observed. On the other hand,
each market participant does not necessarily behave in a nervous manner or wait
watchfully. In this manner, generation of a macro phenomenon that is not de-
scribed by micro interactions is emergence. E.O. Wilson4 claims that “the higher
nature of life is emergent [109]”. In other words, the higher-level phenomenon,
leading to the creation of biological individual from cells and society from in-
4Edward Osborn Wilson (1929–): American entomologist. Researcher of sociobiology and biodiver-
sity. See also the footnote of 188 page.

Introduction
■
7
dividuals, cannot be described only by knowledge of the nature of components
constituting lower levels.
There are two types of emergence [32]:
Weak emergence New property appears as a result of the interactions of
elemental levels. Emergent property can be reduced to
individual components.
Strong emergence The newly emerged property cannot be reduced be-
cause it is more than the sum of its components. Laws
governing the property cannot be predicted by under-
standing the laws governing the structure of another
level.
By becoming a group, it spontaneously acquires properties and trends not
included in the underlying rule. Emergence is recognized in various ﬁelds, such
as physics, biology, chemistry, sociology, and art. However, strong emergence
cannot be dealt with in traditional physics and reductionist approaches.
The author thinks that human intelligence could be explained through emer-
gence. However, many brain researchers and neuroscientists are critical towards
emergence. Many of them take a reductionist method and dislike the idea of a
ghost in the brain, which cannot be explained deterministically. On the other
hand, it is known that the combination of the network conﬁguration to derive the
same behavior is extremely large. Therefore, it is a signiﬁcant obstacle for the
neuroscientist, because further analysis of the neural circuit leads only to an un-
derstanding of the mechanism, and not how it actually operates. Recognizing that
there are different levels in the organization is crucial to the understanding of the
phenomenon of emergence [32]. Recently, the mechanism by which directives
traverse from micro to macro level in a causal manner is being mathematically
researched by the neuroscientists [50].
1.3
Cellular Automaton and Edge of Chaos
The eminent mathematician John von Neumann5 studied self-reproducing au-
tomata in 1946, shortly before his death. He found that self-reproduction is pos-
sible with 29 cell states, and proved that a machine could not only reproduce
5Jon von Neumann (1903–1957): Hungarian-born American mathematician. As symbolized by the
gossip that circulated around Princeton that he slept with a mystery novel as his pillow in the day and
learned mathematics from the devil at night, he conducted pioneering research in various areas, such as
mathematics, physics, engineering, computer science, economics, meteorology, psychology, and political
science, having substantial inﬂuence in posterior advances in these areas. He was also deeply involved
with American nuclear policy and was part of the Manhattan project.

8
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 1.1: State of cell in next step.
Current state of cell
States of neighbor cells
State in next step
On
two or three are “on”
On
Other cases
Off
Off
three are “on”
Off
Other cases
Off
itself, but could also build machines more complex than itself. The research
stopped because of his death; however, in 1966, Arthur Burks edited and pub-
lished von Neumann’s manuscripts. John Conway, a British mathematician, ex-
panded on the work of von Neumann and, in 1970, introduced the Game of Life,
which attracted immense interest. Some people became “Game of Life hack-
ers,” programmers and designers more interested in operating computers than in
eating; they were not the criminal hackers of today. Hackers at MIT rigorously
researched the Game of Life, and their results contributed to advances in com-
puter science and artiﬁcial intelligence. The concept of the Game of Life evolved
into the “cellular automata” (CA), which is still widely studied in the ﬁeld of
artiﬁcial life. Most of the research on artiﬁcial life shares much in common with
the world where hackers played in the early days of computers.
The Game of Life is played on a grid of equal-sized squares (cells). Each cell
can be either “on” or “off”. There are eight adjacent cells to each cell in a two-
dimensional grid (above and below, left and right, four diagonals). This is called
the Moore neighborhood. The state in the next step is determined by the rules
outlined in Table 1.1. The “on” state corresponds to a “•” in the cell, whereas
the “off” state corresponds to a blank. The following interesting patterns can be
observed with these rules.
1. Disappearing pattern (diagonal triplet)
•
•
⇒
•
⇒
disappears
•
2. Stable pattern (2×2 block)
•
•
•
•
•
•
•
•
⇒
•
•
⇒
•
•
⇒
remains stable
3. Two-state switch (Flicker, where vertical triplets and horizontal triplets
appear in turn)
•
•
•
⇒
•
•
•
⇒
•
⇒
repeats
•
•

Introduction
■
9
4. Glider (pattern moving in one direction)
•
•
•
•
•
•
⇒
•
⇒
•
⇒
moves to bottom right
•
•
•
•
•
•
•
“Eaters” that stop gliders and “glider guns” that shoot gliders can be deﬁned, and
glider guns are generated by the collision of gliders. Such self-organizing capa-
bilities mean that the Game of Life can be used to conﬁgure a universal Turing
machine. The fundamental logic gates (AND, OR, NOT) consist of glider rows
and disappearing reactions, and blocks of stable patterns are used as memory.
However, the number of cells needed in a self-organizing system is estimated to
be about 10 trillion (3 million×3 million). The size would be a square whose
sides are 3 km long, if 1 mm2 cells are used.
Consider a one-dimensional Game of Life, one of the simplest cellular au-
tomata. The sequence of cells in one dimension at time t is expressed as follows:
a1
t ,a2
t ,a3
t ,···
(1.1)
Here, each variable is either 0 (off) or 1 (on). The general rule used to determine
the state ai
t+1 of cell i at time t +1 can be written as a function F of the state at
time t as
ai
t+1 = F(ai−r
t
,ai−r+1
t
,··· ,ai
t,··· ,ai+r−1
t
,ai+r
t
)
(1.2)
Here, r is the radius, or range of cells that affects this cell.
For instance, a rule for r = 1,
ai
t+1 = ai−1
t
+ai
t +ai+1
t
(mod 2)
(1.3)
results in the determination of the next state as follows:
time t
:
0010011010101100
time t+1
:
*11111001010001*
An interesting problem is the task of ﬁnding the majority rule. The task is to
ﬁnd a rule that would ultimately end in a sequence of all 1 (0) if the majority of
the cells are 1 (0) with the minimum radius (r) possible for a one-dimensional
binary sequence of a given length. The general solution to this problem is not
known.
A famous example is a majority rule problem with length 149 and radius 3.
The problem is reduced to ﬁnding a function that assigns 1 or 0 to an input with
7 bits (= 3+1+3, radius of 3 plus itself); therefore, the function space is 227.
How can a cellular automaton (CA) obtain a solution to the majority
problem?

10
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
One method is to change the color (black or white) of a cell to the majority
of its neighboring cells. However, this method does not work well, as shown in
Fig. 1.4, because it results in a ﬁxed pattern divided into black and white.
In 1978, Gacs, Kurdyumov and Levin found the rules (GLK) regarding this
problem. Lawrence Davis obtained an improved version of these rules in 1995,
and Rajarshi Das proposed another modiﬁcation. There is also research to ﬁnd
effective rules through GAs (genetic algorithms) or GP (genetic programming).
The concept of Boolean functions is applied when GP is used. The ﬁtness value is
deﬁned by the percentage of correctly processed sequences out of 1000 randomly
generated sequences of length 149.
Rules determined by various methods are summarized in Table 1.2. Here, the
transition rules are shown from 0000000 to 1111111 in 128-bit form. In other
words, if the ﬁrst bit is 0,
F(000 0 000) = 0
(1.4)
Table 1.3 is a comparison of different rules. The rules obtained using GP were
very effective; reference [4] contains the details of this work.
Figure 1.5 shows how a CA obtained by GA can solve this problem well [75,
76]. The regions that were initially dominated by black or white cells become
regions that are completely occupied by either black or white cells. A vertical line
always exists at locations where a black region to the right meets a white region
to the right. In contrast, a triangular region with a chessboard pattern forms where
a white region to the right meets a black region to the right.
Figure 1.4: CA carrying out majority voting (Oxford University Press, Inc., [76]).
The two edges of the growing triangular region at the center with a chess-
board pattern grow at the same pace, progressing the same distance per unit time.
The left edge extends until it collides with a vertical boundary. The right edge
barely avoids the vertical boundary at the left (note that the right and left edges
are connected). Therefore, the left edge can extend for a shorter length, which
means that the length of the white region limited by the left edge is shorter than

Introduction
■
11
Table 1.2: Majority rules.
Name of rule (year)
Transition rules
GKL(1978)
00000000 01011111 00000000 01011111 00000000 01011111
00000000 01011111 00000000 01011111 11111111 01011111
00000000 01011111 11111111 01011111
Davis(1995)
00000000 00101111 00000011 01011111 00000000 00011111
11001111 00011111 00000000 00101111 11111100 01011111
00000000 00011111 11111111 00011111
Das(1995)
00000111 00000000 00000111 11111111 00001111 00000000
00001111 11111111 00001111 00000000 00000111 11111111
00001111 00110001 00001111 11111111
GP(1995)
00000101 00000000 01010101 00000101 00000101 00000000
01010101 00000101 01010101 11111111 01010101 11111111
01010101 11111111 01010101 11111111
Table 1.3: Performance in the majority problem.
Rule
Performance
Number of tests
GKL
81.6%
106
Davis
81.8%
106
Das
82.178%
107
GA
76.9%
106
GP
82.326%
107
Figure 1.5: Behavior of CA driven by GA (Oxford University Press, Inc., [76]).
the length of the black region limited by the right edge. The left edge disappears
at the collision point, allowing the black region to grow. Furthermore, the two
edges disappear at the bottom vertex and the entire lattice row becomes black,
showing that the correct answer was obtained.

12
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 1.6: Explanation of CA behavior from collision of particles (Oxford University Press,
Inc., [76]).
Melanie Mitchell analyzed the information processing structure on CA that
evolved through GA by using the behavior of dynamic systems [75, 76]. The
boundaries between simple regions (edges and vertical boundaries) are consid-
ered carriers of information, and information is processed when these boundaries
collide. Figure 1.6 shows only the boundaries in Fig. 1.5. These boundary lines
are called “particles” (similar to elementary particles in a cloud chamber used in
physics). The particles are represented by Greek letters following the tradition
in physics. Six particles are generated in this CA. Each particle represents a dif-
ferent type of boundary. For instance, η is the boundary between a black region
and a chessboard-patterned region. A number of collisions of particles can be
observed. For example, β + γ results in the generation of a new particle η, and
both particles annihilate in µ +η.
It is easy to understand how information is coded and calculated when the
behavior of CA is expressed in the language of particles. For instance, α and
β particles are coded with different information on the initial conﬁguration. γ
particles contain information that this is the boundary with a white region, and
a µ particle is a boundary with a white region. When a γ particle collides with
a β particle before colliding with a µ particle, this means that the information
carried by β and γ particles becomes integrated, showing that the initial large
white region is smaller than the initial large black region that shares a boundary.
This is coded into the newly generated η particle.
Stephen Wolfram6 systematically studied the patterns that form when dif-
ferent rules (eq. (1.2)) are used. He grouped the patterns generated by one-
dimensional CA into four classes.
6Stephen Wolfram (1959–): British theoretical physicist. At the age of 20, he obtained his Ph.D. de-
gree from California Institute of Technology for his research on theoretical physics. In 1987, he found
the company Wolfram Research. He released the mathematical software Mathematica, which is used for
research in natural sciences and numerous other ﬁelds.

Introduction
■
13
Class I All cells become the same state and the initial patterns disappear.
For example, all cells become black or all cells become white.
Class II The patterns converge into a striped pattern that does not change or
a pattern that periodically repeats.
Class III Aperiodic, chaotic patterns appear.
Class IV Complex behavior is observed, such as disappearing patterns or
aperiodic and periodic patterns.
(a) Class I
(b) Class II
(c) Class III
(d) Class IV
Figure 1.7: Examples of patterns.
Examples of these patterns are shown in Fig. 1.7. The following are the rules
behind these patterns (radius 1):
■
Class I: Rule 0
■
Class II: Rule 245
■
Class III: Rule 90
■
Class IV: Rule 110

14
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Here, transition rules are expressed in 223 bits from 000 to 111, and the number
of the rule is the decimal equivalent of those bits. For instance, the transition
rules for Rule 110 in Class IV are as follows [71]:
01101110binary = 2+22 +23 +25 +26 = 110decimal
(1.5)
In other words, 000, 100 and 111 become 0, and 001, 010, 011, 101, 110 become
1.
Rule 110 has the following interesting characteristics in computer science.
1. It is computationally-universal [126].
2. It shows 1/ f ﬂuctuation [83].
3. Its prediction is P-complete [82].
Kauffman and Packard proposed the concept of the “edge of chaos” from the
behavior of CA as discussed above. This concept represents Class IV patterns
where periodic patterns and aperiodic, chaotic patterns are repeated. The working
hypothesis in artiﬁcial life is “life on the edge of chaos.”

Chapter 2
AI, Alife and Emergent
Computation
It is intriguing that computer scientists use the terms genotype and phenotype
when talking about their programs.
—John Maynard Smith, [111]
2.1
Evolutionary Computation
2.1.1
What is Evolutionary Computation?
EAs (Evolutionary Algorithms) and EC (Evolutionary Computation) are meth-
ods that apply the mechanism of biological evolution to problems in computer
science or in engineering. EAs consider the adaptation process of organisms to
their environments as a learning process. Organisms evolve over a long period of
time by repeating the evolution process whereby species that do not adapt to the
environment become extinct and species that adapt thrive. This can be applied to
practical problems by replacing “environment” with “problem” or “information
that was learned” and “ﬁtness” with “goodness of the solution.”
The most important operators in EAs are selection and genetic operations.
Evolution of individuals does not happen if individuals that adapt better to
their environments are surviving but nothing else is happening. Mutation and
crossover by sexual reproduction result in the generation of a diverse range of in-
dividuals, which in turn promotes evolution. Selection is a procedure where good
individuals are selected from a population. Species that adapt better to the envi-

16
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
ronment are more likely to survive in nature. The selection procedure artiﬁcially
carries out this process.
The typical examples of EAs are Genetic algorithms (GA) and Genetic pro-
gramming (GP). They are the basic mechanisms for simulating complex systems.
Next sections describe these methods in detail with practical applications.
GAs have the following characteristics:
■
Candidate solutions are represented by sequences of characters.
■
Mutation and crossover are used to generate solutions of the next gener-
ation.
Elements that constitute GAs include data representation (genotype or phe-
notype), selection, crossover, mutation, and alternation of generation. The per-
formance of a search is signiﬁcantly inﬂuenced by how these elements are im-
plemented, as discussed below.
The data structure in GAs is either genotype (GTYPE) or phenotype
(PTYPE). The GTYPE structure corresponds to chromosomes of organisms (see
Fig. 2.1), and is a sequence representing a candidate solution (for example, a bit
sequence having a ﬁxed length). This structure is subject to genetic operations
such as crossover and mutation. The implementer can design how to convert can-
didate solutions into sequences. For instance, a GTYPE structure can be obtained
by conversion of a candidate solution into a sequence of integers that is then con-
catenated. On the other hand, PTYPE structures correspond to organisms, and
are candidate solutions obtained by interpreting GTYPE structures. The ﬁtness
values of candidate solutions are calculated for PTYPE structures.
The GA is based on the concept of Darwinian evolution, where individuals
who adapt better to their environments leave more offspring, while less ﬁt indi-
viduals are eliminated. Individuals that adapt to their environments are candidate
solutions that are better solutions to a problem, and the measure is the ﬁtness of
PTYPE structures.
The following selection methods have been proposed. In particular, the tour-
nament selection is frequently used because scaling is not necessary. In all meth-
ods, individuals that have higher ﬁtness are more likely to be selected.
■
Roulette selection
The roulette selection selects individuals with a probability in proportion
to their ﬁtness. This is the most general method in EAs, however, proce-
dures such as scaling are necessary to perform searches efﬁciently1.
■
Tournament selection
The tournament selection is widely used in EAs. This is a strategy in
which a certain number (called as tournament size) of individuals are
1See Steps 5 and 6 on page 5 for detailed example.

AI, Alife and Emergent Computation
■
17
Figure 2.1: GTYPE and PTYPE.
randomly selected from a population, and the best of these individuals is
ﬁnally chosen. This process is repeated until the size of the population is
reached.
■
Truncate selection
Individuals are sorted based on ﬁtness and the top Ps × M individuals
are selected in truncate selection, where M is the population size and Ps
is the selection rate. The selection pressure is very high, therefore, this
method is not used in standard GP, but is often used in the estimation
of distribution algorithm (EDA), which is an expansion of the GA. The
computation cost of this method besides the cost for sorting is very low.
The selection rate in roulette selection is determined by the absolute value
of the ﬁtness. However, the selection pressure may become too high or too low
with roulette selection in problems where the hierarchy of ﬁtness is important

18
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
but the absolute value is not. The tournament selection uses only the hierarchy of
ﬁtness, therefore, the above problem does not occur. The computational cost of
tournament selection is greater because many individuals are selected and ﬁtness
values are compared for the number of tournaments.
Selection signiﬁcantly inﬂuences the diversity of the population and the speed
of convergence, therefore, the choice of the selection algorithm and its param-
eters is very important. For instance, good solutions are often observed with a
small number of ﬁtness evaluations by using the tournament selection because
the selection pressure is very high with a large tournament size. However, the
calculations are more likely to quickly converge to and be trapped in an inappro-
priate local solution (i.e., local optimum).
A different strategy is the elitist selection (good individuals are always in-
cluded in the next generation). The ﬁtness of the best individual never decreases
in this strategy, with increasing number of generations if the environment against
which ﬁtness is measured does not change. However, using the elitist selection
too early in a search may result in a local solution, or premature convergence.
When reproduction occurs, the operators shown in Fig. 2.2 are applied to the
selected GTYPE in order to generate new GTYPE for the subsequent generation.
These operators are called GA operators. To keep the explanation simple, we
express the GTYPE as a one-dimensional array here. Each operator is analogous
to the recombination or mutation of a gene in a biological organism. Generally,
the frequency with which these operators are applied, as well as the sites at which
they are applied, are determined randomly.
Figure 2.2: GA operators.

AI, Alife and Emergent Computation
■
19
Crossover is an analogy of sexual reproduction where new offspring are gen-
erated by combining two parent individuals. There are a number of crossover
methods based on the level of granularity in separating each individual, for ex-
ample, the one-point crossover and the uniform crossover.
The crossover shown in Fig. 2.2 has one crossover point, so it is called a
one-point crossover. Below are some methods for performing the crossover op-
eration:
1. One-point crossover
2. Multi-point crossover (n-point crossover)
3. Uniform crossover
We have already explained the one-point crossover operation (Fig. 2.3(a)).
The n-point crossover operation has n crossover points, so if n = 1, this is equiv-
alent to the one-point crossover operation. With this crossover method, genes
are carried over from one parent alternately between crossover points. A case
in which n = 3 is shown in Fig. 2.3(b). Two-point crossovers, in which n = 2,
are often used. Uniform crossovers are a crossover method in which any desired
number of crossover points can be identiﬁed, so these are realized using a mask
for a bit string consisting of 0 or 1. First, let us randomly generate a character
string of 0s and 1s for this mask. The crossover is carried out as follows. Sup-
pose the two selected parents are designated as Parent A and Parent B, and the
offspring to be created are designated as Child A and Child B. At this point,
the genes for offspring Child A are carried over from Parent A when the cor-
responding mask is 1, and are carried over from Parent B when the mask is 0.
Conversely, the genes for offspring Child B are carried over from Parent A when
the corresponding mask is 0, and are carried over from Parent B when the mask
is 1 (Fig. 2.3(c)).
Mutation in organisms is considered to happen by mutation of nucleotide
bases in genes. The GA mimics mutation in organisms by changing the value of
a gene location (for example, changing 0 to 1 or 1 to 0). Mutation corresponds to
errors during copying of genes in nature, and is implemented in GAs by changing
a character in an individual after crossover (inversion of 0 and 1 in a bit sequence,
for instance). Using crossover generally only results in a search of combinations
of the existing solutions, therefore, using mutation to destroy part of the original
gene is expected to increase the diversity of the population and, hence, widen the
scope of the search. The reciprocal of the length of GTYPE structures is often
used as the mutation rate, meaning that one bit per GTYPE structure mutates on
average. Increasing the mutation rate results in an increase of diversity, but with
the tradeoff of a higher probability of destroying good partial solutions.

20
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a)
(b)
(c)
Figure 2.3: GA crossovers: (a) one-point crossover, (b) n-point crossover and (c) uniform
crossover.

AI, Alife and Emergent Computation
■
21
The ﬂow of a GA is as follows:
Step1 Randomly generate sequences (GTYPE) of the initial population.
Step2 Convert GTYPE into PTYPE and calculate ﬁtness for all individuals.
Step3 Select parents according to the selection strategy.
Step4 Generate individuals in the next generation (offspring) using genetic
operators.
Step5 Check against convergence criteria and go back to Step2 if not con-
verged.
Replacing parent individuals with offspring generated through operations
such as selection, crossover, and mutation to obtain the population of the next
generation is called the alternation of generation. Iterations ﬁnish when an in-
dividual with an acceptable ﬁtness is found or a predetermined number of gen-
erations have been produced. It is also possible to continue the calculations for
as long as possible if resources are available, and to terminate when sufﬁcient
convergence is achieved or further improvement of ﬁtness appears difﬁcult.
All individuals in the ﬁrst generation are randomly generated individuals in
EAs. EAs use genetic operators and therefore have less dependence on the initial
state in comparison to the hill-climbing method, but an extremely biased initial
population will decrease the performance of searches. Hence, the initial individ-
uals must be generated to distribute in the search space as uniformly as possible.
Figure 2.4 shows an example of a good initialization and a bad initialization. If
the triangle △in Fig. 2.4 is the optimal solution, the optimal solution is expected
to be found in a relatively short time from the initial state in Fig. 2.4(a) because
the initial individuals are relatively randomly distributed over the search space.
On the other hand, the initial distribution in Fig. 2.4(b) has fewer individuals near
the optimal solution and more individuals near the local optimal solution, there-
fore, it is more likely that the search will be trapped at the local optimal solution
and it will be difﬁcult to reach the optimal solution.
The initial individuals in GAs are determined by randomly assigning 0 or 1
in each gene location, and as the lengths of genes in GAs are ﬁxed, this simple
procedure can result in a relatively uniform distribution of initial individuals over
the solution space.

22
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Random initialization
(b) Nonuniform initialization
Figure 2.4: Examples of good and bad initialization. The triangles △indicate the optimal
solution.
2.1.2
Evolution Strategy
In Europe (especially in Germany), research groups with similar methodologies
to genetic algorithm (GA) have been active for a long time. This method has
been called Evolution Strategy (referred to as “ES,” hereinafter), with Ingo Re-
ichenberg as the central ﬁgure. In the initial stage, ES differed from GA in the
following two points [74]:
1. It mainly uses mutation as an operator.
2. It deals with real-valued representation.
Each individual of ES is represented as a set of real number vectors, such as
(−→x ,−→
σ ), where −→x represents a position vector in a space of search while −→
σ
represents a standard deviation vector. The mutation is represented as follows:
−→x t+1 = −→x t +N(−→0 ,−→
σ ),
(2.1)
where N(−→0 ,−→
σ ) is a random number created according to a Gaussian distribution
of the average −→0 (i.e., zero vector) and the standard deviation −→
σ .
At an early stage of ES, a search is performed using a group composed of
single individuals. In this case, the offspring that results from a mutation (−→x t+1
in eq. (2.1)) is adopted as a member of the new group only if the adaptability is
improved compared to that of its parent (−→x t). That is, it will become a parent for
the next generation.
As opposed to GA, ES is not affected by crossover. Therefore, a quantita-
tive investigation is not difﬁcult and has resulted in the achievement of robust
mathematical analysis related to different phenomena, such as the effect of the
mutation rate [97, 108]. For instance, the convergence theorem has been proved
[9]. Furthermore, The 1
5 rule had been proposed to optimize the convergent rate.

AI, Alife and Emergent Computation
■
23
The 1
5 Rule
If the successful mutation ratio2 is larger (smaller) than 1
5, increase (de-
crease) −→
σ .
Actually, by observing the successful mutation ratio of the past k generation
ϕ(k), it was determined that the following equation controls the mutation:
−→
σ
t+1 =





cd ×−→
σ
t,
if ϕ(k) < 1/5,
ci ×−→
σ
t,
if ϕ(k) > 1/5,
−→
σ
t,
if ϕ(k) = 1/5.
(2.2)
Particularly in reference [108], cd = 0.82 and ci = 1/0.82 were adopted. The
intuitive meaning of this rule is that “If it brings success, continue to search with
longer steps. Otherwise, shrink them.”
Eventually, ES has been expanded as a search method using a group which
consists of plural individuals. In this case, a crossover operator, average operator
(calculates the mean of two-parent vectors), and other kinds of vector manipula-
tion have been adopted in addition to the previously described mutation operator.
Furthermore, what makes it different from GA is that ES takes the following two
types of methods as the selection method:
1. (µ +λ)−ES
A parent group comprised of µ individuals generates a child of λ indi-
viduals. Select µ individuals as parents of the next generation among
the groups of the total (µ +λ) individuals.
2. (µ,λ)−ES
A parent group comprised of µ individuals generates a child of λ
individuals (on condition that µ < λ). Select µ individuals as a parent
of the next generation among groups of λ individuals.
Generally speaking, (µ,λ)−ES is considered to be superior in an environment
which changes with the progression of time, and when noise is present.
ES has, thus, been applied to various optimization problems [70], and to those
other than real values [47]. Refer to the reference materials [74, 10] for additional
information on ES and its comparison with GA.

24
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
2.1.3
Multi-objective Optimization
In life, we often wish to pursue two or more mutually conﬂicting goals (Fig.
2.5). In ﬁnance, return and risk are generally conﬂicting objectives. In assets
management and investment, return is the proﬁt gained through management
and risk is the uncertainty and amplitude of ﬂuctuations. Risk can be gauged
from price volatility (the coefﬁcient of variation). High volatility means large
price ﬂuctuations. Ordinary volatility is generally expressed in terms of standard
deviation.
In general, risk and return are in a “trade-off” relationship. High-return, low-
risk ﬁnancial products would be ideal, but things are not actually that easy. Usu-
ally, the choice is one of the following two:
■
Low risk, low return : The possibility of loss of the principal is low, but
management returns will be small.
■
High risk, high return: Losses may occur, but any returns might be large.
In considering the optimization problem, so far, we have limited the discus-
sion to maximization (or minimization) of a single objective (i.e., the ﬁtness
function). As in risk versus return, however, it is often necessary to consider
optimization when there are multiple objectives, that is, “multi-objective opti-
mization.”
Let us begin with an example [36]. Assume you are engaged in transport
planning for a town. The means of reducing trafﬁc accidents range from in-
stalling trafﬁc lights, placing more trafﬁc signs, and regulating trafﬁc, to setting
up checkpoints (Fig. 2.6). Each involves a different cost, and the number of trafﬁc
accidents will vary with the chosen approach. Let us assume that ﬁve means (A,
B, C, D, and E) are available, and that the cost and the predicted accident num-
bers are:
A
=
(2,10)
B
=
(4,6)
C
=
(8,4)
D
=
(9,5)
E
=
(7,8),
where the ﬁrst element is the cost and the second is the predicted accident num-
ber, as plotted in Fig. 2.6. The natural impulse is to desire attainment of both
goals in full: The lowest cost and the lowest predicted accident number. Unfortu-
nately, it is not necessarily possible to attain both objectives by the same means
and, thus, not possible to optimize both at the same time.
In such situations, the concept of “Pareto optimality” is useful. For a given
developmental event to represent a Pareto optimal solution, it must be the case

AI, Alife and Emergent Computation
■
25
Figure 2.5: Risk vs returns.
Figure 2.6: Cost vs expected numbers of accidents.
that no other developmental events exist which are of equal or greater desirability,
for all evaluation functions, that is, ﬁtness functions.
Let us look again at Fig. 2.6. Note that the points in the graph increase in
desirability as we move toward the lower left. A, B, and C in particular appear to
be good candidates. None of these three candidates is the best in both dimensions,

26
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
that is, in both “evaluations,” but for each there is no other candidate that is better
in both evaluations. Such points are called “non-dominated” points. Points D and
E, in contrast, are both “dominated” by other points and therefore less desirable.
E is dominated by B, as B is better than E in both evaluations:
Cost of B(4)
<
Cost of E(7)
Predicted accidents for B(6)
<
Predicted accidents for E(8)
D is similarly dominated by C. In this example, therefore, the Pareto optimums
are A, B, and C. As this suggests, the concept of the Pareto optimum cannot be
used to select just one candidate from a group of candidates, and, thus, it cannot
be concluded which of A, B, and C is the best.
Pareto optimality may be deﬁned more formally as follows. Let two points
x = (x1,··· ,xn) and y = (y1,··· ,yn) exist in an n-dimensional search space, with
each dimension representing an objective (an evaluation) function, and with the
objective being a minimization of each to the degree possible. The domination
of y by x (written as x <p y) may be therefore be deﬁned as
x <p y ⇐⇒(∀i)(xi ≤yi)∧(∃i)(xi < yi).
(2.3)
In the following, we will refer to n (the number of different evaluation func-
tions) as the “dimension number.” Any point that is not inferior to any other point
will be called “non-dominated” or “non-inferior,” and the curve (or curved sur-
face) formed by the set of Pareto optimal solutions will be called the “Pareto
front.”
On this basis, it is possible to apply GA to multi-objective optimization. In
the following example, we use the VEGA (Vector Evaluated Genetic Algorithms)
system of Schaffer et al. [105, 106]. Selection with VEGA is performed as fol-
lows (see Fig. 2.7), where n is the evaluation dimension number (the number of
different evaluation functions).
Generation
Generation
Parents
Genotype
Multi-objective ﬁtnesses
Shuﬄe
GA operation
Selection of 
     subsets
based on
each ﬁtness
valule
Figure 2.7: VEGA (vector evaluated Genetic Algorithms) algorithm.

AI, Alife and Emergent Computation
■
27
1. Let the n subsets be Subi (i = 1,··· ,n).
2. In Subi, retain the individual selected only by evaluation function i.
3. Mix Sub1,Sub2,··· ,Subn and shufﬂe.
4. Produce the next-generation offspring, using the genetic operator on
these sets.
Note that the selection is based upon every evaluation function (i.e., the function
value in every dimension). In contrast, reproduction is performed not for each
subset but rather for the complete set. In other words, crossover is performed
for the individuals in Subi and Subj(i ̸= j). In this way, we preserve the superior
individuals in each dimension and at the same time select the individuals that are
superior to the average in one or more dimensions.
Let us now consider the VEGA operation, as applied to optimization of the
binary function:
F21(t)
=
t2,
F22(t)
=
(t −2)2,
where t is the only independent variable. The graphical appearance of this func-
tion is shown in Fig. 2.8 (which is called as a “Pareto map”). The VEGA objec-
tive is to select non-dominated points, such as those shown in the ﬁgure.
Figure 2.8: Pareto maps.

28
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Generation 0
(b) Generation 3
Figure 2.9: Results of multi-objective optimization.
Figure 2.9 shows the results for generations 0 and 3. The population size is set
to 30 individuals in each dimension, and a crossover ratio of 0.95 and a mutation
ratio of 0.01 is applied. As shown, the VEGA can effectively ﬁnd a front (a Pareto
front) containing no dominated points. However, it has lost several intermediate
points.
VEGA selection, as may be seen in this example, involves a problem that may
be described as follows. The selection pressure (in biological terms, the degree of
selectivity applied to the survival and proliferation of the organism’s population)
on the optimum values (A and C in Fig. 2.6) in at least one dimension (evaluation
function) works as desired. In the unlikely event that a utopian individual (an
individual superior in all dimensions) exists, it may be possible to ﬁnd it through
genetic operation on the superior parents in only one dimension. In most cases,
however, no utopian individual exists. It is then necessary to obtain the Pareto
optimal points, some of which are intermediate in all dimensions (B in Fig. 2.6).
In an ideal GA, it is desirable to have the same selection pressure applied on
all of these, and, thus, in Fig. 2.6 on all three of A, B, and C. With VEGA selec-
tion, however, an intermediate point such as B cannot survive. In effect, species
differentiation occurs within the population, with dimension-speciﬁc superiority
of each species. The danger of this is more prevalent when the Pareto optimal re-
gion is convex rather than concave (see Fig. 2.10). Two modiﬁcations have been
proposed in order to resolve this difﬁculty. One is to apply an excess of heuristic
selection pressure on the non-dominant individuals in each generation. The other
is to increase interspecies crossbreeding. Although ordinary GA selection is ran-
dom, this is effective because utopian individuals are more readily produced by
interspecies than intraspecies’ crossover.

AI, Alife and Emergent Computation
■
29
(a) Concave
(b) Convex
Figure 2.10: Pareto fronts.
2.2
How to Make a Bit? – Exploration vs Exploitation
In this section, let us consider how to deal with gambling. This issue is concerned
not only with gambling, but it is also very important in artiﬁcial intelligence and
economics.
So, in the ﬁrst place, what is gambling? One of the simplest bets, which we
will take into consideration, is the two-armed bandit machine3. Let us consider a
slot machine with two levers (R, L) as is presented in Fig. 2.11. This problem is
deﬁned as follows:
7
7 7
7
7
7
A r m  L :
Reward  Avg.
SD.
A r m  R :
Reward  Avg.
SD.
Figure 2.11: A slot machine with two levers.
3In English, it is referred to as two-lever slot machine problem. The name of the problem comes from
the fact that slot machine gambling can wind up player’s money (extort money from player).

30
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Two-Armed Bandit Problem
1. The payoff is paid according to the different payment rate (payoff rate)
for R and L. Each of these has averages µR,µL, and variances σ 2
R,σ 2
L.
2. It is not known beforehand whether µR > µL or µR < µL
3. The total number of bets is set to be N.
Under these conditions, what is the best way to bet on arms R and L?
The multi-armed bandit problem is deﬁned similarly, as M-lever slot machine
(M ≥0).
Now, it is necessary to consider two phases:
Exploration Determine if µR > µL or µR < µL.
Exploitation According to the exploration, bet on the lever with the better
payoff rate.
This problem falls into a dilemma because N is ﬁnite. Too much exploration
(investigation) leads to a local search, which is weak to noise and will be more
difﬁcult to adapt to the error (variance) of the payoff rate. On the other hand, if it
is exploited too much, information gained from exploration will be ignored and
the chance to earn money will be lost.
In recent years, the UCT algorithm has been proposed, which is popularly
used for Monte Carlo tree search in games. In this algorithm, the following UCB
(Upper Conﬁdence Bound) value is employed for the approximate solution of
multi-armed bandit problem in order to decide on promising action:
UCB value
=
Number of times when the lever of interest won
Number of playouts allocated to the lever of interest
+
c
s
2log(Number of playouts)
Number of playouts allocated to the lever of interest
Where c is a user deﬁned constant. Second term of this equation is a play-out4
and the smaller it becomes, the bigger the term grows. On the other hand, the ﬁrst
4Randomly choosing a legal hand to play the game and to end it.

AI, Alife and Emergent Computation
■
31
term becomes smaller with the worsening of the action. Therefore, if the number
of play-outs is small and unluckily the game is lost, the evaluation value is de-
creased, which, in fact, avoids the problem of not playing the game even though
there are promising actions left. UCB value can be thought of as evaluation value
expressing trade-off between exploration (the second term of 2.4) and exploita-
tion (the ﬁrst term). In other words, it evaluates which levers are good and which
ones are bad, at the same time trying to use good levers (i.e., levers with better
payoff rates) for more play-outs. By using the UCB value, it is possible to make
an optimal selection under speciﬁc conditions5.
Let us actually experiment with the slot machine. Here, we observe the to-
tal payoff money (i.e., reward) acquired by selecting and pulling each lever of
the two-lever and ﬁve-lever slot machines, assuming that 1,000 coins are given.
However, winnings will not be used again (that is, it ends with 1,000 trials).
Let us vary the average and variance of the payoff money of each lever. Let
i = 1,2,··· denote levers, and the total payoff for each lever i is Qn(i) for the
slot turned n times (after choosing one lever). ni is the number of times lever i
has been pulled. Of course, n1 + n2 + ··· = n. Let r j(i) be a payoff for pulling
i-th lever for j-th time (note that 0 ≤j ≤ni). Also, from the above deﬁnition,
Qn(i) = Pni
j=1 rj(i) holds.
The strategy to compare is as follows:
1. Choose lever randomly
2. Greedy method: Choose the lever i with the highest average payoff Qn(i)
n
to pull next.
3. ε–Greedy method: Same as greedy method, but lever is chosen randomly
with the probability ε. In this experiment, probability was chosen to be
ε = 0.01.
4. UCB1 method: The following is used as the UCB value. However, ni is
the number of times lever i has been pulled so far.
UCB value
=
Qn(i)
ni
+
r
2logn
ni
5. Revised UCB1 method: The following is used as the UCB value.
UCB correction value
=
Qn(i)
ni
+
r
clogn
ni
5In n trials, the loss generated from choosing the best lever for each time is suppressed to the value
O(log(n)). However, constraints such as the payoff money being in the range of [0,1] are imposed. It has
been shown that, under this condition, no algorithm can exceed the UCB value [6].

32
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
However,
c
=
min
"
1
4, ˆσ 2
i (n)+
r
2logn
ni
#
,
where ˆσ 2
i (n) is an estimated variance. For example, as an unbiased esti-
mate of the population variance, it is calculated as follows:
ˆσ 2
i (n)
=
1
ni −1
ni
X
j=1

r j(i)−Qn(i)
ni
2
This method practically has a better performance than the UCB1 method,
but it does not have a theoretical guarantee.
Let us ﬁrst consider when there are two levers. The average of each payoff is
set to be 0.10, 0.05, and the variance for both payoffs is set to be 0.20. The pay-
offs are normally distributed with this mean and variance, according to Gaussian
distribution (i.e., normal distribution). A typical example of execution is shown
in Fig. 2.12. In this case, the total number of betting N (maximum number of
trials) is set to 1,000. (a) is the total reward (i.e., the sum of payoffs) for each
trial. Eventually, UCB1 and revised UCB1 are shown to give good results. On
the other hand, greedy method and ε–greedy method are almost the same as a
random selection. This indicates that the selection is confused by the variance
value and is unable to choose the optimal lever (see Fig. 5.8(b)). It becomes
more apparent if the number of levers is increased to 10. Figure 2.13 shows the
experimental result with the average payoff of 10 levers being set to: 0.10 0.05
0.05 0.05 0.02 0.02 0.02 0.01 0.01 0.01, and variance of all payoffs is set to 0.20.
In this case, only the ﬁrst lever is shown to be optimal. As can be seen in Fig.
2.13, after exceeding 200 trials both UCB1 and revised UCB1 accurately select
the optimal lever. In contrast, the greedy method achieves grades lower than ran-
dom selection. Table 2.1 shows the average value after repeating the experiment
200 times. The best-performing method is highlighted in gray. The advantages
of UCB1 and revised UCB1 are presented in the table.
On the other hand, UCB1 is not necessarily always good. Depending on con-
ditions, greedy method can be superior. For example, Table 2.2 and Table 2.3 are
experimental results with the average payoff and its variance when the number
of levers is 2 and 5, respectively (number of bets N = 1,000, repetition number
50). In most cases, when the values of the variance and the average payoff are
the same, results of greedy methods are improved. Also, greedy methods seem
to ﬁnd good results, if the average payoff value is biased and extremely good.
If the average value is the same, then random and greedy methods work well in
contrast with UCB1, which sometimes cannot fully learn the variance. However,
there is not much of a difference in results in any of the methods.

AI, Alife and Emergent Computation
■
33
For another type of payoff, consider the Bernoulli type (see section 4.2.1).
This is a method in which the probability µi is deﬁned for each lever i that
pays 1.0 with probability µi and does not provide compensation with probability
1−µi. This remuneration can be thought of as a click model of online advertise-
ments. In this case, each lever becomes an advertisement, and a user browsing an
advertisement is equal to experimenting with a slot machine. In addition, payoffs
are equivalent to clicks, and it becomes the problem of selecting an advertise-
ment maximizing the total clicks. The results are presented in Table 2.4. In such
a situation, the advantage of ε–greedy method stands out. It is also worth noting
that the results of a simple greedy method are not that good. UCB1 is superior to
the simple greedy method, but results were not as good as for normal distribution
type payoff. This seems to reﬂect that there is no variance in the winnings.
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0
100
200
300
400
500
600
700
800
900
1000
#. of trials
Random
Revised UCB1
UCB1
Greedy
εGreedy
Reward
(a) Transition of acquired rewards.
0
100
200
300
400
500
600
700
800
900
1000
#. of trials
Random
Random UCB1
UCB1
Greedy
εGreedy
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Selection prob.
(b) Transition of probability to select the optimal lever (no. 1).
Figure 2.12: Two-lever slot machine. The payoffs are normally distributed.

34
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
0
100
200
300
400
500
600
700
800
900
1000
#. of trials
Random
Revised UCB1
UCB1
Greedy
εGreedy
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Reward 
(a) Transition of acquired rewards.
0
100
200
300
400
500
600
700
800
900
1000
#. of trials
Random
Revised UCB1
UCB1
Greedy
εGreedy
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Selection prob.
(b) Transition of probability to select the optimal lever (no. 1).
Figure 2.13: Ten-lever slot machine. The payoffs are normally distributed.
Let us summarize the above results. It is surprising that the greedy method,
which is the simplest procedure, has achieved good results with many problems.
Since the UCB1 method is sensitive to the variance of the payoff, it can cope with
high variance payoffs. The UCB1 method has slower convergence than other
methods, but the obtained results are better. The UCB1 method is superior when
few levers or high variance payoffs are considered, however, results degrade as
the number of levers increases. Also, when the payoff is of the normal distribu-

AI, Alife and Emergent Computation
■
35
Table 2.1: Experimental results with normally distributed payoffs. Variance values are set to
be the same. The total rewards are shown with different methods. The best-performing method
is highlighted in gray.
#. of levers
2
10
Payoff average
0.10 0.05
0.10 0.05 0.05 0.05 0.02
0.02 0.02 0.01 0.01 0.01
Payoff variance
0.20
0.20
Random
75.335725
34.405275
Greedy method
91.970039
67.387339
ε–Greedy method
85.579438
73.442488
UCB1
94.317254
77.593354
Revised UCB1
95.036724
85.177174
Table 2.2: Experimental results with normally distributed payoffs. The number of levers is
two. The total rewards are shown with different methods. The best-performing method is high-
lighted in gray.
Payoff average
0.0, 5.0
0.0, 5.0
0.0, 5.0
5.0, 5.0
Payoff variance
10.0, 40.0
40.0, 10.0
10.0, 10.0
40.0, 10.0
High risk
Low risk
Same variance
Same average
high return
high return
Random
2382.234993
2466.161786
2501.014429
5033.966115
Greey method
3590.420261
4544.600735
4745.431955
4995.128383
ε–Greedy method
3098.290123
4783.067266
4744.443555
5015.799637
UCB1
4304.636240
4821.753778
4760.705372
5006.838874
Revised UCB1
4478.568354
4807.838191
4776.344278
4996.441438
Table 2.3: Experimental results with normally distributed payoffs. The number of levers is
ﬁve. The total rewards are shown with different methods. The best-performing method is high-
lighted in gray.
Payoff average
0.0,2.0,2.0,
0.0,2.0,2.0,
1.0,1.0,1.0,
1.1,1.2,1.3,
1.0,1.0
1.0,6.0
0,1.0
1.4,1.5
Payoff variance
10.0,1.0,2.0
10.0,1.0,2.0
1.0,2.0,3.0
1.0,1.0,1.0
1.0,2.0
1.0,2.0
4.0,5.0
1.0,1.0
Different variances
Different averages
Same average
Same variance
Random
1200.164188
2189.213874
999.646251
1299.583166
Greedy method
1828.578583
5595.396152
1002.104703
1367.964301
ε–Greedy
method
1895.622982
1968.515276
1000.613158
1123.039962
UCB1
1930.462162
1919.132855
997.706195
1119.840872
Revised UCB1
1929.266064
1962.263051
999.876270
1118.288329

36
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 2.4: Experimental results with payoffs according to Bernoulli distribution. Variance
values are set to be the same. The total rewards are shown with different methods. The best-
performing method is highlighted in gray.
#. of levers
2
10
Payoff average
0.10 0.05
0.10 0.05 0.05 0.05 0.02
0.02 0.02 0.01 0.01 0.01
Random
74.405
33.735
Greedy method
90.31
72.62
ε–Greedy method
100.39
100.1
UCB1
95.635
79.965
Revised UCB1
94.96
81.805
tion type, the results are better than for other distributions (e.g., Bernoulli type).
This is because the best lever is well separated from the other levers in the normal
distribution.
The slot machine problem has been extensively studied in the ﬁelds of statis-
tical decision theory and adaptive control [11]. For example, John Holland6 used
the slot machine problem to create a mathematical model describing how genetic
algorithms (GA) allocate individuals into a schema7 [51]. This is one of the theo-
retical foundations of evolutionary computation. Holland proposed, through an-
alytical analysis, one optimal strategy for two-lever slot machine problem. This
method exponentially increases the probability of choosing a good lever com-
pared with the probability to try out the wrong lever, based on an information
obtained by trial. This is also true for schema sampling in GA. Holland’s schema
theorem claims, that the sub-optimal schema is sampled implicitly, resulting in
an optimization of the online evaluation.
Evolutionary economics is a ﬁeld that studies economics based on complex
adaptive systems and explains economic activities from the perspective of evo-
lutionary theories and biology. Keywords of this ﬁeld are increasing returns and
path dependency [5]. In economics, it refers to positive feedback, or so-called
winning horse effect8. Economist Brain Arthur thought that the economy is a
complex system operated by interactions of numerous agents. In conventional
economics, it was common sense that the equilibrium between demand and sup-
6John Henry Holland (1929–2015): American scientist. He is known for his pioneering research on
complex systems and nonlinear systems. In 1975, he published the epoch-making book “Adaptation in
Natural and Artiﬁcial Systems” on genetic algorithms, which made him the father of that area. Holland’s
famous schema theorem is the basis of GAs.
7A partial structure maintained by a group of genes. Exploration in GA and GP proceeds by combining
schemas by crossover.
8Also known as the bandwagon effect (see Fig. 2.14). A phenomenon that strengthens the support for
the strategy through the information on the prevalence of the strategy. In the ﬁnancial market, it points at a
phenomenon of the exchange rates moving abnormally in one direction as a result of people piggybacking
on the ﬂow of the market.

AI, Alife and Emergent Computation
■
37
Figure 2.14: Bandwagon effect.
ply was kept in balance by Adam Smith’s “Invisible hand of God,” and that the
economy stabilizes accordingly9. However, it is clear that this trend does not hold
if trends in recent stock prices and ﬁnance are analyzed. There is no equilibrium
point, and a certain trigger can make it all (the whole market) to move in one
direction, and it may not be possible to stop it. This is the increasing returns.
In addition, the path dependence of the technology selection means that the
selection of a certain technology is determined by chance irrespective of the su-
periority or inferiority of the technology itself. Once the technology was chosen
(decided on)10, it cannot be changed anymore. It is so because, even if the draw-
backs of the technology are revealed, the migration cost is too high.
One of the well-known examples is the QWERTY keyboard layout [22]. The
top left row of the English notation keyboard we usually use is QWERTY. This
is not optimal for typing English words. Why does it look like this?
A common belief is that the early stage of the typewriter (around the end
of the 19th century) was designed to reduce the speed of typing to prevent the
collision of arms. At that time the delicate machine arms broke by interfering
when typing speed was too fast. Therefore, the frequently used E, A, and S were
deliberately placed in positions that were hard to type.
In response to this, in the 1930s, August Dvorak from the University of Wash-
ington developed a DSK keyboard (the Dvorak Simpliﬁed Keyboard), that could
be used to type more efﬁciently (Fig. 2.15(a)). As a more rational layout, more
frequently used characters were placed around the home position11 to eliminate
excess ﬁnger movement. In fact, there was also a report that the DSK layout had
a higher learning rate and more than 20% faster typing rate after mastering [87].
Figure 2.15(b) and (c) presents the awkward hand movement, taken from the
video recording when actually typing a keyboard (the height of the ﬁgure shows
9Adam Smith (1723–1790): British economist. He organized the free market economy theory and is
known as the father of classical economics.
10This is referred to as lock-in.
11Initial ﬁnger placement. Normally in the QWERTY arrangement, they are ASDF (left hand: From
pinky ﬁnger towards the index ﬁnger) and JKL; (right hand: From index ﬁnger towards the pinky).

38
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Enter
Shift
Ctrl
Alt
Ctrl
Tab
Caps Lock
!
1
＠
2
#
3
$
4
%
5
^
6
&
7
*
8
(
9
)
0
{
[
}
]
“
<
,
>
.
?
+
|
\
:
;
ー
-
Shift
Backspace
/
=
~、
‘
P
Y
F
G
C
R
L
A
O
E
U
I
D
H
T
N
S
Q
J
K
X
B
M
W
V
Z
Alt
(a) DSK arrangement
(b) QWERTY ﬁnger movement
(c) DSK ﬁnger movement
Figure 2.15: Dvorak Simpliﬁed keyboard.
the frequency). The DSK layout is about one-tenth of the QWERTY arrange-
ment.
However, since the present layout was proposed in 1882, QWERTY arrange-
ment is the only one winning. Why is this? The reason that can be thought of,
is that the QWERTY side trained typists by creating a typing school. As a re-
sult, many companies hired QWERTY trained typists. On the other hand, it is
expensive to retrain typist to use DSK layout keyboards. This fact encouraged
production of the QWERTY layout typewriters and gave birth to its users. The
Apple II series, launched by Apple in 1984, came with a built-in switch to change
between QWERTY and DSK layout keyboards, but it had no effect.
In this way, if one is chosen on the spur of the moment, even if it is not known
whether it is really good or not, it may not be possible to stop anymore due to
avalanche-like proﬁt growth. The slot machine problem is used to mathemati-
cally analyze this kind of selection phenomena.
2.3
Wireworld: A Computer Implemented as a Cellular
Automaton
Wireworld was invented by Brian Silverman in 1987 (the following description
was based on the document [29]). This is a kind of cellular automaton, which
is suitable for simulation of electronic logic circuits. Rules are simple, similarly

AI, Alife and Emergent Computation
■
39
to the game of life, but it is shown to be Turing complete. In the wireworld, the
following four states are used for the cell:
■
Empty
■
Electron head
■
Electron tail
■
Conductor
“Electrons” are generated from two adjacent cells, electron tail and electron head.
The transition rules of each of the states are as follows:
■
empty ⇒empty
■
electron head ⇒electron tail
■
electron tail ⇒conductor
■
conductor ⇒electron head if exactly one or two of the neighbouring cells
are electron heads, otherwise remains conductor.
In the wireworld, a logic circuit can be constructed with these simple rules.
To draw a conductor in the cell space, place a conductor on the line and
enclose it in a background (empty) cells. To let electrons ﬂow through the con-
ductor, replace the ﬁrst cell of the conductor with electron tail and the second cell
with electron head. Then, electrons will move along the conductor, according to
transition rules deﬁned above. In every step in the wireworld clock (one cycle),
the conductor cell in front of the electron changes to the electron head cell. The
electron head cell changes to electron tail, and the electron head cell returns to
the conductor cell.
Figure 2.16 shows the operation of the cell diode. In the ﬁgure, empty cells
are white (or are omitted). In this circuit, electrons are allowed to ﬂow only in one
direction. Note the behavior of the rectangle part of 2 × 3. Only the center cell
on one side of the rectangle is the background cell, creating an empty space in
the conductor. Electrons entering from the side without empty space are divided
into two and pull the trigger of the conductor cell on the other side of the empty
space. Electrons entering from the opposite side are divided into three, but the
conductor cell surrounded by the three electron head cells is not ignited, thus,
cannot propagate through the conducting wire. In the ﬁgure, it is shown that
electrons ﬂow to the right, but not to the left.
Figure 2.17 shows how to construct basic logic circuits (NOT, OR, AND,
XOR). These circuits have electron inputs on the left, and the output conductor on
the right. A loop-shaped conductor is placed at the input. Electrons (arrangement
of the electronic tail and electronic head) placed into the loop of the input part

40
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 2.16: Cell diode.
Figure 2.17: Basic logics (NOT, OR, AND, XOR).
continue to circle around in the loop. Electrons passing through the branching
part are split into two and enter the input line of the logic. Note that the input
frequency depends on the size of the loop.
Let us see the NOT operation (Fig. 2.18). Electron entering from the left loop
reaches the NOT part on the right. Here, electrons are emitted from the top-right,
and usually go out from the output on the bottom right. Note that, when there is
a NOT input (electron from the left loop), it cancels out with top-right electron
and nothing is output to the output line on the bottom-right. In this way, when
the input is 1 the output is 0, and when the input is 0 the output becomes 1, thus,
realizing NOT.
Further, in the OR circuit (Fig. 2.19), when electrons come from the left of
one or both input the lines, they are sent out rightward. A diode-like circuit is
necessary at the merging part so that the electrons coming from one input cannot
ﬂow back to another input.

AI, Alife and Emergent Computation
■
41
Figure 2.18: NOT operation.
Figure 2.19: OR operation.
We also tried to simulate a circuit realizing a ﬂip-ﬂop (Fig. 2.20). This circuit
is implemented by combining AND, NOT and OR. When a signal comes from
the bottom input, the state in the ﬂip-ﬂop becomes 1, and the signal is output
from the ﬂip-ﬂop every sixth cycle. When a signal comes from above, the state
of the ﬂip-ﬂop is reset.
Furthermore, Fig. 2.21 shows a circuit realizing 3-bit binary adder. This cir-
cuit performs both input and output from the lower bit. In the ﬁgure, (101)2 = 5
and (110)2 = 6 are set as inputs to the top and bottom inputs, respectively. As
Color version at the end of the book

42
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 2.20: Flip Flop operation.

AI, Alife and Emergent Computation
■
43
Figure 2.21: ADDER operation.
a result of execution, (1011)2 = 11 was obtained as output, thus, it can be con-
ﬁrmed that addition is performed correctly.
2.4
Langton’s Ant
Langton’s ants is a puzzle invented by Christopher Langton12, who is well known
for his work in the area of artiﬁcial life.
12Christopher Langton (1949–): American computer scientist. In the late 1980’s he coined the term
Artiﬁcial Life and held the international conference “International Conference on the Synthesis and Sim-
ulation of Living Systems” (commonly known as Artiﬁcial Life I) at Los Alamos National Laboratory.

44
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
In this puzzle, squares on a plane (referred to as a “cell”) are colored in either
black or white. We then arbitrarily identify one square as the “ant.” The ant can
travel in any of the four cardinal directions at each step taken13.
The movement of the “ant” is according to the following rules:
■
Upon moving onto a black square, turn 90◦to the left.
■
Upon moving onto a white square, turn 90◦to the right.
■
Change the color of the previous square of the ant from white to black or
vice versa after moving to a new square.
Figure 2.22 shows the movement of an ant. In the ﬁgure, the ant faces to either
the north, south, west or east. At the next time step, the ant travels according to
the ﬁgure on the right. Since any color ﬁts into the gray cells, the color should
not be changed once it is determined.
Even with these simple rules, the ant demonstrates astonishingly complex
movement. Figure 2.23 shows the ant’s movement starting on a white cell in a
torus lattice with dimensions of 80 by 80 (connecting bottom to up and right to
left). A total of 10,184 steps formed a “highway” (regular pattern path) as shown
in (a). Subsequently, a total of 15,094 steps (b), 34,476 steps (c) and 44,803 steps
(d) also resulted in the formation of a highway14. After observing approximately
100,000 steps, the highway formation no longer existed.
Jim Propp discovered an interesting behavior of an ant in an indeﬁnite space
(not a torus) as summarized below:
■
The ﬁrst few hundred of steps create beautiful symmetric patterns.
■
After an additional 10,000 steps, the continuously built path becomes
considerably chaotic.
■
After a further 104 steps, a cycle whereby the ant crosses over two squares
repeats indeﬁnitely. Thus, a diagonal “highway” is established.
That is, the ants start the movement of creating a straight “highway” with
no exception after wandering around for about 10,000 steps, although they show
random movement at the beginning. This outcome does not depend on how the
pattern was initially made. This implies that the “highway” is an attractor of
Langton’s ants.
Langston’s ants can also be considered as a cellular automaton (see section
1.3). In this case, the background of the lattice is colored in black or white, and
the ant has eight different color states based on the different combinations of
current direction and background color.
13See section 3.1.1 for more precise simulation of ant colonies.
14Note that a highway bounces back due to the torus shaped space.

AI, Alife and Emergent Computation
■
45
Figure 2.22: Movement of an ant.
(a)
(b)
(c)
(d)
Figure 2.23: Ant trail (starting with a white map).

46
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
When the ant starts in a black rectangle, the following facts are observed:
■
It builds “castle” including straight walls and complex projections.
■
It destroys and rebuilds castles repeatedly in an interesting way as if it
has a certain objective.
■
It ﬁnally wanders around building a highway as if it has lost any interests.
Figure 2.24 shows some experimental results. It was observed that the ant
initially built a “castle” and then a highway, starting with an initial state (a). The
ﬁrst highway has been built based on 9,337 steps from the beginning. Although
it seems contrary to intuition, a highway is completed much faster compared to
starting in only the white map (Fig. 2.23).
Figure 2.25 shows the case starting with a black wall. A highway was built
with 5,035 steps (b), 10,968 steps and 28,992 steps, from the initial state (a).
When the start occurs with a black triangle, it has been observed that it cycli-
cally moves along with the leg of a triangle, and builds a castle wall along with
the base, as observed in the case of starting with a rectangle. A highway was
built with 3,816 steps (b) and 7,797 steps (c) when starting with the initial state
(a) shown in Fig. 2.26.
There are several unsolved problems associated with this puzzle (not a torus
version), which are as follows:
■
If the number of black squares is ﬁnite, is a highway built without fail
irrespective of the initial pattern?
■
Does an ant leave the area without fail after a long period of time elapses
without being caught in the limited area?
■
Does an ant leave the area along a highway without exception?
(a)
(b)
Figure 2.24: Ant trail (starting with a white map a black box).

AI, Alife and Emergent Computation
■
47
(a)
(b)
Figure 2.25: Ant trail (starting with a white map a black wall).
(a)
(b)
(c)
Figure 2.26: Ant trail (starting with a triangle).
Of the three proposed questions, afﬁrmative evidence has been acquired with
respect to the second. We shall examine these notions with various types of initial
maps, and the remaining unsolved problems shall be investigated.


Chapter 3
Meta-heuristics
When you were a tadpole and I was a ﬁsh In the Paleozoic time,
And side by side on the ebbing tide We sprawled through the ooze and slime, ...
—Langdon Smith, 1858–1908, Evolution
3.1
Ant Colony Optimization (ACO)
3.1.1
Collective Behaviors of Ants
Ants march in a long line. There is food at one end, a nest at the other. This is a
familiar scene in gardens and on roads, but the sophisticated distributed control
by these small insects was recognized by humans only a few decades ago.
Ants established their life in groups, or colonies, more than a hundred million
years before humans appeared on Earth. They formed a society that handles com-
plex tasks such as food collection, nest building, and division of labor through
primitive methods of communication. As a result, ants have a high level of ﬁtness
among species, and can adapt to harsh environments. New ideas, including rout-
ing, agents, and distributed control in robotics, have developed based on simple
models of ant behavior. Applications of the ant behavior model have been used
in many papers, and are becoming a ﬁeld of research rather than a fad.
Marching is a cooperative ant behavior that can be explained by the
pheromone trail model (Fig. 3.1). Cooperative behavior is frequently seen in
ant colonies, and has attracted the interest of entomologists and behavioral sci-
entists. Pheromones are volatile chemicals synthesized within the insect, and
are used to communicate with other insects of the same species. Examples are
sex pheromones that attract the opposite sex, alarm pheromones that alert group
members, and trail pheromones that are used in ant marches.

50
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 3.1: Ant trail.
1
2
food
nest
12.5cm
nest 
food 
Figure 3.2: Bridge-shaped paths (adapted from Fig. 1a in [37], Springer-Verlag GmbH).
However, recent research indicates that pheromones are effective within a
distance of only about 1m from the female. Therefore, it is still not known if
males are attracted only because of the pheromones.
Many species of ants leave a trail of pheromones when carrying food to
the nest. Ants follow the trails left by other ants when searching for food.
Pheromones are volatile matter that is secreted while returning from the food
source to the nest. Experiments shown in Fig. 3.2 by Deneubourg and Goss us-
ing Argentine ants linked this behavior to the search for the shortest path [37].
They connected bridge-shaped paths (two connected paths) between the nest and
the food source, and counted the number of ants that used each path. This seems
like a simple problem, but because ants are almost blind they have difﬁculty
recognizing junctions, and cannot use complex methods to communicate the po-
sition of the food. Furthermore, all the ants must take the shorter path to increase
the efﬁciency of the group. Ants handle this task by using pheromones to guide
the other ants.
Figure 3.3 shows the ratio of ants that used the shorter path [37]. Almost
every ant used the shorter path as time passed. Many of the ants return to the
shorter path, secreting additional pheromones; therefore, the ants that followed
also take the shorter path. This model can be applied to the search for the shortest
path, and is used to solve the traveling salesman problem (TSP) and routing

Meta-heuristics
■
51
㪇
㪈㪇
㪉㪇
㪊㪇
㪋㪇
㪌㪇
㪍㪇
㪎㪇
㪏㪇
㪐㪇
㪈㪇㪇
㪇㪄㪉㪇
㪉㪇㪄㪋㪇㪋㪇㪄㪍㪇㪍㪇㪄㪏㪇㪏㪇㪄㪈㪇㪇
㪩㪸㫋㫀㫆㩷㫆㪽㩷㪸㫅㫋㫊㩷㫋㪿㪸㫋㩷㫌㫊㪼㪻㩷㫋㪿㪼㩷㫊㪿㫆㫉㫋㪼㫉㩷㫇㪸㫋㪿㩷㩿㩼㪀
㪫㫀㫄㪼㩷㫇㪸㫊㫊㪼㪻
Figure 3.3: The ratio of ants that used the shorter path (adapted from Fig. 1a in [37], Springer-
Verlag GmbH).
of networks. There are many unknown factors about the pheromones of actual
ants; however, the volatility of pheromones can be utilized to build a model that
maintains the shortest path while adapting to rapidly changing trafﬁc. The path
with a greater accumulation of pheromones is chosen at junctions, but random
factors are inserted to avoid inﬂexible solutions in a dynamic environment.
3.1.2
Simulating the Pheromone Trails of Ants
An easy model can describe the actions of ants as follows:
■
In case of nothing, random search is done.
■
If the food is found, take it back to the hive. Homing ant knows the
position of the hive, and returns almost straight back.
■
Ants that take the food back to hive drop their pheromone.
Pheromones are volatile.
■
Ants not having the food have the habit of being attracted to the
pheromone.
Figure 3.4 shows the simulation of the pheromone trails. Here, the hives are
placed in the center, and there are three (lower right, upper left, lower left) food
sources. (a) is the ﬁrst random search phase. In (b), the closer lower right and
lower left food is found, and the pheromone trail is formed. Upper left is in the
middle of formation. In (c), pheromone trails are formed for all three sources, and
makes the transport more efﬁcient. The lower right source is almost exhaustively
picked. In (d), the lower right food source ﬁnishes, and the pheromone trail has
already dissipated. As a result, a vigorous transportation for the two sources on

52
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a)
(b)
(c)
(d)
Figure 3.4: Pheromone trails of ants.
the left is being done. After this, all the sources ﬁnish, and the ants returns to
random search again.
3.1.3
Generating a Death Spiral
Death spiral refers to a strange condition of the ants whereby they move endlessly
akin to drawing circles, due to their attraction to pheromones released by other
ants. Once they get lost without knowledge of the intended direction, they simply
continue to move in circles, based on the pheromones of their company that are
released from their backside. This spiral march does not stop, and they move
continuously. Gradually, they weaken and die. The rest of them keep moving
without taking a rest, stepping on the dead bodies, are eventually exhausted and
ﬁnally die.
Although it has not been clariﬁed why ants form the death spiral, it has been
inferred that a group of lost ants may cause a death spiral. Therefore, we at-
tempted to cause ants to form a death spiral by adding the following two phe-
nomena to the simulation of the ant search previously described;
■
During normal searching, their colony suddenly disappears, which does
not allow feeding ants to return to their colony. As a result, they continue
to release pheromones.
■
Ants cannot grasp the location of their colony. Feeding ants show such
an abnormal behavior as heading slightly off the position of the colony.

Meta-heuristics
■
53
It has been reported that the death spiral can be caused by electromagnetic
waves generated by smartphones, etc. The hypothesis is that some external stim-
ulus hinders ants from grasping the correct location of their colony, resulting in
them continuing to head to an incorrect position. Based on this point, the second
item has been adopted. That is, we have set a position at a certain distance from
the colony which lies on a line perpendicular to one connecting the current posi-
tion of the ants and the actual position of the colony as an incorrect destination.
This means that ants seeking their colony head for a position shifted by a certain
distance from the actual position of the colony.
Figure 3.5 shows the results of the simulations described above. In (a), the
ﬁgure shows the normal search of ants, and a pheromone trail is built. Next, we
destroyed the colony at point (b). Consequently, many of the ants strayed, as
shown in (c). After a while, what appears to be a spiral was recognized for a
while (d). With the progression of time, the ants formed dumplings in (e). In (f),
when they continued to chase pheromones after losing their colony, they appear
to have come together in one spot instead of moving in a spiral.
(a)
(b)
(c)
(d)
(e)
(f)
Figure 3.5: Death spirals.
3.1.4
ACO using a Pheromone Trail Model
Optimization algorithms based on the collective behavior of ants are called ant
colony optimization (ACO) [30].
ACO using a pheromone trail model for the TSP uses the following algorithm
to optimize the travel path:

54
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Step1 Ants are placed randomly in each city.
Step2 Ants move to the next city. The destination is probabilistically deter-
mined based on the information on pheromones and given conditions.
Step3 Repeat until all cities are visited.
Step4 Ants that make one full cycle secrete pheromones on the route ac-
cording to the length of the route.
Step5 Return to Step1 if a satisfactory solution has not been obtained.
The ant colony optimization (ACO) algorithm can be outlined as follows.
Take ηi j as the distance between cities i and j. The probability pk
ij(t) that an ant
k in city i will move to city j is determined by the reciprocal of the distance 1/ηij
and the amount of pheromone τij(t) as follows:
pk
ij(t) =
τij(t)×ηα
ij
P
h∈Jk
i τih(t)×ηα
ih
.
(3.1)
Here, Jk
i is the set of all cities that the ant k in city i can move to (has not visited).
The condition that ants are more likely to select a route with more pheromone
reﬂects the positive feedback from past searches, as well as a heuristic for search-
ing for a shorter path. The ACO can thereby include an appropriate amount of
knowledge unique to the problem.
The pheromone table is updated by the following equations:
Q(k)
=
the reciprocal of the path that the ant k found
(3.2)
∆τi j(t)
=
X
k∈Ai j
Q(k)
(3.3)
τi j(t +1)
=
(1−ρ)·τij(t)+∆τij(t)
(3.4)
The amount of pheromone added to each path after one iteration is inversely
proportional to the length of the paths that the ants found (eq. (3.2)). The results
for all ants that moved through a path are reﬂected in the path (eq. 3.3)). Here, Aij
is the set of all ants that moved on a path from city i to city j. Negative feedback
to avoid local solutions is given as an evaporation coefﬁcient (eq. (3.4)), where
the amount of pheromone in the paths, or information from the past, is reduced
by a ﬁxed factor (ρ).
The ACO is an effective method for solving the traveling salesman problem
(TSP) compared to other search strategies. Table 3.1 shows the optimized values
for four benchmark problems and various minima found using other methods

Meta-heuristics
■
55
(smaller is better, obviously) [30]. The numbers in brackets indicate the num-
ber of candidates investigated. The ACO is more suitable for this problem com-
pared to methods such as genetic algorithm (GA, section 2.1.1) simulated an-
nealing (SA) and evolutionary programming (EP). However, it is inferior to the
Lee-Kernighan method (the current TSP champion code). The characteristic that
specialized methods perform better in static problems is shared by many meta-
heuristics (high-level strategies which guide an underlying heuristic to increase
their performance). Complicated problems, such as TSPs where the distances be-
tween cities are asymmetric or where the cities change dynamically, do not have
established programs and the ACO is considered to be one of the most promising
methods.
The length and pheromone accumulation of paths between cities are stored
in a table (Fig. 3.6). Ants can recognize information on their surroundings, and
probabilistically decide the next city to visit. The amount of pheromones added
to each path after every cycle is inversely proportional to the length of the cycle
path.
Table 3.1: Comparison between ACO and metaheuristics.
TSP
ACO
GA
EP
SA
Optimal
Oliver 30
420
421
420
424
420
[830]
[3200]
[40,000]
[24,617]
Eil 50
425
428
426
443
425
[1,830]
[25,000]
[100,000]
[68,512]
Eil 75
535
545
542
580
535
[3,480]
[80,000]
[325,000]
[173,250]
KroA 100
21,282
21,761
N/A
N/A
21,282
[4,820]
[103,00]
[N/A]
[N/A]
j1
j2
j3
i
 !t
ij
 
ij
!
1
〒㔌
䊐䉢䊨䊝䊮㊂
amount of pheromone
distance
Figure 3.6: Path selection rules of ants.

56
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
3.2
Particle Swarm Optimization (PSO)
3.2.1
Collective Behavior of Boids
Many scientists have attempted to express the group behavior of ﬂocks of birds
and schools of ﬁsh, using a variety of methods. Two of the most well-known
of these scientists are Reynolds and Heppner, who simulated the movements of
birds. Reynolds was fascinated by the beauty of bird ﬂocks [98], and Heppner,
a zoologist, had an interest in ﬁnding the hidden rules in the instantaneous stops
and dispersions of ﬂocks [46]. These two shared a keen understanding of the
unpredictable movements of birds; at the microscopic level, the movements were
extremely simple, as seen in cellular automata, while at the macroscopic level,
the motions were very complicated and appeared chaotic. This is what is called
an “emergent property” in the ﬁeld of Artiﬁcial Life (Alife). Their model places
a very large weight on the inﬂuence of individuals on each other. Similarly, it is
known that an “optimal distance” is maintained among individual ﬁsh in a ﬁsh
school (see Fig. 3.7).
This approach is probably not far from the mark as the basis for the social
behavior of groups of birds, ﬁsh, animals and, for that matter, human beings. The
sociobiologist E.O.Wilson made the interesting suggestion that the most useful
information to an individual is what is shared from other members of the same
group. This hypothesis forms the basis for the PSO method, which will be ex-
plained in section 3.2.2.
The collective behavior of a ﬂock of birds emphasized the rules for keeping
the optimum distance between an individual and its neighbors.
Figure 3.7: Do the movements of a school of ﬁsh follow a certain set of rules? (@Coral Sea
in 2003).

Meta-heuristics
■
57
The CG video by Reynolds features a group of agents called “boids.” Each
boid moves according to the sum of three vectors: (1) force to move away from
the nearest individual or obstacle, (2) force to move toward the center of the
ﬂock, and (3) force to move toward its destination. Adjusting coefﬁcients in this
summation results in many behavioral patterns. This technique is often used in
special effects and videos in ﬁlms. Figure 3.8 (simple behavior of ﬂocks) and
Fig. 3.9 (situation with obstacles) are examples of simulations of boids.
(a)
(b)
(c)
(d)
Figure 3.8: Simple behavior of boids ((a)⇒(b)⇒(c)⇒(d)).
(a)
(b)
Figure 3.9: Boids in a situation with obstacles ((a)⇒(b)).

58
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
The following are the details of the algorithm that boids follow. Many indi-
viduals (boids) move around in space, and each individual has a velocity vector.
The three factors below result in a ﬂock of boids.
1. Avoid collision: Attempt to avoid collision with nearby individuals.
2. Match pace: Attempt to match velocity of nearby individuals.
3. Move to center: Attempt to be surrounded by nearby individuals.
Each boid has an “optimum distance” to avoid collision, and behaves so as
to maintain this distance with its nearest neighbor [102]. Collision becomes a
concern if the distance between nearby boids becomes shorter than the “optimum
distance.” Therefore, to avoid collision, each boid slows down if the nearest boid
is ahead, and speeds up if the nearest boid is behind (Fig. 3.10).
Smaller than “optimal 
distance”
front boid:
speed up
rear boid:
slow down
Figure 3.10: Avoid collision (1).
Greater than
“optimal distance”
front boid:
slow down
rear boid:
speed up
Figure 3.11: Avoid collision (2).
The “optimum distance” is also used to prevent the risk of straying from the
ﬂock. If the distance to the nearest boid is larger than the “optimum distance,”
each boid speeds up if the nearest boid is ahead, and slows down if it is behind
(Fig. 3.11).
Here, “ahead” and “behind” are deﬁned as ahead or behind a line that crosses
the boid’s eyes and is perpendicular to the direction in which the boid is moving
(Fig. 3.12). Boids try to move parallel to (with the same vector as) their nearest
neighbor. Here, there is no change in speed. Furthermore, boids change velocity
so as to always move toward the center of the ﬂock (center of gravity of all boids).

Meta-heuristics
■
59
ahead
behind
Figure 3.12: Ahead or behind a line that crosses the boid’s eyes.
nearest boid
center of gravity
i-th boid
Figure 3.13: Updating the velocity vector.
In summary, the velocity vector (⃗vi(t)) of the i-th boid is updated at time t as
follows (see Fig. 3.13):
⃗vi(t) =⃗vi(t −1)+ ⃗
Nexti(t −1)+ ⃗Gi(t −1).
(3.5)
Where
⃗
Nexti(t −1) is the velocity vector of the nearest boid to individual i, and
⃗Gi(t −1) is the vector from individual i to the center of gravity. The velocity at
one step before, i.e., ⃗vi(t −1), is added to take inertia into account.
Each boid has its own ﬁeld of view (Fig. 3.14) and considers boids within its
view when ﬁnding the nearest neighbor. However, the coordinates of all boids,
including those out of view, are used to calculate the center of gravity.
Kennedy et al. designed an effective optimization algorithm using the mech-
anism behind boids [62]. This is called particle swarm optimization (PSO), and
numerous applications are reported. The details are provided in section 3.2.2.

60
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
boids within the field
of view
Figure 3.14: Each boid has its own ﬁeld of view.
3.2.2
PSO Algorithm
The classic PSO was intended to be applied to optimization problems. It sim-
ulates the motion of a large number of individuals (or “particles”) moving in a
multi-dimensional space [62]. Each individual stores its own location vector (⃗xi),
velocity vector (⃗vi), and the position at which the individual obtained the highest
ﬁtness value (⃗pi). All individuals also share information regarding the position
with the highest ﬁtness value for the group (⃗pg).
As generations progress, the velocity of each individual is updated using the
best overall location obtained up to the current time for the entire group and the
best locations obtained up to the current time for that individual. This update is
performed using the following formula:
⃗vi = χ(ω⃗vi +φ1 ·(⃗pi −⃗xi)+φ2 ·(⃗pg −⃗xi))
(3.6)
The coefﬁcients employed here are the convergence coefﬁcient χ (a random
value between 0.9 and 1.0) and the attenuation coefﬁcient ω, while φ1 and φ2 are
random values unique to each individual and the dimension, with a maximum
value of 2. When the calculated velocity exceeds some limit, it is replaced by a
maximum velocity Vmax. This procedure allows us to hold the individuals within
the search region during the search.
The locations of each of the individuals are updated at each generation by the
following formula:
⃗xi =⃗xi +⃗vi
(3.7)
The overall ﬂow of the PSO is as shown in Fig. 3.15. Let us now consider
the speciﬁc movements of each individual (see Fig. 3.16). A ﬂock consisting of
a number of birds is assumed to be in ﬂight. We focus on one of the individuals
(Step 1). In the ﬁgure, the ⃝symbols and linking line segments indicate the
positions and paths of the bird. The nearby ◦⃝symbol (on its path) indicates
the position with the highest ﬁtness value on the individual’s path (Step 2). The
distant ◦⃝symbol (on the other bird’s path) marks the position with the highest

Meta-heuristics
■
61
Figure 3.15: Flow chart of the PSO algorithm.
Figure 3.16: In which way do birds ﬂy?

62
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
-7/2<=
-4/2<=
-
,9163621 <2.;05684 :968=
-7/2<=
-4/2<=
-
,9163621 <2.;05684 :968=
+282;.=698
Figure 3.17: Concept of searching process by PSO with Gaussian mutation.
ﬁtness value for the ﬂock (Step 2). One would expect that the next state will
be reached in the direction shown by the arrows in Step 3. Vector 1⃝shows the
direction followed in the previous steps; vector 2⃝is directed towards the position
with the highest ﬁtness for the ﬂock; and vector 3⃝points to the location where
the individual obtained its highest ﬁtness value so far. Thus, all these vectors,
1⃝, 2⃝and 3⃝, in Step 3 are summed in order to obtain the actual direction of
movement in the subsequent step (see Step 4).
The efﬁciency of this type of PSO search is certainly high because focused
searching is available near optimal solutions in a relatively simple search space.
However, the canonical PSO algorithm often gets trapped in local optimum in
multimodal problems. Because of that, some sort of adaptation is necessary in
order to apply PSO to problems with multiple sharp peaks.
To overcome the above limitation, a GA-like mutation can be integrated with
PSO [48]. This hybrid PSO does not follow the process by which every indi-
vidual of the simple PSO moves to another position inside the search area with a
predetermined probability without being affected by other individuals, but leaves
a certain ambiguity in the transition to the next generation due to Gaussian mu-
tation. This technique employs the following equation:
mut(x) = x×(1+gaussian(σ)),
(3.8)
where σ is set to be 0.1 times the length of the search space in one dimension.
The individuals are selected at a predetermined probability and their positions
are determined at the probability under the Gaussian distribution. Wide-ranging
searches are possible at the initial search stage and search efﬁciency is improved
at the middle and ﬁnal stages by gradually reducing the appearance ratio of Gaus-
sian mutation at the initial stage. Figure 3.17 shows the PSO search process with
Gaussian mutation. In the ﬁgure, Vlbest represents the velocity based on the local
best, i.e. ⃗pi −⃗xi in Eq. (3.6), whereas Vgbest represents the velocity based on the
global best, i.e. ⃗pg −⃗xi.

Meta-heuristics
■
63
3.2.3
Comparison with GA
Let us turn to a comparison of the performance of the PSO with that of the GA
using benchmark functions, in order to examine the effectiveness of PSO.
For the comparison, F8 (Rastrigin’s function) and F9 (Griewangk’s function)
are employed. These are deﬁned as:
F8(x1,x2)
=
20+x2
1 −10cos(2πx1)+x2
2 −10cos(2πx2)−(−5.11 ≤xi ≤5.11)
F9(x1,x2)
=
1
4000
2
X
i=1
(xi −100)2 −
2
Y
i=1
cos
xi −100
√
i

+1(−10 ≤xi ≤10)
Figure 3.18 and Fig. 3.19 show the shapes of F8 and F9, respectively. F8 and F9
seek the minimum value. F8 contains a large number of peaks so its optimization
is particularly difﬁcult.
 5
 2.5
0
2.5
5
 5
 2.5
0
2.5
5
0
20
40
60
80
5
 2.5
0
2.5
Figure 3.18: Rastrigin’s function (F8).
10
 5
0
5
10
 10
 5
0
5
10
5
6
7
0
 5
0
5
Figure 3.19: Griewangk’s function (F9).

64
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Comparative experiments were conducted with PSO and GA using the above
benchmark functions. PSO and GA were repeatedly run 100 times. Search space
ranges for the experiments are listed in Table 3.2. PSO and GA parameters are
given in Table 3.3.
The performance results are shown in Figs. 3.20 and 3.21 which plot the ﬁt-
ness values against the generations. Table 3.4 shows the averaged best ﬁtness
values over 100 runs. As can be seen from the table and the ﬁgures, the combi-
nation of PSO with Gaussian mutation allows us to achieve a performance that
is almost equal to that of the canonical PSO for the unimodals, and a better per-
formance than the canonical PSO for the multimodals. The experimental results
with other benchmark functions are further discussed in [53].
PSO is a stochastic search method, as are GA and GP, and its method of
adjustment of ⃗pi and ⃗pg resembles crossover in GA. It also employs the concept
of ﬁtness, as in evolutionary computation. Thus, the PSO algorithm is strongly
related to evolutionary computation (EC) methods.
However, PSO has certain characteristics that other EC techniques do not
have. GA operators directly operate on the search points in a multi-dimensional
search space, while PSO operates on the motion vectors of particles which, in
turn, update the search points (i.e., particle positions). In other words, GA oper-
ators are position speciﬁc and the PSO operators are direction speciﬁc. One of
the reasons PSO gathered so much attention was the tendency of its individuals
to proceed directly towards the target.
Table 3.2: Search space for test functions.
Function
Search space
F8
−65.535 ≤xi < 65.536
F9
−10 ≤xi ≤10
Table 3.3: PSO and GA parameters.
Parameters
PSO,PSO with Gaussian
Real-valued GA
Population
200
200
Vmax
1
Generation
50
50
φ1,φ2
upper limits = 2.0
Inertia weight
0.9
Crossover ratio
0.7(BLX-α)
Mutation
0.01
0.01
Elite
0.05
Selection
tournament (size=6)

Meta-heuristics
■
65
1e-005
0.0001
0.001
0.01
0.1
1
10
0
5
10
15
20
25
30
35
40
45
50
log(Fitness)
Generation
original Rastrigin function
GA
PSO
PSO with Gaussian
Figure 3.20: Standard PSO versus PSO with Gaussian mutation for F8.
1e-006
1e-005
0.0001
0.001
0.01
0.1
0
5
10
15
20
25
30
35
40
45
50
log(Fitness)
Generation
generalized Rastrigin function
GA
PSO
PSO with Gaussian
Figure 3.21: Standard PSO versus PSO with Gaussian mutation for F9.
In the chapter “The Optimal Allocation of Trials” in his book [51], Holland
ascribes the success of EC on the balance of “exploitation,” through search of
known regions, with “exploration,” through search, at ﬁnite risks, of unknown
regions. PSO is adept at managing such subtle balances. These stochastic fac-
tors enable PSO to make thorough searches of the relatively promising regions
and, due to the momentum of speed, also allows effective searches of unknown
regions. Theoretical research is currently underway to derive optimized values
for PSO parameters by mathematical analysis, for stability and convergence
(see [18, 63]).

66
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 3.4: Average best ﬁtness of 100 runs for experiments.
Gen
GA
PSO
PSO with gaussian
F8
1
4.290568
3.936564
3.913959
10
0.05674
0.16096
0.057193
20
0.003755
0.052005
0.002797
30
0.001759
0.037106
0.000454
40
0.001226
0.029099
0.000113
50
0.000916
0.02492
3.61E-05
F9
1
0.018524
0.015017
0.019726
10
0.000161
0.000484
0.000145
20
1.02E-05
0.000118
1.43E-05
30
3.87E-06
6.54E-05
4.92E-06
40
2.55E-06
5.50E-05
2.04E-06
50
1.93E-06
4.95E-05
1.00E-06
3.2.4
Collective Memory and Spatial Sorting
Boid algorithm is simple and the emergence of group actions can be easily re-
alized, however, one of the problems is that it is impossible to control group
behavior. In this section, we consider creating more realistic Boid.
For this purpose, the concept of collective memory has been introduced [21].
This concept considers that the past records of group structure affect interactions
among individuals. The basic principles of this model are as follows:
■
Rule1
All individuals always maintain a minimum distance from each other
(avoidance behavior). This action has the highest priority and is also
observed in the actions of real animals.
■
Rule2
If Rule1 is not carried out, individuals tend to be attracted to other
individuals and lined up with neighboring individuals. It is related to
isolation avoidance.
Each boid’s surroundings can be modeled as shown in Fig. 3.22. Here, the tar-
get individual is drawn in the shape of the bird, and its surroundings are divided
from the closest one as follows:
■
Repulsion: Two individuals in this zone repel each other (abbr. Zor = zone
of repulsion)
■
Orientation: Two individuals in this zone are lined up in the same direc-
tion (abbr. Zoo = zone of orientation)

Meta-heuristics
■
67
Figure 3.22: Modeling of surroundings.
Figure 3.23: Local interaction.
■
Attraction: Two individuals in this zone are attracted to each other (abbr.
Zoa = zone of attraction)
There is a blind spot behind oneself. Interactions between individuals are shown
in Fig. 3.23.
To realize the above Rule1 and Rule2 with local rules, the update rule is set
as follows. The update is performed for each individual i. The position vector of
individual i is⃗ci and the velocity vector is⃗vi.
■
When there are individuals in Zor (zone of repulsion):
The next direction to move is,
⃗dr(t +τ) = −
nr
X
j̸=i
⃗rij(t)
|⃗rij(t) |.
(3.9)
Vector⃗ri j = ⃗cj−⃗ci
|⃗cj−⃗ci| is a unit vector in the direction of an individual j. This
is of the highest priority. Thus, ⃗di(t +τ) = ⃗dr(t +τ).
■
When there are no individuals in Zor (zone of repulsion):
First derive the following vectors:
■
Against Zoo:
⃗do(t +τ) =
no
X
j=1
⃗vj(t)
|⃗vj(t) |,
(3.10)

68
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
Against Zoa:
⃗da(t +τ) =
na
X
j̸=i
⃗rij(t)
|⃗rij(t) |.
(3.11)
The vectors are summed except for the blind spot.
Then the direction of the movement is calculated in the following way:
■
When individuals exist only in Zoo:
⃗di(t +τ) = ⃗do(t +τ)
(3.12)
■
When individuals exist only in Zoa:
⃗di(t +τ) = ⃗da(t +τ)
(3.13)
■
When individuals exist in both:
⃗di(t +τ) = 1
2(⃗do(t +τ)+ ⃗da(t +τ))
(3.14)
■
When there are no individuals in any of them:
⃗di(t +τ) =⃗vi(t)
(3.15)
Note that the Zoo (zone of orientation) changes with time.
Finally, the following adjustments are made.
■
Move ⃗di(t +τ) by a random angle based on the Gaussian distribution.
■
Each individual i is redirected by a vector ⃗vi(t + τ) = ⃗di(t + τ). How-
ever, when the maximum rotation angle θτ is exceeded, it is set to be this
maximum angle.
■
The movement speed of each individual is set to be constant s.
The parameters of the simulations are presented in Table 3.5. Units represent
the scale of the length of the speciﬁc organism of interest. For example, it is very
small for insects, and other parameters are scaled appropriately.
Figure 3.24 shows the experimental results under various conditions. The
chosen parameter values are: N = 100,rr = 1,α = 270,θ = 40,s = 3 and σ =
0.05. Collective behavior is classiﬁed in the following four cases:
■
(A) swarming
■
(B) torus

Meta-heuristics
■
69
Table 3.5: Summary of model parameters.
Parameter
Unit
Symbol
Values explored
Number of individuals
None
N
10–100
Zone of repulsion
Units
rr
1
Zone of orientation
Units
∆ro(ro −rr)
0–15
Zone of attraction
Units
∆ra(ra −ro)
0–15
Field of perception
Degrees
α
200–360
Turning rate
Degrees per second
θ
10–100
Speed
Units per second
s
1–5
Error (std. deviation)
Degrees (rad)
σ
0–11.5(0–0.2 rad)
Time step increment
Seconds
τ
0.1
0
2
4
6
8
10
12
14
ra
0
2
4
6
8
10
12
14
ro
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
pgroup
0
2
4
6
8
10
12
14
ra
0
2
4
6
8
10
12
14
ro
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
mgroup
a
a
b
b
c
c
d
d
e
z
(A)
)
D
(
)
C
(
(B)
(E)
(F)
Figure 3.24: The collective behaviours exhibited by the model [21]: (A) swarming, (B) torus,
(C) dynamic parallel group, (D) highly parallel group. Also shown are the group polarization
pgroup (E) and angular momentum mgroup (F) as a function of changes in the size of the zone
of orientation ∆ro and zone of attraction ∆ra.

70
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
(C) dynamic parallel group
■
(D) highly parallel group
The lower part shows values (average of 30 runs) of the group polarization
pgroup (E) and angular momentum mgroup (F) as a function of the change in the
size of the zone of orientation and zone of attraction (∆ro and ∆ra). Parameter
domain, presented in the ﬁgure from a to d, corresponds to collective behavior
presented above. Region e is the division action, occurring with at least 50%
probability.
The group polarization pgroup (E) and angular momentum mgroup (F) are the
global properties of the model and are deﬁned as follows:
pgroup(t)
=
1
N |
N
X
i=1
⃗vi(t) |
(3.16)
mgroup(t)
=
1
N |
N
X
i=1
⃗ric(t)×⃗vi(t) |
(3.17)
⃗ric
=
⃗ci −⃗cgroup
(3.18)
⃗cgroup(t)
=
1
N
N
X
i=1
⃗ci(t)
(3.19)
Group polarization increases as the degree of alignment between individuals in a
population is increased. On the other hand, angular momentum is a measurement
of the degree of rotation of the center of gravity of the population and is the sum
of the angular momenta of individuals around the center of gravity.
From the results of these ﬁgures, it can be observed that the behavior of the
group is as follows:
■
Swarming: Population by aggregation (an aggregate with cohesion) are
observed. On the other hand, low-level divisions and parallel alignment
among the members can also be seen. This means a low degree of soli-
darity (pgroup) and a low angular momentum (mgroup). This indicates that
the individuals take the action of repulsion and attraction, but they occur
when they are not parallel (Region a of the ﬁgure).
■
Torus: An individual continues to revolve around the center forever. The
direction of rotation is random. The value of pgroup is low, but mgroup is
high. This happens when the ∆ro is relatively small and ∆ra is relatively
large (Region b).
■
Dynamic parallel group: This group shows high pgroup and low mgroup.
This kind of population is more mobile than swarming or torus.
This phenomenon occurs for medium ∆ro and a medium or higher ∆ra
(Region c).

Meta-heuristics
■
71
(a) Swarming
(b) Torus (ring-doughnut)
(c) Directed Shoal
Figure 3.25: Different sizes of Zoo.
■
Highly parallel group: When ∆ro increases, the population undergoes
self-organization and transitions into highly aligned state (low mgroup)
with linear motion (low mgroup) (Region d).
We present the results of a simulation using the above method. This simu-
lation is created based on Boid. As mentioned earlier, each Boid has three radii
and one angle. Those are radius RZor representing the range of the Repulsion,
RZoo representing the range of the Orientation, RZoa representing the range of the
attraction and viewing angle (ﬁeld of perception) αp. Radii are deﬁned to satisfy
the following inequalities:
0 ≤RZor ≤RZoo ≤RZoa.
(3.20)
In the region where RZoo is small, the swarming state in which Boids stay in
place, without moving in parallel or rotating in the same direction, was observed
(Fig. 3.25(a)). This is shown in Fig. 3.26(a). It can be seen that three swarms
are created in the upper left, lower left and lower right. Those groups are closer
to each other because the edges of the screen are connected, however, swarms
occasionally exchanged Boids without being combined.
In the intermediate region of RZoo (i.e., where RZoo is of middle size), it was
observed that Boids circled around the empty core in the same direction (see
Fig. 3.25(b) and Fig. 3.26(b)). In the ﬁgure, a large torus is formed on the left
side. Torus is unstable, and often takes the form of swarming or parallel group
after some time.
When RZoo is relatively big, the Boids have been observed to form a group
and move in parallel (see Fig. 3.25(c) and Fig. 3.26(c)). On the right side of the
ﬁgure, it can be seen that the Boids became a group and move in parallel. If RZoo
is small, the resulting parallel group quickly self-destructs, becoming a swarm,
and repeats to become a parallel group again. On the other hand, if RZoo is large,
the formed parallel group continues to move in a certain direction without self-
destructing.
3.2.5
Boids Attacked by an Enemies
Let us realize the escape behavior or evasive maneuver in Boids. This is equiva-
lent to the behavior of small ﬁsh running away from a big enemy. In the natural

72
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Swarming (Rzor = 2,Rzoa = 16,Rzoo = 3)
(b) Torus (ring-doughnut) (Rzor = 2,Rzoa = 16,Rzoo = 7)
(c) Parallel (Rzor = 2,Rzoa = 30,Rzoo = 10)
Figure 3.26: Simulation results with collective memory and spatial sorting. Time lapses from
left to right.
world, it can be seen that a school panics and disperses. For example, the follow-
ing ﬁsh behaviors are well-known (see Fig. 3.27):
■
Dolphins attack bait ball in South Africa
■
Sardine run
■
Rainbow runner attacks a school of sardines
■
Mackerel tuna attacks a school of sardines

Meta-heuristics
■
73
Figure 3.27: School of sardines attacked by GT (@Moal Boal, Philippines in 2016).
Among the behaviors observed in those cases, the main ones are:
■
Flash expansion (Fig. 3.28(a))
■
Fountain effect (Fig. 3.28(b))
(a) Flash expansion
(b) Fountain effect
Figure 3.28: How to get away from a large predator.

74
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Large ﬁsh are slow in movement and few in numbers. To compensate, they can
see far away and have a habit of going towards a school of small ﬁsh in sight.
Small ﬁsh move fast and are numerous. In order to escape from bigger ﬁsh, their
natural enemies, the moment they enter their ﬁeld of vision, they swiftly change
their course and try to escape. However, since they can see only nearby, the ap-
pearance of big ﬁsh is unnoticed until they are considerably approached (those
are called “instincts”). Individuals belonging to the same species (big ﬁsh, small
ﬁsh) swim according to the Boids basic algorithm, and individuals belonging to
different species are affected by attractive/repulsive forces that change according
to the “instincts” mentioned above. In order to realize this, the Boids algorithm
was revised and the rules of interaction were added to the three basic rules for
the same kind. The three basic rules are as follows:
■
Heading to the center Easier to point in the direction, where more indi-
viduals belonging to the same group reside.
■
Aligning position Approach to be in the same position as the ﬁsh in the
same group. Basically, it works as an attraction. When it gets too close, it
becomes repulsion, so that they do not perfectly group up around a single
point.
■
Aligning direction A force of attraction works in an angular space that
faces in the same direction as the ﬁsh of the same school.
The rules of interaction are as follows:
■
hunting When a smaller opponent is seen, a force of attraction is experi-
enced (hunt mode).
■
avoiding When a bigger opponent is seen, a force of repulsion is experi-
enced (escape mode).
When Boids was realized based on this, it was observed that a group of large
ﬁsh engaged in predatory behavior on a school of small ﬁsh (Fig. 3.29). It is
noteworthy that small ﬁsh keep a constant shape as a school even when escaping
from larger ﬁsh.
Let us create Boids performing evasive maneuver based on Couzin’s algo-
rithm described in section 3.2.4. Those Boids simulate multiple kinds of behav-
ior at the same time. Each Boid has the search distance R′ (fulﬁlling RZor ≥R′)
in addition to the parameters of Couzin’s algorithm. In real animals, communi-
cation used with friends and information used for searching may not necessarily
be the same. For example, whales are capable of long-distance communication
with others by using highly directional ultrasounds, however, it is unlikely that
echolocation will be able to reach that far. Therefore, the R′ is smaller than RZor.
If there are two individuals of different species, they are seen as a predator
and non-predator (prey). In this case, the update rules are set as follows. This
update is performed for each individual i.

Meta-heuristics
■
75
Figure 3.29: Simulation results. A school of small ﬁsh attacked by a few large ﬁsh.
1. When an individual of different species is in Zor (zone of repulsion), the
direction to move next is as follows:
⃗dr(t +τ) = ±
nr
X
j̸=i,sp(i)̸=sp(j)
⃗ri j(t)
|⃗rij(t) |.
(3.21)
sp(i) means species of the individual i. The right side is the direction to the
center of gravity of heterogeneous individuals (individuals from different
species). Here, ± is −for prey species and + for predators. Thus, it is
possible to simulate the situation of “the chasing side and the escaping
side.”
2. When an individual of different species is in Zor (zone of repulsion), i.e.,
when there are heterogeneous individuals within the enemy search range
(a circle with a radius of R′ that is limited by the viewing angle), the di-
rection to move next is as follows:
⃗dr(t +τ) = ±
N′
X
j̸=i,sp(i)̸=sp(j)
⃗ri j(t)
|⃗rij(t) |.
(3.22)
This is the same derivation as the above, except that N′ is the number of
individuals in the enemy search range.
3. When it is neither of the above, Couzin’s algorithm is executed. Observa-
tions at this time are done only for individuals of the same species.
4. If a heterogeneous individual is found through the above processes, the
speed of the next step is multiplied by α, otherwise by α−1. Maximum

76
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
rotation angle θτ is set to different values for the case when a heteroge-
neous individual is found and for the case when it is not. This is to realize
that there is a difference in behavior when escaping from an enemy and
during normal times.
By simulating the enemy approaching, the Boid evasive maneuvers were sim-
ulated. Results are presented below.
■
Flash expansion
It was observed that the Boids acting in parallel spread out to escape
from the enemies. Figure 3.30(a) shows the situation when the tail was
attacked. Red Boid is a predator species and blue Boid is a non-predator
(prey) species. While evasive maneuver is being carried out, the prey
species are depicted in green.
■
Fountain effect
Figure 3.30(b) depicts a situation when prey is attacked from the side.
As can be seen from the ﬁgure, it seems that the Boids spread out when
attacked and try to minimize the sacriﬁces.
3.3
Artiﬁcial Bee Colony Optimization (ABC)
Bees, along with ants, are well-known examples of social insects (Fig. 3.31).
Bees are classiﬁed into three types:
■
employed bees
■
onlooker bees
■
and scout bees
Employed bees ﬂy in the vicinity of feeding sites they have identiﬁed, sending
information about food to onlooker bees. Onlooker bees use the information from
employed bees to perform selective searches for the best food sources from the
feeding site. When information about a feeding site is not updated for a given
period of time, its employed bees abandon it and become scout bees that search
for a new feeding site. The objective of a bee colony is to ﬁnd the highest-rated
feeding sites. The population is approximately half employed bees and scout bees
(about 10–15% of the total), the rest are onlooker bees.
The waggle dance (a series of movements) performed by employed bees to
transmit information to onlooker bees is well known (Fig. 3.32). The dance in-
volves shaking the hindquarters and indicating the angle with which the sun will

Meta-heuristics
■
77
(a) Flash expansion
(b) Fountain effect
Figure 3.30: Evasive maneuver simulation. Time lapses from top left to bottom right.

78
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 3.31: A bee colony.
60 degrees
60 degrees
food source
nest
sun
Figure 3.32: Waggle dance.
be positioned when ﬂying straight to the food source, with the sun represented as
straight up. For example, a waggle dance performed horizontally and to the right
with respect to the nest combs means “ﬂy with the sun at 90 degrees to the left.”
The speed of shaking the rear indicates the distance to the food; when the rear
is shaken quickly, the food source is very near, and when shaken slowly it is far
away. Communication via similar dances is also performed with regard to pollen
and water collection, as well as the selection of locations for new hives.

Meta-heuristics
■
79
The Artiﬁcial Bee Colony (ABC) algorithm [58, 59], initially proposed by
Karaboga et al., is a swarm optimization algorithm that mimics the foraging be-
havior of honey bees. Since ABC was designed, it has been proved that ABC,
with fewer control parameters, is very effective and competitive with other search
techniques, such as genetic algorithm (GA), particle swarm optimization (PSO),
and differential evolution (DE).
In ABC algorithms, an artiﬁcial swarm is divided into employed bees, on-
looker bees, and scouts. N d-dimensional solution candidates to the problem are
randomly initialized in the domain and referred to as food sources. Each em-
ployed bee is assigned to a speciﬁc food source ⃗xi and searches for a new food
source⃗vi by using the following operator:
⃗vij =⃗xij +rand(−1,1)×(⃗xij −⃗xk j),
(3.23)
where k ∈{1,2,··· ,N}, k ̸= i, and j ∈{1,2,··· ,d} are randomly chosen indices.
⃗vi j is the jth element of the vector⃗vi. If the trail to a food source is outside of the
domain, it is reset to an acceptable value. The obtained ⃗vi is then evaluated and
put into competition with⃗xi for survival. The bee prefers the better food source.
Unlike employed bees, each onlooker bee chooses a preferable source according
to the food source’s ﬁtness in order to do further searches in the food space using
eq. (3.23). This preference scheme is based on the ﬁtness feedback information
from employed bees. In classic ABC [58], the probability that the food source⃗xi
can be exploited is expressed as
pi =
fiti
PN
j=1 fitj
,
(3.24)
where fiti is the ﬁtness of the ith food source, ⃗xi. For the sake of simplicity, we
assume that the ﬁtness value is non-negative and that the larger, the better. If the
trail⃗vi is superior to⃗xi in terms of proﬁtability, this onlooker bee informs the rel-
evant employed bee associated with the ith food source,⃗xi, to renew its memory
and forget the old one. If a food source cannot be improved upon within a prede-
termined number of iterations, deﬁned as Limit, this food source is abandoned.
The bee that was exploiting this food site becomes a scout and associates itself
with a new food site that is chosen via some principle. In canonical ABC [58],
the scout looks for a new food site by random initialization.
The details of the ABC algorithm is described as below. The pseudocode of
the algorithm is shown in Algorithm 1.
Step 0: Preparation The total number of search points (N), total number of trips
(Tmax), and a scout control parameter (Limit) are initialized. The numbers
of employed bees and onlooker bees are set to be the same as the total
number of search points (N). The value of the objective function f is taken
to be nonnegative, with larger values being better.

80
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Algorithm 1 The ABC algorithm
Require: Tmax, #. of employed bees (= #. of onlooker bees), Limit
Initialize food sources
Evaluate food sources
i = 1
while i < Tmax do
Use employed bees to produce new solutions
Evaluate the new solutions and apply greedy selection process
Calculate the probability values using ﬁtness values
Use onlooker bees to produce new solutions
Evaluate new solutions and apply greedy selection process
Determine abandoned solutions and use scouts to generate new ones ran-
domly
Remember the best solution found so far
i = i+1
end while
Return best solution
Step 1: Initialization 1 The trip counter k is set to 1, and the number of search
point updates si is set to 0. The initial position vector for each search point
⃗xi = (xi1,xi2,xi3,··· ,xid)T is assigned random values. Here, the subscript
i (i = 1,··· ,N) is the index of the search point, and d is the number of
dimensions in the search space.
Step 2: Initialization 2 Determine the initial best solution ⃗
best.
ig
=
argmax
i
f(⃗xi)
(3.25)
⃗
best
=
xig
(3.26)
Step 3: Employed bee search The following equation is used to calculate a new
position vector⃗vij from the current position vector⃗xij.
⃗vij =⃗xij +φ ·(⃗xij −⃗xk j),
(3.27)
Here, j is a randomly chosen dimensional number, k is the index for some
randomly chosen search point other than i, and φ is a uniform random
number in the range [−1,1]. The position vector ⃗xi and the number of
search point updates si are determined according to the following equation:
I
=
{i | f(⃗xi) < f(⃗vi)}
(3.28)
⃗xi
=

⃗vi
i ∈I
⃗xi
i /∈I
(3.29)
si
=

0
i ∈I
si +1
i /∈I
(3.30)

Meta-heuristics
■
81
Step 4: Onlooker bee search The following two steps are performed.
1. Relative ranking of search points
The relative probability Pi is calculated from the ﬁtness fiti, which is
based on the evaluation score of each search point. Note that fiti =
f(⃗xi). The onlooker bee search counter l is set to 1.
Pi
=
fiti
PN
j=1 fitj
(3.31)
2. Roulette selection and search point updating
Search points are selected for updating based on the probability Pi,
calculated above. After search points have been selected, perform a
procedure as in Step 3 to update the search point position vectors.
Then, let l = l +1 and repeat until l = N.
Step 5: Scout bee search Given a search point for which si ≥Limit, random
numbers are used to exchange generated search points.
Step 6: Update best solution Update the best solution ⃗
best.
ig
=
argmax
i
f(⃗xi)
(3.32)
⃗
best
=
⃗xig when f(xig) > f( ⃗
best)
(3.33)
(3.34)
Step 7: End determination End if k = Tmax. Otherwise, let k = k +1 and return
to Step 3.
The ABC bee search process is explained by a simple example. Figure 3.33
depicts a bee colony and the food ground, which is the search space. Four em-
ployed bees, four onlooker bees, and one scout bee are in the colony. Suppose that
good search segments (higher degree of ﬁtness) are those shown in dark color. In
Fig. 3.33, there are two hills, with the one on the bottom-right representing the
optimum value. Suppose that food grounds 1 to 4 are assigned to each employed
bee upon initialization.
Each employed bee updates its assigned search position (food ground) once
(an action corresponding to an employed bee visiting the food ground once and
performing a quick exploration of the surroundings). In Fig. 3.34, bees 2 and 3
out of the 4 employed bees have updated their positions and moved to a food
ground of higher ﬁtness. However, this change is probabilistic and, therefore,
does not necessarily take them to a place of higher ﬁtness. For this reason, em-
ployed bees 1 and 4 remain in the same place. In other words, they are not per-
forming a local search.

82
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
4
1
2
3
Scout bees
Employed bees
Onlooker bees
Food space
1
4
3
2
colony
Figure 3.33: Bee colonies and search space.
4
1
2
3
1
2
4
3
Each search position is 
updated once.
᪂䛧䛔3
᪂䛧䛔2
colony
Food space
Figure 3.34: Employed bees search.
Next, the onlooker bee performs a search. In this case, food grounds with
higher degrees of ﬁtness are more likely to be visited, and, thus, more easily se-
lected during an update. In Fig. 3.35, onlooker bees 1 and 2 update search posi-
tion 2 (the food ground of employed bee 2) and 3 (the food ground for employed
bee 3), respectively. Since positions with higher degrees of ﬁtness are preferred,
onlooker bees 3 and 4 also visit search position 2, but no update occurs because
it is already a local maximum (Fig. 3.36).
After that, it is the scout bees’ turn to perform the search. This bee resets
food grounds that have not been updated for a ﬁxed number of times. In this
case, search position 2 is selected and replaced by a position displaced at random
(Fig. 3.37). Obviously, the food ground for employed bee 2 is also updated to
this randomly displaced position.

Meta-heuristics
■
83
4
1
2
3
1
2
4
3
The higher the degree of 
fitness of a position, the 
easier it is for that position 
to get selected for updates.
New 3
New 2
colony
Food space
Figure 3.35: Onlooker bees’ search (1).
4
1
2
3
1
4
The higher the degree of 
fitness of a position, the 
easier it is for that position 
to be selected for updates.
3
2
colony
Food space
Figure 3.36: Onlooker bees’ search (2).
This completes a cycle of actions in the colony. Subsequently, employed bees
start the search again. In Fig. 3.38, 3 employed bees update their food grounds.
Onlooker bees then search preferentially for food grounds with a higher degree
of ﬁtness. As a result, food ground 2 becomes the optimal value (Fig. 3.39).
ABC has recently been improved in many aspects. For instance, we analyzed
the mechanism of ABC to show a possible drawback of using parameter per-
turbation. To overcome this deﬁciency, we have proposed a new non-separable
operator and embedded it in the main framework of the cooperation mechanism
of bee foraging (see [45] for details).

84
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
4
1
2
3
1
2
4
3
Randomly move a point that 
has not been updated for a 
fixed number of times.
New 2
colony
Food space
Figure 3.37: Scout bees’ search.
4
1
2
3
1
2
4
3
New 2
New 1
New 4
colony
Each search position is 
updated once.
Food space
Figure 3.38: Employed bees’ new search.
3.4
Fireﬂy Algorithms
Fireﬂies glow due to a luminous organ and ﬂy around. This glow is meant to
attract females. The light generated by each ﬁreﬂy differs depending on the in-
dividual insect, and it is considered that they attract others following the rules
described below:
■
The extent of attractiveness is in proportion to the luminosity.
■
Female ﬁreﬂies are more strongly attracted to males that produce a
strong glow.
■
Luminosity decreases as a function of distance.

Meta-heuristics
■
85
4
1
2
3
colony
3
2
1
4
New 3
New 1
New 2
The higher the degree of 
fitness of a position, the 
easier it is for that position 
to be selected for updates.
Food space
Figure 3.39: Onlooker bees’ new search.
The ﬁreﬂy algorithm (FA) is a search method based on blinking ﬁreﬂies
[129]. This algorithm does not discriminate gender. That is, all ﬁreﬂies are at-
tracted to each other. In this case, the luminosity is determined by an objective
function. To solve the minimization problem, ﬁreﬂies at a lower functional value
(with a better adaptability) glow much more strongly. The most glowing ﬁreﬂy
moves around at random.
Algorithm 2 describes the outline of the FA. The moving formula for a ﬁreﬂy
i attracted by ﬁreﬂy j is as follows:
⃗xnew
i
=⃗xold
i
+βi,j(⃗x j −⃗xold
i
)+α(rand(0,1)−1
2),
(3.35)
where rand(0,1) is a uniform random numbers between 0 and 1. α is a param-
eter to determine the magnitude of the random numbers, and βi,j represents how
attractive ﬁreﬂy j is to ﬁreﬂy i, i.e.,
βi,j = β0e−γr2
i,j.
(3.36)
The variable β0 represents how attractive ﬁreﬂies are when ri,j = 0, which indi-
cates that the two are in the same position. Since ri,j represents the Euclid dis-
tance between ﬁreﬂy i and j, their attractiveness varies depending on the distance
between them.
The most glowing ﬁreﬂy moves around at random, according to the following
formula:
⃗xk(t +1) = ⃗xk(t)+α(rand(0,1)−1
2)
(3.37)
The reason for this is that the entire population converges to the locally best
solution in an initial allocation.

86
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Algorithm 2 Fireﬂy algorithm
Initialize a population of ﬁreﬂies⃗xi (i = 1,2,··· ,n)
▷Minimizing objective
function f(⃗x), ⃗x = (x1,··· ,xd)T.
Deﬁne light absorption coefﬁcient γ
t = 1
▷Generation count.
while t < MaxGeneration and the stop criterion is not satisﬁed do
for i = 1 to n do
▷for all n ﬁreﬂies
for j = 1 to n do
▷for all n ﬁreﬂies
Light intensity Ii,Ij at⃗xi,⃗x j is determined by f
if Ii > Ij then
Move ﬁreﬂy i towards j in all d dimensions
end if
Attractiveness varies with distance r via e−γr
Evaluate new solutions and update light intensity
end for
end for
Rank the ﬁreﬂies and ﬁnd the current best
t = t +1
end while
Postprocess results and visualization
As the distance becomes greater, the attractiveness becomes weaker. There-
fore, under the ﬁreﬂy algorithms, ﬁreﬂies form groups with each other at a dis-
tance instead of gathering at one spot.
The ﬁreﬂy algorithms are suitable for optimization problems on multimodal-
ity and are considered to yield better results compared to those obtained using
PSO. It has another extension that separates ﬁreﬂies into two groups and limits
the effect on those in the same group. This enables global solutions and local
solutions to be searched simultaneously.
3.5
Cuckoo Search
The cuckoo search (CS) [128] is meta–heuristics based on brood parasitic be-
havior. Brood parasitism is an animal behavior in which an animal depends on
a member of another species (or induces this behavior) to sit on its eggs. Some
species of cuckoos (Fig. 3.40) are generally known to exhibit this behavior. They
leave their eggs in the nests of other species of birds such as the great reed
warblers, Siberian meadow buntings, bullheaded shrikes, azure-winged magpies,
etc.1 Before leaving, they demonstrate an interesting behavior, referred to as egg
1Parasitized species are almost always ﬁxed for each female cuckoo.

Meta-heuristics
■
87
Figure 3.40: Cuculus poliocephalus. The lesser cuckoo image from a Japanese stamp.
mimicry: They take out one egg of a host bird (foster parent) already in the nest
and lay an egg that mimics the other eggs in the nest, thus, keeping the numbers
balanced.2 This is because a host bird discards an egg when it determines that
the laid egg is not its own.
A cuckoo chick has a remarkably large and bright bill; therefore, it is ex-
cessively fed by its foster parent. This is referred to as “supernormal stimulus.”
Furthermore, there is an exposed skin region at the back of the wings with the
same color as its bill. When the foster parent carries foods, the chick spreads its
wings to make the parent aware of the region. The foster parent mistakes it for
its own chicks. Thus, the parent believes that it has more chicks to feed than it
actually has and carries more food to the nest. It is considered to be an evolu-
tional strategy for cuckoos to be fed corresponding to their size because a grown
cuckoo is many times larger than the host.
The CS models the cuckoos’ brood parasitic behavior based on three rules,
as described below:
■
A cuckoo lays one egg at a time and leaves it in a randomly selected
nest.
■
The highest quality egg (difﬁcult to be noticed by the host bird) is
carried over to the next generation.
■
The number of nests is ﬁxed, and a parasitized egg is noticed by a
host bird with a certain probability. In this case, the host bird either
discards the egg or rebuilds the nest.
2Furthermore, a cuckoo chick having just been hatched, expels all the eggs of its host. For this reason,
a cuckoo chick has a pit in its back to place its host’s egg, clamber up inside the nest and throw the egg
out of the nest. This behavior was discovered by Edward Jenner, famous for smallpox vaccination.

88
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Algorithm 3 Cuckoo search
Initialize a population of n host nests
▷Minimizing objective function
f(⃗x), ⃗x = (x1,··· ,xd)T.
Produce one egg x0
i in each host i = 1,··· ,n
t = 1
▷Generation count.
while t < MaxGeneration and the stop criterion is not satisﬁed do
Choose a nest i randomly
Produce a new egg⃗xt
i by performing L`evy ﬂights
▷Brood parasite of the
cuckoo.
Choose a nest j randomly and let its egg be⃗xt−1
j
if f(⃗xt−1
j
) > f(⃗xt
i) then
▷The new egg is better.
Replace j’s egg by the new egg, i.e.,⃗xt
i
end if
Sort the nests according to their eggs’ performance
A fraction (pa) of the worse nests are abandoned and new ones are built by
performing L`evy ﬂights
t = t +1
end while
Postprocess results and visualization
Algorithm 3 shows the CS algorithm. Based on this algorithm, a cuckoo lays a
new egg in a randomly selected nest, according to L´evy ﬂight. This ﬂight presents
mostly a short distance random walk with no regularity. However, it sometimes
exhibits a long-distance movement. This movement has been identiﬁed in several
animals and insects. It is considered to be able to represent stochastic ﬂuctuations
observed in various natural and physical phenomena, such as ﬂight patterns, feed-
ing behaviors, etc.
Speciﬁcally, L´evy distribution is represented by the following probability
density function, referred to in Fig. 3.41:
f(x;µ,σ) =
(p σ
2π exp
h
−
σ
2(x−µ)
i
(x−µ)−3/2
(µ < x),
0
(otherwise),
(3.38)
where µ represents a positional parameter, and σ represents a scale parameter.
Based on this distribution, L´evy ﬂight mostly presents a short distance move-
ment, while it also presents a random walk for a long distance movement with a
certain probability. For optimization, it facilitates an effective search compared
to using random walk (Gaussian ﬂight) according to a regular distribution [128].

Meta-heuristics
■
89
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
σ = 0.5
σ = 1.0
σ = 2.0
σ = 3.0
σ = 4.0
x
f(x; 0.0, σ)
Figure 3.41: L´evy distribution.
Let us consider an objective function represented as f(⃗x), ⃗x = (x1,··· ,xd)T.
A cuckoo then creates a new solution candidate for the nest i given by the fol-
lowing equation:
⃗xt+1
i
=⃗xt
i +α ⊗L´evy(λ),
(3.39)
where α(> 0) is related to the scale of the problem. In most cases, α = 1. The
operation ⊗represents multiplication of each element by α. L´evy(λ) represents
a random number vector whereby each element follows a L´evy distribution, and
this is accomplished as follows:
L´evy(λ) ∼rand(0,1) = t−λ
1 < λ ≤3,
(3.40)
where rand(0,1) is a uniform random number between 0 and 1. This formula is
essentially a random walk with the distribution achieved by powered steps with a
heavy tail. Therefore, it includes inﬁnite averages and inﬁnite standard deviation.
An exponential distribution with exponents from -1 to -3 is normally used for a
long-distance movement of L´evy ﬂight.
pa represents a parameter referred to as the switching probability, and its frac-
tion of the worse nests are abandoned from the nest by a host bird and new ones
are built by performing L`evy ﬂights. This probability strikes a balance between
exploration and exploitation.
CS is considered to be robust, compared with PSO and ACO [14].
3.6
Harmony Search (HS)
Harmony search (HS) [33] is a meta-heuristic based on jazz session (generation
process of human improvisation). Musicians are considered to perform improvi-
sation mainly using any one of the methods as outlined below:

90
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
Use already-known scales (stored in their memory).
■
Partially change or modify the already known scales. Play scales next to
the one stored in their memory.
■
Create new scales. Play random scales within their playable area.
A process whereby musicians combine various scales in their memory for the
purpose of composition is regarded as a sort of optimization. While many meta-
heuristics are based on swarm intelligence of life, such as ﬁsh, insects, etc., HS
signiﬁcantly differs from them in terms of exploiting ideas from musical pro-
cesses to search for harmony, according to an aesthetic standard.
Harmony search algorithms (referred to as “HS,” hereinafter) search for the
optimum solution by imitating a musician’s processes according to the following
three rules:
■
Select an arbitrary value from HS memory.
■
Select a value next to the arbitrary one from HS memory.
■
Select a random value within a selectable range.
With HS, a solution candidate vector is referred to as a harmony, and a set of so-
lution candidates is referred to as a harmony memory (HM). Solution candidates
are replaced within HM by a speciﬁc order. This process is repeated a certain
number of times (or until the conditions for termination are met), and ﬁnally, the
best harmony is selected among those that survive in HM as a ﬁnal solution.
Algorithm 4 shows a Harmony Search algorithm. HMCR (Harmony Memory
Considering Rate) represents the probability of selecting a harmony from HM,
while PAR (Pitch Adjust Rate) represents the probability of amending a harmony
selected from HM. HMS is the number of harmonies (sets), which is normally
set to between 50 and 100.
A new solution candidate (harmony) is generated from HM based on HMCR.
HMCR is the probability of selecting component elements3 among the present
HM. Thus, new elements are randomly generated by the probability of 1 −
HMCR. Subsequently, mutation occurs according to the probability of PAR. The
bw parameter (Bandwidth) represents the largest size of the mutation. In case a
newly generated solution candidate (harmony) is better than the poorest solution
of HM, they are replaced.
3It corresponds to allele in genotype under GA.

Meta-heuristics
■
91
Algorithm 4 Harmony search
for i = 1 to HMS do
▷HM initialization.
for j = 1 to n do
▷n: harmony length.
Randomly initialize xi
j in HM
▷xi
j: j-th position of i-th harmony.
end for
end for
while the stop criterion is not satisﬁed do▷Generate a new solution candidate
⃗x.
for j = 1 to n do
if rand(0,1)< HMCR then
Let x j in⃗x be the j-th dimension of a randomly selected HM mem-
ber
if rand(0,1)< PAR then
Apply pitch adjustment distance bw to mutate x j
xj = xj ±rand(0,1)×bw
end if
else
Let xj in⃗x be a random value
end if
end for
Evaluate the ﬁtness of⃗x by f(⃗x)
if f(⃗x) is better than the ﬁtness of the worst HM member then
Replace the worst HM member with⃗x
▷HM update
else
Disregard⃗x
end if
end while
Postprocess results and visualization
This method is also similar to a genetic algorithm (GA); however, it differs in
that all the members of HM become a parent candidate in HS, while only one or
two existing range(s) of chromosomes (parent individual) is/are used to generate
a child chromosome in GA.
3.7
Cat Swarm Optimization (CSO)
Cat swarm optimization (CSO) [16] is an optimization method based on a cat’s
behavior. Cats remain calm most of the time, however, they are interested in their
surroundings and are prudent. The length of time spent chasing prey is extremely
short, so as to save energy, compared with that of their resting time.

92
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
CSO has two main modes:
■
seeking mode: Represents the resting time (observing the surround-
ings while taking a rest). Cats make a decision to move when they
sense prey and danger.
■
chasing mode: Represents the time spent chasing prey. After ﬁnding
a prey, they determine the chasing speed and destination. Thereafter,
they start moving.
With CSO, multiple cats are generated in a searching space and each of them
becomes a solution candidate. These cats are divided into two groups: Seeking
mode and chasing mode. The ratio of these modes is a mixing ratio (MR: The
number of cats in chasing mode / that in seeking mode). Cats spend most of
their time in a calm state in order to observe the surroundings, therefore, MR is
normally set small.
Seeking mode has four parameters, as listed below:
■
SMP: Indicates how many cats in seeking mode are considered. It is used
to deﬁne the memory size of the cats.
■
SRD: Indicates how much each parameter changes. It is the width of the
mutation in a selected dimension. If changed, the difference between the
old and new values should be within the range of SDR.
■
CDC: Indicates how many of the parameters are changed. It deﬁnes the
number of elements to change.
■
SPC: Indicates whether a cat is a candidate for moving or not. It indicates
if the current location of a cat is a candidate for the destination.
Under the condition that M represents the number of dimensions in the
searching space, the process involved in the seeking mode toward the k–th cat
(catk) is as follows:
Step1 Set j as follows:
j =
(
SMP−1
if SPC is true
SMP
otherwise
(3.41)
Step2 Copy the current position of the k–th cat (catk) for j times.

Meta-heuristics
■
93
Step3 For each copy created, replace the positions based on the following equa-
tion:
xk,d = (1±SRD×rand(0,1))×xk,d.
(3.42)
With the condition d ∈{1,2,··· ,M}, the number of elements to be
changed (different numbers of d) is selected within the upper limit of the
CDC.
Step4 Calculate the adaptability (ﬁtness) of the entire candidate points (FSi)
under the condition 1 ≤i ≤j.
Step5 Calculate the selective probability Pi of each candidate point (cati) using
the following equation:
Pi = | FSi −FSb |
FSmax −FSmin
(1 ≤i ≤j),
(3.43)
where FSi represents a functional value (adaptability, i.e., ﬁtness value)
of cati. FSmax and FSmin are the maximum and minimum value of the
function, respectively. If an objective function is used for searching for the
minimum, then FSb = FSmax, if it is for the maximum, then FSb = FSmin.
Step6 Based on roulette selection with the selection probability Pi, select the
destination among the current candidate positions at random, and replace
the positions of catk.
When in tracing mode, each cat moves at its own speed. This mode is repre-
sented by the following three steps:
Step1 Update the velocity (vk,d) for each dimension d by following the formula;
vk,d = vk,d +rand(0,1)×c1 ×(xbest,d −xk,d).
(3.44)
With the condition d ∈{1,2,··· ,M}, xbest,d represents the position of a cat
with the best adaptability. c1 is a user-deﬁned scaling parameter.
Step2 Check whether the velocity exceeds the maximum. If so, update the max-
imum speed.
Step3 Update the position of each cat, according to the following equation:
xk,d = xk,d +vk,d.
(3.45)

94
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Algorithm 5 Cat swarm optimization
Initialize a population of cats cati (i = 1,2,··· ,n) with random positions⃗xi in
M-dimensional space
▷n: the number of cats.
Randomly assign each cat a velocity in range of the maximum velocity value
(i.e.,⃗vi;i = 1,2,··· ,n).
while the stop criterion is not satisﬁed do
According to MR, assign each cat a ﬂag of the seeking or tracing mode
Calculate the ﬁtness function values for all cats and sort them
Xg = the best cat
▷ﬁnd the cat with the best solution.
for i = 1 to n do
if cati’s mode is in seeking mode then
Start seeking mode
else
Start tracing mode
end if
end for
end while
Postprocess results and visualization
3.8
Meta-heuristics Revisited
Some critical opinions have been expressed against meta-heuristics and further
discussions have also been undertaken.
Meta–heuristics frequently uses unusual names, terms associating with na-
ture, and metaphors. For example, in the harmony search, the following terms
are used:
■
harmony
■
pitchAnoteA
■
sounds better
However, these are just saying the following words listed below in another way:
■
solution
■
decision variable
■
has a better objective function value

Meta-heuristics
■
95
Algorithm 6 Evolution Strategy: (µ +1)ES
Initialize the population with randomly generated solutions
while the stop criterion is not satisﬁed do
Create a new solution using recombination and mutation operators
if the new solution is better than the worst solution in the population then
Replace the worst solution by the new one
end if
end while
return the best solution in the population
Although critics insist that these replaced words cause confusion [112], it is
considered that the use of metaphorical expressions itself does not inﬂuence the
ease of understanding. For example, David Hilbert4 is quoted as saying Geometry
does work even if a point, line and face are expressed as a table, chair, and beer
mug in discussing mathematical forms. That is to say, it does not matter at all
in precise discussions when it is axiomatically deﬁned. Nevertheless, we should
be careful about insisting on the novelty of meta-heuristics. It is important to
recognize the distinct difference with existing methods for further discussions.
Wayland[122] has criticized the Harmony Search as simply a special example
of evolution strategy (µ +1)ES .
Let us recall (µ +1)ES, one of the algorithms described in section 2.1.2 (see
Algorithm 6 in page 95). Comparing this algorithm to that of the harmony search
(refer to page 91), it is assumed that the essential method of calculation is the
same although they slightly differ in parameter setting, among others [122]. On
the contrary, discussion on methods of expanding the harmony search algorithm
and the parameter setting methods can be built up in a different way to ES5.
It does matter that researchers who propose meta-heuristics do not recognize
other similar methods [112]. When developing a new method, we never fail to
have discussions on the basis of past investigations.
There is a well-known Chinese proverb which states “w¯eng`uzh¯ix¯in” by Con-
fucius6. Returning to an old method and expanding it to a produce a new one
is not a bad thing and is in fact recommended. Especially, in the pursuit of re-
4David Hilbert (1862–1943): German mathematician. At the 2nd International Congress of Mathe-
maticians (ICM) in Paris in 1900, he made a speech on “problems in mathematics,” where he stressed the
importance of 23 unsolved problems and presented a prospect for future creative research through these
problems. Some of them continue to be themes for research on mathematics and computer science.
5Further discussions have been presented, and there is an insistence on the part of some individuals that
it is a different method. Refer to https://en.wikipedia.org/wiki/Harmony_search for details.
6Confucius (BC552–BC479): A philosopher in the Spring and Autumn period in China. He is the
founder of Confucianism. The “Analects of Confucius”(The Governing Volume) is a collection of sayings
and anecdotes attributed to him and his disciples. For over two thousand years, the ideology of Confucius
was respected and inherited as China’s legitimate ideology and the idea of “learning from the past to
develop new ideas” is attributed to the Analects (Wei Chang book). Confucius has supposedly said that a
requisite for someone to be his disciple is to study ideas and academic disciplines from his predecessors.

96
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
search into rapidly developing AI, long-forgotten technologies are expected to
revive with these new technologies (big data, supercomputers, among others),
and many examples are well known. Now is the time to recognize the impor-
tance of engaging in productive discussions with a particular emphasis on old
methods. The expectation in right-minded research communities is to allow var-
ious proposed methods to compete against each other, which should result in the
continuation of more robust approaches for future generations after a process of
successive elimination7.
7Those who engage in AI research based on evolution naturally expect evolutional phenomena (e.g.,
speciation, diversity, natural selection, preadaptation, punctuated equilibrium and so on) in research re-
sults.

Chapter 4
Emergent Properties and
Swarm Intelligence
Pristine originality is an illusion; all great ideas were thought and expressed
before a conventional founder ﬁrst proclaimed them. Conventional founders
win their just reputations because they prepare for action and grasp the full
implication of ideas that predecessors expressed with little appreciation of their
revolutionary power.
—Stephen Jay Gould
4.1
Reaction-Diffusion Computing
The Reaction-Diffusion Cellular Automaton is a model of a two-dimensional
chemical reaction. In the actual chemical reaction, creation of a certain molecule
is accompanied by precipitation of another molecule. When the ﬁrst molecule is
propagated, the reaction stops. A model realizing such phenomenon on a com-
puter is called Reaction-Diffusion Computing.
This computational model is used for image processing. In fact, diffusion can
be used for noise ﬁltering and reaction for contrast enhancement. Depending on
the parameters, Reaction-Diffusion Computing can realize the following image
manipulations:
■
restore broken contours
■
detect edges
■
improve contrast
For example, there is a study [1, 2] to realize the basic operation of image pro-
cessing using Belousov-Zhabotinsky reaction described in section 5.4.

98
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
We will explain the following processes as an application:
■
Generation of the Voronoi diagram
■
Construction of a skeleton from a contour map
4.1.1
Voronoi Diagram Generation
Assuming that a set of points (called seeds, sites, or generators) is given. Let
us consider constructing the following diagram (called Voronoi diagram; see
Fig. 4.2):
■
All the points in the polygon are nearer to the generator within it than to
the other generators.
■
For a two-dimensional Euclidean plane, the border of the region becomes
part of the bisector of each generator.
A polygon containing a generator is called a Voronoi polygon.
Voronoi polygon is considered as animals’ territory in terms of generators.
For example, in 2011, a new species of blowﬁsh (Torquigener albomaculosus)
was found in Amami isl., Japan (called “Galapagos of the East”). Although the
length of a male specimen is just 15cm, they construct complex radial-shaped
nests at the bottom of the sea with 2m-diameter tunnels (see Fig. 4.1). After
observing such nests, divers started to refer to them as “mystery circles.” When
an egg-laying bed (circle) is completed, a female accepts to lay eggs in the nest
for reproduction. A Voronoi polygon is formed when multiple such nests are
located close together.
Voronoi diagrams can be seen in nature frequently. Figure 4.3 shows a coral
example. These diagrams are used varyingly in engineering as follows:
■
Collision-free path planning
■
Determination of service areas for power substations
■
Nearest-neighbor pattern classiﬁcation
■
Determination of largest empty ﬁgure
The algorithm for creating Voronoi diagrams has been studied in detail in the
ﬁeld of Computational Geometry, but it is considerably complicated and compu-
tationally intensive.
When generators p1, p2, p3,··· , and pn are given, the simplest method is as
follows1:
1In this section, we consider a Voronoi diagram on a two-dimensional Euclidean space.

Emergent Properties and Swarm Intelligence
■
99
Figure 4.1: A mystery circle (@Amami isl., Japan in 2019).
Figure 4.2: A Voronoi diagram.
Figure 4.3: A coral example (Favia favus) of Voronoi diagram (@Anilao, Philippines in 2018).
Step1 Draw the perpendicular bisector I12 connecting points p1 and p2, and ﬁnd
the half plane H12 on the p1 side with the vertical line as the borderline.
Step2 Similarly, obtain half plane H13 for p3 and ﬁnd the common area of the
region H12 ∩H13.
Step3 Similarly, obtain half plane H14 for p4 and ﬁnd the common area of the
region H12 ∩H13 ∩H14.

100
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 4.4: Successive Addition Method.
Step4 In the same way, ﬁnd the area of the common part H12 ∩H13 ∩H14 ∩···∩
H1n. This is the Voronoi polygon V1 for generator p1.
Step5 The same operation is carried out with respect to other points, yielding
the Voronoi polygon Vi (i = 2,3,...,n) of the generator pi, and obtain the
entire Voronoi diagram.
In the worst case, this algorithm takes the trouble (has a complexity) of O(n2)
to create one Voronoi polygon. Therefore, the computational complexity to create
the entire Voronoi diagram, in the worst case, can become O(n3).
To reduce the computational complexity, the following efﬁcient method has
been proposed.
Successive Addition Method First creates a Voronoi diagram with p1, p2, p3 and
then adds points p4, p5,··· , pn one by one to it. Figure 4.4 shows how a
point is added in this process.
Recursive Bisection Method Divide n generators by x coordinates to the left
and right with almost n/2 each side. This method recursively creates a
Voronoi diagram for each side and merges them to create full Voronoi
diagram. This method is based on Divide and Conquer paradigm.
Computational time of the successive addition method is O(n3/2) when gen-
erators are not added in any particular order. However, when the addition order is
devised, it becomes O(n) on average. The recursive bisection method on average
requires O(nlog(n)) computational time.
Now let us create a Voronoi diagram with Reaction-Diffusion Computation.
Here, a Voronoi diagram is derived using a Reaction-Diffusion Cell Automa-
ton [1].
In the following, O(n)-Algorithm with n+3 states and O(1)-Algorithm with
four states are explained.

Emergent Properties and Swarm Intelligence
■
101
O(n)-Algorithm takes the following n+3 states:
Symbols
State
1,2,··· ,n
Excitement (active oxidation)
•
Dormant state (neither)
−
Refractory (active reduction)
#
Precipitation (bisector creation)
The update rule is as follows:
xt+1 =









#,
(xt ∈{1,··· ,n,•}∧| I(x)t |≥2)∨(xt = #),
+,
(xt = •)∧I(x)t = {+},
−,
(xt = +)∧(I(x)t = {+}∨I(x)t = φ),
•,
(xt = −)∨(I(x)t = φ ∧(xt = •)),
(4.1)
where 1,2,··· , and n are + (i.e., + ∈{1,2,··· ,n}). I(x)t is an excited (+) neigh-
boring cell at time t.
As an example, according to this update rule, the following transition occurs:
−
=⇒
•
#
=⇒
#
+
=⇒

−
when | I(x)t |= 0∨I(x)t = {+}
#
when | I(x)t |≥2
•
=⇒



•
when I(x)t = φ
+
when I(x)t = {+}
#
when | I(x)t |≥2
Figure 4.5 shows a simple evolution example of cellular automaton by O(n)-
Algorithm in the 4-neighbor case. In this ﬁgure, • is a blank cell and ∗represents
a generator.
O(1)-Algorithm takes the following 4 states:
Symbols
State
+
Excitement (active oxidation)
•
Dormant state (neither)
−
Refractory (active reduction)
#
Precipitation (bisector creation)
When using O(1)-Algorithm, its update rule is as follows.

102
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
xt+1 =









#,
((xt = •)∧(Sv(x)t = 2∨Sh(x)t = 2))∨(xt = #)∨((xt = +)∧(S(x)t ≥1)),
+,
((xt = •)∧((S(x)t ≥1)∧(Sv(x)t ̸= 2)∧(Sh(x)t ̸= 2)),
−,
(xt = +)∧(S(x)t = 0),
•,
(xt = −)∨((xt = •)∧(Sv(x)t < 2)∧(Sh(x)t < 2)).
(4.2)
t = 1
t = 2
t = 3
1
2
1
1
*
1
1
2
2
*
2
2
1
1
-
1
1
-
*
-
1
1
-
1
1
2
2
-
2
2
-
*
-
2
2
-
2
2
t = 4
t = 5
t = 6
1
1
-
1
1
-
1
-
*
-
1
1
-
-
1
2
1
-
1
2 -
2
1
2
-
2
2
-
*
-
2
2
-
-
2
2
-
2
2
1
-
1
1
-
-
1
-
-
1
*
-
1
2
-
-
1
2
-
2
1
-
-
1
2
-
-
2
1
-
1
2
-
-
2
1
2
-
*
-
2
-
-
2
2
-
-
2
2
-
2
2
1
-
-
1
-
-
1
-
1
2
*
-
#
-
2
-
#
-
-
2
-
-
#
-
-
2
1
-
-
#
-
-
1
-
#
-
*
1
2
-
-
2
-
-
2
2
-
-
2
2
-
2
t = 7
t = 8
t = 9
-
-
1
-
1
2
-
#
-
2
*
#
-
2
#
-
2
#
-
-
#
1
-
#
*
1
-
#
-
1
2
-
-
2
-
-
2
2
-
-
2
-
1
2
-
#
-
2
#
-
2
*
#
-
2
#
-
#
#
-
#
*
1
-
#
1
-
#
-
1
2
-
-
2
-
-
2
-
#
-
2
#
-
2
#
-
2
*
#
-
#
#
#
#
*
-
#
1
-
#
1
-
#
-
1
2
-
-
Figure 4.5: Evolution of reaction-diffusion automata by O(n) algorithm.

Emergent Properties and Swarm Intelligence
■
103
Where S(x)t is an excited (+) neighboring cell at time t. Sh(x) and Sv(x) denote
the numbers of excited (+) neighbors located horizontally2 and vertically3, re-
spectively.
In case of 4 neighbors, transitions are as follows:
−
=⇒
•
#
=⇒
#
+
=⇒

−
when S(x)t = 0
#
when S(x)t ≥1
•
=⇒



•
when S(x)t = 0
+
when (S(x)t ≥1)∧(Sv(x)t < 2)∧(Sh(x)t < 2)
#
when (Sv(x)t = 2)∨(Sh(x)t = 2)
In case of 8 neighbors, transitions are as follows:
−
=⇒
•
#
=⇒
#
+
=⇒

−
when S(x)t < 4
#
when S(x)t ≥4
•
=⇒



•
when S(x)t = 0
+
when (4 > S(x)t ≥1)∧(¬P(x)t)
#
when (S(x)t ≥4)∧P(x)t
The predicate P(t)t is deﬁned as follows:
P(t)t = ((xt
N = +∧xt
S = +)∨(xt
W = +∧xt
E = +)∨(xt
NW = +∧xt
SE = +)∨(xt
NE = +∧xt
SW = +)),
(4.3)
where subindices mean North, South, West, East, North-West, South-East,
North-East and South-West neighbors of cell x, respectively.
Figure 4.6 shows a simple evolution example of cellular automaton by O(1)-
Algorithm.
Figure 4.7 shows the result of the derivation of the Voronoi diagram for two
points under various conditions. Let the distance between two points on x axis be
disx and the distance on y axis be disy.
From this ﬁgure, we can see that both deﬁnition of the neighborhood and
the change in distance between the two points affect how region is divided, and
the thickness of the division line. In particular, when using O(1)-Algorithm for
disx = disy, the area is not completely bisected. In the 4-neighborhood O(N)-
Algorithm, the line that divides the area is a vertical bisector when disx ̸= disy
and the border does not disappear when disx = disy.
Figure 4.8 shows the Voronoi diagram when 10 points are randomly dis-
tributed. The ﬁgure shows from the left: The result of O(N)-Algorithm in case
2East and west neighbors
3North and south neighbors

104
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
t = 1
t = 2
t = 3
+
+
+
+
+
+
*
+
+
+
+
+
+
+
+
*
+
+
+
+
+
+
+
+
+
+
-
-
-
+
+
-
*
-
+
+
-
-
-
+
+
+
+
+
+
+
+
+
+
+
+
-
-
-
+
+
-
*
+
+
-
-
-
+
+
+
+
+
+
t = 4
t = 5
t = 6
+
+
+
+
+
+
-
-
-
-
-
+
-
+
*
-
+
-
#
+
+
+
+
+
+
-
-
-
-
-
#
-
-
-
-
-
+
+
+
+
+
+
#
-
-
+
+
-
*
-
+
+
-
-
+
+
-
-
-
-
-
+
+
+
+
+
+
+
+
-
+
-
+
-
+
*
-
#
+
+
+
+
+
#
-
-
-
-
-
-
#
-
-
-
-
-
-
#
-
+
+
+
+
#
-
*
-
+
-
-
+
-
-
+
-
-
-
-
-
-
-
+
+
+
+
+
+
+
+
-
+
-
+
-
#
+
+
+
+
*
#
-
-
-
-
-
#
#
#
-
-
-
-
#
*
+
+
+
#
-
+
-
+
-
+
-
t = 7
t = 8
t = 9
-
+
-
#
+
+
+
#
-
-
-
-
*
#
#
#
#
#
*
-
-
-
#
+
+
#
-
+
-
+
-
-
#
+
+
#
-
-
-
#
*
#
#
#
#
#
*
#
-
-
#
+
#
-
+
-
#
-
-
#
#
*
#
#
#
#
#
*
#
#
-
#
#
-
Figure 4.6: Evolution of reaction-diffusion automata by O(1) algorithm.
of 4-neighborhood, the result of O(N)-Algorithm in case of 8-neighborhood,
the result of O(1)-Algorithm in case of 4-neighborhood, and the result of O(1)-
Algorithm in case of 8-neighborhood4.
Figure 4.9 shows the outlook of Voronoi simulator. This simulator generates
Voronoi diagrams when it is provided with a seed point. Each seed is assigned
with separate colors, and propagation color is different from each of them. When
4Moore neighborhood, up, down, left, right and diagonal.

Emergent Properties and Swarm Intelligence
■
105
O(N)-Algorithm
4-neighborhood
disx = disy
disx ̸= disy
disx
even
odd
even
odd
even
odd
disy
even
odd
even
even
odd
odd
Figure 4.7: Voronoi diagram for two points.
O(N)-Algorithm
O(1)-Algorithm
4-neighborhood
8-neighborhood
Figure 4.8: Voronoi diagram for ten points.
different propagations come into contact, they are designed to change to the color
of the boundary. The following shows the experimental results of this simulator.
In the algorithm described so far, a chemical substance propagates to the
surrounding area at the next time step. In the previous study [1], L1 norm and
L∞norm were used in the experiments as the deﬁnition of “surroundings.” In
addition to that, this simulator is capable of handling the case of the L2 norm.
O(N)-Algorithm
8-neighborhood
O(1)-Algorithm
4-neighborhood
O(1)-Algorithm
8-neighborhood

106
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 4.9: Voronoi Simulator.
L1 norm
L2 norm
L∞norm
Figure 4.10: The difference in propagation due to the norm (one-point case).
The deﬁnition of each norm for vectors⃗x = (x1,x2,··· ,xn) is as follows:
L1norm
:
| x1 | + | x2 | +···+ | xn |
L2norm
:
q
x2
1 +x2
2 +···+x2n
L∞norm
:
max{| x1 |,| x2 |,··· ,| xn |}
Here, the radii of the L1 norm and the L∞norm are 1 and the radius of the L2
norm is 3. Since the ﬁeld is discrete, the L1 norm and L2 norm are the same for
radius of 1 and 2. The difference in propagation due to the norm is shown in
Fig. 4.10.
Let us see the difference for each norm of the boundary for two seed points.
As can be seen in Fig. 4.11, if two seed points are parallel to the axes, the shape
of the boundary does not depend on the seeds. That is, the line connecting two
points becomes a perpendicular bisector. This is because the L1,L2 and L∞norms
match on the axis. Figure 4.12 shows the propagation for two seeds in a more
general positional relationship. In this case, the difference of the boundary due
to the differences in the norms was observed. It can be seen that the boundary
due to the L1 norm and boundary due to L∞norm are mirror images (in the
mirror-image relationship).

Emergent Properties and Swarm Intelligence
■
107
L1 norm
L2 norm
L∞norm
Figure 4.11: The difference in propagation due to the norm (two-point case).
L1 norm
L2 norm
L∞norm
Figure 4.12: The difference in propagation due to the norm (general two-point case).
L1 norm
L2 norm
L∞norm
Figure 4.13: The difference in propagation due to the norm (three-point case).
Figure 4.13 shows the difference between the boundaries of each norm due to
three seed points. A trend similar to the two-point case is seen, that is, L1 and L∞
are in a mirror-image relationship. To be more precise, it is rotated 90 degrees
around the center of mass of the three seeds and is in reversed axis relationship.
In other words, the boundary of L1 and the boundary of L∞are in the folding

108
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
L1 norm
L2 norm
L∞norm
Figure 4.14: The difference in propagation due to the norm (hexagon case).
L1 norm
L2 norm
L∞norm
Figure 4.15: The difference in propagation due to the norm (random case).
relationship. The axis of the folding is the axis for the L2. However, it should be
noted that it is a discrete ﬁeld, and there is a slight deviation due to the thinning.
Figure 4.14 shows the result when seed points are arranged in a hexagon
shape. The upper row shows the development process and the lower row shows
Color version at the end of the book

Emergent Properties and Swarm Intelligence
■
109
the case just before the end. For L1 the central part is hexagonal, for L2 it is a
circle and for L∞it is a square.
Figure 4.15 shows the state of the simulation when many random seed points
were scattered. It can be seen that the results conﬁrmed so far appeared in various
places.
As is evident from the above results, the generated Voronoi diagram is quite
different depending on the norm. For example, paying attention to the three
Voronoi diagrams in the lower row in Fig. 4.15, the differences become apparent.
For each norm in the region of the seed, there is a clear difference in shape and
area. When considering the Voronoi diagram as a simulation of a real physical
phenomenon, it is necessary to pay close attention to the selection of the norm.
4.1.2
Thinning and Skeletonization for Image Understanding
Thinning is a technique that is often used as a pre-processing operation in the
ﬁelds of character recognition and pattern recognition. The process of converting
a binary image into a 1-pixel wide image is called thinning (see Fig. 4.16).
Figure 4.16: Thinning a coral picture.
Various thinning algorithms are known. For example, Hilditch’s thinning [49]
is executed as follows:
Step1 Let the background pixel be 0 and the ﬁgure pixel be 1.
Step2 Search for pixels that satisfy the following condition.
■
It is a ﬁgure pixel (1).
■
It is a boundary point.
■
It does not delete endpoints.
■
It preserves isolated points.
■
It preserves connectivity.
Step3 Erase pixels satisfying the above conditions.

110
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Step4 Repeat steps from Step1 to Step3 until there are no more pixels to re-
move.
For another example, Tamura’s thinning algorithm [115] uses the pattern
shown in Table 4.1 and is executed as follows:
Step1 In cases of pattern no.1, remove the pixel (i.e., change the color of the
center from black to white). If it corresponds to an exceptional pattern, do
not remove.
Step2 If no pixel is removed, end the process. If some pixel is removed, proceed
to the next step.
Step3 In caes of pattern no.2, remove the pixel. If it corresponds to an excep-
tional pattern, do not remove.
Step4 If no pixel isremoved, end the process. If some pixel is removed, go back
to Step1.
Note that a cell of 1 stands for a black pixel, 0 for a white pixel, and ∗for
either black or white in the table.
Although the conventional algorithms are complicated, it is possible to easily
realize thinning operation using Reaction-Diffusion CA. This method is basically
the same as O(n)-Algorithm and uses the following four states.
Symbol
State
EXC
Excitement (active oxidation)
REF
Refractory (active reduction)
RES
Dormant state (neither)
PRE
Precipitation (bisector creation)
Assuming that the number of excited (EXC) neighboring cells of the cell x at
time t is st(x), the update rule of the thinning operation is as follows (in case of
8 neighbors):
xt+1 =









EXC,
(xt = RES)∧(1 ≤st(x) ≤4),
REF,
(xt = EXC)∧(st(x) ≤4)∨(xt = REF),
RES,
(xt = RES)∧(st(x) = 0),
PRE,
{(xt = RES)∨(xt = EXC)}∧(st(x) ≥5)∨(xt = PRE),
(4.4)
Figure 4.17 shows the result of the application of the thinning. Results were
achieved for different thicknesses of the bar T. The black lines in the green area
are the result of the thinning operation. A successful case of the applied thinning
operations can be seen in the left-most ﬁgure. Note that changing the thickness of
the bar can make blank spaces between the lines or sometimes even unintended
points. A more complicated example is given in Fig. 4.18.

Patterns no. 1
Patterns to
be removed
*
*
*
1   * 
0 
* 
* 
*
*
*
*
*
0 
1
*
*
*
Exceptional
Patterns
(Part I)
*    1     *
*
0
0
*
0 
1
1
0 
*
0
0
1
*
*
*
Exceptional
Patterns
(Part II)
Same as
patterns
no. 2
*
*
*
1
0
1
* 
1    *
0
0 
*
*
1    *
*
0 
*
1
0
1
*
*
*
*
1    *
*
0
0
*     1    *
*
*
*
1
0
*
0
1
*
0
1
*
1
0
*
*
*
*
*
0 
*
*
*
1 
0 
*0 
*
*
*
*
*
0 
1
*
1 
0
1
0
1
0
0
0
1
*
1
1
0
1
*
0 
0
1
0
1
1
*
1
0
0
0
1
0
1
1
0
1
0
0
*
1
0
1
Patterns no. 2
Patterns to
be removed
*
*
*
* 
* 
0 
* 
1    *
*
*
*
1
0 
*
*
*
*
Exceptional
Patterns
(Part I)
1
0
*
0
0
*
*
1    *
*
*
*
1
0
0
Exceptional
Patterns
(Part II)
Same as
patterns
no. 1
*
*
*
1
0
1
*
0 
1
*
1    * 
0
0 
*
*
1    *
*
0 
*
1
0
1
*
*
*
*
1    *
*
0
0
*     1    *
*
*
*
1
0
*
0
1
*
0
1
*
1
0
*
*
*
*
*
0 
*
*
*
1 
0 
*0 
1
*
*
*
*
*
0 
1
*
1 
0
1
0
1
0
0
0
1
*
1
1
0
1
*
0 
0
1
0
1
1
*
1
0
0
0
1
0
1
1
0
1
0
0
*
1
0
1
EmergentPropertiesandSwarmIntelligence
■ 111
Table 4.1: Patterns used for Tamura’s algorithm.
Figure 4.17: Thinning process by reaction-diffusion computing.
*
1
*
*
*

112
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Original image
(b) Middle stage
(c) Final stage
Figure 4.18: Thinning process by reaction-diffusion computing.
4.2
Queuing Theory and Trafﬁc Jams
4.2.1
Most Random Customers and Poisson Arrival
When does the wait occur? Let us model a customer arriving at random (most
irresponsible customers). Dice and roulette can be named as the representatives
of the randomness.
The randomness of the dice includes three elements:
Independence The next face that comes out does not depend on the faces
that have come out so far. It does not depend on the past.
Stationarity The probability of the face being shown does not depend on
time. Dice does not wear down.
Binarity
There are six possible outcomes, from 1 to 6, with no intermediate
values.
Independence is also said to be memorylessness.
Considering the most random customer from analogy to the dice, it becomes
as follows:
Independence The probability that the customer will come in unit time is
the same on average.
Stationarity The probability of the arrival within a certain time does not
depend on when that time begins.
Scarcity
Two people will not come in a minute time (its probability is
zero).
Binarity was replaced by scarcity.

Emergent Properties and Swarm Intelligence
■
113
As is well known, the probability that after rolling the dice n times, the face
“1” comes out k times is:
nCk
1
6
k  
1−1
6
n−k
(4.5)
In this way, an experiment in which trials are repeated independently, an event
occurs with probability p and does not occur with 1 −p, is called a Bernoulli
trial. The probability of an event occurring k times after repeating the trial n
times is:
nCkpk(1−p)n−k
(4.6)
If the arrival of the random customer is thought of as an independent trial (judg-
ing whether or not customers arrive within ∆t time), the probability that k people
will come within the time t is the probability of a Bernoulli trial of an event oc-
curring k times within n trials. Note that the probability p that customers will
arrive at each interval is:
p = λ ·∆t = λ · t
n,
(4.7)
and λ is the average arrival rate.
Deﬁnition 4.1
The number of people arriving within the unit time is called
the average arrival rate λ, and its unit is the number of people/hour.
Here, if the value n is made sufﬁciently large, the probability that the number
of people arriving at time t is k becomes the Poisson distribution of λt.
Theorem 4.1
The discrete probability distribution of the arrival number (per unit time) of the
average arrival rate of λ [people/hour] is described by Poisson distribution:
P(N = k) = λ k
k! e−λ.
(4.8)
Therefore, the arrival number of the most random customers becomes a Pois-
son distribution and is called Poisson arrival.
Figure 4.19 shows an example of Poisson distribution. Poisson distribution
has the following properties:
■
The average value and the variance are equal (λ).

114
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
As the average value increases,
■
Distribution becomes symmetric.
■
It approaches normal distribution.
Avg. ＝ 0.5
Avg. ＝ 2.0
Avg. ＝ 5.0
Avg. ＝ 1.0
0.0
0.2
0.4
0.6
0.0
0.2
0.4
0.6
0.0
0.2
0.4
0.6
0.0
0.2
0.4
0.6
0
1
2
3
4
0
1
2
3
4
5
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
9 10 11 12
Probability
Probability
Probability
Probability
Figure 4.19: Poisson distribution.
The stochastic process based on this distribution is modeled for a series of
random events and is called a Poisson process.
The density distribution of the time (arrival time interval) from the arrival of
one customer until the arrival of another customer is the exponential distribution
of the average 1/λ.
Theorem 4.2
The arrival time distribution of the average arrival time of 1/λ (time/person) is
an exponential distribution expressed as:
f(t) = λe−λt.
(4.9)
Figure 4.20 shows an example of exponential distribution. The exponential
distribution has the feature that both average and standard deviation are equal to
1/λ. Also, it is important to show the memorylessness5 explained in page 112.
5Stochastic variable X conforming to the exponential distribution satisﬁes the following equation:
P(X > s+t | X > s) = P(X > t), where s,t ≥0.
(4.10)

Emergent Properties and Swarm Intelligence
■
115
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
5
10
15
＝3
＝8
Avg.        ： 1/λ
Std. dev. : 1/λ
probability distribution  function
Figure 4.20: Exponential distribution.
The fact that the arrival time interval is an exponential distribution, is well
suited to the situation that “when it comes it keeps coming, and when it does not
come then it does not come easily.” In other words, the less time it takes for the
next customer to come, the more exponentially it grows.
4.2.2
Poisson Distribution and Cognitive Errors
Consider the following as a familiar problem of probability:
Assume that in the town where you live, there is a possibility of a lightning
strike at any time of the year. The frequency is about once a month. In other
words, the probability of lightning striking per day is about 0.03. Today on
Monday, lightning fell in your town. Then, which one of the following has
the highest probability of the next lightning strike? Answer with a reason.
(a) Tuesday tomorrow
(b) A month later
(c) Probability on any day is the same
(d) None of the above (write answers speciﬁcally)
This is a typical example of the Poisson process.
A summary of answers to this problem is shown in Table 4.26. Surprisingly,
most of the students chose the answer (c) that the probability is the same for
6Questionnaire survey from the lecture on “artiﬁcial intelligence” at the author’s university. Lecture
for 3rd-year undergraduate students. Each year about 100 people attend it. Participants are mainly from
engineering departments, with students from literature, law, and medical departments.

116
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 4.2: Comparison of the ratio of answers.
Probability of lightning strike
Frequency
(Answer)
Correct
16.5%
(Tuesday tomorrow)
Incorrect
67.4%
(Probability on any day is the same)
11.6%
(A month later)
2.0%
(Otherwise)
every day, and only about 17% of the students chose the correct answer (a), even
though students majoring in science were the main subjects of the questionnaire.
The reason as to why the correct answer is (a) “Tomorrow” is as follows. The
probability that the next lightning strike will be tomorrow (Tuesday) is 0.03. On
the other hand, the probability that the next lightning strike will be the day af-
ter tomorrow (Wednesday) is 0.03×0.97 = 0.0291, because it corresponds to a
situation that the lightning strike happens on Wednesday and it does not fall to-
morrow on Tuesday. Likewise, the probability that the next lightning strike will
be on Thursday is 0.03 × 0.97 × 0.97 = 0.0282. In this manner, the probability
decreases (exponentially) with every passing day. However, there are not many
people who understand it correctly. Despite changing the font and underlining
the word “next” to avoid overlooking, the answer “the probability on any day is
the same” is most common. Some answers were correct (a), but some students
answered with the wrong reason that “bad weather is likely to appear in series.”
There is a similar example with a hurricane. The frequency for a hurricane hap-
pening once in 100 years, is not important. Hurricane of this class hit Florida two
consecutive years from 2004, causing serious damage. As a result, the insurance
company (Poe Financial Group) lost 10 years worth of the surplus money and
bankrupted [31].
Steven Pinker,7 an evolutionary psychologist who conducted an internet ex-
periment, reported that 5 out of 100 solved the problem of lightning strikes cor-
rectly [93].
As mentioned in the previous section, the interval between events in the Pois-
son process is exponentially distributed. There are many short intervals, but the
longer intervals are less common. In other words, events that occur randomly
appear to be clustered.
However, people cannot easily understand this probability law. It is said that
mathematician William Feller was the ﬁrst to point out this cognitive illusion.
Feller gave an example of London bombing during World War II as an exam-
ple of the cognitive error [93]. London citizens felt that German V2 rockets re-
7Steven Arthur Pinker (1954–): American cognitive psychologist. He is a Harvard University Profes-
sor of psychology and the author of a large number of scientiﬁc publications including “How the Mind
Works,” “The Blank State: The Modern Denial of Human Nature,” and “The Language Instinct: How the
Mind Creates Language.”

Emergent Properties and Swarm Intelligence
■
117
River Thames 
The Regent's Park
The 
Cumber-
land
Figure 4.21: London bombing during World War II.
Table 4.3: Conformity to the Poisson law (Flying-bomb attack on London).
#. of ﬂying bombs
Expected #. of squares
Actual #. of
per square
(Poisson)
squares
0
226.74
229
1
211.39
211
2
98.54
93
3
30.62
35
4
7.14
7
5 and over
1.57
1
Total
576.00
576
peatedly hit a district while other districts were not damaged at all. From that,
they believed that the German army was capable of targeting particular areas.
Figure 4.21 shows the impact areas of 67 V2 rockets that fell in the center of
London. Looking at that, it is true that the damage on the upper right and lower
left is small compared to the upper left and lower right, where the damage is big-
ger. This makes it questionable whether a region in the city is safe, or whether
the bomb’s landing points form a cluster. In other words, is it not randomly dis-
tributed throughout London?
To solve this problem, it is best to do statistical tests, but it can also be com-
pared with Poisson distribution. Here, 144 square kilometers of southern London
will be divided into 576 parcels (= 24 × 24) each being a square of 1/4 square
kilometers. Then, classifying the parcels according to the number of bombs that
fell within the period is as shown in Table 4.3 (a total of 537 shots). This table
shows expected values of impacts in the parcels (based on the Poisson distribu-
tion of the average λ = 537
576) and measured values. From the comparison of the
columns of expected and measured values, it can be seen that they are quite con-
sistent. In other words, the distribution of this data is consistent with the Poisson
distribution, and the bomb wants to be dropped at random [17].

118
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
p
ppp
ppp
p
p
p
p
p
p
p
p
p
p
pp
p
pppp
pp
p
p
pp
pppppppp
ppp
p
pppp
p
p
pp
pp
p
p
p
p
p
ppppp
p
ppppppppp
p
p
ppp
pp
pp
pp
p
p
ppp
pp
pp
p
pppppp
pp
pp
p
pp
pp
p
p
p
p
p
p
p
pp
pp
p
p
p
p
p
p
p
p
pp
pppp
p
ppp
ppppppp
pppp
p
ppp
ppppppp
pppppppp
pp
p
ppppp
ppp
pp
pp
pppp
p
pppp
ppp
p
p
p
pp
pp
pp
p
ppppp
pppp
p
pp
p
p
p
p
pp
p
pppp
p
ppp
ppppp
p
pp
pp
p
ppp
p
p
ppppp
p
p
p
p
p
pp
pppp
p
ppp
ppp
pp
ppp
pp
p
pp
pp
p
p
pppppp
pppp
pp
ppp
pp
ppppp
ppppp
p
p
p
pp
p
p
p
p
p
p
p
ppppppppp
pp
p
pp
pp
p
p
ppp
p
ppp
p
pppppp
p
ppppppp
pppp
pp
ppp
p
ppp
ppp
p
pp
ppp
p
pp
pp
pppp
ppp
p
ppp
p
p
pppp
p
pp
ppp
pp
ppp
p
p
pp
p
p
ppppp
p
p
p
ppp
p
p
pp
ppppp
p
p
p
p
p
pppp
p
pp
pp
p
p
pppp
p
p
ppp
p
ppp
p
p
p
p
p
pppp
p
p
pp
pp
p
p
pp
p
p
pp
pp
p
pp
p
ppp
pppp
pp
pp
pp
p
p
pp
ppp
p
pp
p
p
p
p
p
ppppp
pp
p
p
p
p
p
ppp
pp
pp
ppp
p
p
p
ppp
p
ppp
pp
p
pp
p
p
pppppp
ppp
pp
p
pp
pppp
p
p
pp
p
p
ppp
p
pppp
p
ppp
pppppp
pp
p
p
p
p
ppp
p
ppp
p
p
p
p
p
ppp
p
p
p
p
p
p
pp
p
pp
pp
pp
p
p
p
p
ppp
p
p
pp
p
pppp
p
pp
p
p
ppp
pp
p
pp
pp
p
p
pp
pp
p
p
pp
p
pp
pppp
ppp
pppp
p
p
pp
p
pp
pp
p
pp
p
ppp
p
pp
p
pp
ppp
pppppppppp
p
pp
pppppp
pp
p
pp
pp
pp
p
pp
ppp
pp
pp
pp
ppp
p
ppp
pp
pp
pp
ppp
ppppppp
pp
pppp
p
p
pppp
ppp
p
ppp
pp
p
pp
pp
pp
p
p
p
pp
pp
pp
p
ppp
p
pp
p
pppp
pp
p
p
pp
p
pp
p
pppppp
p
p
p
pp
p
pppp
p
p
pp
pp
ppp
pp
p
p
p
p
ppp
p
ppp
p
pp
p
p
p
pp
p
ppp
p
p
p
p
pppppppp
p
p
p
pppp
p
p
ppp
p
p
p
pp
pppp
pppp
p
p
pp
p
pp
p
p
pp
p
p
p
p
pp
pp
pp
p
ppppp
p
p
pp
ppp
pp
ppp
p
p
p
p
p
pp
pp
pp
pp
pppp
p
pp
p
pp
pppp
p
pp
p
p
p
pp
p
p
pp
p
p
pp
pp
ppppppp
p
p
p
ppp
p
pp
p
p
pppp
pp
pp
p
pppp
pp
ppp
p
ppppp
pp
p
pp
pp
p
p
p
ppp
p
p
pp
p
p
pppp
p
p
p
pp
pp
pppp
p
p
p
p
p
p
p
ppp
p
p
p
pp
ppp
p
p
pp
p
pp
ppp
p
ppppppp
p
p
p
p
pp
p
p
ppp
p
p
pppp
pp
p
p
pp
pp
pp
p
ppp
p
p
ppppppp
p
p
p
ppp
p
p
p
p
pp
p
p
p
p
p
p
ppp
pp
pp
pp
pp
p
p
p
p
p
p
p
p
p
pp
p
pp
ppp
p
pp
p
ppp
p
pppp
ppp
p
p
p
p
p
pppppp
pp
p
pp
p
p
p
pp
p
p
p
p
p
p
p
p
ppp
p
ppp
pp
p
ppppp
p
p
p
pp
ppp
p
p
p
p
p
ppp
ppp
p
ppp
pp
pp
p
pp
pp
p
ppp
p
pp
pp
p
p
pp
p
pppp
p
p
p
pp
p
p
pp
pp
pp
pppp
p
ppp
pp
p
pp
p
pp
pp
p
p
pp
ppp
ppppp
p
pp
p
p
p
p
p
pppp
p
pp
pp
pp
p
p
p
p
p
p
p
p
p
pp
p
p
p
ppp
ppp
p
p
p
pp
p
p
pp
pp
p
p
p
p
p
p
pp
p
p
pp
pp
p
pp
p
p
pppp
p
p
pp
pp
pp
p
p
p
p
p
p
p
p
pp
p
pp
p
p
pp
p
p
p
ppp
p
p
p
ppppp
p
ppppppppp
pp
pp
p
pp
p
p
p
pp
ppp
p
pp
ppp
p
pp
p
pp
p
p
pp
p
pp
pp
p
p
p
p
ppp
ppp
p
pppp
pp
ppp
p
p
pp
p
p
p
p
pp
p
p
p
ppp
p
p
p
p
p
pp
pppp
pp
p
ppp
p
p
p
p
pp
pppp
p
pppp
pppp
p
pp
p
p
ppp
pp
p
p
pp
ppp
p
pp
pp
pp
pppp
p
pp
p
p
p
p
p
pp
p
p
p
p
p
p
p
pp
ppp
p
p
ppp
ppp
p
pp
pp
p
pp
p
ppp
pppp
pp
pp
ppp
p
p
p
pp
p
p
p
pp
ppp
pp
pppp
ppp
pp
pp
p
p
p
p
p
p
ppp
p
p
p
p
p
pp
pp
p
p
pp
pppp
p
pp
p
p
p
p
p
pppppp
p
p
p
p
pp
p
pp
p
p
p
p
p
p
pp
pppp
p
p
p
pp
p
p
p
pp
p
p
p
ppp
p
p
pp
p
p
pp
p
p
ppp
p
p
p
pp
ppp
p
p
p
p
p
ppp
p
pp
pp
ppp
p
p
ppp
p
p
p
p
p
p
p
p
p
p
pp
p
p
pp
p
p
pppp
pp
pp
p
pp
pp
p
pp
pp
p
ppp
p
p
p
p
p
p
pp
pp
p
pppp
p
p
p
pp
p
p
p
p
pp
ppp
pp
p
p
ppppp
ppp
p
pp
p
p
pppp
p
p
p
pp
pp
p
p
p
p
p
p
p
p
ppp
p
p
p
pp
p
pppppp
pp
pp
pp
p
pp
p
p
ppp
p
p
ppp
p
ppp
pp
pp
p
p
pp
pp
ppp
ppp
p
p
pppppp
pppppppp
ppp
p
pp
pppppp
p
ppp
p
p
pp
p
pp
p
pp
p
p
pp
ppp
pp
p
pp
p
p
pp
ppp
p
p
pp
p
pp
p
ppp
p
ppp
p
pppp
p
p
Figure 4.22: Simulation of point generation in two-dimensions: Which is random?
Let us introduce an interesting example pointed out by Stephen Jay Gould 8.
Since ancient times, the human race has been interested in the alignment of stars,
gave the names to the constellations, and sometimes felt the mysterious power
in conglomerations of stars [42]. Is the sequence of stars random, or is there any
intention behind it?
Let us see Fig. 4.22. One of them is a diagram generated by random simula-
tion. Which one looks like a sequence of stars? Most people feel that the left is
a sequence of stars and that the right is a random sequence. Actually the truth is
opposite. The ﬁgure randomly generated by Poisson distribution is the left ﬁgure
of Fig. 4.22 (1,100 points were generated on 100×100 grid). On the other hand,
The right ﬁgure is a model of carefully placed lightning bugs (or ﬁreﬂies, see
also sec. 3.4), so that each point does not come close in consideration of each
territory.
In this way, humans see some rules in random models, at the same time mis-
takenly recognizing randomness in artiﬁcial models.
In other words, humans cannot easily understand the randomness of Poisson
distribution, and they misunderstand that random events are made of clusters
and regularity. To create equal intervals, a non-random process is required. As a
result, mistakes, such as seen in Table 4.2, are made. This also leads to criticism
that true human intelligence (strong artiﬁcial intelligence) cannot be realized.
This is because the main technologies used in current artiﬁcial intelligence and
machine learning are strongly dependent on stochastic reasoning and statistics.
8Stephen Jay Gould (1941-2002): American paleontologist and evolutionary biologist. He advocated
the punctuated equilibrium theory of evolutionary process. He wrote an essay every month for American
magazine “Natural History,” and many books that summarized it became best sellers. As a researcher of
the same evolutionary theory, Richard Dawkins (see the footnote in page 185) was his opponent in debate.

Emergent Properties and Swarm Intelligence
■
119
Daniel Kahneman9 and Amos Tversky10proposed the following two ideas for
such human cognitive errors.
■
Representative heuristics: It is easy to overestimate the probability of
the items similar to typical examples.
■
Law of small numbers: In statistics, the great law of approaching the
theoretical value by increasing the number of trials. It is easy to think
that humans can get close to the theoretical values even with a small
number of trials, and a psychological bias is applied.
Gambler’s fallacy will be considered as a representative heuristic. Which is
easier to get after rolling a fair dice 7 times, 1111111 or 5462135? Stochastically
thinking, both are the same (see memorylessness on page 112). However, when
numbers are arranged like 5462135, it is regarded as a representative example of
randomness11. On the other hand, Bayes’ Law can justify this error. It depends
on strengthening the prior probability that in 1111111 it is easier to roll 1 (it is a
cheating dice). 5462135 has no such surprises.
In this way, it seems that reports such as winning lottery tickets being con-
tinually sold at the same counter, and random playback of music players playing
the same song12, become more convincing.
4.2.3
Queue Management and Scheduling
Depending on the customer’s ﬂow, the queue is formed as follows (see Fig. 4.23).
1. When customers arrive according to arrival distribution, they enter the va-
cant counter to receive service. The service is provided in a ﬁrst-in, ﬁrst-
out (FIFO) order.
9Daniel Kahneman (1934–): Israel-born American psychologist. He proposed the behavioral eco-
nomics and the prospect theory, integrating psychology and cognitive science, and received the 2002
Nobel Prize in Economics.
10Amos Tversky (1937–1996): Israel-born psychologist. He created the prospect theory in a joint re-
search with Kahneman.
11However, if you ask this question as follows, the answer will change. “Yesterday I rolled the dice 7
times. Those numbers are written on this paper. Which one is it?” Also, the expected number of times to
roll the dice to get 1111111 is 67 + 66 + ··· + 6, whereas the expected value of 5462135 is 67 + 6. This
difference in the number of times may be the cause of the error.
12In
February
2015,
Spotify,
internet
based
music
distribution
service,
was
accused
of
the
suspicion
that
song
selection
of
the
shufﬂe
play
was
bi-
ased.
See
also
http://www.dailymail.co.uk/sciencetech/article-2960934/
Is-music-player-really-random-Spotify-says-users-convinced-strange-patterns-\
\shuffle-playlists.html

120
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
＝  ）
1
2
……
Arrival
（a）
Service（window size＝  ）
1
2
Arrival
Full
……
（b）
Queue system
（system Length）
   ＝  ）
1
2
Arrival
Queue (queue length)
……
（c）
   ＝  ）
1
2
Arrival
Queue (queue length)
The customers 
who have ﬁnished
the service, 
leave the counter.
……
（d）
   ＝  ）
1
2
Arrival
Queue (queue length)
The customer 
at the head of 
the queue enters 
an empty counter.
……
（e）
Service（window size
Service（window size
Service（window size
Service（window size
Full
Figure 4.23: Customer’s ﬂow and queue.
2. When customers arrive one after another, the service counters get full.
3. Next customers cannot enter the counter, so they make a queue and wait.
4. When customers come one after another, the queue is extended.
5. The customers who have received the service, leave the counter.
6. The customer at the head of the FIFO queue, enters an empty counter and
receives service.
7. Repeat the above process.
The basic elements that make up the queue in this way are as follows:
■
Number of counters (Window size)
■
Customer arrival distribution
■
Service time distribution

Emergent Properties and Swarm Intelligence
■
121
What is often used is, arrival of the customers according to the most random
arrival (Poisson arrival) and the distribution of service time deﬁned as an expo-
nential distribution (that is, the number of people leaving is Poisson distribution).
By modeling like this, it is possible to solve the phenomenon of the queue
analytically under some conditions. This is based on the derivation of the equi-
librium state (steady distribution), which looks at the stable conditions of the
system. Using it allows the system designer to control the queue. Since many
queuing models cannot be solved analytically, queue simulators13 are used. For
example, the management strategy of a food shop, where queues are created, can
be designed based on labor costs and customer trends.
In addition, it is possible to handle queues more efﬁciently using scheduling
theory. Let us consider doing jobs accumulated in the queue one at a time. In
other words, when multiple jobs (tasks) have to be executed, it is a matter of
determining the order in which one person (or one CPU) will work
In this case, the priority rule (dispatching rule) of job assignment becomes
important. This rule speciﬁes how to resolve conﬂicts when they occur (i.e., con-
ﬂict resolution). A conﬂict means that multiple executable jobs simultaneously
request the job to start (competing). Typical dispatching rules include the follow-
ing:
■
SPT (shortest processing time)
Choose the job with the shortest processing time. This will shorten
the average delivery delay, but will also generate jobs with large de-
livery delay.
■
EDD (earliest due date)
Choose the job closest to the due date. It is valid for the evaluation
with respect to the delivery delay.
■
SLACK (SLACK time)
Choose the job with the smallest SLACK time (= delivery date - cur-
rent time - remaining processing time). Valid for evaluation with re-
spect to the deadlines.
■
RANDOM
Execute the task at random. Although it seems inefﬁcient at ﬁrst
sight, some degree of performance is guaranteed.
■
FIFO (ﬁrst-in, ﬁrst-out)
A neutral rule. It has the same characteristics as RANDOM, but there
is less variation in the result than in RANDOM.
13The queue simulator developed by the author can be obtained from the homepage http://www.iba.
t.u-tokyo.ac.jp/iba/SE/mimura/howto/HowTo.html

122
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Performance of the scheduling depends on what kind of priority rule is
adopted. The difﬁculty of scheduling lies in:
■
Randomness of the arrival of each task
■
Randomness of processing time of each task
The author has taught at university and instructed many students, and they
always have multiple reports with varying degrees of difﬁculty and deadlines. In
some cases, it may not be known beforehand when an assignment will be issued
(Poisson arrivals). If the report assignment is not processed appropriately, the
queue of reports to be submitted will become longer, and one day it will fail, and
reports will not be submitted. As expected, many students take an EDD strategy.
However, this strategy is not necessarily good. Although the SLACK strategy
seems to be relatively efﬁcient, it is necessary to estimate the “estimated pro-
cessing time” correctly. There is a heuristics of scheduling, “Do it immediately”
(It does not have to be 100%, but do it ASAP). This means the importance of
at least trying a little bit, regardless of the deadline, when given the tasks. Then
it becomes possible to estimate the processing time beforehand, which makes it
possible to take a SLACK strategy or use more general scheduling method.
In order to verify this, a 10-semester simulation was conducted and the seri-
ousness of the students was compared (the time spent on reports per day) with the
results (the number of credits acquired) in each strategy. Looking at the results,
it is difﬁcult for negligent students14 to use the most effective scheduling, and it
is best to digest the problem with the SPT (Fig. 4.24(a)). It is more efﬁcient to
prioritize some of the subjects by giving up on others from the beginning, than
trying a little bit in all of them and failing. Also, even for excellent students, the
difference due to dispatching rules will not increase (Fig. 4.24(c)). If a student
works diligently every day in his own way, he will be able to score well. This
may be natural in some ways.
0
SPT
EDD
       RANDOM
FIFO
SLACK
2
4
6
8
10
12
14
16
Numbers of credits 
0
SPT
EDD
FIFO
SLACK
5
10
15
20
25
       RANDOM
Numbers of credits 
0
SPT
EDD
FIFO
SLACK
5
10
15
20
25
       RANDOM
Numbers of credits 
(a) Negligent students
(b) Average students
(c) Excellent students
Figure 4.24: Numbers of credits acquired in each strategy.
14Diligence is deﬁned as hard work at home.

Emergent Properties and Swarm Intelligence
■
123
On the other hand, efﬁcient scheduling is meaningful for the majority of av-
erage students (Fig. 4.24(b)). It can be seen that the SLACK strategy is relatively
good. SLACK will not be an optimal strategy if one of the tasks is extremely
difﬁcult. In heavy tasks, the term “remaining processing time” in SLACK is in-
creased, and other tasks are neglected. As a result of this, the overall grades may
become worse. However, in SLACK, the rate at which tasks can be submitted be-
fore the deadline is increased. Therefore, it is a relatively load resistant strategy
even compared to other strategies.
It was found out that the strategy that does not take anything into considera-
tion, such as RANDOM, works effectively. Interestingly, sometimes it is better
than the SPT or EDD. In the ﬁelds of artiﬁcial intelligence and optimization, ran-
dom operations may be successful. For example, UCT search for Monte Carlo
trees, which recently has drawn attention as a search strategy in games (see page
30) and a random selection algorithm are typical examples.
As for the scheduling methods described above, it has been shown that the
emergent approach, such as GA, GP, PSO and ACO, etc., is very effective. In
fact, an efﬁcient method based on evolutionary computation has been proposed
to approximately solve NP-complete problem, called the JSSP (job scheduling
problem,indexproblem!job scheduling problem see [53, ch.2.8.2] for details), in
a reasonable time, and was applied to a variety of practical problems.
There is a method called ramp metering, which is used to counter trafﬁc con-
gestion and is often seen in the United States these days. In this method, a trafﬁc
signal is placed in front of the accelerating lane entering the expressway to con-
trol the inﬂowing cars (Fig. 4.25). The trafﬁc light alternates between red and
blue. For the most part, it is red, and once every few seconds it becomes blue for
a moment and only one car is allowed to enter at that moment. It is known that
introducing a ramp meter improves trafﬁc congestion. In fact, stopping the ramp
meter in Minnesota caused it to look like the following [31].
Only one car is 
allowed to enter.
Figure 4.25: Ramp metering.

124
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
Trafﬁc at peak time was decreased by 9%.
■
The required time was increased by 22% and the reliability declined.
■
The speed of the car dropped by 7%.
■
Collision accidents during conﬂuence sharply increased by 26%.
However, according to a subjective survey of the drivers, it was found out that
they were not necessarily satisﬁed. The reason is that waiting for the ramp meter,
even though the actual total time is shortened, makes drivers feel as if they did
not get to the destination quickly. It would be better to run, even slowly, than to
stop at a ramp meter. In other words, drivers do not like to be kept waiting, and
they feel frustrated because they are not in control.
In Disneyland, the following two methods were introduced to efﬁciently con-
trol the queue:
■
Fast Pass: Go to the venue near the attraction beforehand and obtain the
fast pass. You can enter the attraction without waiting, during the time
zone speciﬁed on the fast pass. Merit for the customers using fast pass
is the reduction of time spent in queues for popular attractions, while
Disneyland reduces the variance of the arrival time. The number of fast
passes issued in a certain time period is ﬁxed.
■
The indication of the long expected wait time: The customer is able to
ride more attractions than was originally planned by making a sched-
ule with a long-expected waiting time, thus, the satisfaction level is in-
creased15.
According to the real experience, since admissions using fast pass are over-
whelmingly less in numbers than the general admissions, thus, there seems to
be no dissatisfaction that ordinary customers were overtaken at the time of the
meeting. In addition, since the number of times fast pass can be used is lim-
ited, and while waiting for the arranged time zone, customers wait in queues for
other attractions and in total the number of visited attractions does not differ so
much between the customers. Therefore, customers are less dissatisﬁed, and the
variation of the arrival rate is relaxed, which is considered a great advantage.
Even if the “long wait time” is shorter than the actual time, the customer
feels more satisﬁed. On the other hand, it seems that there are many customers
who give up the attraction they wanted to ride and go home because they were
afraid of waiting too long for their turn. In this case, unnecessary complaints
15Another theory is that the longer display is unintentional. It is due to measurement error by the Flick
Card.

Emergent Properties and Swarm Intelligence
■
125
are generated from customers. It is also said to reduce the motivation of the
customers to line up for thrilling vehicles16.
There is a popular website “touringplans.com” to create an itinerary to go
around more attractions, with as few wait times for Disneyland and Disney
World17. There are many enthusiastic fans of this page. For example, in 2003,
Edward Waller and Yvette Bendeck have achieved the feat of conquering all the
attractions of the Magic Kingdom in less than 11 hours18.
On this homepage, the itinerary was created manually by combining queuing
theory and the method of assignment problems by linear programming. This is
based on the queuing theory, as described in section 4.2.3. It is said that around
37% more of the attractions were visited by using the proposed tour plan. Be-
cause the limitations of this approach are seen, the original method has been ex-
tended in order to estimate the waiting time from the past data and recent trends
so as to solve the traveling salesman problem19 with evolutionary computation.
This successfully reduced the wait time by about 90 minutes compared to the
past. Of course, it is also essential to collect good data to create a tour plan. In
fact, for more than several years, the investigators have been placed in a theme
park to collect data, such as expected wait time. In addition, many members of
touringplans have participated in this data collection.
The example described in this section merely suggests the limitation of con-
trolling the queue with optimal criterion optimization. A more ﬂexible model
incorporating human sensibility will be necessary.
4.3
Silicon Trafﬁc and Rule 184
Rule 18420 is known as the Burgers cellular automaton (BCA), and has the fol-
lowing important characteristics (Table 4.4):
1. The number of 1s is conserved.
2. Complex changes in the 0-1 patterns are initially observed, however, a
pattern always converges to a right-shifting or left-shifting pattern.
16https://www.mouseplanet.com/11037/How_Posted_Wait_Times_Compare_To_Your_
Actual_Wait_In_Line
17https://touringplans.com/,https://www.wired.com/2012/11/
len-testa-math-vacation/
18https://touringplans.com/walt-disney-world/touring-plans/ultimate/
hall-of-fame
19Traveling Salesman Problem (TSP):When there are several cities located on the map, the problem
of asking for the minimum length of the closed path (called Hamiltonian closed path) that returns to the
original by passing through a city once. It is known as a typical NP-complete problem, and there are many
applications to circuit wiring and delivery. See [53, ch.2.8.1] for details.
20Refer to page 14 for this decimal representation of one-dimensional cellular automata.

126
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 4.4: Rule 184.
ai−1
t
,ait,ai+1
t
111
110
101
100
011
010
001
000
ai
t+1
1
0
1
1
1
0
0
0
(a) α = 1.0,no signal
(b) α = 0.8,no signal
(c) α = 0.8,signal type 110
Figure 4.26: Congestion simulation using BCA.
Take ai
t as the number of cars on lattice point i at time t. The number is 1
(car exists) or 0 (no car exists). The car on lattice point i moves to the right
when the lattice point is unoccupied, and stays on the lattice point when occupied
at the next time step. This interpretation allows a simple description of trafﬁc
congestion using rule 184.
BCA can be expanded as follows:
■
BCA with signals: Restrict trafﬁc to the right at each lattice point
■
Probabilistic BCA: Move with a probability (α)
Figure 4.26 shows a result of BCA simulation. The top row shows the current
road conditions, and the time passed as we move down. Cars are shown in red
in this simulator. Continuous red areas represent congestion because cars cannot
move forward. Congestion is more likely to form when the probability α that a
car moves forward is small. (a) shows how congestion forms (red bands wider
than two lattice points form where a car leaves the front of the congestion and
another car joins at the rear). A signal is added at the center in (c). The signal
pattern is shown in blue when 1 and red when 0, and the pattern in this simulation
was 110 (i.e., blue, blue, red, blue, blue, red,···).
A simulation of silicon trafﬁc based on these models is shown in Fig. 4.26. A
two-dimensional map is randomly generated, and cars are also placed at random.
Two points are randomly chosen from the nodes on the map, and are designated
as the origin and the destination. The path from the origin to the destination is
determined by Dijkstra’s algorithm. Here, a cost calculation, where the distances
between nodes are weighted with the density of cars in each edge, is performed in
order to determine a path that avoids congestion. The signal patterns are designed
to change the passable edge in a given order at a constant time interval.

Emergent Properties and Swarm Intelligence
■
127
Figure 4.27: Simulation of silicon trafﬁc.
The following is another explanation of trafﬁc modeling. Nagel et al. modeled
trafﬁc congestion using a one-dimensional CA [80]. In their model, each cell
corresponds to a position where cars can pass, and each cell can take one of
two states (one or no car is in the cell) in every time step. Every car moves at a
characteristic speed (integer value from 0 to vmax). The speed of the cars and
state of the cells are updated based on the following rules:
Acceleration Increase the speed by one unit (v := v+1) if the speed of a car
v is less than vmax and the distance from the ﬁrst car ahead is greater
than v+1.
Deceleration Decrease the speed of a car at i to j −1 (v = j −1) if the ﬁrst
car ahead is located at i+ j and j ≤v.
Random number Decrease the speed of all cars by 1 (if larger than 0) at
possibility p (v := v−1).
Move Move all cars at their respective speed v.
The most important parameter in this simulation is the density of cars ρ deﬁned
as
ρ = total number of cars
total number of cells
(4.11)
Figure 4.28 is an example of the ﬂow of cars with vmax = 5 and ρ = 0.1. Cars
move from left to right, and the right end is connected to the left end. The num-
bers indicate the speed of the cars, and the dots show cells with no cars. Consec-
utive cars at speed 0 at the center indicate trafﬁc congestion.

128
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
time 
space (road) 
Figure 4.28: An example of the ﬂow of cars.
Nagel et al. conducted various experiments and found that the nature of con-
gestion changes at ρ = 0.08. Note that congestion is measured by the average
speed of all cars. Thresholds such as ρ = 0.08 are called critical values and are
important parameters in the simulation of complex systems.
Nagels et al. performed the modeling of trafﬁc jams using a one-dimensional
lattice. In their model, each cell corresponds to the vehicles’ passage, at each
time-step, the cells take one of the two states (vacant or occupied by one vehi-
cle). All the vehicles move with a speciﬁc speed (integer number, 0 to vmax).
For example, the movement of a car with vmax = 10,ρ = 0.1 is shown in Fig.
4.29. The vehicle moves in the right-hand direction, and the right end and the left
end are connected. The top row shows the current road conditions, and the time
passed as we move down (displayed till 200 steps). The vehicles’ speed is rep-
resented by shades of colors. In fact, green becomes darker with the increase in
speed, and the red becomes darker as the speed decreases. A chunk of red color
is the point of occurrence of a trafﬁc jam, and the black part shows a cell without
a vehicle. In Fig. 4.29, the occurrence of a series of vehicles with speed 0 in the
middle is a trafﬁc jam. We can see that the trafﬁc jam moves forward with time.
In this model, SlowtoStart has been introduced to realize the effect of inertia.
This is a rule that says, once the vehicle has stopped, then it starts moving after 1
time step even if the front is open to move. This is considered important to bring
the model closer to the actual stability. If SIS is used, it takes time to accelerate,
which leads to more stationary vehicles and worsens the trafﬁc jam (Fig. 4.29).
On the other hand, without SIS, the line of the stationary vehicles will not elon-
gate unless there is a slowdown due to a random number, and, as a result, the
trafﬁc jam is eased (Fig. 4.30).

Emergent Properties and Swarm Intelligence
■
129
Figure 4.29: Simulating trafﬁc jam (with SIS).
Figure 4.30: Simulating trafﬁc jam (without SIS).
Recently, ASEP (Asymmetric Simple Exclusion Process) is being studied
extensively as a model of trafﬁc ﬂow [96]. In this model, the maximum speed of
each vehicle existing in each cell is taken as 1, if the cell in front is vacant, the
vehicle moves to it with a probability p (stops with a probability 1 −p). Let us
take the inﬂow and outﬂow probabilities of new cars as α and β, respectively (in
other words, right and left ends are not connected, and the number of vehicles is
not ﬁxed). Figure 4.31 shows the ASEP model simulation. Parameters are α =
0.3,β = 0.9,p = 0.5 for (a) free phase, α = 0.3,β = 0.3,p = 0.5 for (b) shockwave
phase, and α = 0.6,β = 0.6,p = 0.5 for (c) maximum ﬂow phase. As explained
earlier, the topmost row is the current road situation. As we go down, it shows
the time already passed. In the ASEP model, various mathematical analyses are
done for the equilibrium state [96].
Southeast Asia has many lanes with a dead end. They come together to form
a main line for traveling back or passing one another. Main lines usually have
some U-turn points for traveling to the opposite side. However, making a U-turn
is not immediately possible because the main line is large. Furthermore, trafﬁc is
usually backed up for about an hour due to the heavy trafﬁc volume of the line.
Color version at the end of the book
Color version at the end of the book

130
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) free phase
(b) shockwave phase
(c) maximum ﬂow phase
Figure 4.31: ASEP model simulation.
Figure 4.32: Main line with U-turn places.
Thus, main lines must have U-turn points, however, these installations have
some trade-offs.
■
With fewer U-turn points, the cost of cars running in the wrong direction
would increase.
■
Too many U-turn points would generate a trafﬁc jam because cars making
U-turns would hinder the trafﬁc.
Considering this optimization, we shall perform a simulation experiment.
Figure 4.32 shows a model of a road in Southeast Asia. It comprises one-way
lanes and a long main line. The two-way main line has U-turn points so that
vehicles traveling along the one-way lane can move to another place. One-way
lanes have either exit only (out edge) or enter only (in edge).
The experiment described here was performed with several conditions. In the
simulation described above, it was considered that a vehicle’s movement was
only the distance between the vehicle in front and the intersection of a red signal,
and an angle at the signal was excluded. To make a natural U-turn, the vehicle
slows down at the U-turn point, turns to the vertical direction and waits until
sufﬁcient space is gained in the opposite lane. Accordingly, we performed simu-
lations with the conditions set as follows:

Emergent Properties and Swarm Intelligence
■
131
■
The following one-dimensional equation of motion is used to estimate
the necessary distance (S1) for the car to slow down:
S1 = v2
now −v2
turn
2a
× 2θ
π .
(4.12)
In this formula, vnowand a represents the current velocity and accelera-
tion of the vehicle, respectively. θ represents an angle toward the des-
tination21. vturn represents an appropriate velocity for making a U-turn
and should be 20% of the maximum velocity. The distance S1 decreases
according to θ, i.e., the direction toward the destination. If S1 is smaller
than the distance to the U-turn point, the vehicle should make the U-turn.
■
The following formula determines the distance for a vehicle for the mini-
mum distance to the same U-turn point among those heading for it in the
opposite lane to stop there:
S2 = v2
now
2a
(4.13)
If S2 is shorter than the actual distance to the U-turn point, the vehicle
avoids making the U-turn because it cannot safely stop.
■
Any vehicle must clear the way for an ambulance to pass by. Therefore,
the acceleration should be decreased to half.
As previously stated, since one-way lanes are either exit only (out edge) or
enter only (in edge), selecting any one from each of the six out edges and six in
edges generates 924 combinations. Moreover, the main lines can be selected in
two ways, which generates 1,848 combinations for selection. For these combina-
tions, we performed the simulation with 100 running vehicles for a trafﬁc rate of
0.02. Figure 4.33 shows the plot between the number of U-turns made and the ac-
tual time taken to make the U-turn. It indicates that the more clever the decision
making process for the U-turn with repetitions, the shorter the time to reach the
destination (the averaged delay time). Nevertheless, this result was made under
limited conditions, therefore, further detailed data analysis is required.
The same trafﬁc simulation experiments have been conducted in a two-
dimensional map (see Fig. 4.34 for example).
4.4
Segregation and Immigration: What is Right?
The two different colored pieces (referred to as agents below) are arranged in
two-dimensional space. In that space, each agent wants its neighborhood (Moore
210 ≤θ ≤π
2 and θ becomes closer to 0 as the destination is behind. In the case that the destination is
somewhere after making a turn and moving straight ahead, θ = π
2 .

132
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 4.33: The result of U-Turn road experiment.
Figure 4.34: Trafﬁc simulation with a two-dimensional map.
neighborhood, 8 neighbors up, down, left, right and diagonal) to have the same
color as itself. In other words, if the colors of the surrounding pieces are different
from itself, it moves from there.
This is Thomas Schelling’s segregation model [107]. Schelling22 has studied
how macro-level phenomena emerge from micro-level preferences. Here, prefer-
ence is the threshold percentage of how many neighbors should be of the same
color. The following are the basic rules of Schelling’s model.
22Thomas Schelling (1921–2016): American social scientist and economist. Awarded Nobel Prize in
Economics in 2015.

Emergent Properties and Swarm Intelligence
■
133
Figure 4.35: Thomas Schelling’s segregation model. The original experiments were conducted
with 23 pennies and 22 dimes.
■
All agents belong to one of two groups, and have a preference value
on neighbors of the same color.
■
Agents calculate the ratio of neighbors of the same color.
■
The agent stays if the ratio is above the preference value. Otherwise,
the agent randomly moves to an empty space where the preference
criterion is satisﬁed.
In Schelling’s era, computers were not familiar, so he placed coins (penny
and dime currency) on the chess board, rolled a die and manually experimented
(see Fig. 4.35).
Figures 4.36–4.38 show actual experiments following the above rules. In the
initial state in which the two groups (i.e., tribes of different colors) were alter-
nately intertwined, when the preference was set to 50%, the group became clearly
divided and stabilized (Fig. 4.36). On the other hand, if the preference was set
to be smaller than 33%, it stabilized in the initial state. The white piece in the
ﬁgure indicates vacant land. If the preference was set to be 50% for three group
case, then the population became completely isolated (Fig. 4.37). On the other
hand, if the preference was 33%, a part of the group was stabilized while be-
ing surrounded by other groups (Fig. 4.38). Also, the number of iterations until
reaching stability is considerably smaller when the preference was 33%.
Schelling’s results can be summarized as below:
1. Two colors segregated when the preference value exceeded 0.33. Thus, the
critical value for this simulation is 0.33.
2. When the preference of one color was set higher than that of the other
color, the color with the low preference spread out while the color with
the high preference came together.

134
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 4.36: Simulation of segregation (2 tribes, 50% preference).
Figure 4.37: Simulation of segregation (3 tribes, 50% preference).
Color version at the end of the book

Emergent Properties and Swarm Intelligence
■
135
Figure 4.38: Simulation of segregation (3 tribes, 33% preference).
3. The two colors also segregated when the preference criteria was changed
to “the agent stays when there are three or more neighbors of the same
color.”
In summary, two colors ultimately segregated, and all agents wanted all neigh-
bors to be of the same color. Schelling also found that the addition of relatively
small changes to the preferences resulted in drastic changes to the macroscopic
segregation patterns. In particular, the “prejudice”, or bias in preference, of each
agent and the segregation pattern is correlated. “Color blind” (paciﬁsts with pref-
erence criteria of zero) agents act to mix the colors. However, many critical
values exist for these effects. The most signiﬁcant ﬁnding is that society as a
whole moves toward segregation even when agents only slightly dislike the other
color (low preference threshold). Schelling’s results highlighted many problems
regarding racial discrimination and prejudice, and extensive research using im-
proved versions of this model is still being conducted.
Schelling’s results have raised many problems on “segregation” and “prej-
udice.” A related issue is afﬁrmative action (policies that actively counter dis-
crimination to beneﬁt under-represented groups based on historic and social en-
vironmental reasons).
An example is Starrett City in Brooklyn, NY, the largest federally assisted
public rental housing scheme in the USA for middle-income residents that started
construction in the mid-1970s. There was a requirement for residents that “lim-

136
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
ited the number of African-American and Hispanic residents to less than 40%
of the total number of residents” to create a community where different races
co-exist. The reason behind this policy is the theory of the “critical value” in the
segregation model. Races do not mix when the ratio of whites becomes less than
a certain value because of “white ﬂight.” Therefore, an attempt was made to build
a stable community where people of various races live by keeping an appropri-
ate balance of race and ethnicity. This policy was a success in one aspect: Many
families wanted to live in the development, and there was a waiting list (three to
four months for white households and two years for black households).
However, whether these policies (e.g. preferential admission of minorities)
are “righteous” is ﬁercely debated [104].

Chapter 5
Complex Adaptive
Systems
All life is problem solving. I have often said that from the amoeba to Einstein
there is only one step.
—Karl Popper
5.1
Diffusion-Limited Aggregation (DLA)
Diffusion-Limited Aggregation (DLA) is a growth model of crystals. Growth
processes of the following examples can be handled:
■
Confetti candy,
■
Metal Tree (tree-like metal crystal),
■
Bacterial Colony (see section 6.3),
■
Snowﬂake.
These crystals form a fractal, in which its part and the whole structure are similar.
This structure can be simulated statistically in DLA.
DLA treats the ﬂoating and diffusion of particles in a solvent as a random
walk. Those particles are adsorbed on the crystallized part. In addition, melting
of the crystal can happen in real phenomena, which is not considered here. In
more detail, the DLA behaves as follows:

138
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
There is a seed crystal in the center of the system.
■
Crystal particles from afar approach with a random walk while also
being a subject to Brownian motion.
■
When particles come into contact with a part already in crystal form,
it is adsorbed in that place with a certain probability.
■
One particle at a time is a subject to Brownian motion.
In DLA, the growth of the crystal is characterized by varying the adsorption
rate when particles are adjacent to the crystal. In other words, when the adsorp-
tion rate is high, the particles are more likely to be adsorbed to the outer crystals,
thus, crystals will grow thin and long. On the other hand, when the adsorption
rate is low, the probability of the particles entering the interior of the crystal
becomes high, thus, crystal grows thick.
5.2
How do Snowﬂakes Form?
Snowﬂakes have fascinated many people. Japanese scientist Nakaya Ukichiro
(1900–1962) conducted research on snow at Hokkaido University. He took mi-
croscope photographs of not only beautiful crystals but also variously shaped
snow and classiﬁed the crystals. He also studied the effect of the weather condi-
tions on the type of falling crystals. Then he created a low-temperature chamber,
in which he conducted his research and in 1936, for the ﬁrst time in the world,
he succeeded at creating artiﬁcial snowﬂakes. As a result of this experiment, it
was found that changing the temperature and amount of water vapor produces
different shaped crystals. Based on that, a famous relation chart called “Nakaya
diagram” was created. He mentions the famous words “snow is a letter sent from
heaven.”
Crystal, depending on its structures, has directions in which it grows more
easily, and other directions where the growth is scarce. Crystals are made of reg-
ularly arranged atoms and molecules. On top of that, the surface where atoms are
densely arranged, becomes a surface that is easier to grow. The water molecules
constitute snow crystals. Since water molecules have the property of polarity, as
is well known, oxygen molecules are not only covalently bonded to two hydro-
gen atoms but also loosely bonded to hydrogen atoms of other water molecules
(hydrogen bonds). As a result, one oxygen atom is bonded to four hydrogen
atoms around it so as to form a tetrahedron. Therefore, the bond angles between
all oxygens and hydrogens become 120◦. As a result, the oxygen molecules in
the solid water crystals are arranged like regular hexagons in a row. Thus, the
snow crystals tend to grow in hexagonal symmetry direction.

Complex Adaptive Systems
■
139
Adsorption rate= 0.05
Adsorption rate= 0.5
Adsorption rate= 0.99
Figure 5.1: Snowﬂake growth (1).
Adsorption rate (six directions)=1.0
Different adsorption rate
Figure 5.2: Snowﬂake growth (2).
First, the snow crystal was reproduced based on a simple DLA. Anisotropy,
as described below, was not considered in this case. Figure 5.1 shows the crystal
growth when the adsorption rate was set to be 0.05, 0.9 and 0.99. As can be
seen from the ﬁgure, when the adsorption rate is low, the crystal grows thick and
grows thin when the rate is high. The self-similarity was acquired, however, it
did not resemble the snowﬂake. As it can be seen, the hexagonal symmetry as
described above was not acquired.
Next, in order to consider the anisotropy, the adsorption rate of 1 was set
when extending to one of the six directions (up right, down right, up left, down
left, up, down) and for other directions, it was set to 0.5. The result is presented
in Fig. 5.2(a). It certainly extends in those six directions, but it is far from an
actual snowﬂake. In addition, Fig. 5.2(b) shows the case when the adsorption rate
was changed according to the number of branches. Although the self-similarity
was obtained by this, it did not become like snow crystals. One of the problems
is that the simulation was realized in a square lattice shape. For more accurate
simulation, it may be necessary to use a hexagonal lattice shape.
Let us simulate a snowﬂake that is actually closer to the real one. Here
its process will be explained based on the Mesoscopic lattice map. Gravner

140
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Honeycomb structure
(b) Oblique coordinate
(c) Cartesian coordinate
Figure 5.3: Snowﬂake coordinate.
et al. [40, 41] succeeded in generating a variety of snowﬂakes. The space for
snowﬂake formation is a honeycomb structure in which regular hexagons are
spread as shown in Fig. 5.3(a). The black hexagon is the constituent unit of the
structure (called a cell). Originally it should be displayed in oblique coordinate as
shown in the ﬁgure (b), but it is re-displayed correctly in the Cartesian coordinate
system (c).
The state at the time t of the cell x in the Mesoscopic lattice map is described
by the following four values:
■
at(x): 1 if the cell at x at time t is a part of the crystal, and 0 otherwise.
■
bt(x): boundary mass (parameter of a liquid), i.e., the liquiﬁed amount in
the cell at x at time t
■
ct(x): crystal mass (parameter of a solid), i.e., the solidiﬁed amount in the
cell at x at time t
■
dt(x): diffusive mass (parameter of a gas), i.e., the amount of water vapor
in the cell at x at time t
In a state with a seed of the crystal in the center, the initial values are as
follows:
Only one cell in the center
Other points
a0(x) = 1
a0(x) = 0
b0(x) = 0
b0(x) = 0
c0(x) = 1
c0(x) = 0
d0(x) = 0
d0(x) = ρ
Where ρ is the amount of water vapor. These four values are updated according
to the following four processes per unit time:

Complex Adaptive Systems
■
141
■
Diffusion: Update dt(x) of the cell that has not yet crystallized using
the neighboring cells’ diffusive masses.
■
Freezing: Update bt(x),ct(x) and dt(x) in the cell.
■
Attachment: The cell is crystallized according to bt(x),dt(x) and the
number of crystallized neighboring cells.
■
Melting: Update bt(x),ct(x) and dt(x) in the cell.
The details of each process are described below. For that purpose the follow-
ing symbols are deﬁned:
Nx
=
{x}∪{y | y are x’s neighboring cells}
At
=
{x | at(x) = 1} : A set of cells that are crystals at time t
∂At
=
{x /∈At | at(y) = 1,∃y ∈Nx} :
A set of non-crystal cells in contact with some crystal
Ac
t
=
{x | at(x) = 0} : At’s complementary set
nt(x)
=
#{y ∈Nx | at(y) = 1} : The number of crystals in cells adjacent to cell x
Note that | Nx |= 7 in a honeycomb structure (see Fig. 5.3).
■
Diffusion
For x ∈Ac
t , update dt(x) as follows:
dt(x) ⇐= 1
7
X
y∈Nx
dt(y).
(5.1)
For y ∈At, use dt(x) instead of dt(y).
■
Freezing
For x ∈∂At, update bt(x),ct(x) and dt(x) as follows:
bt(x)
⇐=
bt(x)+(1−κ)·dt(x),
(5.2)
ct(x)
⇐=
ct(x)+κ ·dt(x),
(5.3)
dt(x)
⇐=
dt(x).
(5.4)
■
Attachment
For x ∈∂At, update at(x) as follows:
■
If nt(x) = 1 or 2 and bt(x) ≥β then at(x) ⇐= 1
If nt(x) ≥3 and (bt(x) > 1 or (P
y∈Nx dt(y) < θ and bt(x) ≥α)), then
at(x) ⇐= 1

142
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
■
If nt(x) ≥4, then at(x) ⇐= 1
If at(x) is assigned to be 1, update bt(x),ct(x) with the following equations:
bt(x)
⇐=
bt(x),
(5.5)
ct(x)
⇐=
bt(x)+ct(x).
(5.6)
■
Melting
For x ∈∂At, update bt(x),ct(x) and dt(x) as follows:
bt(x)
⇐=
(1−µ)·bt(x),
(5.7)
ct(x)
⇐=
(1−γ)·ct(x),
(5.8)
dt(x)
⇐=
dt(x)+ µ ·bt(x)+γ ·ct(x).
(5.9)
Figure 5.4 shows an overview of the snowﬂake simulator according to the above
method. The brief explanation of the parameters used in this simulator is given in
Table 5.1.
Figures 5.5–5.7 gives the simulation results repeated up to 10,000 times using
the parameters in Table 5.2.
Figure 5.8 shows a simulator of the growth process of the snowﬂakes. The
growth of snowﬂakes can be observed by setting parameters ρ,β,α,κ,µ,γ and
σ in the bar. Figure 5.9 shows snowﬂakes generated with various parameters. The
simulation parameters in Fig. 5.9(e) are as follows from left to right:
■
ρ = 0.66,β = 1.6,α = 0.4,θ = 0.025,κ = 0.075,µ = 0.015,γ =
0.00005,σ = 0.000006
■
ρ = 0.65,β = 1.6,α = 0.2,θ = 0.0245,κ = 0.1,µ = 0.015,γ = 0.00005,
σ = 0.00001
■
ρ = 0.8,β = 2.6,α = 0.006,θ = 0.005,κ = 0.05,µ = 0.015,γ =
0.0001,σ = 0.00005
For more precise simulation, let us visualize how three-dimensional snow crys-
tals are generated. Here, the amount of water vapor is colored and displayed so that
the process of snowﬂake formation can be seen. By doing so, the water vapor is
visualized in the areas where the water vapor is scarce.
The three-dimensional visualization of the crystal structure was based on the
literature [41]. In this model, calculations are carried out on a lattice with triangular
prisms, in which the cell width is 1 in the horizontal direction and
√
3 in the height
direction, as shown in Fig. 5.10. In this simulation, the points in the height direction
are collectively represented as one cell.

Complex Adaptive Systems
■
143
Figure 5.4: Snowﬂake simulator (1).
Table 5.1: Parameters for snowﬂake growth.
ρ
vapor density
β
attachment threshold (anisotropy)
α
knife-edge instability (the closer to 0, the more the instability in
concavities between ridges)
θ
knife-edge instability (the closer to 0, the more delay in attach-
ment off the ridges)
κ
crystallization (freezing rate= 1−κ)
µ
melting rate (boundary mass back to vapor)
γ
melting rate (nonattached ice reverting to vapor)
σ
random ﬂuctuation (noise)
(a) t = 1000
(b) t = 2000
(c) t = 3000
(d) t = 4000
(e) t = 5000
(f) t = 6000
(g) t = 7000
(h) t = 8000
(i) t = 9000
(j) t = 10000
Figure 5.5: Snowﬂake growth (Simple star).

144
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) t = 1000
(b) t = 2000
(c) t = 3000
(d) t = 4000
(e) t = 5000
(f) t = 6000
(g) t = 7000
(h) t = 8000
(i) t = 9000
(j) t = 10000
Figure 5.6: Snowﬂake growth (Stellar crystal with plate ends).
(a) t = 1000
(b) t = 2000
(c) t = 3000
(d) t = 4000
(e) t = 5000
(f) t = 6000
(g) t = 7000
(h) t = 8000
(i) t = 9000
(j) t = 10000
Figure 5.7: Snowﬂake growth (Plate with dendrite ends).
Table 5.2: Parameters for snowﬂake growth.
ρ
β
α
θ
κ
µ
γ
σ
(a) Simple star
0.65
1.75
0.2
0.026
0.15
0.015
0.0001
0
(b) Stellar crystal with plate ends
0.36
1.09
0.01
0.0745
0.0001
0.14
0.00001
0
(c) Plate with dendrite ends
0.38
1.06
0.35
0.112
0.001
0.14
0.0006
0
The following three parameters were used in each cell in the crystal formation
of the snow:
■
a(z) : Indicates whether the z-th layer is frozen.
■
b(z) : The amount that is liquiﬁed/solidiﬁed in the z-th layer.
■
d(z) : The amount of water vapor in the z-th layer.

Complex Adaptive Systems
■
145
Figure 5.8: Snowﬂake simulator (2).
In order to reduce the amount of computation, structures with symmetry in the
vertical direction and only the single layer were considered, such as star-shaped
snow and snow plates. Therefore, the following restrictions were added:
■
If a certain point is frozen, then all the inner layers are frozen.
■
The crystal is formed symmetrically with respect to the central plane.
■
The layer of ice does not form two or more separate layers.
As described above, each point in the vertical direction is calculated as a single
cell.
By the ﬁrst and last assumptions, the points during freezing are limited to the
unfrozen bottom of each cell. Thus, the a value (a parameter indicating whether it
is frozen) and the b value (a parameter indicating the mass during freezing) can be
updated with a single top surface value of the crystal. In addition, it is normally
necessary to perform the calculations in both planes with respect to the simulation
plane. However, by the second assumption, the amount of calculation can be halved,
by obtaining the result for both sides at the same time. Therefore, the parameters of
each cell can be reduced as follows:
■
a: Number of consecutive frozen layers from the center plane
■
b: The amount that is liquiﬁed/solidiﬁed in the ±a-th layer
■
d(z): The amount of water vapor in the ±z-th layer

146
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
β = 1.9
β = 2.2
β = 2.4
β = 2.6
β = 2.7
β = 2.8
(a) ρ = 0.8,α = 0.004,θ = 0.001,κ = 0.05,µ = 0.015,γ = 0.0001,σ = 0
κ = 0.001
κ = 0.0025
κ = 0.005
κ = 0.0075
κ = 0.01
κ = 0.02
(b) ρ = 0.635,β = 1.6,α = 0.4,θ = 0.025,µ = 0.015,γ = 0.0005,σ = 0
µ = 0.04
µ = 0.05
µ = 0.06
µ = 0.07
µ = 0.08
µ = 0.09
(c) ρ = 0.5,β = 1.4,α = 0.1,θ = 0.005,κ = 0.001,γ = 0.001,σ = 0
ρ = 0.40
ρ = 0.41
ρ = 0.42
ρ = 0.44
ρ = 0.46
ρ = 0.50
(d) β = 1.3,α = 0.08,θ = 0.025,κ = 0.003,µ = 0.07,γ = 0.00005,σ = 0
(e) Different parameters with noise
Figure 5.9: Snowﬂake generation.
The update rule of these values is the same as in the literature [41], and the
iterative calculation is performed in one step, as follows:
■
Diffusion: The water vapor amount d(z) at each point of a cell is updated
to the weighted average of the neighboring cells’ values. The weights are
4 for the six surrounding cells and the same height point of itself, whereas
the weights are 3 for the cells above and below itself.
■
Condensation: For each place in contact with the ice, add (1 −κ) × d(a)
to the b as the solidiﬁed volume, and subtract the same value from d(a).

Complex Adaptive Systems
■
147
Figure 5.10: Three-dimensional lattice for snowﬂake growth.
■
Freeze Determination: Determines whether each point is frozen or not.
Let the number of points frozen at an adjacent point be nt. Then, the point
is determined to be frozen if the solidiﬁed volume b is equal to or larger
than a threshold value. For the threshold, β0 nt is chosen when a = 0 (i.e.,
when there is no frozen layer up and down), and β1 nt is chosen when a ̸= 0
(i.e., when there is a frozen layer up and down).
■
Evaporation: Assuming that some of the water, that was condensed and
solidiﬁed on the boundary, evaporates, µ ×b is added to d(a) and the same
value is subtracted from b.
Since it is not separated into two or more layers at each point, in the visualiza-
tion of the three-dimensional structure, the uppermost layer in each cell is expressed
as the color intensity. To clarify the factors of crystal formation, the amount of wa-
ter vapor at z = 0 was visualized. Since the amount of water vapor in this surface
can be described with d(0), it is visualized in four colors according to this value
(see Fig. 5.11). Furthermore, in order to make it easier to understand, we depict the
point where crystals are generated in a different color when the frozen amount b
at that point exceeds a certain constant value. Since the point where the freezing
occurs is only the boundary of the crystal, the point indicated by this point can
simultaneously represent the boundary of the crystal.
In the literature [41], water vapor is not supplied from the open air. Therefore,
as crystals are formed, the surrounding water vapor is exhausted and the crystal
formation is delayed. Moreover, there is a disadvantage that it is impossible to re-
ﬂect the inﬂuence of the change of the humidity of the open air. Therefore, in this
simulator, an environment not affected by the crystalline water vapor content was
introduced. This environment is called an environmental plane. In the implementa-
tion, calculations were made under the assumption that the amount of water vapor
on that surface did not change, so that the water vapor was constantly supplied from
that side. As a result, water vapor is periodically supplied to the crystal, and it is
expected that the generation of crystals can be accelerated. The point where the
distance from the center point is the same as the boundary was used with the en-

148
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 5.11: Crystal structure for snowﬂake in different colors.
vironmental plane. When converting the point (x,y) on the lattice to the real-world
point (X,Y), the following equation is satisﬁed:
X = x, Y = 2
√
3
y+ 1
√
3
x.
(5.10)
Therefore, when the distance from the center point to the environment plane is r,
the point satisfying the equation,
4(x2 +xy+y2) = 3r2,
(5.11)
is set to be the boundary with the environmental surface so that the amount of water
vapor at that point is always ρ. In order to avoid the inﬂuence of the environment,
the simulation is terminated when the crystal grows close to the vicinity of the
environment.
Figure 5.11 shows the simulation screen. The part indicated by white in the
center is a crystal. The area of light blue surrounding this crystal is being solidiﬁed.
In addition, the surrounding points indicate that a large amount of water vapor is
present in increasing order.
The crystal structures with different values of the initial amount of water vapor
ρ are shown in Fig. 5.12. The β value was taken from Case 2 in reference [41],
and set β10 = 1.6,β01 = β02 = 1.5,β11 = 1.4, and the others were set to 1.0. Other
parameters were ﬁxed with κ = 0.2 and µ = 0.008. The size of the simulation plane
was set to be 300 and 16 layers in height (i.e., 0 ≤z ≤15).
From the ﬁgure, it can be deduced that the smaller the value of ρ is, the smaller
the number of branches becomes. When the ρ value becomes larger, crystals with
more complex shapes are generated. In general, it is said that snow crystals tend
to have a structure with fewer branches when the amount of water vapor is small,
which is consistent with the simulation result.
As we can see the distribution of the amount of water vapor in the simula-
tion, the larger the value of ρ, the larger the amount of water vapor at the tips and

Complex Adaptive Systems
■
149
(a) ρ = 0.20
(b) ρ = 0.15
(c) ρ = 0.10
Figure 5.12: Simulation of snowﬂake growth.
the edges becomes, indicating that the development of the branches is being pro-
moted. On the other hand, when ρ is small, the amount of water vapor between
the branches is greatly reduced so that the amount of water vapor at these points
decreases and the formation of the branches is delayed. It can be seen that the shape
of each branch is determined by this distribution of the water vapor amount.
As described above, by displaying the three-dimensional structure and the
amount of surrounding water vapor in real time, it is possible to conﬁrm the genera-
tion factor of the crystal structure that was not clear in the conventional studies [41]
and observe the formation of the crystals.
5.3
Why do Fish Patterns Change?
5.3.1
Turing Model and Morphogenesis
“Morpho” means “shape”, and “genesis” means “generation.” Alan Turing believed
that the morphogenesis of organisms could be explained by the reaction and diffu-
sion of morphogens, which are hypothetical chemicals, and he proposed the model
described below. It is very interesting that von Neumann and Turing, pioneers in
computer science, were exploring models of life phenomena in the early days of
computers.
Two morphogens, X and Y, activate and inhibit, respectively. X and Y are gov-
erned by the following reaction and diffusion equations. x, y are the concentrations
of X and Y; f, g are the generation rates of X and Y; D is the diffusion coefﬁcient.
∂x
∂t
=
f(x,y)+Dx∇2x,
(5.12)
∂y
∂t
=
g(x,y)+Dy∇2y.
(5.13)
The ﬁrst term on the right-hand side is a generation term from chemical reaction,
and the second term represents movement by diffusion. These are called reaction-
diffusion equations.

150
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) various shells
(b) chambered nautilus (@PNG in 2005)
Figure 5.13: CA patterns found on shells.
Using this model, Turing hypothesized the following claim:
Stable patterns would form:
■
if the evolution of the inhibitor is slower than that of the activator
(i.e., ∂x
∂t > ∂y
∂t ),
■
and if the diffusion of the inhibitor is faster than the activator (i.e.,
Dyy > Dxx).
In fact, the formation of various patterns can be simulated by changing f(x,y) and
g(x,y). The following are simulations of the Turing model using cellular automata
(CA).
Kusch et al. deﬁned the CA rules for reaction and diffusion, as shown below,
and simulated the Turing model [65]. These are one-dimensional CA to reproduce
patterns such as those found on shells and animal fur. Figure 5.13 is a well-known
pattern on a shell. The objective here is to generate such patterns.
Each cell has two variables, u(t) and v(t), corresponding to the amount of ac-
tivator and inhibitor, respectively. u(t) may be 0 or 1, where 0 is the dormant state
(white) and 1 is the activated state (black). u(t) and v(t) transition to u(t + 1) and
v(t +1) through two intermediate steps, each according to the following rules.
(1) If v(t)>=1, then v1=[v(t)(1-d)-e],
else v1=0
(2) If u(t)=0, then u1 = 1 with possibility p,
and u1 = 0 with possibility 1-p.
else u1=1
(3) If u1=1, then v2=v1+w1,
else v2=v1

Complex Adaptive Systems
■
151
(4) If u1=0 and nu>{m0+m1*v2}, then u2=1,
else u2=u1
(5) v(t+1)={<v2>}
(6) If v(t+1)>=w2, then u(t+1)=0,
else u(t+1)=u2
Here, {} indicates the closest integer, < > is the average within distance rv, and
nu is the number of activated cells within distance ru. Statement (1) expresses the
decrease in inhibitors per time step. In particular, a linear decrease is observed with
d = 1 and e = 1, and an exponential decrease with 0 < d < 1 and e = 0. Dormant
cells are activated at a ﬁxed probability according to (2). Statement (3) shows that
activated cells emit inhibitors. Statement (4) states that a cell is activated if the
number of activated cells within a distance (nu) is larger than the linear function
(m0+m1×v2) of inhibitors (v2). This is the description of diffusion of activators.
Statement (5) means that the inhibitor becomes the average within the distance rv,
showing how inhibitors diffuse. Statement (6) states that a cell becomes inactive if
the inhibitor mass is larger than the constant value.
Figure 5.14 shows the results of experiments along with the parameters of d =
0.0,e = 1.0, and initial probability of InitProb= 0.0. Other parameters are shown
in Table 5.3. Note that branching and interruption, as seen in the ﬁgures, are difﬁcult
to reproduce in differential equation based models, but are easily reproduced in CA
models by using appropriate parameters. Results (e) and (h)∼(k) correspond to
Class IV described in section 1.3.
By simulating the Turing model in two dimensions, it is possible to create a
variety of repetition patterns by simply changing the parameters. It can be used to
realize temporal change of the pattern. For example, the pattern of a ﬁsh depends
on its age. Figure 5.15 shows the striped pattern of the young and mature Emperor
Angelﬁsh. There is also a theory that the patterns, that differ greatly between adults
and children, evolved to prevent children from being attacked by their parents be-
cause they are regarded as rivals. Animals grow in size by growth, but, assuming
that the skin area spreads evenly, the pattern should also grow as it is. On the other
hand, if the pattern is a “wave” by the Turing model, pattern reconstruction will
occur to keep the interval. Kondo et al. have demonstrated that the branching of the
stripes of the pattern on the Emperor Angelﬁsh is the same as the change predicted
from Turing’s equation [64]. That is, assuming that the pattern is a wave, charac-
teristic points like branches are unstable, so peculiar changes occur. As a result,
it was also found out that the number of vertical stripes increases with the num-
ber of branches. Furthermore, predictions that the striped pattern moves along the
body surface, and the appearance and disappearance of the metastasis in a regular
pattern, were proven.

152
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a)
(b)
(c)
(d)
(e)
(f)
 
 
 
(g)
(h)
(i)
(j)
(k)
Figure 5.14: Turing model simulation results (see Table 5.3 for the above parameters).
Table 5.3: Parameters for Turing model simulation.
(a)
ru=1,rv=17,w1=1,w2=1,m0=m1=0,p=0.002
(b)
ru=16,rv=0,w1=8,w2=21,m0=0,m1=1,p=0.002
(c)
ru=2,rv=0,w1=10,w2=48,m0=m1=0,p=0.002
(d)
ru=1,rv=16,w1=8,w2=6,m0=m1=0,p=0.002
(e)
ru=1,rv=17,w1=1,w2=1,m0=m1=0,p=0.002
(f)
ru=3,rv=8,w1=2,w2=11,m0=0,m1=0.3,p=0.001
(g)
ru=1,rv=23,w1=4,w2=61,m0=m1=0,p=0,d=0.05,e=0,initProb=0.1
(h)
ru=3,rv=8,w1=2,w2=11,m0=0,m1=0.3,p=0.001
(i)
ru=3,rv=0,w1=5,w2=12,m0=0,m1=0.22,p=0.004,d=0.19,e=0.0
(j)
ru=2,rv=0,w1=6,w2=35,m0=0,m1=0.05,p=0.002,d=0.1,e=0.0
(k)
ru=1,rv=2,w1=5,w2=10,m0=0,m1=0.3,p=0.002

Complex Adaptive Systems
■
153
Figure 5.15: Pattern changes in ﬁshes.
5.4
BZ Reaction and its Oscillation
In addition to the living body, it is known that waves generated by chemical reac-
tions make rhythmic patterns. The representative is the Belousov-Zhabotinsky reac-
tion (hereinafter referred to as the BZ reaction). This is the reaction that Belousov,
a biologist of the former Soviet Union, discovered in the tricarboxylic acid cycle
(one of the important reaction systems involved in biological metabolism). The
characteristic of this reaction is that the color of the solution changes alternately
as a result of the oxidation and reduction repeatedly taking place, thus, changing
the concentration of certain substances periodically and spatially. Recently, the BZ
reaction has been studied in various ways. An interesting research example is a phe-
nomenon in which a sudden reemergence of vibration occurs after several hours up
to 20 hours, even after the vibration has lasted for about 10 hours in a certain con-
centration region [86].
Let us simulate the BZ reaction using CA. This model can be expressed as
follows [28]. Each cell has n + 1 states of 0,1,··· ,n, and these are classiﬁed into
one of the three classes, healthy (in case of 0), infected (1,··· ,n−1), and diseased
(n). The state update rule is as follows:
nt+1 =



at
k1 + bt
k2
Healthy : When nt = 0,
st
at+bt+1 +g
Infected : When 1 < nt < n−1,
0
Diseased : When n ≤nt.
(5.14)
Here, the state value at time t, denoted as nt. at, is the number of infected adjacent
cells, bt is the number of diseased adjacent cells, st is the sum of adjacent cells and

154
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
＝ 1 ＋ 1 ＋ 1 ＋ 2 ＋ 3 ＝ 8
＝ 3
＝ 1
1
2
3
1
0
0
0
0
1
Figure 5.16: CA model for BZ reaction.
(a) g = 10
(b) g = 5
(c) g = 2.5
Figure 5.17: BZ reaction.
its own state value nt. Figure 5.16 shows an example of calculation (for n = 3) for
8 neighborhoods (Moore neighborhood). Additionally, k1 and k2 are constants that
determine the progress of the BZ reaction.
The experimental results of the BZ reaction are shown in Fig. 5.17. Here, the
initial states of the simulation are set to k1 = 3,k2 = 5,n = 1000 and g is set to be
10,5 or 2.5. The greater the g value, the less sick cells (red), and the more infected
(gray, the higher state value, the darker) cells are observed. Note that white cells
are in healthy state. Furthermore, big g is more likely to result in a steady pattern.
Let us take a look at the time variation of the BZ reaction. Here, parameters are
set to be k1 = 2,k2 = 3 and n = 200. The g value is set to be 30 or 70. Two types of
neighborhoods are considered: Moore neighborhood and chess knight’s movement
(i.e., two cells up, one to the right, etc.). Figures 5.18∼5.21 show the snapshots of
the simulation for each parameter and the transition of the number of sick/healthy
cells. The color of the cell is green if it is healthy, red if it is sick, and blue if
it is infected (the higher state value, the brighter). Setting g = 30 in the Moore
neighborhood, the two states in the snapshot in Fig. 5.18 are alternately repeated
and vibrate. If g = 70 it will gradually settle down from the left state, shown in
Fig. 5.19, to the right state. At this time, the vortex pattern occurs and spreads over
the whole as the time passes. Looking at the plot of the number of cells, there are

Complex Adaptive Systems
■
155
Figure 5.18: BZ reaction (Moore neighborhood), g = 30).
also large waves in addition to ﬁne vibrations, suggesting that this CA has multiple
time constants. By using the neighborhood of the knight, when experimenting with
g = 30, the same vibration phenomenon, similar to the previous one seen in the
Moore neighborhood, was observed (Fig. 5.20). On the other hand, if g = 70, the
same pattern continued without converging (Fig. 5.21).
Additionally, biochemical reactions behave differently when dust or dirt enters
the reaction vessel. This process was simulated, as shown in Fig. 5.22. The situation
when there is no obstacle is presented in (a). The parameters are set to be k1 =
1,k2 = 1,g = 10 and n = 50. Figure 5.22(b) shows a case where a point obstacle is
placed in the upper left. Inserting this single point increased the number of vortex
centers by two. In (c) the shape of the reaction vessel is changed, and holes are
opened in the middle. At this time, a new vortex is not created, but the diffraction
phenomenon can be observed along the boundary of the obstacle.
5.5
Why do We have Mottled Snakes? Theory of Mur-
ray
As an interesting application, let us introduce Murray’s theory of mammalian coat
(fur) pattern [78, 79].
In this theory, the fur pattern of animals seen in nature can be recreated,
by changing two numerical parameters. Whether the pattern becomes spotted or
striped depends on the timing of the chemical reactions in the skin. Therefore, an-
Color version at the end of the book

156
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 5.19: BZ reaction (Moore neighborhood), g = 70).
Figure 5.20: BZ reaction (Chess knight’s neighborhood), g = 30).
imal fur pattern differences can be thought of as a result of purely mathematical
rules [114, 27].
If the skin area is very small or large, no pattern will appear. On the other hand,
if it is not too wide and too narrow, and the shape is long or thin, stripes appear
orthogonal to the major axis of that shape. An example is a zebra. In this animal,

Complex Adaptive Systems
■
157
Figure 5.21: BZ reaction (Chess knight’s neighborhood), g = 70).
(a) no obstacle
(b) one-point obstacle
(c) wall obstacle
Figure 5.22: BZ reaction (with/without an obstacle).
the fetus has a long pencil-like shape for four weeks in the early pregnancy. Since
the chemical reaction occurs at this time, it can be thought to be striped pattern.
Spots will appear when the whole shape is close to a square. An example of this is
a leopard. In this animal, a spot will be formed as a chemical reaction occurs when
the fetus is round. However, the tail is different. During the growth of the fetus, the
tail ends up with a striped pattern because it has a long pencil-like shape.
From Murray’s theory, the following theorem can be derived as a prediction:
Theorem 5.1
Snake’s theorem
Snakes are always striped (rings) and have no spotted patterns.(see Fig. 5.23).

158
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 5.23: Snake’s theorem.
Theorem 5.2
Body and tail theorem
There are no spotted tail animals with the striped torso, On the other hand, there
is a striped animal with a spotted torso (see Fig. 5.24).
Ironically, the above emergent behavior cannot be explained in evolutionary the-
ory. On the other hand, according to Murray’s theory, this can be proved using the
following facts:
■
Many animal embryos are composed of a rounded torso and a thin tail.
■
There is no fetus with a round tail and a thin torso.
The smaller the diameter of the tail is, the less likely it is that the striped pattern will
become unstable and its instability will be higher for bodies with bigger diameters.
So far, theoretical studies on emergent phenomena of morphogenesis, such as
the Turing model and Murray’s law, have been explained. One of the most common
criticisms about such research is that “It just happened that accidentally chosen
parameter recreated the same behavior as real animals.” That is, in many studies,
one of the sufﬁcient conditions of the pattern formation (the possibility of causing
a phenomenon) is deﬁned. On the other hand, what is more important in biology
are requirements (this mechanism must work).
These two-way (bidirectional) studies are expected in the future, and, recently,
some interesting results have been obtained. For example, cone cells (photoreceptor
cells), that correspond to different frequencies of light in the ﬁsh retina, form a
regular mosaic pattern. On zebraﬁsh retina, there are four patterns of cone cells with

Complex Adaptive Systems
■
159
Figure 5.24: Body and tail theorem.
peaks of sensitivity for the light of blue, red, green, and ultraviolet wavelengths.
Mochizuki et al. assumed that the cells would be rearranged based on adhesion, and
proposed a CA model to consider the placement of cells in the lattice space [77].
Then, the requirements of intercellular adhesion were mathematically derived for
the formation of zebraﬁsh sequences. Furthermore, interactions between cells were
predicted, and validation in actual organisms was carried out.
It is also conﬁrmed that there is a chemical reaction system according to the
Turing model in reality. This was triggered by the success of the chemical pro-
duction of reaction-diffusion waves. As an example, the waves moving on the skin
of the mouse were observed. This suggests that the Turing wave works not only
with ﬁsh but also widely as a fundamental principle of skin pattern formation of
vertebrates [64].


Chapter 6
Emergence of Intelligence
As one’s days dwindle, life begins to shift from an iterative prisoner’s dilemma,
in which defection can be punished and cooperation rewarded, to a one-shot pris-
oner’s dilemma, If you can convince your children that your soul will live on and
watch over their affairs, they are less emboldened to defect while you are alive.
—Steven Pinker
6.1
Evolution of Cooperation and Defection
6.1.1
How to Clean a Fish
There is a ﬁsh called Cleaner Wrasse (Labroides dimidiatus). In Japan, it is usually
seen in the south sea, and it is a long slender ﬁsh of about 10cm in length. This ﬁsh
is known for cleaning other ﬁsh of parasites. When diving in the southern sea, it
can be easily observed how these ﬁsh enter into the mouth and gills of a large ﬁsh,
such as the Japanese parrotﬁsh (Fig. 6.1). At this time, it is impressive that a large
ﬁsh seems to be comfortable opening the mouth wide and letting small Wrasse
freely enter and exit. If a large ﬁsh closes its mouth, it can easily get food (Wrasse),
but such a betrayal never happens. This is an example of co-evolution (symbiosis)
between different species. The Parrotﬁsh is cleaned of parasites and leftovers, and
Cleaner Wrasse obtains precious food.
However, the story is not so simple. It seems that Wrasse is not always an obe-
dient cleaner. Sometimes it tears off and eats some of the gills during cleaning.
In other words, it pinches a little food while cleaning. Certainly, sometimes it can
be observed that the ﬁsh being cleaned suddenly moves the body as if it trying to
scare the Wrasse away. At this time, the Parrotﬁsh punishes the Cleaner Wrasse by
chasing it aggressively or swimming away and the Wrasse runs away in a hurry.
Normally, however, the Wrasse approaches and begins cleaning again. Punishment
seems to lead to a tendency to reduce the betrayal (biting the Parrotﬁsh) of the

162
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.1: A big cod ﬁsh cleaned by a cleaner ﬁsh (@Exmouth, Western Australia in 2014).
Cleaner Wrasse. In fact, based on the ﬁeld data, the ﬁsh requesting to be cleaned
punishes by chasing or swimming away from the ﬁsh cleaning their body, not the
parasite. Experimental results showed, that the Wrasse’s tendency to eat parasites
increased when it was punished, while the tendency to bite the Parrotﬁsh is de-
creased [23]. From a recent research [94], it was found out that the Cleaner Wrasse
is more prone to betray when there are a few ﬁsh around. In other words, if there
are eyewitnesses, they act honestly, and when they are not being watched over, they
act maliciously. This is a somewhat clever strategy that also considers the effect of
attracting customers in the future.
Additionally, to make things more complicated, there are also professional
thieves that imitate the Cleaner Wrasse. This false cleanerﬁsh is a different kind
of the Blenniidae family. However, the size, mottle, and color are exactly the same
as the Cleaner Wrasse. It is said that the position of the mouth is slightly lower
than that of the Blenniidae, but it is not possible to distinguish unless very carefully
observed. This false cleanerﬁsh has a bad character and pretends to clean the big
ﬁsh, and after it is allowed to clean the big ﬁsh, it runs away after eating off the ﬁn
and the skin.
Such ﬁsh strategies can be called the betrayal and cooperation. How did that
strategy evolve? The appearance of the false cleanerﬁsh that mimics the Cleaner
Wrasse occured because there were big ﬁsh deceived by it. In other words, the strat-
egy of the false cleanerﬁsh is effective because a ﬁsh (Wrasse), that does the clean-
ing honestly, exists. If a lot of Wrasses abandon honesty and become “thieves,”
both the false cleanerﬁsh and the Wrasse will be driven away by big ﬁsh. On the
other hand, from the viewpoint of the big ﬁsh, although it is sometimes “robbed,”
it should be better to accept the “sweepers,” which work honestly in most cases
even if they have to make a little sacriﬁce. This is because the false cleanerﬁsh and
the Wrasse cannot be distinguished from the outside. However, if the number of
Wrasse is reduced, and only the false cleanerﬁsh stay, the story will be different.
The symbiotic relationship between Wrasse, false cleanerﬁsh and the big ﬁsh to be
cleaned is founded on a delicate balance. Wrasse’s betrayal (“pinching food”) may
be a remnant of the evolutionary process or a new parasitic relationship.

Emergence of Intelligence
■
163
The relationship between betrayal and cooperation, as described above, is mod-
eled in the framework of a game theory called the Prisoner’s Dilemma.
6.1.2
The Prisoner’s Dilemma
Two suspects (A and B) have been arrested by the police for a certain crime (Fig.
6.2). Although the suspects are accomplices and their guilt is strongly suspected,
insufﬁcient evidence exists to prove their guilt beyond doubt, and, therefore, the
police separate the suspects and wait for a confession. A and B can adopt one of
two strategies, namely, to confess (i.e., to defect, denoted by D below) or refuse to
confess (i.e., to cooperate, denoted by C below), as shown in Table 6.1. The case
where A does not confess is labeled A1, and the case where A confesses is labeled
A2. Similarly for B, B1 denotes a refusal to confess and B2 denotes a confession. If
neither suspect confesses, both of them would receive two-year sentences; however,
if only one of them confesses, the one confessing would be sentenced to only one
year due to extenuating circumstances, while the other suspect would receive a
ﬁve-year sentence. Finally, if both suspects confess, both of them would receive
three-year sentences.
Therefore, although the sentence is only two years for both suspects if neither
of them confesses, the loss for one suspect is considerably greater if a confession is
obtained from only their accomplice. The dilemma here is whether A and B should
confess. In terms of pure strategy, it is straightforward to show that the pair (A2,
B2) is an equilibrium point, and that no other equilibrium points exist. Here, the
equilibrium point indicates that if either suspect adopts confession as their strategy,
their accomplice is at a disadvantage if they do not adopt the same strategy1. Con-
versely, if the suspects adopt the strategic pair (A1, B1), neither suspect confesses
in the belief that their accomplice will not confess either, and the sentence for both
of them is only two years. This sentence is obviously a more favorable outcome
than the equilibrium point for both suspects, but since the suspects are separated
from each other when taken into custody, they are unable to cooperate. This type
of game is known as the prisoner’s dilemma.
Situations such as those arising in the prisoner’s dilemma are also often en-
countered in biology as well as in human society. For example, if an appropriate
incentive is present, the following situations can be modeled similarly to the pris-
oner’s dilemma.
■
Social interaction between animals in nature:
■
Social grooming in primates.
■
Cleaner ﬁsh and the ﬁsh that they clean.
■
Parasites and hosts.
1Detailed deﬁnition and discussion are given later in page 164.

164
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
・Not confession　　　
Cooperate
・Confession　　　　
Defect
14
13
Figure 6.2: The prisoner’s dilemma.
Table 6.1: Beneﬁt and cost in the prisoner’s dilemma.
(a) Beneﬁt and cost from the viewpoint of A
B1
B2
B: not confess (C)
B: confess (D)
A1
−2
−5
A: not confess (C)
Two years of imprisonment
Five years of imprisonment
A2
−1
−3
A: confess (D)
One year of imprisonment
Three years of imprisonment
(b) Beneﬁt and cost from the viewpoint of B
B1
B2
B: not confess (C)
B: confess (D)
A1
−2
−1
A: not confess (C)
Two years of imprisonment
One year of imprisonment
A2
−5
−3
A: confess (D)
Five years of imprisonment
Three years of imprisonment
■
Social interaction between humans:
■
Interaction between countries.
■
Interaction between tribes.
Payoff is considered using a game theory concept of “equilibrium point.” The
equilibrium point is deﬁned as follows:
Deﬁnition 6.1
Equilibrium point
Equilibrium point is a strategy characterized by the notion that if an opponent
chooses a strategy, one must also choose the same strategy to avoid a loss.

Emergence of Intelligence
■
165
More precisely, consider the following payoff table related to two players:
 X
Y
X
a
b
Y
c
d

(6.1)
In other words,
■
In the match X ×X, both gain a.
■
In the match Y ×Y, both gain d.
■
In the match X ×Y, X gains b and Y gains c.
In this case, we have:
■
If a > c, then strategy X is a strict Nash equilibrium point.
■
If a ≥c, then strategy X is a Nash equilibrium point.
■
If d > b, then strategy Y is a strict Nash equilibrium point.
■
If d ≥b, then strategy Y is a Nash equilibrium point.
This idea is attributed to John Nash2. Using Brower’s ﬁxed-point theorem, he
proved that a non-collaborative game with at least three players will always have
an equilibrium point.
In the prisoner’s dilemma game, it is easy to verify that a pure combination of
strategies (A2, B2) represents an equilibrium point. Moreover, no other equilibrium
point exists. However, if one does not confess, believing that the other will not
do it either, characterizing an (A1, B1) combination, both will be sentenced to
only 2 years. This result is better than the equilibrium points for both; however,
since each prisoner is in solitary conﬁnement and cannot cooperate, the players
fall into a dilemma. Point (A1, B1) is “Pareto efﬁcient,” where a strategy change
would bring an advantage to a player, but he refrains from doing it if that results
in a disadvantage to the other one; this is a demonstration of care. In other words,
this is a combination of strategies that respects the common part of both players’
preferences. In the prisoner’s dilemma, a dilemma is characterized by the fact that
the Nash equilibrium point is not Pareto efﬁcient.
2John Nash (1928–2015): American economist and mathematician. In 1994, he received the Nobel
Prize in economics. The American movie “Beautiful Mind (2001)” is a mystery story that depicted a part
of his life and won many Academy Awards. An interesting quote from the movie is that what Adam Smith
wanted to say can be mathematically proved now.

166
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
6.1.3
Iterated Prisoner’s Dilemma
This section considers an extended version of the prisoner’s dilemma, known as the
“iterated prisoner’s dilemma” (IPD), in which the prisoner’s dilemma is repeated
a number of times with the same participants, and a ﬁnal score is obtained as the
sum of the scores in all iterations. Hereinafter, the choice made at each step is
referred to as a move, or, more precisely, a move indicates either cooperation (C:
no confession) or defection (D: confession).
As with the prisoner’s dilemma, situations similar to that in IPD can regularly
be observed in nature and in human society. A well-known example is regurgita-
tion in vampire bats [24]. These small bats, living in Central and South America,
feed on mammalian blood at night. However, their bloodsucking endeavors are not
always successful, and at times they face starvation. Therefore, vampire bats that
feed successfully will regurgitate part of their food and share it with other vampire
bats that were unable to ﬁnd food. Bats who receive food in this manner later re-
turn the favor. Wilkinson et al. observed 110 instances of regurgitation in vampire
bats, of which 77 cases were from a mother to her children, and other genetic rela-
tions were typically involved in the remaining cases. Nevertheless, in several cases,
regurgitation was also witnessed between companions sharing the same den, with
no genetic afﬁnity. Modeling this behavior on the basis of the prisoner’s dilemma,
where cooperation is deﬁned as regurgitation and defection is deﬁned as the lack
thereof, yields results similar to those in Table 6.2. A correlation between weight
loss and the possibility of death due to starvation has also been found in bats. There-
fore, the same amount of blood is of completely different value to a well-fed bat
immediately after feeding and to a bat that has almost starved to death. In addition,
it appears that individual bats can identify each other to a certain extent, and, as a
result, they can conceivably determine how a certain companion has behaved in the
past. Thus, it can be said that bats will donate blood to “old friends.”
To set up the IPD, one player is denoted as P1 and the other party is denoted as
P2. Table 6.1 is then used to create Table 6.3, describing the beneﬁt for P1. In this
table, larger values indicate a higher beneﬁt (which, in the prisoner’s dilemma, is
in the form of shorter imprisonment terms for the suspects).
Here, the following two inequalities deﬁne the conditions required for the emer-
gence of the dilemma:
T > R > P > S,
(6.2)
R > T +S
2
.
(6.3)
The ﬁrst inequality is self-explanatory, while the implications of the second one
will be explained later.
As a possible strategy in IPD (iterated prisoner’s dilemma: Iterated PD), for
example, the following ones can be thought of:

Emergence of Intelligence
■
167
Table 6.2: The strategy of vampire bats [24].
Companion cooperates.
Companion defects.
I cooperate.
Reward: high
Compensation: zero
On the night of an unsuccessful
hunt, I receive blood from you and
avoided death by starvation. On
the night of a successful hunt, I
donate blood to you, which is an
insigniﬁcant expenditure to me at
that time.
Although on a successful night I do-
nate blood to you and save your life,
you do not donate blood to me when
my hunt is unsuccessful and I face
death by starvation.
I defect.
Temptation: high
Penalty: severe
You save my life when my hunt is
unsuccessful. However, on a suc-
cessful night, I do not donate blood
to you.
I do not donate blood to you even
when my hunt is successful, and
death through starvation is a real
threat to me on the night of an
unsuccessful hunt.
Table 6.3: Codes and beneﬁt chart in IPD.
The other party (P2) does not
confess.
The other party (P2) confesses.
P2 = C
P2 = D
I (P1) do not confess.
Beneﬁt: 3
Beneﬁt: 0
P1 = C
Code: R
Code: S
I (P1) confess.
Beneﬁt: 5
Beneﬁt: 1
P1 = D
Code: T
Code: P
1. All-C: Always cooperate regardless of the opponent’s hand (do not confess).
2. All-D: Always betray regardless of the opponent’s hand (confess).
3. Repeat cooperation and betrayal.
4. RANDOM: Decide a hand for each round at random.
5. GRIM: C for the ﬁrst time, C until the opponent betrays, and forever D after
betrayal, and never forgives.
6. GRIM*: It is the same as GRIM, but at the end it must be D (“parting shot”).
7. WSLS: Win and Lose (Win-Stay-Lose-Shift) strategy, also called Pavlov
strategy. See Table 6.5.

168
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
The cooperative solution, which does not appear in a single game, evolves in
IPD. Cooperation becomes inevitable since defection entails subsequent revenge.
An example of a well-known cooperation strategy is “tit for tat” (TFT):
TFT (tit for tat)
1. The ﬁrst move is performed at random.
2. All subsequent moves mirror P2’s move in the previous iteration.
In other words, this strategy follows the maxim “Do unto others as they have done
unto you (an eye for an eye)”. Thus, P1 confesses if P2 has confessed in their previ-
ous move, and vice versa. This strategy is recognized as being strong in comparison
with other strategies (i.e., the total score is usually high). Variants of TFT include
the following.
1. Tolerant TFT: Cooperate until P2 defects in 33% of their moves.
2. TF2T: Defect only if P2 has defected twice in a row.
3. Anti-TFT: The opposite strategy to TFT.
The beneﬁt at each move and the total score are compared for All-C, All-D, TFT,
and Anti-TFT strategies when pitted against each other in Table 6.4. The following
can be inferred from the results presented in the table:
1. TFT is equivalent or superior to All-C.
2. TFT is generally superior to Anti-TFT. However, Anti-TFT is superior ifP2
adopts the All-C strategy.
3. All-C yields a lower score against Anti-TFT and All-D; however, it performs
well against a TFT strategy.
4. All-D can outperform TFT.
In 1979, Axelrod3 extended an invitation to a number of game theorists and psy-
chologists to submit their strategies for IPD. Subsequently, a tournament was held
using the 13 submitted strategies in addition to the RANDOM strategy. In this tour-
nament, each strategy was set against the others in turn, including itself, in a round-
robin pattern. To avoid stochastic effects, ﬁve matches were held for each strategic
combination. Each match consisted of 200 iterations of the prisoner’s dilemma in
3Robert Axelrod(1943–): American political scientist.

Emergence of Intelligence
■
169
Table 6.4: Comparison of the performance of the four strategies when pitted against each
other.
Strategy of P2 (score / total score)
Strategy of P1
All-C
TFT
anti-TFT
All-D
All-C
3333/12
3333/12
0000/0
0000/0
TFT
3333/12
3333/12
0153/9
0111/3
anti-TFT
5555/20
5103/9
1313/8
1000/1
All-D
5555/20
5111/8
1555/16
1111/4
order to ensure a sufﬁciently long interaction, as described below. Rather than con-
sidering the outcome of individual matches, the ultimate ranking was determined
on the basis of the total score.
The winner of the contest was the TFT strategy proposed by Anatol Rapoport4,
a psychologist (and philosopher) at the University of Toronto. This program was
the smallest of those submitted, consisting of only four lines of BASIC code. Upon
analyzing the results, Axelrod derived the following two morals:
1. One should not deliberately defect from the other party (politeness).
2. Even if the other party defects, one should respond with defection only once
and should not hold a grudge (tolerance).
Any strategy abiding by these morals is referred to as being “nice”. Axelrod con-
sidered that a superior strategy could be derived based on these two morals. He held
a second tournament, this time running a wide advertising campaign asking for en-
tries through general computer magazines and publicizing the above two morals
as a reference for the participants. Although this call for entries attracted a broad
spectrum of participants from six countries, the TFT strategy proposed by Rapoport
again emerged victorious, and Axelrod stated various other morals upon analyzing
the results from both contests [7].
Looking at the scores obtained in the tournaments, nice strategies similar to
TFT dominated the top half of the list. In contrast, the majority of the strategies in
the bottom half of the list involved deliberate defection, while the lowest score was
obtained by the RANDOM strategy. The question naturally arises as to why this set
of circumstance occurs.
Let us consider the strength of the TFT strategy. First, we focus on the case
where two nice strategies are competing. Since both strategies denounce deliberate
defection, each of the 200 moves is one of cooperation. As a result, the two par-
ties receive a total of 3×200 = 600 points. This situation is different when a nice
strategy is competing against one that promotes defection. For example, consider
4Anatol Rapoport (1911–): Russian-born Jewish American mathematical psychologist. His vast con-
tribution includes experiments on game theory and related research on social psychology, as well as peace
studies.

170
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
competition between JOSS and TFT strategies. JOSS behaves almost identically to
TFT; however, it defects on random occasions, corresponding to a player with a
nice strategy occasionally being tempted to cheat. Thus, JOSS occasionally obtains
ﬁve points by outperforming TFT (which supports cooperation and denounces de-
fection), and although they are almost identical, JOSS appears to provide a greater
beneﬁt.
In the competition, TFT and JOSS cooperated for the ﬁrst ﬁve moves.
TFT:
CCCCC
JOSS:
CCCCC
At this point, JOSS switched its tactics and defected (cheated).
TFT:
CCCCCC
JOSS:
CCCCCD
^
Although at the next move JOSS returned to its nice tactics, TFT took revenge by
defecting.
TFT:
CCCCCCD
JOSS:
CCCCCDC
^
From here on, the moves of the two strategies alternated between defection and
cooperation,
TFT:
CCCCCCDCDCD
JOSS:
CCCCCDCDCDC
^^^^
and the respective scores at that juncture became
JOSS: 5 0 5 0 c
TFT:
0 5 0 5 c
giving an average of 0+5
2 = 2.5, which is less than the 3 points obtained if the two
parties had continually cooperated (cf. the assumption in eq. (6.3)).
Next, JOSS cheated again and defected on the 25th move,
TFT:
CCCCCCDCDCD...D
JOSS:
CCCCCDCDCDC...D
^

Emergence of Intelligence
■
171
resulting in reciprocal defection until the end of the match.
TFT:
CCCCCCDCDCD...DDDDDDD
JOSS:
CCCCCDCDCDC...DDDDDDD
^^^^^^
This chain of enmity yielded an average score of 1, and the match ended unchanged
since neither side was able to forgive the other’s defection and restore cooperation.
At the conclusion of the match, the total score was 241 to JOSS and 236 to
TFT, much lower than the 600 points that they could have obtained if they had
cooperated throughout.
Hence, TFT has the following characteristics:
1. TFT never attacks the other party. The score obtained by TFT is the same or
lower than that of the other party.
2. TFT does not hold a grudge. TFT seeks revenge only once, rather than pun-
ishing the other party repeatedly. In this way, TFT does not disregard the
chance for reconciliation with the other party.
3. TFT does not require a victim, in other words, it does not exploit the other
party. As a result, two players adopting the TFT strategy can enjoy a mutu-
ally beneﬁcial relationship.
Conversely, although the All-D strategy can obtain the highest score of 5 points
when competing against the All-C (victim) approach, the performance of using
All-D is lower when competing against a different strategy.
An important assumption in IPD is that the players do not know when the com-
petition ends, otherwise it would allow for tactics based on “the ultimate defense
measure” (always defecting on the last move). In addition, cooperative evolution
requires a long competition since a relationship of trust, which is a prerequisite for
cooperation, cannot be established in the short term. This idea can be explained us-
ing examples such as a student canteen (which adopts an honest business policy of
expecting repeat clients in order to establish a long-standing relationship with stu-
dents) and seaside noodle shops (which survive due to ﬁrst-time customers, even
with expensive and unappetizing food).
Some disadvantages of TFT include
■
In response to noise (when unintentional actions are made by mistake),
TFT’s moves begin to alternate between cooperation and defection.
■
TFT cannot exploit the All-C strategy (since it usually cooperates).
To address these issues in TFT tactics, the so-called Pavlov strategy (or WSLS,
Win-Stay-Lose-Shift strategy) has been proposed, whereby P1’s current move is
determined not only by the previous move of P2, but also by P1’s preceding move

172
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 6.5: Comparison between Pavlov and TFT strategies.
preceding move
P1
P2
Pavlov strategy
TFT strategy
C
C
C
C
C
D
D
D
D
C
D
C
D
D
C
D
(Table 6.5). For example, if an incorrect choice has been made (e.g., if P1 cooper-
ates or defects, expecting P2 to cooperate, and instead the P2 defects), the previous
choice is reversed in the current move. On the contrary, if P2 cooperated while P1
defected in the previous move, changing this choice in the current move is not nec-
essary. The Pavlov strategy is considered to be robust with respect to noise. For
example, in a competition where both players have adopted the Pavlov strategy,
even if the cooperative relationship is broken due to noise and results in moves
(C,D), a cooperative relationship is returned to via moves (D,D) followed by (C,C).
Nevertheless, this does not imply that the Pavlov strategy is superior to the TFT
strategy. For instance, although the TFT strategy consistently defects against the
All-D strategy, the Pavlov strategy alternates between cooperation and defection,
and its average score is lower than that for TFT.
IPD has been extended in various ways and has been actively used in research in
the ﬁelds of AI, AL and evolutionary economics [60]. Examples include studies in
which strategic sexual selection is implemented by introducing the concept of gen-
der, and studies where competition is extended to include three or more individuals,
resulting in the emergence of cooperation and conspiracy.
Axelrod added norms to the prisoner’s dilemma [8]. Here, defectors may be
socially sanctioned by reduction of their score. Defecting players had a possibility
to be detected by other players, and a strategy to decide whether to penalize a
defector that was found (whether to reduce the score of the defector) was added to
the strategy to play the prisoner’s dilemma.
In other words, when a betrayal action is found, social sanctions against it are
inﬂicted in the form of a penalty. When a player betrays, the possibility that other
players will witness it is incorporated. In addition to the strategy of playing the Pris-
oner’s Dilemma, a strategy to decide whether to punish the traitor when witnessed
(whether to deduct the traitor’s score or not) is integrated.
In each generation, each player performs a single game with all the other play-
ers. Every time the player cheats during the match, that action may be witnessed
by other players. Each time it is witnessed, the traitor is punished according to the
probability set as the strength of the witness’s vengefulness. The evolutionary pro-
cess is applied after all the matches are over. At this time, it is probable that the
child’s strategy mutates. Sometimes children can obtain boldness and the strength
of vengefulness disparate from their parents.

Emergence of Intelligence
■
173
If the social norms are not set at all, the group will eventually overﬂow with
traitors. Since cooperative relationships are not necessarily evolved only with
norms, additional meta-norms are also introduced. This is the norm to punish those
who pretend to be blind. In other words, when a player who punishes players who
pretend not to see traitors is introduced, the latter evolves to punish the traitors and
the player who was punished for betrayal was seen to tend to evolve to cooperate.
6.1.4
ESS: Evolutionarily Stable Strategy
Evolutionarily Stable Strategy (ESS) corresponds to a situation when a strategy
occupies a group and prevents other strategies from invading it. This idea was ad-
vocated in 1973 by John Maynard Smith5 and George Price6. When the strategies
A and B are given, and the gain for A is assumed to be E(A,B), then the conditions
of the ESS are as follows:
ESS: Evolutionarily Stable State
Strategy I is ESS if either one of the following two conditions is satisﬁed for
any strategy J (̸= I):
■
E(I,I) > E(J,I) or
■
E(I,I) = E(J,I) and E(I,J) > E(J,J).
If E(I,I) = E(J,I) and E(I,J) > E(J,J) holds true, this is known as “weak ESS.”
As mentioned above, no other strategies can invade a group with the ESS strat-
egy. Narrow Nash equilibrium has the same characteristics, but it does not hold for
Nash equilibrium.
All-C is obviously not an ESS. For example, if All-D invades a group of only
All-C, the All-C group collapses with All-D being the sole winner. Interestingly, it
is known that the TFT is not an ESS.
The following relationship between the solution of Nash equilibrium and ESS
strategy holds true:
Narrow Nash equilibrium =⇒ESS =⇒Weak ESS =⇒Nash equilibrium
5John Maynard Smith (1920-2004): British biologist. Received the Kyoto Prize in 2001. He is said to
be a biologist who had the most inﬂuence on biology in the twentieth century.
6George R. Price (1922-1975): American population geneticist. Turned from atheism to Christianity,
and gave all his wealth to the poor and becomes homeless as result. A book “The Price of Altruism”
depicting the sublime way of life leading up to suicide is worth reading [44].

174
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Let us consider the GRIM strategy next. This is C for the ﬁrst time, C until the
opponent betrays, and forever D after it is betrayed and will never forgive. This
strategy is regarded as a symbol of the reciprocal behavior of human beings [23].
One example of this is the diamond market in Antwerp (Belgium). Here, trans-
actions are performed in a relatively small group of experts. The trader is given
a diamond in the bag to check the item at home. If a person steals it, he will be
permanently expelled from the community. The GRIM strategy never tolerates a
betrayal, but otherwise places full trust.
The payoff matrix of the m time matchup between All-D and GRIM is as fol-
lows [85]:

GRIM
ALL-D
GRIM
mR
S+(m−1)·P
ALL-D
T +(m−1)·P
mP

.
(6.4)
Therefore, if mR > T + (m −1) · P then GRIM becomes a Nash equilibrium in a
narrow sense against All-D. All-D cannot invade when all the members of the group
are GRIM. However, All-D also becomes a Nash equilibrium point in a narrow
sense, since the mP > S+(m−1)·P.
Let us look at a matchup between GRIM* (same as GRIM, but is always D at
the end) and GRIM. The payoff matrix of the m time matchup is as follows:

GRIM
GRIM*
GRIM
mR
S+(m−1)·R
GRIM*
T +(m−1)·R
P+(m−1)·P

.
(6.5)
It is clear that GRIM* is more advantageous than GRIM, and GRIM* evolutionarily
invades the population made up of GRIM.
Now, once everyone becomes GRIM*, they will betray each other even one
round before the last round. If betrayal happens one round before the ﬁnal round,
the second from the last round will also end up with everyone betraying each other.
That is, GRIM* is more advantageous than GRIM, and GRIM** is more advanta-
geous than GRIM*. This argument will be continued indeﬁnitely, and in the end,
All-D will be advantageous. In other words, it was shown that All-D is the solution
of the Nash equilibrium in a narrow sense and the only ESS.
Consider the strength of the TFT again. In the battle between TFT and All-D,
the payoff matrix becomes the same as GRIM vs All-D payoff matrix, as is shown
below:

TFT
ALL-D
TFT
mR
S+(m−1)·P
ALL-D
T +(m−1)·P
mP

.
(6.6)
As it was previously mentioned, when m exceeds the threshold of T−P
R−P, i.e., m >
T−P
R−P, then GRIM was stable against invasion by All-D. On the other hand, TFT is

Emergence of Intelligence
■
175
advantageous in comparison with GRIM, in that it starts cooperating again if the
other party cooperates. Unlike GRIM, TFT can escape from the world of eternal
betrayal.
A few interesting ﬁndings have resulted from recent research on IPD[95]. First,
contrary to intuition, in IPD, the amount of past memory (history) when playing
games over and over does not produce a statistically signiﬁcant effect. In other
words, players with a long memory and players with a very short one (who deﬁne
strategies based solely on the immediately previous game) tend to achieve the same
score when IPD is carried out for an extremely long time.
Considering the above, a strategy called zero-determinant (ZD) was devised
for players who memorize just one game backwards. That was an optimal strategy
derived in terms of linear algebra. This strategy allows a player to unilaterally set
a linear relationship between the player’s own payoff and the co-player’s payoff
regardless of the strategy of the co-player [55]. However, it only works under spe-
ciﬁc conditions and does not always exist7. It is shown that, when a ZD strategy
is employed, even if the other player changes strategies in an evolutionary manner
(adaptively) the changes end up being fruitless.
6.1.5
IPD using GA
This section describes an experiment based on research conducted by Cohen et
al. [20], where an IPD strategy evolves by using a GA. As before, the ﬁrst player
is denoted as P1 and the other party is denoted as P2, and the beneﬁt chart in Table
6.3 is used to score the moves.
In this experiment, errors (i.e., when an unintentional move is made) and noise
(where the move communicated to the opponent is different from the actual move)
are introduced, and the emergence of cooperation in the case of incomplete infor-
mation is examined. There are 256 players in the IPD, arranged on a 16×16 lattice,
and the competition between players consists of four iterations. A single experiment
consists of 2,500 periods, where a single period comprises the following steps:
■
Competitions between all players.
■
Learning and evolution of strategies in accordance with an appropriate pro-
cess.
The basic elements of the experiment are as follows.
7It is necessary to hypothesize that the player’s expected payoff is generated in the steady state of a
Markov process.

176
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
1. Strategy space–an expression of the strategy adopted by a player, which is
divided into the following two types:
(a) Binary: Pure strategy.
(b) Continuous: Mixed strategy (stochastic strategy).
2. Interaction process—the set of opponents competing with a player is referred
to as the neighborhood of that player. The neighborhood can be chosen in
the following six ways:
(a) 2DK (2 Dimensions, Keeping): The neighbors above, left, below, and
right (NEWS: North, East, West and South) of each player on the lat-
tice are not changed, and the neighborhood is symmetrical. In other
words, if B is in the neighborhood of A, then A is necessarily in the
neighborhood of B.
(b) FRNE (Fixed Random Network, Equal): For each agent, there is a
neighborhood containing a ﬁxed number of agents (four). These agents
are selected at random from the entire population and remain ﬁxed
until the end. The neighborhood is symmetrical.
(c) FRN (Fixed Random Network): Almost the same as FRNE, although
with a different number of agents in the neighborhood of each agent.
The neighborhood is unsymmetrical. The average number of agents is
set to 8.
(d) Tag: A real number between 0 and 1 (a tag or sociability label) is
assigned to each agent, introducing a bias whereby agents with similar
tags can more easily compete with each other.
(e) 2DS (2 Dimensions, 4 NEWS): The same as 2DK, except that the
agents change their location at each period, thus, NEWS is set anew.
(f) RWR (Random-With-Replacement): A new neighborhood can be cho-
sen at each period. The average number of agents in the neighborhood
is set to 8, and the neighborhood is unsymmetrical.
3. Adaptation Process: The evolution of a strategy can follow any of the fol-
lowing three paths:
(a) Imitation: Searching the neighborhood for the agent with the highest
average score and copying its strategy if this score is larger than one’s
own score.
(b) BMGA (Best-Met-GA): Almost the same as Imitation; however, an
error is generated in the process of copying. The error can be any of
the following.
i. Error in comparing the performance: The comparison includes an
error of 10%, and the lower estimate is adopted.

Emergence of Intelligence
■
177
ii. Miscopy (sudden mutation): The mutation rate for genes in the
mixed strategy (Continuous) is set to 0.1, with the addition of
Gaussian noise (mean, 0; standard deviation, 0.4). In addition, the
rate of sudden mutation per gene in the pure strategy (Binary), in
which probabilities p and q are ﬂipped (i.e., 0 and 1 are swapped),
is set to 0.0399. When p mutates, probability i is also changed to
the new value of p.
(c) 1FGA (1-Fixed-GA): BMGA is applied to agents selected at random
from the entire population.
The strategy space is expressed through the triplet (i, p,q), where i is the prob-
ability of making move C at the initial step, p is the probability of making move C
when the move of the opponent at the previous step was C, and q is the probability
of making move C when the move of the opponent at the previous step was D.
By using this representation, various strategies can be expressed as follows:
All-C
i = p = 1, q = 1
All-D
i = p = 0, q = 0
TFT
i = p = 1, q = 0
anti-TFT
i = p = 0, q = 1
Here, the value of each of i, p and q is either 0 or 1. This form of representation of
the strategy space is known as a binary (or pure) strategy.
Conversely, the values of p and q can also be real numbers between 0 and 1,
and this representation is known as a continuous strategy. In particular, since 256
players took part in the experiment conducted by Cohen, the initial population was
selected from among the following combinations:
p
=
1
32, 3
32,··· , 31
32,
(6.7)
q
=
1
32, 3
32,··· , 31
32.
(6.8)
Speciﬁcally, each of the 16×16 combinations is set as a single agent (player) with
values p,q. Moreover, the value of i is set to that of p.
The performance (the average score of all players in each competition) is cal-
culated as the average value over 30 runs. Note that the occurrence of defection
is high if the average value of the population performance is close to 1.0, while
cooperation dominates if the value is close to 3.0. Nevertheless, the value does not
necessarily reach 3.0 even if the entire population adopts the TFT strategy due to
(1) sensor noise in the comparison and (2) copying errors in the adaptation process.

178
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
The score of the population can be obtained from the following measures:
■
Av1000: The average score of the last 1000 periods.
■
NumCC: The number of periods where the average score for the entire
population was above 2.3.
■
GtoH: Out of 30 iterations, the number of iterations in which the average
score for the entire population was above 2.3.
■
FracH: The proportion of periods with a score exceeding 2.3 after the aver-
age score for the entire population has reached 2.3.
■
FracL: The proportion of periods with a score below 1.7 after the average
score for the entire population has reached 2.3.
■
Rstd: The averaged score distribution for each period during iteration.
Here, an average score of 2.3 is regarded as the threshold above which the popu-
lation is cooperative, and a score below 1.7 indicates a population is dominated by
defection.
The results for all interaction and adaptation processes, in addition to all com-
binations of representations in the experiments (2×6×3 = 36 combinations), are
shown in Tables 6.6 and 6.7.
In conducting the experiments, an oscillatory behavior at the cooperation level
was observed for all combinations, namely, the average score oscillated between
high (above 2.3) and low (below 1.7) cooperation. This oscillation can be explained
from repetition of the following steps.
1. If a TFT player happens to be surrounded by TFT players, defecting players
are excluded.
2. TFT players soon establish a relationship of trust with other TFT players,
and, over the course of time, the society as a whole becomes considerably
more cooperative.
3. Defecting players begin to exploit TFT players.
4. Defecting players take a course to self-destruction after exterminating the
exploited players.
5. Return to 1.
Two important factors exist in the evolution of cooperation between players.
The ﬁrst factor is preservation of the neighborhood in the interaction process, where
the neighborhood is not limited to two-dimensional lattices. A cooperative rela-
tionship similarly emerges in ﬁxed networks, such as FRN. In addition, agents that

Emergence of Intelligence
■
179
Table 6.6: IPD beneﬁt chart and codes (Binary representation).
Interaction process
Adaptation process
Av1000
NumCC
GtoH
FracH
FracL
Rstd
2DK
1FGA
2.560 (.013)
30
19
.914
.001
0.173 (.017)
BMGAS
2.552 (.006)
30
10
.970
.000
0.122 (.006)
ImitS
3.000 (.000)
30
7
1.00
.000
0.000 (.000)
FRNE
1FGA
2.558 (.015)
30
21
.915
.000
0.171 (.012)
BMGAS
2.564 (.007)
30
9
.968
.000
0.127 (.007)
ImitS
2.991 (.022)
30
6
1.00
.000
0.000 (.000)
FRN
1FGA
2.691 (.008)
30
22
.990
.000
0.120 (.010)
BMGAS
2.629 (.010)
30
14
.913
.006
0.229 (.016)
ImitS
1.869 (1.01)
13
7
1.00
.000
0.000 (.000)
Tag
1FGA
2.652 (.010)
30
15
.975
.000
0.132 (.010)
BGMAS
1.449 (.186)
30
255
.191
.763
0.554 (.170)
ImitS
1.133 (.507)
2
6
1.00
.000
0.000 (,000)
2DS
1FGA
2.522 (.024)
30
26
.867
.000
0.197 (.020)
BMGAS
2.053 (.128)
30
95
.443
.280
0.532 (.064)
ImitS
1.000 (.000)
0
-
-
-
0.000 (.000)
RWR
1FGA
2.685 (.0009)
30
54
.985
.000
0.127 (.013)
BMGAS
1.175 (.099)
13
972
.191
.763
0.109 (.182)
ImitS
1.000 (.000)
0
-
-
-
0.000 (.000)
stochastically select their neighborhoods by using tags are more cooperative than
agents with random neighborhoods.
The second factor is the process of generating and maintaining the versatility of
the strategy. For example, cooperation is not common in adaptation processes that
are similar to Imitation. Since errors do not exist in Imitation, the versatility of the
strategy is soon lost, which often results in an uncooperative population. However,
on rare occasions, extremely cooperative populations arise even in Imitation, and
Imitation was the strategy producing the highest scoring population.
On the other hand, it is known that cooperative behavior may be difﬁcult to
evolve depending on the structure of the network in the neighborhood. In particular,
recent studies [119] have shown that cooperative behaviors are easier to evolve in
heterogeneous networks than in homogeneous networks8. There is also a report
that, when humans play IPD games, the cooperation evolves in a similar manner in
both the homogeneous and the heterogeneous networks [38].
6.1.6
IPD as Spatial Games
When the prisoner’s dilemma is conﬁgured as a spatial game, it results in interesting
patterns, such as the “big bang” or the “kaleidoscope” [85].
8Networks with a regular lattice-like structure of the neighborhood are called homogeneous networks.
On the other hand, if the network has a randomness or scale-free structure, it is heterogeneous.

180
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Table 6.7: IPD beneﬁt chart and codes (Continuous representation).
Interaction process
Adaptation process
Av1000
NumCC
GtoH
FracH
FracL
Rstd
2DK
1FGA
2.025 (.069)
30
127
.213
.172
0.313 (.045)
BMGAS
2.554 (.009)
30
26
.998
.000
0.075 (.006)
ImitS
2.213 (.352)
15
27
.868
.000
0.016 (.003)
FRNE
1FGA
2.035 (.089)
30
122
.226
.161
0.292 (.049)
BMGAS
2.572 (.007)
30
26
.996
.000
0.077 (.007)
ImitS
2.176 (.507)
14
26
.901
.000
0.017 (.014)
FRN
1FGA
1.884 (.120)
30
162
.182
.311
0.379 (.044)
BMGAS
2.476 (.026)
30
40
.939
.003
0.124 (.062)
ImitS
1.362 (.434)
3
12
1.00
.000
0.009 (.004)
Tag
1FGA
2.198 (.057)
30
80
.380
.064
0.248 (.058)
BGMAS
1.613 (.277)
30
331
.117
.499
0.469 (.093)
ImitS
1.573 (.187)
0
-
-
-
0.012 (.004)
2DS
1FGA
1.484 (.086)
30
695
.040
.764
0.273 (.080)
BMGAS
1.089 (.003)
1
978
.001
.977
0.024 (.009)
ImitS
1.096 (.013)
0
-
-
-
0.006 (.000)
RWR
1FGA
1.502 (.109)
30
612
.055
.729
0.332 (.064)
BMGAS
1.098 (.036)
9
1384
.021
.883
0.063 (.097)
ImitS
1.104 (.032)
0
-
-
-
0.006 (.001)
The rules of the spatial game are as follows. Each player occupies a lattice
point on a two-dimensional lattice space9 and competes with all adjacent players
(Moore neighborhood, 8 neighbors placed vertically, horizontally, and diagonally;
see Fig. 6.3). The score of this matchup is summed and the payoff is calculated.
Depending on the gain, the players at each lattice point change their strategies to
the highest scoring strategy of the opponent with whom they fought. The following
payoff table is adopted here:
C
D
C
1
0
D
b
0

,
(6.9)
where 3/2 < b < 5/3.
In addition, the cell (squares) of each player is colored in the following way:
■
Blue: C in the previous generation, C in the current generation
■
Red: D in the previous generation, D in the current generation
■
Green: D in the previous generation, C in the current generation
■
Yellow: C in the previous generation, D in the current generation
9The top and bottom, and the left and right of the square are connected, i.e., it has a toroidal structure.

Emergence of Intelligence
■
181
Each player battles with all adjacent players using  
the Moore neighborhood. The ﬁtness is derived from 
the total of these battle scores.
Figure 6.3: IPD as the spatial game.
Figure 6.4: Walker.
Therefore, blue and red indicate a cell of a player who has not changed, and green
and yellow indicate a cell that changed. The more green and yellow cells there are,
the more changes occurred.
For example, let us take a look at the arrangement in Fig. 6.4. Here, the red cell
indicates All-D and the blue cell indicates All-C. In other words, it is a cluster of
10 collaborators (All-C) surrounded by traitors. Numbers and symbols in cells are
the total payoff obtained from the matches against the Moore neighborhood. For
example, the value 5 for the blue at the base of the arrow is given as follows:
5 matches against All-C+3 matches against All-D = 5×1+3×0 = 5
Let us change the color of the cells one after another by executing IPD. Then this
cluster (walker) moves in the direction of the arrow in the ﬁgure. In other words, in
each generation legs move alternately: Right, left, right, left,... and it looks as if it
were walking with two legs.
Figure 6.5 shows the kaleidoscope obtained by evolution, Fig. 6.6 shows the
evolution leading to the Big Bang. A collaborative Big Bang is born by the collision
of the walkers (Fig. 6.4). A kaleidoscope is produced by a single traitor (All-D)

182
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.5: Kaleidoscope.
that invades a ﬁxed size square collaborator (All-C). The expansion of a constantly
changing symmetric pattern appears for a surprisingly long time. Since the total
number of possible placements is ﬁnite, the kaleidoscope reaches a ﬁxed pattern or
cycle after a long time.
Figure 6.6: Big Bang.
Color version at the end of the book

Emergence of Intelligence
■
183
There are also studies that extend this space model by incorporating asyn-
chronous elements [52]. The game was changed to a procedure in which ﬁrst a
selection is made from matchups of several neighborhood groups, after which an-
other selection is made from a matchup with a different group. As a result, it has
been observed that in the end all planes were occupied by traitors.
IPD has been extended in various ways, and it is actively studied in ﬁelds such
as artiﬁcial intelligence, artiﬁcial life, evolutionary economics, etc. Some of them
will be explained below.
6.1.7
Can the Dilemma be Eliminated with Quantum Games?
Research has been made to solve the dilemma based on quantum games. This is an
attempt to explain the seemingly irrational human behavior by considering a con-
ditional strategy. Let us make Alice and Bob the players of the Prisoner’s Dilemma
game. The payoff table in Table 6.8 was used. Alice predicts Bob’s behavior and
decides her action. When she is sure that Bob will betray her, she will most likely
cheat as well. On the other hand, even when Bob chooses to cooperate, Alice will
betray him with a certain probability. According to psychological experiments, she
will betray with 80% probability (cooperates with 20% probability). When Bob’s
next action is unknown (probabilities of his betraying or cooperating are equal), the
chance to betray in classical theory is 90%(= (80+100)/2) (called “natural law”).
However, according to psychological experiments, the probability of betrayal of
subjects is about 40%. This point was difﬁcult to explain with the conventional
logical reasoning.
Table 6.8: IPD payoff matrix and its code.
Opponent (P2) does not confess
Opponent (P2) confesses
P2 = C
P2 = D
Oneself (P1) does not confess
payoff : 3
payoff : 0
P1 = C
Code : R
Code : S
Oneself (P1) confess
payoff : 5
payoff : 1
P1 = D
Code : T
Code : P
Here, a strategy called quantum strategy Q will be introduced. This is a quan-
tum superposition of cooperation and betrayal at the degree. In other words, before
Alice and Bob choose a strategy, they make each state entangled (quantum entan-
glement) and inform them about that, after which they select a strategy. At this
time, it is shown that quantum strategy Q is a Nash equilibrium point and pareto
efﬁcient. This eliminates the Prisoner’s Dilemma. Although the natural law is not
satisﬁed, it can be explained as a probability theorem of quantum physics. In other

184
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Cooperate probability
Defect  probability
100％  Confession
80％  Confession
1.0
－1.0
－1.0
1.0
     ＝ 0.8
Cooperate probability
Defect  probability
1.0
－1.0
－1.0
1.0
(a) For different confession prob-
abilities
(b) Vector sum
Figure 6.7: State vectors for prisoner’s dilemma by quantum computing.
words, acts of betrayal with 80% and 100% probability are independent rational
judgments, but the probability of cheating decreases as a result of interference be-
tween subjects. In Fig. 6.7, dimensions of the quantum state vector are assumed to
be probabilities to remain silent and confess. In Fig. 6.7(a), a state vector with 100%
probability to confess (black) and a state vector with 80% probability to confess are
drawn. There are several state vectors with 80% chance to confess, which satisfy
a2 +b2 = 1.0,a2 = 0.2 and b2 = 0.8. When the opponent’s state is not known, the
two vectors, black and white, are superimposed to create a new state vector. At this
time, when the state vector pointing to the lower right is selected, it becomes as
shown in (b). Normalizing the resulting vector and solving it for the probability to
confess c, yields the following c =
1−b
√
a2+(1−b)2 ≈0.5, which provides results close
to the psychological experiment presented above.
Furthermore, considering the quantum games, it has been shown that coop-
erative behavior evolves more easily even on network structures, whereas, in the
classical framework, the cooperation has been difﬁcult to evolve [69].
The interpretation of quantum games described here does not mean that the
human brain or intelligence is based on quantum computation. It is considered as a
means of expressing the ﬂuidity of thought that is called “Quantum Cognition.” On
the other hand, the physicist Roger Penrose, famous for the Penrose Tile, argued
that our spirits can transcend ideas based on reason, so it cannot be reproduced
with machines [88], and that the quantum mechanical properties are involved in
the problems of behavior and consciousness of the brain on the macro scale, thus,
proposing the “quantum brain theory” [89].

Emergence of Intelligence
■
185
6.1.8
The Ultimatum Game: Are Humans Selﬁsh or Coopera-
tive?
The basis of the Prisoner’s Dilemma is that humans are inherently selﬁsh, that is,
usually they are supposed to betray. This theory is based on the idea of “selﬁsh
genes10” advocated by Dawkins11. According to this, altruistic behavior and group
selection will be wrong. Rather, the selﬁsh behavior of the gene causes a non-selﬁsh
motive (a gentle, deep altruistic mind [91]) in the human brain.
However, is mankind not really considerate? Regarding this point, recent re-
search results of the ultimatum games have raised a big question. The ultimatum
game is deﬁned as follows (Fig. 6.8):
Ultimatum game
There are two players A and B.  They are not being informed of one another 
yet. They are given a chance to share a ﬁxed amount of money and have only 
one chance. A is handed over 20$ and presents an amount between 0 and 20$ 
to B. B decides whether to accept or decline the amount presented by A. When 
B accepts, they share money as proposed by A and if B refuses, they both get 
nothing. What is the optimal strategy for A at this time?
The economically optimal strategy for this is clear. One dollar (i.e., the min-
imum unit for the proposal) should be better than nothing. Therefore, B should
accept the proposal even if it is only one dollar. From this, A should propose to
give only 1 dollar and keep the rest for himself. However, ordinary people do not
think in this way. Usually, B declines when presented with a lower amount. B ex-
presses his anger even at the price of missing money when he is clearly being taken
advantage of. On the other hand, A usually does not offer such an unfair proposal. A
proposes half of the money received to B. To avoid the proposal to be turned down,
people often behave in a generous way. For example, the answers obtained by the
author12 are shown in Fig. 6.9. In this Japanese version of the question, the amount
of money presented to A is 10,000JPY (Japanese Yen). 1US$=109.4270JPY (as of
Jan. 31, 2019). The minimum unit for the proposal is 1JPY.
10The idea that genes are skillfully acting to succeed in breeding and leave as many descendants as
possible. We are considered to be machines for gene survival (i.e., vehicles), living to make genes in our
bodies survive. Many biological phenomena are explained by this way of thinking.
11 Richard Dawkins (1941–): English evolutionary biologist. He has written many general books and
general introductions to biology and, as a result, espoused thinking on the “Selﬁsh gene,” “Meme” (cul-
tural information replicators), and “expanded phenotypes” (host operation due to the parasites, dams made
by beavers, and mounds of white ants can be seen as phenotypes). His revolutionary ideas and provocative
comments about evolution are still causing many discussions. He is a famous atheist.
12Data summarized in the same class as Table 4.2. However, this data is the total of students attending
three lectures in 2014-2015.

186
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
A: 
 
B: Responder
 
＄20 in one dollar bills
 
Oﬀer :  $X
Accept
Reject
A,B : Nothing
A : $(20 - X)
B: $X
　
Proposer
Figure 6.8: Ultimatum game.
0
10000
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
100
10
1
0
9000
8000
7000
6000
5000
4000
3000
2000
1000
100
10
1
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
25
30
35
A :  Proposed amount
B : Accepted amount
Frequency
Frequency
Avg.
Med.
Mode
Max
Min
4704
5000
5000
10000
0
Avg.
Med.
Mode
Max
Min
3225
3000
5000
10000
0
Figure 6.9: Results of ultimatum game.
It is surprising that science students who should be good at logical thinking are
losing money due to their morals. The median value of A’s offer is 5,000JPY, while
the mode of B’s accepted amount of money is 5,000JPY. However, the existence of
rationality is somewhat seen, thanks to the fairly frequent appearance of 1JPY as
B’s accepted amount of money.
As a stricter game, there is the following game called a Dictator game
(Fig. 6.10).
Dictator game
There are two players A and B. They are given a chance to share a certain 
amount of money and have only one chance. A is handed 20$ and gives the 
preferred amount of money between 0 and 20$ to B. Only A (=dictator) decides 
the result. What is the optimal strategy for A at this time?

Emergence of Intelligence
■
187
A: 
 
B: Responder
 
＄20 in one dollar bills
 
Oﬀer :  $X 
Accept  
Reject
 
A,B : Nothing
 
A : $(20 - X)
B: $X
 
Dictator
Figure 6.10: Dictator Game.
As a result of experiments conducted in many areas (Chicago, West Mongolia,
Tanzania), even in this kind of ultimatum game, A (the dictator) shared a lot of
money. It is said that on average 4$, i.e., 20% of the whole amount, was given to
the opponents [68].
These results shook the foundations of standard economics. The foundation of
modern economics is based on Homo Economicus (Economic man: Ultra-rational
and egocentric person). The above result means that “economic man” as described
in the traditional economic textbook is non-existent. In other words, compassion
and cooperative behavior may originally exist in human beings.
Furthermore, when a research group inhibited the function of the right dorsal
prefrontal cortex of the brain using electric stimulation during the ultimatum game,
subjects were driven by greed and accepted a small amount of money, even though
they kept thinking that it was unfair [32]. It seems that this area of the brain usually
suppresses self-interest (accepting any money) and prevents selﬁsh desires from in-
ﬂuencing the decision-making process. This area is extremely important for taking
fair action. In fact, there is a report that those who have damaged this area when
they were young, cannot stand in the position of others, even after becoming adults,
and they take up self-centered attitudes whenever confronted with ethical problems.
In other words, this area suppresses selﬁsh reactions and seems to play a major role
in acquiring social knowledge.
In addition, when oxytocin13 is sprayed on the subject’s nose, that person seems
to trust his/her partner even more [43]. As a result, when an experiment of Pris-
oner’s Dilemma game was conducted, in which teams were formed to compete, the
subjects sprayed with oxytocin tried to contribute to their team without making a
selﬁsh decision. They also acted more actively to interfere with the members of
the other teams and to protect their teams. However, oxytocin does not bring about
13A hormone synthesized in the paraventricular nucleus of the hypothalamus and neurosecretory cells
of the anterior nucleus. It is secreted from the posterior pituitary. The hormone is also called “love potion”
and “affection hormone” because it relieves stress and brings a happy mood.

188
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
altruism. It raises a sense of belonging to the social group to which the subject
belongs, and promotes cooperative behavior. Conversely, aggressiveness to people
outside of the social group appears to be raised. In other words, the neurobiological
mechanism of oxytocin secretion supports the view that it evolved to promote and
maintain harmonious and cooperative relationships in a closely-related group.
6.2
Evolutionary Psychology and Mind Theory
Evolutionary psychology is a ﬁeld aiming at a biological explanation of human be-
havior. It was founded by anthropologist John Tooby, psychologist Leda Cosmides,
and many other biologically-oriented anthropologists. They objected to the idea of
viewing the brain as a general-purpose learning machine. They claimed that each
module of the brain existed to be functional and that its design was the result of
natural selection. Therefore, the human mind is thought to have evolved to adapt to
past environments [99]D
Tooby uses the analogy of the army knife. The army knife consists of screw-
drivers and blades specialized for different functions. Similarly, there are visual
modules, language modules, sympathy modules, etc., in the brain14. The brain is
equipped with these innate means of data processing [99].
Evolutionary psychology focuses on the human mind so that it is distant from
social biology that explains actions by focusing on genes15 [109]. In particular,
Tooby states that the theory of natural selection itself is an elegant inference device.
Through this theory, a chain of deductive thoughts can be explained [13].
Evolutionary psychology has succeeded in explaining human cognition and
mental mechanisms in terms of evolution. The representative research results were
presented by an evolutionary linguist, Steven Pinker [90]. He claimed that language
was rooted in human nature and that communication evolved through ordinary nat-
ural selection. That is, the language is a highly specialized organ. Examples of
people acquiring sign language, etc. are raised as evidence, that the language is
an instinct (language instinct) [92]. In other words, the language is not simply a
product of culture, but an innate product unique to humans.
Since the history of evolutionary psychology is still shallow, while a variety
of studies have been published, some of which have been overwritten by new ex-
perimental results. Also, there are many who object to the evolutionary biological
view of humanity. For example, Steven Jay Gould has criticized that evolutionary
14On the other hand, the neuropsychologist Elkhonon Goldberg advocates the idea of a brain with
gradient-type working principle. Modularization is true for the thalamus, but the cerebral cortex is a
gradient-type and it is said to complement the modular approach [35].
15A research ﬁeld to study the social behavior of animals based on the results of biology. Edward
Wilson, a proponent of social biology [124], included a human in the last chapter of his book “Man: From
Sociobiology to Sociology.” Regardless of his intentions, he was criticized as an eugenicist and debate
on social biology came about. The controversy gradually escalated emotionally and water was poured on
Wilson during one of his lectures.

Emergence of Intelligence
■
189
psychology explains everything only with adaptation, many of which are just af-
terthoughts of the “what and why story” type [90]. In this section, let us explain
recent topics on Prisoner’s Dilemma based on evolutionary psychology.
Economists Andrew Schotter and Barry Sopher [73] veriﬁed the cultural suc-
cession process in a game similar to the Prisoner’s Dilemma called “Battle of the
Sexes”. In this game, the couple ﬁghts on whether to go to opera or soccer. They
do not want to go separately, but the destinations are different. There are two Nash
equilibrium points in this game (see Table 6.9). In other words, this is an unfair co-
operative game where there are no optimal and equally satisfying solutions for the
two. Such games are often found in everyday life. Andrew Schotter and Barry So-
pher have experimentally veriﬁed whether a tradition arises in this game and what
kind of cultural succession process it produces. As a result, the process of cultural
evolution similar to a punctuated equilibrium process16 was observed.
Table 6.9: Battle of the sexes.
2 ,  1
0 ,  0
Opera
Football
Husband
0 ,  0
Opera
Wife
 :  Equilibrium point
Football
1 ,  2
Interestingly, there are cases where the prisoner’s dilemma disappears [61]. The
upper part of Fig. 6.11 is a standard prisoner’s dilemma. The lower part shows that
the payoff matrix changes if the two prisoners are siblings sharing half of the genes.
Half of the payoff of a sibling prisoner B is added to prisoner A’s payoff. Likewise,
half of the payoff of the prisoner A is added to the prisoner B. At this time, the best
policy for each is to cooperate regardless of what the other party chooses. If the
situation of close relatives is taken into account in the process of evolution, there is
a good chance that the cooperative behavior will emerge.
It is known that human beings have cognitive behaviors that cannot be explained
with rational thinking. Self-deception and cognitive dissonance17 are representative
examples. It may be possible to explain this mysterious cognitive behavior from the
point of view of evolution. For example, according to the zoologist Robert Trivers
16A hypothesis advocated by Stephen Jay Gould and Niles Eldredge. The idea that evolution occurs
intensively in a short period of time when species diverge. Neutral gene recombination frequently takes
place during long stagnation. Some kind of trigger, such as a change in the environment, causes a rapid
evolution. There is a maxim that stagnation is data.
17An unreasonable behavior to convince oneself, in order to eliminate uncomfortable feelings with
inconsistent perception. For example, the reason why a person cannot quit smoking even though the person
knows that it is bad for health, is the belief that continuous smoking helps with mental stability or is a diet.

190
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
7
3
6
2
2
3
6
7
8
4.5
9
5.5
5.5
4.5
9
8
dilemma
A & B brothers
Best strategies 
for both :
Cooperation
7 ＋ 2 ÷ 2
    Beneﬁt from 
the viewpoint of A
     Beneﬁt from 
the viewpoint of B
Figure 6.11: Disappearance of the prisoner’s dilemma.
and Steven Pinker, adaptation to cooperation is said to have generated moral emo-
tions [93]. In addition to that, exaggerations on one’s kindness and skills can be
seen through, thus, further improving skills [90]. In other words, a psychologi-
cal arms race (co-evolution) occurs, such as one between liars and lie detectors.
There is a proverb saying “Liars have to be quick to learn (Liar should have good
memories.)”. Triggers for the lies to be found out are external clues such as incon-
sistencies, contradictions, hesitation, twitching blushes, sweating, etc. In order to
avoid such external cues, Trivers’s hypothesis is that some degree of self-deception
evolved through natural selection. If you lie to yourself, you will be more likely to
be trusted by others.
Although it is challenging, there is an evolutionary psychology explanation for
religion. For example, Richard Dawkins, known as an atheist, states that religion
is a virus [25]. He insists that sometimes religion is harmful and its alternative,
atheism should be socially accepted.
Also, religion is not a single subject. What is called a religion in modern west-
ern societies is a culture with another set of laws and customs, which survived
alongside the laws and customs of the nation by accident. Religion has created art,
philosophy, and law, and it has made a proﬁt for those who advertised it. We con-
clude this section with Pinker’s quotation about religious emergence, which solves
the problem of the afterlife (see page 161).
6.3
How does Slime Solve a Maze Problem? Slime In-
telligence
Figure 6.12 shows a simulator of bacterial colonies based on DLA. In this system,
the eating speed of bacteria, movement speed, the amount of food necessary for the
division, the diffusion rate of nutrients, etc. are variable. The distribution of bacte-
ria and distribution of nutrients are shown in another window, and the number of
bacteria and the total amount of nutrients are plotted on graphs. When an exper-

Emergence of Intelligence
■
191
Figure 6.12: Bacteria colony simulator.
iment was conducted, the bacteria were observed to form circular colonies when
sufﬁcient nourishment was available. However, when the density of nutrients was
low and the nutrients needed for the division were small in comparison to the eating
speed, the colony formed a pattern as shown on the right. This is considered to be
a strategy to search for nutrients in a wide area using the least amount of bacteria
in an environment with scarce resources.
As a more practical simulation, let us consider the growth of red bread mold
(Neurospora crassa). How mold grows is described in detail in the literature [101].
Sufﬁcient nutrition, oxygen, humidity, and appropriate temperature are necessary
for mold to grow. When these conditions are met, the mold grows and multiplies by
producing spores that are released into the air. The released spores usually fall to
places unsuitable for growth and die. However, spores that luckily landed in places
satisfying the conditions survive. When the spores survive, they grow, produce spo-
rangia, and new spores are released into the air. Here, the simulator modeling mold
growth followed the rules below:
■
There must be enough oxygen for mold to grow.
■
The humidity must be between 70 and 90 percent.
■
Temperature must be between 10 and 35 degrees.
■
The mold ﬂoating in the air hits the surface at a certain rate.
■
After a speciﬁc step, mold begins to emit spores.
■
Whether the mold attached to the surface grows, depends on temperature,
humidity, and characteristics of the surface.

192
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.13: Mold growth simulator.
(a) step = 30
(b) step = 100
(c) step = 150
(d) step = 200
(e) step = 250
(f) step = 300
(g) step = 350
(h) step = 400
Figure 6.14: Transition of mold distribution in the simulation. Mold is shown as white dots.
Although, strictly speaking, these are not DLA, they can be thought to be an exten-
sion. Figure 6.13 shows an overview of the simulator. Nutrition rate, humidity, and
temperature can be set in this simulator. The growth of the mold is drawn on two
dimensions. In addition, the occupancy of the mold is plotted and displayed every
hour.
Figure 6.14 shows the growth of the mold every 100 steps. In the ﬁgure, the
mold is rendered in white. Almost every place was occupied by mold in about 500
steps. The parameters at this time are as shown in Table 6.10.
During the simulation, the mold seems to create groups (clusters) and grow.
When spores spread out from the current mature mold, they fall down near the
mold that produced them and grow more easily. Figure 6.15 shows the transition of
the mold occupancy rate in time in the simulation. This coincides well with actual

Emergence of Intelligence
■
193
Table 6.10: Experimental parameters (growth of Neurospora crassa).
Parameter
Value
world X size
80
world Y size
80
nutrition rate
0.5
temperature
20 [◦C]
humidity
80 [%]
oxygen
true
mold emerge rate
0.001
mold growth step
5
Figure 6.15: Percentage of mold in the simulation.
results of mold observation18. In favorable conditions, mold grows in three stages.
It multiplies very slowly at the beginning. After occupying 10% to 20% of the
space, the growth speed dramatically increases. However, the growth rate declines
when the mold occupies 80% to 90% of the space. This phenomenon is thought
to occur from the balance of the number of fungi scattering new spores to the free
areas and the space occupied by new spores.
Slime mold (Myxomycete) are small creatures living in fallen trees and soil and
have two forms, one that moves like amoeba and one that looks like a mushroom.
In other words, a trophozoite called plasmodium is a unique living organism, pos-
sessing both the animal nature that feeds on microorganisms while it is moving as
well as the vegetable nature of breeding through spores. The life cycle of the mold
is shown in Fig. 6.16.
18For example, refer to the experimental results (condition A) shown in https://explorable.com/
mold-bread-experiment.

194
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Fruiting body
Plasmodium
Zygote
Spore
Gemination
Schizomycete
Figure 6.16: The life cycle of slime mold.
A study has been conducted to solve the shortest path of a 2-dimensional maze
using slime mold [81]. This search method using slime mold consists of the fol-
lowing two steps:
Step1 Fill the maze with the plasmodium of the slime mold.
Step2 Place sources of nutrition at the entrance and exit.
In Step2, the shortest path is obtained by optimizing the shape in which the
slime mold incorporates nutrition. This is made possible by the interactions of con-
tractive movements by rhythmic pulse present in slime mold and morphogenesis
of the tubes transporting nutrients. It is surprising that primitive organisms, such
as slime mold, exhibit intelligent behavior, that is, the simple behavioral principles
of the slime mold cells solve advanced problems. This related study has won the
(cognitive science ﬁeld) Ig Nobel Prize in 2008 (see Fig. 6.17).
Nakagaki et al. modeled the nutrient ﬂow made up of trophozoite of the slime
mold constituting the tube-shaped ﬂow path from the ﬂuid dynamics (hydrodynam-
ics) point of view and realized a method of performing a route search as a physical
simulation.
The basic behavior of the Physarum polycephalum (many-headed slime) con-
sists of morphological and ecological behaviors.
The following are known as morphological properties:
1. When scattered trophozoite come in contact with each other, they easily fuse
and form a single trophozoite.
2. Macroscopically, they consist of ﬁnely-branched, tube-shaped (tubular) ﬂow
path networks.

Emergence of Intelligence
■
195
(a)
(b)
(c)
(d)
(e)
(f)
Figure 6.17: Network formation by slime intelligence. Slime molds successfully formed
the pattern of a railway system quite similar to the Japanese railroad networks centering
Tokyo [116].
3. They try to maintain individuality as much as possible (prefer to bind to-
gether rather than dividing).
4. They try to surround a source of nutrition with their big bodies.
Also, there are the following ecological properties related to the route search:
1. Tubes in (parallel to) the direction of contraction movements of the rhythm
cells are strengthened, and perpendicular/orthogonal tubes fade.
2. The faster the rhythm of the contraction movement is, the more the cell is
supplied with nourishment.
In the following simulation, the above-mentioned slime maze route search will
be reproduced with a single slime mold cell as an individual. Individuals connected
to each other are distributed throughout the maze beforehand. The lifetime of the
individuals is assumed from the transportation and ingestion of the nutrients. In
other words, attention is paid only to the properties of the cells to intentionally
approach and bind to the food or cells that absorbed the nutrition. At this time, each
cell has a certain ﬁeld of view (5 cells) in the horizontal or vertical direction, and if
a cell neighboring the food or food itself is detected in the ﬁeld, they approach it.
The whole ﬁeld, the distribution of cells, etc. can be adjusted with the control panel
(see Fig. 6.18).
Initial values were set to 5,000 slime mold cells distributed over a square maze
with a width of 124 dots, and the result is shown in Fig. 6.19. The yellow dots in
the ﬁgure are mold, and the two red dots are the starting point and the goal of the

196
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.18: Maze search simulator.
(a) Initial stage
(b) Intermediate stage 1
(c) Intermediate stage 2
(d) Final stage
Figure 6.19: Simulation of maze search by slime mold.
maze. In the initial state (a), yellow slime mold cells are widely distributed. Out of
those, the mold cells close to the food in red squares try to come into contact with
the food. Also, the orange cells that came into contact with the food, become targets
of other yellow cells. As a result, an intermediate state (b) can be observed, where
cells are in progress on the path. Additionally, as the stage proceeds, the pathway
branching occurs and cells merge, thus, the maze is explored (see the intermediate

Emergence of Intelligence
■
197
stage (c)). In the ﬁnal situation (d), the cells grow until the connection between the
starting point and the goal is formed, providing a solution of the maze.
The simulations described above are based on DLA, and the only rule followed
by each cell is to come into contact with an element containing nutrition. However,
even the simplest rule, such as the one used here, enables exploration of the path
out of a simple maze. But the temporal and spatial costs of the route search are
too high (bad) compared to the other maze-solving algorithms. For more practical
simulation of a slime mold, it is necessary to add additional information related to
the physiological properties of slime mold, such as cell generation and destruction,
the principle of tube formation, and the transport of nutrients.
6.4
Swarm Robots
6.4.1
Evolutionary Robotics and Swarm
Evolutionary robotics [84] is a method for producing autonomous robots via evolu-
tionary processes. In other words, it is an attempt to train appropriate robot behavior
by representing the robot control system as a genotype and then applying EA search
techniques.
Evolutionary robotics was proposed in 1993 by Cliff, Harvey, and Hus-
bands [19], but evolutionary simulations of artiﬁcial objects with sensor motor sys-
tems were being conducted in the latter half of the 1980s. At that time, the control of
production robots required special skills and advanced programming abilities. In a
clear departure from traditional robotic design, a new generation of robots appeared
in the 1990s that shared features of simple biological systems. These features are
robustness, simplicity, compactness, and modularity. Those robots were designed
to be easily programmed and controlled, even by people with backgrounds quite
different from that of a conventional robot technician.
For example, Floreano et al. [84] have veriﬁed the Red Queen effect19 through
an evolutionary experiment with two Khepera robots (predator and prey). Each
robot has six infra-red sensors at the front, and another two at the back. The preda-
tor is also equipped with visual sensors. The maximum speed of the prey is twice
that of the predator. If the prey can run away cleverly, it will never be caught by the
predator. Against this, the predator has more sensor information available to track
the prey. The behavior of the predator and the prey were represented as genotypes
via evolutionary ANN20 direct coding and coevolved. One of the indexes used to
characterize ﬁtness was how well the prey survived. In other words, the longer the
prey can evade capture by the predator, the greater the prey’s ﬁtness. Conversely,
the faster the predator can catch the prey, the greater its ﬁtness. The result of co-
19Red Queen effect is an evolutionary type of arms race. The name is derived from Alice’s Adventures
in Wonderland. It is a model of co-evolution.
20EANN, see [54] for details.

198
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
evolution was the successive appearance of a series of fascinating tracking and
evasion maneuvers, just as predicted by the Red Queen hypothesis. Examples of
these behaviors include the “spider strategy” (backing away slowly until a wall be-
comes visible and then catching the prey as it approaches) and circling one another
like a dance.
In recent years, evolutionary robotics has been the focus of attention from re-
searchers in a variety of ﬁelds, including artiﬁcial intelligence, robotics, biology,
and social behavioral science. In addition, evolutionary robotics shares many fea-
tures with other AI approaches. These include:
■
behavior-based robots,
■
robot learning,
■
multi-agent learning and cooperative learning,
■
artiﬁcial life,
■
evolvable hardware, and
■
swarm robotics.
Swarm robotics is a ﬁeld of multi–robotics in which a large number of robots
are coordinated in a distributed and decentralised way according to swarm intelli-
gence. The main source of inspiration for swarm robotics comes from observing the
behavior of social animals and, in particular, social insects such as bees, ants and
termites (see Ch. 3 for details). Studies have revealed that no centralized coordina-
tion mechanisms exist behind the synchronized operations in social insects, and yet
their system’s performance is often robust, scalable and ﬂexible. These properties
are desirable for multi–robot systems and can be regarded as motivations for the
swarm robotic approach [12].
The robot systems can be separated into two categories: Centralized and decen-
tralized systems.
In centralized systems, a single camera observes the environment and the robots
as a whole, and a central computer processes the camera information to remotely
control each robot in order to accomplish a task. Another important characteristic is
that they possess a leader, which guides the rest of the robots. This leads to several
limitations, e.g., the leader’s death results in the loss of the team’s guidance, etc.,
which means to be not robust in real-world applications.
The decentralized systems operate without a leader, leaving the decision mak-
ing and the completion of certain tasks to each robot as individual. Given this, the
robots should have enough sensors (such as cameras, distance sensors, etc.) in or-
der to obtain sufﬁcient information from its surroundings and produce a similar
behavior as found in nature [117]. The advantages of these systems are robustness,
scalability, ﬂexibility and collective behavior.
In this section, we will explain how swarm robotics work for a transportation
problem in the decentralized way.

Emergence of Intelligence
■
199
6.4.2
Transportation Task for Swarm
The transportation of signiﬁcantly large objects by swarm robots has been stud-
ied since the industry started to develop. Engineers [56, 118] and researchers [3,
118, 117, 120, 127] have done plenty of work on this topic. For the task described
in [127], two large robots were used. However, these solutions tend to become ex-
pensive. Nowadays, we can obtain computationally powerful robots at an affordable
price with good hardware quality. On the other hand, for the sake of transportation,
conventional robot hardware does not offer enough power and torque. Neverthe-
less, researchers have developed the idea of using swarm intelligence on low-cost
robots in order to overcome these limitations.
The object transportation task using autonomous systems has been approached
and applied in many works. A collective transport strategy, inspired by the food
retrieval procedure of ant colonies, has been implemented on a swarm of robots
that are smaller than the object. The three most common types of strategies are
pulling, pushing, and caging [15].
In [72], the transportation of multiple objects dispersed in the environment
is studied. A non-prehensile manipulation method with a single robot was used,
putting emphasis on the path planning and task allocation.
The transportation of an object was carried out by a multi-robot optimiza-
tion [103]. In this research, two robots jointly carried a stick from an initial position
to a ﬁnal destination, without colliding against the obstacles. The sensory data of
the robots are the input variables of the optimization problem, while the output
variables are the necessary amount of rotation and translation of the stick.
Alkilabi et al. [3] presented a set of simulation experiments, in which au-
tonomous robots are required to coordinate their actions in order to transport a
cuboid object too heavy for a single robot to move.
6.4.3
Occlusion-based Pushing (OBP)
Occlusion-based pushing [15] consists of pushing an object as long as the robot
cannot see the goal (or the source of light or color). The goal is the immediate
place where the robots have to push the object (a red circle in the environment, see
Fig. 6.23). Because the robots have no speciﬁc ways to grasp the object, pushing is
the best way to move the object.
A positive aspect of this transportation strategy is that, rather than being treated
as a problem to solve, occlusion is utilized to organize a swarm of robots to push a
large object toward a goal. The algorithm deals with pushing the object across the
portion of its surface, where it occludes the direct line of sight to the goal. This may
result in the transportation of the object along a path that is not necessarily optimal,
but the object is always moved to the goal. Figure 6.20 illustrates how these robots
work together by OBP method.

200
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Goal
←
←
←
←
←
←
←
←
Object
Position  
B
Position  
A
Position  
C
Object Motion
Occluded Area
Figure 6.20: Illustration of how a swarm of robots can push a large object in a 2-D environ-
ment [15].
The simplicity of this method makes it particularly suitable for the implemen-
tation on mobile robots that have limited capabilities. In the long term, this simple
multi–robot strategy will be realized at very small scales and for such important
tasks as rescue scenarios.
Figure 6.21 presents a State diagram representation of the OBP algorithm pro-
posed by Chen [15]. This diagram produces the following robot behavior:
■
Once the object is found, the robot moves toward it (“Approach Ob-
ject”).
■
When the robot has reached the object, it enters the state of “Check for
Goal” to check whether the goal is visible from the robot’s position.
■
If the goal is not visible, the robot will push the object simply by mov-
ing against it (“Push Object”).
■
If the goal is visible, the robot will attempt to ﬁnd another position
around the object (“Move Around Object”).
In order to estimate the effectiveness of the OBP algorithm, we have imple-
mented a decentralized swarm system. For the sake of simulation, we use V-REP21
as our rendering tool. The chosen mobile robot is the Khepera III22. The device can
be found in the simulator as a premade sample. The OBP method was implemented
according to the diagram presented in Fig. 6.21.
21Coppelia Robotics. V–REP. http://www.coppeliarobotics.com/
22K-Team Corporation https://www.k-team.com/

Emergence of Intelligence
■
201
Figure 6.21: State diagram of a robot realizing the occlusion-based cooperative transporta-
tion [15].
Figure 6.22 shows the environment used for the ﬁrst experiment (i.e., Environ-
ment 1). It is an area of 16m2 of working space with the object placed in the bottom-
right corner. The goal is located in the top-left corner. We formed ﬁve groups of
robots: Teams of 1, 2, 3, 6 and 12 robots, respectively. For each group, simulation
was repeated ﬁve times in order to get the group’s best time. The object has a mass
of 1Kg for all trials.
Figure 6.22: Environment 1: OPB experimental area. Goal is represented as a red circle and
the object to transport as a blue circle.
In Fig. 6.23, we show the snapshots of robots’ behaviors. The time is indicated
as “minutes:seconds” with an interval of 30 second between the successive snap-
shots. Each horizontal series of snapshots corresponds to a different group of robots
performing the OBP method. The number of robots is shown in the left side of the
ﬁgure.

202
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
#.robots
1
2
3
6
12
Time
0:00
0:30
1:00
1:30
2:00
2:30
3:00
3:30
4:00
4:30
Figure 6.23: Swarm robots’ behaviors by OBP algorithm (Environment 1).

Emergence of Intelligence
■
203
Figure 6.24: Best time for each group of robots.
For each group, all trials were successfully completed, but some runs took more
time. In Fig. 6.24, we show the best time for each group. The best performance is
given by a 6-robot team. This is because all the six robots are constantly working
so as to push the object to the destination. In a 12-robot team, some robots tended
to interrupt others.
6.4.4
Guide-based Approach to OBP
The traditional OBP only works in open spaces, where the robots can see the goal
or the object after a 360-degree turn. However, if there are some obstacles between
the object and the goal, additional self-organizing mechanisms are required.
Chen [15] presented two ways of pushing an object to the destination through
an environment, where it is not possible to have a direct glance of the object and the
goal within a 360-degree turn. The ﬁrst idea is to manually place robots for the sake
of forming a path with sub-goals; then, when some robot gets close to the object,
this one will start to help the other robots push the object. The second approach is
to cover one robot with the goal’s color as a guide. Then, with a remote-control,
the guide is moved through the environment until it gets close to the destination,
while the rest of the robots are pushing the object. Because of the limitation of
this method, i.e., the necessity of manual or remote control, we have proposed an
algorithm that allows the swarm to push an object through a non-opened environ-
ment [66]. Note that the robots are assumed to have no information about the space
surrounding them. Also, the robots do not have any ways to communicate between
them.
Our algorithm allows the robots to produce a path, without their communica-
tion, so as to lead pusher robots where to bring the object back to the destination.
This path will work like the pheromone trail of an ant community (see sec.3.1.1).
A robot that takes up the role of creating a path is called a “guide.”

204
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) Environment 1: Open space
environment for OBP. Lines in-
dicate that every robot can see
the goal and the object.
(b) Environment 2: Non-open
space. Lines indicate the limita-
tion of the OBP method.
Figure 6.25: OBP experimental environments.
In our framework, each robot can take one of the following two modes:
■
Push mode: A robot pushing the object by applying the OBP method
(as shown in Fig. 6.21).
■
Guide mode: A robot placing itself in a strategic position as a sub-goal
for the purpose of leading other robots in Push mode.
When the traditional OBP condition is not fulﬁlled, our algorithm allows the robots
to explore the area in order to decide whether to take a guide role or a pusher role
(see Fig. 6.26). Guiding robots are necessary to let other robots retrieve the object
to home successfully23.
In the following part, Home is the place where the robots are deployed (rep-
resented as a red circle, see Fig. 6.25(b)). The difference between “Goal” and
“Home” are as follows. The former is used in the original OBP method developed
by Chen [15]. It is the immediate place where the robots have to push the object.
The latter refers to the ﬁnal destination of the object, which is the place where the
robots are deployed. In our method, “Goal” will be a sub-goal represented by some
guide robot.
The ﬁrst step is to determine if the home and the object are visible within one
360-degree turn. If both targets are visible in the ﬁrst check, the robot will perform
Push mode (the left state in Fig. 6.26). If only the home is visible, the robot will
play a guide role (the right state in Fig. 6.26).
23Note that this guiding strategy is robust because the guiding role emerges in a decentralized way. See
the limitation discussion of the centralized robot system in page 198.

Emergence of Intelligence
■
205
Figure 6.26: State diagram of the ﬁrst stage of guided OBP method.
Guide mode has a state called “Explore.” This state is performed by following a
Braitenberg obstacle avoidance method [110], for the purpose of sorting the walls
and robots in the environment until it can detect one of the targets (Home, guide
or object). The other two states are “Light Color & Turn” and “Follow object”.
The former refers to the change of colors on the robot’s external case from green
to orange (color used to identify the guide robots). Then, the robot rotates on its
own spot until the object becomes visible. The latter is performed after the object
is visible from the robot. The robot takes small turns while observing the object,
allowing it to keep the object perpendicular to its vision line.
The algorithm shown in Fig. 6.27 is executed in the following ways: The robot
ﬁrst explores the environment until it ﬁnds the object. Then, the robot approaches
the object until it is touched. Afterwards, the robot checks if Home or a guide is
visible from its position. If it is, the robot starts to perform Push mode towards
the visible sub-goal. Otherwise, the robot explores again the environment until it
ﬁnds Home or the closest guide. If the robot can see the guide or Home and also
see another robot, it will go back to search for the object. If it can just see another
guide or home, it will place itself at a speciﬁc distance to be watched by the other
robots and form the path. When the robot is already placed in that position and is
watching the approaching object, it will wait until the object gets close so as to
change its state to Push mode.
When our method (Guided Pushing) is applied to a swarm of robots, it results
in producing a path in the environment and, therefore, the rest of the robots learn
to move the object, as if the robots use this guidance as a pheromone trail.
6.4.5
Let us See How they Cooperate with each other
In this section, we shall show how swarm robots work together by the proposed
Guide-based OBP. The experimental set-ups are described below.

206
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.27: State diagram of Guide mode.
The robots can recognize four colors so as to differentiate the targets and other
team members:
■
Red: Home representation.
■
Blue: Object representation.
■
Green: Normal robot representation.
■
Orange: Guide robot representation.
In order to show the effectiveness of our method, we have conducted experi-
ments with V-REP simulator. The environmental design is based on the work of
Chen [15] with slight modiﬁcation. As shown in Fig. 6.28, we enlarged the envi-
ronment to 64m2 with an obstacle of 25m2, resulting in a total working area of 39m2
with walls of 0.2m high. The object has a mass of 2Kg.
In a larger and complicated environment, the traditional OBP method would
be stuck at the “Search Object” (see Fig. 6.21). This is due to the impossibility of
perceiving the object in a non-open environment. The robots can not locate and
approach the object because of the obstacle of 25m2 in the area.

Emergence of Intelligence
■
207
Figure 6.28: Environment 2: First experiment of Guided OBP method. Note that a red circle
is Home.
Guide
t = 0
t = 4:40
t = 5:03
t = 7:27
t = 8:25
Horizontal
Path(⇐)
t = 10:00
t = 11:00
t = 12:00
t = 13:00
t = 14:00
Vertical
Path (⇑)
t = 15:00
t = 16:00
t = 17:00
t = 18:00
t = 19:20
Figure 6.29: Experimental results (Guided OBP method).
In Fig. 6.29, we show the snapshots of the simulation. The whole experiment
took 19:20 (min:sec). Remember that the robots would not start the pushing process
until they know where to push. We can observe that the time the last guide is placed
(at the end of the guiding process) is 8:25 (min:sec), leaving the rest of the task to
the pusher robots. When the object is close to one of the guides (see the snapshot
of time 8:25 in Fig. 6.29), this one becomes a pusher that helps the rest in the
process of retrieving the object. Overall swarm robots’ motion in Guide mode is
represented by arrow direction, i.e., ⇐and ⇑.
Color version at the end of the book

208
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
Figure 6.30: Environment 3: Second Experiment of Guided OBP method.
The third environment includes an external area of 96m2 and a total working
space of 86m2 with walls of 0.6m high (see Fig. 6.30). These obstacles occupy
10m2 of the total area. Here, Guide mode algorithm is tested in this larger area, in
order to observe how effectively the robots produce a path between the object and
the home. The object has a mass of 2Kg.
Figure 6.31 shows the snapshots of the complete simulation. The total duration
was 2:40:00 (h:min:sec). The guiding process was 1:15:22 long and the rest of the
time was spent on the pushing process. The snapshots are divided into the follow-
ing three types: (1) the guide formation process in the ﬁrst line, (2) the vertical
movement in Guide mode, and (3) the horizontal movement in Guide mode. Note
that the guide positions (marked with orange circles) were successfully identiﬁed
by the pusher robots.
These results have demonstrated how the swarm of robots accomplished the
task of retrieving an object in a complex and non-open environment.

Emergence of Intelligence
■
209
Guide
0:00:00
0:33:53
0:57:35
0:59:15
1:08:27
1:15:22
Vertical
Path 1 (⇑)
1:17:00
1:18:00
1:19:00
1:22:00
1:23:00
1:25:00
Horizontal
Path 1(⇐)
1:26:00
1:27:00
1:28:00
1:29:00
1:30:00
1:32:00
Vertical
Path 2 (⇓)
1:33:00
1:36:00
1:39:00
1:42:00
1:45:00
1:48:00
Horizontal
Path 2 (⇐)
1:49:00
1:54:00
1:58:00
2:03:00
2:08:00
2:11:00
Vertical
Path 3 (⇑)
2:12:00
2:25:00
2:40:00
2:14:00 
2:17:00 
2:20:00
Figure 6.31: Guided OBP method applied in a large environment.


Chapter 7
Conclusion
Constantly making new combinations, and interesting combinations of ideas
stick together, and eventually percolate up into full consciousness. That’s not
too different from a biological population in which individuals fall in love and
combine to produce new individuals.
—Gregory Chaitin, META MATH! The Quest for Omega
7.1
Summary and Concluding Remarks
In this book, we have explained the concept of “emergence” for AI, focusing on
swarm. The basic algorithm to achieve that is based on the theory of evolution.
As a ﬁnal thought, we will explore the fact that evolution is still ongoing (“evo-
lution at work”).
The Galapagos Islands are famous for being the place where Charles Darwin
came up with the theory of evolution in the middle of his voyage with the surveying
British Navy ship Beagle. He had the idea after observing that the beaks of ﬁnch
birds vary slightly in shape depending on the species, and that the shells of giant
tortoises vary from island to island. However, this story is somewhat exaggerated.
Darwin did not study these birds in depth in the ﬁeld. Those who have read “On
the origin of species” realize that the data from Galapagos was not fully utilized.
Rather, his discussion was always restricted to artiﬁcial selection.
Nevertheless, this island continues to draw considerable attention as a core for
research on evolution theory. For instance, Peter Grant from Princeton University
conducted a thorough 20-year-long research on ﬁnches [39], clarifying that their
beaks change dramatically due to selection pressure caused by food availability,
which varies due to climate changes. Moreover, he found that evolution occurs at

212
■
AI and Swarm: Evolutionary Approach to Emergent Intelligence
(a) land iguana
(b) marine iguana
(c) hybrid iguana
Figure 7.1: Galapagos iguana.
an extremely fast rate (a matter of years), rather than the previously imagined period
of tens of thousands of years.
Furthermore, the Galapagos Islands are home to two types of iguanas, namely,
the land iguana and the marine iguana (see Fig. 7.1). The yellow land iguana
originally lived on land and used to eat prickly pear cactus sprouts. However, co-
evolution with cactuses resulted in lack of food. Eventually, arms races took place
among cactuses, land iguanas, and giant tortoises. Cacti grew taller in order to pro-
tect their sprouts from being eaten. They also developed pricks in their stalks, to
stop land iguanas from climbing on them. Thus, food shortage started to occur. It
is conjectured that they evolved into sea iguanas that are able to swim and eat sea
weeds. The lifestyle of sea iguanas differs greatly from that of land iguanas. Their
bodies became black to better catch day light, and they became able to hold their
breath and dive deep into the water. When they return to land, they expel salt from
their noses and bask in the sun. An interesting fact that became a recent topic is
the birth of hybrid iguanas, resulting from a female sea iguana and a male land
iguana. They are pink-colored iguanas that exhibit dual characteristics. They also
have nails that allow them to climb cactuses. However, at present they do not seem
to be fertile.
As a way of thinking, evolution is not restricted to living beings with genes. The
idea also holds in a variety of other systems. We can ﬁnd several famous phrases
and sayings about the matter. Matt Ridley [100] referred to the idea of evolution
above as a way of thinking that can be widely applied as the “general theory of
evolution.” In contrast, the evolution of living bodies based on natural selection
proposed by Darwin is considered a “special theory of evolution.” As seen in this
book so far, the swarming mechanism can explain several phenomena in society,
currency, technology, language, culture, politics, and moral based on the general
theory of evolution.
This book explains a variety of topics based on this idea. We hope that from
now on, further developments based on evolution and emergence will occur in AI
methodologies.

References
[1] Adamatzky, A., “Voronoi–like partition of lattice in cellular automata,” Math-
ematical and Computer Modelling, vol.23, no.4, pp.51–66, 1996.
[2] Adamatzky, A., Costello, B., and Asai, T., Reaction-Diffusion Computers, El-
sevier Science, 2005.
[3] Alkilabi, M.H.M., Lu, C., and Tuci, E., “Cooperative Object Transport Using
Evolutionary Swarm Robotics Methods,” in Proc. of the European Confer-
ence on Artiﬁcial Life (ECAL), pp.464–471, 2015.
[4] Andre, D., Bennett III, F.H., and Koza, J.: “Evolution of Intricate Long-
Distance Communication Signals in Cellular Automata using Genetic Pro-
gramming,” in Artiﬁcial Life V: Proceedings of the Fifth International Work-
shop on the Synthesis and Simulation of Living Systems, 1996.
[5] Arthur, W.B., Increasing Returns and Path Dependence in the Economy, Uni-
versity of Michigan Press, 1994.
[6] Auer, P., Cesa-Bianchi, N., and Fischer, P., “Finite-time analysis of the multi-
armed bandit problem,” Machine Learning, vol.47, no.2, pp.235–256, 2002.
[7] Axelrod, R.: “The Evolution of Cooperation,” Basic Books, New York, NY,
1984.
[8] Axelrod, R.: “An evolutionary approach to norms,” American Political Sci-
ence Review, vol.80, no.4, pp.1095–1111, 1986.
[9] B¨ack, T., Hoffmeister, F., and Schwefel, H.-P., “An Survey of Evolution
Strategies,” in Proc. 4th International Conference on Genetic Algorithms
(ICGA91), pp.2–9, Morgan Kaufmann, 1991.
[10] B¨ack, T., “An overview of evolutionary algorithms for parameter optimiza-
tion,” Evolutionary Computation, vol.1, no.1, pp.1–23, 1993.

214
■
References
[11] Bellman, R., Adaptive Control Processes–A Guided Tour, Princeton Univer-
sity Press, 1961.
[12] Bonabeau, E., Dorigo, M., and Theraulaz, G., Artiﬁcial Intelligence from Nat-
ural to Artiﬁcial Systems, Oxford University Press, New York, 1999.
[13] Brockman, J., This Explains Everything: 150 Deep, Beautiful, and Elegant
Theories of How the World Works, Harper Perennial, 2013.
[14] Civicioglu, P., and Besdok, E., “A conceptual comparison of the Cuckoo-
search, particle swarm optimization, differential evolution and artiﬁcial bee
colony algorithms,” Artiﬁcial Intelligence Review, vol.39, no.4, pp.315–346,
2013.
[15] Chen, J., Gauci, M., Li, W., Kolling, A., and Groβ, R., “Occlusion-based
cooperative transport with a Swarm of miniature mobile robots,” IEEE Trans-
actions on Robotics, vol.31, no.2, pp.307–321, 2015.
[16] Chu, S.-C., Tsai, P.-W., and Pan, J.-S., “Cat Swarm Optimization,” in Proc.
Paciﬁc Rim International Conference on Artiﬁcial Intelligence (PRICAI
2006), pp.854-858, 2006.
[17] Clarke, R.D., “An application of the Poisson distribution,” vol.72, no.3, Jour-
nal of the Institute of Actuaries, 1946.
[18] Clerc, M., and Kennedy, J.: “The Particle Swarm: Explosion, Stability, and
Convergence in a Multimentional Complex Space,” IEEE tr. Evolutionary
Computation, vol.6, no.1, pp.58–73, 2002.
[19] Cliff, D, Harvey, I., and Husbands, P., “Explorations in evolutionary robotics,”
Adaptive Behavior, vol.2, pp.72-110, 2000.
[20] Cohen, M.D., Riolo, R.L., and Axelrod, R.: “The emergence of social or-
ganization in the prisoner’s dilemma: How context-preservation and other
factors promote cooperation,” Santa Fe Institute working paper 99-01-002,
http://cscs.umich.edu/research/techReports.html, 1999.
[21] Couzin, I.D., Krausew, J., Jamesz, R., Ruxtony, G.D., and Franksz, N.R.,
“Collective memory and spatial sorting in animal groups,” Journal of The-
oretical Biology, vol.218, no.1, pp.1–11, 2002.
[22] David, P.A., “Clio and the economics of QWERTY,” The American Economic
Review, vol.75, no.2, Papers and Proceedings of the Ninety-Seventh Annual
Meeting of the American Economic Association, pp.332–337,1985.
[23] Davies, N.B., Krebs, J.R., and West, S.A., “An Introduction to Behavioural
Ecology,” Wiley-Blackwell, 4th Edition, 2012.

References
■
215
[24] Dawkins, R.: The selﬁsh gene, Oxford Univ. Press, 1991.
[25] Dawkins, R.: The God Delusion, Mariner Books, 2008.
[26] Deb, K.D., Pratap, A., Agarwal, S., and Meyarivan, T., “A fast and elitist mul-
tiobjective genetic algorithm : NSGA-II,” IEEE Transactions on Evolutionary
Computation, vol.6, no.2, pp.182–197, 2002.
[27] Devlin, K., The Math Instinct: Why You’re a Mathematical Genius (Along
with Lobsters, Birds, Cats, and Dogs), Basic Books, 2009.
[28] Dewdney, A.K., “The hodgepodge machine makes waves,” Scientiﬁc Ameri-
can, vol.259, no.2, 1988.
[29] Dewdney, A.K., “The cellular automata programs that create wireworld, rug-
world and other diversions, Computer Recreations,” Scientiﬁc American, 72,
pp.146–149 Jan. 1990.
[30] Dorigo, M., and Gambardella, L.M.: “Ant colonies for the traveling salesman
problem”, Tech. Rep. IRIDIA/97-12, Universite Libre de Bruxelles, Belgium,
1997.
[31] Fung, K., Numbers Rule Your World: The Hidden Inﬂuence of Probabilities
and Statistics on Everything You Do, McGraw-Hill Education, 2010.
[32] Gazzaniga, M.S., Who’s in Charge?: Free Will and the Science of the Brain,
Ecco, 2011.
[33] Geem, Z.W., Kim, J.H., and Loganathan, G.V., “A new heuristic optimiza-
tion algorithm: Harmony search,” Simulation, vol.76, no.2, pp.60–68, 2001.
Physical Review E, vol.79, 2009.
[34] Ghosh, A., Dehuri, S., and Ghosh, S. (eds.), “Objective Evolutionary Algo-
rithms for Knowledge Discovery from Databases,” Springer, 2008.
[35] Goldberg, E., The New Executive Brain: Frontal Lobes in a Complex World,
Oxford University Press, 2009.
[36] Goldberg, D.E., Genetic Algorithms in Search, Optimization and Machine
Learning, Addison Wesley, 1989.
[37] Goss, S., Aron, S., Deneubourg, J.L., and Pasteels, J.M.: “Self-organized
shortcuts in the argentine ant,” Naturwissenschaften, vol. 76, pp. 579–581,
1989.
[38] Gracia-L´azaro, C., Ferrer, A., Ruiz, G., Taranc´on, A., Cuesta, J.A., S´anchez,
A., and Morenoa, Y., “Heterogeneous networks do not promote coopera-
tion when humans play a Prisoner’s Dilemma,” Proceedings of the National
Academy of Sciences, PNAS, vol.109, no.32, pp.12922–1292, 2012.

216
■
References
[39] Grant, P.R., and Grant, B.R., How and Why Species Multiply: The Radia-
tion of Darwin’s Finches, Princeton Series in Evolutionary Biology, Princeton
Univ Press, 2011.
[40] Gravner, J., and Griffeath, D., “Modeling snow crystal growth II: A meso-
scopic lattice map with plausible dynamics,” Physica D: Nonlinear Phenom-
ena, vol.237, no 3, pp.385–404, 2008.
[41] Gravner, J., and Griffeath, D., “Modeling snow-crystal growth: A three- di-
mensional mesoscopic approach,” Physical Review E, vol.79, 2009.
[42] Gould, S.J., Bully for Brontosaurus: Reﬂections in Natural History,
W.W.Norton & Co.Inc., 1992.
[43] Haidt, J., The Righteous Mind: Why Good People Are Divided by Politics and
Religion, Vintage, 2013.
[44] Harman, O., The Price of Altruism: George Price and the Search for the Ori-
gins of Kindness, W.W.Norton & Co.Inc., 2011.
[45] He, C., Noman, N., and Iba, H., “An Improved Artiﬁcial Bee Colony Algo-
rithm with Non-separable Operator,” in Proc. of International Conference on
Convergence and Hybrid Information Technology, 2012.
[46] Hepper, F., and Grenader, U., “A stochastic nonlinear model for coordinated
bird ﬂocks,” AAAS publication, Washington,DC, 1990.
[47] Herdy, M., “Appication of the Evolution Strategy to Discrete Optimization
Problems,” in Parallel Problem Solving from Nature (PPSN), Schwefel, H.-P.
and M¨anner, R. (eds.), pp.188–192, Springer-Verlag, 1990.
[48] Higashi, N., and Iba, H.: “Particle Swarm optimization with gaussian muta-
tion,” Proceedings of IEEE Swarm Intelligence Symposium (SIS03), pp.72–
79, 2003.
[49] Hilditch, C.J., Linear Skeletons From Square Cupboards in Meltzer,B. &
Michie,D. (eds.), Machine Intelligence 4, pp.403–420, Edinburgh University
Press, 1969.
[50] Hoel, E.P., Albantakis, L., and Tononi, G., “Quantifying causal emergence
shows that macro can beat micro,” Proceedings of the National Academy of
Sciences, PNAS, vol.110, no.49, pp.19790–19795, 2013.
[51] Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems. University of
Michigan press, 1975.
[52] Huberman, B.A., and Glance, N.S.: “Evolutionary games and computer sim-
ulations,” Proceedings of the National Academy of Sciences, PNAS, vol.90,
no.16, pp.7716–7718, 1993.

References
■
217
[53] Iba, H., and Noman, N.: “New Frontiers in Evolutionary Algorithms: Theory
and Applications,” ISBN-10:1848166818, World Scientiﬁc Publishing Com-
pany, 2011.
[54] Iba, H.: “Evolutionary Approach to Machine Learning and Deep Neural Net-
works: Neuro-Evolution and Gene,” ISBN-10:9811301999, Springer, 2018.
[55] Ichinose, G., and Masuda, N., “Zero-determinant strategies in ﬁnitely re-
peated games,” Journal of Theoretical Biology, vol.438, pp.61–77, 2018.
[56] Inglett, J.E., and Rodriguez-Seda, E.J., “Object Transportation by Coopera-
tive Robots,” in Proc. SoutheastCon 2017, Charlotte, NC, pp. 1-6, 2017.
[57] Ishibuchi, H., Tsukamoto, N., and Nojima, Y., “Evolutionary many-objective
optimization: A short review,” in Proc. IEEE Congress on Evolutionary Com-
putation, pp. 2419–2426, 2008.
[58] Karaboga, D., and Basturk, B.: “A powerful and efﬁcient algorithm for numer-
ical function optimization: Artiﬁcial Bee Colony (ABC) algorithm,” Journal
of Global Optimization, vol.39, pp.459–471, 2007.
[59] Karaboga, D., Gorkemli, B., Ozturk. C., and Karaboga, N.: “A Comprehen-
sive Survey: Artiﬁcial Bee Colony (ABC) Algorithm and Applications,” Arti-
ﬁcial Intelligence Review, Doi:10.1007/s10462-012-9328-0, 2012.
[60] Kendall, G., Yao, X., and Chong, S.-Y.: “The Iterated Prisoners’ Dilemma: 20
Years on,” World Scientiﬁc Pub Co Inc., 2007.
[61] Kenrick, D.T., Sex Murder and the Meaning of Life: A Psychologist Investi-
gates How Evolution Cognition and Complexity are Revolutionizing our View
of Human Nature, Basic Books, 2011.
[62] Kennedy, J., and Eberhart, R.C.: “Particle swarm optimization,” Proceedings
of IEEE the International Conference on Neural Networks, pp.1942–1948,
1995.
[63] Kennedy, J., and Eberhart, R.C.: “Swarm Intelligence,” Morgan Kaufmann
Publishers, 2001.
[64] Kondo, S., and Asai, R., “A reaction diffusion wave on the skin of the marine
angelﬁsh Pomacanthus,” Nature, vol.376, pp.765-?768, 1995.
[65] Kusch, I., and Markus, M.: “Mollusc Shell pigmentation: Cellular automaton
simulations and evidence for undecidability,” Journal of theoretical biology,
vol.178, pp.333-340, 1996.

218
■
References
[66] Landaez, Y., Dohi, H., and Iba, H., “Swarm Intelligence for Object Retrieval
Applying Cooperative Transportation in Unknown Environments,” in Proc.
2018 4th International Conference on Robotics and Artiﬁcial Intelligence
(ICRAI 2018), 2018.
[67] Levesque, H.J., “Is it enough to get the behaviour right?,” Proc. of 21st Joint
Conference on Artiﬁcial Intelligence (IJCAI-09), pp.1439–1444, 2009.
[68] Levitt, S.D., and Dubner, S.J., Freakonomics – A Rogue Economist Explores
The Hidden Side Of Everything, William Morrow/Harper-collin, 2005.
[69] Li, A., and Yong, X., “Entanglement Guarantees Emergence of Co-
operation
in
Quantum
Prisoner’s
Dilemma
Games
on
Networks,”
doi:10.1038/srep06286 Scientiﬁc Reports 4, Article number: 6286, 2014.
[70] Lohmann, R., “Structure evolution and incomplete induction,” in Proc.
2nd Parallel Problem Solving from Nature (PPSN92), pp.175–185, North-
Holland, 1992.
[71] Martinez, G.J.: “Introduction to Rule 110,” Rule 110 Winter WorkShop, 2004
http://www.rule110.org/amhso/results/rule110-intro/introRule110.html
[72] Melo, R.S., Macharet, D.G., and Campos, M.F.M., “Multi–object Transporta-
tion Using a Mobile Robot,” in Proc. 12th Latin American Robotics Sympo-
sium and 3rd Brazilian Symposium on Robotics (LARS-SBR), pp.234–239,
2015.
[73] Mesoudi, A., Cultural Evolution: How Darwinian Theory Can Explain Hu-
man Culture and Synthesize the Social Sciences, University of Chicago Press,
2011.
[74] Michalewics, Z., Genetic Algorithms + Data Structures = Evolution Pro-
grams, Springer-Verlag, 1992.
[75] Mitchell, M.: “Life and evolution in computers,” History and Philosophy of
the Life Sciences vol.23, pp.361–383, 2001.
[76] Mitchell, M.: “Complexity: A Guided Tour,” Oxford University Press, 2009.
[77] Mochizuki, A., “Pattern formation of the cone mosaic in the zebraﬁsh retina:
a cell rearrangement model,” J. Theor. Biol., vol.215, no.3, pp.345–61, 2002.
[78] Murray, J.D., Mathematical Biology: I. An Introduction, Springer, 3rd print-
ing 2008.
[79] Murray, J.D., Mathematical Biology II: Spatial Models and Biomedical Ap-
plications, Springer, 3rd printing 2008.

References
■
219
[80] Nagel, K., and Shreckenberg, M.: “A cellular automaton model for freeway
trafﬁc,” Journal de Physique I, vol.2, no.12, pp.2221–2229, 1992.
[81] Nakagaki, T., Yamada, H., and Toth, A., “Intelligence: Maze-Solving by an
Amoeboid Organism,” Nature, vol.407, p.470, 2000.
[82] Neary, T., and Woods, D.: “P-completeness of cellular automaton Rule 110,”
Proceedings of ICALP 2006 - International Colloquium on Automata Lan-
guages and Programming, Lecture Notes in Computer Science, vol.4051,
pp.132-143, Springer, 2006.
[83] Ninagawa, S.: “1/f noise in elementary cellular automaton rule 110,” Pro-
ceedings of the 5th international conference on Unconventional Computa-
tion, UC06, Lecture Notes in Computer Science, vol.4135/2006, pp.207–216,
Springer, 2006.
[84] Nolﬁ, S., and Floreano, D. Evolutionary Robotics, MIT Press, 2000.
[85] Nowak, M.A.: “Evolutionary Dynamics: Exploring the Equations of Life,”
Belknap Press of Harvard University Press , 2006.
[86] Onuma, H., Okubo, A., Yokokawa, M., Endo, M., Kurihashi, A., and Sawa-
hata, H., “Rebirth of a Dead Belousov–Zhabotinsky Oscillator,” The Journal
of Physical Chemistry A, vol.115, no.49, pp14137–14142, 2011.
[87] Parkinson, R., “The Dvorak Simpliﬁed Keyboard: Forty years of Frustration,”
Computers and Automation magazine, vol.21, pp.18–25, November, 1972.
[88] Penrose, R., The Emperor’s New Mind: Concerning Computers, Minds, and
The Laws of Physics, Pxford Univ Press, 1989.
[89] Penrose, R., “Beyond the doubting of a shadow a reply to commentaries on
shadows of the mind,” PSYCHE: An Interdisciplinary Journal of Research On
Consciousness, vol.2, no.23, 1996.
[90] Pinker, S., How the Mind Works, W.W.Norton & Co.Inc., 1991.
[91] Pinker, S., The Blank Slate: The Modern Denial of Human Nature, Penguin
Books, 2003.
[92] Pinker, S., The Language Instinct: How The Mind Creates Language, Harper
Perennial Modern Classics, 2007.
[93] Pinker, S., The Better Angels of Our Nature: Why Violence Has Declined,
Penguin Books, 2012.
[94] Pinto, A., Oates, J., Grutter, A., and Bshary, R., “Cleaner wrasses Labroides
dimidiatus are more cooperative in the presence of an audience,” Current Bi-
ology, vol.21, no.13, pp.1140–1144, 2011.

220
■
References
[95] Press, W.H., and Dyson, F.J., “Iterated Prisoner’s Dilemma contains strate-
gies that dominate any evolutionary opponent,” PNAS: Proceedings of the
National Academy of Sciences, vol.109, no.26, pp.10409–10413, 2012.
[96] Rajewsky, N., Santen, L., Schadschneider, A., and Schreckenberg, M.: “The
asymmetric exclusion process: Comparison of update procedures,” Journal of
Statistical Physics, vol.92, pp.151–194, 1998.
[97] Rechenberg, I., “Evolution strategy and human decision making,” Human de-
cision making and manual control, Willumeit, H. P. (ed.), pp.349–359, North-
Holland, 1986.
[98] Reynolds, C.W.: “Flocks, herds and schools: A distributed behavioral model,”
Computer Graphics, vol.21, no.4, pp.25–34, 1987.
[99] Ridley, M., The Agile Gene: How Nature Turns on Nurture, Harper Perennial,
2004.
[100] Ridley, M., “The Evolution of Everything: How New Ideas Emerge,” Harper,
2015.
[101] Robbins, C., and Morrell, J., Mold, Housing and Wood, Western Wood Prod-
ucts Association, 2002.
[102] Rucker, R.: “Artiﬁcial Life Lab,” Waite Group Press, 1993.
[103] Sadhu, A.K., Rakshit, P., and Konar, A., “A modiﬁed Imperialist Competi-
tive Algorithm for multi-robot stick-carrying application,” Robotics and Au-
tonomous Systems, vol.76, pp.15–35, 2015.
[104] Sandel, M.: “Justice: What’s the Right Thing to Do?” Penguin, 2010.
[105] Schaffer, J.D., and Grefenstette, J.J., “Multi-Objective Learning via Genetic
Algorithms,” in Proc. of the 9th International Joint Conference on Artiﬁcial
Intelligence, pp.593-595, 1985.
[106] Schaffer, J.D., “Multiple Objective Optimization with Vector Evaluated Ge-
netic Algorithms,” in Proc. of an International Conference on Genetic Algo-
rithms and Their Applications, pp.93-100, 1985.
[107] Schelling, T.C.: “Dynamic models of segregation,” Journal of Mathematical
Sociology, vol.1., pp.143-186, 1971.
[108] Schwefel, H., Numerical optimization of computer models, John Wiley &
Sons, 1981.
[109] Segerstrale, U., Defenders of the Truth: The Sociobiology Debate, Oxford
Univ Press, 2001.

References
■
221
[110] Shayestegan, M., and Marhaban, M.H., “A Braitenberg Approach to Mo-
bile Robot Navigation in Unknown Environments,” in Ponnambalam, S.G.,
Parkkinen, J., Ramanathan, K.C. (eds.), Trends in Intelligent Robotics, Au-
tomation, and Manufacturing. IRAM 2012, Communications in Computer and
Information Science, vol 330. Springer, 2012.
[111] Smith, J.M., and Szathmary, E., “The Origins of Life: From the Birth of Life
to the Origin of Language,” Oxford University Press, 2000.
[112] S¨orensen, K., “Metaheuristics–the metaphor exposed,” International Trans-
actions in Operational Research, vol.22, no.1, pp.3–18, 2015.
[113] S¨orensen, K., Sevaux, M., and Glover, F., “A History of Metaheuristics,”
arXiv:1704.00853v1 [cs.AI] 4 Apr 2017, to appear in Mart,R., Pardalos,P.,
and Resende,M., Handbook of Heuristics, Springer.
[114] Stewart, I., The Mathematics of Life, Basic Books, 2013.
[115] Tamura, H., “A comparison of line thinning algorithms from digital geome-
try viewpoint,” in Proc. Int. Joint Conf. on Pattern Recognition, Kyoto, Japan,
pp.715–719, 1978.
[116] Tero, A., Takagi, S., Saigusa, T., Ito, K., Bebber, D.P., Fricker, M.D., Yu-
miki, K., Kobayashi, R., and Nakagaki, T., “Rules for Biologically Inspired
Adaptive Network Design,” Science, vol.327, no.5964, pp.439–442, 2010.
[117] Torabi, S., “Collective Transportation of Objects by a Swarm of Robots,”
Master Thesis in Complex Adaptative Systems, Chalmers University of Tech-
nology, 2015.
[118] Uchiyama, N., Mori, A., Kajita, Y., Sano, S., and Takagi, S., “Object-
transportation control for a human-operated robotic manipulator,” in Proc.
2008 IEEE International Conference on Emerging Technologies and Factory
Automation, pp.164–169, 2008.
[119] Vukov, J., Szab´o, G., and Szolnoki, A., “Evolutionary prisoner’s dilemma
game on Newman-Watts networks,” Phys. Rev.E, vol.77, pp.026109–, 2008.
[120] Wang, Y., and de Silva, C.W., “An object transportation system with mul-
tiple robots and machine learning,” in Proc. of the 2005 American Control
Conference, pp.1371–1376, 2005.
[121] Weyland, D., “A critical analysis of the harmony search algorithm? How not
to solve sudoku,” Operations Research Perspectives, vol.2, pp.97–105, 2015.
[122] Weyland, D., “A Rigorous Analysis of the Harmony Search Algorithm - How
the Research Community can be misled by a ”novel” Methodology,” Interna-
tional Journal of Applied Metaheuristic Computing, vol.1, no.2, pp.50–60,
2010.

222
■
References
[123] Wilson, E.O., The Insect Societies, Belknap Press of Harvard University
Press, 1971.
[124] Wilson, E.O., Sociobiology: The Abridged Edition Abridged Edition, Belk-
nap Press of Harvard University Press, 1980.
[125] Witten, T.A., and Sander, L.M., “Diffusion-limited aggregation, a kinetic
critical phenomenon,” Phys. Rev. Lett., vol.47, pp.1400-1403, 1981.
[126] Wolfram, S.: “A New Kind of Science,” Wolfram Media, 2002.
[127] Yamashita, A., Fukuchi, M., Ota, J., Arai, T., and Asama, H., “Motion plan-
ning for cooperative transportation of a large object by multiple mobile robots
in a 3D environment,” in Proc. 2000 ICRA Millennium Conference, IEEE In-
ternational Conference on Robotics and Automation, pp.3144–3151, 2000.
[128] Yang, X.-S., and Deb, S., 2009. “Cuckoo search via Levy ﬂights,” in Proc.
World Congress on Nature & Biologically Inspired Computing (NaBIC 2009),
pp.210–214, IEEE Publications, 2009.
[129] Yang, X., Nature-Inspired Metaheuristic Algorithms, 2nd ed., Luniver Press,
2010.

Index
(µ +λ)−ES, 23
(µ,λ)−ES, 23
1/f ﬂuctuation, 14
ε–greedy method, 31
1
5 rule, 22
110 rule, 13, 14
184 rule, 125
acceleration, 131
active oxidation, 101, 110
active reduction, 101, 110
adaptability, 93
adaptation process, 176, 178
adaptive control, 36
adder, 41
addition room, 4
adsorption, 139
affection hormone, 187
afﬁrmative action, 135
agent, 131
aggregation, 70
algorithm
Dijkstra’s algorithm, 126
estimation of distribution algo-
rithm, 17
evolutionary algorithm, 15
ﬁreﬂy algorithm, 85
O(n)-Algorithm, 101, 110
O(1)-Algorithm, 101
OBP algorithm, 200
Tamura’s algorithm, 110
UCT algorithm, 30, 123
Alice, 183
Alice’s Adventures in Wonderland,
197
Alife, see artiﬁcial life
altruistic behavior, 185
Analects of Confucius, 95
angular momentum, 70
anisotropy, 139
ant, 6, 49, 198, 203
ant colony optimization, 53, 54
anti-TFT, 168
Apple, 2
Apple II series, 38
arms races, 190, 197, 212
army knife, 188
arrival rate, 124
average arrival rate, 113
artiﬁcial bee colony, 79
artiﬁcial intelligence, 1, 115, 183
general artiﬁcial intelligence, 1
narrow artiﬁcial intelligence, 1
strong artiﬁcial intelligence, 1,
118
weak artiﬁcial intelligence, 1

224 
■ Index
artiﬁcial life, 1, 8, 43, 56, 183, 198
artiﬁcial neural network, 197
artiﬁcial selection, 211
asymmetric simple exclusion pro-
cess, 129
attachment, 141
attenuation coefﬁcient, 60
attraction, 67
attractiveness, 84
average, 30, 113, 114, 117
avoiding, 74
bacteria, 190
bacteria colony simulator, 191
bandwagon effect, 36
Bayes’ Law, 119
Beagle, 211
bee
bee colony, 78, 198
bee search, 80
employed bee, 76, 81
onlooker bee, 76, 81
scout bee, 76, 81
behavior
altruistic behavior, 185
behavior-based robot, 198
collective behavior, 6, 49, 56,
68, 198
cooperative behavior, 179, 187–
189
ecological behavior, 194
human behavior, 188
morphological behavior, 194
oscillatory behavior, 178
reciprocal behavior, 174
social behavior, 56, 188
behavioral economics, 119
Belousov-Zhabotinsky reaction, 97,
153
benchmark, 54
Bernoulli trial, 113
betrayal, 172, 183
big bang, 179, 182
binarity, 112
biodiversity, 6
bisector, 99, 101, 110
blind spot, 67, 68
blowﬁsh, 98
Bob, 183
Boid, 66, 74
Braitenberg obstacle avoidance method,
205
branch, 148, 151
Brownian motion, 138
Burgers cellular automata, 125
BZ reaction, 153
caste, 6
castle, 46
cat swarm optimization, 91
cell, 44, 140
photoreceptor cell, 158
cellular automata, 7, 44, 56
Burgers cellular automata, 125
one-dimensional cellular automata,
9, 127, 150
reaction-diffusion cellular au-
tomata, 97
chaotic patterns, 13
chasing mode, 92
Chinese room, 3
chromosome, 91
class IV, 13, 151
classical economics, 37
Cleaner Wrasse, 161
click model, 33
clock, 39
cluster, 192
co-evolution, 161, 190, 197, 198, 212
coding
direct coding, 197
cognitive
cognitive dissonance, 189
cognitive error, 115
cognitive science, 119
collective behavior, 6, 49, 56, 68, 198

Index 
■ 225
collective memory, 66
collision, 58
colony, 81, 191
color blind, 135
compassion, 187
complementary set, 141
complex system, 4, 36
computational
computational complexity, 5
computational geometry, 98
condensation, 146
conductor, 39
confession, 166
confetti candy, 137
conﬂict resolution, 121
Confucianism, 95
contour, 97
contrast, 97
convergence coefﬁcient, 60
cooperation, 6, 83, 166, 167, 171,
177, 178, 183
cooperative behavior, 179, 187–189
coordinate
Cartesian coordinate, 140
oblique coordinate, 140
coral, 98, 99, 109
counter, 120
critical value, 128, 133, 135, 136
crossover, 15, 19, 22
crossover point, 19
multi-point crossover, 19
n-point crossover, 19
one-point crossover, 19
two-point crossover, 19
uniform crossover, 19
crystal, 138
stellar crystal, 144
cuckoo search, 86
customer, 112, 162
cycle, 39
death spiral, 52
decision-making process, 187
defection, 166, 167, 178
delay time, 131
diamond market, 174
dice, 112
dictator game, 186
diffusion, 141, 146, 149
diffusion rate, 190
diffusion-limited aggregation, 137
Disney World, 125
Disneyland, 124
dispatching rule, 121
distribution
Bernoulli distribution, 33
exponential distribution, 114
Gaussian distribution, 22, 32,
62, 68
initial distribution, 21
normal distribution, 32, 114
service time distribution, 120
diversity, 96
divide and conquer, 100
dormant, 101, 110
dorsal prefrontal cortex, 187
earliest due date, 121
eater, 9
ecological behavior, 194
economic man, 187
economics
behavioral economics, 119
classical economics, 37
evolutionary economics, 36, 172,
183
modern economics, 187
edge, 97
edge of chaos, 14
egocentric person, 187
electron
electron head, 39
electron tail, 39
emergence, 6, 211
strong emergence, 7
weak emergence, 7

226 
■ Index
emergent property, 4, 6, 56
Emperor Angelﬁsh, 151
employed bee, 76, 81
empty, 39
energy
energy metabolism, 153
Enigma, 2
equilibrium, 36
escape behavior, 71
estimation of distribution algorithm,
17
Euclidean
Euclidean distance, 85
Euclidean space, 98
evaporation, 147
evasive maneuver, 71, 76
evolution
Darwinian evolution, 16
evolution strategy, 22, 95
general theory of evolution, 212
special theory of evolution, 212
evolutionarily stable strategy, 173
evolutionary
evolutionary algorithm, 15
evolutionary ANN, 197
evolutionary computation, 15,
64
evolutionary economics, 36, 172,
183
evolutionary linguist, 188
evolutionary programming, 55
evolutionary psychology, 188
evolutionary robotics, 197
evolvable hardware, 198
excitement, 101, 110
expanded phenotype, 185
exploitation, 30, 65, 89
exploration, 30, 65, 81, 89, 205
eye for eye, 168
Fast Pass, 124
fetus, 157
ﬁeld of perception, 69
ﬁnancial market, 6, 36
ﬁnch, 211
ﬁreﬂy, 84, 118
ﬁrst-in, ﬁrst-out, 119, 121
ﬁtness, 10, 93, 197
ﬂash expansion, 73
ﬂexibility, 198
Flick Card, 124
ﬂicker, 8
ﬂip-ﬂop, 41
folding relationship, 108
foraging, 83
fountain effect, 73
freezing, 141, 145
function
Boolean function, 10
Griewangk’s function, 63
objective function, 89
Rastrigin’s function, 63
fungi, 193
GA operator, 18, 64
Galapagos
Galapagos iguana, 212
Galapagos Islands, 211
Galapagos of the East, 98
Gambler’s fallacy, 119
game, 30
dictator game, 186
game of life, 8, 39
game theory, 163, 169
quantum game, 184
spatial game, 179
ultimatum game, 185
gas, 140
Gaussian
Gaussian distribution, 22, 62, 68
Gaussian ﬂight, 88
Gaussian mutation, 62
Gaussian noise, 177
gene, 18, 19, 177, 188
selﬁsh gene, 185
general AI, 1

Index 
■ 227
general-purpose learning machine,
188
generation, 15, 18, 23, 28
alternation of generation, 21
generator, 98, 100
genetic algorithms, 10, 16, 22, 36,
55, 64, 91
genetic operation, 16, 21
genetic programming, 16
genotype, 16
giraffe, 159
glider gun, 9
goal, 199, 200, 204
Google, 5
gradient-type, 188
greedy method, 31
group
group formation, 6
group polarization, 70
group selection, 185
guide
guide mode, 204
guide-based OBP, 203
guided pushing, 205
harmony
harmony memory, 90
harmony memory considering
rate, 90
harmony search, 89, 95
hexagon, 108, 138
highway, 44
Hilditch’s thinning, 109
history, 175
home, 204
home position, 37
Homo Economicus, 187
honeycomb, 140
host, 87
human
human behavior, 188
human cognition, 188
human intelligence, 7
human mind, 188
humidity, 147, 191
hunting, 74
hurricane, 116
hybrid iguana, 212
hydrodynamics, 194
hydrogen bond, 138
Ig Nobel Prize, 194
iguana
Galapagos iguana, 212
hybrid iguana, 212
land iguana, 212
marine iguana, 212
imitation, 179
increasing returns, 36
independence, 112
instincts, 74
Invisible hand of God, 37
iterated prisoner’s dilemma, 166
jaguar, 159
Japanese railroad, 195
jazz session, 89
job
job assignment, 121
job scheduling problem, 123
kaleidoscope, 179, 182
keyboard
DSK keyboard, 37
QWERTY keyboard, 37
Khepera III, 200
kindness, 190
L´evy ﬂight, 88
land iguana, 212
Langton’s ant, 43
language instinct, 188
lattice, 159, 180
Mesoscopic lattice, 139
law of small numbers, 119
learning
cooperative learning, 198

228 
■ Index
machine learning, 118
multi-agent learning, 198
Lee-Kernighan method, 55
leopard, 157
liar, 190
life cycle, 193, 194
lightning bug, 118
lightning strike, 115
liquid, 140
local
local interaction, 67
local maximum, 82
local optimum, 18, 21, 62
local search, 30, 81
lock-in, 37
Loebner prize, 2
logic circuit, 39
logical reasoning, 183
London bombing, 117
Los Alamos National Laboratory, 43
lottery ticket, 119
love potion, 187
machine translation, 5
mackerel tuna, 72
macroscopic, 194
macroscopic phenomena, 6, 135
Magic Kingdom, 125
majority
majority problem, 9
majority rule, 9
Manhattan project, 7
marine iguana, 212
Markov process, 175
mask, 19
Mathematica, 12
maze route search, 195
maze search simulator, 196
melting, 141, 142
meme, 185
memory, 175
collective memory, 66
memorylessness, 112, 114, 119
mental mechanisms, 188
Mesoscopic lattice, 139
metaheuristics, 55
metal tree, 137
metanorm, 173
metaphor, 94
method
ε–greedy method, 31
Braitenberg obstacle avoidance
method, 205
greedy method, 31
Lee-Kernighan method, 55
OBP method, 199
recursive bisection method, 100
revised UCB1 method, 31
successive addition method, 100
UCB1 method, 31
microscopic
microscopic phenomena, 6
microscopic preference, 132
mirror image, 106
miscopy, 177
mixed strategy, 176
mixing ratio, 92
mode
chasing mode, 92
guide mode, 204
push mode, 204
seeking mode, 92
modern economics, 187
module, 188
mold, 191
mold growth simulator, 192
red bread mold, 191
slime mold, 193
Monte Carlo tree search, 30, 123
morphogenesis, 149
morphological behavior, 194
multi-dimensional space, 64
multi-modal problem, 62
multimodality, 86
Murray’s theory, 155
music player, 119

Index 
■ 229
musician, 89
mutation, 15, 18, 19, 22
mystery circle, 98
Nakaya diagram, 138
narrow AI, 1
Nash equilibrium, 173, 174, 183, 189
Natural History, 118
natural language understanding, 5
natural law, 183
natural selection, 96
nearest-neighbor, 98
neighborhood, 56, 66, 103, 176, 179
Moore neighborhood, 8, 104,
132, 154, 180
random neighborhood, 179
neighboring cell, 101, 103, 141
network
ﬁxed network, 178
heterogeneous network, 179
homogeneous network, 179
network structure, 184
path network, 194
neurobiological mechanism, 188
neuroscientist, 7
Neurospora crassa, 191
nice, 169
Nobel Prize, 119, 132, 165
Ig Nobel Prize, 194
noise
Gaussian noise, 177
nonlinear system, 36
norm, 105, 172
L2 norm, 105
L∞norm, 105
L1 norm, 105
metanorm, 173
NP-complete, 123, 125
nutrient, 191
nutrition, 191
nutrition rate, 192
occlusion-based pushing, 199
offspring, 19
On the origin of species, 211
online advertisement, 33
onlooker bee, 76, 81
optimal
optimal distance, 56, 58
optimal lever, 32
optimization, 79, 125
cat swarm optimization, 91
multi-objective, 24
particle swarm optimization, 56,
64
orientation, 66
oscillatory behavior, 178
oxidation, 153
oxygen, 191
oxygen molecule, 138
oxytocin, 187
P-complete, 14
paciﬁst, 135
parallel group, 71
dynamic parallel group, 70
highly parallel group, 70, 71
parameter
positional parameter, 88
scale parameter, 88
parasite, 185
parent, 19
pareto
pareto efﬁcient, 183
pareto front, 26
pareto optimality, 24, 28
Parrotﬁsh, 161
particle, 12, 60, 64
particle swarm optimization, 56, 64
parting shot, 167
path
path dependency, 36
path network, 194
path planning, 98
pattern
pattern classiﬁcation, 98
pattern generation, 6

230 
■ Index
steady pattern, 154
Pavlov strategy, 167, 171
payoff, 30, 189
expected payoff, 175
payoff matrix, 174, 189
payoff table, 183
penalty, 172
periodical patterns, 13
phenotype, 16
pheromone, 49, 52, 54, 55
pheromone trail, 49, 51, 53, 203
Physarum polycephalum, 194
pitch adjust rate, 90
plasmodium, 193
play-out, 30
Poisson
Poisson arrival, 121, 122
Poisson distribution, 113, 117,
121
Poisson process, 114, 115
politeness, 169
positional parameter, 88
preadaptation, 96
precipitation, 101, 110
predator, 74, 197
preference, 132, 135
microscopic preference, 132
prejudice, 135
premature convergence, 18
prey, 74, 197
priority rule, 121
prisoner’s dilemma, 166
problem
majority problem, 9
multi-modal problem, 62
traveling salesman problem, 50,
125
two-armed bandit problem, 30
processing time, 122
prospect theory, 119
PSO
PSO algorithm, 60
PSO with mutation, 64
PSO workﬂow, 60
psychological bias, 119
psychological experiment, 183
punctuated equilibrium, 96, 118, 189
punish, 162, 171, 172
pure strategy, 176
push mode, 204
quantum
quantum brain theory, 184
quantum cognition, 184
quantum entanglement, 183
quantum game, 184
quantum state vector, 184
queue, 119
queue simulator, 121
queuing theory, 112, 125
racial discrimination, 135
radius, 9, 106
ramp metering, 123
random customer, 112
randomness, 112, 118
reaction-diffusion
reaction-diffusion cellular au-
tomata, 97
reaction-diffusion computing, 97
reaction-diffusion equation, 149
reactive-diffusion wave, 159
reciprocal behavior, 174
recombination, 18
recursive bisection method, 100
Red Queen effect, 197
reduction, 153
reductionist approach, 7
refractory, 101, 110
religion, 190
representation
binary representation, 179
continuous representation, 180
real-valued representation, 22
representative heuristics, 119
reproduction, 18
repulsion, 66

Index 
■ 231
retina, 158
return, 24
high return, 24, 35
increasing returns, 36
low return, 24
revised UCB1 method, 31
reward, 31
rhythm, 153
risk, 24
high risk, 24, 35
low risk, 35
robot
behavior-based robot, 198
Khepera robot, 197
swarm robot, 197
robotics, 49
evolutionary robotics, 197
swarm robotics, 198
robustness, 198
rotation angle, 68, 76
roulette selection, 93
rule
1
5 rule, 22
110 rule, 13, 14
184 rule, 125
dispatching rule, 121
majority rule, 9
priority rule, 121
update rule, 101
sardines, 72, 73
scalability, 198
scale parameter, 88
scarcity, 112
scheduling theory, 121
schema, 36
schema theorem, 36
school of ﬁsh, 56
scout bee, 76, 81
search, 16
bee search, 80
cuckoo search, 86
harmony search, 89
local search, 81
maze route search, 195
Monte Carlo tree search, 30,
123
search efﬁciency, 62
search region, 60
seed, 98, 104, 109, 138
seeking mode, 92
segregation, 135
segregation model, 132
selection, 15, 16, 93
artiﬁcial selection, 211
roulette selection, 16, 81
selection operator, 16
selection pressure, 28
tournament selection, 16
truncate selection, 17
self-deception, 189
self-interest, 187
self-organizing system, 9
self-reproducing, 7
self-similarity, 139
selﬁsh gene, 185
sensor noise, 177
service, 120
shell, 150
shortest processing time, 121
signal, 126
silicon trafﬁc, 126
simulated annealing, 55
simulator
bacteria colony simulator, 191
maze search simulator, 196
mold growth simulator, 192
snowﬂake simulator, 143, 145
Voronoi simulator, 106
site, 98
skeletonization, 109
slime
many-headed slime, 194
slime intelligence, 190, 195
slime mold, 193
snowﬂake, 138

232 
■ Index
sociability label, 176
social
social behavior, 56, 188
social psychology, 169
sociobiology, 6, 188
solid, 140
solidarity, 70
spatial game, 179
speciation, 96
species, 161
spider strategy, 198
sporangia, 191
spore, 191
Spotify, 119
standard deviation, 22, 69, 89, 114,
177
star, 118, 143
state vector, 184
quantum state vector, 184
stationarity, 112
statistical decision theory, 36
steady pattern, 154
steady state, 175
stochastic
stochastic reasoning, 118
stochastic strategy, 176
stochastic variable, 114
stock prices, 37
strategy
evolution strategy, 22, 95
evolutionarily stable strategy,
173
mixed strategy, 176
Pavlov strategy, 167
pure strategy, 176
spider strategy, 198
stochastic
stochastic strategy, 176
zero-determinant strategy, 175
strong AI, 1, 3, 118
strong emergence, 7
student
average student, 122
excellent student, 122
negligent student, 122
subjective survey, 124
successive addition method, 100
supernormal stimulus, 87
swarm robotics, 198
swarming, 68, 70
symbiosis, 161
symmetry, 145
hexagonal symmetry, 139
system
autonomous system, 199
centralized system, 198
complex system, 36
decentralized system, 198, 200
nonlinear system, 36
self-organizing system, 9
system theory, 4
tag, 176
Tamura’s thinning, 110
task, 121
temperature, 191
territory, 98
theorem
Body and tail theorem, 158
Snake’s theorem, 157
theory
game theory, 163, 169
Murray’s theory, 155
prospect theory, 119
quantum brain theory, 184
queuing theory, 125
scheduling theory, 121
statistical decision theory, 36
thieves, 162
thinning, 109
Hilditch’s thinning, 109
Tamura’s thinning, 110
tiger, 159
tit for tat, 168
tolerance, 169
toroidal structure, 180

Index 
■ 233
torso, 158
torus, 68, 70, 71
touringplans.com, 125
tournament, 169
tournament size, 16
trafﬁc
trafﬁc congestion, 123, 126, 127
trafﬁc jams, 112
trafﬁc signal, 123
transition, 101, 103
transportation, 199
traveling salesman problem, 50, 125
tribes, 133
trophozoite, 193, 194
Turing
Turing complete, 39
Turing machine, 2, 9
Turing model, 2, 149
Turing test, 2
two-armed bandit, 29
two-armed bandit problem, 30
two-lever slot machine, 29, 36
typewriter, 37
U-turn, 129
UCB value, 30
UCB1 method, 31
UCT algorithm, 30, 123
ultimate defense measure, 171
ultimatum game, 185
ultra-rational, 187
uniform random, 89
update rule, 101
upper conﬁdence bound, 30
V2 rocket, 116
vampire bats, 166
variance, 30, 113
vector
location vector, 60
position vector, 67
quantum state vector, 184
state vector, 184
velocity vector, 67
velocity, 93, 131
velocity vector, 60
vengefulness, 172
virus, 190
Voronoi
Voronoi diagram, 98, 103
Voronoi polygon, 98, 100
vortex, 155
waggle dance, 76
wait, 112
wait time, 125
expected wait time, 124
walker, 181
water vapor, 138, 140, 144, 147, 148
wave, 151
weak AI, 1, 3
weak emergence, 7
what and why story, 189
white ﬂight, 136
Win-Stay-Lose-Shift, 167, 171
window size, 120
winning horse effect, 36
wireworld, 38
witness, 172
Wolfram Research, 12
World War II, 2, 116
zebra, 156
zebraﬁsh, 158
zero-determinant strategy, 175
zone
zone of attraction, 67, 69
zone of orientation, 66, 69
zone of repulsion, 66, 69, 75


Chapter 2: Fig. 2.19, p. 41
Color Section 
L1 norm
L2 norm
L∞norm
Chapter 4: Fig. 4.14, p. 108

236 < AI and Swarm: Evolutionary Approach to Emergent Intelligence
Chapter 4: Fig. 4.30, p. 129
Chapter 4: Fig. 4.29, p. 129
Chapter 4: Fig. 4.37, p. 134

Chapter 5: Fig. 5.18, p. 155
Chapter 6: Fig. 6.5, p. 182
Color Section < 237 

238 < AI and Swarm: Evolutionary Approach to Emergent Intelligence
Guide
t = 0
t = 4:40
t = 5:03
t = 7:27
t = 8:25
Horizontal
Path(⇐)
t = 10:00
t = 11:00
t = 12:00
t = 13:00
t = 14:00
Vertical
Path (⇑)
t = 15:00
t = 16:00
t = 17:00
t = 18:00
t = 19:20
Chapter 6: Fig. 6.29, p. 207


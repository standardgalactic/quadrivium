Lecture Notes in Computer Science
6186
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Alfred Kobsa
University of California, Irvine, CA, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Germany
Madhu Sudan
Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbruecken, Germany
www.ebook3000.com

Alessandro Armando Gavin Lowe (Eds.)
Automated Reasoning for
Security Protocol Analysis and
Issues in the Theory of Security
Joint Workshop, ARSPA-WITS 2010
Paphos, Cyprus, March 27-28, 2010
Revised Selected Papers
1 3

Volume Editors
Alessandro Armando
Università di Genova
DIST Dipartimento di Informatica, Sistematica e Telematica
Viale Causa 13, 16145 Genova, Italy
E-mail: armando@dist.unige.it
Gavin Lowe
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford, OX1 3QD, UK
E-mail: gavin.lowe@comlab.ox.ac.uk
Library of Congress Control Number: 2010936301
CR Subject Classiﬁcation (1998): D.4.6, K.6.5, C.2, E.3, D.2, D.3
LNCS Sublibrary: SL 4 – Security and Cryptology
ISSN
0302-9743
ISBN-10
3-642-16073-5 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-16073-8 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2010
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
06/3180
www.ebook3000.com

Preface
This volume contains the workshop proceedings of ARSPA-WITS 2010, the Joint
Workshop on Automated Reasoning for Security Protocol Analysis and Issues
in the Theory of Security, held during March 27–28, 2010 in Paphos (Cyprus).
ARSPA-WITS 2010 oﬀered a forum for discussing new results in theories of com-
puter security, open questions and fundamental concerns about existing theories,
and issues related to the development and application of automated reason-
ing techniques and tools for the formal speciﬁcation and analysis of security
protocols.
There were 28 submissions. All the submissions were thoroughly evaluated
on the basis of at least three referee reports, and an electronic Program Com-
mittee meeting was held by using the EasyChair on-line conference system. The
committee decided to accept the 11 papers included in this volume. The authors
were given the opportunity to revise their paper in light of the feedback received
during the workshop. The workshop program was enriched by two invited talks
by Flemming Nielson and Catuscia Palamidessi, whose titles and abstracts are
also included in this volume.
April 2010
Alessandro Armando
Gavin Lowe

Conference Organization
Program Chairs
Alessandro Armando
Università di Genova, Italy (Co-chair)
Gavin Lowe
Oxford University, UK (Co-chair)
Program Committee
Lujo Bauer
CMU, USA
Yannick Chevalier
Université Toulouse III, France
Luca Compagna
SAP Research, France
Cas Cremers
ETHZ, Switzerland
Jorge Cuellar
Siemens, Germany
Pierpaolo Degano
Università di Pisa, Italy
Sandro Etalle
Technical University of Eindhoven and
University of Twente, The Netherlands
Riccardo Focardi
Università di Venezia, Italy
Dieter Gollman
Technische Universität Hamburg-Harburg,
Germany
Joshua Guttman
MITRE, USA
Jan Jürjens
TU Dortmund and Fraunhofer ISST, Germany
Gavin Lowe
Oxford University, UK (Co-chair)
Catherine Meadows
Naval Research Laboratory, USA
John Mitchell
Stanford University, USA
Sebastian Mödersheim
IBM Zurich Research Lab, Switzerland
Michael Rusinowitch
INRIA-Lorraine, France
Mark Ryan
University of Birmingham, UK
Graham Steel
INRIA, France
Luca Viganò
Università di Verona, Italy
Bogdan Warinschi
University of Bristol, UK
External Reviewers
Aizatulin, Mihhail
Arapinis, Myrto
Bartoletti, Massimo
Bauer, Andreas
Bodei, Chiara
Bugliesi, Michele
Bursuc, Sergiu
Centenaro, Matteo
Colon, Michael
Delaune, Stephanie
Dupressoir, Francois
Ferrari, Gian-Luigi
Hevia, Alejandro
Hirsch, Martin
www.ebook3000.com

VIII
Conference Organization
Koleini, Masoud
Meier, Simon
Pellegrino, Giancarlo
Perreira, Olivier
Pontes Soares Rocha, Bruno
Portokalidis, Georgios
Qunoo, Hasan
Schoenmakers, Berry
Sgandurra, Daniele

Table of Contents
The CaPiTo Approach to Protocol Validation (Invited Talk) . . . . . . . . . .
1
Flemming Nielson, Han Gao, and Hanne Riis Nielson
Reasoning about Probabilistic Security Using Task-PIOAs . . . . . . . . . . . .
2
Aaron D. Jaggard, Catherine Meadows, Michael Mislove, and
Roberto Segala
Secrecy and Authenticity Types for Secure Distributed Messaging . . . . . .
23
Michele Bugliesi, Stefano Calzavara, and Damiano Macedonio
Modular Plans for Secure Service Composition. . . . . . . . . . . . . . . . . . . . . . .
41
Gabriele Costa, Pierpaolo Degano, and Fabio Martinelli
A Type System for Access Control Views in Object-Oriented
Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
M´ario Pires and Lu´ıs Caires
Formal Analysis of Key Integrity in PKCS#11. . . . . . . . . . . . . . . . . . . . . . .
77
Andrea Falcone and Riccardo Focardi
Secure Upgrade of Hardware Security Modules in Bank Networks . . . . . .
95
Riccardo Focardi and Flaminia L. Luccio
Interactive Information Flow (Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . .
111
Catuscia Palamidessi, M´ario S. Alvim, and Miguel E. Andr´es
Portunes: Representing Attack Scenarios Spanning through the
Physical, Digital and Social Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Trajce Dimkov, Wolter Pieters, and Pieter Hartel
Match It or Die: Proving Integrity by Equality. . . . . . . . . . . . . . . . . . . . . . .
130
Matteo Centenaro and Riccardo Focardi
Towards Automatic Analysis of Election Veriﬁability Properties . . . . . . . .
146
Ben Smyth, Mark Ryan, Steve Kremer, and Mounira Kourjieh
AnBx - Security Protocols Design and Veriﬁcation . . . . . . . . . . . . . . . . . . .
164
Michele Bugliesi and Paolo Modesti
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
www.ebook3000.com

The CaPiTo Approach to Protocol Validation
(Invited Talk)
Flemming Nielson, Han Gao, and Hanne Riis Nielson
DTU Informatics
The Technical University of Denmark
Abstract. We show how to model service-oriented applications using
the process algebra CaPiTo so that, on the one hand, we can achieve
an abstract speciﬁcation without being overwhelmed by the underlying
implementation details and, on the other hand, we can obtain a con-
crete speciﬁcation respecting the industrial standards used for ensuring
security. We consider this development important in order to get a good
agreement between the protocols analysed by formal tools and the ap-
plications developed by practitioners.
We then show how to transform the concrete speciﬁcation into the
LySa analysis framework, used in the SENSORIA EU project and origi-
nally developed in the DEGAS EU project, for analysing cryptographic
protocols under a Dolev-Yao attacker. This allows us to perform a con-
trol ﬂow analysis for ensuring the authenticity (as well as conﬁdentiality)
of messages exchanged between services. The LySa analysis framework is
implemented in polynomial time in the size of the protocol speciﬁcation
using the Succinct Solver, that can solve a superset of Datalog clauses.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, p. 1, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Reasoning about Probabilistic Security
Using Task-PIOAs⋆
Aaron D. Jaggard1,⋆⋆, Catherine Meadows2,⋆⋆⋆,
Michael Mislove3,⋆⋆⋆, and Roberto Segala4,†
1 Rutgers University, New Brunswick, NJ
adj@dimacs.rutgers.edu
2 Naval Research Lab, Washington, DC
catherine.meadows@nrl.navy.mil
3 Tulane University, New Orleans, LA
mwm@math.tulane.edu
4 University of Verona, Italy
roberto.segala@univr.it
Abstract. Task-structured probabilistic input/output automata (Task-
PIOAs) are concurrent probabilistic automata that, among other things,
have been used to provide a formal framework for the universal compos-
ability paradigms of protocol security. One of their advantages is that
that they allow one to distinguish high-level nondeterminism that can
aﬀect the outcome of the protocol, from low-level choices, which can’t.
We present an alternative approach to analyzing the structure of Task-
PIOAs that relies on ordered sets. We focus on two of the components
that are required to deﬁne and apply Task-PIOAs: discrete probabil-
ity theory and automata theory. We believe our development gives in-
sight into the structure of Task-PIOAs and how they can be utilized
to model crypto-protocols. We illustrate our approach with an example
from anonymity, an area that has not previously been addressed using
Task-PIOAs. We model Chaum’s Dining Cryptographers Protocol at a
level that does not require cryptographic primitives in the analysis. We
show via this example how our approach can leverage a proof of security
in the case a principal behaves deterministically to prove security when
that principal behaves probabilistically.
1
Introduction
Process algebras and other models of concurrent computation have become a sta-
ple for reasoning about security (cf. [1, 20], to name two examples). Originally,
⋆The authors wish to thank the Center for Discrete Mathematics and Theoretical
Computer Science and the DIMACS Special Focus on Communication Security
and Information Privacy for their support that allowed us to gather at Rutgers to
work on this project for a week in May/June, 2008.
⋆⋆The author wishes to acknowledge the support of NSF grant 0751674 and of the
ONR during this research.
⋆⋆⋆These authors wish to acknowledge the support of the ONR during this research.
† The author acknowledges the support of the MIUR PRIN project “SOFT” and
FP7 grant INFSO-ICT-223844.L.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 2–22, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
3
this work concentrated on the “Dolev-Yao” model in which cryptographic algo-
rithms were modeled as black boxes. However, there is a growing body of research
aimed at marrying concurrency theory with cryptographic reasoning. Prominent
among these approaches is the work of Canetti, Lynch, Segala, et al. [6], which
uses Task-PIOAs to provide a formalization of Canetti’s Universal Composabil-
ity model [5]. Task-PIOAs are probabilistic input/output automata equipped with
tasks, which deﬁne their execution sequences. The approach is highlighted by two
aspects: First, this approach encodes the basic constituents of cryptography – one-
way functions, hardcore predicates, etc., into the model and reasons about them
in proving security properties.
The second distinguishing aspect of [6] is that it divides the nondeterminism
in a concurrent system into high-level choices that can aﬀect the outcome of
the protocol, and low-level nondeterminism that does not aﬀect the outcome.
By allowing the adversary to see the outcomes only of the low-level choices,
the model overcomes the problem that concurrency models generally aﬀord the
adversary too much power by allowing it to see the outcomes of all the choices
made during the execution of a protocol.
Because of all this, the Task-PIOA work is often cited as a promising approach
to modeling crypto-protocols. At the same time, the Task-PIOA analysis of obliv-
ious transfer [6], for example, is daunting. Indeed, there are three components
that must be mastered to appreciate the subtleties of Task-PIOAs: the theory
of discrete probability that is applied to Task-PIOAs, the theory of probabilistic
automata that underlies Task-PIOAs, and the incorporation of cryptographic
primitives that requires detailed and often-tedious reasoning. We note that this
rather formidable array of components is inherent in all approaches to marrying
formal methods with cryptographic reasoning, and is not speciﬁc to Task-PIOAs;
all approaches to marrying these aspects suﬀer from the same complexity. Our
approach is to separate the cryptographic primitives in the model from the other
aspects, and to treat these other facets ﬁrst, reserving a similar analysis of the
cryptographic components for future work.
Thus, the purpose of the current paper is to present the ﬁrst two components
– discrete probability theory applied to Task-PIOAs, and the theory of proba-
bilistic automata needed to analyze Task-PIOAs – from a diﬀerent perspective,
one we hope will make the technology more easily digested. Our approach relies
on ordered sets as an underlying theme for both discrete probability and for the
automata theory needed to describe Task-PIOAs.
The universal composability approach to proving security employs the notion
of a simulator as part of a mechanism to compare “real-world” processes with
“ideal processes” that are secure by design. Task-PIOAs use simulation relations
common to cryptographic proofs to carry out such comparisons. These relations
validate security properties by showing the trace distributions on visible events
of the real-world process are contained in those of the simulating process. As a
result, an unbiased observer cannot tell if it is interacting with the real-world
process or with the simulator, and the security of the real-world process follows.

4
A.D. Jaggard et al.
We illustrate our results with an example in which cryptographic primitives
don’t play a role. This allows us to demonstrate the techniques developed in
this paper to model a security protocol, avoiding cryptographic details that
are independent of our approach to combining discrete probability theory with
the theory of probabilistic automata. The example comes from anonymity: it
is the canonical example of Chaum’s Dining Cryptographers Protocol [9]. Our
approach also has the advantage that it explicitly models a method by which the
choice of who pays is made, a detail left unspeciﬁed in Chaum’s original paper,
and one that is also left unspeciﬁed in other approaches such as [4].
Task-PIOAs use task schedules to resolve nondeterminism, and a Task-PIOA
simulation relation typically shows that each task schedule for a real-world pro-
cess has a corresponding schedule for its simulating process. In this paper we
introduce a somewhat simpler notion of simulation we call Task-PIOA maps.
The main diﬀerence is that, while Task-PIOA simulations utilize maps between
task schedules, Task-PIOA maps map states to states, actions to actions, and
tasks to tasks. This makes deﬁning a Task-PIOA map somewhat simpler, and
their use is well-suited for indistinguishability proofs in which the only diﬀer-
ence between the two processes is the choice of a secret. This is the case for
our proof of anonymity of Dining Cryptographers. Anticipating some details,
the protocol is secure if the probability distributions on the announcements of
the cryptographers doesn’t vary depending on which one is paying. Our anal-
ysis shows that the distributions in the case of a probabilistic choice of who
pays can be calculated compositionally from the distributions that arise from
its deterministic subprocesses. Our argument relies on Chaum’s original use of
conditional probability [9] to prove anonymity, an approach also applied in [4,7].
The rest of the paper is organized as follows. In the next section, we give
a comparison with other work. The following sections give some results about
discrete probability theory based on order-theoretic arguments, and the basic
deﬁnitions and results used by Task-PIOAs. Following that, we give the main
results of the paper which deﬁne Task-PIOA maps and show they preserve trace
distributions of task-based probabilistic input/output automata. This is followed
by a brief description of the Dining Cryptographers Protocol and how to encode
it using process algebra. We next encode the Dining Cryptographers Protocol
using a task-based probabilistic input/output automata and then show that the
protocol is secure if the choice of who pays is either deterministic or probabilistic.
In the ﬁnal section we summarize our results and discuss possible future work.
2
Comparison with Other Work
As we already commented, the use of process algebras and of models of concur-
rency is now commonplace in analyzing crypto-protocols, as well as other areas of
security. What is novel about the Task-PIOA approach as presented in [6] is the
level of detail at which processes are treated. Most security models operate under
the traditional Dolev-Yao assumption and assume perfect encryption. However,
these assumptions have been shown to be unrealistic in some contexts [17], which
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
5
is one of the impetuses for the universal composability approach [5] to modeling
security. This need for more realism has been accompanied by a myriad of de-
tails that make analyses much more intricate. In particular, in what is probably
the most closely-related work to the Task-PIOA approach, [17, 20], Probabilis-
tic Polynomial-time Turing Machines are used as the basis for a model. This
approach also involves myriad details about computations, details we seek to
abstract away in our approach to understanding parts of the PIOA approach.
Part of the problem with these more realistic models is the level of detail, but
an added problem with [6] is the presentation of the model. Indeed, the Task-
PIOA approach is composed of two parts: (i) the presentation of task-structured
probabilistic input/output automata (Task-PIOAs), and (ii) the application of
these automata to model cryptographic primitives in the analysis of the oblivious
transfer protocol. Here, we give an alternative presentation of Task-PIOAs, us-
ing techniques from order theory. While our approach requires familiarity with
ordered sets, the results needed are not especially deep, and the ordered sets
approach carries with it computational intuitions that are not so clear in [6].
The Dining Cryptographers Protocol originated in the work of Chaum [9],
and it quickly became a prototypical test example for modeling anonymity. For
example, Schneider and Sidiropoulos [21] use CSP to analyze the protocol, after
ﬁrst replacing probabilistic choice by nondeterministic choice. While CSP is lim-
ited to nondeterministic choice, there are models of concurrency that support
both nondeterminism and probabilistic choice, and these require a mechanism to
account for how the nondeterminism in the system is resolved. The most com-
monly used approach is to include a scheduler that resolves the nondeterministic
choices. In security analyses, the scheduler is often seen as an adversary that can
control the occurrence of events in order to maximize the chances of compromis-
ing the security of the system. Garcia et al. [11] introduce admissible schedulers
that limit the power of the adversary so that it is “realistic” and cannot see the
outcomes of choices that should be hidden from it.
Probably the most exhaustive treatment of the Dining Cryptographers using
concurrency appears in the papers [4,7,8]. In [4], an analysis shows the limitations
of substituting nondeterminism for probabilistic choice, as was done in [21], and
analyses are presented of the Dining Cryptographers with users that are nonde-
terministic or probabilistic in their choices, under some added hypotheses. In [7],
the pi-calculus is augmented with labels guarding choices, so that the scheduler
can enable a choice, but not control or see its outcome. In [8], a new notion of
conditional capacity of a channel is introduced, and this is used to analyse the
degree of anonymity of a system in which some information is leaked on purpose.
Finally, [14] applies simulation relations based on coalgebras to analyze the Din-
ing Cryptographers. As is typical of other treatments, the presentation is at an
abstract level, concentrating solely on the case in which the choice of who pays is
deterministic, and in which the implementation of the choice is left unspeciﬁed.
Moreover, this model does not support nondeterminism, and presents everything
in a purely probabilistic framework.

6
A.D. Jaggard et al.
There is a growing body of literature on modeling probabilistic processes,
including models of concurrency that support both nondeterminism and prob-
abilistic choice (see, e.g., [18]), as is the case here. A major theme along this
line stems from the seminal work of Larsen and Skou [16] on labeled Markov
processes (see also [10, 19]). A major diﬀerence between Task-PIOAs and that
work is the nature of the transition relation that deﬁnes the evolution of the pro-
cesses. We comment in more detail on this point in Section 9. Nevertheless, it
should be noted that the Task-PIOA maps we introduce are a form of functional
simulation.
In summary, our main results are as follows:
– The use of order theory to analyze Task-PIOAs, and in particular, the Ap-
ply operator that applies a task to a probabilistic input/output automa-
ton, which make clear how processes evolve under the application of task
schedules.
– The notion of a Task-PIOA map and the proof that, under a given task
schedule, these maps take the trace distributions of a domain Task-PIOA
into the trace distributions of the target Task-PIOA.
– An analysis of the Dining Cryptographers Protocol using Task-PIOAs. Our
version of Dining Cryptographers includes a Master that chooses who pays
(either one of the cryptographers or the Master itself), a concept introduced
in [4]. We prove the anonymity of the protocol from the point of view of the
cryptographers when the Master is deterministic, and then use that result
to show that anonymity also holds in the case of a probabilistic Master.
3
Ordered Structures and Domain-Theoretic Arguments
In this section we present the results on ordered sets and their application to
models of computation we need to analyze probabilistic input/output automata.
We emphasize at the outset that our results are valid in the case of directed
complete partial orders (e.g., domains) but we restrict the presentation to the
case of ordered sets, since that’s all that is needed in the current setting.
Deﬁnition 1. An ordered set is a non-empty set together with a partial order,
i.e., a reﬂexive, antisymmetric and transitive relation. If P and Q are ordered
sets, and if f : P →Q is a mapping between them, then
– f is monotone if x ≤P y implies f(x) ≤Q f(y).
– f is progressive if P = Q and x ≤P f(x) for all x ∈P.
We use Ord to denote the category of ordered sets and monotone maps.
The prototypical example of an ordered set is the family A∗of ﬁnite words over
an alphabet A in the preﬁx order: s ≤t ⇔(∃u ∈A∗) t = su.
If X is a set, then a discrete probability measure over X has the form

i∈I riδxi, where I is a countable set, δxi denotes the point mass at xi ∈X and
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
7

i ri = 1, where 0 < ri for each i. If I is ﬁnite, then the measure is said to be sim-
ple. For μ = 
i∈I riδxi, then the support of μ is deﬁned by supp μ = {xi | i ∈I}.
If P is an ordered set, the order on P lifts to the family Disc(P) of discrete
probability measures on P: 
i riδxi ≤
j sjδyj iﬀ{ri | xi ∈A} ≤{sj |
yj ∈A} for each upper set A.1 Thus, μ ≤ν iﬀμ(A) ≤ν(A) for every upper set
A. For simple measures m
i=1 riδxi ≤n
j=1 sjδyj iﬀthere is a family tij ≥0,
1 ≤i ≤m and 1 ≤j ≤n satisfying:
(a) (∀i) ri = 
j tij,
(b) (∀j) 
i tij = sj, and
(c) tij > 0 ⇒xi ≤P yj.
(Cf. Theorem 4.10 of [15] for details.) There is a monad Disc: Ord →Ord that
associates to each ordered set P the family Disc(P) of discrete probability mea-
sures on P and that sends each monotone mapping f : P →Q to the mapping
Disc(f): Disc(P) →Disc(Q) by Disc(f)(
i riδxi) = 
i riδf(xi). The unit of
the monad is the mapping x 
→δx and the multiplication m: Disc(Disc(P)) →
Disc(P) is just integration: 
i riδ
j sijδxij = 
ij risijδxij. This monad mul-
tiplication is used implicitly in the deﬁnition of the function Apply(−, −) – cf.
Remark 1.
4
Probabilistic Input/Output Automata
We use task-structured probabilistic input/output automata as our basic model
of computation. This notion is due to Canetti, Lynch, et al. [6]. In this section,
we summarize the main ideas from [6] that are necessary to understand this
paper.
Deﬁnition 2. A Probabilistic Input/Output Automaton A = (Q, q, I, O, H, D)
is a 6-tuple where:
1. Q is a countable set of states containing q ∈Q as the start state;
2. I, O and H are pairwise disjoint, countable sets of actions, referred to as
input, output and internal actions, respectively. The set Act ::= I ∪O∪H is
called the set of actions of A. The set of external actions of A is E ::= I∪O
and the set of locally controlled actions is L ::= O ∪H.
3. D ⊆Q × Act × Disc(Q) is a transition relation. An action a is enabled in
state q if (q, a, μ) ∈D for some μ ∈Disc(Q), which we denote by μq,a.
We also assume D satisﬁes:
Input enabling: For every q ∈Q and a ∈I, a is enabled in q.
Transition determinism: For every q ∈Q and a ∈Act, there is at most one
μ ∈Disc(Q) such that (q, a, μ) ∈D.
Probabilistic input/output automata also admit a hiding operator: If S ⊆OA is
a set of output actions of A, then A \ S denotes the process where O = OA \ S
and H = HA ∪S. The result is that the output of actions in S are visible only
to the process executing them, and to the recipient processes of the actions.
1 A is an upper set if A = ↑A = {x ∈P | (∃a ∈A) a ≤P x}.

8
A.D. Jaggard et al.
A probabilistic input/output automaton can be viewed as a labelled transition
system where the transition relation is a partial function Δ: Q × Act ⇀Disc(Q)
deﬁned by Δ(q, a) = μq,a, if such a measure exists. Input enabling implies the
restriction Δ|Q×I : Q × I →Disc(Q) is a total function.
We also need a method for composing PIOAs.
Deﬁnition 3. Let Ai = (Qi, qi, Ii, Oi, Hi, Di), i = 1, 2 be PIOAs. We say A1
and A2 are compatible if Act1∩H2 = Act2∩H1 = ∅= O1∩O2. If this is the case,
then we deﬁne A1∥A2 = (QA1∥A2, qA1∥A2, IA1∥A2, OA1∥A2, HA|∥A2, DA1∥A2)
where:
– QA1∥A2 = Q1 × Q2, qA1∥A2 = ⟨q1, q2⟩,
– IA1∥A2 = (I1 ∪I2) \ (O1 ∪O2), OA1∥A2 = O1 ∪O2, and HA1∥A2 = H1 ∪H2,
– DA1∥A2 = {(⟨q1, q2⟩, a, μ1 × μ2) | (a ∈Acti ⇒(qi, a, μi) ∈Di),
and (a ̸∈Actj ⇒μj = δqj)}.
The main points to note of the composition are that (1) each output action
and each hidden action is conﬁned to exactly one component, (2) the inputs of
either component can be provided by outputs of the other component, so the
composition of processes can be closed – its set of inputs can be empty, and (3)
if an action is in the alphabet of only one component, then the other component
“idles” in its current state while the enabled component executes the action.
However, even though one component may be able to perform an action in a
given state, the other component can block the action if the action is in its
alphabet, but it cannot execute the action in its current state.
4.1
Tasks
Tasks provide a mechanism for resolving nondeterminism – the choice of which
action to execute in a given state, in the execution of a PIOA.
Deﬁnition 4. A
Task-structured
Probabilistic
Input/Output
Automaton
(Task-PIOA) is a pair T = (A, R) where
– A = (Q, q, I, O, H, D) is a probabilistic I/O automaton, and
– R ⊆(O × O) ∪(H × H) is an equivalence relation on the locally controlled
actions. The equivalence classes of R are called tasks.
A Task-PIOA T is said to be action deterministic if for each state q ∈A and
each task T ∈R, there is at most one action a ∈T that is enabled in q.
A task schedule for a Task-PIOA T is a ﬁnite sequence ρ = T1T2 · · · Tn of
tasks in R.2
2 Our results in general include a treatment of the case of inﬁnite task schedules (in
which case the utility of domain theory comes to the fore), but since we only need
ﬁnite task schedules in the present setting, we limit our presentation to that case.
Nonetheless, the order-theoretic approach pays a dividend by giving a clear and
concise deﬁnition of the application of a task to a process.
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
9
4.2
Applying Task Schedules
Since we have no ﬁnal states in Q, there is a question of what constitutes a
computation of a Task-PIOA. We use task schedules to specify the sequences
of actions a process should execute. The probability distribution generated by
the process as it executes that sequence of tasks then deﬁnes a computational
behavior of the process.
Our interest is in the trace distributions over ﬁnite sequences of observable
events of the automaton A. For an automaton A = (Q, q, I, O, H, D), we let
Exec∗A = {qa0 . . . qn−1an−1qn | qi ∈Q, ai ∈Act, qi+1 ∈supp μqi,ai (∀i < n)}.3
Note that Exec∗A is ordered by the preﬁx ordering as a subset of Q×(Act×Q)∗.
If a ∈Act, then we deﬁne the set of ﬁnite execution fragments whose last state
enables the action a:
Aa = {α ∈Exec∗A | a enabled in lstate(α)},
where lstate(α) = qn is the ﬁnal state of α = qa0 · · · an−1qn. Likewise, for a task
T , let AT be the set of ﬁnite execution fragments whose last state enables the
task T :
AT = {α ∈Exec∗A | (∃a ∈T ) a enabled in lstate(α)}.
For each ﬁnite execution fragment α ∈AT , if the action a ∈T is enabled
in lstate(α), we let μlstate(α),a be the target measure of the unique transition
(lstate(α), a, μlstate(α),a) ∈D. Then we deﬁne
Apply(−, −): Disc(Exec∗A) × T →Disc(Exec∗A)
by
Apply(μ, T ) =

α̸∈AT
μ(α)δα +

α∈AT
μ(α)

s
μlstate(α),aT (s)δαaT s

.
(1)
Remark 1. There are some points to note about this deﬁnition.
1. Since each task T allows only one action aT ∈T to be enabled in lstate(α) for
each α ∈supp μ, there is no sum over actions in the second summand on the
right. This implicitly uses the monad Disc: the restriction on tasks implies
there is a partial map (q, T ) 
→
q′ μq,aq(q′)δq′ : Q × T ⇀Disc(Q), where
aq ∈T is enabled in q, which Disc extends to deﬁne the second summand of
the map Apply.
2. The execution sequences in whose last state T is not enabled still appear as
the ﬁrst summand on the right in the deﬁnition of Apply(μ, T ). This ensures
that the result of applying T to μ is a probability distribution (without the
ﬁrst summand, the result would have a total mass of 1 −μ(AT )).
3 Note that all such sequences begin at q. Recall x ∈supp μ iﬀμ(x) > 0.

10
A.D. Jaggard et al.
Thus, Apply(μ, T ) applies the task T to each execution fragment α ∈supp μ,
thus resolving the nondeterminism of which action should occur after α has
been executed. For a ﬁnite task schedule ρ = T1 · · · Tn, we deﬁne
Apply(μ, ρ) = Apply(Apply(μ, T1 · · · Tn−1), Tn).
Proposition 1. Let μ = 
α μ(α)δα ∈Disc(Exec∗(A)), then
μ ≤Apply(

α
μ(α)δα, ρ) =

α
μ(α)Apply(δα, ρ).
In particular, Apply(μ, ρ) is well-deﬁned for any task sequence ρ = T1 · · · Tn.
Proof. By induction on the length of ρ – see Appendix.
Corollary 1. If μ = 
i piμi, then μ ≤Apply(
i piμi, ρ) = 
i piApply(μi, ρ).
Proof. By the Proposition, we know that μi ≤Apply(μi, ρ) for each index i, so
μ =

i
piμi ≤

i
piApply(μi, ρ) = Apply(

i
piμi, ρ),
the last equality following by expanding each μi as a convex sum of point masses
and then applying the Proposition once again.
5
Comparing PIOAs
An important component of any modeling paradigm is the ability to compare
models of distinct processes, in order to analyze the diﬀerences between them.
In the development of Task-PIOAs in [6], this is done via simulation relations,
which are used to show that the probability distributions over observable be-
haviors of one Task-PIOA are a subset of those of another Task-PIOA. This is
useful for simplifying a process by devising a simpler, simulating process, and
then deductions about the behavior of the simpler process can be used to prove
properties about the more complicated one. This approach is based on matching
a task schedule for the ﬁrst process to one for the simulating process, but in
our setting, the security speciﬁcation requires we use the same task schedule for
both processes. Rather than use the Task-PIOA simulation relations from [6],
we present a simpler approach to simulation that meets this requirement. The
setting is as follows:
If T = (A, R) is a Task-PIOA, and A ⊆(I ∪O) is a set of observable actions,
then traceA : Exec∗A →A∗is the function that extracts the sequence of A-
actions (i.e., the sequences from A∗) from an execution sequence of A. Since
we are using only discrete measures, all sets are measurable, and so traceA is
a measurable map. Then Disc(traceA)(μ) = μ ◦trace−1
A
is a measure on A∗for
each measure μ ∈Disc(Exec∗A). In fact, if μ = 
i riδαi, then Disc(traceA)(μ)
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
11
= 
i riδtraceA(αi). We let SA denote the family of task schedules on A, and for
ρ ∈SA, let Apply(δq, ρ) = 
α∈supp Apply(δq,ρ) rαδα. Then we deﬁne
tdistρ
A(A) = Disc(traceA)(Apply(δq, ρ)) =

α∈supp Apply(δq,ρ)
rαδtraceA(α)
be the measure on A∗induced by traceA from the measure Apply(δq, ρ). We also
call
tdistsA(A) = {tdistρ
A(A) | ρ ∈SA}
the A-trace distributions of A. We elide the subscript if A = I ∪O.
Deﬁnition 5. Let Ai = (Qi, qi, Ii, Oi, Hi, Di), i = 1, 2 be two Task-PIOAs, with
task sets Ti, i = 1, 2, respectively. We deﬁne a Task-PIOA map φ: A1 →A2 to
consist of
– A mapping φ: Q1 →Q2 of the states of A1 to the states of A2 satisfying
φ(q1) = q2. We call this the state component of φ.
– A mapping φ: Act1 →Act2 of actions of A1 to actions of A2. We call this
the action component of φ.
We also require the mapping φ: A1 →A2 to satisfy the compatibility conditions:
(i) ⟨q, a, μ⟩∈D1 ⇒⟨φ(q), φ(a), Disc(φ)(μ)⟩∈D2; i.e., if q ∈Q1 and a ∈Act1
is enabled in state q, then φ(a) is enabled in state φ(q) and Disc(φ)(μq,a) =
μφ(q),φ(a).
(ii) For each T ∈T1, φ(T ) ∈T2.
Remark 2. Note that if φ does not rename states or actions, then this coincides
with the notion of reﬁnement in process algebra.
The map φ induces Exec∗φ: Exec∗A1 →Exec∗A2 by
Exec∗φ(qa0 · · · an−1qn) = φ(q)φ(a0) · · · φ(an−1)φ(qn),
which in turn gives Disc(Exec∗φ): Disc(Exec∗A1) →Disc(Exec∗A2) by
Disc(Exec∗φ)(

i
riδαi) =

i
riδExec∗φ(αi).
Our main result is:
Theorem 1. Let φ: A1 →A2 be a Task-PIOA map of Task-PIOAs satisfying
φ(AT ) = Aφ(T ) for each task T ∈T1. If A ⊆Act1 is the set of observable actions,
then Disc(φ|∗
A)(tdist(A1)) ⊆tdist(A2).
Proof. We give an outline of the proof here, and provide full details in the Ap-
pendix. First, if μ ∈Disc(Exec∗A1), then μ = 
α∈supp μ rαδα. So, for T ∈T1, we
can apply our deﬁnition of Apply from Equation 1 and of Disc(Exec∗φ) above to
show Disc(Exec∗φ)(Apply(μ, T )) = Apply(Disc(Exec∗φ)(μ), φ(T )). An induction
on n then shows the analogous result holds with any task schedule ρ = T1 · · · Tn
in place of T . The next step is to use this equation to show
Disc(φ|∗
A)(Disc(trace)(Apply(μ, ρ))) = (Disc(trace) ◦Disc(Exec∗φ))(Apply(μ, ρ)),
which implies the result.

12
A.D. Jaggard et al.
6
Dining Cryptographers
The Dining Cryptographers Protocol is originally due to Chaum [9]. It postulates
several cryptographers who are dining together. They are trying to determine
whether one of them paid for dinner or NSA paid, without revealing to each
other which cryptographer paid. In [9] the implementation of the decision of who
pays is left unspeciﬁed; we use the solution of Bhargava and Palamadessi [4] in
which a Master (that is, NSA) chooses who pays. The cryptographers use the
following protocol to accomplish this. Each cryptographer has a coin that he
ﬂips, and he reports the outcome of that ﬂip to the cryptographer to his right.
The cryptographers then report the outcomes of the two coin ﬂips they know
– theirs and the one to their left – by announcing either Agree or Disagree.
A cryptographer reports the outcome correctly if she is not paying, but if she
is paying for dinner, then she reverses the announcement and lies about the
outcome of the coin tosses she has witnessed. A simple argument based on parity
shows that the Master is paying if the number of Disagree announcements is even,
while one of the cryptographers is paying if there are an odd number of Disagree
announcements. Moreover, if the cryptographers are honest and if the coins they
use are fair, this protocol protects the identity of the paying cryptographer from
the other cryptographers, as long as there are at least three cryptographers.
We will use probabilistic input/output automata [6] to model the Dining
Cryptographers. The formal model will be introduced below, but ﬁrst we give a
process algebraic description of the component processes. We focus on the least
number of cryptographers that makes the analysis meaningful, which is three.
The processes are:4
Master ::= choose payer ◦tell pay0 ◦tell pay1 ◦tell pay2
Cryptoi ::= rec payi ◦ﬂip coini ◦learn coini−1 ◦tell coini ◦
compare coins ◦announcei ◦receive announcements
The composition operator ◦in this syntax is a meant to allow the actions to
occur in any order speciﬁed by the environment. The action tell payi sends the
message to cryptographer i, i = 0, 1, 2 indicating whether or not she is paying.
We believe the meaning of the components of the processes listed here is clear,
so they require no further explanation. But some names will change when we
encode the processes as probabilistic input/output automata, in order make the
communications synchronous. The process representing our protocol is then the
composition
Master ∥Crypto0 ∥Crypto1 ∥Crypto2,
where ∥denotes parallel composition.
7
A Model of Dining Cryptographers
We now show how to model the Dining Cryptographers using task-based prob-
abilistic input/output automata. Figure 1 describes the Master process and its
4 We us indices i = 0, 1, 2 modulo 3, so 0 −1 = 2, 2 + 1 = 0, etc.
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
13
transition relation. This includes the set of actions for the process, labeled ac-
cording to whether they are input, output or internal actions; it also gives the
state space of the process and the list of tasks. The Master has an internally
deﬁned distribution r · δ⟨master,⊥Q0⟩+ 
i riδ⟨cryptoi,⊥Q0⟩that gives the proba-
bility of choosing the payer – i.e., r + 
i ri = 1. Also, the transition relation
encodes the Master’s actions of sending messages to each of the cryptographers
indicating which one, if any, is paying, and the state space records the fact that
the message to each cryptographer has been sent. The Master also observes the
announcements of the cryptographers. Actions determine tasks: e.g., one of the
Master’s tasks is payi = {payi(T ), payi(F)}.
Master
Actions:
Input:
Output:
payj(c), c ∈{T, F}, j ∈{0, 1, 2}
Internal:
choose payer
State:
Payer × Q0, where Q0 = 2
i=0{⊥, T},
Payer = {⊥, Master, Cryptoi
| i = 0, 1, 2},
all initially ⊥
Tasks:
{choose payer},
{payj(c) | c ∈{T, F}}j=0,1,2
Transitions:
D = {⟨⟨⊥, ⊥Q0⟩, choose payer, r · δ⟨Master,⊥Q0 ⟩+ 
i riδ⟨Cryptoi,⊥Q0 ⟩⟩}
∪{⟨⟨Master, q⟩, payj(F), δ⟨Master,q′⟩| qj = ⊥, q′
j = T, j = 0, 1, 2}
∪{⟨⟨Cryptoi, q⟩, payi(T), δ⟨Cryptoi,q′⟩⟩| qi = ⊥& q′
i = T}
∪{⟨⟨Cryptoi, q⟩, payj(F), δ⟨Cryptoj,q′⟩⟩| qj = ⊥& q′
j = T, j ̸= i}
Fig. 1. The Master automaton chooses the payer
Cryptoi
Actions:
Input:
payi(c), c ∈{T, F}
coini−1(c), c ∈{H, T}
(We again use subtraction mod 3.)
announcei−1(c), c ∈{Agree, Disagree}
announcei+1(c), c ∈{Agree, Disagree}
Output:
coini(c), c ∈{H, T}
announcei(c), c ∈{Agree, Disagree}
Internal:
ﬂipi
comparei
State:
Qi = Payi × Coini−1 × Coini × Coin senti
×Comparei × Announcei−1 × Announcei+1
Payi = {payi(c) | c ∈{⊥, T, F}}
initially ⊥
Coini−1 = {coini−1(c) | c ∈{⊥, H, T}}
initially ⊥
Coini−1 = {coini−1(c) | c ∈{⊥, H, T}}
initially ⊥
Coin senti = {coin senti(x) | x ∈{F, T}}
initially F
Comparei = {⊥, T, F}, initially ⊥
Announcej = {announcej(c)}, j = i −1, i + 1
c ∈{⊥, Agree, Disagree}, initially ⊥
Tasks:
{ﬂipi}, {coini(c)}, {comparei},
{announcei(c)}
Fig. 2. The actions, tasks and state set of Cryptographer i

14
A.D. Jaggard et al.
Cryptoi
Transitions:
Qi = Payi × Coini−1 × Coini × Coin senti × Comparei × Announcei
Payi = Comparei = {⊥, T, F}; Coini−1 = Coini = {⊥, H, T};
Announcei = {⊥, Agree, Disagree}
D = {⟨q, payi(c), δq′⟩| q1 =⊥, q′
1 = c, j ̸= 1 ⇒qi = q′
i}
∪{⟨q, coini−1(c), δq′⟩| q2 =⊥, q′
2 = c, j ̸= 2 ⇒qi = q′′
i }
∪{⟨q, ﬂipi, 1
2δq′ + 1
2δq′′⟩| q3 =⊥, q′
3 = H, q′′
3 = T, j ̸= 3 ⇒qi = q′
i = q′′
i }
∪{⟨q, coini(c), δq′⟩| q3 = c ̸= ⊥, q4 = F, q4 = T, j ̸= 3, 4 ⇒qi = q′
i}
∪{⟨q, comparei, δq′⟩| q2 = q3 ̸= ⊥, q5 =⊥, q′
5 = T, j ̸= 5 ⇒qj = q′
j}
∪{⟨q, comparei, δq′⟩| ⊥̸= q2 ̸= q3 ̸= ⊥, q5 =⊥, q′
5 = F, j ̸= 5 ⇒qj = q′
j}
∪{⟨q, announcei(c), δq′⟩| q1 = F, q5 = T, q′
6 =⊥, c = q′
6 = Agree, j ̸= 6 ⇒qj = q′
j}
∪{⟨q, announcei(c), δq′⟩| q1 = T, q5 = T, q′
6 =⊥, c = q′
6 = Disagree, j ̸= 6 ⇒qj = q′
j}
∪{⟨q, announcei(c), δq′⟩| q1 = F, q5 = F, q′
6 =⊥, c = q′
6 = Disagree, j ̸= 6 ⇒qj = q′
j}
∪{⟨q, announcei(c), δq′⟩| q1 = T, q5 = F, q′
6 =⊥, c = q′
6 = Agree, j ̸= 6 ⇒qj = q′
j}
Fig. 3. The transition relation for Cryptographer i
Figure 2 gives the actions, state set and tasks of the cryptographers, while
the transition relation is given in Figure 3. Together, these model the cryptog-
raphers as task-based probabilistic input/output automata. These are the most
complicated Task-PIOAs we use, having several states and actions to perform.
In essence, each cryptographer receives a message from the Master indicating
whether she is paying, and then uses this information in determining whether
to honestly announce the outcome of the coin ﬂips seen, or to lie. Each cryptog-
rapher also has an unbiased coin: this is speciﬁed by the output of ﬂipi that is
1
2δH + 1
2δT . Cryptoi reports the outcome of her coin ﬂip to Cryptoi+1, compares
her ﬂip with the one from Cryptoi−1 and announces the outcome as described.5
The component names have changed from the syntax in Section 6; for example,
rec payi is now the input action payi for Cryptoi, while learn coini−1 is short-
ened to the input action coini−1; this synchronizes the outputs of one process
with the inputs of another.
Our model of the Dining Cryptographers is then the composition
DC = Master ∥Crypto0 ∥Crypto1 ∥Crypto2,
of the Master with the cryptographers. Note that all actions except the an-
nouncements are hidden. Indeed, by deﬁnition, the outputs of one component
that are inputs of another become hidden in the composition of the two: the
Master has no input actions, its outputs are the inputs to the cryptographers,
while each cryptographer provides the input for the “next” cryptographer (the
5 Here again, we are using arithmetic modulo 3.
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
15
one to the right, numbering in the counterclockwise direction). Since all input
actions are from other components, DC is a closed Task-PIOA.
A prototypical task schedule for this combined process is:
choose payer, pay0, pay1, pay2, ﬂip0, ﬂip1, ﬂip2, coin0, coin1, coin2,
compare0, compare1, compare2, announce0, announce1, announce2
8
Proving the Protocol Secure
To say DC is secure, we mean that if one of the cryptographers is paying, then
none of the non-paying cryptographers can tell which of the other cryptographers
is paying. In [9] this is expressed by the fact that the a priori probability that a
particular cryptographer is paying before the protocol is run is the same as the
a posteriori probability that cryptographer is paying after it is run.
Our proof is broken down into two cases. We ﬁrst consider the case that the
Master is deterministic, in which case we show the anonymity of the paying
cryptographer from the perspective of the other cryptographers. We then use
this result to prove the anonymity of the paying cryptographer in the case of
a probabilistic Master who randomly chooses a cryptographer (or himself) to
pay.
8.1
A Deterministic Master
The anonymity guarantee of the Dining Cryptographers – if one of the cryptog-
raphers is paying for dinner, then neither of the two non-payers can distinguish
the behaviors of the remaining cryptographers – can be proved by showing that
neither non-paying cryptographer can see a diﬀerence in the distributions on the
announcements of the other two cryptographers. To accomplish this, we deﬁne a
deterministic Master using the distribution r ·δ⟨Master,⊥Q0 ⟩+
i riδ⟨Cryptoi,⊥Q0⟩
satisfying ri = 1 and r = 0 = rj for j ̸= i, that speciﬁes a Master Mi that
selects Cryptoi, i = 0, 1 or 2 as the payer. Let
Ai = Mi ∥C0 ∥C1 ∥C2
(1)
denote the process with Master Mi, i = 0, 1, 2. We also let S denote the set of
task schedules.
Proposition 2. If all cryptographers use fair coins, then
(∀ρ ∈S) tdistρ
Ai(Ai+1) = tdistρ
Ai(Ai−1) (∀i = 0, 1, 2), 6
where Ai is the set of actions observable by cryptographer i.
Proof. Without loss of generality, we also assume i = 0 (so Crypto0 is not
paying). The proof proceeds by deﬁning a mapping from φ: A1 →A2 that
simply reverses the roles of Crypto1 and Crypto2 and reverses the values of the
received messages, pay1(c) and pay2(c).
6 We are again counting mod 3, since there are three cryptographers.

16
A.D. Jaggard et al.
More formally, we wish to apply Theorem 1. Let T denote the set of tasks
of DC, and let QAi = QM × 2
i=0 Qi denote the state space of Ai, i = 1, 2,
where QM = {Master, Cryptoi | i = 0, 1, 2} × 2
j=0{⊥, T } is the state space of
the Master and Qj is the state space of the jth cryptographer.
Deﬁne φ: QA1 →QA2 by φ(Crypto1) = Crypto2, φ(Crypto2) = Crypto1, and
φ is the identity on all other states. Next, deﬁne φ: Act1 →Act2 by φ(pay1(c)) =
pay2(c), and φ(pay2(c)) = pay1(c). On all other actions, φ is the identity. We
show φ: A1 →A2 is a Task-PIOA map satisfying the hypothesis of Theorem 1.
We begin by checking conditions (i) and (ii) of Deﬁnition 5. For (i), the crucial
points are ﬁrst, what happens to pay1(T ), which is enabled in ⟨Crypto1, ⊥⟩.
But φ(pay1(T )) = pay2(T ), which is enabled in ⟨Crypto2, ⊥⟩in A2. Likewise,
pay2(F) is enabled in ⟨Crypto1, ⊥⟩in A1, while φ(pay2(F)) = pay1(F), which
is enabled in ⟨Crypto2, ⊥⟩in A2. Next, we have that φ(δCrypto1) = δφ(Crypto1),
which is the output distribution of the action choose payer; this follows because
φ(Crypto1) = Crypto2.
The second point is to check that φ(δ⟨Crypto1,T ⟩) = δ⟨φ(Crypto1),φ(T )⟩, but this
holds because φ(Crypto1) = Crypto2 and φ(T ) = T . Lastly, the fact that all coins
are fair implies that Disc(φ)(⟨⊥, ﬂip1, 1
2δH + 1
2δT ⟩) = ⟨⊥, ﬂip2, 1
2δH + 1
2δT ⟩, which
is the only case in which μq,a is not a point mass; for point masses the result
follows from the deﬁnition of φ and from Disc(φ)(δx) = δφ(x). This validates the
remaining requirement in condition (i) of Deﬁnition 5.
Finally, A1 and A2 have the same tasks, and φ permutes the two actions listed
and their corresponding enabling states, so φ(T ) is a task and φ(AT ) = Aφ(T ) for
each task T . Thus φ is a Task-PIOA map satisfying the hypothesis of Theorem 1,
so tdist(A1) ⊆tdist(A2), but since φ is its own inverse, the reverse is also true.
Corollary 2. The Dining Cryptographers is deterministically secure: i.e., in the
case that one of the cryptographers is paying, a non-paying cryptographer cannot
tell which one of the other cryptographers is paying.
Proof. We ﬁrst calculate the probability distributions on the announcements:
For a non-paying cryptographer, the coins are fair, so the outcome of his own
coin is 1
2δH + 1
2δT . The same is true of the coin he sees from his neighbor on
the left, so the probability of announcing Agree is 1
4δHH + 1
4δT T , which is 1
2.
Obviously the probability of announcing Disagree is also 1
2. The payer has just
been shown to have the same probability distribution on announcements, so the
probability is 1
2Agree and 1
2Disagree as well.
Now, a non-paying cryptographer sees the same distribution on the announce-
ments of each of the other cryptographers regardless of which one is paying, so
a non-paying cryptographer cannot tell which one is paying on the basis of the
probability distributions on announcements they make.
8.2
The Probabilistic Case
In this section we consider the case of a probabilistic Master – i.e., one that
chooses the payer probabilistically. This uses the rest of the deﬁnition of the
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
17
Master as given in Figure 1. Thus, the Master process begins with the proba-
bilistic choice Mpr = r ·δmaster +2
i=0 riδci which denotes choosing the Master
with probability r and choosing cryptographer i with probability ri. We call this
Master Mpr, so our system now is
Apr := Mpr ∥C0 ∥C1 ∥C2.
Even though the Master paying is one possibility, the notion of anonymity re-
mains clear: if a cryptographer pays, then the identity of the payer should remain
unknown to the other cryptographers. Indeed, anonymity is now a conditional
probability: for a given sample space X and probability measure P, the condi-
tional probability of A given that B has occurred is P(A|B) = P (A∩B)
P (B) , provided
P(B) ̸= 0.
The set E of observable events consists of possible sequences of announcements
by the cryptographers. We denote the probability of a sequence O ∈E under
the probability measure induced by applying the task sequence ρ to the process
Apr as Apply(δApr
q
, ρ)(O), and if O is conditioned on the event O′, we denote the
probability by Apply(Apr, ρ)(O|O′).
Theorem 2. If all cryptographers use unbiased coins, then the Dining Cryptog-
raphers is probabilistically secure, by which we mean (∀ρ ∈S) (∀O ∈E)
Apply(δApr
q
, ρ)(O|M0) = Apply(δApr
q
, ρ)(O|M1) = Apply(δApr
q
, ρ)(O|M2).
I.e., the probability distributions on announcements are the same whether the
probabilistic Master chooses cryptographer 0, 1 or 2 as the payer.
Proof. There are eight possible announcement sequences under any ρ. If the
Master has chosen to pay, any of the four with an even number of Disagree
announcements occurs, but if one of the cryptographers is paying, then one of
the four with an odd number of Disagree announcements occurs. We divide E
into the set EE of sequences with an even number of Disagree announcements,
and EO of sequences with an odd number of Disagree announcements.
For any ρ, Corollary 1 implies Apply(δApr
q
, ρ) = r · Apply(δAm
q
, ρ) + 
i ri ·
Apply(δAi
q , ρ), where Ai is as in Equation 1 and Am denotes the case of a deter-
ministic Master choosing himself to pay. We assume r ̸= 0 ̸= rj for j = 0, 1, 2.
Proposition 2 shows the probability distributions Apply(δAi
q , ρ) all agree on
the observable events, and Corollary 2 notes the probability that any of the
cryptographers announces Agree is
1
2, as is the probability that any of them
announces Disagree; it’s easy to show the same in case the Master is paying.
Hence, for any sequence O of announcements and each master Mj, j = 0, 1, 2,
Apply(δApr
q
, ρ)(O ∩Mj) = Apply(δApr
q
, ρ)((O ∈EE) ∩Mj)
+Apply(δApr
q
, ρ)((O ∈EO) ∩Mj).
In case O ∈EE, then the Master is paying, and the only event that can oc-
cur is Mm, in which case the probability of any of the possible announcement
sequences is 1
4, since they are equally likely.

18
A.D. Jaggard et al.
The other possibility is that a cryptographer is paying, i.e., O ∈EO. Then
Apply(δ
Apr
q
, ρ)((O ∈EO) ∩Mj) =

i
riApply(δAi
q , ρ)

((O ∈EO) ∩Mj)
=

i
riApply(δAi
q , ρ)((O ∈EO) ∩Mj) = rj
4 .
Since Apply(δApr
q
, ρ)(Mj) = rj ̸= 0, we have Apply(δApr
q
, ρ)(O|Mj) = 1/4. This
holds for each j = 0, 1, 2 for which rj ̸= 0. This proves that the paying cryptog-
rapher is unknown to the other two.
9
Summary and Future Work
We have presented an alternative derivation of the Task-PIOA framework of
Canetti, et al. [6] based on order theory. The presentation focuses on two of
the components needed for Task-PIOAs, discrete probability and probabilistic
automata theory. Our main result is a proof that Task-PIOA maps preserve
trace distributions on observable events. We applied our approach by analyzing
the Dining Cryptographers Protocol. Our model has much more detail than
traditional analyses of the protocol using concurrency, but the point has been
to illustrate the use of the Task-PIOAs. Of particular note is the modularity of
our proof that the protocol meets its security speciﬁcation – the anonymity of a
paying cryptographer from the other cryptographers. We ﬁrst show this for the
case of a deterministic Master, and then use this result to prove the result also
holds in the case of a probabilistic Master.
We purposely have avoided the use of cryptographic primitives, and our pre-
sentation has been restricted to an example where these aspects are not needed.
The addition of encryption would allow an interesting extension of the Din-
ing Cryptographers. Namely, it would support adding a component process one
might call Anonymizer that provides the service of making the paying cryptogra-
pher anonymous even to the Master, one of the original goals of the protocol [9].
This could be accomplished by having the Master’s messages to the cryptogra-
phers ﬁrst sent to the Anonymizer, which then reroutes the messages using a
randomized permutation of the recipients.
We commented in Section 2 that the PIOA approach diﬀers signiﬁcantly from
the approach utilizing labeled Markov processes, as in [10,16,19]. The diﬀerence
concerns the transition relation D ⊆Q×Act×Disc(Q) that deﬁnes the evolution
of processes. For labeled Markov processes, this relation (or the closely related
D ⊆Q × Disc(Act × Q)), assume D generates a function Δ: Q × Act →Disc(Q)
(resp., Δ: Q →Disc(Act×Q)), but in the PIOA approach, Δ is a partial function
(cf. comments following Deﬁnition 2). The usual method for transforming partial
functions into total functions – adjoining a deadlock state and mapping elements
of the domain where Δ is not deﬁned to that deadlock state (or, more precisely,
point mass at deadlock in the present situation) won’t work for PIOAs, because
of how the executions of PIOAs are deﬁned (see the footnote to Equation 1).
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
19
In future work, we plan to apply our approach to other settings where they
are appropriate. Among the areas being scrutinized is the Goldreich-Micali-
Wigderson [13] secure multiparty computation protocol, where we also antic-
ipate using the universal composability result for oblivious transfer from [6].
The GMW Protocol works by composing a number of simpler protocols, includ-
ing oblivious transfer and communication mechanisms similar to those among
the Dining Cryptographers. The work presented in this paper is thus the ﬁrst
step in that path.
Acknowledgment
We wish to thank the referees for their helpful comments on the ﬁrst draft of
this paper.
References
1. Abadi, M., Gordon, A.: A calculus for cryptographic protocols: The spi calculus.
Information and Computation 148, 1–70 (1999)
2. Abramsky, S., Jung, A.: Domain theory. In: Abramsky, S., Gabbay, D.M.,
Maibaum, T.S.E. (eds.) Handbook of Logic in Computer Science, vol. 3,
pp. 1–168. Clarendon Press, Oxford (1994)
3. de Alfaro, L., Henzinger, T., Jhala, R.: Compositional methods for probabilistic
systems. In: Larsen, K.G., Nielsen, M. (eds.) CONCUR 2001. LNCS, vol. 2154,
pp. 351–365. Springer, Heidelberg (2001)
4. Bhargava, M., Palamidessi, C.: Probabilistic anonymity. In: Abadi, M., de Al-
faro, L. (eds.) CONCUR 2005. LNCS, vol. 3653, pp. 171–185. Springer, Heidelberg
(2005)
5. Canetti, R.: Universally Composable Security: A New Paradigm for Cryptographic
Protocols, http://eprint.iacr.org/2000/067
6. Canetti, R., Lynch, N., et al.: Using Probabilistic I/O Automata to Analyze an
Oblivious Transfer Protocol (preprint), http://hdl.handle.net/1721.1/33154
7. Chatzikokolakis, K., Palamidessi, C.: Making Random Choices Invisible to the
Scheduler. In: Caires, L., Vasconcelos, V.T. (eds.) CONCUR 2007. LNCS, vol. 4703,
pp. 42–58. Springer, Heidelberg (2007)
8. Chatzikokalakis, K., Palamidessi, C., Panangaden, P.: Anonymity protocols as
noisy channels. Information and Computation 206, 378–401 (2008)
9. Chaum, D.: The dining cryptographers problem: Unconditional sender and recipi-
ent untraceability. Journal of Cryptology 1, 65–75 (1988)
10. Desharnais, J., Gupta, V., Jagadeesan, R., Panangaden, P.: Approximating Labeled
Markov Processes. Information and Computation 184(1), 160–200 (2003)
11. Garcia, F., van Rossum, P., Sokolova, A.: Probabilistic anonymity and admissible
schedulers, June 2 (2007) arXiv:0706.1019, http://eprintweb.org/S/authors/
All/so/Sokolova
12. Goldreich, O.: Secure Multi-party Computation, http://www.wisdom.weizmann.
ac.il/~oded/pp.html
13. Goldreich, O., Micali, S., Wigderson, A.: Proofs that yield nothing but their valid-
ity, or All languages in NP have Zero-knowledge proofs. JACM 38, 691–729 (1991)

20
A.D. Jaggard et al.
14. Hasuo, I., Kawabe, Y.: Probabilistic anonymity via coalgebraic simulations. In: De
Nicola, R. (ed.) ESOP 2007. LNCS, vol. 4421, pp. 379–394. Springer, Heidelberg
(2007)
15. Jones, C.: Probabilistic non-determinism. University of Edinburgh, Edinburgh
(1992)
16. Larsen, K.G., Skou, A.: Bisimulation through Probabilistic Testing. Information
and Computation 94(1), 1–28 (1991)
17. Lincoln, P., Mitchell, J.C., Mitchell, M., Scedrov, A.: A Probabilistic Poly-Time
Framework for Protocol Analysis. In: ACM Conference on Computer and Commu-
nications Security, pp. 112–121 (1998)
18. Mislove,
M.:
Nondeterminism
and
probabilistic
choice:
obeying
the
laws.
In: Palamidessi, C. (ed.) CONCUR 2000. LNCS, vol. 1877, pp. 350–364. Springer,
Heidelberg (2000)
19. Mislove, M., Pavlovic, D., Worrell, J.: Labelled Markov Processes as Generalized
Stochastic Relations. Electronic Notes Theoretical Computer Science 172, 459–478
(2007)
20. Mitchell, J., Ramanathan, A., Scedrov, A., Teague, V.: A probabilistic polynomial-
time process calculus for the analysis of cryptographic protocols. Theoretical Com-
puter Science 353, 118–164 (2006)
21. Schneider, S., Sidiropoulos, A.: CSP and anonymity. In: Martella, G., Kurth, H.,
Montolivo, E., Bertino, E. (eds.) ESORICS 1996. LNCS, vol. 1146, pp. 198–218.
Springer, Heidelberg (1996), http://dx.doi.org/10.1007/3-540-61770-1_38
22. Segala, R.: Modeling and Veriﬁcation of Randomized Distributed Real-time Sys-
tems, PhD Thesis, MIT Technical Report MIT/LCS/TR-676 (1995)
23. Segala, R., Lynch, N.: Probabilistic simulations for probabilistic processes. Nordic
Journal of Computing 2, 250–273 (1995)
A
Appendix - Proofs
We present the proofs of selected results not proved in the body of the paper.
Proposition 1. Let μ = 
α μ(α)δα ∈Disc(Frags∗(A)), then
μ ≤Apply(

α
μ(α)δα, ρ) =

α
μ(α)Apply(δα, ρ).
In particular, Apply(μ, ρ) is well-deﬁned for any task sequence ρ = T1 · · · Tn.
Proof. If ρ = λ the empty sequence, then Apply(μ, ρ) = μ, so the result is trivial,
while if ρ = T is a single task, then
Apply(μ, T ) =

α̸∈AT
μ(α)δα +

α∈AT
μ(α)

s
μlstate(α),a(s)δαas

(2)
=

α̸∈AT
μ(α)Apply(δα, T) +

α∈AT
μ(α)Apply(δα, T)
=

α
μ(α)Apply(δα, T).
The fact that μ ≤Apply(μ, T ) is obvious from Equation 2.
www.ebook3000.com

Reasoning about Probabilistic Security Using Task-PIOAs
21
Next, if ρ = ρ′T is ﬁnite, then
Apply(μ, ρ) = Apply(Apply(μ, ρ′), T ) = Apply(

α
μ(α)Apply(δα, ρ′), T )
=

α
μ(α)Apply(Apply(δα, ρ′), T )
Assuming by induction that μ ≤Apply(μ, ρ′), then μ ≤Apply(μ, ρ) follows from
the basis step.
Theorem 1. Let φ: A1 →A2 be a Task PIOA map of task PIOAs satisfying
φ(AT ) = Aφ(T ) for each task T ∈T1. If A ⊆Act1 is the set of observable actions,
then Disc(φ|∗
A))(tdist(A)) ⊆tdist(A2).
Proof. We ﬁrst show that if Ai ⊆Acti, i = 1, 2 are the sets of observable events
and if φ(A1) ⊆A2, then the following diagram commutes:
Disc(Exec∗A1) × T
Disc(Exec∗φ)×φ

Apply
 Disc(Exec∗A1)
Disc(Exec∗φ)

Disc(trace)
 Disc(A∗
1)
Disc(φ|∗
A1 )

Disc(Exec∗A2) × T
Apply
 Disc(Exec∗A2)
Disc(trace)
 Disc(A∗
2),
where φ∗: A∗
1 →A∗
2 is given by φ∗(a0 · · · an) = φ(a0) · · · φ(an). Indeed, the
commutativity of the left square is a diagram chase, with one twist: we need
φ(T ) ∈T2 for each T ∈T1: Indeed, if 
i riδαi ∈Disc(Exec∗A1) and T ∈T1,
then
Disc(Exec∗φ)(Apply(

i
riδαi, T)) =
= Disc(Exec∗φ)
⎛
⎝
αi̸∈AT
riδαi +

αi∈AT
ri

q∈Q1
μlstate(αi),aT (q)δαiaT q
⎞
⎠
1=

αi̸∈AT
riδExec∗φ(αi) +

αi∈AT
ri

q∈Q1
μlstate(Exec∗φ(αi)),φ(aT )(φ(q))δExec∗φ(αiaT q)
2=

Exec∗φ(αi)̸∈Aφ(T )
riδExec∗φ(αi) +

Exec∗φ(αi)∈Aφ(T )
ri

φ(q)∈Q2
μlstate(Exec∗φ(αi)),φ(aT )(φ(q))δExec∗φ(αiaT q)
= Apply

Disc(Exec∗φ)(

i
riδαi), φ(T)

,

22
A.D. Jaggard et al.
where
1= follows from the deﬁnition of Disc(Exec∗φ), while
2= follows from the
assumptions (i) that φ(T ) ∈T2 is a task in A2 and (ii) that φ(AT ) = Aφ(T )
(to preserve the ﬁrst summand in each case), and from the calculation that
Exec∗φ(lstate(αi)) = lstate(Exec∗φ(αi)). The right square is then a simple dia-
gram chase.
Now the result follows: Let ρ = T1 · · · Tn ∈T ∗
1 be a task schedule. Then for

i riδαi ∈Disc(Exec∗A1), we have
Disc(Exec∗φ)(Apply(

i
riδαi, ρ))=Disc(φ∗)(Apply(Apply(

i
riδαi, T1 · · · Tn−1), Tn)))
= Apply

Disc(Exec∗φ)(Apply(

i
riδαi, T1 · · · Tn−1)), φ(Tn))

by 2). It then follows by induction on n that
Disc(Exec∗φ)(Apply(

i
riδαi, ρ)) = Apply

Disc(Exec∗φ)(

i
riδαi), φ(T1) · · · φ(Tn)

.
Thus,
Disc(trace) ◦Disc(Exec∗φ)(Apply(

i
riδαi, ρ)) =
Disc(trace)

Apply

Disc(Exec∗φ)(

i
riδαi), φ(T1) · · · φ(Tn)

.
But the left square in 2) implies
Disc(trace)◦Disc(Exec∗φ)(Apply(
X
i
riδαi, ρ)) = Disc(φ|∗
A1)
 
Disc(trace)(Apply(
X
i
riδαi, ρ))
!
,
which implies the claim.
www.ebook3000.com

Secrecy and Authenticity Types
for Secure Distributed Messaging⋆
Michele Bugliesi, Stefano Calzavara, and Damiano Macedonio
Universit`a Ca’ Foscari Venezia, Dipartimento di Informatica
{michele,scalzava,mace}@dsi.unive.it
Abstract. We introduce a calculus with mobile names, distributed prin-
cipals and primitives for secure remote communication, without any ref-
erence to explicit cryptography. The calculus is equipped with a system
of types and eﬀects providing static guarantees of secrecy and authentic-
ity in the presence of a Dolev-Yao intruder. The novelty with respect to
existing type systems for security is in the structure of our secrecy and
authenticity types, which are inspired by the formulas of BAN Logic,
and retain much of the simplicity and intuitive reading of such formulas.
Drawing on these types, the type system makes it possible to character-
ize authenticity directly as a property of the data exchanged during a
protocol rather than indirectly by extracting and interpreting the eﬀects
the protocol has on that data.
1
Introduction
Distributed protocols draw on cryptographic constructs to protect the secrecy
and integrity of sensitive data against any potential attack. When designing dis-
tributed applications, however, it is often convenient to rely on more abstract,
structured primitives for secure remote messaging and let a compiler automat-
ically build defensive implementations on top of the underlying cryptographic
infrastructure.
Following an increasingly popular approach in the speciﬁcation of distributed
systems, in [7,6] the ﬁrst author and Focardi isolated a core set of security
abstractions for programming distributed protocols, and showed them eﬀective
both for high-level protocol design and for security analysis in adversarial set-
tings. In the present paper, we further investigate the eﬀectiveness of that ap-
proach by developing a typed version of the abstractions.
We introduce a variant of the calculus in [6], that features mobile names,
distributed principals and primitives for secure remote communication, without
any reference to explicit cryptography. The calculus is equipped with a system
of types and eﬀects which disciplines the use of the messaging abstractions to
provide static guarantees of strong secrecy and strong authenticity in the pres-
ence of a Dolev-Yao intruder. Strong secrecy is formalized in terms of behavioral
⋆Work partially supported by MIUR Projects SOFT “Security Oriented Formal Tech-
niques” and IPODS “Interacting Processes in Open-ended Distributed Systems”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 23–40, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

24
M. Bugliesi, S. Calzavara, and D. Macedonio
equivalence in the style of Abadi’s seminal work [1]. Authenticity, in turn, is
proved by establishing a form of injective agreement between statements and
expectations in the style of [13].
The novelty with respect to existing security type systems is in the choice of
our secrecy and authenticity types. Speciﬁcally, the type system makes it possible
to characterize authenticity directly as a property of the data exchanged during
a protocol rather than indirectly by extracting and interpreting the eﬀects the
protocol has on that data. Our types are inspired by the formulas of BAN Logic
[8], and retain much of the simplicity and intuitive reading of such formulas;
their use for type-checking similarly inherits the declarative style of BAN Logic’s
deductive system. The simple structure of the types, and the relative ease of
type-checking also derive from the high-level nature of the underlying process
calculus and its messaging primitives, that encapsulate and abstract away all
cryptographic details. To illustrate, we may write a principal speciﬁcation of the
form1:
receive (x : From(p)) on a.P
to mean that we expect a piece of data on channel a from principal p, and be
guaranteed statically that, in all protocol runs to which this principal partici-
pates, any piece of data received on a at this step does indeed come from p.
In spite of their simplicity, the calculus and the type system are rather ex-
pressive and support a wide range of distributed protocol idioms. We exemplify
the practical eﬀectiveness of our approach by showing the type system at work
on the security analysis of a variant of the 2KP e-payment protocol [5].
Plan. §2 presents the calculus, §3 the system of types and eﬀects. §4 details the
secrecy and authenticity properties enforced by the system. §5 presents the e-
payment protocol case study, and §6 concludes the presentation. We omit proofs
for lack of space (see [9] for details).
2
The Calculus
Syntax. We presuppose three countable sets of disjoint symbols, for princi-
pal identities p, q, r, (channel) names a, b, c and variables w, x, y, z. We let m, n
range over all names and u, v over names or variables when the distinctions are
immaterial. Tuples are indicated by a tilde, as in ˜n, ˜x, ˜v.
The calculus is a variant of the calculus from [7], where we structure the syntax
in two layers, for networks and processes, and introduce the explicit network form
pP to note the process P running on behalf of the principal identity p.
Networks
M, N, O ::= pP
(Principal)
|
M|N
(Parallel)
|
(νa : τ)M
(Restriction)
|
0
(Empty)
1 The actual syntax of our calculus is diﬀerent from the one used in this illustrative
example.
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
25
Processes
P, Q, R ::= u@v⟨∗: ˜u⟩◦
(Output)
|
a(u : ˜x)◦.P
(Input)
|
0
(Inaction)
|
P|Q
(Parallel)
|
test(u = v) then P else Q (Choice, test ∈{if,check})
|
rec X.P
(Recursion)
|
X
(Recursion Var)
|
(νa : τ)P
(Restriction)
The null, parallel composition, and recursive forms are just as in the pi-calculus.
Conditionals are also like pi-calculus matching: for typing purposes, however, we
use a special syntax to express the matching that corresponds to nonce-checks.
The restriction operators (both for processes and networks) have the familiar
pi-calculus syntax, but weaker scoping rules (see below); they are annotated
with tagged types to be described in § 3. As to input/output, we have various
messaging forms, depending on the instantiation of ∗, u and ◦. The notation ∗
stands for ∗or ’−’ (u similarly abbreviates u or ’−’), where ’−’ can be interpreted
as an anonymous identity; instead, ◦is short for • or the empty string ε. The
intuitive reading is as follows: u@v⟨∗: ˜u⟩◦denotes an output on channel u
directed to principal v with payload ˜u; the payload is certiﬁed as originating
from the sender if ∗is ∗, and it is secret for the receiver v if ◦is •. Dually,
a(u : ˜x)◦.P denotes an input on a from principal u of the payload ˜x. As we
detail below, input and output must agree on the secrecy and authenticity tags
to synchronize. Note, ﬁnally, that the subject of an input must be a channel (not
a variable): like in the local pi-calculus [18], we thus disallow the transmission of
the input capability on any channel. The notions of free and bound names and
variables arise as expected.
Semantics. We formalize the semantics of networks via a labelled transition
system. Following [2] (and the MIM semantics of [6]) in our transition system
two principals can never synchronize directly; rather, every interaction requires
the mediation of the intruder, which intercepts each message exchanged and then
delivers it to the recipient when the recipient is ready to consume it. To formalize
the dynamics of message interception, we extend the syntax of networks with an
additional, run-time process form to represent the copies of the messages stored
(or cached) upon interception:
M, N ::= . . . as above . . . | c@q⟨p : ˜m∥˜n⟩◦
i
Each intercepted output is cached at a fresh index i which is generated upon
interception and remains available as a reference to the cached output. The
cached output exhibits two views of the message content: the actual payload ˜m,
and the view of the payload ˜n as available to an external observer. As we discuss
below, the two views diﬀer when the intercepted output bears the secrecy tag
◦= •.

26
M. Bugliesi, S. Calzavara, and D. Macedonio
Table 1. Labelled Transition System
(Input)
σ = { ˜m/˜x}
pc(q : ˜x)◦.P
c@p(q: ˜
m)◦
−−−−−−−→pPσ
(Plain Output)
p ̸= q,
p = p iﬀ∗= ∗,
i fresh
pc@q⟨∗: ˜m⟩
(i)c@q⟨p: ˜
m⟩i
−−−−−−−−→c@q⟨p : ˜m∥˜m⟩i
(Secret Output)
p ̸= q,
p = p iﬀ∗= ∗,
|˜n| = | ˜m|,
i, ˜n fresh
pc@q⟨∗: ˜m⟩•
(i,˜n:Public)c@q⟨p:˜n⟩•
i
−−−−−−−−−−−−−−→c@q⟨p : ˜m∥˜n⟩•
i
(Open Process)
(νa : τ)pP
α
−→N
p(νa : τ)P
α
−→N
(Open Network)
N
(i,˜b:˜τ)c@q⟨p: ˜
m⟩◦
i
−−−−−−−−−−−→N ′, a ∈{ ˜m, c} \ {˜b, i}
(νa : τ)N
(i,˜b:˜τ,a:τ)c@q⟨p: ˜
m⟩◦
i
−−−−−−−−−−−−−→N ′
(Forward)
N ≡(ν˜a : ˜τ)( ˆ
N|c@q⟨p : ˜m∥˜n⟩◦
i ),
ˆ
N
c@q(p: ˜
m)◦
−−−−−−−→N ′
N
(i)
−−→(ν˜a : ˜τ)N ′
(Replay)
N ≡(ν˜a : ˜τ)( ˆN|c@q⟨−: ˜m∥˜n⟩◦
i ),
ˆ
N
c@q(−: ˜
m)◦
−−−−−−−→N ′
N
(i)
−−→(ν˜a : ˜τ)(N ′|c@q⟨−: ˜m∥˜n⟩◦
i )
(Test True)
pP
α
−→pR
ptest (m = m) then P else Q
α
−→pR
(Test False)
pQ
α−→pR,
m ̸= n
ptest (m = n) then P else Q
α−→pR
(Recursion)
pP{rec X.P/X}
α−→pP ′
prec X.P
α−→pP ′
(New)
N
α
−→N ′,
a /∈n(α)
(νa : τ)N
α−→(νa : τ)N ′
(Parallel Process)
pP|pQ
α−→N
pP|Q
α−→N
(Parallel Network)
M
α−→M ′,
bn(α) ∩fn(N) = ∅
M|N, N|M
α−→M ′|N, N|M ′
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
27
The labelled transitions are collected in Table 1. We comment on the in-
put/output and replay/forward rules, the remaining rules are standard. The
(Input) rule allows a principal to input values from the network and proceed af-
ter propagating the bindings for the input variables to the continuation process.
The (Output) rules formalize the interplay between output and interception. An
output generates a label and caches a copy of the intercepted message at a fresh
index. The label shows the view of the output available to an observer, while
the cached copy holds both the internal and the external view of the payload.
If the output is plain, the two views coincide; if it is secret, the external view is
a tuple ˜n of fresh names: as discussed in [7], this corresponds to assume an im-
plementation of a secret output in terms of randomized encryption. The (Open)
rules implement the scope extrusion mechanisms: notice that channel names are
always extruded in an output. All bound names (bar the intercept indexes) in
the output labels come with associated type annotations: these bear no com-
putational/observational meaning, and are only convenient when we formalize
the properties of the type system. The (Replay) and (Forward) rules allow two
principal to synchronize via the intruder by transmitting the cached outputs to
their intended receivers. If the original output was certiﬁed, the cached copy
gets consumed when used; non-certiﬁed outputs, instead, may be replayed back
to an input-ready principal any number of times. It is worth noticing that pri-
vate (restricted) names exchanged via a secret output are never exposed to an
observer, unless of course the receiver leaks them. To see that, notice that the
exchange arises as a result of a (Secret Output) transition, which does not expose
the private names of its payload, followed by a (Replay)/(Forward) transition,
which only exhibits the index of the intercepted message. The secret exchange
of restricted names is still possible, and achieved in (Forward) and (Reply), re-
lying on structural congruence for scope extrusion. The deﬁnition of structural
congruence is standard, and omitted for brevity.
3
The Type System
Types, eﬀects and type environments. The type system is built around
types (T, U, V ) and eﬀects (E, F), both assigned to values and variables. The
structure and intuitive reading of types is as follows:
– Public: values that are known or can be leaked publicly;
– Secret(˜u): values that must be kept secret among the principals in ˜u;
– Any: values with unknown status, might be either Public or Secret;
– Chan⟨˜T ; ˜U⟩: channels with payload a tuple of type ˜T, ˜U;
– Prin: principal identities.
When occurring in a type (and later in an eﬀect), the notation ˜u indicates a set
rather than a tuple. In a channel type Chan⟨˜T ; ˜U⟩, we assume that each U ∈˜U
is a Secret type, while each T ∈˜T is a type other than Secret. Secret types allow
us to deﬁne groups of secrecy (much in the spirit of the work in [10]) and these,
in turn, will be instrumental in deriving authenticity judgements. Any plays the

28
M. Bugliesi, S. Calzavara, and D. Macedonio
Table 2. Types and environments formation
(Good Type)
Γ; Δ ⊢⋄,
τ consistent,
fn(τ) ∪fv(τ) ⊆dom(Γ) ∪dom(Δ)
Γ; Δ ⊢τ
(Empty)
∅; ∅⊢⋄
(Type)
Γ; Δ ⊢T,
u /∈dom(Γ)
Γ, u : T; Δ ⊢⋄
(Eﬀect)
Γ; Δ ⊢˜E,
u /∈dom(Δ)
Γ; Δ, u : ˜E ⊢⋄
same role as in [1]: values with this type must be protected as secrets, but cannot
be used as secrets, because they might in fact be public.
As to eﬀects, their purpose is to encode time-dependent information about
values: as such, unlike types, they are not invariant through reduction. Their
syntax and intuitive reading is as follows:
– Fromp(˜u): values received by p coming from any of the principals ˜u;
– Freshp(˜u): fresh values received by p coming from any of the principals ˜u;
– Withp(˜u): values received by p in a message with payload ˜u;
– Nonce(p) and Checked(p): nonces created/checked2 by p.
The eﬀects From, Fresh and With are only associated with variables (not names
or identities), as they express properties that pertain to the transmission of a
name (hence to the variable where the name gets received) rather than to the
name itself. The eﬀects Nonce and Checked, in turn, help to single out the steps
of nonce veriﬁcation.
A type T and a set of eﬀects ˜E can be composed to form what we call
a tagged type, noted T/ ˜E. We let τ range over tagged types, and use T and
T/∅interchangeably, as we do for ˜E and Any/ ˜E. Also, we let τ.T and τ.E de-
note the type and eﬀect components of a tagged type, respectively. Through-
out, we assume that the eﬀect sets in a tagged type are consistent, that is
they do not contain more that one Nonce and/or Checked element. Further,
we introduce the following notion of generative types, i.e., of types that may
legally be associated with fresh names: T/ ˜E is generative for a principal p iﬀ
T ∈{Public, Secret(˜u), Chan⟨˜U; ˜V ⟩} and ˜E ⊆{Nonce(p)}; τ is generative for
a network M iﬀM ≡(ν˜a : ˜τ)(pP | M ′) and τ is generative for p.
The type system derives various forms of typing and subtyping judgements.
The type environments for these judgements consist of two components, Γ and
Δ, mapping names and variables to types and (sets of) eﬀects, respectively. All
environments occurring in a typing judgement must be well-formed, according to
the rules in Table 2. We make the implicit assumption that the eﬀects associated
with a variable do not include Nonce elements, and dually, that the eﬀects given
2 Although, strictly speaking, Nonce and Checked are not time-dependent, it is tech-
nically convenient to treat them as eﬀects.
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
29
Table 3. Ordering on types and eﬀects
(IdPublic)
Prin ≤Public
(ChanPublic)
Chan⟨˜T; ˜U⟩≤Public
(FreshFrom)
Freshp(˜v) ≤Fromp(˜v)
(NonceChecked)
Nonce(p) ≤Checked(p)
(ContraWith)
˜u ⊆˜v
W ithp(˜v) ≤W ithp(˜u)
(CoFresh)
˜v ⊆˜u
Freshp(˜v) ≤Freshp(˜u)
(CoFrom)
˜v ⊆˜u
Fromp(˜v) ≤Fromp(˜u)
(EﬀectSet)
∀F∈˜F. ∃E∈˜E. E ≤F
˜E ≤˜F
to names only include Nonce and Checked elements. We use the notation ˜v : ˜τ
for v1 : τ1, . . . , vn : τn and ˜v : τ for v1 : τ, . . . , vn : τ. In addition, we introduce
the following notation to single out various components of a type environment:
Nonces(Γ; Δ) = {n | Γ; Δ ⊢n : Nonce(p)}
Secrets(Γ; Δ) = {a | Γ; Δ ⊢a : Secret(˜u)}.
Ordering on types and eﬀects. Types and eﬀects are organized in the pre-
order relation deﬁned by the rules in Table 3. Rules (IdPublic) and (ChanPublic)
imply that trusted identities are public, and so are channel names, which indeed
are always leaked upon output. Rule (FreshFrom) states that Freshp(˜v) has
stronger authenticity guarantees than Fromp(˜v). Rule (NonceChecked) states
that a new nonce can safely be promoted to the status “checked”: this is sound,
as in our our system the eﬀect Nonce may only be assigned to names, not to
variables. The pre-order on eﬀects is lifted to sets of eﬀects using the standard
upper powerset (or Egli-Milner) construction: the resulting relation is still a
pre-order with set union as (total) meet operator. The remaining rules in the
table are self-explanatory. In addition, we deﬁne Any as the top element of the
pre-order (on types) and stipulate that T/ ˜E ≤T ′/ ˜E′ iﬀT ≤T ′ and ˜E ≤˜E′.
Finally, we let τ = τ′ if and only if τ ≤τ′ and τ′ ≤τ.
Typing of values. The typing rules for values are collected in Table 4. The
(Domain) and (Subsumption) rules are standard, and (Tag) is self-explanatory.
The two correlation rules are inspired by BAN Logic [8]: (Correlation Fresh)
states that if y is a nonce checked by p, then any name received by p with y
must be fresh; (Correlation From) states that if y is a shared secret between p
and ˜u, then any name received by p with y must come from a principal in ˜u.
The (Combine) rule is used to gather the information inferred by the correlation
rules.

30
M. Bugliesi, S. Calzavara, and D. Macedonio
Table 4. Typing of values
(Domain)
Γ; Δ ⊢⋄,
Γ(u) = τ ∨Δ(u) = τ
Γ; Δ ⊢u : τ
(Subsumption)
Γ; Δ ⊢u : τ,
τ ≤τ ′,
Γ; Δ ⊢τ ′
Γ; Δ ⊢u : τ ′
(Tag)
Γ; ∅⊢u : T,
∅; Δ ⊢u : ˜E
Γ; Δ ⊢u : T/ ˜E
(Correlation Fresh)
Γ; Δ ⊢y : Checked(p),
Γ; Δ ⊢x : Withp(y)
Γ; Δ ⊢x : Freshp
(Combine)
Γ; Δ ⊢x : Freshp,
Γ; Δ ⊢x : Fromp(˜u)
Γ; Δ ⊢x : Freshp(˜u)
(Correlation From)
Γ; Δ ⊢y : Secret(p, ˜u),
Γ; Δ ⊢x : Withp(y)
Γ; Δ ⊢x : Fromp(˜u)
Typing of processes. Processes are always type-checked with respect to a
principal identity. The typing judgement for processes has the form Γ; Δ ⊢p P,
where p is intended to be the identity of the principal running P. We start
illustrating the typing of processes with the rules for input/output in Table 5.
Rule (Public Output) governs the communication of public values, which is
legal provided that the type of the payload is consistent with the type expected
by the channel. Note that the output can be either plain or secret. The (Secret
Output) rule requires both the sender and the receiver to be part of the secrecy
group declared for each secret included in the message.
As for input, in all rules the continuation process is type-checked against an
environment that stores the interdependence of each input variable with all the
remaining components of the message received. If the sender is unknown, the
payload type declared by the channel is ignored, as the message may come from
the intruder. In absence of adequate guarantees about the sender, the input
variables associated to public positions can be safely treated as public, while
those associated to secret positions must be given type Any (as they might be
secret, if the sender is well-typed, or anything when the sender is untyped). On
the other hand, if the sender of a message is a known principal, the receiver can
trust the payload type of the channel.
The last two rules in Table 5 involve the conditional forms. Following [15],
the (If) rule exploits the equality between u and v to reﬁne the types of the two
values in the typing for the then branch. The notation Γ ⊓u : V indicates the
environment Γ, u : V if u /∈dom(Γ), otherwise the environment Γ \ {u : U}, u :
U ⊓V , where ⊓is the partial meet operator on types. Notice that we only
reﬁne the types of u and v and not their eﬀects, as reﬁning the eﬀects would be
unsound. To illustrate, given the assumption u : Fromp(q), associating the same
eﬀect to v would be unsound, as v might come from another principal r, even
though u = v. The (Check) rule implements a nonce veriﬁcation mechanism, and
involves two operations on the eﬀect environment Δ: Δ⊓u : ˜E is deﬁned similarly
to Γ ⊓u : U, whereas Δ[n : ˜E] changes the current association of n to ˜E. The
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
31
Table 5. Typing of processes: input/output and conditionals
(Public Output)
Γ; Δ ⊢u : Chan⟨˜T; ∅⟩,
Γ; Δ ⊢˜u : ˜T,
Γ; Δ ⊢v : Prin
Γ; Δ ⊢p u@v⟨∗: ˜u⟩◦
(Secret Output)
Γ; Δ ⊢u : Chan⟨˜T; ˜U⟩,
∀i (Ui = Secret(˜vi) ∧p, v ∈˜vi),
Γ; Δ ⊢˜u : ( ˜T, ˜U),
Γ; Δ ⊢v : Prin
Γ; Δ ⊢p u@v⟨∗: ˜u⟩•
(Non-certiﬁed Input)
Γ; Δ ⊢a : Chan⟨˜T; ˜U⟩,
|˜x| = | ˜T| ∧|˜y| = | ˜U|,
Γ, ˜x : Public, ˜y : Any; Δ, ˜x : W ithp(˜x, ˜y), ˜y : W ithp(˜x, ˜y) ⊢p P
Γ; Δ ⊢p a(−: ˜x, ˜y)◦.P
(Trusted Input)
Γ; Δ ⊢a : Chan⟨˜T; ˜U⟩,
Γ; Δ ⊢u : Prin,
|˜x| = | ˜T| ∧|˜y| = | ˜U|,
Γ, ˜x : ˜T, ˜y : ˜U; Δ, ˜x : {Freshp(u), W ithp(˜x, ˜y)}, ˜y : {Freshp(u), W ithp(˜x, ˜y)} ⊢p P
Γ; Δ ⊢p a(u : ˜x, ˜y)◦.P
(If)
Γ; ∅⊢u : U,
Γ; ∅⊢v : V,
Γ ⊓u : V ⊓v : U; Δ ⊢p P,
Γ; Δ ⊢p Q
Γ; Δ ⊢p if (u = v) then P else Q
(Check)
Δ(n) = Nonce(p),
Γ; ∅⊢u : U,
Γ; ∅⊢n : V,
Γ ⊓u : V ⊓n : U; (Δ ⊓u : Checked(p))[n : Checked(p)] ⊢p P
Γ; Δ ⊢p Q
Γ; Δ ⊢p check (u = n) then P else Q
intuition underlying nonce veriﬁcation is the following: consider the process c(−:
x, y).check (x = n) then P else Q, run by the principal p, where n has the eﬀect
Nonce(p). Then, the continuation P is type-checked in an environment where
n’s eﬀect is consumed, and turned to Checked(p), and x is deemed Checked(p).
Now, since y : With(x, y), we can infer y : Freshp.
The remaining rules for processes and networks are reported in Table 6.
The (Parallel) rule splits the Δ part of the environment between the paral-
lel processes P and Q to avoid that the same nonce is checked by both P
and Q; the disjoint union Δ1 ⊎Δ2 indicates the environment Δ1 ⊓Δ2 in case
Nonces(Δ1) ∩Nonces(Δ2) = ∅, and is undeﬁned otherwise. The condition
Nonce(Γ; Δ) = ∅in (Recursion) similarly ensures that a nonce is never checked
more than once: it is necessary since the body of a recursive process can be

32
M. Bugliesi, S. Calzavara, and D. Macedonio
Table 6. Other typing rules of processes and networks
(New)
Γ, a : T; Δ′ ⊢p P,
T/ ˜E generative for p,
[Δ′ ≜if ˜E ̸= ∅then (Δ, a : ˜E) else Δ]
Γ; Δ ⊢p (νa : T/ ˜E)P
(Parallel)
Γ; Δ1 ⊢p P,
Γ; Δ2 ⊢p Q
Γ; Δ1 ⊎Δ2 ⊢p P|Q
(Recursion)
Nonces(Γ; Δ) = ∅,
Γ, X : Proc; Δ ⊢p P
Γ; Δ ⊢p rec X.P
(Principal)
Γ; Δ ⊢p P
Γ; Δ ⊢pP
(Cache)
Γ; Δ ⊢⋄
Γ; Δ ⊢c@q⟨p : ˜m∥˜n⟩◦
i
(Network Parallel)
Γ; Δ1 ⊢M,
Γ; Δ2 ⊢N
Γ; Δ1 ⊎Δ2 ⊢M|N
(Zero)
Γ; Δ ⊢⋄
Γ; Δ ⊢p 0
(Proc)
Γ; Δ ⊢X : Proc
Γ; Δ ⊢p X
(Network Zero)
Γ; Δ ⊢⋄
Γ; Δ ⊢0
(Network New)
Γ, a : T; Δ′ ⊢M,
T/ ˜E generative for M,
[Δ′ ≜if ˜E ̸= ∅then (Δ, a : ˜E) else Δ]
Γ; Δ ⊢(νa : T/ ˜E)M
instantiated multiple times, but it is type-checked only once [17]. In the (New)
rules we restrict the possible types for new names to generative tagged types to
control the way new names are introduced at run-time. The remaining rules are
fairly standard.
4
Properties of the Type System
We start our analysis of the type system properties with subject reduction.
4.1
Subject Reduction
This is a standard result, but its formulation in our type system is more elab-
orate than usual, due to the structure of our types and eﬀects and our LTS
characterization of the operational semantics.
Substitution. As we already said, types are preserved during computation,
while eﬀects are not. In particular, the authenticity eﬀects From, Fresh and
With associated with an input variable are not preserved by the substitution of
that variable with the name received.
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
33
This is reﬂected in the following formulation of the substitution lemma, which
as usual is crucial in the proof of subject reduction. Given a set of eﬀects ˜E, let
| ˜E| denote the eﬀect-erasure of E, that is the subset of E resulting from erasing
all occurrences of the eﬀects From, Fresh and With from ˜E.
Lemma 1 (Substitution)
(i) If Γ, x : T ⊓Γ ′; Δ, x : ˜E ⊓Δ′ ⊢u : U/ ˜F and Γ; Δ ⊢n : T/| ˜E|, then
Γ ⊓Γ ′{n/x}; Δ ⊓Δ′{n/x} ⊢u{n/x} : (U/| ˜F|){n/x};
(ii) If Γ, x : T ⊓Γ ′; Δ, x : ˜E ⊓Δ′ ⊢p P and Γ; Δ ⊢n : T/| ˜E|, then
Γ ⊓Γ ′{n/x}; Δ ⊓Δ′{n/x} ⊢p P{n/x}.
Based on this result, we may show that typing is preserved by each of the tran-
sitions in our LTS. Since some of the transitions may introduce fresh names,
typing the derivative of a transition requires a new type environment that de-
pends on the transition itself. We use the notation (Γ; Δ) after α to refer to
such environments. Given a judgement Γ; Δ ⊢M and a transition M
α−→M ′,
the environment (Γ; Δ) after α has a straightforward, but lengthy deﬁnition3
(cf. Appendix A). To illustrate, assume Γ; Δ ⊢M and consider a transition
M
c@p(−: ˜m,˜n)
−−−−−−−−→M ′, with Γ; Δ ⊢c : Chan⟨˜T ; ˜U⟩, and | ˜m| = | ˜T |, |˜n| = | ˜U|. Then
(Γ; Δ) after α is the environment Γ ⊓˜m : Public ⊓˜n : Any; Δ, which reﬁnes the
information on the types of the names ˜m and ˜n received.
Admissible transitions. A ﬁnal technical subtlety is that the construction of
the type environment (Γ; Δ) after α may fail (and the resulting environment be
undeﬁned). This may happen, for instance, after the input transition discussed
above, as the meet operation involved in the construction of (Γ; Δ) after α fails
if the names ˜m transmitted on c are already known to the environment at a
Secret type (Secret ⊓Public is undeﬁned). We rule out such transitions as non-
admissible, in the following sense.
Deﬁnition 2 (Admissible Transition). We say that a transition M
α−→M ′
is admissible for Γ; Δ, written Γ; Δ ⊢α, if, whenever α = c@q(p : ˜m, ˜n) with
Γ; Δ ⊢c : Chan⟨˜T; ˜U⟩and | ˜m| = | ˜T|, |˜n| = | ˜U|, one has ˜m ∩Secrets(Γ; Δ) = ∅
and p = −.
If M
α−→M ′ is admissible for (Γ; Δ), it is easy to show that (Γ; Δ) after α is
always deﬁned, as the deﬁnition rules out the transitions that would pass secrets
for the non-secrets positions of an input preﬁx.
Theorem 3 (Subject Reduction). Assume Γ; Δ ⊢M, and let M
α−→M ′ be
admissible for Γ; Δ. Then (Γ; Δ) after α ⊢M ′.
3 The notation is loose here, as indeed the deﬁnition needs to consider the reduction
step M
α
−→M ′ and not just the transition label α: hence, strictly speaking, we should
rather say (Γ; Δ) after α in M
α−→M ′.

34
M. Bugliesi, S. Calzavara, and D. Macedonio
It is important to remark that the restriction to admissible transitions does
not involve any loss of expressive power for the attacker. Notice to this regard
that well-typed principals only have admissible transitions and, by Proposition 5
below, we know that they never leak their secrets. Hence, restricting to admissible
transitions only amounts to assume that the attacker cannot impersonate any
trusted principal, and does not know the initial secrets shared by the principals.
This is a sound initial assumption for any network and, by Theorem 3, it is a
property that is preserved by well-typed networks (there is no circularity here,
as the proof of Proposition 5 does not rely on Theorem 3).
4.2
Secrecy
We ﬁrst show that well-typed networks do not leak their secrets. Following [10],
we ﬁrst deﬁne what it means to leak an unrestricted secret.
Deﬁnition 4 (Revelation). Let N≡N ′ | pc@q⟨p : ˜m⟩◦| P and take (Γ; Δ)
and s such that Γ; Δ ⊢N and Γ; Δ ⊢s : Secret(˜r). We say that N reveals s iﬀ
s = c, or s ∈˜m and either q ̸∈˜r or ◦̸= •. We say that N reveals a secret of
(Γ; Δ) if N reveals s for some s ∈Secrets(Γ; Δ).
The deﬁnition readily extends to the general case when a secret may be re-
stricted. Let Γ; Δ ⊢N with N≡(ν˜a : ˜τ)N ′. N leaks a secret iﬀN ′ reveals a
secret of Γ, ˜a : ˜τ.T ; Δ, ˜a : ˜τ.E. In other words, a network leaks a secret whenever
it either outputs it in clear, or sends it to a principal outside the secrecy group,
or uses it as a channel (the name of a channel is always leaked upon output).
Proposition 5 (Group Secrecy). Assume Γ; Δ ⊢M. Then M does not leak
any secret.
The proof of this proposition follows directly by an inspection of the typing
rules. By Theorem 3, we then know that well-typed networks do not leak their
secrets at any step of computation. Indeed, as we show next, the type system
provides stronger secrecy guarantees, in that it prevents any (possibly) implicit
or indirect ﬂow of secret information. As in [1] we formalize strong secrecy in
terms of behavioral equivalence, which in turn we deﬁne based on the bisimilarity
relation that results from our LTS semantics. Given a transition M
α−→M ′ we
let ˆα denote α with the type annotations stripped away. Again, we restrict to
admissible transitions, with respect to the secrecy assumptions provided by a
typing environment.
Deﬁnition 6 (Bisimilarity). A symmetric relation R between networks is a
(Γ; Δ)-bisimulation if whenever M R N and M
α−→M ′ with Γ; Δ ⊢α, there
exists N ′ such that N
α′
−→N ′ with Γ; Δ ⊢α′, ˆα = ˆα′ and M ′ R N ′. (Γ; Δ)-
bisimilarity, noted ∼(Γ;Δ), is the largest (Γ; Δ)-bisimulation.
Theorem 7 (Strong Secrecy). Let Γ; Δ ⊢N. Then N ∼(Γ;Δ) Nσ for all
injective substitutions σ of the names in Secrets(Γ; Δ).
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
35
Notice the technical diﬀerence from the original characterization in [1]: in that
case secrecy is characterized as the inability to tell networks apart based on the
names of type Any they exchange. Our present formulation is directly based on
secret names, instead.
4.3
Authenticity
As anticipated at the outset, we formalize authenticity by establishing a form
of injective agreement between statements and expectations in the style of [13].
We start by introducing a new construct to express the authenticity expectations
about an input variable. The syntax of processes is extended as follows, where
E ∈{Fromp(˜u), Freshp(˜u)}.
P, Q, R ::= . . . as in §2 . . . | expect⟨x : E⟩.P
The expect⟨·⟩preﬁx is not a binder, hence the variable x in expect⟨x : E⟩.P
must be bound by an enclosing input preﬁx. The preﬁx form expect⟨x : E⟩.P is
well-typed in a given type environment if the eﬀect expected for x is consistent
with the eﬀects associated with x in the given environment (and the continuation
P is well-typed). When x gets substituted by a name, as in expect⟨m : E⟩.P,
the preﬁx is disregarded (as names are never bound to authenticity eﬀects).
Table 7. Typing rules for expect⟨·⟩
Γ; Δ ⊢x : E,
Γ; Δ ⊢p P
Γ; Δ ⊢p expect⟨x : E⟩.P
Γ; Δ ⊢p P
Γ; Δ ⊢p expect⟨m : E⟩.P
At run-time, expectations play the role of assertions that express the authenticity
properties of a message exchange: speciﬁcally, the intention is to check that, in
any run of a process, each expectation, say expect⟨m : E⟩, can be associated with
a previous output of m that validates the authenticity property on the exchange
of m as expressed by the eﬀect E. In particular, an expectation of an authentic
message from p may not only be justiﬁed by a certiﬁed output from p (as one
would certainly expect), but also by an anonymous output that includes in its
payload a shared secret between the receiver and p, since secrets are not leaked
in well-typed networks. Similarly, an expectation of a fresh, authentic message
additionally requires the presence of a nonce in case the justifying output is
anonymous, since we are interested in preventing replay attacks.
To formalize the justiﬁcation mechanisms we just illustrated, we annotate
run-time network conﬁgurations so as to connect each expect⟨·⟩statement with
the input preﬁx that binds the variable predicated by the statement. Since the
run-time input transitions of a network are triggered by corresponding (Re-
play/Forward) transitions, we create the desired association by annotating the
expect preﬁx with the index i of the (i) transition induced by the input that

36
M. Bugliesi, S. Calzavara, and D. Macedonio
binds the variable predicated by the preﬁx. Note that, in doing so, we actually
link by the index i each received value predicated by an expect with a previous
output containing that value, as desired.
We formalize all this by introducing an indexed version of the LTS from §2,
which we derive by modifying the (Input), (Replay) and (Forward) rules in Table
1 as in Table 8, and introducing a new rule for the expect preﬁx.
Table 8. Indexed LTS
(Input)
σ = { ˜m/˜x}
pc(q : ˜x)◦.P
c@p(q: ˜
m)◦
i
−−−−−−−→pidx(P, i, ˜x)σ
(Expect)
pexpect(i)⟨m : E⟩.P
(i)p expects⟨m:E⟩
−−−−−−−−−−−−→pP
(Forward)
N ≡(ν˜a : ˜τ)( ˆ
N|c@q⟨p : ˜m∥˜n⟩◦
i ),
ˆ
N
c@q(p: ˜
m)◦
i
−−−−−−−→N ′
N
(i)
−−→(ν˜a : ˜τ)N ′
(Replay)
N ≡(ν˜a : ˜τ)( ˆ
N|c@q⟨−: ˜m∥˜n⟩◦
i ),
ˆ
N
c@q(−: ˜
m)◦
i
−−−−−−−→N ′
N
(i)
−−→(ν˜a : ˜τ)(N ′|c@q⟨−: ˜m∥˜n⟩◦
i )
The input label in the premise of the (Replay) and (Forward) rules now con-
veys the index i of the induced replay/forward transition, while the (Input) rule
states that, before applying the substitution σ, every expect⟨·⟩in the contin-
uation, whose variable will become instantiated, must be indexed with i. The
goal is achieved via a function idx, deﬁned by structural induction on processes
(cf. Appendix A). The indexed occurrences of expect⟨·⟩type-check just as the
unindexed occurrences (cf. Table 7).
We formalize the intended correspondence between expectations and output
messages by means of the following, somewhat elaborate deﬁnition.
Deﬁnition 8 (Justiﬁed Expectation). Given a type environment (Γ; Δ),
consider the sequence of reductions N0
α0
−→N1
α1
−→· · ·
αm−1
−−−−→Nm. Let then
(Γk; Δk) be the environment (Γ; Δ) after α0, . . . , αk−1 for each k ∈[1..m], where
αk is admissible for Γk; Δk, and let αj = (i)p expects⟨m : E⟩. We say that αj
is justiﬁed by αh (h < j) if αh = (i,˜b : ˜τ)c@p⟨q : ˜m⟩◦
i and either of the following
conditions holds:
– if E = Fromp(˜q), then either q ∈˜q or Nh+1 ≡(ν˜a : ˜τ)(N ′ | c@p⟨q : ˜m∥˜n⟩◦
i )
and there exists s ∈˜m such that Γh, ˜a : ˜τ.T ; Δh, ˜a : ˜τ.E ⊢s : Secret(p, ˜r)
with ˜r ⊆˜q;
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
37
– if E = Freshp(˜q), the conditions of the previous item hold and, when q = −,
we additionally require that Γh, ˜a : ˜τ.T ; Δh, ˜a : ˜τ.E ⊢n : Nonce(p) for some
n ∈˜m.
This notion of justiﬁcation is clearly inspired by the correspondence assertions
proposed in [19], and akin to similar deﬁnitions employed in companion type
systems for authentication and authorization [14,13]: an “expect” can in fact be
seen as an end and its corresponding output as a begin. However, note that the
previous deﬁnition allows us to characterize authenticity directly as a property
of the data exchanged during a protocol rather than indirectly by extracting and
interpreting the eﬀects the protocol has on that data.
Theorem 9 (Authenticity). Let Γ; ∅⊢N and N≡N0
α0
−→N1 · · ·
αm−1
−−−−→Nm
be a sequence of reductions with αk admissible for (Γk; Δk) (k ∈[1..m]). Then
every expect label in the reduction sequence is justiﬁed by a previous output label
in the sequence.
5
Typing a Variant of 2KP
We show the type system at work on a simpliﬁed variant of the e-payment
protocol 2KP [5] by IBM. This is a signiﬁcant example, because the protocol
relies on two diﬀerent authentication schemes: a signature mechanism and the
presentation of a shared secret. The latter kind of authentication is important,
as in real settings it is unlikely that every principal possesses a signing key.
We can describe the protocol as follows. The customer cust sends the de-
scription desc of the order to the merchant merc along the channel init. The
merchant checks that the description received is the expected one and creates a
new transaction identiﬁer tid. The merchant sends back the description, pack-
aged with tid, to the customer via channel invc, and then sends a request to the
acquirer bank along the channel req, providing tid and the price of the order.
At this stage, the customer allows the payment to the merchant by a commu-
nication along paym with the acquirer, where it provides tid, price and credit
card information can. Finally, the acquirer checks that the two requests (from
the customer and the merchant) agree and that the credit card details provided
by the customer are right. If so, the acquirer clears the transaction and sends
a notiﬁcation to the customer and the merchant. The protocol can be coded as
the network 2KP = Ncust | Nmerc | Nbank, where each Ni is deﬁned in Table 9.
We use composed conditional guards as syntactic sugar for a series of nested
conditionals. The 2KP network is well-typed under the following assumptions:
desc : Public, can : Secret(cust, bank), price : Secret(cust, merc, bank)
The type of the communication channels is immediately derived from the types of
the exchanged data, while cust, merc and bank must be given type Prin. Below,
we detail the most interesting aspects in the type derivation for the example.
First, since merc is able to certify its messages, bank can easily derive that
the expedition of ztid is eﬀectively from merc, since that variable is closed by a

38
M. Bugliesi, S. Calzavara, and D. Macedonio
Table 9. 2KP Protocol Speciﬁcation
Ncust ≜cust init@merc⟨−: desc⟩| invc(merc : xdesc, xtid).if (desc = xdesc)
then paym@bank⟨−: xtid, price, can⟩• | conf(bank : xauth).0 
Nmerc ≜merc init(−: ydesc).if (ydesc = desc)
then (ν tid : Public)(invc@cust⟨∗: ydesc, tid⟩|
req@bank⟨∗: tid, price⟩•) | resp(bank : yauth).0 
Nbank ≜bank req(merc : ztid, zprice)•.paym(−: z′
tid, z′
price, zcan)•.
if (can = zcan ∧ztid = z′
tid ∧zprice = z′
price)
then expect⟨ztid : Freshbank(merc)⟩.
expect⟨z′
tid : Frombank(cust)⟩.
(resp@merc⟨∗: ztid⟩| conf@cust⟨∗: ztid⟩) 
signed input. Once again, notice that this communication mode guarantees the
freshness of the message. Secondly, even though zcan is closed by a non-certiﬁed
input, the receiver can use that variable with type Secret(cust, bank) and not
with type Any to type-check the continuation, since zcan is checked against can
and the typing rule for the conditional branch reﬁnes the types for the considered
values upon a successful match. Finally, although cust has not a mean to certify
its messages, the expedition of z′
tid must come from cust, since z′
tid : With(zcan)
and zcan : Secret(cust, bank) by the previous point. Note that, even if z′
tid = ztid
and ztid is fresh, the type-checker cannot assert the freshness of z′
tid. Even if this
may seem limitative in this particular case, given the nature of the tid, this is
the sound choice to make, since in general it is unsound to infer the freshness of
a piece of data from its equality with a fresh value.
We remark that the type-checking is completely syntax-directed and com-
positional, yielding a rather eﬀective tool for protocol veriﬁcation. In spite of
this simplicity, the type system turns out to be quite expressive and ﬂexible,
using the correlation rules to infer authenticity information and the conditional
branches to reﬁne the types of the values.
6
Conclusions
The analysis of distributed systems built upon secure channel abstractions has
been subject of active research in the recent literature, based on various for-
malisms: model checking [3], CSP-style traces speciﬁcations [12], Strand spaces
[16], inductive veriﬁcation [4] and process calculi [2,11].
The present paper continues on the line of work initiated in [7,6], by intro-
ducing a type system to provide static security guarantees for the high-level
abstractions for distributed messaging proposed in those papers.
www.ebook3000.com

Secrecy and Authenticity Types for Secure Distributed Messaging
39
The type system enforces two main security guarantees – strong secrecy and
strong authenticity in the presence of a Dolev-Yao intruder – which are compa-
rable to those provided by companion type systems for security. The novelties
of our approach are mainly technical, but they also bear conceptual signiﬁcance.
In particular, the ability to characterize authenticity as a property of data itself,
which is distinctive of our type system, appears to constitute an important step
towards the integration of security types within typing systems for (semi) struc-
tured datatypes available in modern programming languages. This kind of inte-
gration within languages accommodating more structured interaction primitives
such as those available in session description languages and/or choreography
languages represent one of the lines of work we plan for our future research.
References
1. Abadi, M.: Secrecy by typing in security protocols. Journal of the ACM 46(5),
749–786 (1999)
2. Ad˜ao, P., Fournet, C.: Cryptographically sound implementations for communicat-
ing processes. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP
2006. LNCS, vol. 4052, pp. 83–94. Springer, Heidelberg (2006)
3. Armando, A., Carbone, R., Compagna, L., Cu´ellar, J., Tobarra, M.L.: Formal
analysis of saml 2.0 web browser single sign-on: breaking the saml-based single
sign-on for google apps. In: FMSE, pp. 1–10 (2008)
4. Bella, G., Longo, C., Paulson, L.C.: Verifying second-level security protocols. In:
Basin, D., Wolﬀ, B. (eds.) TPHOLs 2003. LNCS, vol. 2758, pp. 352–366. Springer,
Heidelberg (2003)
5. Bellare, M., Garay, J.A., Hauser, R., Herzberg, A., Krawczyk, H., Steiner, M.,
Tsudik, G., Herreweghen, E.V., Waidner, M.: Design, implementation, and de-
ployment of the iKP secure electronic payment system. IEEE Journal on Selected
Areas in Communications 18, 611–627 (2000)
6. Bugliesi, M., Focardi, R.: Channel abstractions for network security. Mathematical
Structures in Computer Science xx, xxx–xxx (2010)
7. Bugliesi, M., Focardi, R.: Language based secure communication. In: Proceedings of
the 21st IEEE Computer Security Foundations Symposium, CSF 2008, Pittsburgh,
Pennsylvania, June 23-25, pp. 3–16. IEEE Computer Society, Los Alamitos (2008)
8. Burrows, M., Abadi, M., Needham, R.M.: A logic of authentication. ACM Trans.
Comput. Syst. 8(1), 18–36 (1990)
9. Calzavara, S.: Security Types for Distributed Applications. Master’s thesis,
Universit`a Ca’ Foscari di Venezia (2009)
10. Cardelli, L., Ghelli, G., Gordon, A.D.: Secrecy and group creation. Inf. Com-
put. 196(2), 127–155 (2005)
11. Corin, R., Deni´elou, P.M., Fournet, C., Bhargavan, K., Leifer, J.J.: Secure im-
plementations for typed session abstractions. In: 20th IEEE Computer Security
Foundations Symposium, CSF 2007, Venice, Italy, July 6-8, pp. 170–186. IEEE
Computer Society, Los Alamitos (2007)
12. Dilloway, C., Lowe, G.: Specifying secure transport channels. In: CSF, pp. 210–223
(2008)
13. Fournet, C., Gordon, A.D., Maﬀeis, S.: A type discipline for authorization policies.
ACM Trans. Program. Lang. Syst. 29(5) (2007)

40
M. Bugliesi, S. Calzavara, and D. Macedonio
14. Gordon, A.D., Jeﬀrey, A.: Types and eﬀects for asymmetric cryptographic proto-
cols. Journal of Computer Security 12(3-4), 435–483 (2004)
15. Hennessy, M., Rathke, J.: Typed behavioural equivalences for processes in the
presence of subtyping. Mathematical Structures in Computer Science 14(5),
651–684 (2004)
16. Kamil, A., Lowe, G.: Specifying and modelling secure channels in strand spaces.
In: Workshop on Formal Aspects of Security and Trust, FAST (2009)
17. Kobayashi, N., Pierce, B.C., Turner, D.N.: Linearity and the pi-calculus. ACM
Trans. Program. Lang. Syst. 21(5), 914–947 (1999)
18. Merro, M., Sangiorgi, D.: On asynchrony in name-passing calculi. Mathematical
Structures in Computer Science 14(5), 715–767 (2004)
19. Woo, T.Y.C., Lam, S.S.: A semantic model for authentication protocols. In: IEEE
Symposium on Security and Privacy, vol. 178 (1993)
A
Appendix
The function after. The function (Γ; Δ) after α in (M
α−→M ′) is partially
deﬁned as follows:
– if α = c(−: ˜m, ˜n) with Γ; Δ ⊢c : Chan⟨˜T ; ˜U⟩, | ˜m| = | ˜T| and |˜n| = | ˜U|, then
(Γ; Δ) after α in (M
α−→M ′) = (Γ ⊓˜m : Public ⊓˜n : Any; Δ);
– if α = c(p : ˜m, ˜n) with Γ; Δ ⊢c : Chan⟨˜T; ˜U⟩, | ˜m| = | ˜T| and |˜n| = | ˜U|, then
(Γ; Δ) after α in (M
α−→M ′) = (Γ ⊓˜m : ˜T ⊓˜n : ˜U; Δ);
– if α = (i,˜b : ˜τ)c@p⟨q : ˜m⟩◦
i with bj : τj = Tj/ ˜Ej for each j, then
(Γ; Δ) after α in (M
α−→M ′) = (Γ,˜b : ˜T; Δ′)
with Δ′ = Δ ∪{bj : Nonce(q) | ˜Ej = Nonce(q)};
– if α = (i), then
(Γ; Δ) after α in (M
α−→M ′) = (Γ ⊓˜m : Public⊓˜n : Any; Δ) if i ∈fn(M ′),
(Γ; Δ) after α in (M
α−→M ′) = (Γ ⊓˜m : ˜T ⊓˜n : ˜U; Δ) otherwise.
The deﬁnition exploits the fact that a message cached at i is consumed by a
transition (i) if and only if it is signed by a principal.
The function idx. The function idx(P, i, ˜y) is deﬁned by induction on the
structure of the process P as follows:
– idx(expect⟨x : E⟩.P, i, ˜y) = expect(i)⟨x : E⟩.idx(P, i, ˜y), if x ∈˜y;
– idx(expect⟨x : E⟩.P, i, ˜y) = expect⟨x : E⟩.idx(P, i, ˜y), if x /∈˜y;
– idx(u@v⟨∗: ˜u⟩◦, i, ˜y) = u@v⟨∗: ˜u⟩◦;
– idx(a(u : ˜x)◦.P, i, ˜y) = a(u : ˜x)◦.idx(P, i, ˜y);
– idx(0, i, ˜y) = 0;
– idx(P|Q, i, ˜y) = idx(P, i, ˜y)|idx(Q, i, ˜y);
– idx(test(u = v) then P else Q, i, ˜y) = test(u = v) then idx(P, i, ˜y) else
idx(Q, i, ˜y);
– idx(rec X.P, i, ˜y) = rec X.idx(P, i, ˜y);
– idx(X, i, ˜y) = X;
– idx((νa : τ)P, i, ˜y) = (νa : τ)idx(P, i, ˜y).
www.ebook3000.com

Modular Plans for Secure Service Composition⋆
Gabriele Costa1,2, Pierpaolo Degano2, and Fabio Martinelli1
1 Istituto di Informatica e Telematica – Consiglio Nazionale delle Ricerche
2 Dipartimento di Informatica – Universit`a di Pisa
Abstract. Service Oriented Computing (SOC) is a programming
paradigm aiming at characterising Service Networks. Services are entities
waiting for clients requests and they often result from the composition
of many services.
We address here the problem of statically guaranteeing security of open
services, i.e. services with unknown components. Security constraints are
expressed by local policies that service components must obey.
We present here a type and eﬀect system that safely over-approximates,
in the form of history expressions, the possible run-time behaviour of open
services, collecting partial information on the behaviours of their compo-
nents. From a history expression, we then extract a plan that drives exe-
cutions that never rise security violations.
Finally, we show how partial plans satisfying security requirements
can be put together to obtain a safe orchestration plan.
1
Introduction
In the last decade, history-based security [1] has emerged as a powerful approach
to guarantee that program do not misbehave. Histories, i.e. execution traces, are
sequences of relevant actions generated by running programs. Hence, a security
policy consists in a distinction between accepted and rejected traces. A program
is considered secure if all its executions obey the given security policies. Many
techniques use histories for deciding whether a program is secure or not. Mainly,
two distinct methods exist: run-time enforcement and static veriﬁcation (e.g.
see [20,10,19,15]).
Brieﬂy, policy enforcement consists in applying a monitor to the running pro-
gram. Whenever the monitor target tries to perform an action, the current trace
is checked against the enforced policy. A security violation, i.e. an attempt to
extend the actual trace to a forbidden one, activates some emergency operation,
e.g. the program termination. Instead, static veriﬁcation aims to prove program
security regardless of any possible execution scenario. Roughly, one proves that
the program can only produce policy-compliant traces by only looking at the
program text. Some authors use a mixed approach (e.g. [17]) that combines a
static check with a dynamic one in case the ﬁrst fails. Often, security policies
⋆Work partially supported by EU-funded project “SENSORIA”, by EU-funded
project “CONSEQUENCE” and by EU-funded project “CONNECT”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 41–58, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

42
G. Costa, P. Degano, and F. Martinelli
are expressed as global safety properties of systems and in this case execution
monitors rely on ﬁnite state automata for detecting illegal traces.
Recently, [3] proposed a more ﬂexible approach in which so-called local policies
only guard portions of code and have a local scope. Originally, local policies have
been coupled with an extension on the λ-calculus used to specify services [4].
Then, also Java has been extended [2] with local policies showing the feasibility
of this approach on a real-world language.
In this paper, we use right-bounded local policies, a variant of those deﬁned
in [3]; for brevity we will omit in the following “right-bounded” whenever un-
ambiguous. The main diﬀerence between the two versions is that in the original
one an active policy is checked on the whole past execution history, while in our
proposal it is checked on the history produced since the policy is activated (see
Tab. 2). In the next section we shall introduce only intuitively usage automata
that are used to specify local policies. Usage automata are a variant of non-
deterministic ﬁnite state automata, parameterized over system resources. More
details and the formal deﬁnition are in [6,7].
The approach can be summarized as follows. The speciﬁcation of a system
already contains the local policies to be obeyed. Compliance is then established
by model-checking a suitable, safe abstraction of the system behaviour (for the
λ-calculus this is done through a type and eﬀect system, for Java through a
control ﬂow analysis).
Systems may behave diﬀerently according to how requests are resolved into
services. The association between requests and services, called plan, drives the
execution at run-time. If the plan determines an abstract behaviour passing the
model-checking phase (the plan is viable) then the original code can be run
securely with no monitoring.
The above approach only deals with closed networks, i.e. with services whose
components are fully speciﬁed a priori. However, services are built incrementally
and components may appear and disappear dynamically. It is important then to
also analyse open networks, i.e. networks having unspeciﬁed participants. As a
matter of fact, service-oriented paradigms aim to guarantee compositional prop-
erties and should be independent from the actual implementation of (possibly
unknown) parties. Moreover, closed networks orchestrated by a global, composi-
tion plan require to be completely reorganized whenever a service falls or a new
one rises.
Here, we extend the results of [4,5] on open networks. In particular, we will
single out partial plans that only involve parts of the known network and that
however can be safely adopted within any operating context. We also show how
these partial plans can be combined together, along with the composition of the
services they come from. As expected, composability of viable plans is subject
to some constraints, and we also outline a possible way to eﬃciently check when
these constraints are satisﬁed.
The paper is structured as follows. Section 2 introduces our motivating ex-
ample. To carry on our investigation in a pure framework, we use here an
extension of λreq[4], the syntax and semantics of which are in Section 3. Section 4
www.ebook3000.com

Modular Plans for Secure Service Composition
43
introduces history expressions and our type and eﬀect system. In Section 5 we
provide our results on plans compositionality and propose a way to eﬃciently
check their validity. Finally, Section 6 concludes the paper.
2
An Example
In this section we introduce our working example. Figure 1 shows a simple service
network for travel booking. Rounded boxes denote service locations (dashed
lines contain homogeneous services, i.e. services oﬀering similar functionalities).
Clients contact the travel agency providing a credit card number for payments
and receive back a receipt. Every instance of the travel agency books exactly
one room and one ﬂight. The responsibility of doing an actual reservation is
delegated to booking services. Each booking service receives a card number and
uses it for paying a reservation. Payment services are in charge for authorising
a purchase. A payment service charges the needed amount on the credit card
(possibly after checking some credentials of the card number), and returns TRUE.
Otherwise, it answers FALSE if a failure occurs.
Each service has its own security requirements expressed through a corre-
sponding usage automaton. For example, a booking service would like to “per-
form at least one availability check before each payment”. This is represented
by Fig. 2b: every action but check(x) and charge(x) is allowed and leaves the
automaton in state 0 (self-loops on actions not labelling existing arcs are omitted
in Fig. 2). After a check, any action is permitted in state 1, including charging.
Instead, a charge action in state 0 leads to the oﬀending state 2. Additionally,
booking service can be forbidden to check more than once (ϕBH). Similarly, the
travel agency can declare diﬀerent rules focussing on diﬀerent aspects, e.g. “never
book two times the same service (hotel or ﬂight)” (ϕTA).
Clients are unspeciﬁed, so the network is open. However, clients do not aﬀect
at all the security policies introduced so far. We can therefore check secure this
open network.
As a matter of fact, services only put security constraints on their own traces
and on those of the services they invoke. This is because our policies are
Travel Agency
Book −Here −F
Pay −With −Us
Pay −On −Line
Booking services
Payment services
Book −Here −H
Book −Now −F
Book −Now −H
Fig. 1. A travel booking network

44
G. Costa, P. Degano, and F. Martinelli
0
1
2
check(x)
check(x)
0
1
2
check(x)
charge(x)
(a) Book-Here policy (ϕBH)
(b) Book-Now policy (ϕBN)
0
unbook(s)
1
book(s)
book(s)
2
(c) Travel Agency policy (ϕTA)
Fig. 2. Security policies as usage automata (self-loops are omitted)
right-bounded and so their scope spans from their activation until their deac-
tivation points. Note that in this way, the services we model cannot discrimi-
nate against clients. Everyone can invoke a service, because service behaviour is
client-independent.
3
Service Network
The programming model for service composition based on λreq was ﬁrst intro-
duced in [4]. Its syntax resembles the classical call-by-value λ-calculus with two
main diﬀerences: security framing and a call-by-contract service request.
Service networks are set of services. Each service e is hosted in a location ℓ.
As expected, we assume that there exists a public service repository Srv. An
element of Srv has the form eℓ: τ
H
−→τ ′, where eℓis the code of the service, ℓis
a unique service location (sometimes used to denote the service itself), τ −→τ′
is the type of eℓ, and H is the eﬀect of eℓ(see Section 4.1).
3.1
Syntax
The syntax of λreq is in Table 1. The expression ∗represents a ﬁxed, closed, event-
free value. Resources, ranged over by r, r′, belong to ﬁnite, statically deﬁned
classes R, R′. Access events α, β are side eﬀects, parametrized over resources.
Function abstraction and application follow the classical syntax (we use z in
λzx.e as a binding to the function in e). Security framing applies the scope of
a policy ϕ to a program e. Service request requires more attention. We state
that services can not be directly accessed (for instance using a public name or
address). Clients invoke services through their public interface, i.e. their type.
The label ρ is a unique identiﬁer associated with the request. A policy ϕ is
attached to the request in a call-by-contract fashion: the invoked service must
www.ebook3000.com

Modular Plans for Secure Service Composition
45
Table 1. Syntax of λreq
e, e′ ::=
∗
unit
r
resource
x
variable
α(e)
access event
if g then e
else
e′
conditional
λzx.e
abstraction
e e′
application
ϕ[e]
security framing
reqρ τ
ϕ−→τ ′
service request
obey the policy ϕ. Since both τ and τ ′ can be higher-order types, we can model
simple value-passing interaction, as well as mobile code scenarios.
We use v to denote values, i.e. resources, variables, abstractions and requests.
Moreover, we introduce the usual abbreviations: λx.e = λzx.e with z ̸∈fv(e),
λ.e = λx.e with x ̸∈fv(e) and e; e′ for (λ.e′)e.
Here we make explicit the conditions of branching, similarly to [11]. Brieﬂy,
we check equality between resources, and we have negation and conjunction.
Deﬁnition 1. (Syntax of guards)
g, g′::=true | [¯x = ¯y] | ¬g | g∧g′
(¯x,¯y range over variables and resources)
We use false as an abbreviation for ¬true, [¯x ̸= ¯y] for ¬ [¯x = ¯y] and g ∨g′ for
¬(¬g∧¬g′). We also deﬁne as expected an evaluation function B mapping guards
into boolean values, namely {tt, ﬀ}. E.g. B([¯x = ¯x]) = tt and B([¯x = ¯y]) = ﬀ
if ¯x ̸= ¯y. In our model we assume resources to be uniquely identiﬁed by their
(global) name, i.e. r and r′ denote the same resource if and only if r = r′.
Moreover, we use [¯x ∈D] for 
d∈D[¯x = d].
Example 1. We specify the services Travel-Agency, Book-Here-S, Book-Now-S
(S ∈{F, H}), Pay-On-Line and Pay-With-Us in Fig. 1, through eTA, eBH−S, eBN−S,
ePOL and ePWU (resp.) as follows.
ePWU
= λx.if [x ∈C] then charge(x); TRUE else FALSE
ePOL
= λx.if [x ∈C′] then check(x); charge(x); check(x); TRUE else FALSE
eBH−S = λx.(λy.if [y = TRUE] then book(S) else ∗)
((reqρ Card
ϕBH
−−→Bool)x)
eBN−S = λx.book(S); (λy.if [y = FALSE] then unbook(S) else ∗)
((reqρ′ Card
ϕBN
−−→Bool)x)
eTA
= λx.ϕTA

(req¯ρ Card −→1)x; (req¯ρ′ Card −→1)x; rcpt


46
G. Costa, P. Degano, and F. Martinelli
Brieﬂy, ePWU receives a card number x, veriﬁes whether it is a registered one
(i.e. [x ∈C]) and charges on x. Service ePOL works similarly, but veriﬁes money
availability (event check) before and after the operation.
Booking services eBH−S require a payment and then, if it has been authorised,
book the hotel (ﬂight). On the contrary, eBN−S book the hotel (ﬂight) and then
cancel the reservation if the payment has been refused. Both eBH−S and eBN−S
require the behaviour of the invoked service to comply with the policies ϕBH
and ϕBN.
Finally eTA simply calls two booking service instances and releases a receipt
(rcpt of type Rec) to the client. Note that now the travel agency applies its
policy ϕTA to the sequential composition of service requests.
3.2
Operational Semantics
Clearly, the run-time behaviour of a network of services depends on the way they
interact. As we already mentioned, requests do not directly refer to a speciﬁc
service that will be actually invoked during the execution, but to its behaviour,
i.e. its type and eﬀect (deﬁned below), only. A plan resolves the requests by
associating them with services locations. Needless to say, diﬀerent plans lead
to diﬀerent executions. A service network can have many, one or even no valid
plans. A plan is said to be valid if and only if the executions it drives complies
with all the active security policies. More precisely, an execution trace η is valid
w.r.t. a policy ϕ, in symbols η |= ϕ, if and only if it is not a word in the language
of the security automaton of ϕ.
As expected, we assume that services can not be composed in a circular way.
This condition amounts to say that there exists a partial order relation ≺over
services. We deﬁne Srv⌋ℓ= {eℓ′ : τ ∈Srv | ℓ≺ℓ′} as the sub-network that can
be seen from a service hosted at ℓ.
A computational step of a program is a transition from a source conﬁguration
to a target one. In our model, conﬁgurations are pairs η, e where η is the execution
history, that is the sequence of action events done so far (ε being the empty one),
and e is the expression under evaluation. Actually, the syntax of histories and
expressions is slightly extended with markers [m
ϕ as explained in the comment to
the rule for framing. Note that the automaton for a policy ϕ will simply ignore
these markers.
Formally, a plan is a (partial) mapping from request identiﬁers (ρ, ρ′, . . .) to
service locations (ℓ, ℓ′, . . .) deﬁned as
π, π′ ::= ∅| {ρ 
→ℓ} | π; π′
An empty plan ∅is undeﬁned for any request, while a singleton {ρ 
→ℓ} is only
deﬁned for ρ. Plan composition π; π′ combines two plans. It is deﬁned if and only
if for all ρ such that ρ ∈dom(π)∩dom(π′) ⇒π(ρ) = π′(ρ), i.e. the same request
is never resolved by diﬀerent services. Two such plans are called modular.
www.ebook3000.com

Modular Plans for Secure Service Composition
47
Table 2. Operational semantics of λreq
(S−Ev1)
η, e →π η′, e′
η, α(e) →π η′, α(e′)
(S−Ev2)
η, α(r) →π ηα(r), ∗
(S−If)
η, if g then ett else eﬀ→π η, eB(g)
(S−App1)
η, e1 →π η′, e′
1
η, e1e2 →π η′, e′
1e2
(S−App3)
η, (λzx.e)v →π η, e{v/x, λzx.e/z}
(S−App2)
η, e2 →π η′, e′
2
η, ve2 →π η′, ve′
2
(S−Frm1)
η, ϕ[e] →π η[m
ϕ , ϕm[e]
m fresh
(S−Frm2)
η[m
ϕ η′, ϕm[v] →π ηη′, v
(S−Frm3)
η[m
ϕ η′, e →π η[m
ϕ η′′, e′
η′′ |= ϕ
η[m
ϕ η′, ϕm[e] →π η[m
ϕ η′′, ϕm[e′]
(S−Req)
e¯ℓ: τ
H
−→τ ′ ∈Srv⌋ℓ
π(ρ) = ¯ℓ
H |= ϕ
η, (reqρ τ
ϕ−→τ ′)v →π η, e¯ℓv
Given a plan π we evaluate λreq expressions, i.e. services, accordingly to the
rules of the operational semantics in Table 2. Actually, a transition should be
also labelled by the location ℓhosting the expression under evaluation. For read-
ability, we omit this label.
Brieﬂy, an action α(r) is appended to the current history (possibly after the
evaluation of its parameter), a conditional branching chooses between two pos-
sible executions (depending on its guard g) and application works as usual. The
rule (S−Frm1) opens an instance of framing ϕ[e] and records the activation in
the history with a marker [m
ϕ , and in the expression with ϕm[e] (to keep diﬀerent
instantiations apart we use a fresh m). The rule (S−Frm2) simply deactivates
the framing and correspondingly purges the relevant marker from the history.
The rule (S−Frm3) checks whether the m-th instantiation of ϕ is respected by
the history since it has been activated (recall that the usage automaton for ϕ
ignores the markers [m′
ϕ′ , where of course m′ ̸= m). This is how our right-bounded
local mechanism is implemented. A service request ﬁrstly retrieves the service e¯ℓ
that the current plan π associates with ρ, and that belongs to Srv⌋ℓ, i.e. visible
from the present evaluation location ℓ(omitted in the rule, as stipulated above).
Then, the eﬀect of the selected service is checked against the policy ϕ required
by the client. If successful, the service is ﬁnally applied to the value provided by
the client.
Example 2. Consider the service Book-Here-F as implemented in Example 1. If
it is invoked with parameter c ∈C starting from an empty trace, its execution
under the plan π = {ρ 
→PWU} is:

48
G. Costa, P. Degano, and F. Martinelli
ε, (eBH−F c)
→π ε, (λzy.if [y = TRUE] then book(F) else ∗)((reqρ Card
ϕBH
−−→Bool)c)
→π ε, (λzy.if [y = TRUE] then book(F) else ∗)((ePWU)c)
→π ε, (λzy.if [y = TRUE] then book(F) else ∗)
(if [c ∈C] then charge(c); TRUE else FALSE)
→π ε, (λzy.if [y = TRUE] then book(F) else ∗)(charge(c); TRUE)
→π charge(c), (λzy.if [y = TRUE] then book(F) else ∗)(TRUE)
→π charge(c), if [TRUE = TRUE] then book(F) else ∗
→π charge(c), book(F)
→π charge(c)book(F), ∗
Example 3. Let z = λzx.ϕ[α; z(x)] and let ϕ be the policy saying that “a single
α is allowed”. Then, we have the following computation (under the empty plan).
ε, (z ∗) →∅ε, ϕ[α; z(∗)] →∅[1
ϕ, ϕ1[α; z(∗)] →∅[1
ϕα, ϕ1[z(∗)]
Note that the last transition is possible because α |= ϕ. Then the computation
proceeds as follows.
→∅[1
ϕα, ϕ1[ϕ[α; z(∗)]] →∅[1
ϕα[2
ϕ, ϕ1
ϕ2[α; z(∗)]

This conﬁguration is stuck. Indeed, a further step would lead to the conﬁguration
[1
ϕα[2
ϕα, ϕ1
ϕ2[z(∗)]

. In spite of the fact that the second instance of ϕ is satisﬁed
by the suﬃx α of the history after the marker [2
ϕ, the ﬁrst instance is not:
α[2
ϕα ̸|= ϕ (recall that the marker [2
ϕ is invisible to ϕ).
4
Type and Eﬀect System
We now introduce the type and eﬀect system for λreq. Our system builds upon
[4] introducing two new rules: guarded eﬀects and a new typing rule, namely
strengthening.
4.1
History Expressions
History expressions are used to denote sets of histories. They are deﬁned through
the abstract syntax of Table 3, which extends the syntax introduced in [4].
A history expression can be empty (ε), a single access event to some resource
(α(r)) or it can be obtained either through sequential composition (H · H′) or
non-deterministic choice (H + H′). Moreover, we use safety framing ϕ[H] for
specifying that all the execution histories represented by H are under the scope
of the policy ϕ. Additionally, μh.H (where μ binds the free occurrences of h
in H) represents recursive history expressions. Finally, we introduce guarded
histories gH (where g respects the syntax of guards given in Def. 1).
www.ebook3000.com

Modular Plans for Secure Service Composition
49
Table 3. Syntax of history expressions
H, H′ ::=
ε
empty
h
variable
α(r)
access event
H · H′
sequence
H + H′
choice
ϕ[H]
security framing
μh.H
recursion
gH
guard
Table 4. Semantics of history expressions
εσ
δ = ∅
hσ
δ = δ(h)
α(r)σ
δ = {α(r)}
H · H′σ
δ = Hσ
δ H′σ
δ
H + H′σ
δ = Hσ
δ ∪H′σ
δ
gHσ
δ =
Hσ
δ
if σ |= g
∅
otherwise
ϕ[H]σ
δ = ϕ[Hσ
δ ]
μh.Hσ
δ = 
n>0
f n(!)
where f(X) = Hσ
δ{X/h}
The denotational semantics of history expressions (Table 4) maps a history
expression H to a set of histories H. The domain H is the lifted complete partial
order of sets of histories [21]. Sets are ordered by (lifted) inclusion ⊆⊥(where
∀H.⊥⊆⊥H and H ⊆⊥H′ whenever η ∈H ⇒η ∈H′).
We ﬁrst need a couple of standard, auxiliary notions. An environment δ binds
variables (h, h′, . . .) to history expressions (H, H′, . . .) and a substitution σ maps
variable names (x, y, . . .) to variable names or resources (x, y, . . . or r, r′, . . .).
Given δ and σ, the semantic function maps a history expression to a corre-
sponding set. As expected, ε denotes the empty set under any conﬁguration and
α(r) denotes the singleton containing only α(r). The semantics of a sequential
composition H · H′, of non-deterministic choice H + H′ and of ϕ[H] is obvious.
The semantics for gH is a little more tricky. We assert gH to deﬁne two distinct
sets: Hσ
δ , if σ satisﬁes g (we write σ |= g if and only if B(gσ) = tt), or ∅
otherwise. The semantics of μh.H is the least ﬁxed point of the function f (see
[3] for further details).

50
G. Costa, P. Degano, and F. Martinelli
4.2
Typing Relation
Before introducing a type and eﬀect system for our calculus, we deﬁne the rela-
tion ⊑σ as the partial order between history expressions
H ⊑σ H′ if and only if Hσ
∅⊆⊥H′σ
∅
We write H ⊑H′ when ∀σ.H ⊑σ H′.
Types and type environments are introduced in Table 5. Type environments
are deﬁned in a standard way as mappings from variables to types. Types can be
either base types, i.e. unit or resources, or higher-order types τ
H
−→τ′ annotated
with the history expression H.
A typing judgement (Table 6) has the form Γ, H ⊢g e : τ and means that the
expression e is associated with the type τ and the history expression H. The
proposition g records information about the branching path collected during the
typing process.
Rules (T−Unit, T−Res, T−Var) for ∗, resources and variables are straightfor-
ward. An event has type 1 and produces a history that is the one obtained from
the evaluation of its parameter increased with the event itself (T−Ev). Note that,
since the class of resources is ﬁnite, an event only has ﬁnitely many instantia-
tions. An abstraction has an empty eﬀect and a functional type carrying a latent
Table 5. Types and type environments
τ, τ′ ::= 1 | R | τ
H
−→τ′
Γ, Γ ′ ::= ∅| Γ; x : τ
Table 6. Typing relation
(T−Unit) Γ, ε ⊢g ∗: 1
(T−Res) Γ, ε ⊢g r : R
(T−Var) Γ, ε ⊢g x : Γ(x)
(T−Abs)
Γ; x : τ; z : τ
H
−→τ ′, H ⊢g e : τ ′
Γ, ε ⊢g λzx.e : τ
H
−→τ ′
(T−Ev)
Γ, H ⊢g e : R
Γ, H · 
r∈R
α(r) ⊢g α(e) : 1
(T−App)
Γ, H ⊢g e : τ
H′′
−−→τ ′
Γ, H′ ⊢g e′ : τ
Γ, H · H′ · H′′ ⊢g e e′ : τ ′
(T−Frm)
Γ, H ⊢g e : τ
Γ, ϕ[H] ⊢g ϕ[e] : τ
(T−If)
Γ, H ⊢g∧g′ e : τ
Γ, H ⊢g∧¬g′ e′ : τ
Γ, H ⊢g if g′ then e else e′ : τ
(T−Wkn)
Γ, H ⊢g e : τ
H ⊑H′
Γ, H′ ⊢g e : τ
(T−Req)
I = {H | eℓ′ : τ
H
−→τ ′ ∈Srv⌋ℓ∧H |= ϕ}
Γ, ε ⊢g reqρ τ
ϕ
−→τ ′ : τ

i∈I Hi
−−−−−−→τ ′
(T−Str)
Γ, H ⊢g e : τ
g ⇒g′
Γ, g′H ⊢g e : τ
www.ebook3000.com

Modular Plans for Secure Service Composition
51
eﬀect, i.e. the eﬀect that will be produced when the function is actually applied
(T−Abs). The application moves the latent eﬀect to the actual history expression
and concatenates it with the actual eﬀects according the call-by-value semantics
(T−App). Security framing extends the scope of the property ϕ to the eﬀect of
its target (T−Frm). The rule for conditional branching says that if we can type
e and e′ to the same τ generating the same eﬀect H, then we can extend τ
and H to be the type and eﬀect of the whole expression (T−If). Moreover, in
typing the sub-expressions we take into account g′ and its negation, respectively.
Similarly to abstractions, service requests have an empty eﬀect (T−Req). How-
ever, the type of a request is obtained as the composition of all the types of the
possible servers. In particular, the resulting latent eﬀect is the (unguarded) non-
deterministic choice among them. Observe that we only accept exact matching
for input/output types (see [4] for a diﬀerent composition of types). The last
two rules are for weakening and strengthening. The ﬁrst (T−Wkn) states that is
always possible to make a generalisation of the eﬀect inferred from an expression
e. Finally, (T−Str) applies a guard g′ to an eﬀect H provided that g ⇒g′ (if
and only if ∀σ.σ |= g ⇒σ |= g′). This rule says that we can use the information
stored in g for wrapping an eﬀect in a guarded context.
Referring to the type system introduced above, we assert the following state-
ment to be always respected by services in Srv
eℓ: τ
H
−→τ ′ ∈Srv =⇒∅, ε ⊢tt eℓ: τ
H
−→τ ′
This assumption guarantees that Srv records are always consistent with our
type system.
Example 4. Let ePWU be the service introduced in Example 1, then the following
derivation is possible (dots stands for trivial or symmetrical derivations)
...
x : Card, [x ∈C] charge(C) ⊢[x∈C] charge(x); TRUE : Bool
...
x : Card, [x ∈C] charge(C) ⊢true IF : Bool
∅, ε ⊢true ePWU : Card
[x∈C]charge(C)
−−−−−−−−−→Bool
where charge(C) = 
c∈C charge(c) and
IF = if [x ∈C] then charge(x); TRUE else FALSE.
Similarly, we observe that
∅, ε ⊢true ePOL : Card [x∈C′]check(C′)charge(C′)check(C′)
−−−−−−−−−−−−−−−−−−−−−−→Bool
Example 5. Consider now eBH−S from Example 1. For the rightmost expression
we can derive
x : Card, ε ⊢true reqρ Card
ϕBH
−−→Bool : τ
x : Card, ε ⊢true x : Card
x : Card, H ⊢true (reqρ Card
ϕBH
−−→Bool)x : Bool

52
G. Costa, P. Degano, and F. Martinelli
where
τ = Card
[x∈C]charge(C)
−−−−−−−−−→Bool
Moreover we can derive
Γ, book(S) ⊢[y=TRUE] book(S) : 1
Γ, [y = TRUE]book(S) ⊢[y=TRUE] book(S) : 1
Γ, ε ⊢[y̸=TRUE] ∗: 1
Γ, [y = TRUE]book(S) ⊢[y̸=TRUE] ∗: 1
Γ, [y = TRUE]book(S) ⊢true if [y = TRUE] then book(S) else ∗: 1
where Γ = x : Card; y : Bool. Combining the two we obtain
∅, ε ⊢true eBH−S : Card
[x∈C]charge(C)·[y=TRUE]book(S)
−−−−−−−−−−−−−−−−−−−−→1
Similarly, the result of typing eBN−S is
∅, ε ⊢true eBN−S : Card
book(S)·[x∈C′]check(C′)charge(C′)check(C′)[y=FALSE]unbook(S)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→1
Example 6. Consider now the term eTA. The following derivation is possible
...
x : Card, ε ⊢true req¯ρ Card −→1 : Card
H+H′
−−−−→1
x : Card, ε ⊢true x : Card
x : Card, H + H′ ⊢true (req¯ρ Card −→1)x : 1
where
H = [x ∈C]charge(C) · [y = TRUE]book(S)
H′ = book(S) · [x ∈C′] check(C′)charge(C′)check(C′) · [y = FALSE]unbook(S)
Then, we can type the whole service to
∅, ε ⊢true eTA : Card
ϕTA[(H+H′)·(H+H′)]
−−−−−−−−−−−−−−→Rec
Our type and eﬀect system produces history expressions that approximate the
run-time behaviour of programs. The soundness of our approach relies on pro-
ducing safe history expressions, i.e. any trace produced by the execution of an
expression e (under any valid plan) is denoted by the history expression ob-
tained typing e. To prove type and eﬀect safety we need the following lemmata.
Lemma 1 states that type and eﬀect inference is closed under logical implica-
tion for guards, and Lemma 2 says that history expressions are preserved by
programs execution.
Lemma 1. If Γ, H ⊢g e : τ and g′ ⇒g then Γ, H ⊢g′ e : τ
Lemma 2. (Subject reduction) Let Γ, H ⊢g e : τ and η, e →∗
π η′, e′. For each
g′ such that g′ ⇒g, if Γ, H′ ⊢g′ e′ : τ then ∀σ.σ |= g′ =⇒η′H′σ
∅⊆ηHσ
∅
www.ebook3000.com

Modular Plans for Secure Service Composition
53
Then, the soundness of our approach is established through the following theo-
rem, where we use η for both a history generated by the operational semantics
and a history belonging to the denotational semantics of a history expression H.
To compare histories of the two kinds, given η ∈Hσ
∅we write η−Φ for η where
all the security framings ϕ[] have been removed, as deﬁned below:
ε−Φ = ε
(αη)−Φ = α(η−Φ)
ϕ[η]−Φ = η−Φ
We can now state our type safety theorem.
Theorem 1. (Type safety) If Γ, H ⊢true e : τ and ε, e →∗
π η′, v, then ∀σ. ∃η∈
Hσ
∅such that η′ = η−Φ.
The type and eﬀect system of [3] has no rule for strengthening like our rule
(T−Str). The presence of this rule in our system makes it possible to discard
some of the denoted traces. These traces correspond to executions that, due to
the actual instantiation of formal parameters, can not take place. Consequently,
our type and eﬀect system produces more compact history expressions than
those of [3]. This has several advantages, and we mention here a couple of them.
First, having small history expression reduces the search space contributing to
speed up veriﬁcation algorithms. Secondly, it removes possible false negatives
raising from traces that violate the active policy but will never be produced at
run-time.
History expressions validity guarantees that no security violations can happen
at run-time. We deﬁne it as follows.
Deﬁnition 2. (Validity of History Expressions)
H |= ϕ if and only if ∀σ and ∀ϕ occurring in H, ∀η ∈Hσ
∅: η |= ϕ.
We say that a history expression H is valid if and only if ∀η ∈H and ∀ϕ
occurring in H then η |= ϕ. Note in passing that validity of histories is not
compositional being our framework a history-dependent one. E.g. if ϕ says “never
α” from the validity of α and of ϕ[β] it does not follow αβ |= ϕ. As a matter
of fact, the validity of H is established through model-checking in polynomial
time [8].
5
Modular Plans
In this section we introduce the main contribution of this work, i.e. how plans
can be composed.
5.1
Properties of Plans
In Section 3.2 we saw how plans drive the execution of services turning service
requests into actual service invocations. We can also interpret plans as a sub-
stitution strategy of request expressions. We can therefore inline eℓfor reqρ τ

54
G. Costa, P. Degano, and F. Martinelli
whenever π(ρ) = ℓ(note that recursion is not a problem here because the binding
request-service deﬁned by π is static). We call this semantics preserving process
ﬂattening, and ﬂat a service without requests.
Deﬁnition 3. (Flattening)
∗|π= ∗
r |π= r
x |π= x
α(e) |π= α(e |π)
(λzx.e) |π= λzx.e |π
(e e′) |π= e |π e′ |π
ϕ[e] |π= ϕ[e |π]
(if g then e else e′) |π= if g then e |π else e′ |π
(reqρ τ −→τ ′) |π=
reqρ τ −→τ ′
if π(ρ) = ⊥
eℓ|π
if π(ρ) = ℓ
Flattening an expression e with larger plans makes smaller the set of histories
associated with e. This property is stated by the following theorem.
Theorem 2. Given two plans π and π′, if Γ, H ⊢g e |π: τ
and
Γ, H′ ⊢g
e |π;π′: τ then H′ ⊑H
To securely compose plans we must guarantee that the resulting plan preserves
validity. Hence, we introduce the key concept of plan completeness. A plan π is
complete w.r.t. a service e if it is valid and makes e ﬂat.
Deﬁnition 4
Given a closed term e, a plan π is said to be complete for e if and only if
1. e |π is ﬂat and
2. if ∅, H ⊢true e |π: τ then H is valid.
Complete, modular plans for services can be composed preserving completeness,
as stated below.
Lemma 3. Given two terms e,e′ and two modular plans π,π′ complete for e and
e′, respectively, then π; π′ is complete for both e and e′.
We present now a way of incrementally building services in a secure manner.
We ﬁrst state a theorem characterising our notion of composition. As expected,
we cannot aim at a full compositionality because history validity itself is not
compositional. The lemma above helps in ﬁnding conditions that permit to ob-
tain secure services by putting together components already proved secure. We
shall then outline a way of eﬃciently reuse the proofs of validity already known
for components, so to incrementally prove the validity of a plan for the com-
posed service. (Recall that services are closed expressions, typable in an empty
environment.)
www.ebook3000.com

Modular Plans for Secure Service Composition
55
Theorem 3. Let πi be modular plans complete for services ei, and let ∅, Hi ⊢gi
ei : τi, i = 0, 1. Then,
– π0; π1 is complete for both α(e0) and if g then e0 else e1
– ∀ϕ, if H0 |= ϕ then
- π0; π1 is complete for ϕ[e0]
- {ρ 
→ℓ0}; π0 is complete for reqρ τ
ϕ−→τ ′, where ℓ0 hosts e0
– if τ0 = τ1
H′
0
−−→τ′ and H0 · H1 · H′
0 is valid, then π0; π1 is complete for e0e1
We now sketch a procedure for checking the validity of the composition of his-
tories that occurs when applying a service to another one (third item above).
First, notice that H0 · H1 · H′
0 is valid if and only if (i) H0 is valid, (ii) ∀ϕ in H1,
H0 · H1 |= ϕ and (iii) ∀ϕ′ in H′
0, H0 · H1 · H′
0 |= ϕ′. Item (i) holds by hypothesis
(as well as H1 |= ϕ and H′
0 |= ϕ′). So, it suﬃces to deﬁne a method for checking
validity of the sequential composition of two histories H · H′ w.r.t. a policy ϕ.
To check validity of H · H′ w.r.t. ϕ, we look for a usage policy ψ that rejects
any trace η that, once extended with any η′ of H′, leads to a violation of ϕ, i.e.
ηη′ ̸|= ϕ. Clearly for H · H′ |= ϕ, η should not be trace of H if η ̸|= ψ.
We proceed as follows, recalling that ϕ is a regular language, the strings
of which are rejected by a ﬁnite-state automaton A. We ﬁrst build AR, the
automaton that rejects the reverse strings ηR, for η not in the language of A.
Note that η ̸|= ϕ if and only if ηR belongs to the language of AR. Now, mark
all the states q of AR reachable with ηR for all η denoted by H′. For all such
q, build a new automaton AR
q equal to AR, except that its initial state is q. Let
ηR
q belong to the language of AR
q , for some q. This implies that there exists η
denoted by H′ such that ηR · ηR
q belongs to the language of AR. Consequently,
ηq · η ̸|= ϕ. The required policy ψ is given by the reverse of the union of the
languages accepted by the automata AR
q speciﬁed above, for all relevant q.
For example, let H′ = charge(x) + check(x) and ϕ = ϕBH (see Fig. 2a).
Applying the procedure introduced above, we ﬁnd ψ = “never check(x)”.
Example 7. Consider again the service network of Section 2 typed in Section 4.
Let HPWU, HPOL, HBH, HBN and HTA their latent eﬀects.
Consider now the three service policies deﬁned in Figure 2. It is immediate
to verify that the plans π1 = {ρ 
→PWU} and π4 = {ρ′ 
→POL} are modular and
complete for Book-Here-F and Book-Now-H, respectively.
We try to obtain the service Travel Agency by sequentially composing the
above two components (recall that sequentialization is a simple form of applica-
tion). To guarantee security we have to prove the validity of HBH−F · HBN−H (or
vice versa). However, state 2 of ϕBN is reachable through a preﬁx of a history in
the semantics of HBH−F, e.g. charge(x)book(F) originated when x ∈C. Similarly,
for HBN−F · HBH−H.
Instead, if we consider HBH−F · HBH−H no violations can occur (neither service
performs a check). Now, the composed service is secure because it is compliant
with the policy ϕTA.

56
G. Costa, P. Degano, and F. Martinelli
2
4
1
3
Travel Agency
Pay −With −Us
Pay −On −Line
Book −Here −F
Book −Here −H
Book −Now −F
Book −Now −H
Fig. 3. A travel booking network
Similarly for the three cases HBH−H · HBH−F, HBN−F · HBN−H and HBN−H · HBN−F.
Finally, although HBH−F · HBH−F (and the analogous three compositions) can
be proved secure, it does not obey ϕTA.
6
Conclusion and Related Work
In this paper we presented a framework for secure service composition. Services
are implemented in λreq [3], a service-oriented version of the call-by-value λ-
calculus. Security properties are deﬁned through usage automata and locally
applied to pieces of code using a proper operator, namely security framing. A
type and eﬀect system extracts a history expression from a service. Since it con-
siders service assertions on resources, our type and eﬀect system can also deal
with open service networks, i.e. networks where one or more services are unspec-
iﬁed. History expressions are safe approximation of a program, i.e. they denote
every possible sequence of security relevant actions ﬁred at run-time. Moreover,
history expressions can be model-checked against local policies. The result of
this process is a policy compliance proof, if it exists, guaranteeing that no se-
curity violation will happen at run-time. We used this technique for generating
viable composition plans. Plans produced in this way drive the service execu-
tion respecting the security constraints. Additionally, we showed a composition
strategy for plans. If two or more plans satisfy the necessary conditions they can
be composed in a wider plan. Then, we deﬁned a simple procedure for verifying
the validity of composed plans.
This work is a ﬁrst step toward a bottom-up composition strategy for service
network. Many further directions can be investigated. An interesting result could
be obtained by exploiting the correspondence between history expressions and
basic process algebras (BPAs [9]). Using an approach similar to [11], we could
deﬁne a symbolic transition system (STS). STSs oﬀer many interesting properties
and allow for a ﬁne-grained analysis of security properties.
Another possible improvement consists in extending out programming model
introducing a concurrent composition of services. This scenario poses several
issues increasing the complexity of our framework. Mainly, the well known state
explosion problem could make the model checking step practically infeasible.
www.ebook3000.com

Modular Plans for Secure Service Composition
57
To the best of our knowledge, there are no proposals for the secure composition
of services within a linguistic framework. Below we brieﬂy survey on a few related
work.
In [12] the authors present a framework for contract-based creation of chore-
ographies. Roughly, they use a contract system for ﬁnding a match between
contracts and choreographies. In this way they verify whether a given contract,
declared by a service joining the network, is consistent with the current choreog-
raphy. However, this framework exploits a global knowledge about the network
structure while our model also deals with open networks.
Castagna et al. [14] use a variant of CCS with internal and external choice
operators [18] for deﬁning service contracts. A subcontract relation guarantees
that the choreography always respects the contracts of both client and service.
Nevertheless, this approach assumes that clients always know the request con-
tract and it is focused on ﬁnding a satisfactory composition with some available
service. Since the composition depends on the possible instantiations of a service,
such a contract could be unavailable during the analysis phase. In this sense, our
technique overcomes this limitation by producing valid plans independently from
the actual instantiation of resources.
Busi et al. [13] propose an analysis of service orchestration and choreography.
In their work, the validity of the service orchestration is a consequence of its
conformance with respect to the intended choreography. The main diﬀerence
with our work consists in the approach to choreography. Indeed, they start from
a pre-deﬁned choreography and verify the validity of an orchestration. Instead,
our approach aims at checking partial service composition without relying on
any global orchestrator.
In [16] a framework for the synthesis of orchestrators is presented. This tech-
nique consists in automatically producing an orchestrator guaranteeing that the
service composition respects the desired security policy. This approach deﬁnes
a composition that complies with the policy of a client. Since our method pro-
duces partial compositions that respect all the involved policies (of both clients
and servers), it seems to be more general. However, as [16] generates dynamic
orchestrators, the two systems work under completely diﬀerent assumptions.
References
1. Abadi, M., Fournet, C.: Access control based on execution history. In: NDSS (2003)
2. Bartoletti, M., Costa, G., Degano, P., Martinelli, F., Zunino, R.: Securing Java
with local policies. Journal of Object Technology (JOT) (2008)
3. Bartolett, M., Degano, P., Ferrari, G.L.: History-based access control with local
policies. In: Sassone, V. (ed.) FOSSACS 2005. LNCS, vol. 3441, pp. 316–332.
Springer, Heidelberg (2005)
4. Bartoletti, M., Degano, P., Ferrari, G.L.: Planning and verifying service composi-
tion. Journal of Computer Security (JCS) 17(5), 799–837 (2009); Abridged version
In: Proc. 18th Computer Security Foundations Workshop (CSFW) (2005)
5. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Secure service orchestra-
tion. In: Aldini, A., Gorrieri, R. (eds.) FOSAD 2007. LNCS, vol. 4677, pp. 24–74.
Springer, Heidelberg (2007)

58
G. Costa, P. Degano, and F. Martinelli
6. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Types and eﬀects for resource
usage analysis. In: Seidl, H. (ed.) FOSSACS 2007. LNCS, vol. 4423, pp. 32–47.
Springer, Heidelberg (2007)
7. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Local policies for resource
usage analysis. ACM Trans. Program. Lang. Syst. 31(6), 1–43 (2009)
8. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Model checking usage policies.
In: Kaklamanis, C., Nielson, F. (eds.) TGC 2008. LNCS, vol. 5474, pp. 19–35.
Springer, Heidelberg (2009)
9. Bergstra, J.A., Klop, J.W.: Algebra of communicating processes with abstraction.
Theor. Comput. Sci. 37, 77–121 (1985)
10. Besson, F., Jensen, T.P., Le M´etayer, D.: Model checking security properties of
control ﬂow graphs. Journal of Computer Security 9(3), 217–250 (2001)
11. Boreale, M., De Nicola, R.: A symbolic semantics for the pi-calculus. Inf. Com-
put. 126(1), 34–52 (1996)
12. Bravetti, M., Lanese, I., Zavattaro, G.: Contract-driven implementation of chore-
ographies. In: Kaklamanis, C., Nielson, F. (eds.) TGC 2008. LNCS, vol. 5474,
pp. 1–18. Springer, Heidelberg (2009)
13. Busi, N., Gorrieri, R., Guidi, C., Lucchi, R., Zavattaro, G.: Choreography and
orchestration: A synergic approach for system design. In: Benatallah, B., Casati, F.,
Traverso, P. (eds.) ICSOC 2005. LNCS, vol. 3826, pp. 228–240. Springer, Heidelberg
(2005)
14. Castagna, G., Gesbert, N., Padovani, L.: A theory of contracts for web services.
ACM Trans. Program. Lang. Syst. 31(5) (2009)
15. Ligatti, J., Bauer, L., Walker, D.: Edit automata: enforcement mechanisms for
run-time security policies. Int. J. Inf. Sec. 4(1-2), 2–16 (2005)
16. Martinelli, F., Matteucci, I.: Synthesis of web services orchestrators in a timed
setting. In: Dumas, M., Heckel, R. (eds.) WS-FM 2007. LNCS, vol. 4937,
pp. 124–138. Springer, Heidelberg (2008)
17. Martinelli, F., Matteucci, I.: Synthesis of local controller programs for enforcing
global security properties. In: ARES, pp. 1120–1127 (2008)
18. De Nicola, R., Hennessy, M.: Ccs without tau’s. In: Ehrig, H., Levi, G., Montanari,
U. (eds.) TAPSOFT 1987. LNCS, vol. 249, Springer, Heidelberg (1987)
19. Schneider, F.B.: Enforceable security policies. ACM Trans. Inf. Syst. Secur. 3(1),
30–50 (2000)
20. Skalka, C., Smith, S.F.: History eﬀects and veriﬁcation. In: Chin, W.-N. (ed.)
APLAS 2004. LNCS, vol. 3302, pp. 107–128. Springer, Heidelberg (2004)
21. Winskel, G.: The formal semantics of programming languages. MIT Press, Cam-
bridge (1993)
www.ebook3000.com

A Type System for Access Control Views in
Object-Oriented Languages
M´ario Pires and Lu´ıs Caires
CITI and Faculdade de Ciˆencia e Tecnologias, Universidade Nova de Lisboa
Abstract. Access control to objects in common object-oriented lan-
guages is statically veriﬁed but cannot be changed at run-time. However,
dynamic authorization is required by most applications and it would be
desirable to check more ﬂexible access control policies also statically, at
least partially. In this work, we introduce a model where “views” to ob-
ject references represent the current access control policy of a principal
for a given object, and ﬁrst class authorizations support dynamic modiﬁ-
cation of those policies. To demonstrate our concepts, we have developed
a core language, equipped with a provably correct type and eﬀect sys-
tem capable of detecting unauthorized method calls at compile-time, and
deﬁned and implemented a typechecking algorithm.
1
Introduction
Computer security is often ensured by resorting to special programming idioms
and techniques. However, programming errors are common, which in the con-
text of increasingly demanding security requirements may easily lead to security
breaches. Language-based security [14,19] approaches the enforcement of soft-
ware security by imposing rules to program structure, preferably in a way that
is usually amenable to compile-time veriﬁcation. In this work, we focus on access
control, a mechanism to guarantee that resources are only used by authorized
principals, conforming to other conﬁdentiality and integrity requirements.
Language-based access control is frequently enforced with type-based veriﬁ-
cations. In this context, type systems are extremely useful. In general, types
prevent the occurrence of errors during a program execution by forcing the de-
veloper to produce programs in conformance with some typing discipline. If we
are interested in verifying security properties, types may also be used to stat-
ically impose security constraints, that will ensure that all runs of a program
comply with certain security properties, so that their violation will be seen as
an execution error. In this work, we introduce a model where “views” to object
references represent the current access control policy of a principal for a given ob-
ject, and ﬁrst class authorizations support dynamic modiﬁcation of such policies.
This idea of decoupling the veriﬁcation of access policies from the protected ob-
ject to the (dynamically changing) user-object relationships has not been much
explored before: our aim is then to develop a type-based analysis for such model.
The simplest access control mechanism available in object-oriented languages
is usually based on the visibility modiﬁers public and private (among others) [11,
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 59–76, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

60
M. Pires and L. Caires
class File {
public void open() {...}
public void read() {...}
public void write() {...}
public void close() {...}
private void seek() {...}
}
void create(){
File f = new File();
DoRead(f);
}
void DoRead(File f){
f.open(); f.read(); f.write(); f.close();
}
Fig. 1. Motivating Example
section 6.6]. These modiﬁers restrict where objects’ methods can be called; either
anywhere or only within a class. A motivating example is presented in Figure 1,
where we have a skeleton of a class File with the usual methods public open,
read, write and close, and a private method seek() (for example). We also have
a method DoRead() that receives an object instance of that class. As the name of
the method implies, it should only read the ﬁle. However, since method write() is
public, the respective call in DoRead() is not considered an error. This restriction
could only be veriﬁed in run-time and each time a method would be called. Also,
if we want to add permissions for diﬀerent clients to read or write the ﬁle, we
end up creating more dynamic checks to validate the respective calls. If we think
that some methods once executed cannot be undone, these validations had to
be even more complex, slowing down the method execution time.
As we can see, this simple example raises some issues. The goal of the type
system developed in this work is to solve these problems during compile-time.
If a program typechecks, it can safely execute with a minimum of run-time
veriﬁcations to ensure that objects are used according to the authorized policy.
To that end, we introduce two new values to the language that, used together,
achieve this goal: access views and authorizations.
Our approach is based on moving the (unique) access control policy for each
object to a setting where a diﬀerent access control policy may be available for
each client of an object, under control of the programmer (with assistance from
the type system), as explained below. First, we change where access control
is imposed: from the object’s perspective (visibility modiﬁers) to the client’s
perspective (access privileges). So, Access Views are protected references; a pair
with the usual reference and an access policy - the client’s privileges over the
object. Second, we add ﬁrst class Authorizations to enable dynamic modiﬁcation
of access views’ policies. They can be seen as security tokens that grant access
to an object and can be stored in structures, passed between processes or be
an argument for a method call. Furthermore, authorizations are created with
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
61
class File : { (open; (read + write)* ; close)* } {
void open() {...}
void read() {...}
void write() {...}
void close() {...}
void seek() {...}
}
void create(){
File f = new File;
Auth readOnly = authorization(f, {open; read*; close});
Auth writeOnly = authorization(f, {open; write*; close});
DoRead(f,readOnly);
}
void DoRead(File f, Auth a){
authorize f:a
case {open; read*; close} :
{ f.open();
f.read(); f.read(); f.close() }
case error: { }
}
Fig. 2. Modiﬁed Example
cryptographic techniques, being impossible to forge or replicate. The previous
example is now rewritten in Figure 2.
As explained above, visibility modiﬁers are not needed in method declara-
tions. Instead, the developer deﬁnes a maximal class policy that reﬂects the
most permissive access privilege. Moreover, the object creator, who has maxi-
mal privilege over it, can delegate various authorizations with diﬀerent policies
using the programming primitive authorization, granting rights to access the ob-
ject to whoever can obtain those authorizations. To apply an authorization and
change an access view’s policy, we use the primitive authorize. This primitive in-
vokes a run-time veriﬁcation that the given authorization validates the intended
policy for the object. On the other hand, the type system statically veriﬁes that
all methods are only called according to the current access view policy.
To enforce the use of authorizations, any receiving access view has an empty
(less permissive) policy, i.e., one does not have privileges over method arguments
and results of method calls. So, to access any objects, one needs to apply valid
authorizations, changing the policies of access views.
As in the example, we can verify that method DoRead() uses the ﬁle as the
intended protocol. If a method were called outside an authorize block by a non-
creator client, it would be considered an unauthorized method call and detected
by the typechecker as a typing error. Also, notice that the method seek() is not
present in the maximal class policy. So, since even the object’s creator cannot
call it, the method is private to the class.
As explained in the next sections, our type system is generic in the sense that
it works with a family of interesting policy languages (Deﬁnition 1).

62
M. Pires and L. Caires
For further examples and explanation of concepts presented on this paper,
as well as complete proofs of theorems, see [17]. A sketch of a distributed ﬁle
system is also shown in that document, where ﬁles are protected using access
views and clients have authorizations associated with them in order to access
each ﬁle. Other applications of our type system could be to control the access to
a web-service. Each client would receive the service’s reference protected with a
usage policy (an access view) and would have to apply an authorization in order
to use it, allowing the same web-service to be used diﬀerently by distinct clients.
This work’s contributions are the following. We introduce access views, that de-
scribe the usage discipline of an object from the client perspective, not from the
object perspective, which is the key new idea explored in the work. This allows
to verify statically the usage of an object by diﬀerent clients with diﬀerent priv-
ileges over that object. Also, we introduce ﬁrst class authorizations as tokens to
allow dynamic modiﬁcations of those privileges within access views. We develop a
generic type system with respect to policy languages and deﬁned and implemented
a typechecking algorithm for the core language deﬁned in Section 2 on a prototype
typechecker [18]. We enforce correct access control to objects with types, devel-
oped a type and eﬀect system [15], deﬁned in Section 3, to reﬂect policy changes,
and proved type soundness. In Section 4 we discuss the main challenges that arise
from our approach, in Section 5 we compare our solution with some related work
and in Section 6 we present some concluding remarks and future work.
2
Core Language
Our core language is a simpliﬁed object-oriented language, extended with prim-
itives to create and apply authorizations, and related primitive types (access
views, authorizations).
Access control policies play a central role in our model. Instead of proposing a
concrete language for expressing policies, we identify a set of general conditions
a policy language must comply in order to ﬁt our framework, so to keep a
degree of generality. A policy π is a speciﬁcation of the privileges to access an
object, describing which methods may be invoked at a given moment. Any policy
language must come equipped with the following relations:
Deﬁnition 1 (Policy Language Properties)
1. Validity, written M ⊢π, where M is a set of methods. This states the domain
of the policy π, e.g., the set of methods M it refers to.
2. Step, written π
m
→π′, where π and π′ are policies and m is a method identi-
ﬁer. To accommodate history-dependent access control, each time a method
is called, the access control policy for the object can change. Step precisely
speciﬁes how a policy may evolve as an eﬀect of a method call.
3. Sub-policy, written π <: π′, where π and π′ are policies. This relation states
that π′ is more permissive than π. Depending on the particular policy lan-
guage used, one may deﬁne several notions of sub-policy, as far as it is
consistent with step, in the following sense (of simulation):
If π1 <: π2 and π1
m
→π′
1 then π2
m
→π′
2 and π′
1 <: π′
2.
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
63
We deﬁne the full access policy to be the policy {∗} such that {∗}
m
→{∗} for all
methods m. Notice that then π <: {∗} for all π. It is also useful to deﬁne for each
class a maximal class policy, which represents the maximal privilege an access
view can have for an object instance of that class. Finally, the empty policy is
the policy {∅} with no privileges, e.g. {∅}
m
↛for all m.
In most examples presented in this paper, we use a regular expression lan-
guage to describe policies, e.g. {open ; (write + read)∗; close}. Other exam-
ples of policies languages include: a simple method set, e.g. {a, b, c}, where step
{a, b, c}
m
→{a, b, c} is possible if the set contains the method identiﬁer m, not
changing the policy, thus mimicking the use of public modiﬁer when declaring
methods a, b and c in traditional programming languages; and a slightly diﬀerent
language, where each method has a counter associated with it, e.g. {a 2, b 3, c 1},
enabling to specify how many times a method can be called.
P
::= defn ∗{e}
defn
::= class cn : π body
body
::= { ﬁeld ∗method∗}
ﬁeld
::= Ts fd = e
method ::= Ts m(Vs x) { e }
e ::= new cn
|
e.fd
|
e.fd = e
|
e.m(e)
|
this
|
x
|
n
|
a
|
let x = e in e
|
authorization(e, π)
|
authorize e : e case {π} : {e} case error: {e}
cn ∈class names
fd ∈ﬁeld names
m ∈method names
x ∈variable names
n ∈N
a ∈authorization names
π ∈policies
Fig. 3. Abstract syntax of our core language
The syntax of our language is deﬁned in Figure 3. A program is a set of
class declarations and a main expression as the entry point for the program. A
class contains a set of mutable ﬁelds with initial values, a set of methods and a
maximal class policy declaration. A method declaration has just one parameter
to simplify the presentation. An expression is either an object creation, a ﬁeld
access, a ﬁeld update, a method call, a variable binding, an integer, the reserved
word this as the self reference, a variable or the new primitives authorization
and authorize to create and apply authorizations, respectively. Notice that we
do not introduce a sequential primitive (e1; e2), since it can be encoded with a
let primitive (let
= e1 in e2). The values of our language are given by
v ::= n
|
u
|
this
|
a
(Values) .
We do not consider locations as values, since they are only accessible through
access views, but they are also created when a new object is created. As such,

64
M. Pires and L. Caires
the values for our language are integers n (representing all basic values), access
views u, the special access view this and authorizations a. Both access views and
authorizations only appear during evaluation time when variables are evaluated.
Types, as used in the syntax, are given by
Ts, Vs ::= int
|
cn
|
Auth
(Types) .
We adopt a nominal type system, where we use class names cn to represent types.
In addition to class types, there is the integer type int (representing all basic
types) and the new authorization type Auth. Note that authorizations do not
have static information about the policy. Otherwise, if we had the type Auth{π},
the veriﬁcation performed by the authorize primitive during run-time would be
simpler, since the authorized policy (π) would be available during the static
analysis, but authorizations would be depended on policies and programming
would be very restrictive and diﬃcult, e.g., an hashtable of authorizations would
be restricted to a single kind of policy. Therefore, our decision was based on
the usability of authorizations and, as a consequence, the authorized primitive
has an additional overhead during run-time to verify if the new policy is a sub-
policy of the authorized one, stored on the authorization. Also, note that at
the syntax level, we do not make explicit the information about policies when
declaring an object; they will be assigned by the typing rules. Though they are
not needed during execution, policies are necessary to deﬁne the notion of error
(Section 3.2): a method call is an error if the policy does not allow that call.
The semantics of our language is deﬁned as a small-step operational semantics,
following a call-by-value evaluation strategy (Figure 4). Let S be a store and e
an expression such that < S ; e > is a valid conﬁguration, a reduction rule is a
single step evaluation of e to an expression e′, where the resulting store is S′.
The judgement has the form < S ; e > −→< S′ ; e′ >.
The store contains the program classes’ information and the locations of the
objects in run-time to maintain a unique object state. Locations hold objects,
since the only mutable elements are object’s ﬁelds. Objects {Fs Ms} have two
components: a set Fs, to associate ﬁeld names with their values, and a set Ms,
to store the object’s methods information.
Fs ::=
∅
|
Fs, fd →v
(Fields)
Ms ::=
∅
|
Ms, m(x) = e
(Methods)
Access to locations by running code may only be done through access views,
protecting them with an access control policy. For technical convenience, autho-
rizations are also referenced through the store.
S ::=
(Store)
|
∅
(empty store)
|
S, c →{Fs Ms π}
(class information)
|
S, u →(l, π)
(access view)
|
S, l →{Fs Ms}
(location)
|
S, a →[l, π]
(authorization)
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
65
The operations over stores are deﬁned as follows. We say Dom(S) to denote the
domain of S, S(c) to denote the value of c stored at S, and S[u ←(l, π)] to
denote the policy modiﬁcation of access view (or ﬁeld) u to the policy π. An
expression e is closed in store S if e does not have free variables and if all access
views and authorizations occurring in e are elements of Dom(S). We say that
< S ; e > is a valid conﬁguration if e is closed in store S.
We now describe some key reduction rules. When a new object is created,
by rule [R - Object Creation] a new location and access view with the maximal
(most permissive) policy are created for the object. When accessing a ﬁeld, by
rule [R - Field Access], its current policy is lost to a new access view. On the
other hand, when updating a ﬁeld with an access view, rule [R - Field Update
- Base] does the opposite: the access view loses its policy to the ﬁeld. Rule [R -
Variable Binding] follows the same approach, the access view loses its policy to
the new variable. This prevents aliasing of authorized policy. Otherwise, if we
could copy the policy, it could be violated, e.g., if we had an access view with an
authorized policy to call a method only one time, we could create other access
views with the same policy as the initial and call the method with each one of
them, violating the authorized policy. With our solution, only one of the new
access views would have the authorized policy.
A method’s code is assumed to be always executed with “administrator” priv-
ileges (full access policy), having unrestricted access to the object’s ﬁelds and
methods through the self reference. As explained before, both method parame-
ters and results of method calls will have empty policies associated with them.
So, when calling a method, [R - Method Call - Base], the method’s parameter
will be substituted by a new access view (u3) to the same location (lu) as the
argument’s but with an empty policy. On the other hand, we substitute the self
reference with a new access view (ut) with the same properties as the self refer-
ence. Finally, as a consequence of calling the method, the current policy of the
object’s access view (u1) takes a step with the method’s name (π
m
→π′).
Creating an authorization has no eﬀect in any policy, by rule [R - Autho-
rization]. The condition π <: πu ensures that the new policy π is a sub-policy
of the current policy πu. One can only create authorizations with equal or less
privileges than his own. When applying an authorization, a veriﬁcation is per-
formed to validate the pair (location, policy) with the authorization (deﬁnition
2). If the pair is valid, then rule [R - Authorize OK] is applied and the access
view’s policy is changed. Otherwise, rule [R - Authorize KO] is applied, leaving
the access view unchanged. In either case, when the authorize block ends, the
policy of access view u is permanently altered if the object is used, according to
the typing rule [T - Authorize].
Deﬁnition 2 (Authorization Applicability). We deﬁne authorization ap-
plicability, written a ⊢p, to validate a pair p = [lp, πp] with an authorization
a = [la, πa], as follows:
[la, πa] ⊢[lp, πp]
def
= lp = la ∧πp <: πa
.

66
M. Pires and L. Caires
[R - Object Creation]
[R - Field Access]
l, u /∈Dom(S)
c →{Fs Ms π} ∈S
< S ; new c > −→< S′ ; u >
u →(l, πt) ∈S
u′ /∈Dom(S)
l →{Fs Ms} ∈S
fd →(l′, π) ∈Fs
< S ; u.fd > −→< S′ ; u′ >
S′ = S ⊎{l →{Fs Ms}, u →(l, π)}
S′ = S[u.fd ←(l′, ∅)] ⊎{u′ →(l′, π)}
[R - Field Update]
[R - Field Update - Base]
< S ; e >−→< S′ ; e′ >
< S ; u.fd = e > −→< S′ ; u.fd = e′ >
u →(l, π) ∈S
u′ →(l′, π′) ∈S
l →{Fs Ms} ∈S
fd ∈Dom(Fs)
< S ; u.fd = u′ > −→< S′ ; u′ >
S′ = S[u.fd ←(l′, π′), u′ ←(l′, ∅)]
[R - Variable Binding]
[R - Variable Binding - Base]
< S ; e1 >−→< S′ ; e′
1 >
< S ; let x = e1
in e2
> −→< S′ ; let x = e′
1
in e2
>
u →(l, π) ∈S
u2 /∈Dom(S)
< S ; let x = u
in e2
> −→< S′ ; e2{u2 /x} >
S′ = S[u ←(l, ∅)] ⊎{u2 →(l, π)}
[R - Authorize OK]
[R - Authorize KO]
u →(l, πu) ∈S
S(a) ⊢[l, π]
< S ;
authorize u : a
case π : {e1}
case error: {e2}
> −→< S′ ; e1 >
u →(l, πu) ∈S
S(a) ⊬[l, π]
< S ;
authorize u : a
case π : {e1}
case error: {e2}
> −→< S ; e2 >
S′ = S[u ←(l, π)]
[R - Method Call]
[R - Authorization]
< S ; e >−→< S′ ; e′ >
< S ; u.m(e) > −→< S′ ; u.m(e′) >
u →(l, πu) ∈S
π <: πu
a /∈Dom(S)
< S ; authorization(u, π) > −→< S′ ; a >
S′ = S ⊎{a →[l, π]}
[R - Method Call - Base]
u1 →(l, π) ∈S
u2 →(lu, πu) ∈S
u3, ut /∈Dom(S)
l →{Fs Ms} ∈S
m(x) = e ∈Ms
π
m
→π′
< S ; u1.m(u2) > −→< S′ ; e{u3 /x}{ut/this} >
S′ = S[u1 ←(l, π′)] ⊎{u3 →(lu, ∅), ut →(l, ∗)}
Fig. 4. Reduction Rules
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
67
3
Type System
In this section, we present and discuss our type system. The types are given by:
T, V
::=
O{π}
|
Auth
|
int
(Types)
O
::=
M
|
[F M]
(Object Types)
F
::=
∅
|
F, fd : T
(Fields)
M, N ::=
∅
|
M, m : T(V )
(Methods)
Object references are typed by access types. An access type O{π} is composed
by an object type O and the current policy π for the object. The types Auth and
int are associated with authorizations and integer values, respectively. An object
type may be either external, to type variables and access views where ﬁeld types
are not visible (M), or internal, to type variables and access views alias of the
self reference “this”, where ﬁelds are visible ([F
M]). Method types have the
form T (V ), where T is the return type and V is the parameter type.
Types are assigned to variables, access views and authorizations by typing
environments deﬁned as follows:
Δ ::=
(Typing Environment)
|
∅
(empty environment)
|
Δ, c : [F M π]
(class)
|
Δ, x : T
(variable)
|
Δ, this : [F M]{∗} (self reference)
|
Δ, a : Auth
(authorization)
|
Δ, u : O{π}
(access view)
Operations over typing environments are deﬁned as usual. We write Dom(Δ)
to denote the domain of Δ, Δ(x) to denote the type of x in Δ, and Δ[id ←π]
to denote the policy modiﬁcation of access view or variable id to the policy
π. We also use the standard subset relation (⊆) and we deﬁne a special subset
relation we call π-subset (⊆π), that ignores access view policies, since we need to
compare environments from diﬀerent points of the program, where the policies
of the same access views may vary.
– (π-subset). Let Δ1 and Δ2 be typing environments.
Δ1 ⊆π Δ2 ≜(∀u : O{π1} ∈Δ1 ⇒u : O{π2} ∈Δ2) ∧
∧(∀(x : T ) ∈Δ1 . x is not an access view ⇒(x : T ) ∈Δ2)
An expression e is closed in a typing environment Δ if all free identiﬁers (vari-
ables, access views and authorizations) of e are declared in Δ.
3.1
Typing Rules
The typing rules are deﬁned in Figure 5, following the structure of a type and
eﬀect system. A typing judgement takes the form Δ ⊢e : T 
→Δ′, asserting
that expression e is typed with type T in the typing environment Δ, with some

68
M. Pires and L. Caires
[T - Object Creation]
[T - Authorization Value]
c : [F M π] ∈Δ
Δ ⊢new c : M{π} →Δ
a : Auth ∈Δ
Δ ⊢a : Auth →Δ
[T - id - 1]
[T - id - 2]
id : M{π} ∈Δ
Δ ⊢id : M{∅} →Δ
id : M{π} ∈Δ
Δ ⊢id : M{π} →Δ[id ←∅]
[T - This - 1]
[T - This - 2]
this : [F
M]{∗} ∈Δ
Δ ⊢this : [F M]{∅} →Δ
this : [F M]{∗} ∈Δ
Δ ⊢this : [F
M]{∗} →Δ
[T - Field Access]
[T - Variable Binding]
id : [F
M]{π} ∈Δ
fd : M′{π′} ∈F
Δ ⊢id.fd : M′{∅} →Δ
Δ1 ⊢e1 : M1{π1} →Δ2
Δ2, x : M1{π1} ⊢e2 : M2{π2} →Δ3, x : M1{π3}
Δ1 ⊢let x = e1 in e2 : M2{π2} →Δ3
[T - Field Update]
[T - Method Call]
id : [F
M]{π} ∈Δ
fd : M′{π′} ∈F
Δ ⊢e : M′{πe} →Δ′
Δ ⊢id.fd = e : M′{∅} →Δ′[id.fd ←πe]
Δ ⊢e : M2{∅} →Δ′
id : M{πa} ∈Δ
id : M{πb} ∈Δ′
m : M1(M2) ∈M
πb
m
→π
Δ ⊢id.m(e) : M1{∅} →Δ′[id ←π]
[T - Authorization]
[T - Authorize]
id : M{πid} ∈Δ
π <: πid
M ⊢π
Δ ⊢authorization(id, π) : Auth →Δ
id : M{πid} ∈Δ
a : Auth ∈Δ
M ⊢π
Δ[id ←π] ⊢e1 : T →Δ′
Δ ⊢e2 : T →Δ′
Δ ⊢
authorize id : a
case π : {e1}
case error: {e2}
: T →Δ′
[T - Class]
∀i ∈{1..n}. Δ ⊢ei : Ti{πi} →Δ
Ft = {fdi : Ti{πi} | i ∈{1..n}}
Mt = {mi : Mi(Vi) | i ∈{1..k}}
∀j ∈{1..k}. Δj = Δ, this : [Ft Mt]{∗}, xj : Nj{∅}
Δj ⊢e′
j : Mj{∅} →Δj
Δ ⊢c class cn : π{
T1 fd1 = e1 ... Tn fdn = en
M1 m1(N1 x1){e′
1} ... Mk mk(Nk xk){e′
k}
}
Fig. 5. Typing Rules
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
69
eﬀects that originate the typing environment Δ′. As such, we can say that Δ is
a π-subset of Δ′, Δ ⊆π Δ′, since eﬀects only target policies.
Notice that, for presentation purposes, we consider that expressions and vari-
ables are only typed by access types, allowing us to focus on policies evolution.
However, authorization types are also considered when expected.
For typing accesses to views we need two rules: one rule to apply when the
access policy is returned along with the value, and other when the policy is kept
in the environment (registered in the eﬀect). We illustrate the subtlety with the
following example:
u : M1{π1} ∈Δ1
(b)
Δ1 ⊢u : M1{π1} →Δ1[u ←∅]
Δ1[u ←∅], x : M1{π1} ⊢
x.m(0) : T →Δ2, x : M1{π2}
(a)
Δ1 ⊢let x = u in x.m(0) : T →Δ2
By rule [T - Variable Binding] (a), access view u needs to be well-typed with
M1{π1}, which means that it loses its policy, by applying rule [T - id - 2] (b).
On the other hand, in the following example, rule [T - id - 1] (d) is applied, since
rule [T - Method Call] (c) does not need to change the access view’s policy.
u : M2{πu} ∈Δ
(d)
Δ ⊢u : M2{∅} →Δ
x : M{πx} ∈Δ
m : M1(M2) ∈M
πx
m
→π
(c)
Δ ⊢x.m(u) : M1{∅} →Δ[x ←π]
Rule [T - Method Call] is the most interesting rule. When typing an access
view’s method call, its policy changes, taking a step (πx
m
→π). If this step is
not possible, the method call was not authorized, and the method call is not
typable. Also, notice that a returned value is always assigned an empty policy.
A diﬀerent judgement (Δ ⊢c defn) is used to type classes, since they produce
no eﬀect. So, a class is well-typed if all ﬁeld’s initial expressions are well-typed
and if all methods are well-typed. Note methods cannot change the typing envi-
ronment, i.e., ﬁeld’s policies are invariant, as explained in Section 4.
3.2
Type Safety
We now state and prove our main technical result of this work: the type sound-
ness theorem. To that end, we prove type safety by combining a type preser-
vation theorem and a progress theorem (along the lines of [22]). If a program
is well-typed, then its evaluation does not get stuck; it will not reach an error
conﬁguration, particularly unauthorized method calls.
In the substitution lemma, we have to take into account the access views.
As such, variables (or access views) can only be substituted by access views
with the same object type and policy. Otherwise, the eﬀects could not be the
same. Furthermore, the access view cannot already exist in the environment or
its policy could be violated (aliasing problem). Weakening is deﬁned as usual.

70
M. Pires and L. Caires
Lemma 1 (Substitution). Let Δ, x : M{π} ⊢e : T 
→Δ′, x : M{π′} and
u /∈Dom(Δ). Then Δ, u : M{π} ⊢e{u/x} : T 
→Δ′, u : M{π′}.
Proof. By induction on the derivation of Δ, x : M{π} ⊢e : T 
→Δ′, x : M{π′} .
Lemma 2 (Weakening). Let Δ ⊢e : T 
→Δ′ and x /∈Dom(Δ). Then Δ, x :
T ′ ⊢e : T 
→Δ′, x : T ′.
Proof. Follows by induction on the derivation of Δ ⊢e : T 
→Δ′.
We say that we have a store type conformance between a typing environment Δ
and a store S, written Δ ⊢S, if all elements of dom(S) are well-typed in Δ.
We can now state and prove the subject reduction and progress theorems.
Theorem 1 (Subject Reduction). Let Δ and Δ′′ be typing environments, e
an expression such that e is closed in Δ, and T a type.
if
Δ ⊢e : T 
→Δ′′
and
∃S . Δ ⊢S ∧< S ; e >−→< S′ ; e′ >
then
∃Δ′, Δ′′′ . Δ′ ⊢S′, Δ ⊆π Δ′, Δ′′ ⊆Δ′′′
and
Δ′ ⊢e′ : T 
→Δ′′′.
Proof. By induction on the derivation of < S ; e >−→< S′ ; e′ > (see [17]).
Theorem 2 (Progress). Let Δ and Δ′ be typing environments, e an expression
such that e is closed in Δ, and T a type.
if
Δ ⊢e : T 
→Δ′
then
∃S . Δ ⊢S ∧< S ; e >−→< S′ ; e′ >
or
e is a value.
Proof. By induction on the typing derivation Δ ⊢e : T 
→Δ′ (see [17]).
We need to deﬁne the set of error conﬁgurations - the errors that the typechecker
is able to capture; an expression where the operational semantics get stuck. For
the sake of brevity, we will omit traditional error conﬁgurations, such as class not
found, ﬁeld not found, etc. The two we show in Figure 6 are the unauthorized
method call and the creation of an authorization with an invalid policy, which
follow from this work. Preventing the ﬁrst kind of error is a main motivation
for this work: the ability to detect an unauthorized method call statically. As
we can see, if the access view’s policy does not have a step with the method’s
name (π
m
↛), then the method call is considered a typing error. Therefore, a
well-typed program will only perform authorized method calls. The second error
shows how it is possible to detect invalid creations of authorizations. If the new
policy is not a sub-policy of the current access view’s policy (π ̸<: πu), then the
authorization creation is considered a typing error. This prevents unauthorized
clients to create authorizations to a protected object.
So, we deﬁne the type soundness theorem as follows:
Theorem 3 (Type Soundness). Let Δ and Δ′′ be typing environments, e an
expression such that e is closed in Δ, T a type, and S a store such that Δ ⊢S.
if Δ ⊢e : T 
→Δ′′
then ∀e′ . < S ; e >−→∗< S′ ; e′ >
⇒< S′ ; e′ > is not an error conﬁguration.
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
71
⎡
⎣
F - Method Call
security policy violation
unauthorized access
⎤
⎦

F - Authorization
invalid policy

u →(l, π) ∈S
l →{Fs Ms} ∈S
m(x) = e ∈Ms
π
m
↛
< S ; u.m(e) >
u →(l, πu) ∈S
π ̸<: πu
< S ; authorization(u, π) >
Fig. 6. Selected error conﬁgurations
Proof. Applying the progress theorem (2) and subject reduction theorem (1) we
get the desired type soundness property.
4
Further Discussion
In this section we discuss the main challenges that arise from our approach and
present the solutions for them.
Fields. Fields can hold access views. Therefore, their policies must be followed
for every possible object usage. Figure 7 demonstrates the problem. Class B and
C have a ﬁeld with initial policy {(f; g)∗} and a method m() that uses the ﬁeld.
The problem resides in class B, where there is only one call to a ﬁeld’s method.
If we called method m() two times, the ﬁeld’s policy would be violated. To solve
this problem, ﬁeld’s policies are invariant. Therefore, methods are well-typed
only if all ﬁelds’ policies remain the same at the end of each method. So the
method m() of class B is not well-typed and method m() of class C is well-typed.
This conservative solution has some practical consequences: ﬁelds’ policies
must be cyclic; they must be able to reach the same state at some point. As
such, policy languages must take that into consideration in order to be properly
used. Another consequence are set methods. In Figure 9, we have two possible
implementations for a set method for class C. The ﬁrst is not well-typed, since we
are assigning an access view (other) with an empty policy to the ﬁeld, changing
its policy to an empty policy also, breaking the invariant. To correctly modify
the value of a ﬁeld, one must apply an authorization to change the new access
view’s policy to the same as the ﬁeld’s, as in the second set. This reﬂects an
intuitive argument: if we have ﬁxed privileges over some data and it is updated
to another data, we must have the same privileges over the new data to be able
to perform the same actions. On the other hand, if the initial policy of a ﬁeld
is empty, a set method can be quite simple, but when using the ﬁeld, one must
apply an authorization to gain access to it, as we can see in Figure 8.
Self reference this. The self reference is considered to be a special access view;
it has full access ({∗}) to the object. A perfect analogy is the diﬀerence between
user space and administrator space. While in user space there are operations
that cannot be performed, in administrator space one can do anything.

72
M. Pires and L. Caires
class A : { (f ; g)* } {
int f() { 0 }
int g() { 0 }
}
class B : { m* } {
A a = new A;
// a: A{ (f;g)* }
int m()
{ this.a.f() }
}
class C : { m* } {
A a = new A;
// a: A{ (f;g)* }
int m()
{ this.a.f(); this.a.g() }
}
Fig. 7. Field’s policies problem
class D { (set+m)* } {
A a = null ; // a: A{ }
void set (A other) {
this.a = other;
// a:A{ } --> OK
}
int m(Auth auth) {
this.a.g(); // --> ERROR
authorize this.a : auth
case { g;g } :
// --> a : A{ g;g }
{this.a.g(); this.a.g()}
// --> a : A{ }
case error : { 0 }
// --> a : A{ }
// a : A{ } --> INV. OK
}
}
Fig. 8. Field with empty policy
int set (A other) {
this.a = other; 0
// a:A{} --> ERROR
}
int set(A other, Auth auth){
authorize other : auth
case { (f+g)* } :
{ this.a = other; 0 }
// a: A{ (f+g)* } --> OK
case error: { 0 }
// a: A{ (f+g)* } --> OK
}
Fig. 9. Set method implementations
class E : { (f ; g)* } {
int f() { this.g() }
int g() { this.h() } // OK
int h() { 0 }
}
{
E e = new E; // e:E{(f;g)*}
e.f(); e.g() // ALL OK
}
Fig. 10. Self reference usage
class F : { m* } {
int m(A a1, A a2,
Auth auth){
authorize a1 : auth
case { f;g } : { //(1)
authorize a2 : auth
case { f;g } : { //(2)
a1.f(); a2.f();
a2.g(); a1.g() //(3)
}
case error { 0 } }
case error { 0 }
}
}
{
A a = new A;
F f = new F;
Auth auth =
authorization(a, {f;g});
f.m(a, a, auth); // (4)
}
Fig. 11. Aliasing example
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
73
In our case, normal access views are protected with a policy, restricting which
methods can be called. The access view this does not have that restriction -
it can call any method, any time. As we can see in Figure 10, in method g()
we call method h() with the self reference. Also, notice that the maximal class
policy of E does not allow method h() to be called. So, only the self reference has
access to it, just as a private method. Finally, method calls performed by the
self reference, as in method f(), do not interfere with any method caller’s policy.
Aliasing. In this work, access view aliasing is not a problem with respect to
policies, since each access view has its own policy. Consider Figure 11. When we
call method m() in (4) with the same access view, we are introducing aliasing
inside the method. However, the access views a1 and a2 are considered diﬀerent
by the type system. After instruction (1), the access view a1 has policy {f; g}
and only after instruction (2), access view a2 will also have policy {f; g}. Though
they are equal, they are not the same. So, the sequence of calls in (3) is well-typed
and safe to execute. Fields, representing the object state, could be the source
of aliasing problems. However, since we consider ﬁelds’ policies to be invariant,
after any method call the object’s type state does not change.
Therefore, our solution has the beneﬁt of being a statically veriﬁed dynamic
access control mechanism that does not have aliasing problems.
5
Related Work
Researchers have explored access control in many diﬀerent environments, from
logics for access control [7,2], to process calculus with access control [3,1] and
programming languages with native access control support [10,21,13]. A common
technique used to ensure access control is the development of a type system.
Bengtson et al. developed a language with reﬁnement types [6], where types
are extended with access formulas that need to hold to use the respective value.
Fournet et al. [10] developed a type system for a process calculus where a special
message is typed with a logical expression that needs to be previously assumed
for the channel to be well-typed. Both works use primitives to create and ver-
ify access conditions (assume/assert, statements/expectations), as we do with
authorizations, but our authorizations are ﬁrst class values, and we develop the
multiple view approach. It would be interesting to investigate how our policy
language could be deﬁned as a more expressive authorization logic.
Fable [21] is a programming language by Swamy et al. where programmers
create labels to protect data and attach them to individual value types. So,
each variable has a label restricting its usage, much like our policies can control
methods calls. However, we use ﬁrst class authorizations capable of changing
policies, whereas they deﬁne functions to change the labels of values.
Jia et al. [13] developed the AURA language with a type system that contains
a logic to create authorizations. These authorizations are also values of the lan-
guage, though they are context dependent and, consequently, are not ﬁrst class
values. Our authorizations, on the other hand, can be stored on data structures,
used as an argument to a method call or as a return value.

74
M. Pires and L. Caires
Skalka and Smith [20] and Bartoletti et al. [4,5] combine type and eﬀect
systems with model-checking techniques, where the type systems infer a sequence
of events, called history, and use model-checking over the history to verify the
desired access policies. However, these policies need to be followed by all users
of a resource, while in our work, users have their own policy to follow.
With session types introduced in [12,8,9], interfaces are typed with a sequence
of events describing the behavior or protocol for that interface, i.e., the order by
which messages are transmitted with that interface, including the type of each
message. Policies held by access views in our approach could considered as the
behavior of the respective object, but while a session type is assign to an interface
and all users must follow (and share) the same protocol, with access views each
user has its own policy, describing the privileges they might use. Additionally,
access view’s policies can be modiﬁed to a completely diﬀerent one, whereas
session types ﬁx the usage protocol once for all.
In user-based access control (UCON [16]) each component of an access control
system is categorized by how and when the usage decisions should be performed
to authorize the access to objects by subjects depending on their rights. In our
framework, an access policy is both a mutable subject attribute and a right.
The decision process deﬁned by the UCON model can be seen as our authorize
command, which veriﬁes if a given authorization (another subject attribute,
in UCON) is applicable to an object (deﬁnition 2). Hence, this is a pre-decision
process. Authorizations in UCON are function predicates that decide if a subject
can perform the requested rights on the object. This is analogue to our subset
relation between policies (π <: π′), which validates the requested policy with
an authorized one. Finally, our step relation (π
m
→π′) is a post-update in the
UCON model, changing the corresponding access view’s policy (S[u ←π]).
6
Concluding Remarks
We have presented a provably correct type and eﬀect system which guarantees
an authorized usage of objects with respect to policies. The system supports
dynamic modiﬁcation of policies using ﬁrst class authorizations, and keeps track
of diﬀerent policies for a same object by means of access views. Access views
allow diﬀerent users to exercise diﬀerent authorized usages for the same object,
unlike in other approaches in which a policy is attached to the protected object,
and not to the user-object relationship. Additionally, our type system is generic
and can be instantiated with diﬀerent access control policy deﬁnition languages.
As future work, we need to analyze how access views and authorizations can
be eﬃciently implemented in a distributed run-time environment. In particu-
lar, authorization values must be manipulated using cryptographic operations.
Additionally, as we are dealing with an object-oriented language, we need to
analyze the impact that the maximal class policy has over inheritance, since it is
the only class-depended property we have introduced. Exception handling and
other extensions to the core language should also be considered to determine the
practical applicability of our solution. Finally, we could consider the delegation
www.ebook3000.com

A Type System for Access Control Views in Object-Oriented Languages
75
of authorizations as an authorized capability and design richer policy languages.
An implementation of our typechecking algorithm is publicly available in [18].
Acknowledgments. We acknowledge the Certiﬁed Interfaces research project by
CMU-Portugal. We also would like to thank Bernardo Toninho and Francisco
Martins for interesting insights on some version of this work, and the anonymous
referees for their comments.
References
1. Abadi, M.: Secrecy by typing in security protocols. J. ACM 46(5), 749–786 (1999)
2. Abadi, M.: Logic in access control. In: Proceedings of LICS 2003, pp. 228–233.
IEEE Computer Society, Los Alamitos (2003)
3. Abadi, M., Burrows, M., Lampson, B., Plotkin, G.: A calculus for access control
in distributed systems. ACM Trans. Program. Lang. Syst. 15(4), 706–734 (1993)
4. Bartoletti, M., Degano, P., Ferrari, G.: History-based access control with local
policies. In: Proc. of Foundations of Software Science and Computation Structure
2005, pp. 316–332. Springer, Heidelberg (2005)
5. Bartoletti, M., Degano, P., Ferrari, G., Zunino, R.: Types and eﬀects for resource
usage analysis. In: Proc. of Foundations of Software Science and Computation
Structure 2007. LNCS, pp. 32–47. Springer, Heidelberg (2007)
6. Bengtson, J., Bhargavan, K., Fournet, C., Gordon, A.D., Maﬀeis, S.: Reﬁnement
types for secure implementations. In: CSF 2008: Proceedings of the 2008 21st IEEE
Computer Security Foundations Symposium, pp. 17–32. IEEE Computer Society
Press, Los Alamitos (2008)
7. Crampton, J., Loizou, G.: A logic of access control. The Computer Journal 44,
54–66 (2001)
8. Dezani-Ciancaglini, M., Mostrous, D., Yoshida, N., Drossopoulou, S.: Session types
for object-oriented languages. In: Thomas, D. (ed.) ECOOP 2006. LNCS, vol. 4067,
pp. 328–352. Springer, Heidelberg (2006)
9. Dezani-Ciancaglini, M., Drossopoulou, S., Mostrous, D., Yoshida, N.: Objects and
session types. Inf. Comput. 207(5), 595–641 (2009)
10. Fournet, C., Gordon, A.D., Maﬀeis, S.: A type discipline for authorization policies.
In: Sagiv, M. (ed.) ESOP 2005. LNCS, vol. 3444, pp. 141–156. Springer, Heidelberg
(2005)
11. Gosling, J., Joy, B., Steele, G., Bracha, G.: Java(TM) Language Speciﬁcation, 3rd
edn. (Java (Addison-Wesley)) Addison-Wesley Professional, Reading (2005)
12. Honda, K.: Types for dyadic interaction. In: Best, E. (ed.) CONCUR 1993. LNCS,
vol. 715, pp. 509–523. Springer, Heidelberg (1993)
13. Jia, L., Vaughan, J.A., Mazurak, K., Zhao, J., Zarko, L., Schorr, J., Zdancewic,
S.: Aura: a programming language for authorization and audit. In: ICFP 2008:
Proceeding of the 13th ACM SIGPLAN international conference on Functional
programming, pp. 27–38. ACM Press, New York (2008)
14. Kozen, D.: Language-based security. In: Mathematical Foundations of Computer
Science, pp. 284–298. Springer, Heidelberg (1999)
15. Nielson, F., Nielson, H.R.: Type and eﬀect systems. ACM Computing Surveys,
114–136 (1999)
16. Park, J., Sandhu, R.: The UCONABC usage control model. ACM Trans. Inf. Syst.
Secur. 7(1), 128–174 (2004)

76
M. Pires and L. Caires
17. Pires, M.: A type system for access control in an object-oriented language. Master’s
thesis, Faculdade de Ciˆencias e Tecnologia, Universidade Nova de Lisboa (2009)
18. PLASTIC. Plastic homepage (2009), http://ctp.di.fct.unl.pt/PLASTIC/
19. Schneider, F.B., Morrisett, G., Harper, R.: A language-based approach to security.
In: Informatics: 10 Years Back, 10 Years Ahead, pp. 86–101. Springer, Heidelberg
(2000)
20. Skalka, C., Smith, S.: History eﬀects and veriﬁcation. In: The 2nd Asian Symp on
Programming Languages and Systems, pp. 107–128. Springer, Heidelberg (2004)
21. Swamy, N., Corcoran, B.J., Hicks, M.: Fable: A language for enforcing user-deﬁned
security policies. IEEE Symp. on Security and Privacy (2008)
22. Wright, A.K., Felleisen, M.: A syntactic approach to type soundness. Information
and Computation 115, 38–94 (1994)
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11⋆
Andrea Falcone and Riccardo Focardi
Universit`a Ca’ Foscari di Venezia, Italy
andrea.falcone@tin.it, focardi@dsi.unive.it
Abstract. PKCS#11 is a standard API to cryptographic devices such
as smarcards, hardware security modules and usb crypto-tokens. Though
widely adopted, this API has been shown to be prone to attacks in which
a malicious user gains access to the sensitive keys stored in the devices. In
2008, Delaune, Kremer and Steel proposed a model to formally reason on
this kind of attacks. We extend this model to also describe ﬂaws that are
based on integrity violations of the stored keys. In particular, we consider
scenarios in which a malicious overwriting of keys might fool honest users
into using attacker’s own keys, while performing sensitive operations.
We further enrich the model with a trusted key mechanism ensuring that
only controlled, non-tampered keys are used in cryptographic operations,
and we show how this modiﬁed API prevents the above mentioned key-
replacement attacks.
1
Introduction
PKCS#11 [9] deﬁnes an API to cryptographic devices, such as smartcards, hard-
ware security modules and usb crypto-tokens. Apart from providing access to
the functionalities oﬀered by devices as, e.g., data encryption, the API is speciﬁ-
cally developed to carefully manipulate cryptographic keys. As an example, keys
marked as sensitive should never be accessible as plaintext outside the device:
an application requiring the encryption of conﬁdential data with a sensitive key
stored in the device, will refer to the key via a handle pointing to the location
where the key is stored; the cryptographic operation will then happen inside the
device without exposing the key to the external, untrusted world. Access to de-
vices is regulated via a PIN-based authentication check but RSA Security clearly
states that PIN leakage, foreseeable, e.g., if the device is used on an untrusted
machine, should not ruin the security of PKCS#11 devices.
Following this consideration, the attacker is assumed to execute any possible
sequence of legal API calls. Even under this fairly strong attacker model, the
secrecy of sensitive keys should still be preserved. Unfortunately, in [4] Clulow
showed that this is not the case: there exist rather simple API call sequences that
enable to recover the value of a sensitive key. The simplest example is the wrap-
decrypt attack in which a sensitive key is wrapped, i.e. encrypted, with another
key which has been generated with the double role of wrapping (sensitive) keys
⋆Work partially supported by Miur’07 Project SOFT: “Security Oriented Formal
Techniques”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 77–94, 2010.
© Springer-Verlag Berlin Heidelberg 2010

78
A. Falcone and R. Focardi
and decrypting data. These roles are, indeed, conﬂicting. It is clear, in fact, that
once the sensitive key has been wrapped and exported, encrypted, out of the
device it is suﬃcient to ask for decryption to obtain the key as plaintext.
In [5], Delaune, Kremer and Steel (DKS) proposed a model of PKCS#11 for
formally reasoning on Clulow’s attacks. We propose an extension of the DKS-
model, in order to reason on how both an attacker and a regular user act on
a cryptographic device. The model can therefore explore how the users’ actions
would inﬂuence each other’s, accounting for some new capabilities we give to the
attacker:
1. overwriting of keys in the device;
2. interception of messages sent on the network by the regular user;
3. disconnection from the system, interrupting the session with the device.
The ﬁrst capability takes advantage of a vulnerability we found in the API,
which may fool the regular user into using some untrusted key. The second
capability is typical of attacker models for protocol analysis but is not considered
in the DKS-model where there is no distinction between the API calls performed
by the honest user and the ones performed by the attacker. For example, in
DKS, the result of an encryption call is directly available to the attacker. In our
model, this is true only if the call has been performed by the attacker, otherwise
the ciphertext has to be explicitly intercepted. The third capability is useful to
distinguish attacks where the opponent is always connected to the device, like
the ones described in [5], from attacks requiring only a temporary connection. In
this way we can describe rather convenient scenarios for the attacker who does
not need to constantly break into the user’s system in order to stay connected
to the device.
Disconnection might also be caused by an external event. Suppose, for exam-
ple, that the attacker is be able to break the integrity of a key in a token only
when it is used on a speciﬁc compromised machine, at some public access point.
Any subsequent use of that key will be potentially compromised, even when the
user connects from her highly secure workstation, inaccessible to any attacker.
Our extended model is able to capture this behaviour. In fact, we detail a sce-
nario in which the attacker exploits the loss of one key integrity and manages
to violate the secrecy of some sensitive data subsequently sent on the network,
encrypted using the tampered key. The second part of the attack does not re-
quire the attacker to be connected to the device and is independent of where the
token is used next.
It is worth noticing that dropping the explicit disconnect event would still
ﬁnd the same attack sequences, as disconnecting simply reduces the actions that
the attacker can perform. However, discovering that an attack can be performed,
even partially, in a disconnected state is interesting, as it enables the new scenar-
ios described above and points out a dangerous ﬂaw in the API enabling future
exploits even when the token becomes inaccessible to the attacker.
We conclude by studying a modiﬁed API with a simple mechanism to ensure
that only controlled, non-tampered keys are used in sensitive operations, and
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
79
then show how this countermeasure impedes the previously described attack. The
mechanism is based on the trusted key attribute of PKCS#11 used to prevent
exporting keys in an insecure way, and is extended to any key involved in sensitive
operations, i.e., any such key is required to be trusted. As trusted keys cannot
be set by regular users, this prevents the key integrity problems discussed above.
It is quite surprising that a similar mechanism is not mandatory in the API.
As a matter of fact, in all the experiments we have performed on real crypto-
tokens and smartcard, we have ascertained that no one supports the trusted key
attribute.
The paper is organized as follows. In Section 2 we introduce the DKS-model
[5]. In Section 3 we present the vulnerability we found concerning the integrity of
keys, we introduce our model and we illustrate its adequacy through a detailed
example of attack; in Section 4 we reﬁne this ﬁrst model, tweaking a small but
signiﬁcant detail and giving birth to a variation of the model; we then formally
compare the relative expressive power of the two proposals. Finally, in Section 5
we study a modiﬁed API preventing the previously described attack. Section 6
gives some concluding remarks.
2
Background: The DKS-Model of PKCS#11
In [5], Delaune, Kremer and Steel (DKS) proposed a model of PKCS#11 for
formally reasoning on API-level attacks. This model is the starting point of our
work and we describe it in the following.
Terms. Let N be a possibly inﬁnite set of names, representing keys, data values,
nonces, etc., and X a possibly inﬁnite set of variables. Let also Σ be a ﬁnite set of
function symbols, with arity ar : Σ →N, representing, e.g., handles to keys and
cryptographic primitives. The set of plain terms PT is deﬁned by the following
grammar:
t, ti := x
x ∈X
| n
n ∈N
| f (t1, . . . , tn)
f ∈Σ and ar (f) = n
PKCS#11 speciﬁes a number of diﬀerent attributes for keys and data stored
in the devices, dictating their respective usage. For example, a sensitive key
should never be exported from the device as plaintext. Let now A be a set
of unary function symbols disjoint from Σ, called attributes. The set of at-
tribute terms AT is built by applying these attributes to plain terms, i.e.,
AT = {att (t) | att ∈A, t ∈PT }. Attribute terms are interpreted as proposi-
tions, meaning that there will be a truth value associated with them, represent-
ing the value of a given attribute of some plain term. A literal is an expression
a or ¬a where a ∈AT . The set of all terms is T = PT ∪AT . Finally, we denote
as σ substitutions of variables into terms and we write tσ to note the application
of σ to all variables of t.
Example 1. Term senc (k2, k1) ∈PT , with k1, k2 ∈N, represents the symmetric
encryption of key k2 under key k1. Terms h(n1, k1) and h(n2, k2) specify two

80
A. Falcone and R. Focardi
handles n1, n2 to the respective keys k1, k2. Attributes sensitive(n2), extract(n2)
and wrap(n1) speciﬁes that k2 is sensitive and extractable and k1 is a key for
wrapping, i.e., encrypting other extractable, possibly sensitive keys. Wrapping
keys, such as k1, is useful to export sensitive and extractable keys in a secure,
encrypted form.
Syntax. Rules for modelling APIs are expressed in the form
T ; L
new ˜n
−−−−→T ′; L′
where T, T ′ ⊆PT are plain terms, L and L′ are sets of literals, ˜n ∈N is a set of
names. T and L specify the conditions under which the rule is applicable, whereas
T ′ and L′ specify what will be the result of this rule, if applied. Intuitively, the
rule can be executed if all the terms in T are present and all the literals in L
are evaluated to true in the current state. We will see, in fact, that a state is
a pair of two elements: a set of ground plain terms S, which represents data in
the attacker knowledge, and a partial valuation function V , which maintains the
values of the key attributes. The eﬀect of the rule is that the terms in T ′ are
added to the state and the valuation of the attributes is updated according to
L′. Label ‘new ˜n’ binds names ˜n in T ′ and L′, forcing them to be diﬀerent from
all the other names in the state. This mechanism, named fresh renaming, allows
for modelling nonce or key generation.
Example 2. Symmetric encryption is modelled as follows.
h (x1, y1) , y2; encrypt (x1) →senc (y2, y1)
Intuitively, if the attacker knows some data y2 that he wants to encrypt and he
also has a reference, represented by the handle term h (x1, y1), to some internally
stored key y1 with the encrypt attribute set, then the attacker may perform an
API call and obtain the ciphertext resulting from the encryption of y2 under key
y1, i.e., senc (y2, y1).
The DKS-model rules for more PKCS#11 functionalities are reported in
Appendix A.
Semantics. As previously mentioned, A state (S, V ) is a pair of two elements:
a set of ground plain terms S, which represents data in the attacker knowledge,
and a partial valuation function V , which maintains the boolean values of the
key attributes. V is extended to literals and to sets of literals as expected, i.e.,
V (¬a) = ¬V (a) and V (L) = 
l∈L V (l) when V (l) is deﬁned for all l ∈L.
Given a rule1 T ; L →T ′; L′ transition (S, V ) ⇝(S′, V ′) may take place if
there exists a grounding substitution θ for the rule such that:
– T θ ⊆S, i.e., terms in T are in the enemy knowledge;
– V (Lθ) = true, i.e., all literals in L are evaluated to true.
1 We omit here the formalization of rule fresh renaming, consisting of removing label
‘new ˜n’ from the arrow after having renamed names ˜n so that they diﬀer from all
the other names in the rule and in the state. For more detail see [5].
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
81
Then, S′ = S ∪T ′θ, and function V ′ is deﬁned as follows:
dom (V ′) = dom (V ) ∪{a | (a ∈L′θ) ∨(¬a ∈L′θ)}
and ∀a ∈dom (V ′):
V ′ (a) =
⎧
⎪
⎨
⎪
⎩
true
if a ∈L′θ
false
if ¬a ∈L′θ
V (a)
otherwise
Intuitively, literals in L′ override the evaluation function for their attributes. All
the other attributes are untouched.
Example 3 (The wrap-decrypt attack, single key variant). We show how the mod-
els work by illustrating a variant of the wrap-decrypt attack of [4] with one single
key. The DKS-rules for wrap and decrypt are as follows:
h (x1, y1) , h (x2, y2) ; wrap (x1) , extract (x2) →senc (y2, y1)
h (x1, y1) , senc (y2, y1) ; decrypt (x1) →y2
It now suﬃcient to consider an initial state (S0, V0) with S0 = {h(n, k)} and
V0(wrap(n)) = V0(extract(n)) = V0(decrypt(n)) = V0(sensitive(n)) = true.
Intuitively, we consider a single key k with handle n, with the attributes wrap,
extract, decrypt and sensitive set. Notice that, being it sensitive, k should never
be extracted from the device as plaintext. Intuitively, being k both a wrapping
and extractable key, it can be wrapped under itself. To see this, formally, consider
the ﬁrst rule above (wrap) and the substitution θ = {n/x1, n/x2, k/y1, k/y2}.
We have:
T θ = {h (x1, y1) , h (x2, y2)}θ
= {h(n, k)} ⊆S0
V0(Lθ) = V0({wrap (x1) , extract (x2)}θ)
= V0({wrap (n) , extract (n)})
= V0(wrap (n)) ∧V0(extract (n)) = true
Thus (S0, V0) ⇝(S0 ∪{senc(k, k)}, V0), with term senc(k, k) representing the
wrapping of k under itself and obtained as T ′θ = senc (y2, y1) θ = senc(k, k).
Now, being k also a decryption key it can be used to decrypt term senc(k, k)
giving the sensitive key k as a plaintext. We leave as an exercise the formal
application of the decrypt rule using substitution θ′ = {n/x1, k/y1, k/y2} and
giving transition (S0 ∪{senc(k, k)}, V0) ⇝(S0 ∪{senc(k, k), k}, V0). This attack
shows that wrap and decrypt are conﬂicting attributes for keys: they should never
be both set on the same key.
The decision procedure arising from the model has been automated via model
checking, leading to a series of experiments reproducing the attacks already
shown by Clulow in [4], and also ﬁnding new ones [11]. The authors also proposed
‘patches’ on the API preventing the attacks in the model.

82
A. Falcone and R. Focardi
3
Extending the Model: Key Integrity
We shift the focus of the analysis by taking into account a new vulnerability
we observed on real tokens: the attacker is able to overwrite keys stored on the
token, in such a way that the user application will refer the new substitute key
as it was its own. Assume one application refers to keys by means of their label
attribute, a short textual description, managed by the applications themselves,
on which no security policy is enforced by the API. Thus, an attacker logged
into the token can overwrite one of the regular user’s keys, say k1, with another
key of his choice, k2, by simply copying k1’s label onto k2 and then deleting k1
from the token. As a result, the next time a honest user will refer the target key
k1 by means of its label, he will be given access to the attacker’s key k2, in a
transparent way. We have tested this attack on diﬀerent USB crypto-tokens we
possess.
We now extend the DKS-model of previous section so to also represent the
above mentioned scenario. In particular, we describe the actions of two users:
together with the enemy E we take into consideration a regular, trusted user T .
Their sessions on the device may run concurrently, thus each step of the system
represents an API call performed by one of them. States are enlarged to provide
both users with their own sets of known terms, namely SE and ST . The states
also maintain the attribute values for token keys and the connection status of
the users. The state of the system may be altered as a result of the users’ actions:
for instance an encryption operation would generate in the model a new term,
representing the resulting ciphertext, to be added to the knowledge set of the
user requesting this operation.
Having both an attacker and a regular user acting at the same time on the
token, this model can describe scenarios in which the two aﬀect each other’s
actions. Based on this idea, we give the enemy new capabilities: the ability to
intercept information sent on the network by the trusted user and the ability to
indirectly aﬀect the actions of the regular user, by modifying keys stored on the
token. In fact, in our model the attacker has the ability to overwrite keys stored
on the device, as discussed above.
Furthermore, the enemy has the ability to disconnect himself from the sys-
tem, ceasing the session with the device. Disconnecting from the token means
losing all the references to the keys stored on it and also losing the ability to
require cryptographic operations from the device. Note that nonetheless, even
if disconnected, the attacker can still perform cryptographic operations imple-
mented in software libraries. As discussed in the Introduction, this allows for
more practical and convenient attacks, in which a compromised key also com-
promises subsequent cryptographic operations based on such a key. The enemy,
for example, does not need to be connected to the device to decrypt any sensitive
data encrypted under a previously tampered key he possesses. For the sake of
simplicity we omit modelling a complementary ‘reconnect’ action but this could
easily accounted for in the model.
We formalize the above ideas by extending the DKS-model of section 2.
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
83
Table 1. Rules for key overwriting and disconnected users
Key overwriting
h (x1, y2) , senc (y1, y2) ; unwrap (x1)
used n
−−−−→h (n, y1) ; extract (n) , L
h (x1, priv (z)) , aenc (y1, pub (z)) ; unwrap (x1)
used n
−−−−→h (n, y1) ; extract (n) , L
h (x1, y2) , senc (priv (z) , y2) ; unwrap (x1)
used n
−−−−→h (n, priv (z)) ; extract (n) , L
where, L = ¬wrap (n) , ¬unwrap (n) , ¬encrypt (n) , ¬decrypt (n) , ¬sensitive (n).
Disconnected
x, y −→senc (x, y)
senc (x, y) , y −→x
pub (z) , x −→aenc (x, pub (z))
aenc (x, pub (z)) , priv (z) −→x
Syntax. We extend rule syntax as follows:
T ; L
new ˜n, used ˜m
−−−−−−−−−→T ′; L′
The only diﬀerence is the new label ‘used ˜m’ forcing the usage of names occur-
ring in some handle already known by the user: when the rule is ﬁred, all the
occurrences in T ′ and L′ of names in ˜m must be replaced by names already in
use as the ﬁrst element m of a handle term h(m, k) in the actual user’s knowl-
edge set. This mechanism, named used renaming, allows us to model the key
overwriting operation, because it pinpoints handle names already in use, thus
selecting the handle to be replaced.
Key overwrite rules. Based on the extended syntax, we give new rules for
modelling key overwrite. As an example:
h (x1, y2) , senc (y1, y2) ; unwrap (x1)
used n
−−−−→h (n, y1) ; extract (n) , L
with L = ¬wrap (n) , ¬unwrap (n) , ¬encrypt (n) , ¬decrypt (n) , ¬sensitive (n),
models an unwrap operation overwriting key referred by handle n. Knowing the
handle h (x1, y2) and the wrapped key senc (y1, y2), and assuming x1 refers to
an unwrapping key, we can unwrap y1 giving a used handle n to refer to it, thus
overwriting a key already stored in the device. The appropriate attributes are
set to the unwrapped key. Notice that this rule corresponds to the ﬁrst unwrap
DKS-rule of appendix A with ‘new n’ replaced by ‘used n’.
The other rules to model key overwriting for asymmetric cryptography are
given in table 1. In the same table we also give rules for operations implemented
in software and called ‘Disconnected’, being them executable even when users
are not connected to the token. These rules are not described in [5] but they are
implicitly part of the DKS-model too, as they correspond to standard Dolev-Yao
attackers.

84
A. Falcone and R. Focardi
Semantics. A state is now represented by a tuple (ST , SE, V, CT , CE), where
– ST , SE ⊆PT represent the knowledge of the trusted user and the enemy;
– V , as before, is a partial boolean function evaluating attributes AT ;
– CT , CE ∈{true, false} are two booleans indicating the connection status of
the two users.
We deﬁne the following operations on set of ground plain terms:
H (S) = {h (n, k) ∈S | n ∈N}
ˆH (S1, S2) = {h (n, k) ∈S1 | n ∈N and ∃k′ such that h (n, k′) ∈S2}
H(S) returns the set of all the handles in a given set S of ground plain terms.
ˆH(S1, S2) returns all the handles in S1 also referring to keys in S2.
Example 4. Let SE = {k1, h (n1, k2) , h (n2, k3)} and ST = {k1, h (n1, k4)}, then
H (SE) = {h (n1, k2) , h (n2, k3)}, H(ST ) = {h (n1, k4)} give the handles in SE
and ST . Moreover, ˆH(SE, ST ) = {h (n1, k2)} since n1 refers to a key also in ST .
Similarly, ˆH(ST , SE) = {h (n1, k4)}.
In the following we will use σ ∈{T, E} to select one of the users, with the
complement ¯σ deﬁned as expected, i.e. ¯T = E and ¯E = T . The system can
evolve through one of the following transitions:
– σ-call: user σ performs a call to the API or an operation in software, the
former only possible if she is connected;
– Send: one or more terms of the trusted user’s knowledge become part of the
attacker’s knowledge. This mimics the attacker intercepting data sent over
the network by the regular user;
– Disconnect: the attacker closes his session with the token and loses all his
handles.
We describe the three kinds of transition in detail.
The σ-call transition. This transition extends the execution of a rule in the DKS-
model: ﬁrst, if the rule is not one of the ‘Disconnected’ (table 1), i.e. if the rule
uses handles referring to keys stored in the device, the user must be connected.
Second, the premises of the rule must be in the knowledge of user σ and the
non-handle terms generated by the rule-ﬁring are visible only to such user, i.e.
the one performing the call, while new handles are visible to both. As for DKS-
model, we assume rules have been renamed so that new names are diﬀerent from
any other name around (fresh renaming). Moreover, for each label ‘used m’, we
rename m so that there exists h(m, k) ∈H(ST ) ∪H(SE), i.e., m is the handle
of some key in one of the knowledge sets (used renaming).
Given T ; L →T ′; L′ transition (ST , SE, V, CT , CE) ⇝(S′
T , S′
E, V ′, CT , CE)
takes place if there exists a grounding substitution θ for the rule such that
– Cσ ∨H (T θ) = Lθ = H (T ′θ) = L′θ = ∅, i.e., either the user is connected or
the rule does not refer to any handles and attributes (it is a ‘Disconnected’);
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
85
– T θ ⊆Sσ, i.e., terms in T θ are in the knowledge of user σ;
– V(Lθ) = true, i.e., all literals in L are evaluated to true.
Then S′
σ = Sσ \ ˆH (Sσ, T ′θ) ∪T ′θ and
S′
¯σ =

S¯σ \ ˆH (S¯σ, T ′θ) ∪H (T ′θ)
if C¯σ
S¯σ
if ¬C¯σ
Intuitively, the terms in the rule consequence are added to the knowledge set of
the σ-user; terms that represent homonym handles, i.e. key handles that share
their ﬁrst argument with some handle in the rule consequence, must be sub-
tracted from the system, since they are going to be overwritten. The manipula-
tion of token keys is immediately visible to all the sessions on the token, thus the
other user’s knowledge set (i.e. S¯σ) must be updated accordingly: only handle
terms in the rule consequence are added to it and homonym handles are sub-
tracted; of course, if instead ¯σ is disconnected, there will be no change on user’s
knowledge.
Function V ′ is deﬁned as follows
dom (V ′) = dom (V ) \ ˆA ∪{a | (a ∈L′θ) ∨(¬a ∈L′θ)}
where ˆA =

att (n) | ∃h (n, k) ∈ˆH (Sσ, T ′θ) , att ∈A, n ∈N

is the set of the
attribute-terms in the V function domain that will be “forgotten” due to their
respective handles being overwritten in this transition. Also, attribute terms in
the rule consequence are added to the function domain. V ′ values then are deﬁned
as in DKS-model (section 2). Finally, notice that CT and CE are unchanged since
no disconnection can happen within this transition.
Example 5 (σ-call transition). We illustrate how a symmetric encryption opera-
tion performed by the attacker is represented in the model. Consider the names
set N = {m, n, k1, k2}. Assume, also, that at step number i the state is qi =
(ST,i, SE,i, Vi, true, true) with SE,i = {h (n, k1) , k2}, Vi (encrypt (n)) = true. In
state qi the attacker has thus access to key k1 through the handle named n,
which is entitled for encryption, and he knows the value of key k2; also, both
the users are connected to the token. The values of the other state elements are
irrelevant here.
To carry out the next step in computation, an applicable transition must
be looked for, on behalf of either the regular user or the attacker. Of all the
applicable transitions in this state, an E-call for the symmetric encryption is
chosen. We report the corresponding rule here for convenience:
h (x1, y1) , y2; encrypt (x1) →senc (y2, y1)
There is no need for a renaming operation on this rule before execution, and it is
applicable in the current state qi with the substitution θi = {n/x1, k1/y1, k2/y2}:
in fact, we have that {h (x1, y1) , y2} θ1 ⊆SE,i and Vi ({encrypt (x1)} θ1) = true.
Therefore, qi ⇝qi+1 = (ST,i+1, SE,i+1, Vi+1, true, true) such that:

86
A. Falcone and R. Focardi
– ST,i+1 = ST,i
– SE,i+1 = SE,i ∪{senc (y2, y1)} θi = SE,i ∪{senc (k2, k1)}
– Vi+1 is such that
• dom (Vi+1) = dom (Vi)
• Vi+1 (encrypt (x1)) = true, thus Vi+1 = Vi
Example 6 (overwriting a key). We now illustrate what happens when ﬁring the
previously described key overwriting rule:
h (x1, y2) , senc (y1, y2) ; unwrap (x1)
used n
−−−−→h (n, y1) ; extract (n) , L
We give a synthetic representation of the scenario, detailing only the knowledge
set of the attacker, who is executing the rule.
step
SE
i
h (n1, k1) , senc (k3, k2) , h (n2, k2)
i+1
h (n1, k3) , senc (k3, k2) , h (n2, k2)
We see that key k3 is unwrapped and overwrites key k1. Formally, to ﬁre the
overwriting rule we ﬁrst have to perform the ‘used renaming’ operation obtaining:
h (x1, y2) , senc (y1, y2) ; unwrap (x1) −→h (n1, y1) ; extract (n1) , L {n1/n}
We have removed the ‘used n’ label by renaming n in a way it matches an existing
handle, in this case h (n1, k1). Now we can ﬁre the rule using the substitution
θi = {n2/x1, k2/y2, k3/y1}. We obtain that
SE,i+1 = SE,i \ ˆH (SE,i, T ′θ) ∪T ′θ =
= SE,i \ ˆH (SE,i, {h(n1, k3)}) ∪{h(n1, k3)} =
= SE,i \ {h(n1, k1)} ∪{h(n1, k3)} =
= {h (n1, k3) , senc (k3, k2) , h (n2, k2)}
Notice that the existing conﬂicting handle h(n1, k1) is correctly removed before
the new overwriting key h(n1, k3) is added to the knowledge.
The Send transition. When a user asks a device for generating a ciphertext it is
plausible that she is going to send it on the network or store it in a place she does
not completely trust. This transition models exactly the attacker intercepting
such an encrypted data.
Given a subset I ⊆ST of the trusted user’s knowledge set, the system can
perform a Send transition (ST , SE, V, CT , CE) ⇝(ST , SE ∪I, V, CT , CE).
Example 7 (Send transition). Suppose that the trusted user has to send a ci-
phertext on the network and that this will be intercepted by the attacker. Con-
sider the state is q = (ST , SE, V, CT , CE), where ST = {h (n1, k1) , senc (d, k1)},
meaning that the trusted user has access to the key k1 through the handle
named n1 and he also knows the ciphertext resulting from the encryption of
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
87
some data d under key k1. Furthermore, SE = {h (n1, k1)}. The values of the
other state elements are irrilevant here. In this state the Send transition is ap-
plicable, considering I ⊆ST , I = {senc (d, k1)}. Thus, we have q ⇝q′ =
(ST , SE ∪{senc (d, k1)} , V, CT , CE).
The Disconnect transition. Since this transition models the disconnection of the
attacker from the device with the consequence of losing access to all the token
keys, key handles must be deleted from the attacker’s knowledge set in the
process.
The system may evolve by a Disconnect transition (ST , SE, V, CT , CE) ⇝
(ST , SE \ H (SE) , V, CT , false).
Example 8 (Disconnect transition). Let q = (ST , SE, V, true, true), where SE =
{h (n1, k1) , h (n2, k2)}, meaning that the attacker has access to two keys through
their respective handles. A disconnect operation is issued and reﬂected in the
model as qi ⇝qi+1 = (ST , ∅, V, true, false).
A complete key overwriting attack. The features introduced in our model
make it possible to describe a scenario in which a key integrity violation is ex-
ploited by the attacker to illegally obtain secret data. The regular user possesses
some secret data d, which he needs to send on the net, after having encrypted it
to protect its secrecy. Also, recall that the attacker knows the regular user’s PIN
code for the token and he is also able to send commands to the token. Further-
more, the attacker knows which key will be used for the encryption operation
and has access to the corresponding handle (but not to the key value).
In Table 2 we give a summary of the attack sequence, detailing only the
knowledge sets; in bold font we emphasize what is new at each step. In the
initial state the device holds two keys and both users have handles to them: kt
is the trusted user’s key and ki is a wrap/unwrap key that will be exploited by
the attacker to import in the device his own key ke (initially not stored on the
device); the trusted user knows the sensitive data d.
During steps 1 and 2 the attacker illegally imports his key in the device,
through a process already shown in [5, section 5.3, experiment 2]: he encodes key
ke in the same format used for wrap operations and makes the device encrypt
it under ki. This yields the same result as ke was regularly wrapped by the
token. Then, the attacker calls an unwrap of this piece of data, creating as a
result a copy of ke in the device and obtaining a new handle for it. We adapted
this process to model the key overwriting operation: while unwrapping key ke,
the trusted user’s key is deleted after having copied its label onto ke. Note that
having key ki is not mandatory, it’s just for clarity: key kt itself could be used
to import ke, in case its unwrap attribute is set to true.
Having done this, at step 3 the attacker can disconnect himself from the sys-
tem: the user key kt as been replaced and any subsequent encryption (apparently)
based on such a key will be compromised, as show below.
At step 4 the trusted user encrypts d: she (unconsciously) refers to ke because
her reference is now pointing to it. At step 5 the trusted user sends the cipher-
text resulting from encryption on the net; the attacker intercepts it. Finally, at

88
A. Falcone and R. Focardi
Table 2. Key overwriting attack
step
transition
σ
ST
SE
0
-
-
d, h (t, kt) , h (i, ki)
h (t, kt) , h (i, ki) , ke
1
encrypt
E
d, h (t, kt) , h (i, ki)
h (t, kt) , h (i, ki) , ke, senc (ke, ki)
2
overwrite
E
d, h (t, ke) , h (i, ki)
h (t, ke) , h (i, ki) , ke, senc (keki)
3
Disconnect
-
d, h (t, ke) , h (i, ki)
ke, senc (keki)
4
encryption
T
d, h (t, ke) , h (i, ki),
senc (d, ke)
ke, senc (keki)
5
Send
-
d, h (t, ke) , h (i, ki),
senc (d, ke)
ke, senc (keki) , senc (d, ke)
6
decryption
(disconn.)
E
d, h (t, ke) , h (i, ki),
senc (d, ke)
ke, senc (keki) , senc (d, ke) , d
step 6 the attacker can decrypt the data d. Notice that this last operation is per-
formed oﬀ-line, possibly using some software implementation of the decryption
algorithm.
4
The H-Transition System
We give another formalization of the transition system, aiming at making it more
rational in the way it manages the handle terms: as can be seen in examples of
previous section the key handles are often redundantly maintained in both the
knowledge sets. In the new transition system, the handle terms are made explicit
from the knowledge sets and uniquely maintained in a new element in the state
vector, named H. The state becomes thus (H, ST , SE, V, CT , CE) and we note
the new transition as ⇝H. We now describe how σ-call, Send and Disconnect
are modiﬁed to accommodate the new separate information on handles.
The H σ-call transition. The σ-call transition requires the same conditions
as in the previous transition system to be met in order to execute a rule from
R (recall that a call to the API is only possible for connected users). The only
diﬀerence is that the handle terms are to be found in the H element of the state
vector. More speciﬁcally, in place of condition T θ ⊆Sσ, requiring that terms in
T θ are in the knowledge of user σ, we require:
– H (T θ) ⊆H, i.e., all the required handles are in H;
– T θ \ H (T θ) ⊆Sσ, i.e., all the required terms that are not handles are in the
knowledge of user σ.
If these conditions are met, then the system can evolve with the transition
(H, ST , SE, V, CT , CE) ⇝(H′, S′
T , S′
E, V ′, CT , CE), where:
H′ = H \ ˆH (H, T ′θ) ∪H (T ′θ)
K′
σ = Kσ ∪T ′θ \ H (T ′θ)
K′
σ = Kσ
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
89
Handle terms in the rule’s consequence are added to the handle set, accounting
for overwriting operations if needed. The non-handle terms in the rule’s con-
sequence are added to the knowledge set of the σ-user. Notice that for ¯σ-user
nothing is done, since handles are stored in the ‘centralized’ set H.
The attribute valuation function V is updated as before, carefully removing
all the attributes of overwritten keys.
The H Send transition. The H Send transition is analogous to the Send.
Given a subset I ⊆ST of terms in the trusted user’s knowledge set, the system
can perform (H, ST , SE, V, CT , CE) ⇝H (H, ST , SE ∪I, V, CT , CE).
The H Disconnect transition. The H Disconnect transition diﬀers from the
Disconnect in that no handle terms are deleted from the state vector; just the
connection ﬂag is updated: (H, ST , SE, V, CT , CE) ⇝H (H, ST , SE, V, CT , false).
Expressiveness results. We now show that the two presented models are
equivalent regarding their ability to describe evolutions of the token-users sys-
tem. We formalize and prove a theorem stating that, given a ⇝transition there
exists a correspondent ⇝H transition, and vice-versa. First we give some pre-
liminary deﬁnitions.
Let ¯H (S) = {S \ H (S)} be the set of all the non-handle terms in a set S..
We let also Q and QH denote the set of states in the two models.
Deﬁnition 1 (State encoding). The state encoding function · : Q −→QH
is deﬁned as (ST , SE, V, CT , CE) ≜

H (ST ) , ¯H (ST ) , ¯H (SE) , V, CT , CE

.
This deﬁnition of state encoding is based on the observation that the trusted user
never disconnects from the token: the H element of QH is ﬁlled with handles
taken explicitly from the trusted user’s knowledge set ST .
Deﬁnition 2 (Inverse state encoding). The inverse state encoding function
·−1 : QH −→Q is deﬁned as
(H, ST , SE, V, CT , CE)−1 ≜
 
ST −1, SE−1 , V, CT , CE
!
where Sσ−1 =

Sσ ∪H,
if Cσ
Sσ
if ¬Cσ
Next lemma states that ·−1 is the inverse of ·. Proof is given in Appendix B.
Lemma 1. ∀q ∈Q we have q−1 = q.
Relying on these encodings, we formalize our main result.
Theorem 1 (models equivalence). Let qi, qf ∈Q
qi ⇝qf
⇐⇒
qi ⇝H qf
The proof for this theorem is long, but rather mechanical; we give a sketch of it
in Appendix B.

90
A. Falcone and R. Focardi
5
Adding Trusted Keys
In this section, we propose a way to remedy the PKCS#11 API vulnerability
that lets an attacker break a key integrity, discussed above. The idea is based on
the use of one of the cryptographic key attributes, already part of the API: the
trusted attribute. As the name suggests, this attribute is used to discriminate
between trusted and untrusted keys. It is designed to be part of a mechanism
by which the API can prevent exporting keys in an insecure way, by forbidding
the usage of untrusted keys for wrapping sensitive keys. The trusted attribute
can only be set by the Security Oﬃcer in charge of the token. Since the Securiy
Oﬃcer’s PIN code is diﬀerent from that of the normal user and is never used on
untrusted hosts, the enemy cannot manipulate the trusted attribute.
Key integrity can be thus checked by just reading the value of its trusted
attribute: if the trusted attribute value is found to be false, it means the key has
possibly been tampered. Note that this check can be done either explicitly by
the user application, or implicitly by the API or the device itself.
In order to model this new mechanism, we simply check the trusted attribute
of every key appearing in a rule, before its application. To this aim, we need
to update all the rules for the σ-call transition which yield new key handles
and have them explicitly set the trusted attribute of the newly created handles
to false, as a default value. The rules aﬀected by this update are all the key
generation rules and all the unwrap rules. To this aim it is suﬃcient to extend
L with ¬trusted(n).
T σ-call semantics. We let T r (S) = {trusted (n) | ∃h (n, k) ∈S with n ∈N}
be the set of trusted attribute terms relative to all the handle terms in S. We add
to the rule applicability conditions a check for the trusted attribute value for all
the key handle terms in rule premises T . This new condition will only allow the
execution of a rule if each of the required key handles has the trusted attribute
set to true. Formally, this is achieved by asking V (T r (T )) = true.
Note that this new behavior does not directly forbid the key overwriting opera-
tion but it makes it possible to detect it in a simple way, as shown below.
Key overwriting attack and trusted keys. Given the above modiﬁcations to
the semantics, the example attack showed in Example 3 on page 87 is prevented.
In Table 3 on the next page we show how the attack sequence is revealed and
blocked. Suppose that initially the trusted attribute on the handle for kt is set
to true. After step 2 the attacker has successfully overwritten kt with ke, h(t, kt)
being replaced by h(t, ke); nonetheless, the new handle h(t, ke) does not possess
the trusted attribute. Thus, at step 4 the normal user’s request for a symmetric
encryption is refused, because the supplied key, referenced by the name t, fails
the trusted attribute check. Therefore, the same attack will not be possible. Note
that the key still gets overwritten, but the loss of integrity is disclosed.
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
91
Table 3. Key overwriting attack
st.
transition
σ
ST
SE
trusted (t)
0
-
-
d, h (t, kt) , h (i, ki)
h (t, kt) , h (i, ki) , ke
true
1
encryption
E
d, h (t, kt) , h (i, ki)
h (t, kt) , h (i, ki) , ke,
senc (ke, ki)
true
2
unwrap
E
d, h (t, ke) , h (i, ki)
h (t, ke) , h (i, ki) , ke,
senc (keki)
false
3
Disconnect
d, h (t, ke) , h (i, ki)
ke, senc (keki)
false
4
encryption
(not exec)
T
-
-
-
6
Conclusions
We have proposed an extension to an existent model for the PKCS#11 API, in
order to broaden its descriptive capabilities, taking into account the integrity of
cryptographic keys. We have granted the attacker some new capabilities, among
which the overwriting of keys on the token. We have given a result reasoning
on the expressiveness of two variations of the model semantics. Finally, we have
proposed a simple mechanism to ensure in the model that only non-tampered
keys are used in cryptographic operations.
The PKCS#11 API does not provide a way to satisfactorily manage the in-
tegrity of keys stored on a device, in a transparent manner for the user appli-
cations. We believe this is a major ﬂaw of the standard. As discussed in the
introduction, a user might need to perform some critical operation connecting
a crypto-token to an untrusted host and she would expect no one is allowed
to tamper with her crypto-device in a way that might compromise any further
usage of it. Connecting a crypto-device to an untrusted host should not, in fact,
compromise what is done/stored inside the device. We have shown that, apart
from known attacks where sensitive keys are extracted, an enemy might also
substitute a user key with one he knows and learn any data subsequently en-
crypted with such compromised key. This is, in our opinion, a rather irritating
behaviour for what is expected to be a ‘secure’ crypto-device.
Alternative ways to check key integrity, other than the proposed use of the
trusted attribute, could be: the use of cryptographic functions to validate key
values (e.g. digital signatures or MACs), of course having computational costs;
the introduction of a new attribute to specify that a key cannot be deleted,
making a key impossible to be overwritten.
The use of the trusted attribute we suggest as a countermeasure to the in-
tegrity issue has an advantage in that it is simple and fast. Also, it can be
implemented at several levels: at user application level, without the need to al-
ter the API or the devices, or at API or device level, gaining transparency for
the applications. A main disadvantage is that, since only keys with the trusted
attribute set to true are authorized for any operation, to be of use, a key needs to

92
A. Falcone and R. Focardi
be  ̏promoted˝right after its generation by the Security Oﬃcer in charge of the
token. This might become quite restrictive especially in applications that need
to establish new session-keys. We intend to explore alternative, less-restrictive,
solutions.
We are glad that the automated DKS framework of [11] has been extended
by Graham Steel so to include our model and analysis. In [12] it is possible to
ﬁnd the result of the performed experiments. First, a device patched to prevent
conﬁdentiality attack is modelled and analysed in the standard DKS-model. As
expected no attacks are found. Then, the model is extended so to capture key
overwriting and the same patched device is found to be vulnerable. The attack
sequence pointed out by the SATMC model checker is essentially the same as the
one presented in Table 2. It only diﬀers on what is encrypted with the tampered
key and then disclosed by the attacker: a freshly generated sensitive key ks1
instead of generic sensitive data d.
References
1. Armando, A., Compagna, L.: SAT-based model-checking for security protocols.
International Journal of Information Security 7(1) (January 2008)
2. Bond, M., Anderson, R.: API-Level Attacks on Embedded Systems. IEEE Com-
puter Magazine, 67–75 (2001)
3. Cimatti, et al.: NuSMV version 2: an OpenSource Tool for Symbolic Model Check-
ing. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS, vol. 2404, pp. 359–364.
Springer, Heidelberg (2002)
4. Clulow, J.: On the security of PKCS#11. In: Walter, C.D., Ko¸c, ¸C.K., Paar, C.
(eds.) CHES 2003. LNCS, vol. 2779, pp. 411–425. Springer, Heidelberg (2003)
5. Delaune, S., Kremer, S., Steel, G.: Formal analysis of PKCS#11. In: Proceed-
ings of the 21st IEEE Computer Security Foundations Symposium (CSF 2008),
Pittsburgh, PA, USA, June 2008, pp. 331–344. IEEE Computer Society Press, Los
Alamitos (2008)
6. Dolev, D., Yao, A.: On the security of public key protocols. IEEE Transactions in
Information Theory, 198–208 (1983)
7. International Telecommunication Union: X.690 - Abstract Syntax Notation One,
ASN.1 (2002)
8. RSA Laboratories: PKCS#8: Private-Key Information Syntax Standard (1993)
9. RSA Security Inc: PKCS #11 v.2.20: Cryptographic Token Interface Standard
(June 2004)
10. Steel, G.: Analysis of Security APIs FAQ,
http://www.lsv.ens-cachan.fr/~steel/security_APIs_FAQ.html
11. Steel, G.: Experiments: Secure Conﬁguration of PKCS11,
http://www.lsv.ens-cachan.fr/~steel/pkcs11/
12. Steel, G.: Experiments: Key Integrity in PKCS#11,
http://www.lsv.ens-cachan.fr/~steel/pkcs11/replacement.php
www.ebook3000.com

Formal Analysis of Key Integrity in PKCS#11
93
A
DKS-Rules Modelling Cryptographic Operations
Wrap
h (x1, y1) , h (x2, y2) ; wrap (x1) , extract (x2) →senc (y2, y1)
h (x1, priv (z)) , h (x2, y2) ; wrap (x1) , extract (x2) →aenc (y2, pub (z))
h (x1, y1) , h (x2, priv (z)) ; wrap (x1) , extract (x2) →senc (priv (z) , y1)
Unwrap
h(x1, y2), senc(y1, y2); unwrap(x1)
new n
−−−−→h(n, y1); extract(n), L
h(x1, priv(z)), aenc(y1, pub(z)); unwrap(x1)
new n
−−−−→h(n, y1); extract(n), L
h(x1, y2), senc(priv(z), y2); unwrap(x1)
new n
−−−−→h(n, priv(z)); extract(n), L
Key generation
new n,k1
−−−−−−→h (n, k1) ; ¬extract (n) , L
new n,s
−−−−−→h (n, priv(s)) , pub (s) ; ¬extract (n) , L
Encryption
h (x1, y1) , y2; encrypt (x1) →senc (y2, y1)
h (x1, priv (z)) , y1; encrypt (x1) →aenc (y1, pub (z))
Decryption
h (x1, y1) , senc (y2, y1) ; decrypt (x1) →y2
h (x1, priv (z)) , aenc (y2, pub (z)) ; decrypt (x1) →y2
Attribute set
h (x1, y1) ; ¬wrap (x1) →wrap (x1)
...
Attribute unset
h (x1, y1) ; wrap (x1) →¬wrap (x1)
...
where L = ¬wrap (n) , ¬unwrap (n) , ¬encrypt (n) , ¬decrypt (n) , ¬sensitive (n).
The ellipsis in the attribute set and unset rules indicates that similar rules exist
for other attributes.
B
Proofs from Section 4
To prove Lemma 1 we need the following simple lemma, stating that equality of
the handles in ST and SE is preserved by transitions as long as the attacker stays
connected. When he disconnects, the handles in SE are permanently removed.
This allows us to implicitly assume H(ST ) = H(SE) if CE and H(SE) = ∅
otherwise, for all states (ST , SE, V, CT , CE) in Q.
Lemma 2. Let (ST , SE, V, true, true) be such that H(ST ) = H(SE). Then,
(ST , SE, V, true, true) ⇝∗(S′
T , S′
E, V ′, C′
T , C′
E) implies H(S′
T ) = H(S′
E) if C′
E
and H(S′
E) = ∅, otherwise.

94
A. Falcone and R. Focardi
Proof. Lemma 1. Let q ∈Q, q = (ST , SE, V, CT , CE). By deﬁnition of encoding,
q =

H (ST ) , H (ST ) , H (SE) , V, CT , CE

. To this we apply the inverse encod-
ing; we have to distinguish whether the attacker is connected or disconnected
from the device.
If the attacker is connected:

H (ST ) , H (ST ) , H (SE) , V, CT , CE
−1 =
=
 
H (ST )
−1 ,

H (SE)
−1 , V, CT , CE
!
=

H (ST ) ∪H (ST ) , H (SE) ∪H (ST ) , V, CT , CE

= (ST , SE, V, CT , CE) because H (SE) = H (ST )
= q
If the attacker is disconnected:

H (ST ) , H (ST ) , H (SE) , V, CT , CE
−1 =
=
 
H (ST )
−1 ,

H (SE)
−1 , V, CT , CE
!
=

H (ST ) ∪H (ST ) , H (SE) , V, CT , CE

= (ST , SE, V, CT , CE) because H (SE) = ∅
= q
□
Proof sketch. Theorem 1.
(=⇒) Let qi ⇝qf. We explore case by case, for each kind of derivation applied.
We express qf as a function of qi by the applied transition and we express q′
i as
a function of qi by the deﬁnition of state encoding. Then, given the applicability
conditions of the assumed derivation, we verify that analogous conditions, re-
quired to apply the same transition in the H system, hold in state q′
i. Thus, we
can apply the same transition to state q′
i, which by hypothesis is the encoding
of qi, yielding state q′
f. Finally, we show that q′
f obtained in this way is the
encoding of qf: this is done checking the vector state elements one at a time, to
show that the state encoding deﬁnition is complied with.
(⇐=) This part of the proof is symmetric to the ﬁrst part and it relies on the
inverse state encoding.
www.ebook3000.com

Secure Upgrade of Hardware Security Modules
in Bank Networks⋆
Riccardo Focardi and Flaminia L. Luccio
Universit`a Ca’ Foscari Venezia
{focardi,luccio}@dsi.unive.it
Abstract. We study the secure upgrade of critical components in wide
networked systems, focussing on the case study of PIN processing Hard-
ware Security Modules (HSMs). These tamper-resistant devices, used by
banks to securely transmit and verify the PIN typed at the ATMs, have
been shown to suﬀer from API level attacks that allow an insider to re-
cover user PINs and, consequently, clone cards. Proposed ﬁxes require
to reduce and modify the HSM functionality by, e.g., sticking on a single
format of the transmitted PIN or adding MACs for the integrity of user
data. Upgrading HSMs worldwide is, of course, unaﬀordable. We thus
propose strategies to incrementally upgrade the network so to obtain up-
graded, secure subnets, while preserving the compatibility towards the
legacy system. Our strategies aim at ﬁnding tradeoﬀs between the cost
for special “guardian” HSMs used on the borderline between secure and
insecure nodes, and the size of the team working in the upgrade process,
representing the maximum number of nodes that can be simultaneously
upgraded.
Keywords: Security APIs, PIN processing, Hardware Security Modules,
Upgrade strategies.
1
Introduction
Automated Teller Machines (ATMs) verify the user’s Personal Identiﬁcation
Number (PIN) by sending it to the issuing bank on an international bank net-
work. During their journey, PINs are decrypted and re-encrypted on the tra-
versed network switches by special tamper-resistant devices called Hardware
Security Modules (HSMs). As shown in ﬁgure 1, the keypad itself is an HSM
that performs the ﬁrst PIN encryption with a symmetric key k1 shared with the
neighbour acquiring bank. Before the encrypted PIN is routed to the next net-
work node, it is passed to the HSM in the switch that decrypts and re-encrypts
it with another key k2, shared with the destination node, and so on.
In the last years, several API-level attacks have been discovered on these HSMs
[5,6,9]. The attacker is assumed to be an insider gaining access to the HSM at
some bank switch. Then, by performing subtle sequences of API calls he is able
⋆Work partially supported by Miur’07 Project SOFT: “Security Oriented Formal
Techniques”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 95–110, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

96
R. Focardi and F.L. Luccio
Fig. 1. PIN processing infrastructure
to deduce the value of the PIN. As an example, the so-called dectab attack is
based on manipulating public user data given as input to the PIN veriﬁcation
API at the issuing bank HSM. One of these data is a decimalization table that
maps an intermediate hexadecimal representation of the user PIN into a decimal
number. By modifying the way numbers are decimalized and by observing if this
aﬀects the result of the veriﬁcation, the attacker can deduce which are the actual
PIN digits: if the original dectab maps, e.g., A into 0 and the attacker modiﬁes
it so to map A into 1 causing a failure in the PIN veriﬁcation API, then he
is guaranteed that a 0 digit was occurring in the decimalized user PIN. If it
were not the case, PIN veriﬁcation result would be unaﬀected by the change
in the dectab. The attack goes on using another public parameter, the oﬀset,
to reconstruct the whole PIN code. The interested reader is referred to, e.g.,
[8,9,12,20], for more detail.
The interest in these API-level attacks has recently increased [1,2] and it
becomes more an more plausible that some of them have been really exploited by
malicious insiders to steal thousands of user PINs. This has motivated research
on formal methods for analysing PIN recovery attacks and API-level attacks
in general [20]. In particular, in [8] we have proposed a language-based setting
for analysing PIN processing API via a type-system. We have formally modelled
existing attacks, proposed some ﬁxes and proved them correct via type-checking.
These ﬁxes typically require to reduce and modify the HSM functionality by,
e.g., sticking on a single format of the transmitted PIN or adding MACs for the
integrity of user data. Notice, in fact, that the above mentioned attack is based
on the absence of integrity on public user data such as the dectab and the oﬀset.
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
97
There are crucial diﬃculties when trying to implement these ﬁxes on a real
bank network: ﬁrst of all, upgrading the bank network HSMs worldwide is com-
plex and very expensive; moreover, adding improved APIs in the HSMs is not
enough: we also need to remove old ﬂawed APIs if we want to stop the attacks.
This typically makes upgraded HSMs incompatible with the old ones, for exam-
ple when one switch expects a MAC and the previous non-upgraded one cannot
provide it. These diﬃculties can be circumvented at the price of loosing some
security: in [11] we have proposed a low-impact, easily implementable ﬁx requir-
ing no hardware upgrade which makes attacks 50000 times slower, but yet not
impossible.
Our contribution. We propose a novel way of upgrading critical components
in wide networked system that mitigates the above discussed diﬃculties. In par-
ticular, we propose strategies to incrementally upgrade the network so to obtain
upgraded, secure subnets while preserving the compatibility towards the legacy
system. This should make the upgrading process more appealing to banks, as
they might decide to invest money for upgrading part of the system still main-
taining the interoperability with the non-upgraded part. Of course, PINs trav-
elling through non-upgraded subnets would be exposed to the same attacks as
before, but PINs traversing secured subnets would be robust against API-level
attacks.
To guarantee the compatibility with the old system we propose the adoption
of special borderline HSMs translating from/to the old protocol. These HSMs are
temporarily placed on the borderline between upgraded and non-upgraded nodes
and can be thought as the ‘union’ of the old and the new HSMs, thus supporting
the functionalities of both and being able to translate from/to the old protocol.
Of course, this hardware is far from being secure but it can be used to temporarily
keep the network working while the upgrade is performed. Our strategies aim at
ﬁnding tradeoﬀs between the cost for borderline HSMs and the size of the team
working in the upgrade process, representing the maximum number of nodes
that can be simultaneously upgraded: since HSMs are quite expensive, it might
make sense to have bigger teams of technicians able to upgrade a whole subtree
in one shot, with no need of placing many borderline HSMs around.
We formally state the HSM upgrading problem and we prove that, when only
one technician is present, it is strictly related to the Connected Monotone Decon-
tamination (CMD) problem [3], where a team of agents aims at decontaminating
a graph and capturing an intruder. The analogy is that decontaminated nodes
should never be directly in contact with contaminated ones to avoid the intruder
re-contaminating them, thus some agents must stay on the borderlines to ‘guard’
the decontaminated subnetwork. This is similar to what happens when placing a
borderline HSM between upgraded and non-upgraded nodes. We then generalize
the optimum algorithm for the CMD problem to our setting, also considering
many technicians. We prove the new algorithm correct and we show how it can
be applied to estimate upgrading cost for a real bank (sub)network.
Paper structure. The paper is organized as follows: in section 2 we formalize
the HSM upgrading problem and we prove it equivalent to the CMD problem, up

98
R. Focardi and F.L. Luccio
to one borderline HSM; in section 3, we give an algorithm for upgrading HSMs
which is parametrized by the size of the technician team; in section 4 we give
an example of application of the algorithm to estimate the upgrading cost on a
network; ﬁnally, in section 5 we give some concluding remarks and we discuss
future work.
2
The HSM Upgrading Problem
Given a bank network, we want to study strategies for incrementally upgrading
HSMs while keeping the network functionality up, apart from the lapse of time
in which HSMs are physically substituted. While applying an upgrading tech-
nique, only part of the HSMs will be upgraded. As we have mentioned in the
introduction, in this upgraded hardware we certainly want to remove old ﬂawed
functionalities, thus it will not be able, in general, to support the old protocol
and communicate with non-upgraded HSMs. To this aim, the technicians per-
forming the upgrade can place special borderline HSMs translating from/to the
old protocol.
We brieﬂy discuss two possible settings, depending on whether these special
HSMs are placed on links as in ﬁgure 2, or on switches, as in ﬁgure 3. The former
setting makes all the paths in the upgraded network secure, but in case we have
many links towards the non-upgraded network it would require to install many
borderline HSMs thus becoming more complicated and expensive. The latter
setting is cheaper and more ﬂexible: once an HSM becomes borderline all the
neighbouring switches can be indiﬀerently upgraded or not. Notice, however,
that the paths passing through borderline HSMs in the upgraded network are
still subject to the attacks. This is because borderline nodes still support old,
Upgraded
Network
Old
Network
ATM
Issuing
Bank
subject to
API−level attacks
Fig. 2. Borderline HSM on a link
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
99
Upgraded
Network
Old
Network
ATM
Issuing
Bank
subject to
API−level attacks
Fig. 3. Borderline HSM on a node
ﬂawed functionalities. This setting is cheaper than the former, but the same
degree of security is only achieved later on, when more nodes will be upgraded.
Given that we believe the cost is likely to be the major issue we prefer to analyse
this latter setting and leave the former as a future work.
Another issue to be considered is which network topology we can expect to be
faced with, when starting an upgrade. In the USA, real systems are composed
of regional networks shared by diﬀerent banks and are hierarchically connected
through switches of one of the existing national networks [13]. In our opinion, a
reasonable assumption to make is that the updates start inside regional networks
which will be composed of diﬀerent trees, each representing a diﬀerent bank
network, rooted at the issuing bank. It seems plausible that diﬀerent banks will
upgrade their hardware independently, thus we imagine they will ﬁrst install
a suﬃcient number of borderline switches to ‘separate’ their network from the
other bank networks in the same region. We assume this preliminary separation
as done and we focus on single trees representing single bank networks, rooted
at the issuing bank node and having all the ATMs as leaves.
We can now formally state the HSM upgrading problem as an algorithmic
problem on trees.
The HSM upgrading problem. We consider an initially non-upgraded tree
network that has to be upgraded by a set of technicians arbitrarily moving inside
the network and possibly placing borderline HSMs. All technicians and borderline
HSMs are placed in one initial node. A technician can move only in a connected way,
i.e., from a node to another along an edge, in doing so she upgrades the traversed
nodes. Borderline HSMs can only be moved by one technician; in that case the HSM
and the technician move together along edges. The technician moves such special
hardware when all the neighbouring nodes, except the destination one, have been

100
R. Focardi and F.L. Luccio
upgraded. In this case, in fact, the borderline HSM is not any more needed. We
implicitly assume that this hardware, in the presence of one technician and with
all neighbouring nodes upgraded is switched oﬀ, making the node upgraded. At the
end of the computation all the nodes are upgraded.
An HSM upgrading strategy is a sequence of moves that will upgrade an ini-
tially non-upgraded network. While devising eﬃcient updating techniques we
consider diﬀerent parameters that we would like to optimize:
1. The number B of available borderline HSMs: each of them has a cost which
may be very high, typically around 10000$, thus B has to be minimized;
2. The number U of technicians in the upgrading team: each technician can
upgrade one HSM in one node, so having many technicians allows for simul-
taneous upgrading of subsets of nodes; the salary we pay to the technicians
is proportional to this parameter, thus even U should me minimized;
Deﬁnition 1 (HSM upgrading number). It is the minimal number of bor-
derline HSMs needed to solve the HSM upgrading problem on a given tree T and
with a given number U of technicians. We note it uhn(T, U).
2.1
Upgrading vs. Decontaminating
We now show that the HSM upgrading problem with a single technician is very
strictly related to the Connected Monotone Decontamination (CMD) problem,
formalized below.
The CMD problem [3]. We consider an initially contaminated network that
has to be decontaminated by a set of agents (or searchers) arbitrarily moving
inside the network. All nodes and edges are initially contaminated, except for
the initial node where all the agents are located, which is guarded. An agent
can move only in a connected way, i.e., from a node to another along an edge.
The network contains an intruder that contaminates the nodes and edges it
traverses. The intruder cannot traverse guarded nodes, otherwise it would be
captured by the agents, but we assume that he will immediately recontaminate
nodes or edges that are left unguarded. In the node decontamination problem
a node is guarded when it contains at least one agent, clean when an agent
has been on the node and all the neighbouring nodes are clean or guarded, and
contaminated otherwise. At the end of the computation all the nodes of the
network are simultaneously clean. In the edge decontamination problem an edge
becomes clean whenever an agent passes through it, and its starting node is
either clean or guarded. At the end of the computation all the nodes and edges
of the network are simultaneously clean.
A decontamination (or search) strategy is a sequence of moves that will clear
an initially contaminated network. A strategy is monotone, if a decontaminated
node or edge is never recontaminated. Finally, the strategy has to be eﬃcient,
where eﬃciency is measured in terms of the size of the agent team. A strategy
is optimal if the size of the team is minimal.
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
101
Deﬁnition 2 (Connected search number). It is the minimal team size needed
to solve the edge CMD problem on a given network G, and is noted csn(G).
The problem of ﬁnding an optimal strategy has been studied in some speciﬁc
topologies such as trees [3]. In the same work it is also proved that, for trees,
a non-monotone solution, i.e., allowing re-contamination of nodes, would not
reduce the optimal number of required agents.
Originally, the decontamination problem has been introduced in [7,17] and
has been extensively studied in the literature under the term graph search (e.g.,
see [10,14,15,16,18]). The diﬀerence with the CMD problem is that searchers do
not necessarily move in a connected way, i.e., may ‘jump’ from one node to an-
other. In the HSM upgrading problem technicians do not travel via the network
they repair, but via lorries or cars. In an ideal setting we would thus have to
think our scenario as a composition of two networks: the bank network over-
lapped with a geographical network where technicians physically move. Another
approach could consist of considering only the bank network where the travelling
cost is added by assigning weights to the edges (where weights represent physical
distances), and by letting searchers jump from one node to the other. It is inter-
esting to note that for any non-weighted tree T with n nodes the ratio between
the connected search number csn(T ) and the regular search number sn(T ) (i.e.,
for agents that may jump) is bounded by, csn(T )/sn(T ) ≤2 (see [4]), i.e., any
optimal connected strategy will require at most twice the number of agents than
a non-connected one (at least in the non-weighted case). We thus leave the two
above mentioned extensions as a future work and for simplicity, in this work we
concentrate on the scenario in which we do not consider the technician motion
as a parameter to optimize.
Equivalence of HSM upgrading and CMD. We now prove that the HSM
upgrading problem solved using the minimal number of borderline HSMs and a
single technician is equivalent, up to one extra agent, to the problem of ﬁnding a
connected monotone technique for the edge decontamination of the tree network
using the smallest number of agents.
Theorem 1. Given a tree T , we have uhn(T, 1) ≤csn(T ) ≤uhn(T, 1) + 1.
Proof. Let us ﬁrst assume we have solved the HSM upgrading problem in a tree
using the minimal number uhn(T, 1) of borderline HSM and a single technician.
We show that the adopted strategy can be mapped into a valid strategy for
the CMD problem with a number of agents equal to uhn(T, 1) + 1. We can see
upgraded and non-upgraded HSMs respectively as decontaminated and contam-
inated nodes. Moreover, we see each borderline HSM and the (single) technician
as agents. We now show that moves of the technician and of borderline HSMs
(that are anyway transported by the technician) are correctly mapped into agent
moves.
We ﬁrst consider the technician moving alone, bringing no HSM. The only
crucial case is when the node where she moves from has no borderline HSM on
it meaning, in the other problem, that she is the only agent on the node. In this

102
R. Focardi and F.L. Luccio
case we are guaranteed that all the neighbouring nodes are either upgraded or
borderline HSMs, as an upgraded node (the one where the technician is moving
from) can never be directly connected to a non-upgraded one. The corresponding
move in the CMD problem can be safely performed.
We now consider the technician moving with some borderline HSMs. If at
least one HSM is left on the node the move can be safely simulated in the CMD
problem, as at least one agent will stay on the originating node, guarding it.
If all the borderline HSMs are moved by the technician we are in a situation
similar to the one where the technician alone is moving: all the neighbouring
nodes need to be either upgraded or borderline HSMs, except the destination
node which, if needed, will be protected by one of the incoming borderline HSMs.
This is perfectly safe in the CMD problem, as the moving agents will guard the
destination node. The important thing is that the node left alone is protected
by the upgraded/guarded neighbouring nodes.
When all the nodes are upgraded in the HSM upgrading problem they will
correspondingly be decontaminated in the CMD problem, thus the strategy in
the former problem is also a strategy in the latter, with a number of agents equal
to uhn(T, 1) + 1. This implies csn(T ) ≤uhn(T, 1) + 1.
Similarly, let us now assume we have solved the CMD problem with csn(T )
agents. We show that the adopted strategy can be mapped into a valid strat-
egy for the HSM upgrading problem with one technician and with a number
of borderline HSMs equal to csn(T ). We can see decontaminated and contam-
inated nodes as upgraded and non-upgraded HSMs, respectively. Moreover, we
see each agent as a borderline HSM. We now show that moves of the agents
are correctly mapped into borderline HSM moves, via the technician. Notice
that no agent is mapped into the technician, so the position of the technician
on the tree is immaterial. Notice also that, at any point of the computation,
all upgraded/borderline HSMs are on a connected subtree of the network, given
that agents start from a single initial node, they move in a connected way and
HSMs can never be downgraded. Thus, we can freely move the technician on
this upgraded/borderline nodes reaching all the borderline HSMs.
Now, any agent move is simulated by the technician reaching the correspond-
ing borderline HSM and moving it in the destination node. The only crucial
case is when the node which is left has no other agents guarding it. In this
case, however, we are guaranteed that all of the neighbouring nodes, except the
destination one, are upgraded or guarded, since an agent cannot allow a decon-
taminated node to be recontaminated. Thus, the corresponding borderline HSM
can be safely removed from the node.
When all the nodes are decontaminated in the CMD problem they will cor-
respondingly be upgraded or borderline in the HSM upgrading problem. At this
point the technician can go around an collect all the borderline HSMs ‘deactivat-
ing’ them. Thus the strategy in the former problem is also a strategy in the latter,
with a number of borderline HSMs equal to csn(T ). This implies uhn(T, 1) ≤
csn(T ).
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
103
3
Upgrading Strategies
In this section we show diﬀerent techniques to solve the HSM upgrading prob-
lem. We ﬁrst provide strategies which aim at minimizing B, i.e., the number of
borderline switches used, and then present trade-oﬀs between B and U, i.e.,
the number of nodes U that can be simultaneously upgraded by a team of
technicians.
Case U = 1. Nodes are sequentially updated by a unique technician. As we
have proved in the previous section, in this case this problem is equivalent to
the CMD problem where agents are seen as borderline HSMs. We thus slightly
modify the algorithm presented in [3] in order to solve our problem.
The main idea is to ﬁrst compute, from every possible starting point, i.e.,
every node of the tree T , the minimal number of borderline switches required
for the solution of the upgrading problem. Then, one of the nodes with minimal
value is chosen as starting point and the actual upgrading algorithm is applied.
In order to compute this minimal value two diﬀerent rules (similar to the ones
of [3]) need to be applied. For each edge {x, y}, we ﬁrst compute λx(x, y), i.e.,
the minimal number of borderline HSMs necessary for the upgrade of the tree
rooted at y while arriving from x.
Rule 1 for computing minimal number of borderline HSMs on
an edge
1. An edge e = {x, y} leading from a node x to a leaf y requires only
the technician moving from x to y to upgrade the HSM on y, thus
λx(x, y) = 0;
2. An edge e = {x, y} leading from a node x to a node y that has
other k out-going edges requiring l1, . . . , lk borderline HSMs, with
l1 ≥l2 ≥, . . . , ≥lk, requires λx(x, y) = max{l1, l2 + 1, 1} borderline
HSMs.
From this value we can then compute the minimal number of borderline HSMs
which are necessary for the upgrade of the tree starting from any node. The two
rules are:
Rule 2 for computing minimal number of borderline HSMs on
a node
1. A leaf y requires a number of borderline HSMs deﬁned by λy(y, x)
to move to node x; in fact, being a leaf we do not need to leave any
borderline HSM on y;
2. A node x that has h out-going edges each respectively requir-
ing l1, . . . , lh borderline HSMs, with l1 ≥l2 ≥, . . . , ≥lh, needs
max{l1, l2 + 1} borderline HSMs.
We now apply Rules 1 and 2 and then choose one of the nodes, say z, requiring
a minimal number B of borderline HSMs. Similarly to the algorithm of [3],

104
R. Focardi and F.L. Luccio
consider Tz, the tree rooted at z and, for each node y of Tz, order its children by
increasing values assigned by λz to the related links. Apply then the following:
Algorithm Upgrade
1. Start at z with B borderline HSMs and a technician;
2. Traverse Tz in pre-order (by increasing values of λz). While moving
from a node x to a node y use λx(x, y) borderline HSMs and a
technician, while returning to x bring back λx(x, y) borderline HSMs
and a technician.
Example 1. Let us now show how to apply Rules 1, 2 and Algorithm Upgrade
on a small example. Assume we have a tree T of 6 nodes called a, b, c, d, e, f (see
ﬁgure 4). We ﬁrst compute the values associated to the edges, i.e., we apply Rule
1. E.g., consider node d. As d is a leaf by Rule 1.1 we have λb(b, d) = 0, that
is there is just a technician moving from b to d. The same holds for the other
leaves e and f, thus λc(c, e) = 0, and λc(c, f) = 0. Consider now node a and
edge {a, b} and apply Rule 1.2. Node b has only one other out-going edge, i.e.,
{b, d} that requires l1 = λb(b, d) = 0 borderline HSMs. Thus, edge {a, b} requires
λa(a, b) = max{l1 = 0, 1} = 1 borderline HSM. That is, path {a, b, d} is a chain
thus we need at least one borderline HSM to upgrade it. On the other hand, if
we consider edge {a, c} and apply Rule 1.2 we have that node c has two other
out-going edges, i.e., {c, e} and {c, f} that respectively require l1 = λc(c, e) = 0
and l2 = λc(c, f) = 0 borderline HSMs. Thus, edge {a, c} requires λa(a, c) =
max{l1 = 0, l2 = 0 + 1 = 1, 1} = 1 borderline HSM. To compute the minimal
number of borderline HSMs on a node we apply Rule 2. E.g., consider a leaf d. It
requires a number of borderline HSMs deﬁned by λd(d, b) = 1 to move to node
b, thus we write 1 inside node d. Consider now node c that has 3 out-going edges
each respectively requiring l1 = 1, l2 = 0, l3 = 0 borderline HSMs, thus c needs
max{l1 = 1, l2 + 1 = 0 + 1 = 1} = 1 borderline HSM.
After having applied Rules 1 and 2 to all the nodes, we choose one of them
requiring a minimal number of borderline HSMs, e.g., c requiring 1 HSM. We
consider Tc, the tree rooted at c and order its children by increasing values
assigned by λc to the related links, e.g., we consider node e, then f, then a, and
ﬁnally apply Algorithm Upgrade starting from c with 1 borderline HSMs and
a technician.
Figure 5 shows an application of Algorithm Upgrade starting from node c.
Black nodes represent updated HSMs. Nodes surrounded by squares represent
borderline HSMs, the lady represents the technician. At the end the technician
goes back to c.
We now prove the correctness and the optimality of algorithm Upgrade.
Theorem 2. Algorithm Upgrade together with Rule 1 and Rule 2 correctly
and optimally solve the HSM upgrading problem in a bank tree network T .
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
105
1
1
1
b
d
1
2
1
a
1
1
1
c
0
f
1
1
e
0
1
0
1
Fig. 4. Application of Rule 1 and 2
1
b
d
1
2
a
c
f
1
1
e
1
1
b
d
1
2
a
c
f
1
1
e
1
1
b
d
1
2
a
c
f
1
1
e
1
1
b
d
1
2
a
c
f
1
1
e
1
1
b
d
1
2
a
c
f
1
1
e
1
1
b
d
1
2
a
c
f
1
1
e
1
Fig. 5. Algorithm Upgrade starting from c
Proof. The general correctness directly derives from the one of the rules and the
algorithm presented in [3]. In particular, we ﬁrst have to prove that an optimal
monotone strategy for HSM upgrading with a single starting point on any tree
T exists. Then, we choose the entry point that minimizes the number of required
HSMs. Another key point for the correctness of the strategy is the property that
the minimal number of required HSMs from a node x is given by the minimal
number between the biggest minimal value of a son of x and the second biggest
minimal value plus one. These values will correspond to the edge labels and will
imply a speciﬁc ordering in the strategy visit. Finally, we prove that using the

106
R. Focardi and F.L. Luccio
minimal number of HSMs, we can upgrade the system as moving back and forth
with such number of HSMs does not leave parts of the network unprotected.
The existence of an optimal monotone strategy for CMD on any tree T has
been proved in [3] using a variation of the proof of [15] for the non-monotone
setting with the addition of the single starting point (i.e., the initially guarded
node). The main idea that we borrow is that it is possible to build a progressive
connected crusade (i.e., a sequence of moves where a new single un-traversed
edge is added at each step, leading to the visit of the whole network and using
only connected sub-networks) using at most uhn(T, 1) HSMs on the borderline
between the non-upgraded and upgraded sub-networks.
The correctness of the optimal strategy for the HSM upgrading problem also
derives from the one of [3]. In particular, from what is stated above we can
limit the upgrading to strategies having a single entry point, thus one of the
starting point requiring the smallest number of HSMs will be chosen. Another
key point is the fact that, given a tree T rooted at r (call it Tr) and con-
sidering a sub-tree of Tr rooted at x (Tr[x]) with k children x1, . . . , xk such
that uhn(Tr[xi], 1) ≥uhn(Tr[xi+1], 1) for all i = 1, . . . , k, then uhn(Tr[x], 1) =
max{uhn(Tr[x1], 1), uhn(Tr[x2], 1)+1}. Obviously uhn(Tr[x], 1) ≥uhn(Tr[x1], 1)
otherwise Tr[x1] cannot be upgraded. We now have two possible cases:
1. uhn(Tr[x1], 1) ≥uhn(Tr[x2], 1)+1. Then Tr[x] is upgraded by visiting Tr[x1]
as the last subtree, after having placed an HSM on x and cleaned all the
other children;
2. uhn(Tr[x1], 1) = uhn(Tr[x2], 1). Assume S is a strategy that upgrades Tr[x]
using uhn(Tr[x2], 1) HSMs. If Tr[x2] is upgraded before Tr[x1] no HSM may
be left on x, thus an untraversed edge (towards x1) remains unprotected. The
same holds if Tr[x2] is upgraded after Tr[x1]. Thus more than uhn(Tr[x2], 1)
HSMs have to be used. In fact, uhn(Tr[x2], 1)+1 HSMs are suﬃcient by ﬁrst
visiting Tr[x2] and then Tr[x1] with one HSM on x.
Label now each edge {a, b} of the network with a function λa(a, b) that is 0 if b is
a leaf and max{l1, l2+1, 1} if b has h out-going edges each respectively requiring
l1, . . . , lh HSMs, with l1 ≥l2 ≥, . . . , ≥lh. The next step borrowed from [3] is to
prove that λa(a, b) = uhn(Ta[b], 1) and this is done by induction on the height
of Ta[b].
All the proofs of [3] can thus be directly mapped to our problem. There are
some diﬀerences on the rules which we report below. Rule 1.1 for the HSM
upgrading problem avoids the placement of a borderline switch (i.e., the agent
in the other problem) on y, i.e., at the end of the chain, but, by the recursive
construction, a borderline switch will however be placed on x while upgrading y,
with the only exception being a tree composed of a single edge {x, y}, that we do
not treat (it could be treated via an ad-hoc rule but we prefer not to complicate
the algorithm for a trivial case). Now, if x has degree at least 2, by Rule 2 node
x will require at least one borderline HSM (used on x) and one technician (the
two edges out-going from x have associated 0). Thus, y will be safely upgraded.
Moreover, Rule 1.2 states that, if x is the starting point of a chain of HSMs we
need at least one borderline HSM to upgrade the system. This is included in the
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
107
constant 1 of the relation λx(x, y) = max{l1, l2 + 1, 1}. This is peculiar of our
algorithm and is not present in [3].
Consider now Algorithm Upgrade. In point 1 we start from the node that
requires the minimal number B = uhn(T, 1) of borderline HSMs. B has been
correctly computed by Rule 1 and 2. Consider now point 2 of Algorithm Up-
grade. The proof is similar to the one of [3] with the inclusion of the case
where a node is a leaf that has to be directly updated by the technician (that is
in this case λx(x, y) = 0 no borderline HSMs, only the technician). Very brieﬂy,
the main idea is that technician and HSM move may be forward and backward.
Backward moves are safe as they leave an upgraded network and move to an
already upgraded one. Forward moves upgrade a non-upgraded subtree by in-
creasing number of HSMs. Moreover the relation λx(x, y) = max{l1, l2 + 1, 1}
assures that at least one HSM will be left on node x, avoiding new attacks and
enough HSMs can be moved to y.
Finally, observe that Rule 1 and 2 compute the minimal number of borderline
switches required on the tree and starting from every possible node, and B is the
minimal of such values.
Case U > 1. We now assume that our network is a tree T , and that U = k > 1,
i.e., that nodes can be simultaneously upgraded by a team of k technicians.
Using this issue we now try to compute what is the related decrease in B. To
do this we extend Rule 1 and Algorithm Upgrade 1 basing our changes on the
following observation: given U = k, and the tree T , if we have a subtree Tz of T
rooted at z and containing <= k nodes, we can then use the team to upgrade Tz
in one shot, one technician sent to each node. That is, the team can collaborate
to upgrade a contiguous part of T , decreasing the number B of borderline nodes
required by Algorithm Upgrade 1.
In order to do this we have to compute, for each node z of T , the number
of nodes in each subtree of T rooted in z (i.e., in Tz). More precisely, assuming
z has h neighbouring nodes z1, . . . , zh, we have to compute sz,zi the size of the
subtrees Tz[zi] of Tz rooted in zi, for i = 1, . . . , h. We can do this recursively as
follows:
Computing the number of nodes of subtrees
1. An edge e = {x, y} leading from a node x to a leaf y. We trivially
have sx,y = 1;
2. Consider an edge e = {x, y} leading from a node x to a node y that
has other k out-going edges {y, z1}, . . . , {y, zk} with subtree sizes
sy,zi, with i = 1, . . . , k. Then the size of the subtree of Tx rooted at
y is sx,y = sy,z1 + . . . + sy,zk + 1.
Based on this computation we can give a new rules, Rule 1 new which substi-
tutes Rule 1:

108
R. Focardi and F.L. Luccio
Rule 1 new for computing minimal number of borderline HSMs
on an edge using U technicians
1. An edge e = {x, y} leading from x to y such that sx,y ≤U requires
only the technician team moving from x to y to upgrade the HSM
on the whole subtree rooted in y, thus λx(x, y) = 0;
2. An edge e = {x, y} leading from x to y such that sx,y > U and having
other k out-going edges requiring l1, . . . , lk borderline HSMs, with
l1 ≥l2 ≥, . . . , ≥lk, requires λx(x, y) = max{l1, l2 + 1, 1} borderline
HSMs.
Theorem 3. Algorithm
Upgrade together with Rule 1 new and Rule 2 cor-
rectly solve the HSM upgrading problem in a bank tree network T where U tech-
nicians operate. The number of borderline switches required is value B computed
via Rule 1 new and Rule 2.
Proof. Rule 2 and the Upgrade algorithm are the one previously presented,
their correctness thus follows. The correctness of Rule 1 new derives from the
one of Rule 1. Moreover, the computation of the sub-tree sizes is based on the
standard saturation technique widely used in the ﬁeld of distributed algorithms
(see, e.g., [19]). This technique assumes that the computation of the size of a
sub-tree is started at the leaves (which in this case count one) and is propagated
on the sub-tree by collecting values from all but one edge and propagating it to
through the remaining edge. Finally, given U technicians that may work on a
non-upgraded network of ≤U nodes, all these technicians may obviously upgrade
this sub-tree in parallel.
4
Estimating the Upgrading Cost: An Example
We consider a simple example to show how to compute a trade-oﬀbetween
the number U of technicians and the the number of needed borderline HSMs.
The example is depicted in ﬁgure 6. We use the same notation adopted in the
previous section to label edges and nodes. On the left, we execute the algorithm
with U = 1: it gives 2 on all nodes, thus the minimal number of needed borderline
HSMs is 2. We now want to evaluate the beneﬁts of hiring one more technician.
On the right we have executed the algorithm with U = 2, pointing out the
diﬀerence in red, and we see that in 2 nodes just one HSM is needed. Thus, if we
start from those nodes with two technicians we only need one borderline HSM
to upgrade the whole network. Notice that we cannot do best than this as one
HSM is for sure needed.
We can now reason as follows: let CH be the cost for one HSM and CU the
cost for one technician. In the ﬁrst case the overall upgrade cost is estimated as
2CH +CU while in the second case we estimate as CH +2CU. It is now clear that
depending on how CH and CU are related we will go one direction or the other.
For example, if a technician costs around 5000$ and an HSM around 10000$, we
will opt for the second solution spending 20000$ instead of 25000$.
www.ebook3000.com

Secure Upgrade of Hardware Security Modules in Bank Networks
109
2
1
0
0
0
0
1
2
2
0
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
0
0
0
0
1
2
2
0
2
2
2
2
2
2
0
1
1
1
1
1
1
1
1
U=1
U=2
Fig. 6. An example of cost estimation
5
Conclusion
We have proposed a novel way of upgrading critical components in wide net-
worked system which are incremental and aim at ﬁnding a trade-oﬀbetween
the extra hardware needed to maintain the functionality of the network, and
the size of the technician teams simultaneously operating on the bank network
switches. The presented techniques are not speciﬁc for HSMs and PIN manag-
ing but could be reused in any networked system requiring the upgrade of some
security-critical component.
There are some aspects that we have not treated in this paper and might be
interesting to investigate. We have mentioned in section 2 that borderline HSMs
might be placed on edges instead of nodes. This increases the number of secured
paths but, intuitively, requires more hardware to isolate secure subnetworks from
insecure ones. To understand the beneﬁts of this approach it would be useful
to measure the degree of security of the network, i.e., the size of the secure
subgraph from the ATMs to the related issuing bank or, in other words, the
number of secured, upgraded paths in a network. Having this measure, we could
compare the present strategies with ones placing borderline HSMs on edges, also
comparing the degree of security provided by the new approach. The trade-oﬀ,
thus, might be between the cost and the degree of security.
Finally, another interesting issue, we have previously mentioned, is the opti-
mization of a new parameter, i.e., the technician motion either inside a weighted
network or on a new independent physical network overlapped to the bank net-
work. In this case, time complexity would probably become an issue as in the
traveling salesman problem. Depending on the size of the analyzed trees, the
solution might thus require heuristic approaches.

110
R. Focardi and F.L. Luccio
References
1. Hackers crack cash machine PIN codes to steal millions. The Times online,
http://www.timesonline.co.uk/tol/money/consumer affairs/
article4259009.ece
2. PIN Crackers Nab Holy Grail of Bank Card Security. Wired Magazine Blog Threat
Level, http://blog.wired.com/27bstroke6/2009/04/pins.html
3. Barri`ere, L., Flocchini, P., Fraigniaud, P., Santoro, N.: Capture of an intruder by
mobile agents. In: Proceedings of the 14th ACM Symposium on Parallel Algorithms
and Architectures (SPAA),Winnipeg, Manitoba, Canada, pp. 200–209 (2002)
4. Barri`ere, L., Fraigniaud, P., Santoro, N., Thilikos, D.M.: Searching is not jump-
ing. In: Bodlaender, H.L. (ed.) WG 2003. LNCS, vol. 2880, pp. 34–45. Springer,
Heidelberg (2003)
5. Berkman, O., Ostrovsky, O.M.: The unbearable lightness of PIN cracking. In: Diet-
rich, S., Dhamija, R. (eds.) FC 2007 and USEC 2007. LNCS, vol. 4886, pp. 224–238.
Springer, Heidelberg (2007)
6. Bond, M., Zielinski, P.: Decimalization table attacks for pin cracking. Technical Re-
port UCAM-CL-TR-560, University of Cambridge, Computer Laboratory (2003),
http://www.cl.cam.ac.uk/TechReports/UCAM-CL-TR-560.pdf
7. Breish, R.: An intuitive approach to speleotopology. Southwestern cavers VI(5),
72–82 (1967)
8. Centenaro, M., Focardi, R., Luccio, F., Steel, G.: Type-based analysis of PIN pro-
cessing APIs. In: Backes, M., Ning, P. (eds.) ESORICS 2009. LNCS, vol. 5789,
pp. 53–68. Springer, Heidelberg (2009)
9. Clulow, J.: The design and analysis of cryptographic APIs for security devices.
Master’s thesis, University of Natal, Durban (2003)
10. Ellis, J.A., Sudborough, I.H., Turner, J.S.: The vertex separation and search num-
ber of a graph. Information and Computation 113, 50–79 (1994)
11. Focardi, R., Luccio, F., Steel, G.: Blunting diﬀerential attacks on PIN process-
ing APIs. In: Knapskog knapskog@q2s.ntnu.no, S.J. (ed.) NordSec 2009. LNCS,
vol. 5838, pp. 88–103. Springer, Heidelberg (2009)
12. Focardi, R., Luccio, F.L.: Cracking bank PINs by playing Mastermind. In: Pro-
ceedings of the Fifth International Conference on Fun with algorithms (FUN 2010).
LNCS, Springer, Heidelberg (2010)
13. Hayashi, F., Sullivan, R., Weiner, S.E.: A Guide to the ATM and Debit Card
Industry. Federal Reserve Bank of Kansas City (2003)
14. Kirousis, L.M., Papadimitriou, C.H.: Searching and pebbling. Theoretical Com-
puter Science 47, 205–218 (1986)
15. Lapaugh, A.: Recontamination does not help to search a graph. Journal of the
ACM 40(2), 224–245 (1993)
16. Megiddo, N., Hakimi, S., Garey, M., Johnson, D., Papadimitriou, C.: The com-
plexity of searching a graph. Journal of the ACM 35(1), 18–44 (1988)
17. Parson, T.: Pursuit-evasion problem on a graph. Theory and applications of graphs,
426–441 (1976)
18. Peng, S., Ko, M., Ho, C., Hsu, T., Tang, C.: Graph searching on chordal graphs.
Algorithmica 27, 395–426 (2002)
19. Santoro, N.: Design and Analysis of Distributed Algorithms. John Wiley & Sons,
Chichester (2006)
20. Steel, G.: Formal Analysis of PIN Block Attacks. Theoretical Computer Sci-
ence 367(1-2), 257–270 (2006)
www.ebook3000.com

Interactive Information Flow
(Invited Talk)
Catuscia Palamidessi1, M´ario S. Alvim1, and Miguel E. Andr´es2
1 INRIA and LIX, ´Ecole Polytechnique Palaiseau, France
2 Institute for Computing and Information Sciences, The Netherlands
Abstract. In recent years, there has been a growing interest in consid-
ering the quantitative aspects of Information Flow, partly because often
the a priori knowledge of the secret information can be represented by a
probability distribution, and partly because the mechanisms to protect
the information may use randomization to obfuscate the relation between
the secrets and the observables.
We consider the problem of deﬁning a measure of information leakage
in interactive systems. We show that the information-theoretic approach
which interprets such systems as (simple) noisy channels is not valid
anymore when the secrets and the observables can alternate during the
computation, and inﬂuence each other. However, the principle can be
retrieved if we consider more complicated types of channels, that in In-
formation Theory are known as channels with memory and feedback. We
show that there is a complete correspondence between interactive sys-
tems and such kind of channels. Furthermore, the proposed framework
has good topological properties which allow to reason compositionally
about the worst-case leakage in these systems.
References
1. Andr´es, M.E., Palamidessi, C., van Rossum, P., Smith, G.: Computing the leakage
of information-hiding systems. In: Proc. of TACAS (2010) (to appear)
2. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: Anonymity protocols as noisy
channels. Inf. and Comp. 206(2-4), 378–401 (2008)
3. Clark, D., Hunt, S., Malacaria, P.: Quantiﬁed interference for a while language.
In: Proc. of QAPL 2004. ENTCS, vol. 112, pp. 149–166. Elsevier, Amsterdam (2005)
4. Desharnais, J., Jagadeesan, R., Gupta, V., Panangaden, P.: The metric analogue of
weak bisimulation for probabilistic processes. In: Proc. of LICS, pp. 413–422 (2002)
5. Malacaria, P.: Assessing security threats of looping constructs. In: Proc. of POPL,
pp. 225–235. ACM, New York (2007)
6. James, M.: Causality, feedback and directed information. In: Proc. of the Interna-
tional Symposium on Information Theory and its Applications, Honolulu (1990)
7. Smith, G.: On the foundations of quantitative information ﬂow. In: de Alfaro, L.
(ed.) FOSSACS 2009. LNCS, vol. 5504, pp. 288–302. Springer, Heidelberg (2009)
8. Tatikonda, S., Mitter, S.K.: The capacity of channels with feedback. IEEE Trans-
actions on Information Theory 55(1), 323–349 (2009)
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, p. 111, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Portunes: Representing Attack
Scenarios Spanning through the Physical,
Digital and Social Domain
Trajce Dimkov, Wolter Pieters, and Pieter Hartel
Distributed and Embedded Security Group
University of Twente, The Netherlands
{trajce.dimkov,wolter.pieters,pieter.hartel}@utwente.nl
Abstract. The security goals of an organization are realized through
security policies, which concern physical security, digital security and se-
curity awareness. An insider is aware of these security policies, and might
be able to thwart the security goals by combining physical, digital and
social means. A systematic analysis of such attacks requires the whole
environment where the insider operates to be formally represented. This
paper presents Portunes, a framework which integrates all three secu-
rity domains in a single environment. Portunes consists of a high-level
abstraction model focusing on the relations between the three security
domains and a lower abstraction level language able to represent the
model and describe attacks which span the three security domains.
Using the Portunes framework, we are able to represent a whole new
family of attacks where the insider is not assumed to use purely digital
actions to achieve a malicious goal.
Keywords: insider threat, physical security, security awareness, security
model.
1
Introduction
Malicious insiders are a serious threat to organizations. Motivated by greed or
malice, insiders can disrupt services, modify or steal data, or cause physical dam-
age to the organization. Protecting assets from an insider is challenging [1] since
insiders have knowledge of the security policies in place, have certain privileges
on the systems and are trusted by colleagues. An insider may use the knowledge
of the security policies to avoid detection and use personal credentials or social
engineer colleagues to carry out an attack. Thus, the environment in the orga-
nization where the insider operates spans all three security domains, physical
security, digital security and security awareness of the employees. If the environ-
ment is represented formally, it is possible to analyze potential insider attacks
systematically.
The three security domains presented in the environment focus on diﬀerent
elements of security. Physical security restricts access to buildings, rooms and
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 112–129, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010
www.ebook3000.com

Portunes: Representing Attack Scenarios
113
objects. Digital security is concerned with access control on information sys-
tems. Finally, security awareness of employees focuses on resistance to social
engineering, and is achieved through education of the employees.
The majority of formal models for the insider threat assume the insider uses
only digital means to achieve an attack. Therefore an essential part of the envi-
ronment of interest is not captured. Indeed, a study performed by the National
Threat Assessment Center in the US (NTAC) [2] shows that 87% of the attacks
performed by insiders required no technical knowledge and 26% used physical
means or the account of another employee as part of the attack. Thus, a whole
family of attacks, digitally-enabled physical attacks and physically-enabled dig-
ital attacks [3], in which the insider uses physical, digital and social means
to compromise the asset cannot be presented nor analyzed. An example of a
physically-enabled digital attack is the road apple attack [4], where an insider
tricks an employee into plugging a malicious dongle into a server located in a
physically restricted area. The road apple attack will be used as the main exam-
ple in the paper.
Representing all three security domains in a single formalism is challenging.
Firstly, the appropriate abstraction level needs to be found. A too low level of
abstraction for each domain (down to the individual atoms, bits or conversa-
tion dynamics) makes the representation complicated and unusable. However,
abstracting away from physical spaces, data and relations between people might
omit details that contribute to an attack. Secondly, the domains have diﬀerent
properties making them hard to integrate. For example, mobility of digital data is
not restricted by its locality as it is the case with objects in the physical domain.
Likewise, physical objects cannot be reproduced as easily as digital data.
The contribution of this paper is the Portunes framework1, a framework which
integrates all three security domains in a single environment. Portunes consists
of a model and a language. The model is a high-level abstraction of the environ-
ment focusing on the relations between the three security domains. It provides
a conceptual overview of the environment easy to understand by the user. The
language is at a relatively low level of abstraction, close to the enforcement mech-
anisms. The language is able to describe attacks which span the three security
domains.
The rest of the paper is structured as follows. Section 2 gives an overview of
related work which contributed to the design of Portunes. Section 3 formalizes
the Portunes model and Portunes language. We use the road apple attack as
an example of the scenarios Portunes is designed to represent. The ﬁnal section
concludes and identiﬁes future work.
2
Related Work
The design of the Portunes model and Portunes language is inﬂuenced by sev-
eral research directions, such as insider threat modeling, physical modeling and
1 After Portunes, the Roman god of keys.

114
T. Dimkov, W. Pieters, and P. Hartel
process calculi. This section lists several papers which inﬂuenced the design of
Portunes and describes how Portunes extends or deviates from them.
Dragovic et al. [5] are concerned with modeling the physical and digital do-
main to determine data exposure. Their model deﬁnes a containment relation
between layers of protection. Data security is determined not by access control
policies, but by the number of layers of protection above the data and the con-
ﬁdentiality provided by each layer. The Portunes model uses a similar relation
to present the location of elements, but uses access control policies to describe
security mechanisms. Scott [6] focuses on mobility of software-agents in a spatial
area and usage policies that deﬁne the behavior of the agents depending on the
locality of the hosting device. The mobility of the agents is restricted through
edges on a graph. The Portunes model adds semantics on the graph structure by
giving meaning to the nodes and edges and deﬁnes invariants enforced directly
into the semantics of the language.
KLAIM [7] is a process calculus for agent interaction and mobility, consisting of
three layers: nodes, processes and actions. There are several KLAIM dialects, in-
cluding μKlaim [8], OpenKlaim [9] and acKlaim [10]. The goal of the acKlaim lan-
guage is to present insider threats by combining the physical and digital security
domain. Mobility is presented by remote evaluation of processes. The Portunes
language builds upon these KLAIM dialects. Firstly, the actions for mobility and
embedding of objects (login, logout) are similar to OpenKlaim. Secondly, the se-
curity policies expressed in Portunes language are similar to acKlaim and μKlaim.
However, in the Portunes language mobility is represented by moving nodes rather
than evaluating processes. Additionally, the Portunes language introduces delega-
tion, whereby a node can delegate a task to another node.
3
Portunes
This section presents the Portunes framework. We ﬁrst present the requirements
which Portunes needs to satisfy and the motivation behind some of the design
decisions. Based on the requirements, we formally deﬁne the Portunes model and
the Portunes language. To show the expressiveness of the framework, we use an
instance of the road apple attack as an example.
3.1
Requirements and Motivation
A model integrating multiple security domains needs to be expressive enough to
present the details of an attack in each security domain. In a previous work [11],
we provided the basic requirements for an integrated security model to be ex-
pressive enough to present detailed attacks. Brieﬂy, an integrated security model
should be able to present the data of interest, the physical objects in which the
data resides, the people that manipulate the objects and the interaction between
data, physical objects and people.
An additional requirement for Portunes is to restrict interactions and states
which are not possible in reality. For example, it is possible to put a laptop in
a room, however, putting a room in a laptop is impossible; a person can move
www.ebook3000.com

Portunes: Representing Attack Scenarios
115
Spatial node
Physical node
Digital node
Spatial layer
Object layer
Digital layer
Fig. 1. Graphic presentation of Portunes
only to a neighboring location, while data can move to any location; data can be
easily copied, while the reproduction of a computer requires assembling of other
objects or materials.
3.2
The Portunes Model
To present the diﬀerent properties and behavior of elements from physical and
digital security, the Portunes model stratiﬁes the environment of interest in three
layers: spatial, object and digital. The spatial layer presents the facility of the
organization, including rooms, halls and elevators. The object layer consists of
objects located in the facility of the organization, such as people, computers and
keys. The digital layer presents the data of interest. Stratiﬁcation of the envi-
ronment in three distinct layers allows speciﬁcation of actions that are possible
only in a single layer (copying can only happen for digital entities) or between
speciﬁc layers (a person can move data, but data cannot move a person).
The Portunes model abstracts the environment of an organization in a graph.
The model stratiﬁes the nodes of the graph in three layers and restricts the
edges between layers to reﬂect reality. A node abstracting a location, such as an
elevator or a room, belongs to the spatial layer L and it is termed a spatial node.
A node abstracting a physical object, such as a laptop or a person, belongs to the
object layer O and it is termed an object node. A node abstracting data, such as
an operating system or a ﬁle, belongs to the digital layer D. The edges between
spatial nodes denote a neighbor relation and all other edges in the model denote
a containment relation. The ontology used in Portunes is given in Figure 2. An
edge (n, m) between two spatial nodes means n is a neighbor of m. This is a
symmetric relation where the direction of the edge is not important. For all other
nodes, an edge (n, m) means that node n contains node m; this is an asymmetric
relation.
The above statements are illustrated in Figure 1 and formalized in the follow-
ing deﬁnition.

116
T. Dimkov, W. Pieters, and P. Hartel
layer
node
edge
spatial
location
neighbors
contains
object physical object contains
contains
digital
data
contains
Fig. 2. The ontology of Portunes
Deﬁnition 1. Let G = (Node, Edge) be a directed graph and D : Node →
Layer a function mapping a node to the Layer = {L, O, D}. A tuple (G, D) is
a Portunes model if it satisﬁes the following invariants C(G, D):
1. Every object node can have only one parent.
∀n ∈Node : D(n) = O →indegree(n) = 1
2. One of the predecessors of an object node must be a spatial node.
∀n ∈Node : D(n) = O →∃m ∈Node : D(m) = L ∧∃⟨m, ...., n⟩; where
⟨m, ...., n⟩denotes a ﬁnite path from m to n.
3. There is no edge from an object to a spatial node.
∄(n, m) ∈Edge : D(n) = O ∧D(m) = L
4. There is no edge from a digital to an object node.
∄(n, m) ∈Edge : D(n) = D ∧D(m) = O
5. A spatial and a digital node cannot be connected.
∄(n, m) ∈Edge : (D(n) = D ∧D(m) = L) ∨(D(n) = L ∧D(m) = D)
6. The edges between digital nodes do not generate cycles.
̸ ∃⟨n, ..., m⟩: D(n) = ... = D(m) = D ∧n = m
The intuition behind the invariants is as follows. An object node cannot be at
more than one place, thus an object node can have only one parent (1). An
object node is contained in a known location (2). An object node cannot contain
any spatial objects (3) (for example, a laptop cannot contain a room) nor can
a digital node contain an object node (4) (for example, a ﬁle cannot contain a
laptop). A spatial node cannot contain a digital node and vice versa (5), and a
digital node cannot contain itself (6).
Theorem 1. A graph G = (Node, Edge) in a Portunes model (G, D) can have
cycles only in the spatial layer:
∃⟨n, ..., m⟩: n = m →D(n) = ... = D(m) = L
Proof. The proof is presented in the appendix.
Example: Road apple attack. To show how Portunes can be used for rep-
resenting insider threats across domains, we will use the example of the road
www.ebook3000.com

Portunes: Representing Attack Scenarios
117
1 world
2 hall
3
4
secureRoom
remoteServer
5 insider
6 employee
7 server
8 dongle
9 rootkit
10 serverData
4
3
6
5
7
8
9
2
1
10
Fig. 3. Graph of the road apple attack environment
D(hall) = D(secureRoom) = D(world) = L
D(remoteServer) = D(insider) = D(employee) = D(server) = D(dongle) = O
D(serverData) = D(rootkit) = D
Fig. 4. The function D for the road apple attack environment
apple attack [4]. In this attack, an insider uses the trust of an employee (social
domain) to steal sensitive data (digital domain) from a a server in a restricted
area (physical domain).
To describe the attack, the environment in which the attack takes place needs
to include information from all three security domains. Concerning physical se-
curity, the organization has a restricted area where a server with sensitive data
resides. Additionally there is a public area where employees can socialize. Re-
garding the digital domain, the sensitive data on the server is isolated from the
rest of the network, making the data accessible only locally. The security aware-
ness of the employees is such that they trust each other enough to share oﬃce
material (for example: CDs and dongles).
An abstraction of the environment is represented as a Portunes model in Fig-
ure 3 and 4. The nodes hall, secureRoom and world are spatial nodes, serverData
and rootkit are digital nodes. All other nodes are object nodes. In Section 3.4
we will revisit the example and show how the road apple attack takes place.
3.3
The Portunes Language
In the previous section, we deﬁned a graph-based model to present the facilities
of an organization, the objects in a facility and the data of interest. This model
is on a conceptual level, and it simpliﬁes the presentation of the environment
to the user. In this section we introduce the Portunes language, which is closer
to the enforcement mechanisms. The language consists of nodes, processes and
actions, where a node in the Portunes model represents a node in the Portunes
language. The main goal of the language is to model the interaction between the
nodes in the Portunes model.

118
T. Dimkov, W. Pieters, and P. Hartel
The language captures two interactions, mobility and delegation. By making
all nodes ﬁrst class citizens, every node can move. For example, a node repre-
senting an insider can move through the organization and collect keys, which
increase his initial privileges. The Portunes language lets a delegator node dele-
gate a task to a delegatee node. During the execution of the task, the delegatee
uses the privileges of the delegator. To delegate a task, the delegatee needs to
trust the delegator. For example, an insider can delegate a task to a colleague.
The colleague will execute the task only if he trusts the insider.
The above two interactions, mobility and delegation, are restricted by the
invariants from Deﬁnition 1 and by the security policies associated with each
node. Policies on nodes from the spatial and object layer represent the physical
security. These policies restrict the physical access to spatial areas in the facility
and the objects inside the spatial areas. Policies on nodes from the digital layer
represent the digital security of the organization and focus on access control on
the data of interest. In the Portunes language people can interact with other
people. Policies on people give the social aspect of the model, or more precisely,
they deﬁne under which circumstances a person trusts another person.
Syntax. As with other members of the KLAIM family, the syntax of the Por-
tunes language consists of nodes, processes and actions. The Portunes language
lacks the tuple spaces and the actions associated with tuple spaces, which are
present in the KLAIM family of languages, and focuses on the connections be-
tween nodes. This is because connectivity is the main interest from the perspec-
tive of security modeling.
The syntax of the Portunes language is shown in Figure 5. A single node
l ::δ
s P consists of a name l ∈L, where L is a ﬁnite set of names, a set of node
names s ∈P(L), representing nodes that are connected to node l , an access
control policy δ and a process P. The relation between the graph of the Portunes
model and the expressions in the Portunes language is intuitive: a node l in the
graph represents a node with name l in the language, an edge (l, l′) in the graph
N ::=
Node
|
l ::δ
s P
Single node
|
N1 ∥N2
Net composition
P ::=
Process
|
nil
Null process
|
P1 | P2
Process composition
|
al.P
Action preﬁxing
a ::=
Action
|
login(l)
Login
|
logout(l)
Logout
|
eval(P)@l
Spawning
Fig. 5. Syntax of the Portunes language
www.ebook3000.com

Portunes: Representing Attack Scenarios
119
connects l to a node name l′ in the set s of the node l ::δ
s P. Thus, the node
name uniquely identiﬁes the node in the model, while the set s deﬁnes which
other nodes the node contains or is a neighbor of. These two relations identify
the relative location of each element in the environment. A net is a composition
of nodes.
A process P is a composition of actions. Namely, nil stands for a process
that cannot execute any action and al.P for the process that executes action a
using privileges from node l ∈L and then behaves as P. The label l identiﬁes a
node from where the privileges originate, and it is termed the origin node. The
structure P1|P2 is for parallel composition of processes P1 and P2. A process P
represents a task. A node can perform a task by itself or delegate the task to
another node.
An action a is a primitive which manipulates the nodes in the language. There
are three primitives, login(l), logout(l) and eval(P)@l. The actions login(l) and
logout(l) provide the mobility of a node, by manipulating the set s. The action
eval(P)@l delegates a task P to a node l by spawning a process in the node.
Example: For a node representing a room, room ::δ
s nil, the access control policy δ
deﬁnes the conditions under which other entities can enter or leave the room. The
set s contains the names of all nodes that are located in the room or connected to
the room. Let a supervisor and a person be in a hall hall ::δ
{person, supervisor} nil
which is neighboring the room. An example of a supervisor delegating a task to a
person is: supervisor ::δ
s eval(P)@personsupervisor where P is a process denoting
the task, person is the target node and the label supervisor is the origin node.
A person entering the room as part of the task delegated from supervisor is
presented through person ::δ
s login(room)supervisor.P ′, while a person leaving
the room person ::δ
s logout(room)supervisor.P ′′.
Depending on the privileges of the origin node which depend on its identity,
location and credentials, a node can grant a set of capabilities C = {ln, lt, e},
where ln is a capability to execute the action login, lt to execute the action
logout and e to execute the action eval. The access control policy δ is a func-
tion δ : (L ∪{⊥}) × (L ∪{⊥}) × P(L) →P(C). The ﬁrst and the second
parameter denote identity based access control and location based access con-
trol respectively. If the identity or the location does not inﬂuence the policy, it
is replaced by ⊥. The third parameter denotes credential based access control,
which requires a set of credentials to allow an action. If a policy is not aﬀected
by credentials, the third parameter is an empty set. A security policy can present
a situation where: 1) only credentials are needed, such as a door that requires a
key (⊥, ⊥, {key}) 
→{ln}, 2) only the identity is required, such as a door that
requires biometrics information (John, ⊥, ∅) 
→{ln} or 3) only the location is
required, such as data that can be reached only locally (⊥, oﬃce, ∅) 
→{ln}.
The policy supports combinations of these attributes, such as a door requiring
biometrics and a key (John, ⊥, {key}) 
→{ln}. The least restrictive policy that
can be used is: (⊥, ⊥, ∅) 
→{ln, lt, e}.

120
T. Dimkov, W. Pieters, and P. Hartel
grant(lo, δt, a) = ∃k1, k2 ∈L ∪{⊥}, ∃K ∈P(L) : a ∈δt(k1, k2, K) ∧
(k1 = lo ∨k1 = ⊥)
"
#$
%
(1)
∧(k2 ∈parents(lo) ∨k2 = ⊥)
"
#$
%
(2)
∧(K ⊆children(lo)
"
#$
%
(3)
),
where parents(lo) = { lpo| lpo ::
δpo
spo R ∈N ∧lo ∈spo} and
children(lo) = { so| lo ::δo
so R ∈N}
lt ≻ln l =
⎧
⎨
⎩
true
iﬀ
(D(lt) = L ∧D(l) = O) ∨(D(lt) = O ∧D(l) = D)
lt ≻≻ln l
iﬀ
D(lt) = D(l)
false
otherwise
.
where l ≻e lt =
(D(l) ̸= L ∧D(lt) ̸= L)
"
#$
%
(4)
∧¬(D(l) = D ∧D(lt) = O)
"
#$
%
(5)
∧(lt ∈children(l)
"
#$
%
(6)
∨(∃lp ::
δp
sp R ∈N : l ∈sp ∧lt ∈sp
"
#$
%
(7)
) ∨D(lt) = D
"
#$
%
(8)
)
Fig. 6. Auxiliary function grant and ≻relations
Auxiliary functions. Having deﬁned the behavior of nodes using three prim-
itive actions, we now look at the context where these actions can be executed.
A node l ::δ
s al′.P can be restricted in executing an action a from an origin node
l′ to a target node for three reasons. The origin node might not have suﬃcient
privileges, execution of the action a invalidates the invariants in Deﬁnition 1
from the Portunes model, or the target node might not be in proximity of the
node l. This section deﬁnes auxiliary functions for an implicitly given net N,
which take care these restrictions. The auxiliary functions are deﬁned in Figure
6 and are used to simplify the operational semantics of the language.
The grant function checks if an origin node has suﬃcient privileges to execute
an action to a target node. The ﬁrst parameter deﬁnes the name of the origin
node, the second parameter deﬁnes the policies on the target node and the third
parameter is a label of an action. Intuitively, a node can execute an action
depending on the identity lo of the origin node (1), its location parents(lo) (2)
or the keys children(lo) it contains (3). Note that the value of grant depends
solely of the origin node, not the node executing the process.
The relation lt ≻ln l states that node lt can contain node l. The goal of
this relation is to enforce the invariants 3-6 in Deﬁnition 1. From the relation,
an object node can always interact with spatial nodes and a digital node can
always interact with object nodes. The relation lt ≻≻ln l provides ordering
between nodes from the same layer. The relation is deﬁned by the user because
the ordering depends on the elements we want to model in the environment. For
example, an operating system usually can contain a ﬁle, but not vice versa. Yet,
in scenarios where the systems are virtualized, it is possible and desirable to
model a ﬁle containing an operating system. The only assumption on lt ≻≻ln l
is that it does not invalidate invariant 7 in Deﬁnition 1, or put diﬀerently, the
relation does not allow generation of cycles between nodes in the digital layer.
www.ebook3000.com

Portunes: Representing Attack Scenarios
121
The ordering relation l ≻e lt states that node l can delegate a task to node
lt by means of spawning a process. The relation restricts delegation of tasks
between nodes depending on the layer a node belongs to and the proximity
between nodes. An object node can delegate a task to a digital node or another
object node, while a digital node can delegate a task only to another digital
node. Thus, spatial nodes cannot delegate tasks, nor can a task be delegated to
spatial nodes (4), and digital nodes cannot delegate tasks to object nodes (5).
Furthermore, a non-digital node can delegate a task only to nodes it contains (6)
or nodes that are in the same location (7). In digital nodes the proximity does
not play any role in restricting the delegation of a task (8). The decision (8)
assumes the world is pervasive and two digital nodes can delegate tasks from
any location as long as they have the appropriate privileges.
The expressions from Figure 6 focus on the relation between nodes. The grant
function provides the security constraints in the language based on the location
and identity nodes, while the ≻ln, ≻≻ln and ≻e relations provide non-security
constraints derived from the layer the nodes belong to and their location. In
addition, we put a restriction on the processes inside a node, to distinguish
tasks originating from a single node. We call such processes simple processes,
and deﬁne an additional auxiliary function which helps determine if a process is
a simple process.
Deﬁnition 1. Let origin(P) →P(L) be a function which returns all the action
labels of a process P. A process P, which is either nil or contains actions only
from one origin node is a simple process. origin(P) ⊆{l0}
Operational semantics. Similar to Bettini et al. [9], the semantics of the
Portunes language is divided into process semantics and net semantics. The
process semantics is given in terms of a labeled transition relation
a
−→and
describes both the intention of a process to perform an action and the availability
of resources in the net. The label a contains the name of the node executing the
action, the target node, the origin node and a set of node names which identify
which nodes are the target node contains. The net semantics given in terms of a
transition relation ⇒describes possible net evolutions and relies on the labeled
transition
a
−−→from the process semantics.
The process semantics of the language is deﬁned in Figure 7. A node can
login to another node [login] if it has suﬃcient privileges to perform the action
(grant) if the node can be contained in the target node (≻ln) and if the process is
a simple process with origin node lo (origin). As a result of executing the action,
node l enters in node lt, or put diﬀerently, the target node lt now contains node l.
For a node to logout from a target node [logout], the target node must contain
the node (l ∈st), the origin node must have proper privileges (grant) and the
process must be a simple process with origin node lo (origin). The action results
in l leaving lt, speciﬁed through removing its node name from st. Spawning a
process [eval] requires both the node executing the action and the target node
to be close to each other or the target node to be digital (l ≻e lt), the origin node

122
T. Dimkov, W. Pieters, and P. Hartel
origin(P) ⊆{lo}
lt ≻ln l
grant(lo, δt, ln)
l ::δ
s login(lt)lo.P∥lt ::δt
st Q
login(l,lt,lo,st)
−−−−−−−−−−→l ::δ
s P∥lt ::δt
st∪l Q
[login]
origin(P) ⊆{lo}
grant(lo, δt, lt)
l ∈st
l ::δ
s logout(lt)lo.P∥lt ::δt
st Q
logout(l,lt,lo,st)
−−−−−−−−−−−→l ::δ
s P∥lt ::δt
st\{l} Q
[logout]
origin(P) ⊆{lo}
origin(Q) ⊆{lo}
l ≻e lt
grant(lo, δt, e)
l ::δ
s eval(Q)@llo
t .P∥lt ::δt
st R
eval(l,lt,lo,st)
−−−−−−−−−→l ::δ
s P∥lt ::δt
st R|Q
[eval]
l ::δ
s P
a
−−→l ::δ
s P
′
l ::δ
s P|Q
a
−−→l ::δ
s P
′|Q
[pComp]
Fig. 7. Process semantics
N
eval(l,lt,lo,st)
−−−−−−−−−→N1
N ⇒N1
N1 ⇒N
′
1
N1 ∥N2 ⇒N
′
1 ∥N2
N
logout(l,lt1 ,lo,st1 )
−−−−−−−−−−−−→N1
N
login(l,lt2 ,lo,st2 )
−−−−−−−−−−−→N2
D(l) = D
N ⇒N2
N
logout(l,lt1 ,lo,st1 )
−−−−−−−−−−−−→N1 N1
login(l,lt2 ,lo,st2 )
−−−−−−−−−−−→N2 (lt1 ∈st2 ∨lt2 ∈st1 ∨D(l) = D)
N ⇒N2
Fig. 8. Net semantics
should have the proper privileges (grant) and both processes P and Q need to
be simple processes with origin node lo (origin). The action results in delegating
a new task Q to the target node, which contains actions originating from the
same origin node as the task P.
The net semantics in Figure 8 use the process semantics to deﬁne the possible
actions in the Portunes language. Spawning a process is limited solely by the
process semantics [neteval]. To move, a node executes the logout and login
actions in sequence [netmove]. Both actions should have the same origin node
and should be executed by the same node. Furthermore, an object node can move
only to a node in its proximity, while digital nodes do not have this restriction
(lt1 ∈st2 ∨lt2 ∈st1 ∨D(l) = D). Data can be copied, which is presented by
data entering a new node without leaving the previous [netcopy]. The standard
rules for structural congruence apply and are presented in Figure 9.
Theorem 1. Nodes from the object and spatial layer cannot move to remote
locations.
Proof. (Sketch) Follows from the netmove premise: lt1 ∈st2 ∨lt2 ∈st1
Theorem 2. Nodes from the object and spatial layer can inﬂuence only child
and sibling nodes.
www.ebook3000.com

Portunes: Representing Attack Scenarios
123
(ProcCom)
P1|P2 ≡P2|P1
(NetCom)
N1∥N2 ≡N2∥N1
(Abs)
P1|nil ≡P1
Fig. 9. Structural congruence of processes and nets
Proof. (Sketch) The property follows from the premise of the eval action: ≻e
Theorem 3. Let G be a Portunes graph and N be a network of nodes in Por-
tunes language. Let Map(N) →G map a Portunes program in a Portunes model,
such that C(Map(N), D) holds.
The transitions generated from the semantics of Portunes language do not
invalidate C(Map(N), D).
Proof. The proof is presented in the appendix.
3.4
Using the Portunes Framework to Calculate Attack Scenarios
Having deﬁned Portunes in the previous sections, this section shows how the
framework can aid in calculating attack scenarios. The Portunes model helps
represent the environment graphically and puts constraints on structure. The
user needs to deﬁne: (1) a net composition that corresponds to the graph with
variables instead of processes, (2) the function D, which stratiﬁes the graph,
and (3) the relation ≻≻ln which tells which node can be contained in which
other node.
PPPPPPP
P
lt
l
1
2
3
4
5
6
7
8
9
10
1. world
2. hall
3. secureRoom
4. remoteServer
1
5. insider
1
1
1
6. employee
1
1
1
7. server
1
8. dongle
9. rootkit
10. serverData
1
Fig. 10. Deﬁnition of the auxiliary relation ≻≻ln for the road apple attack environment
The previous steps provide a representation of the environment of interest. It
is now possible to present attack scenarios through process deﬁnitions. The last
step (4) is to ﬁnd concrete process expressions (i.e.instantiations of the variables
in item (1) that invalidate a goal. An attack scenario can be generated by hand
or automatically, by using model checking techniques. Here we use the road apple
attack as an example of an attack scenario.

124
T. Dimkov, W. Pieters, and P. Hartel
Example: Road apple attack - continued. In section 3.2 we introduced the
Portunes model of the environment where the road apple takes place. We deﬁned
the relation between the elements through a graph and their properties through
the function D. Now, we additionally deﬁne the ≻≻ln relation and the security
policies on each of the nodes.
The relation ≻≻ln is deﬁned in Figure 10 through a boolean table. For exam-
ple, cell (4,8) is the result of remoteServer ≻≻ln dongle and indicates that the
remote server can contain the dongle.
Figure 11 presents the environment as a net composition. This representation
does not provide visual information about the relation between elements, as in
the Portunes model. However, the representation contains detailed information
about the security policies in place, making it suitable for analysis.
Having deﬁned the environment, now it is possible to reason about possible
attack scenarios. An attack scenario is deﬁned through generating processes in
the nodes. Figure 12 shows the dynamics of the actual road apple attack as four
processes, P1, P2, P3 and P4. All actions in the process P1 have an origin node
insider, in P2 an origin node employee, in P3 an origin node dongle and in P4
an origin node rootkit. For clarity, the labels on the actions representing the
world ::(⊥,⊥,∅) 	→{ln,lt}
{remoteServer, insider, hall} nil
|| hall ::(⊥,⊥,∅) 	→{ln,lt}
{employee, secureRoom} nil
|| secureRoom ::(employee,⊥,∅) 	→{ln,lt}
{server}
nil
|| remoteServer ::(⊥,⊥,∅) 	→{ln}
{}
nil
|| insider ::(⊥,⊥,∅) 	→{ln,lt,e}
{dongle}
P1
|| employee ::(insider,⊥,∅) 	→{ln} ; (employee,⊥,∅) 	→{ln,lt,e}
{}
P2
|| server ::(⊥,secureRoom,∅) 	→{ln,lt} ; (⊥,server,∅) 	→{ln,lt}
{serverData}
nil
|| dongle ::(⊥,⊥,∅) 	→{e} ; (dongle,⊥,∅) 	→{ln,lt}
{rootkit}
P3
|| rootkit ::(dongle,⊥,∅) 	→{ln,lt,e}
{}
P4
|| serverData ::(⊥,server,∅) 	→{e}
{}
nil
Fig. 11. The road apple attack environment in the Portunes language
P1=logout(world).login(hall).
(a )
eval(logout(insider).login(hall).logout(hall).
login(employee))@dongle
(b )
P2=eval(logout(employee).login(secureRoom).
logout(secureRoom).login(server))@dongle.
(c )
logout(hall).login(secureRoom)
P3=eval(logout(dongle).login(server))@rootkit
P4=eval(login(remoteServer))@serverData
Fig. 12. The road apple attack in the Portunes language
www.ebook3000.com

Portunes: Representing Attack Scenarios
125
4
3
6
5
7
8
9
2
1
10
1 world
2 hall
3
4
secureRoom
remoteServer
5 insider
6 employee
7 server
8 dongle
9 rootkit
10 serverData
Fig. 13. Portunes model of the road apple attack environment after the execution of
the attack
origin node are omitted from the process deﬁnitions. The insider (P1) goes in
the hall and waits for the employee (process P1 until reaches point a). Then, the
insider gives the employee the dongle containing the rootkit, which the employee
accepts (P1 reaches b). Later, the employee plugs the dongle in the secure server
(P2 reaches c) using its own credentials and the server gives the dongle (P3)
access to the local data. When the rootkit (P4) reaches the server, it copies
all the data to the remote server. The above actions represent the road apple
attack with a dongle automatically running when attached to a computer [12].
After executing the processes from Figure 12, the data will reside in the remote
server, presented through an edge (remoteServer, data) in the Portunes model
in Figure 13.
The process deﬁnitions follow the semantics of the language. Thus, no attack
deﬁned through processes will violate a security policy. This makes the frame-
work suitable for presenting scenarios where the insider does not violate a policy,
but achieves his goal by combining physical access, social engineering and digital
actions.
The road apple attack is just one attack scenario. An insider may gain posses-
sion of the data by using alternative routes. For example, the employee might be
tricked into letting the insider in the secure room, as shown through the process
deﬁnitions in Figure 14. A proper reasoning about the data exposure requires
all attack scenarios to be available to the security professional. The Portunes
framework aids in the reasoning of data exposure, by helping answer questions
such as:
1. In which locations can an object A end up? For example, show all locations
where the server data can reside.
2. Who can reach location A? For example, show all elements who can reach
the secure room.
3. What are the scenarios that violate a speciﬁc goal? For example, show all
attack scenarios where the server data ends up in a remote server.
To answer these questions, we implemented a proof of concept implementation of
the framework and used model checking to generate all possible attack scenarios
by automatically generating the processes P1 - P4. However, model checking
requires heuristics to improve the scalability and we are currently exploring other

126
T. Dimkov, W. Pieters, and P. Hartel
P1=logout(world).login(hall).eval(eval(login(remoteServer)@serverData)@server
P2=eval(logout(hall).login(secureRoom))@insider
P3=nil
P4=nil
Fig. 14. Alternative attack scenario
techniques for the generation of attack scenarios. We will discuss the algorithms
in more detail in future work.
4
Conclusion and Future Work
This paper presents Portunes, a framework consisting of a high-level model and
a language inspired by the KLAIM family of languages. Portunes is capable of
representing attacks spanning the digital, physical and social domain. To capture
the three domains eﬃciently, Portunes is able to represent 1) physical properties
of elements, 2) mobility of objects and data, 3) identity, credentials and location
based access control and 4) trust and delegation between people. The applica-
bility of Portunes is demonstrated using the example of the road apple attack,
showing how an insider can attack without violating existing security policies by
combining actions from all three domains.
As a future work, we plan to generate attack scenarios automatically from
environments presented through the Portunes framework. We are looking at ex-
isting model checking techniques and heuristics to generate all possible action
traces for each of the processes. Additionally, we are interested in mechanisms
to isolate actions which contribute to an attack and automatically generate at-
tack trees.
References
1. INFOSEC
Research
Council.
Hard
problem
list
(November
2005),
http://www.cyber.st.dhs.gov/docs/IRC_Hard_Problem_List.pdf
2. Randazzo, M.R., Keeney, M., Kowalski, E., Cappelli, D., Moore, A.: Insider threat
study: Illicit cyber activity in the banking and ﬁnance sector. U.S. Secret Service
and CERT Coordination Center Software Engineering Institute (2004)
3. DePoy, J., Phelan, J., Sholander, P., Smith, B.J., Varnado, G.B., Wyss, G.D.,
Darby, J., Walter, A.: Critical infrastructure systems of systems assessment
methodology. Technical Report SAND2006-6399, Sandia National Laboratories
(October 2007)
4. Stasiukonis, S.: Social engineering the usb way (2006),
http://www.darkreading.com/document.asp?doc_id=95556
5. Dragovic, B., Crowcroft, J.: Containment: from context awareness to contextual ef-
fects awareness. In: Proceedings of 2nd Inernational Workshop on Software Aspects
of Context. CEUR Workshop Proceedings (2005)
6. Scott, D.J.: Abstracting Application-Level Security Policy for Ubiquitous Comput-
ing. PhD thesis, University of Cambridge, Cambridge (2004)
www.ebook3000.com

Portunes: Representing Attack Scenarios
127
7. De Nicola, R., Ferrari, G.L., Pugliese, R.: KLAIM: A kernel language for
agents interaction and mobility. IEEE Transactions on software engineering 24(5),
315–330 (1998)
8. Gorla, D., Pugliese, R.: Resource access and mobility control with dynamic priv-
ileges acquisition. In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J.
(eds.) ICALP 2003. LNCS, vol. 2719, pp. 119–132. Springer, Heidelberg (2003)
9. Bettini, L., Loreti, M., Pugliese, R.: An infrastructure language for open nets.
In: SAC 2002: Proceedings of the 2002 ACM Symposium on Applied Computing,
pp. 373–377. ACM, New York (2002)
10. Probst, C.W., Hansen, R.R., Nielson, F.: Where can an insider attack? In: Dim-
itrakos, T., Martinelli, F., Ryan, P.Y.A., Schneider, S. (eds.) FAST 2006. LNCS,
vol. 4691, pp. 127–142. Springer, Heidelberg (2007)
11. Dimkov, T., Tang, Q., Hartel, P.H.: On the inability of existing security models to
cope with data mobility in dynamic organizations. In: Proceedings of the Workshop
on Modeling Security. CEUR Workshop Proceedings (2008)
12. AlZarouni, M.: The reality of risks from consented use of usb devices. In: Valli,
C., Woodward, A. (eds.) Proceedings of the 4th Australian Information Security
Conference, pp. 5–15 (2006)

128
T. Dimkov, W. Pieters, and P. Hartel
Appendix
Proof (of Theorem 1). The theorem follows from three properties, which we
prove in turn:
1. There are no cycles between layers.
2. There are no cycles in the object layer.
3. There are no cycles in the digital layer.
1. There are no cycles between layers
̸ ∃⟨n0...ni...nk⟩: n0 = nk ∧D(n0) ̸= D(ni)
Lets assume that such a cycle exists:
∃⟨n0...ni...nk⟩: n0 = nk ∧D(n0) ̸= D(ni)
Thus, there are at least two edges in the graph which connect nodes from
diﬀerent layers:
∃(nj−1, nj), (nl, nl+1) ∈Edge : D(nj−1) ̸= D(nj) ∧D(nl) ̸= D(nl+1) ∧
D(nj−1) = D(nl+1) ∧D(nj) = D(nl)
From the invariants 3, 4, 5 (tabulated in Table 1) follows that such a pair of
edges does not exist.
Table 1. Invariants 3,4,5 forbid any cycles between layers
Layer
1(L1)
Layer
2(L2)
Edge
Edge
from
L1
to
L2
from L2 to L1
L
O
+
- (invariant 3)
L
D
- (invariant 5) - (invariant 5)
O
D
+
- (invariant 4)
2. There are no cycles in the object layer.
̸ ∃⟨n, ..., m⟩: D(n) = ... = D(m) = O ∧n = m
Lets assume such a cycle exists:
∃⟨n, ...ni..., m⟩: D(n) = ... D(ni) ... = D(m) = O ∧n = m.
From invariant 2,
∃m ∈Node : D(m) = L ∧∃⟨m, ....n
′
i−1, ni⟩, follows
∃(n
′
i−1, ni), (ni−1, ni). If n
′
i−1 ̸= ni−1 there is a contradiction with invari-
ant 1. Otherwise D(n
′
i−1) = O, and the analysis is repeated for the path
⟨m, ....n
′
i−1⟩. Because ⟨m, ....n
′
i−1⟩is ﬁnite, at one point the path reaches
a spatial node, and n
′
i−1 ̸= ni−1. This again contradicts with invariant 1.
Thus, such cycle does not exist.
3. There are no cycles in the digital layer.
̸ ∃⟨n, ..., m⟩: D(n) = ... = D(m) = D ∧n = m
This comes directly from invariant 6.
www.ebook3000.com

Portunes: Representing Attack Scenarios
129
Proof (of Theorem 3). Suppose there is a net N1 which satisﬁes the invariants
C(Map(N1), D). Suppose exists a net N2 which is a product of a net transfor-
mation on N1. ∃N2 : N1 ⇒N2. We need to prove that C(Map(N2), D) also
holds.
The relation ⇒is used in the net actions neteval, netcopy and netmove.
1. neteval does not cause any changes of the structure of the net. Thus any
execution of neteval cannot invalidate an invariant.
2. netmove removes an edge (lt1, l) and generates a new one (lt2, l). We need
to show that the
login(l,lt2 ,lo,st2 )
−−−−−−−−−−−→action does not invalidate any invariant.
Suppose the rule invalidates an invariant.
(a) Let D(l) = O. After
logout(l,lt1 ,lo,st1 )
−−−−−−−−−−−−→, indegree(l) = 0. Latter, when
login(l,lt2 ,lo,st2 )
−−−−−−−−−−−→is applied, indegree(l) = 1. Thus, invariant 1 is not
invalidated.
(b) Let D(l) = O. After
login(l,lt2 ,lo,st2 )
−−−−−−−−−−−→is applied, from ≻ln, D(lt2) = L or
D(lt2) = O. The former case does not invalidate the second invariant by
deﬁnition. Since C(Map(N1), D), ∃m ∈Node : ∃⟨m...lt2⟩∧D(m) = S,
the latter case also does not invalidate the second invariant.
(c) The invariants 3, 4, 5 are not invalidated by the deﬁnition of ≻ln.
(d) The last invariant is not invalidated because of the assumption in ≻≻.
3. The eﬀect of netcopy is an additional edge in the graph edge (lt, l) generated
by the relation
login(l,lt,lo,st)
−−−−−−−−−−→. The premise of netcopy enforces a restriction
D(lt) = D. Additional restriction comes from the relation ≻ln, which allows
an edge to be generated only between a node from the object and digital
layer D(l) = D ∧D(lt) = O or between two nodes from the digital layer
D(l) = D ∧D(lt) = D. The former does not invalidate any of the invariants,
while the latter is restricted by the assumption on ≻≻.

Match It or Die: Proving Integrity by Equality⋆
Matteo Centenaro and Riccardo Focardi
Universit`a Ca’ Foscari Venezia
{mcentena,focardi}@dsi.unive.it
Abstract. Cryptographic hash functions are commonly used as modiﬁ-
cation detection codes. The goal is to provide message integrity assurance
by comparing the digest of the original message with the hash of what is
thought to be the intended message. This paper generalizes this idea by
applying it to general expressions instead of just digests: success of an
equality test between a tainted data and a trusted one can be seen as a
proof of high-integrity for the ﬁrst item. Secure usage of hash functions
is also studied with respect to the conﬁdentiality of digests by extending
secret-sensitive noninterference of Demange and Sands.
Keywords: Information Flow, Language-based Security, Security Type
System, Hash Functions.
1
Introduction
A hash function takes as input an arbitrary message and ‘summarizes’ it into a
digest. In cryptography, hash functions are commonly used as modiﬁcation de-
tection codes [9]: we are guaranteed of the integrity of a message M whenever the
hash of M coincides with a previously computed digest of the original message.
Moreover, hash functions are also commonly employed to protect data secrecy as
done, e.g., in Unix password ﬁles. To provide both integrity and conﬁdentiality,
hash functions are required to respectively be collision resistant and one-way
[9], meaning that it should be infeasible to exhibit two messages with the same
digest and to ﬁnd a message whose digest matches a given one.
A ﬁrst example of everyday usage of hash functions is password-based authen-
tication: a one-way hash of the user password is securely stored in the system and
is compared with the hash of the password typed by the user at the login prompt,
whenever the user wants to access her account. The following code is a simpli-
ﬁed fragment of the Unix su utility used to let a system administrator perform
privileged actions. The password ﬁle is modeled as an array passwd[username].
trial = hash(t_pwd);
if (trial = passwd[root]) then
<< launch the administrator shell >>
⋆Work partially supported by Miur’07 Project SOFT: “Security Oriented Formal
Techniques”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 130–145, 2010.
  Springer-Verlag Berlin Heidelberg 2010
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
131
The typed password t pwd is given as input to the program thus we regard it
as untrusted. In fact, from the program perspective there could be an enemy
‘out there’ trying to impersonate the legitimate administrator. The same holds
for trial, which is computed from an untrusted value. Existing type systems
for noninterference would consequently consider the guard of the if branch as
tainted, or low-integrity, since its value is computed from untrusted data and
possibly under the control of the enemy. For this reason, the code in the if-
then branch would be required to never modify high-integrity values. Clearly the
administrator shell can make any change to the system including, e.g., modifying
user passwords, and this program would be consequently rejected.
One of the motivations for hashing passwords is to protect conﬁdentiality. In
fact, if the hash function is one-way, it is infeasible for an opponent to ﬁnd a
password whose hash matches the one stored in the password ﬁle. In practice,
brute force dictionary attacks suggest that the password ﬁle should be never-
theless kept inaccessible to non administrators, as it is done, e.g., in the shadow
password mechanism of Unix. However, if password entropy is ‘high enough’
it might be safe to let every user access the hashed passwords. Formally, this
would correspond to assigning the array passwd[] a low-conﬁdentiality secu-
rity level. Consider now the following update of Alice’s password to the new,
high-conﬁdentiality value alice pwd:
passwd[alice] := hash(alice_pwd);
This assignment would be rejected by usual type systems for noninterference, as
it downgrades the conﬁdentiality level of the password.
Hash functions are also often used for integrity checks. We consider a software
producer who wants to distribute an application on the Internet, using diﬀerent
mirrors in order to speedup the downloads. A common way to assure users
downloading a binary ﬁle my blob.bin from mirrors of its integrity, is to provide
them with a trusted digest swdigest of the original program. The browser would
then run a code similar to the following:
if (hash(myblob.bin) = swdigest) then
trusted_blob.bin := my_blob.bin;
<< install trusted_blob >>
The idea is that the user will install the given binary only if its digest matches
the one of the original program provided by the software company. In fact, if the
hash function is collision resistant, it would be infeasible for an attacker to modify
the downloaded program while preserving the digest. Once the check succeeds,
my blob.bin can be safely ‘promoted’ to high-integrity and installed into the
system. This is modelled by assigning my blob.bin to the high-integrity variable
trusted blob.bin. This is usually regarded as a direct integrity ﬂaw and rejected
by usual type systems for noninterference. Moreover, installing the application
can be thought as writing into a high-integrity area of the ﬁle system and, as for
the root shell above, would be forbidden in a branch with a low-integrity guard.
Our contribution. We have discussed how typical examples of programs that
use cryptographic hash functions break standard notions of noninterference, even

132
M. Centenaro and R. Focardi
if they are intuitively secure. In this work, we study how to extend noninterfer-
ence notions so that such kinds of program can be type checked and proved
secure. We model hash functions symbolically: the hash of a value v is simply
h(v). We do not assume any deconstructor allowing to recover v from h(v) thus
modelling the fact h is one-way, and we also assume h(v) = h(v ′) if and only if
v = v ′, modelling collision resistance. As is customary in symbolic settings, what
has negligible probability in the computational world becomes here impossible.
We focus on what we informally call ‘match-it-or-die’ programs which, like
the above examples, always perform integrity checks at the outer level and fail
whenever the check is not passed. For these programs, the attacker is not in-
terested in causing a failure, as no code would be executed in such a case.
This enables us to type check programs that assign to high-integrity variables
even in a low-integrity if branch, as in the Unix su example. We then observe
that assignments such as trusted blob.bin := my blob.bin are safe under the
check hash(myblob.bin) = swdigest. In fact, since swdigest is high-integrity,
when its value matches the one of hash(myblob.bin), we are guaranteed that
myblob.bin has not been tampered with. This allows us to type check programs
like the application downloading example.
Moreover, we investigate the conﬁdentiality requirements for using hash func-
tions to preserve data secrecy. We ﬁrst observe that if the entropy of the hashed
value is low an attacker might try to compute, by brute force, the hash of all
the possible values until he ﬁnds a match. We thus select, as our starting point,
a recent noninterference variant called secret-sensitive noninterference [6] which
distinguishes small and big secrets and allows us to treat their corresponding
digests accordingly. If a secret is big, meaning that it is infeasible to guess its
actual value, then the brute force attack above is also infeasible. We show that
it is safe to downgrade the hash of a big secret, assuming some control over
what secret is actually hashed. In fact, two hashes of the same big secret are
always identical and the opponent might deduce some information by observing
equality patterns of digests. This requires a nontrivial extension of the notion of
memory equivalence so to suitably deal with such equality patterns.
Finally, we give a security type system to statically enforce that programs
guarantee the proposed noninterference notions.
Structure of the paper. In Section 2 we give the background on secret-
sensitive noninterference [6]; Section 3 extends the noninterference notions so
to correctly deal with hash functions. Integrity check by equality is analyzed
in Section 4. The security type system enforcing noninterference is given in
Section 5 while Section 6 discusses related works. The paper closes with some
ﬁnal remarks and ideas for future work in Section 7.
2
Secret-Sensitive Noninterference
Secret-sensitive noninterference (SSNI) [6], by Demange and Sands, is a vari-
ant of noninterference which distinguishes small, guessable secrets from big,
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
133
unguessable ones. As discussed in the introduction, this distinction will be use-
ful to discipline the downgrading of digests of secret values, as we will see in
Section 3.
Size aware security lattice.
Secrets are partitioned into big (Hb) and
Fig. 1. Size aware Lattice
small (Hs) ones. Preorder ⊑C among conﬁden-
tiality levels is deﬁned as L ⊑C Hb ⊑C Hs,
meaning that public, low data can be regarded
as secret and, as discussed above, small se-
crets need to be treated more carefully than
big ones. We extend this size aware conﬁden-
tiality lattice by composing it with the basic
two-level integrity lattice in which H ⊑I L.
Notice that integrity levels are contravariant:
low-integrity, tainted values have to be used
more carefully than high-integrity, untainted
ones. The product of these two lattices is de-
picted in Figure 1. We will write ℓ= ℓCℓI to
range over the product lattice elements. The ordering between the new security
levels ℓis denoted by ⊑and is deﬁned as the componentwise application of ⊑C
and ⊑I.
Language. Programs under analysis are written in a standard imperative while-
language. We assume a set of variables Var ranged over by x, a set of values Val
ranged over by v and a set of arithmetic and boolean operations ranged over by
op . The syntax for expressions e and commands c follows.
e ::= x | e1 op e2
c ::= skip | x := e | if e then c1 else c2 | while e do c | c1; c2
Memories M : Var →Val are ﬁnite maps from variables to values. We write
e ↓M v to note the atomic evaluation of expression e to value v in memory M.
Command semantics is given in terms of a small-step transition between conﬁg-
urations ⟨M, c⟩. Transitions are labeled with an event α ∈Var ∪{τ} indicating
that an assignment to variable α (or no assignment if α is τ) has happened.
Command semantics rules are standard and are omitted here for lack of space;
they can be found in the full version of the paper [4].
Observable behaviour. We assume to have a security environment Γ mapping
variables to their security levels. Users at level ℓmay only read variables whose
level is lower or equal than ℓ. Let M|ℓbe the projection of the memory M to
level ℓ, i.e., memory M restricted to variables visible at level ℓor below.
Deﬁnition 1 (Memories ℓ-equivalence). M and M′ are ℓ-equivalent, written
M =ℓM′, if M|ℓ= M′|ℓ.
Intuitively, users at level ℓwill never be able to distinguish two ℓ-equivalent
memories M and M′.

134
M. Centenaro and R. Focardi
Similarly, users may only observe transitions assigning to variables at or below
their level. Transition
α−→ℓis deﬁned as the least relation among conﬁgurations
such that:
⟨M, c⟩
x−→⟨M′, c′⟩
Γ(x) ⊑ℓ
⟨M, c⟩
x−→ℓ⟨M′, c′⟩
⟨M, c⟩
α−→⟨M′, c′⟩
α = τ or α = x with Γ(x) ̸⊑ℓ
⟨M, c⟩
τ−→ℓ⟨M′, c′⟩
We write
α⇒ℓto denote
τ−→
∗
ℓ
α−→ℓ, if α ̸= τ, or
τ−→
∗
ℓotherwise. Transitions
τ⇒ℓare
considered internal, silent reductions which are unobservable by anyone. Notice,
instead, that for observable transitions
x⇒ℓ, the level of x is always at or below
ℓ, i.e., Γ(x) ⊑ℓ.
Secure programs. The main idea of SSNI [6] is that for unguessable secrets,
brute force attacks will terminate only with negligible probability. Intuitively,
this allows for adopting a termination insensitive equivalence notion when com-
paring program behaviour. Guessable secrets, instead, can be leaked by brute
force using ‘termination channels’, and for those values it is necessary to distin-
guish between terminating and nonterminating executions.
A conﬁguration ⟨M, c⟩diverges for ℓ, written ⟨M, c⟩⇑ℓ, if it will never perform
any ℓ-observable transition
x−→ℓ. A termination insensitive ℓ-bisimulation requires
that observable transitions of the ﬁrst program are simulated by the second one,
unless the latter diverges on ℓmeaning that it cannot execute any observable
action at or below ℓ.
Deﬁnition 1 (Termination insensitive ℓ-bisimulation)
A symmetric relation R on conﬁgurations is a termination insensitive ℓ-
bisimulation (ℓ-TIB) if ⟨M1, c1⟩R ⟨M2, c2⟩implies M1 =ℓM2 and whenever
⟨M1, c1⟩
α−→ℓ⟨M′
1, c′
1⟩then either
– ⟨M2, c2⟩
α⇒ℓ⟨M′
2, c′
2⟩and ⟨M′
1, c′
1⟩R ⟨M′
2, c′
2⟩or
– ⟨M2, c2⟩⇑ℓ
Conﬁgurations ⟨M1, c1⟩, ⟨M2, c2⟩are termination insensitive ℓ-bisimilar, written
⟨M1, c1⟩≈ℓ⟨M2, c2⟩, if there exists a ℓ-TIB relating them.
Termination sensitive bisimulation, instead, always requires observable actions
to be simulated.
Deﬁnition 2 (Termination sensitive ℓ-bisimulation)
A symmetric relation R on conﬁgurations is a
termination sensitive ℓ-
bisimulation (ℓ-TSB) if ⟨M1, c1⟩R ⟨M2, c2⟩implies M1 =ℓM2 and whenever
⟨M1, c1⟩
α−→ℓ⟨M′
1, c′
1⟩then ⟨M2, c2⟩
α⇒ℓ⟨M′
2, c′
2⟩, ⟨M′
1, c′
1⟩R ⟨M′
2, c′
2⟩. Con-
ﬁgurations ⟨M1, c1⟩and ⟨M2, c2⟩are termination sensitive ℓ-bisimilar, written
⟨M1, c1⟩≃ℓ⟨M2, c2⟩, if there exists a ℓ-TSB relating them.
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
135
A secure program will preserve small secrets from being leaked via the termi-
nation channel while will be more liberal with respect to the big ones. This is
achieved by requiring termination sensitive bisimilarity whenever the inspected
memories are the same at level HbL, meaning they only diﬀer on small secrets.
Notice that, as usual, the attacker is assumed to be at level LL.
Deﬁnition 3 (Secret-sensitive NI)
A command c satisﬁes secret-sensitive NI if ∀M1 =LL M2 it holds
1. ⟨M1, c⟩≈LL ⟨M2, c⟩and
2. M1 =HbL M2 implies ⟨M1, c⟩≃HbL ⟨M2, c⟩.
3
Hash Functions and Secrecy
This section extends secret-sensitive NI to programs that use hash functions. As
already observed, hash functions could be subject to brute force attacks, unless
the hashed messages are big enough to make exhaustive search infeasible. The
idea is to take advantage of the two distinct secret levels Hb and Hs to protect
digests of small secrets and treat more liberally the digests of big secrets.
Hash expressions. The language of the previous section is augmented with a
new hash expression whose semantics is deﬁned in terms of a special constructor
h. Formally, hash(e) ↓M h(v) if e ↓M v with v ∈Val. We then partition Val
into the sets of small and big values Val s, Valb, ranged over by vs and vb. We
deﬁne the sets of small and big digests as Val d
δ = {h(v) | v ∈Val δ}, with δ ∈
{s, b}. As discussed in the introduction, this simple modelling of hash functions
is coherent with the assumption of being one-way (no deconstructor expressions)
and collision resistant (digests of diﬀerent values never collide).
Memories ℓ-equivalence. Lifting the notion of memory equivalence when
dealing with digests requires to carefully handle equality patterns. In fact, in
our symbolic model, equal digests will correspond to equal hashed messages.
Consider the program x := hash(y), where x is a public variable and y is a
secret one. It must be considered secure only if y is a big secret variable, indeed
leaking the digest of a small secret is equivalent to directly reveal the secret since
an attacker could perform a brute force attack on the hash.
The equivalence notion between memories needs, however, to be relaxed in
order to capture the fact that big secrets are, in practice, random unpredictable
values. We illustrate considering again x := hash(y) and assuming y to be a big
secret variable. We let M1(x) = 0 = M2(x), M1(y) = vb ̸= v ′
b = M2(y), then it
holds M1 =LL M2. Executing the above code the resulting memories diﬀer on the
value stored in the public variable x: M′
1(x) = h(vb) ̸= h(v ′
b) = M′
2(x). It follows
that M′
1 ̸=LL M′
2 and the program does not respect noninterference so it would
be rejected as insecure. However, vb and v ′
b are two big random numbers and we
can never expect they are equal. Thus, the only opportunity for the attacker is
to see if they correspond to other big values in the same memory. Requiring the
equality of big secrets and digests across memories is, consequently, too strong.

136
M. Centenaro and R. Focardi
This boils down to the idea of patterns, already employed in [5,7,8] for crypto-
graphic primitives. We illustrate through an example. Consider program
z := hash(x); w := hash(y) where z and w are public variables and x and y are
big secrets. Consider the following memories:
M1
M2
x : vb x : vb
y : v ′
b y : vb
z : 0 z : 0
w : 0 w : 0
(1)
executing the above code would make public two diﬀerent digests in M1 and the
very same digests in M2. The attacker is able to learn that the ﬁrst memory
stores two diﬀerent secrets values while the second does not. In summary, we do
not require the equality of big secrets and digests across memory but only that
the equality patterns are the same.
As the last example shows, in order to safely downgrade digests of big secrets
we need to control how big secrets are stored in the memories. We do this by
projecting out from memories big secret values which are either stored in big
secret variables or hashed and observable from ℓ. This is done by the following
function r, taking as parameters the value v and the level ℓv of a variable.
rℓ(v, ℓv) =
⎧
⎨
⎩
v if v ∈Val b and ℓv = HbℓI
v ′ if v = h(v ′), v ′ ∈Val b and ℓv ⊑ℓ
0 otherwise
A big secret projection rℓ(M) is deﬁned as rℓ(M)(x) = rℓ(M(x), Γ(x)), for all
x ∈Dom(M). Two memories will be comparable if their big secret projections
can be matched by renaming big values , i.e., if two big values are the same in
one projection then it will also be the case that they are equal in the other one.
Deﬁnition 4 (Comparable memories)
Two memories M1 and M2 are ℓ-comparable, noted M1 ▷◁ℓM2, if there exists a
bijection μ : Val b →Val b such that rℓ(M1) = rℓ(M2)μ.
Example 1. The two above memories (1) are not comparable if observed at level
LL, i.e., M1 ̸▷◁LL M2. In fact, rℓ(M1)(x) = vb ̸= v ′
b = rℓ(M1)(y) while rℓ(M2)(x) =
vb = vb = rℓ(M2)(y). Thus there exists no bijection μ such that rℓ(M1) = rℓ(M2)μ,
since μ cannot map vb to both vb and v ′
b.
Two memories are ℓ-equivalent if they are ℓ-comparable and their observable big
digests expose the same equality patterns. A digest substitution ρ is a bijection
on digests of big values: ρ : Val d
b →Vald
b.
Deﬁnition 5 (Memory ℓ-equivalence with hash functions)
Two memories, M1 and M2, are ℓ-equivalent, written M1 =h
ℓM2, if M1 ▷◁ℓM2
and there exists a digest substitution ρ such that M1|ℓ= M2|ℓρ.
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
137
Secure programs. The bisimulation deﬁnitions given in the previous section
are left unchanged except for the relation used to compare memories which is
now =h
ℓin place of =ℓ. Secret-sensitive NI is thus rephrased as follows.
Deﬁnition 6 (Secret-sensitive NI with hash functions)
A command c satisﬁes secret-sensitive NI if ∀M1 =h
LL M2 it holds
1. ⟨M1, c⟩≈LL ⟨M2, c⟩and
2. M1 =h
HbL M2 implies ⟨M1, c⟩≃HbL ⟨M2, c⟩.
4
Proving Integrity by Equality
In a recent work [5], we prove integrity of low-integrity data using Message Au-
thentication Codes (MACs). The secrecy of the MAC key ensures the integrity
of the exchanged data: once the MAC is recomputed and checked, we are guar-
anteed that no one has manipulated the received data. This work generalizes the
idea by applying it to the simpler case of an equality test with respect to a high-
integrity value: if the test is successful we are guaranteed that the compared,
low-integrity, value has not been tampered with.
Integrity can be checked via noninterference by placing the observer at level
HsH. This amounts to quantifying over all the values in low-integrity variables
and observing any interference they possibly cause on high-integrity variables.
Deﬁnition 7 (Integrity NI)
A program c satisﬁes integrity NI if for all M1, M2 such that M1 =HsH M2 it
holds ⟨M1, c⟩≈HsH ⟨M2, c⟩.
Consider the program if (x = y) then c1 else c2 where x is a low-integrity
variable and y is a high-integrity one. If either c1 or c2 modiﬁes high-integrity
variables this program is rejected. If it were not, an opponent manipulating
the low-integrity variable x might force the program to execute one of the two
branches and gain control on the fact high-integrity variables are updated via c1
or c2.
Consider now the case of the simpliﬁed su utility discussed in the introduc-
tion. Similarly to what we have seen above, an attacker might insert a wrong
administrator password making the check fail. However, the program is in what
we have called match-it-or-die form: the else branch is empty and nothing is
executed after the if-then command. In the deﬁnition we have given, we obtain
that the program diverges and the termination insensitive notion of Integrity NI
would consider the program secure.
A special case of integrity test is the one which involves the comparison be-
tween the on-the-ﬂy hash of a low-integrity message and a trusted variable. Upon
success, integrity of the untrusted data will be proved and it will be possible to
assign it to a high-integrity variable. Consider the following program where y
and z are trusted variables while x is a tainted one.
if (hash(x) = y) then
z := x;

138
M. Centenaro and R. Focardi
As in the software distribution example of the introduction the assignment is safe,
since it will be executed only if the contents of the variable x has been checked to
be high-integrity by comparing its digest with the high-integrity digest y.
5
Security Type System
This section presents a security type system to statically analyze programs that
use hash functions and derive data integrity by equality tests.
The proposed solution is based on the type system by Demange and Sands
[6]. We only report typing rules for expressions and integrity check commands.
All the remaining rules are as in [6].
We distinguish among four diﬀerent types of values: small, big and their re-
spective digests. Value types V T are S (small), B (big), S# (hash of a small) and
B# (hash of a big) and are ranged over by vt. These value types are populated
by the respective values:
(v-small) v ∈Val s
⊢v : S
(v-big) v ∈Valb
⊢v : B
(v-hashs) v ∈Val ds
⊢v : S#
(v-hashb) v ∈Val db
⊢v : B#
Security types are of the form τ = λℓ, where λ ∈{P, D} distinguish between
plain values and digests, while ℓis the associated security level. A security type
environment Δ is a mapping from variable to their security types. Given τ = λℓ
the two functions T and L give respectively its variable type and security level,
i.e., T(τ) = λ and L(τ) = ℓ.
A subtype relation is deﬁned over security types, it is meant to preserve λ,
as we do not want to mix plain values with digests. Moreover plain big secrets
do not appear in the relation meaning that they can only be picked from Valb.
Subtyping ≤is the least relation such that τ1 ≤τ2 if T(τ1) = T(τ2) = D and
L(τ1) ⊑L(τ2) or T(τ1) = T(τ2) = P, L(τi) ̸= HbℓI and L(τ1) ⊑L(τ2).
To prove that the type system enforces the security properties stated above
it must be that plain big secret variables really store big values. To guarantee
that small values are never assigned to big variables a conservative approach will
be taken: every expression which involves an operator and returns a plain value
lifts the conﬁdentiality level of its result to Hs, whenever it would be Hb. The
following function on security levels performs this upgrade:
ℓ⊔=

HsℓI if ℓ= HbℓI
ℓ
otherwise
A variable x respects its security type τ = λℓwith respect to a memory M if
λ = P and ⊢M(x) : S or ⊢M(x) : B and similarly if λ = D then ⊢M(x) : S# or
⊢M(x) : B#. A memory will be said to be well-formed if it respects the type of
its variables, more precisely the expected properties are:
1. All variable respects their security types
2. Public variables do not store plain big values
3. Plain big secret variables only store big values.
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
139
Table 1. Security Type System - The Most Signiﬁcant Rules
Expressions
(var) Δ(x) = τ
Δ ⊢x : τ
(sub) Δ ⊢e : τ ′
τ ′ ≤τ
Δ ⊢e : τ
(eq) Δ ⊢e : τ
Δ ⊢e2 : τ
L(τ) = ℓ
Δ ⊢e1 = e2 : Pℓ⊔
(op) Δ ⊢e1 : Pℓ
Δ ⊢e2 : Pℓ
Δ ⊢e1 op e2 : Pℓ⊔
(hash-b)
Δ ⊢x : PHbℓI
Δ ⊢hash(x) : DLℓI
(hash-s)
Δ ⊢x : Pℓ
Δ ⊢hash(x) : Dℓ
Integrity Test Commands
(int-test)
Δ ⊢x : τ
Δ ⊢y : τ ′
T(τ) = T(τ ′)
L(τ) = ℓCL
L(τ ′) = ℓCH
ℓC ⊑C Hb
Δ ⊢c : (ℓCH, t, f)
Δ ⊢if x = y then c else FAIL : (ℓCH, t ⊔ℓCL, ↑)
(int-hash)
Δ ⊢x : PℓCL
Δ ⊢y : DℓCH
Δ(z) = PℓCH
ℓC ⊑C Hb
Δ ⊢c : (ℓCH, t, f)
Δ ⊢if hash(x) = y then z := x; c else FAIL : (ℓCH, t ⊔ℓCL, ↑)
From now on it will be supposed that all the memories are well-formed and
indeed it can be proved that memory well-formedness is preserved by typed
programs.
Expression typing rules are depicted in Table 1. Rules (var) and (sub) are
standard. Rule (eq) types the equality test of two expressions requiring that they
type the same τ and judging the (boolean) result as a plain small value. Rule (op)
let any operator to be applied only to plain expressions, since in our symbolic
model of the hash function no operation is deﬁned on digests except for the
equality test. These two rules use the ℓ⊔function to promote the conﬁdentiality
level of their result to Hs whenever necessary, as already discussed above.
Hashes are typed either by (hash-b) or (hash-s). Rule (hash-b) performs a
controlled declassiﬁcation, the idea is that since the message is a plain big value
its secrecy is not broken by releasing its digest. Indeed it can be proved that this
does not break noninterference. The latter typing rule does nothing special and
just preserve the security level of its argument.
The type system has to enforce a termination insensitive noninterference for
big secrets and termination sensitive for small ones. This latter requirement can
be achieved by some strong limitations on the while loops [13] or by accounting
for the termination eﬀect of a command [3,11]. Demange and Sands type system
[6] is built upon the work of Boudol and Castellani [3]. For lack of space, we
only illustrate the two new rules added to the SSNI original type system [6].
The remaining type system can be found in the full version of the paper [4].
A command type is a triple (w, t, f) where w and t are security levels and f is a
termination ﬂag ranging over ↓and ↑which respectively note that the command

140
M. Centenaro and R. Focardi
always terminates or that it could not terminate. A program is considered to
be always terminating if it does not contain any while loop. The two ﬂags are
ordered as ↓⊑↑. A type judgement of the form Δ ⊢c : (w, t, f) means that
c does not assign to variables whose security level is lower than w (w is the
writing eﬀect of c), observing the termination of c gives information on variables
at most at level t (t is the termination eﬀect of c) and the termination behaviour
is described by f.
Rules (int-test) and (int-hash) are new contributions of this work and imple-
ment the integrity veriﬁcation tests discussed in Section 4. The former one let a
trusted computation happens if the integrity of a tainted variable is proved by
an equality test with an untainted one. The latter is pretty similar but is speciﬁc
for the hash case and asserts that the intended original message, stored in the
untrusted variable used to compute the on-the-ﬂy digest, can be assigned to a
trusted variable, whenever the check succeeds.
The else branch in both cases must be the special command FAIL. It is a
silent diverging while loop of the form while true do skip. Requiring that each
integrity test command executes such a program in case its guard condition
is not satisﬁed assure that all the typed programs are in the ‘match-it-or-die’
form. In fact, upon failure no observable actions will be ever executed which is
equivalent to say that, from an attacker point of view, no code would be run.
The conﬁdentiality level of the variables involved in (int-test) and (int-hash)
guard is constrained to be at most Hb to avoid brute force attacks to small
secrets. The command in the if branch (c) is typed with a writing eﬀect which
has an high-integrity level, thus letting it to have write clearance to high-integrity
variables, and the same conﬁdentiality level as the two expressions compared in
the guard. The termination eﬀect of the overall command is constrained by
the one of c and by ℓCL since the test contains variables which are at most at
that security level. Note that the two integrity tests would be potentially non
terminating due to the FAIL branch.
Results. The proposed type system enforces the security properties given in
Section 3 and Section 4: If a program type checks then it is both secret-sensitive
and integrity noninterferent.
Theorem 1 (SSNI by typing)
If Δ ⊢c : (w, t, f) then c satisﬁes Secret-sensitive NI.
Theorem 2 (Integrity NI by typing)
If Δ ⊢c : (w, t, f) then c satisﬁes integrity NI.
Proofs are omitted here for lack of space but can be found in the full version of the
paper [4]. In the Appendix the simpliﬁed su utility and the software distribution
examples given in the introduction, are shown to type check. A program which
let a user update its password is also presented and typed.
6
Related Works
This section discusses related work in the literature.
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
141
Hash functions. A secure usage of hash function in the setting of information
ﬂow security has been already explored by Volpano in [12]. There are, however,
many diﬀerence with respect to our work. First, Volpano does not account for
data integrity and, consequently, integrity checks, which is one of the major
contributions of our work. On the other side, we limit our study to a symbolic
treatment of hash functions, distinguishing between two diﬀerent kind of secrets,
while Volpano aims at a computational result.
Secrecy. The use of patterns to deﬁne a memory equivalence notion suitable to
be used to compare digests, originates from the work of Abadi and Rogaway [2]
and Abadi and J¨urjens [1] whose aim was to establish a link between the formal
and computational treatment of encryption. The same idea has recently been ap-
plied in the information ﬂow security [7,8] to prove that randomized cyphertexts
could be leaked without breaking noninterference. Deterministic encryption has
been modeled in a symbolic setting for information ﬂow by Centenaro, Focardi,
Luccio and Steel [5] extending the idea of pattern. That work anyway does not
account for hash functions. The distinction between big and small secrets and
the two diﬀerent bisimilarity notions which have to be applied to protect them
is completely inspired by Demange and Sands [6].
Integrity checks. A recent work on the type checking of PIN processing secu-
rity APIs [5] already implements an integrity test by means of Message Authen-
tication Codes (MACs). It might appear that this was possible thanks to the
usage of a secret key in the computation of the MAC. Instead, this work shed
new light on the fundamental reasoning underlying such integrity checks, which
are generalized to a less restrictive context and also applied to the special case
of the hash function.
7
Conclusions
We have studied the security of programs that use hash functions in the setting
of information ﬂow security. We have shown how to prove data integrity via
equality tests between a low and a high-integrity variable.
We have extended secret-sensitive noninterference to guarantee that leaks via
the hash operator could not occur: the intuition is that the digest of a big enough
secret s would not be subject to a brute force attack and so releasing it to the
public will not break the conﬁdentiality of s.
A classical noninterference property has been instead used to check that se-
cure programs do not taint high-integrity data. Equality tests to enforce data
integrity have been introduced: this idea is a generalization of the integrity proof
performed using Message Authentication Codes (MACs) in [5]. The equality of
a tainted variable with a trusted one is regarded as an evidence of the fact that
the value stored in the untrusted variable is indeed untainted. This kind of in-
tegrity proof is widely adopted in real applications and this work gives the tools
to reason about its security.

142
M. Centenaro and R. Focardi
Future Work. Hash functions could be used in commitment protocols. Suppose
Anna challenges Bruno to solve a problem and claims she has solved it. To prove
her statement Anna takes the answer and appends it to a random secret nonce,
she then sends the hash of such message to Bruno. When the challenge ﬁnishes
or when Bruno gives up, Anna has to reveal him the secret nonce thus he can
check that the correct answer was sent in the ﬁrst step of the process.
Formally studying this scenario in an information ﬂow setting would be chal-
lenging. Some form of declassiﬁcation would be allowed since at certain point in
time the secret nonce has to be released. When the random will be downgraded
then the digest could not be thought to protect Anna’s answer anymore. Ana-
lyzing the security of this problem will require an interaction of a declassiﬁcation
mechanism, suitable to reason about the when dimension of downgrading [10],
with the solution presented here for the secure usage of hash functions.
To guarantee that small values are never assigned to big variables we have
taken the very conservative approach of forbidding expressions to return a big
secret. In practice, this might be relaxed by adding some data ﬂow analysis
in order to track values derived from big secrets. For example, the xor of two
diﬀerent big secrets might be considered a big secret, but the xor of two equal
big secrets is 0. We intend to investigate this issue more in detail in the next
future.
One of the anonymous reviewer has let us notice a strong similarity between
our notion of memory equivalence, based on patterns, and the notion of static
equivalence in process calculi. Big secrets resemble the notion of bound names
which can be α-converted preserving equality patterns. We leave as a future
work the intriguing comparison between the two formal notions.
Acknowledgements. We would like to thank the anonymous reviewers for their
very helpful comments and suggestions.
References
1. Abadi, M., J¨urjens, J.: Formal eavesdropping and its computational interpretation.
In: Kobayashi, N., Pierce, B.C. (eds.) TACS 2001. LNCS, vol. 2215, pp. 82–94.
Springer, Heidelberg (2001)
2. Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the computa-
tional soundness of formal encryption). JCRYPTOL: Journal of Cryptology 15(2),
103–127 (2002)
3. Boudol, G., Castellani, I.: Noninterference for concurrent programs. In: Orejas, F.,
Spirakis, P.G., van Leeuwen, J. (eds.) ICALP 2001. LNCS, vol. 2076, pp. 382–395.
Springer, Heidelberg (2001)
4. Centenaro, M., Focardi, R.: Match it or die: Proving integrity by equality (2009),
http://www.dsi.unive.it/~mcentena/cf-hash-full.pdf
5. Centenaro, M., Focardi, R., Luccio, F.L., Steel, G.: Type-based analysis of pin
processing apis. In: Backes, M., Ning, P. (eds.) ESORICS 2009. LNCS, vol. 5789,
pp. 53–68. Springer, Heidelberg (2009)
6. Demange, D., Sands, D.: All secrets great and small. In: Castagna, G. (ed.) ESOP
2009. LNCS, vol. 5502, pp. 207–221. Springer, Heidelberg (2009)
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
143
7. Focardi, R., Centenaro, M.: Information ﬂow security of multi-threaded distributed
programs. In: Proceedings of the 3rd ACM SIGPLAN Workshop on Program-
ming Languages and Analysis for Security (PLAS), Tucson, AZ, USA, June 8,
pp. 113–124. ACM Press, New York (2008)
8. Laud, P.: On the computational soundness of cryptographically masked ﬂows. In:
Necula, G.C., Wadler, P. (eds.) 35th Annual ACM SIGPLAN-SIGACT Sympo-
sium on Principles of Programming Languages (POPL), San Francisco, Ca, USA,
January 10-12, pp. 337–348. ACM Press, New York (2008)
9. Menezes, A.J., van Oorschot, P.C., Vanstone, S.A.: Handbook of Applied Cryptog-
raphy. CRC Press, Boca Raton (1997)
10. Sabelfeld, A., Sands, D.: Declassiﬁcation: Dimensions and principles. Journal of
Computer Security 17(5), 517–548 (2009)
11. Smith, G.: A new type system for secure information ﬂow. In: Proceedings of
the 14th IEEE Computer Security Foundations Workshop (CSFW), Cape Breton,
Nova Scotia, June 11-13, pp. 115–125. IEEE, Los Alamitos (2001)
12. Volpano, D.: Secure introduction of one-way functions. In: Proceedings of the 13th
IEEE Computer Security Foundations Workshop (CSFW), Cambridge, England,
July 3-5, pp. 246–254. IEEE, Los Alamitos (2000)
13. Volpano, D., Smith, G.: Eliminating covert ﬂows with minimum typings. In: Pro-
ceedings of the 10th Computer Security Foundations Workshop (CSFW), Rock-
port, Massachusetts, USA, June 10-12, pp. 156–169. IEEE Computer Society, Los
Alamitos (1997)
Appendix
A
Case Studies
In this section the case studies presented in the introduction are shown to type
check and a new example will be introduced. Note that some syntactic sugars
which were given introducing the examples has been removed here in order to
show fully typed codes.
A simpliﬁed su command. The ﬁrst example is a simpliﬁed version of the su
Unix utility. Let root shell be a command which requires an high-integrity level
to be computed, i.e., Δ ⊢root shell : (ℓCH, t, f). The user entered password
t pwd will be deemed to be secret and low-integrity, i.e., Δ(t pwd) = PHbL.
root passwd, instead, stores the digest of the administrator password and it will
be a high-integrity data since it is supposed to be stored in a write-protected ﬁle,
let Δ(root passwd) = DℓCH. Note that the array notation have been replaced
by a single variable, this does not aﬀect the aim of the example which is proving
the security of the Unix implementation of the password-based authentication
mechanism.
trial := hash(t_pwd);
if (trial = root_passwd) then
root_shell;
else
FAIL;

144
M. Centenaro and R. Focardi
The password is supposed to be strong thus it has been typed as a big secret.
If such an assumption is removed, the code does not type, indeed the above
program could be used to mount a brute-force attack on the password. The type
system prevents such fact by requiring that the conﬁdentiality level of the guard
is at most a big secret in rule (int-test).
The conﬁdentiality level of root passwd could be either setted to L or Hb.
This models the fact that having strong passwords, they could be safely stored
in a public location.
Let Δ(trial) = DHbL, the expression hash(pwd) is typed DHbL by rule (hash-
b) and the ﬁrst assignment is then typed (HbL, LH, ↓) by (assign). The if branch
is typed (HbH, HbL ⊔t, ↑) by (int-test): if ℓC = L by sub-typing root passwd
will be typed DHbH while if ℓC = Hb nothing special is needed. The sequential
composition of the two commands is then typed by (seq-1), indeed LH ⊑HbL.
Software Distribution. A software company distributes an application using
diﬀerent mirrors on the Internet. Having downloaded the program from one of
the mirrors, a user will install the given binary only if its digest matches the one
of the original application provided by the software company.
if (hash(my_blob.bin) = swdigest) then
trusted_blob.bin := my_blob.bin;
install := 1;
else
FAIL;
Let my blob.bin be the variable storing the downloaded binary, it is a low-
integrity public variable, i.e., Δ(my blob.bin) = PLL. The trusted digest given by
the software company is stored in the swdigest variable which is a high-integrity
one (Δ(swdigest) = DLH). The installation of the application is simulated by
ﬁrst saving the low-integrity binary in the trusted location trusted blob.bin
(Δ(trusted blob.bin) = PLH) and then by assigning 1 to the high-integrity
variable install (Δ(install) = PLH).
The if branch types (LH, LL, ↑) by (int-hash), indeed all the requirements on
variables are satisﬁed by letting ℓC = L in the typing rule and, the assignment
to install types (LH, LH, ↓).
A new case study is now introduced, it shows a program which let a system
administrator to manage the password ﬁle.
A simpliﬁed passwd. This example presents a password update utility. It is a
simpliﬁed version of the passwd Unix command where we require that only the
administrator can perform such a task. This is due to the fact that the integrity
of the password ﬁle must be preserved.
Three parameters are expected: the administrator password and the user old
and new passwords.
root_trial := hash(t_root);
www.ebook3000.com

Match It or Die: Proving Integrity by Equality
145
user_trial := hash(old);
if (root_trial = root_passwd) then
if(user_trial = user_passwd) then
user_passwd := hash(new);
else
FAIL;
else
FAIL;
Variable root passwd stores the digest of the root password while user passwd
the hash of the user one. These model, as in the ﬁrst example, the needed portions
of the password ﬁle (Δ(root passwd) = DHbH and Δ(user passwd) = DHbH).
The typed root password t root is low-integrity, i.e., Δ(t root) = PHbL as well
as the root trial variable used to store its digest (Δ(root trial) = DHbL).
Similarly, Δ(old) = PHbL and Δ(user trial) = DHbL. The new variable which
stores the new user password must be regarded as high-integrity (Δ(new) =
PHbH). This time the user input will be considered trusted since the intended
user will be authenticated and only in that case the new password will be used.
This is the only way to make the hash operator to type at a high-integrity level
when storing the digest of the new password to user passwd. In fact, there is
no way to prove the integrity of a fresh new password by equality test.
The ﬁrst two assignments type (HbL, LH, ↓) by (assign). The innermost if
branch types (HbH, HbL, ↑) by (int-test): variable user passwd types DHbH and
user trial DHbL, the assignment to user passwd types (HbH, LH, ↓) by (assign)
and the hash expression is typed by DLH by (hash-b) and promoted by subtyp-
ing to DHbH. In a similar way the main if branch is again typed by (int-test)
obtaining (HbH, HbL, ↑).
The whole program is thus typed (HbH, HbL, ↑) by (seq-1).

Towards Automatic Analysis of
Election Veriﬁability Properties⋆,⋆⋆
Ben Smyth1,2, Mark Ryan1, Steve Kremer3, and Mounira Kourjieh1,4
1 School of Computer Science, University of Birmingham, UK
2 ´Ecole Normale Sup´erieure & CNRS & INRIA, France
3 LSV, ENS Cachan & CNRS & INRIA, France
4 Universit´e de Toulouse, France
research@bensmyth.com, M.D.Ryan@cs.bham.ac.uk,
kremer@lsv.ens-cachan.fr, kourjieh@irit.fr
Abstract. We present a symbolic deﬁnition that captures some cases
of election veriﬁability for electronic voting protocols. Our deﬁnition is
given in terms of reachability assertions in the applied pi calculus and
is amenable to automated reasoning using the software tool ProVerif.
The deﬁnition distinguishes three aspects of veriﬁability, which we call
individual, universal, and eligibility veriﬁability. We demonstrate the ap-
plicability of our formalism by analysing the protocols due to Fujioka,
Okamoto & Ohta and a variant of the one by Juels, Catalano & Jakob-
sson (implemented as Civitas by Clarkson, Chong & Myers).
Keywords: Electronic voting protocols, election veriﬁability, applied pi
calculus, ProVerif, automated reasoning.
1
Introduction
Electronic voting systems are being introduced, or trialled, in several countries
to provide more eﬃcient voting procedures with an increased level of security.
However, current deployment has shown that the security beneﬁts are very hard
to realise [10,21,9,25]. Those systems rely on the trustworthiness of the hardware
and software that is used to collect, tally, and count the votes, and on the
individuals that manage those systems. In practice, it is very hard to establish
the required level of trust.
The concept of election veriﬁability that has emerged in the academic litera-
ture [16,19,23] aims to address this problem. It signiﬁcantly reduces the necessity
to trust electronic systems, by allowing voters and election observers to verify
independently that votes have been recorded, tallied and counted correctly. To
⋆A preliminary version of this work was presented at the WISSec’09 workshop.
⋆⋆This
work
has
been
partly
supported
by
the
EPSRC
projects
UbiVal
(EP/D076625/2), Trustworthy Voting Systems (EP/G02684X/1) and Verifying In-
teroperability Requirements in Pervasive Systems (EP/F033540/1); the ANR SeSur
AVOT´E project; the Direction G´en´erale pour l’Armement (DGA); and the Univer-
sity of Birmingham’s Blue BEAR cluster.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 146–163, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
147
emphasise a voter’s ability to verify the results of the entire election process, it
is sometimes called end-to-end veriﬁability [22].
We present a preliminary deﬁnition of election veriﬁability in a formal and
general setting, and we analyse voting protocols from the literature. We work
in the applied pi calculus [2], and we use the ProVerif software tool [8] to au-
tomate veriﬁcation. (The calculus and the tool have already been successful in
analysing other properties of electronic voting systems [14,4].) Our approach
puts signiﬁcant emphasis on the automatic analysis of the veriﬁability property,
using ProVerif. To that end, we introduce a number of encodings that make
ProVerif work more eﬃciently, which are of independent interest.
Election veriﬁability allows voters and observers to verify that the election
outcome corresponds to the votes legitimately cast. We distinguish three aspects
of veriﬁability:
Individual veriﬁability: a voter can check that her own ballot is included in
the bulletin board.
Universal veriﬁability: anyone can check that the election outcome corre-
sponds to the ballots.
Eligibility veriﬁability: anyone can check that each vote in the election out-
come was cast by a registered voter and there is at most one vote per voter.
(Note that some authors use the term “universal veriﬁability” to refer to the
conjunction of what we call “universal veriﬁability” and “eligibility veriﬁabil-
ity.” This distinction is made for compatibility with protocols which do not
oﬀer eligibility veriﬁability.) These three aspects of veriﬁability are related to
the following correctness properties [4], deﬁned with respect to honest protocol
executions:
Inalterability: no one can change a voter’s vote.
Declared result: the election outcome is the correct sum of the votes cast.
Eligibility: only registered voters can vote and at most once.
Election veriﬁability properties are diﬀerent from correctness properties, since
they assert that voters and observers can check that the correctness properties
hold, even when administrators deviate from the protocol (that is, do not perform
an honest execution).
We deﬁne election veriﬁability as a family of three tests, corresponding to the
three aspects identiﬁed above. The individual veriﬁability test is performed by
each voter. The other two tests can be conducted by any observer.
1.1
Contribution
We present a deﬁnition of election veriﬁability which captures the three desired
aspects: individual veriﬁability, universal veriﬁability and eligibility veriﬁability.
The deﬁnition is a suﬃcient condition for election veriﬁability, but it is not be
necessary; that is, there are some protocols which oﬀer veriﬁability but are not
captured by our deﬁnition.

148
B. Smyth et al.
We formalise our deﬁnition as reachability assertions in the applied pi calculus.
This makes the deﬁnition amenable to automated reasoning. In order to make
ProVerif work better with our deﬁnitions, we introduce a number of encodings
and optimisations.
We demonstrate the applicability of our deﬁnition by analysing three proto-
cols. The ﬁrst protocol is a simple illustrative protocol which is trivially ver-
iﬁable because voters digitally sign their ballot. (Note that this protocol does
not achieve other properties such as privacy). We then analyse the protocol by
Fujioka et al. [16], and a variant of the one by Juels et al. [19]; the latter has
been implemented as Civitas [13,12].
1.2
Related Work
Juels, Catalano & Jakobson [18,19] present the ﬁrst deﬁnition of universal veriﬁ-
ability in the provable security model. Their deﬁnition assumes voting protocols
produce signature proofs of knowledge demonstrating the correctness of tallying.
Automated analysis is not discussed.
Universal veriﬁability was also studied by Chevallier-Mames et al. [11] with
the aim of showing an incompatibility result: protocols cannot satisfy veriﬁability
and vote privacy in an unconditional way (without relying on computational
assumptions). To see this, note that they require functions f and f ′ such that
for any bulletin board BB and list of eligible voters L the function f(BB, L)
returns the list of actual voters and f ′(BB, L) returns the election outcome (see
Deﬁnition 1 of [11]). From these functions one could consider any single bulletin
board entry b and compute f({b}, L), f ′({b}, L) to reveal a voter and her vote. As
witnessed by [18,19], weaker versions of veriﬁability and vote privacy properties
can hold simultaneously. Our deﬁnitions do not reﬂect such an incompatibility
with vote-privacy and permit a large class of electronic voting protocols claiming
to provide veriﬁability to be considered.
Baskar, Ramanujan & Suresh [7] and subsequently Talbi et al. [24] have for-
malised individual and universal veriﬁability with respect to the FOO [16] elec-
tronic voting protocol. Their deﬁnitions are tightly coupled to that particular
protocol and cannot easily be generalised. Moreover, their deﬁnitions charac-
terise individual executions as veriﬁable or not; whereas such properties should
be considered with respect to every execution (that is, the entire protocol).
During the period between the ARSPA-WITS workshop and the post pro-
ceedings we have taken the opportunity to revise the deﬁnitions presented here
to cater for a larger class of electronic voting protocols. A preliminary version
of this new work appears in [20].
1.3
Outline
Section 2 recalls the applied pi calculus. In Section 3 we introduce a generic def-
inition of veriﬁability which is independent of any particular formal framework.
In Section 4 our deﬁnition is formalised as reachability assertions in the con-
text of the applied pi calculus. The three case studies are analysed in Section 5.
Finally, we conclude and give directions for future work (Section 6).
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
149
2
Applied Pi Calculus
The applied pi calculus [2] is a language for modelling concurrent systems and
their interactions. It is an extension of the pi calculus which was explicitly de-
signed for modelling cryptographic protocols. For this purpose, the applied pi
calculus allows terms to be constructed over a signature rather than just names.
This term algebra can be used to model cryptographic primitives.
2.1
Syntax
The calculus assumes an inﬁnite set of names a, b, c, k, m, n, s, t, r, . . ., an inﬁ-
nite set of variables v, x, y, z, . . . and a ﬁnite signature Σ, that is, a ﬁnite set
of function symbols each with an associated arity. A function symbol of ar-
ity 0 is a constant. We use metavariables u, w to range over both names and
variables. Terms F, L, M, N, T, U, V are built by applying function symbols to
names, variables and other terms. Tuples u1, . . . , ul and M1, . . . , Ml are occa-
sionally abbreviated ˜u and ˜
M. We write {M1/x1, . . . , Ml/xl} for substitutions
that replace x1, . . . , xl with M1, . . . , Ml. The applied pi calculus relies on a sim-
ple type system. Terms can be of sort Channel for channel names or Base for
the payload sent out on these channels. Function symbols can only be applied
to, and return, terms of sort Base. A term is ground when it does not contain
variables. The grammar for processes is shown in Figure 1 where u is either
a name or variable of channel sort. Plain processes are standard. As usual we
abbreviate conditionals as if M = N then P, when Q is the null process; and
similarly we may omit the process P in message input and output when P is 0.
Extended processes introduce active substitutions which generalise the classical
let construct: the process ν x.({M/x} | P) corresponds exactly to the process
let x = M in P. As usual names and variables have scopes which are delimited
by restrictions and by inputs. All substitutions are assumed to be cycle-free.
The sets of free and bound names, respectively variables, in process A are
denoted by fn(A), bn(A), fv(A), bv(A). We also write fn(M), fv(M) for the
names, respectively variables, in term M. An extended process A is closed if
it has no free variables. A context C[ ] is an extended process with a hole. We
obtain C[A] as the result of ﬁlling C[ ]’s hole with A. An evaluation context
P, Q, R ::=
processes
0
null process
P | Q
parallel
!P
replication
ν n.P
name restriction
u(x).P
message input
u⟨M⟩.P
message output
if M = N then P else Q
conditional
A, B, C ::= extended processes
P
plain process
A | B
parallel composition
ν n.A
name restriction
ν x.A
variable restriction
{M/x}
active substitution
Fig. 1. Applied pi calculus grammar

150
B. Smyth et al.
is a context whose hole is not under a replication, a conditional, an input, or
an output.
The signature Σ is equipped with an equational theory E, that is a ﬁnite
set of equations of the form M = N. We deﬁne =E as the smallest equivalence
relation on terms, that contains E and is closed under application of function
symbols, substitution of terms for variables and bijective renaming of names.
We introduce linear processes as a subset of plain processes generated by the
grammar
P ::= 0 | ν n.P | c(x).P | c⟨M⟩.P
| if M = N then P else 0
(P ̸= 0)
| if M = N then 0 else P
(P ̸= 0)
Linear processes can be sequentially composed in a natural way. Let P be a
linear processes and Q a plain process. We deﬁne the plain process P ◦Q to be
Q if P = 0 and otherwise by replacing the unique occurrence of “.0” in P by
“.Q”. (Note that “.0” does not occur in “else 0”. ) Moreover, we note that if P
and Q are both linear processes then P ◦Q is also a linear process.
2.2
Semantics
We now deﬁne the operational semantics of the applied pi calculus by the means
of two relations: structural equivalence and internal reductions. Structural equiv-
alence (≡) is the smallest equivalence relation closed under α-conversion of both
bound names and variables and application of evaluation contexts such that:
Par-0
A | 0 ≡A
Par-A
A | (B | C) ≡(A | B) | C
Par-C
A | B ≡B | A
New-0
νn.0 ≡0
New-C
νu.νw.A ≡νw.νu.A
Repl
!P ≡P |!P
Rewrite
{M/x} ≡{N/x}
if M =E N
Alias
νx.{M/x} ≡0
Subst
{M/x} | A ≡{M/x} | A{M/x}
New-Par
A | νu.B ≡νu.(A | B)
if u ̸∈fn(A) ∪fv(A)
Internal reduction (−→) is the smallest relation closed under structural equiva-
lence, application of evaluation contexts and such that
COMM
c⟨x⟩.P | c(x).Q −→P | Q
THEN
if M = M then P else Q −→P
ELSE
if M = N then P else Q −→Q
if M, N ground and M ̸=E N
2.3
Notational Conventions
By convention we assume a signature contains: the binary function pair; the
unary functions fst, snd; and the constant ε. Moreover, the associated equa-
tional theory includes the equations fst(pair(x, y)) = x and snd(pair(x, y)) = y.
Arbitrary length tuples can be constructed as pair(x1, pair(x2, . . . , pair(xn, ε)));
which, for convenience, we abbreviate (x1, . . . , xn). We also write Mj for
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
151
fst(snd(snd(. . . snd(M)))) where j ≥1 and there are j −1 occurrences of snd.
We also write sometimes c(x1, . . . , xn) (or c(˜x)) for the sequence of inputs
c(x1). . . . .c(xn).
To aid readability, we also allow boolean combinations of M = N in condi-
tionals in the “if-then-else” process, and in the output of terms. If φ is such a
combination, then the output c⟨φ⟩.P is an abbreviation for “if φ then c⟨true⟩.P”.
Boolean combinations of conditionals in “if” statements are handled as follows.
First, the boolean combination is written using only the boolean connectives
∧and ¬. Then ∧is encoded using nested “if” processes; and negation (¬) is
encoded by swapping “then” and “else” branches.
2.4
Events and Reachability Assertions
For the purpose of protocol analysis processes are annotated with events which
mark important actions performed by the protocol which do not otherwise aﬀect
behaviour. We adopt the formalism presented by Abadi, Blanchet & Fournet [1]
to capture events. Events are modelled as outputs f⟨M⟩where f ∈F is an
“event channel”: a name in a particular set F disjoint from the set of ordinary
channels a, b, c. Message input on event channels must use “event variables” e, e′.
We assume that protocols should be executed in the presence of a so-called
Dolev-Yao adversary [15]. The adversary is permitted to input f(e) on event
channels but is forbidden from using the bound event variable e in any other
manner. The former condition prevents processes blocking, whereas the latter en-
sures the adversary’s knowledge cannot be extended by the occurrence of events.
Deﬁnition 1 (Adversary). An adversary is a closed process such that, any
event channel f ∈F and event variable e, only occur in inputs of the form f(e).
A reachability assertion is speciﬁed as an event f⟨˜X⟩where ˜X is a tuple of vari-
ables and constants. A process satisﬁes reachability if there exists an adversary
who is able to expose the event (Deﬁnition 2). When such an adversary does not
exist, we say the process satisﬁes the unreachability assertion f⟨˜X⟩.
Deﬁnition 2 (Reachability). The closed process P satisﬁes the reachability
assertion f⟨˜
X⟩where ˜X is a tuple of variables and constants if there exists an
adversary Q such that P | Q −→∗C[f⟨˜
X⟩.P ′] for some evaluation context C and
process P ′.
3
Election Veriﬁability
Election veriﬁability can be formalised with respect to tests ΦIV , ΦUV , ΦEV
corresponding to the three aspects of our formalisation. A protocol is said to
be election-veriﬁable if three such tests exist that satisfy some conditions that
we detail below. Each of the tests ΦIV , ΦUV , ΦEV is a predicate which after
substitutions from the bulletin board and elsewhere evaluates to true or false.
The designers of electronic voting protocols need not explicitly specify these

152
B. Smyth et al.
cryptographic tests since our deﬁnition assumes the existence of tests (perhaps
devised after design) which satisfy our conditions. This extends the applicability
of our methodology whilst also permitting the scrutiny of tests speciﬁed by
protocol designers.
3.1
Overview
Individual veriﬁability. The test ΦIV takes parameters v (a vote), ˜x (a voter’s
knowledge), y (a voter’s public credential) and z (a bulletin board entry). For
ΦIV to be a suitable test it must allow a voter to identify her bulletin board
entry. Formally we require for all votes s, if the voter with public credential
D votes for candidate s, then there exists an execution of the protocol which
produces ˜
M such that some bulletin board entry B satisﬁes:
ΦIV {s/v, ˜
M/˜x, D/y, B/z}
(1)
Moreover, the bulletin board entry should determine the vote; that is, for all
bulletin board entries B, public credentials D, D′ votes s, s′ and tuples ˜
M, ˜
M ′
we have:
ΦIV {s/v, ˜
M/˜x, D/y, B/z} ∧ΦIV {s′/v, ˜
M ′/˜x, D′/y, B/z} ⇒(s = s′)
(2)
This ensures the test will hold for at most one vote. Additionally, individual
veriﬁability requires voters to accept distinct bulletin board entries:
Bulletin board entries are distinct
(3)
This condition requires protocol executions to introduce some freshness (e.g.
randomness).
Universal veriﬁability. This property is encapsulated by the test ΦUV which
takes parameters v (a vote) and z (a bulletin board entry). Given ΦIV , the test
ΦUV is suitable if every bulletin board entry which is accepted by a voter is also
accepted by an observer; and the entry is counted by the observer in the correct
way. The property requires that for all executions of the protocol producing ˜
M
with respect to the voter’s vote s, if there exists a bulletin board entry B and
public credential D such that the voter accepts the bulletin board entry as hers,
then the observer also accepts the entry:
ΦIV {s/v, ˜
M/˜x, D/y, B/z} ⇒ΦUV {s/v, B/z}
(4)
Moreover, the observer counts the vote correctly. That is, for all bulletin board
entries B and votes s, s′ if the test succeeds for s and s′ then they must be votes
for the same candidate:
ΦUV {s/v, B/z} ∧ΦUV {s′/v, B/z} ⇒(s = s′)
(5)
This ensures that an observer may only count a vote in one way.
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
153
We remark that the implication in property (4) is only one way because the
adversary is able to construct ballots which would be accepted by an observer;
however, such a ballot should not be accepted by a voter. This malicious be-
haviour can be detected by eligibility veriﬁability. We also note that this formal-
isation is stronger than what is actually required: here a voter needs to be able
to identify the bulletin board entry corresponding to her vote. This is not the
case in all protocols, for example in the protocol by Juels et al. (described in
Section 5) the connection between the set of bulletin board entries and the elec-
tion outcome is lost by the mix. However, this formalisation has the advantage
of being amenable to automated veriﬁcation.
Eligibility veriﬁability. The property is encoded by the test ΦEV which takes
parameters y (a voter’s public credential) and z (a bulletin board entry). Given
ΦIV , the test ΦEV is considered suitable if it ensures: 1) an observer can attribute
a bulletin board entry to a public credential if and only if the corresponding voter
would accept that entry as hers; and 2) a bulletin board entry can be attributed
to at most one voter. Formally the property requires for all bulletin board entries
B and executions of the protocol producing ˜
M, with respect to the voter’s vote s
and public voter credential D, the voter accepts B as hers iﬀthe bulletin board
entry can be attributed to her public credential:
ΦIV {s/v, ˜
M/˜x, D/y, B/z} ⇔ΦEV {D/y, B/z}
(6)
This ensures that if ΦIV succeeds for a voter then she is assured that her vote
is considered eligible by an observer; and if ΦEV succeeds for a public credential
then the corresponding voter must have constructed that bulletin board entry.
The condition relies upon a relationship between the voter’s knowledge ˜
M and
the voter’s public credential D . As for universal veriﬁability this condition may
be too strong as some veriﬁable protocols do not allow a voter to link a bulletin
board entry to her credentials.
The second condition requires that the test must uniquely determine who
cast a bulletin board entry (hence we can ensure that at most one ballot per
registered credential may appear on the bulletin board); that is, for all bulletin
board entries B and public voter credentials D, D′ if the test succeeds for D and
D′ then the credentials are equivalent:
ΦEV {D/y, B/z} ∧ΦEV {D′/y, B/z} ⇒(D = D′)
(7)
This property enables re-vote elimination. The concept of re-voting is particu-
larly useful since it is used by some protocols to provide coercion resistance. In
such protocols re-vote elimination is performed with respect to a publicly de-
ﬁned policy to ensure voters vote at most once. Finally, eligibility veriﬁability
also requires that voters must have unique public credentials:
Public voter credentials are distinct
(8)
3.2
Verifying an Election
Voters and observers check the outcome of the election by performing the tests
ΦIV , ΦUV and ΦEV on the bulletin board and data derived from elsewhere (for

154
B. Smyth et al.
example, learnt by the voter during an execution of the protocol). For individual
veriﬁability, each voter should be in possession of her vote s, public credential D
and ˜
M representing the knowledge learnt during an execution of the protocol,
such that there exists j ∈[1, | ˜B|] satisfying the test ΦIV {s/v, ˜
M/˜x, D/y, Bj/z}.
For universal veriﬁability, the bulletin board must be such that the observer can
map the ballots to the votes appearing in the election outcome. That is, there
exists a bijective function f : {1, . . ., | ˜B|} →{1, . . . , | ˜B|}, such that for all j ∈
[1, | ˜B|], the test ΦUV {sj/v, Bf(j)/z} holds. Similarly, for eligibility veriﬁability to
hold an observer must be able to map each public credential to a bulletin board
entry. That is, there exists a bijective function g : {1, . . . , | ˜B|} →{1, . . . , | ˜B|}
such that for all j ∈[1, | ˜B|] the test ΦEV {Dj/y, Bg(j)/z} holds.
Compatibility with privacy. Some electronic voting protocols utilise mixnets to
obtain privacy. For simplicity we omit formalising the security of mixnets and
hence omit modelling the mix. This clearly violates privacy properties. However,
since mixnets are veriﬁable, privacy and election veriﬁability properties may
coexist in practice.
4
Election Veriﬁability in the Applied Pi Calculus
A voting protocol is captured by a voter process V and a process K modelling
administrators whom are required to be honest for the purpose of election veriﬁ-
ability. Dishonest administrators need not be explicitly modelled since they are
part of the adversarial environment. The process K is assumed to publish public
voter credentials; and is commonly responsible for the distribution of voter keys.
Channels ˜a are assumed to be private and appear in both V, K. In addition, we
consider a context A which performs setup duties; for example, the instantiation
of keys for honest administrators. Dishonest administrator keys are modelled as
free names. Deﬁnition 3 formalises a voting process speciﬁcation accordingly. The
deﬁnition allows us to analyse election veriﬁability with respect to an unbounded
number of voters and arbitrarily many candidates.
Deﬁnition 3 (Voting process speciﬁcation). A voting process speciﬁcation
is a tuple ⟨A, V, K[c⟨D⟩], ˜a⟩where V is a linear process, A and K are contexts
such that V, A, K do not contain any occurrence of event channels and event
variables. The term D models public voter credentials. The variable v ∈fv(V )
refers to the value of the vote, v ̸∈(bv(A) ∪bv(V )), c ̸∈(˜a ∪bn(A[V | K])) and
(fv(V )\{v} ∪fv(K)) ⊆bv(A).
We also suppose that the votes are generated by a special vote generation process
G which “chooses” the vote for a given voter and sends it to the voter on a
channel name b. A typical vote generation process would be
G &= !ν s.((!b⟨s⟩) | c⟨s⟩)
This process models the generation of all possible votes. Intuitively, the channel
b allows the voter to select her vote v and the nested replication allows several
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
155
voters to choose the same vote s, while other voters may vote diﬀerently. Each
vote s is also made available to the environment by publishing it on the channel
c. Unless speciﬁed diﬀerently we suppose in the remaining that G is deﬁned as
above. Given a voting process speciﬁcation ⟨A, V, K[c⟨D⟩], ˜a⟩and a vote gen-
eration process G we can build the process modelling the voting protocol VP
as follows:
VP &= ν b.(A[!ν ˜a.((b(v).V ) | K[c⟨D⟩])] | G)
where b ̸∈(˜a ∪fn(A[V | K]) ∪bn(A)). Observe that since the key generation
process is under replication it is possible to construct unique credentials for each
voter.
The formalisation of election veriﬁability (Deﬁnition 5) can naturally be ex-
pressed as reachability assertions [26,8] associated with the properties 1-8 of §3.1
which relate to tests ΦIV , ΦUV , ΦEV . First the tests must be incorporated into
an augmented voting process (Deﬁnition 4).
Deﬁnition 4 (Augmented voting process). Given a voting process speciﬁ-
cation ⟨A, V, K[c⟨D⟩], ˜a⟩, a vote generation process G and tests ΦIV , ΦUV , ΦEV
the augmented voting process is deﬁned as VP+ = ν b.(A[!ν ˜a, b′.(&V | &K)] | G |
P) | Q | R where
&V
= b(v).V ◦c(z).b′(y).(pass⟨(ΦIV , z)⟩| fail⟨ψ⟩)
&K = K[b′⟨D⟩| cred⟨D⟩| c⟨D⟩]
P
= b(v′).b(v′′).c(˜x′).c(˜x′′).c(y′).c(y′′).c(z′).fail⟨φ′ ∨φ′′ ∨φ′′′⟩
Q
= pass(e).pass(e′).fail⟨e1 ∧e′
1 ∧(e2 = e′
2)⟩
R
= cred(e).cred(e′).fail⟨e = e′⟩
ψ
= (ΦIV ∧¬ ΦUV ) ∨(ΦIV ∧¬ ΦEV ) ∨(¬ ΦIV ∧ΦEV )
φ′ = ΦIV {v′/v, ˜x′/˜x, y′/y, z′/z} ∧ΦIV {v′′/v, ˜x′′/˜x, y′′/y, z′/z} ∧¬(v′ = v′′)
φ′′ = ΦUV {v′/v, z′/z} ∧ΦUV {v′′/v, z′/z} ∧¬(v′ = v′′)
φ′′′ = ΦEV {y′/y, z′/z} ∧ΦEV {y′′/y, z′/z} ∧¬(y′ =E y′′)
such that fail, pass, cred are event channels, ˜x = (bv(V )∩(fv(Riv)\{z})), and
b, b′ are fresh, that is, b, b′ ̸∈(˜a ∪fn(A[V | K]) ∪bn(A[V | K])).
The augmented voting process extends V to bind the voter’s intended vote v,
assigns the voter’s public credential to y and introduces a claimed bulletin board
entry z. As in the non-augmented process, the process !ν s.((!b⟨s⟩) | c⟨s⟩) pro-
duces candidates for whom the voters are allowed to vote. The number of can-
didates and for whom each voter casts her vote is controlled by the adversarial
environment. The events capture the desired reachability assertions. That is,
reachability of pass⟨(ΦIV , z)⟩captures propositional property 1 of §3.1; unreach-
ability of fail⟨ψ⟩models propositional properties 4 and 6 of §3.1; and unreach-
ability of fail⟨φ′ ∨φ′′ ∨φ′′′⟩denotes properties 2, 5 and 7 of §3.1. The universal
quantiﬁers of the propositional properties 2,4-7 are captured by allowing the ad-
versary to input the required parameters. Process Q exploits communication on
event channel pass to detect the scenario in which two voters accept the same

156
B. Smyth et al.
bulletin board entry, hence capturing property 3 of §3.1, by the unreachability
of the event fail⟨e1 ∧e′
1 ∧(e2 = e′
2)⟩where e1 and e′
1 capture whether two
distinct voters accept the bulletin boards e2 and e′
2. Similarly, communication
on the event channel cred is used to detect the situation in which two voters
are assigned the same public credential, thus modelling property 8 of §3.1 by the
unreachability of fail⟨e = e′⟩.
Deﬁnition 5 (Election veriﬁability). A voting process speciﬁcation ⟨A, V,
K[c⟨D⟩], ˜a⟩satisﬁes election veriﬁability if there exists tests ΦIV , ΦUV , ΦEV such
that the augmented voting process VP+ and tests satisfy the following conditions:
1. VP+ satisﬁes the unreachability assertion: fail⟨true⟩.
2. VP+ satisﬁes the reachability assertion: pass⟨(true, x)⟩.
3. The tests ΦIV , ΦUV , ΦEV satisfy the following constraints:
– fv(ΦIV ) ⊆bv(V ) ∪{v, y, z}
– fv(ΦUV ) ⊆{v, z}
– fv(ΦEV ) ⊆{y, z}
– (fn(ΦIV ) ∪fn(ΦUV ) ∪fn(ΦEV )) ∩bn(VP+) = ∅
We remark that the restriction fn(ΦIV ) ∩bn(VP+) = ∅does not lose gener-
ality, since restricted names “ν n” can be referenced by variables as follows:
“ν n.let x n = n in”.
Many protocols in literature do not provide eligibility veriﬁability. We there-
fore deﬁne a weakly augmented voting process and weak election veriﬁability
to capture only individual and universal veriﬁability. A weakly augmented vot-
ing process is deﬁned as an augmented voting process but with R = 0, ψ =
(ΦIV ∧¬ ΦUV ) and replacing fail⟨φ′ ∨φ′′ ∨φ′′′⟩with fail⟨φ′ ∨φ′′⟩. The def-
inition of weak election veriﬁability is obtained by omitting conditions on ΦEV
from Deﬁnition 5.
5
Case Studies
We demonstrate the applicability of our methodology by analysing electronic
voting protocols from literature. The ProVerif tool [8] has been used for automa-
tion and our input scripts are available online1. ProVerif’s ability to reason with
reachability assertions is sound (when no trace is found the protocol is guaran-
teed to satisfy the unreachability assertion) but not complete (false reachability
traces may be found). As a consequence reachability traces output by ProVerif
for Condition 2 of Deﬁnition 5 must be checked by hand. In this paper all such
traces correspond to valid reachable states.
5.1
Postal Ballot Protocol
Description. Consider an electronic variant of a “postal ballot” (or “mail-in
ballot”) protocol whereby a voter receives her private signing key skV from
1 http://www.bensmyth.com/publications/10arspa/
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
157
a keying authority, constructs her signed ballot and sends it to the bulletin
board. The keying authority is also responsible for publishing the voter’s public
veriﬁcation key pk(skV ) which will serve as her public credential. The protocol
does not satisfy all of the desirable electronic voting properties; but it should
certainly provide election veriﬁability.
Formalisation in applied pi. The corresponding voting process speciﬁcation is
given by ⟨A, V, K[c⟨D⟩], (a)⟩where
A &=
V &= a(x).c⟨(v, sign(v, x))⟩
K[c⟨D⟩] &= ν skV .(a⟨skV ⟩| c⟨D⟩)
D &= pk(skV )
We model digital signatures (without message recovery) by the equation
checksign(sign(x, y), x, pk(y)) = true
The resulting (non-augmented) voting process is then deﬁned as
VPpostal &= νb.

!νa. (b(v).a(x).c⟨(v, sign(v, x))⟩
| ν skV .(a⟨skV ⟩| c⟨pk(skV )⟩))
| !νs.(!b⟨s⟩| c⟨s⟩)

Analysis. The augmented voting process VP+ can be derived with respect to
tests:
ΦIV
&= z =E (sign(v, x), v, pk(x))
ΦUV &= checksign(z1, z2, z3) =E true ∧v =E z2
ΦEV &= checksign(z1, z2, z3) =E true ∧y =E z3
ProVerif is able to automatically verify the protocol satisﬁes election veriﬁability.
5.2
Protocol Due to Fujioka, Okamoto and Ohta
Description. The FOO protocol [16] involves voters, a registrar and a tallier. The
protocol relies on a commitment scheme and blind signatures which we model
by the following equational theory.
checksign(sign(x, y), x, pk(y)) = true
unblind(sign(blind(x, y), z), y) = sign(x, z)
unblind(blind(x, y), y) = x
open(commit(x, y), y) = x
The voter ﬁrst computes her ballot as a commitment to her vote m′ = commit(v, r)
and sends the signed blinded ballot sign(blind(m′, r′), skV ) to the registrar.
The registrar checks the signature belongs to an eligible voter and returns
sign(blind(m′, r′), skR) the blind signed ballot. The voter veriﬁes that this
input (variable bsb in the process V below) corresponds to the registrar’s sig-
nature and unblinds the message to recover her ballot signed by the registrar
m = sign(m′, skR). The voter then posts her signed ballot to the bulletin board.

158
B. Smyth et al.
Once all votes have been cast the tallier veriﬁes all the entries and appends an
identiﬁer l to each valid record2. The voter then checks the bulletin board for her
entry, the triple (l, m′, m), (modelled in V below by the input in variable bbe)
and appends the commitment factor r. Finally, using r the tallier opens all of
the ballots and announces the declared outcome. The protocol claims to provide
individual and universal veriﬁability but does not consider eligibility veriﬁability.
Formalisation in applied pi. The voting process speciﬁcation is ⟨, V, K[c⟨pk(x)⟩],
(a)⟩where K = c(x).(a⟨x⟩| ) and V is deﬁned as follows:
V = ν r.let x r = r in
ν r′.let x r′ = r′ in
a(x).
let m′ = commit(v, r) in
c⟨(pk(x), blind(m′, r′), sign(blind(m′, r′), x))⟩.
c(bsb).
if checksign(bsb, blind(m′, r′), pk(skR)) = true then
let m = unblind(bsb, r′) in
c⟨(m′, m)⟩.
c(bbe).
if m′ = bbe2 ∧m = bbe3 then
c⟨(bbe1, r)⟩
Analysis. Let tests ΦIV , ΦUV be deﬁned as follows:
ΦIV
&= z =E ⟨bbe1, commit(v, x r), unblind(bsb, x r′), x r, v⟩
∧checksign(z3, z2, pk(skR)) =E true
ΦUV &= z2 =E commit(z5, z4) ∧checksign(z3, z2, pk(skR)) =E true ∧z5 =E v
ProVerif enables automatic veriﬁcation of election veriﬁability with respect to
weak election veriﬁability.
5.3
Protocol Due to Juels, Catalano and Jakobsson and Clarkson,
Chong and Myers
Description. The protocol due to Juels, Catalano & Jakobsson [19], which has
been implemented by Clarkson, Chong & Myers as Civitas [13,12], involves vot-
ers, registrars and talliers.
The registrars announce the candidate list ˜s = (s1, . . . , sl) and provide an
anonymous credential k to each legitimate voter. For each such credential an
encrypted version penc(k, r′′, pk(skT )) is published on the bulletin board.
Each voter selects her vote s ∈˜s and computes the ciphertexts M = penc(s, r,
pk(skT )) and M ′ = penc(k, r′, pk(skT )). The ﬁrst ciphertext contains her vote
and the second contains her credential. In addition, the voter constructs a sig-
nature proof of knowledge, that is, a non-interactive zero-knowledge proof of
2 The value l is used for practical purposes only; it does not aﬀect security.
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
159
knowledge, demonstrating the correct construction of her ciphertexts and that
she has chosen a valid candidate; that is, s ∈˜s. The voter posts her ciphertexts
and signature proof of knowledge to the bulletin board.
After some predeﬁned deadline the outcome is computed as follows. First, the
talliers discard any entries for which signature proofs of knowledge do not hold
and eliminate re-votes by performing pairwise plaintext equivalence tests3 on all
the ciphertexts containing voting credentials posted by the voter. Re-vote elimi-
nation is performed in a veriﬁable manner with respect to some publicly
deﬁned policy, e.g. keeping the last vote. Then, the talliers perform a veriﬁable
re-encryption mix on the votes and credentials, keeping the link between each
such pair. The aim of this mix is that the voter herself cannot trace her vote any-
more which allows the protocol to achieve coercion-resistance. After the mix all
invalid credentials are discarded using plaintext equivalence tests between the en-
tries posted to the bulletin board by the mix and those published by the registrar.
Finally, the talliers perform a veriﬁable decryption and publish the result.
Formalisation in applied pi. We model a simpliﬁed version of the protocol
described above which omits the mix. The resulting protocol is not coercion-
resistant, but still provides anonymity. This change is made because our deﬁ-
nition is too strong to hold on the complete protocol. We discuss future work
regarding a more general deﬁnition in our conclusion.
The formalisation of signature proofs of knowledge in the applied pi calculus
that we adopt is due to Backes et al. [5,6]. A signature proof of knowledge is
a term spki,j( ˜U, ˜V , F) where ˜U = (U1, . . . , Ui) denotes the witness (or private
component), ˜V = (V1, . . . , Vj) deﬁnes the public parameters and F is a formula
over those terms. More precisely F is a term without names or variables, but
includes distinguished constants αk, βl where k, l ∈N. The constants αk, βl in
F denote placeholders for the terms Uk ∈˜U, Vl ∈˜V used within a signature of
knowledge spki,j( ˜U, ˜V , F). For example, the signature proof of knowledge used
by voters in the Juels, Catalano & Jakobsson voting protocol [19] demonstrates
possession of a vote s, credential k and randomisation factors r, r′ such that
M = penc(s, r, pk(skT )), M ′ = penc(k, r′, pk(skT )) and s ∈˜s; that is, the proof
shows the ciphertexts are correctly formed and s is a valid candidate. This can
be captured by spk4,3+l((s, r, k, r′), (M, M ′, pk(skT ), s1, . . . , sl), F) where F is
deﬁned as β1 = penc(α1, α2, β3) ∧β2 = penc(α3, α4, β3) ∧(α1 = β4 ∨. . . ∨α1 =
β4+l). A term spki,j( ˜U, ˜V , F) represents a valid signature if the term obtained
by substituting Uk, Vl for the corresponding αk, βl evaluates to true. Veriﬁcation
of such a statement is modelled by the function veri,j. The equational theory
includes the following equations over all i, j ∈N, tuples ˜x = (x1, . . . , xi), ˜y =
(y1, . . . , yj) and formula F which is a ground term over Σ∪{αk, βl | k ≤i, l ≤j}
without any names:
3 A plaintext equivalence test [17] is a cryptographic predicate which allows the com-
parison of two ciphertexts. The test returns true if the ciphertexts contain the same
plaintext.

160
B. Smyth et al.
publicp(spki,j(˜x, ˜y, F)) = yp where p ∈[1, j]
formula(spki,j(˜x, ˜y, F)) = F
In addition, we deﬁne equations such that veri,j(F, spki,j( ˜U, ˜V , F ′)) =E true if
F =E F ′ and F{U1/α1, . . . , Ui/αi, V1/β1, . . . , Vj/βj} holds where i = | ˜U|, j = | ˜V |
and F, F ′ are ground terms over Σ ∪{αk, βl | k ≤i, l ≤j} without names. We
omit the details of these equations which are similar to [5,6].
The protocol uses a variant of the ElGamal encryption scheme [19]. In addition
to the standard equation dec(penc(x, y, pk(z)), z) = x, this scheme allows the
construction of a dedicated decryption key for a particular ciphertext which is
modelled by the equation:
dec(penc(x, y, pk(z)), commit(penc(x, y, pk(z)), z)) = x
Veriﬁable decryption can then be achieved using the signature proof of knowl-
edge spk1,3((α1), (β1, β2), F′) where F′ is given by β1 = commit(β2, α1). The
proof shows that if β2 = penc(M, N, pkα1) for some terms M, N, then β1 is a
decryption key for β2. Finally, plaintext equivalence tests are modelled by the
equation
pet(penc(x, y, pk(z)), penc(x, y′, pk(z)),
petkey(penc(x, y, pk(z)), penc(x, y′, pk(z)), z)) = true
The voting process speciﬁcation can now be deﬁned as ⟨A, V, K[c⟨D⟩], (a)⟩where
public credential D = penc(k, r′′, pk(skT )) and A, V, K are speciﬁed as follows:
A = ν skT .(c⟨pk(skT )⟩| (!A′) | )
V = ν r.let x r = r in ν r′.let x r′ = r′ in a(x k).
let x s1 = s1 in . . . let x sl = sl in
let x pkT = pk(skT ) in
let M = penc(v, r, pk(skT )) in
let M ′ = penc(x k, r′, pk(skT )) in
let N = spk4,3+l((v, r, x k, r′), (M, M ′, pk(skT ), s1, . . . , sl), F) in
c⟨(M, M ′, N)⟩
K = ν k.ν r′′.(a⟨k⟩| )
A′ = c(y).if ver4,3+l(F, y) = true then
c(z).if pet(z, public2(y), petkey(z, public2(y), skT )) = true then
c⟨spk1,2((skT ), (commit(public1(y), skT ), public1(y)), F′)⟩).
c⟨petkey(z, public2(y), skT )⟩
For simplicity we consider a single registrar K and tallier A′ who are assumed
to be honest. Moreover, we assume the existence of a secure mix protocol and
hence do not model or verify the shuﬄe. The registrar process is standard and
is responsible for publishing the public voter credentials. Process A′ captures
the tallier’s responsibility to provide suitable keys for plaintext equivalence tests
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
161
used for eligibility checking and performing veriﬁable decryption of honestly
constructed ballots.
Analysis. The protocol is dependent on the candidate list and therefore cannot
be veriﬁed with respect to an arbitrary number of candidates. We must therefore
restrict the adversarial environment to consider a ﬁxed number of candidates.
We therefore deﬁne the vote generation process as G &= (!b⟨s1⟩) | . . . | (!b⟨sl⟩)
where s1, . . . , sl are free names representing the candidates for whom voters may
cast their votes. Let the tests ΦIV , ΦUV , ΦEV be deﬁned as follows:
ΦIV
&= φ′ ∧z1 =E spk4,3+l((v, x r, x k, x r′), (M, M ′, x pkT, x s1, . . . , x sl), F)
ΦUV &= φ ∧dec(public2(z2), public1(z2)) =E v
ΦEV &= φ′ ∧ver4,3+l(F, z1) =E true
φ
&= ver1,2(F′, z2) =E true ∧public1(z1) =E public2(z2)
φ′
&= φ ∧pet(y, public2(z1), z3) =E true
where M = penc(v, x r, x pkT ) and M ′ = penc(x k, x r′, x pkT ). The aug-
mented voting process can now be derived with respect to the size of the can-
didate list l. Using ProVerif in association with a PHP script that generates
ProVerif scripts for diﬀerent values of l, the protocol can be successfully veriﬁed
to satisfy election veriﬁability with respect to l ∈[1, 100].
6
Conclusion
This paper presents a preliminary formal deﬁnition of election veriﬁability for
electronic voting protocols. The idea of tests for individual, universal and eligi-
bility veriﬁability (and the associated acceptability conditions) is independent
of any particular formalism. We instantiate this idea in terms of reachability
assertions in the context of the applied pi calculus. The deﬁnition is suitable for
automated reasoning using the ProVerif software tool, which we demonstrate by
providing the code used for our analysis of the protocol by Fujioka, Okamoto
& Ohta [16] and a variant of the one by Juels, Catalano & Jakobsson [19] and
Clarkson, Chong & Myers [13,12].
The deﬁnition is work in progress, because it is currently insuﬃciently general
to take account of protocols that use homomorphic encryption (such as Helios
2.0 [3]). Moreover, it is likely that the ‘pointwise’ nature of the tests ΦUV and
ΦEV is too strong for some protocols; more likely, the observer performs a test
on the whole bulletin board at once, rather than a separate test on each of
its entries. In future work, we intend to generalise our deﬁnitions of universal
veriﬁability and eligibility veriﬁability to work with a greater variety of voting
systems. We also intend to study more carefully the relation between veriﬁabil-
ity and coercion resistance. Finally, we aim to formalise the security of re-vote
elimination policies, present in many realistic systems, which we omitted from
our case studies.

162
B. Smyth et al.
Acknowledgements
We are particularly grateful to Michael Clarkson for careful reading of an earlier
draft, and for his perceptive questions and comments. In addition, we would like
to thank the anonymous WISSec’09 reviewers for helpful remarks.
References
1. Abadi, M., Blanchet, B., Fournet, C.: Just fast keying in the pi calculus. ACM
Transactions on Information and System Security (TISSEC) 10(3), 1–59 (2007)
2. Abadi, M., Fournet, C.: Mobile values, new names, and secure communication.
In: POPL 2001: Proceedings of the 28th ACM Symposium on Principles of Pro-
gramming Languages, pp. 104–115. ACM, New York (2001)
3. Adida, B., de Marneﬀe, O., Pereira, O., Quisquater, J.-J.: Electing a univer-
sity president using open-audit voting: Analysis of real-world use of Helios.
In:
Electronic
Voting
Technology/Workshop
on
Trustworthy
Elections,
EVT/WOTE (2009)
4. Backes, M., Hritcu, C., Maﬀei, M.: Automated veriﬁcation of remote elec-
tronic voting protocols in the applied pi-calculus. In: CSF 2008: Proceedings of
the 21st IEEE Computer Security Foundations Symposium, Washington, USA,
pp. 195–209. IEEE, Los Alamitos (2008)
5. Backes, M., Maﬀei, M., Unruh, D.: Zero-Knowledge in the Applied Pi-calculus and
Automated Veriﬁcation of the Direct Anonymous Attestation Protocol. Cryptology
ePrint Archive: Report 2007/289 (July 2007)
6. Backes, M., Maﬀei, M., Unruh, D.: Zero-Knowledge in the Applied Pi-calculus
and Automated Veriﬁcation of the Direct Anonymous Attestation Protocol.
In: S&P 2008: Proceedings of the 2008 IEEE Symposium on Security and Pri-
vacy, Washington, DC, USA, pp. 202–215. IEEE Computer Society, Los Alamitos
(2008)
7. Baskar, A., Ramanujam, R., Suresh, S.P.: Knowledge-based modelling of voting
protocols. In: TARK 2007: Proceedings of the 11th International Conference on
Theoretical Aspects of Rationality and Knowledge, pp. 62–71. ACM, New York
(2007)
8. Blanchet, B.: Automatic veriﬁcation of correspondences for security protocols.
Journal of Computer Security 17(4), 363–434 (2009)
9. Bowen, D.: Secretary of State Debra Bowen Moves to Strengthen Voter Conﬁ-
dence in Election Security Following Top-to-Bottom Review of Voting Systems.
California Secretary of State (August 2007), press release DB07:042,
http://www.sos.ca.gov/elections/voting_systems/ttbr/db07_042_ttbr_
system_decisions_release.pdf
10. Bundesverfassungsgericht (Germany’s Federal Constitutional Court). Use of
voting computers in 2005 Bundestag election unconstitutional (March 2009),
Press release 19/2009,
http://www.bundesverfassungsgericht.de/en/press/
bvg09-019en.html
11. Chevallier-Mames, B., Fouque, P.-A., Pointcheval, D., Stern, J., Traore, J.: On
Some Incompatible Properties of Voting Schemes. In: WOTE 2006: Proceedings of
the International Association for Voting Systems Sciences Workshop on Trustwor-
thy Elections (2006)
www.ebook3000.com

Towards Automatic Analysis of Election Veriﬁability Properties
163
12. Clarkson, M.R., Chong, S., Myers, A.C.: Civitas: Toward a secure voting system.
Technical Report 2007-2081, Cornell University (May 2007),
http://hdl.handle.net/1813/7875 (Revised, March 2008)
13. Clarkson, M.R., Chong, S., Myers, A.C.: Civitas: Toward a secure voting system.
In: S&P 2008: Proceedings of the 2008 IEEE Symposium on Security and Privacy,
pp. 354–368. IEEE Computer Society, Los Alamitos (2008)
14. Delaune, S., Kremer, S., Ryan, M.D.: Verifying privacy-type properties of electronic
voting protocols. Journal of Computer Security 17(4), 435–487 (2009)
15. Dolev, D., Yao, A.C.: On the security of public key protocols. Information The-
ory 29, 198–208 (1983)
16. Fujioka, A., Okamoto, T., Ohta, K.: A Practical Secret Voting Scheme for Large
Scale Elections. In: ASIACRYPT 1992: Proceedings of the Workshop on the The-
ory and Application of Cryptographic Techniques, London, pp. 244–251. Springer,
Heidelberg (1992)
17. Jakobsson, M., Juels, A.: Mix and match: Secure function evaluation via cipher-
texts. In: Okamoto, T. (ed.) ASIACRYPT 2000. LNCS, vol. 1976, pp. 162–177.
Springer, Heidelberg (2000)
18. Juels, A., Catalano, D., Jakobsson, M.: Coercion-Resistant Electronic Elections.
Cryptology ePrint Archive, Report 2002/165 (2002)
19. Juels, A., Catalano, D., Jakobsson, M.: Coercion-resistant electronic elections.
In: WPES 2005: Proceedings of the 2005 ACM workshop on Privacy in the elec-
tronic society, pp. 61–70. ACM, New York (2005),
http://www.rsa.com/rsalabs/node.asp?id=2860
20. Kremer, S., Ryan, M., Smyth, B.: Election veriﬁability in electronic voting proto-
cols. Technical Report CSR-10-06, University of Birmingham (2010)
21. Ministerie van Binnenlandse Zaken en Koninkrijksrelaties (Netherland’s Ministry
of the Interior and Kingdom Relations). Stemmen met potlood en papier (Voting
with pencil and paper) (May 2008), Press release,
http://www.minbzk.nl/onderwerpen/grondwet-en/verkiezingen/nieuws-en/
112441/stemmen-met-potlood
22. Participants of the Dagstuhl Conference on Frontiers of E-Voting. Dagstuhl accord.
(2007), http://www.dagstuhlaccord.org/
23. Sako, K., Kilian, J.: Secure voting using partially compatible homomorphisms.
In: Desmedt, Y.G. (ed.) CRYPTO 1994. LNCS, vol. 839, pp. 411–424. Springer,
Heidelberg (1994)
24. Talbi, M., Morin, B., Tong, V.V.T., Bouhoula, A., Mejri, M.: Speciﬁcation of
Electronic Voting Protocol Properties Using ADM Logic: FOO Case Study.
In: Chen, L., Ryan, M.D., Wang, G. (eds.) ICICS 2008. LNCS, vol. 5308,
pp. 403–418. Springer, Heidelberg (2008)
25. UK Electoral Commission. Key issues and conclusions: May 2007, electoral pilot
schemes (2007) http://www.electoralcommission.org.uk/elections/pilots/
May2007
26. Woo, T.Y.C., Lam, S.S.: A semantic model for authentication protocols. In: S&P
1993: Proceedings of the 1993 IEEE Symposium on Security and Privacy, Wash-
ington, DC, USA, pp. 178–194. IEEE Computer Society, Los Alamitos (1993)

AnBx - Security Protocols Design
and Veriﬁcation⋆
Michele Bugliesi and Paolo Modesti
Universit`a Ca’ Foscari Venezia
Dipartimento di Informatica
{bugliesi,modesti}@dsi.unive.it
Abstract. Designing distributed protocols is challenging, as it requires
actions at very diﬀerent levels: from the choice of network-level mech-
anisms to protect the exchange of sensitive data, to the deﬁnition of
structured interaction patterns to convey application-speciﬁc guaran-
tees. Current security infrastructures provide very limited support for
the speciﬁcation of such guarantees. As a consequence, the high-level
security properties of a protocol typically must often be hard-coded ex-
plicitly, in terms of low-level cryptographic notions and devices which
clutter the design and undermine its scalability and robustness.
To counter these problems, we propose an extended Alice & Bob nota-
tion for protocol narrations (AnBx) to be employed for a purely declarative
modelling of distributed protocols. These abstractions provide a compact
speciﬁcation of the high-level security guarantees they convey, and help
shield the design from the details of the underlying cryptographic infras-
tructure. We discuss an implementation of the abstractions based on a
translation from the AnBx notation to the AnB language supported by the
OFMC [1,2] veriﬁcation tool. We show the practical eﬀectiveness of our
approach by revisiting the iKP e-payment protocols, and showing that
the security goals achieved by our declarative speciﬁcation outperform
those oﬀered by the original protocols.
1
Introduction
On-line transactions represent an important share of the overall world trade and
security constitutes a major concern in these kind of applications, as agreeing,
on the terms of a transaction in a distributed and open environment like the
internet, requires protection against threats from intruders and/or from the po-
tential misbehavior of other participants. Establishing the desired safeguards
is challenging as it involves actions at diﬀerent levels: from the choice of core,
network-level mechanisms to protect the exchange of sensitive data, to the deﬁni-
tion of structured, application-speciﬁc measures to enforce the high-level behav-
ioral invariants of the participants. Current security infrastructures oﬀer eﬀective
abstractions only for the core mechanisms, based on tools such as TLS/SSL [3]
⋆Work partially supported by MIUR Projects SOFT “Security Oriented Formal Tech-
niques” and IPODS “Interacting Processes in Open-ended Distributed Systems”.
A. Armando and G. Lowe (Eds.): ARSPA-WITS 2010, LNCS 6186, pp. 164–184, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
165
to provide tunneling support for communication. On the other hand, little to
no support is provided for the speciﬁcation of more structured interaction pat-
terns, so that high-level security invariants must typically be expressed, and
hard-coded explicitly, in terms of low-level cryptographic notions such as salt-
ing, nonces, keyed-hashing, encryptions, signature schemes, and compositions
thereof. As a result, the application code and data structures get intertwined
with low-level code that not only gets in the way of a clear understanding of the
applications’ business logic, but also undermines its scalability and robustness.
To counter these problems, various papers in the recent literature (see, e.g.,
[4,5,6]) have advocated a programming discipline based on (i) high-level secu-
rity abstractions and mechanisms for composing them to support structured
interaction patterns [7,8], and (ii) automatic techniques to build defensive im-
plementations on top of well-established cryptographic infrastructures and tools.
Following this line of research, in the present paper we isolate a core set of
channel and data abstractions to be employed for a purely declarative modelling
of distributed protocols. Our abstractions are part of AnBx, a dialect of the well-
known Alice & Bob (AnB) notation for protocol narrations, which supports var-
ious mechanisms for securing remote communications based on abstract security
modes, without any reference to explicit cryptography. The AnBx abstractions
are readily translated into corresponding public-key cryptographic protocols de-
scribed by standard AnB narrations. This provides an abstract, yet eﬀective
implementation of the AnBx speciﬁcation language, to be employed as the basis
for the development of fully-ﬂedged implementation.
Main contributions and results. We developed a compiler for the automatic
translation from AnBx to the AnB notation used in the symbolic model-checker
OFMC (Open-source Fixed-point Model Checker [1,2]), and veriﬁed the sound-
ness of our implementation with OFMC itself. The translation allows the veriﬁ-
cation of AnBx protocols with any OFMC-interoperable veriﬁcation tool [9]. To
experiment and validate the practical eﬀectiveness of the AnBx approach to pro-
tocol design, we revisited the iKP e-payment protocol family (Internet Keyed
Payment Protocol [10,11]) as a case study, and contrasted the security goals
achieved by our version with those oﬀered by the original protocol.
Interestingly, our AnBx versions of the iKP protocols outperform the original
protocols (for all i’s), i.e. they satisfy stronger security goals and properties. This
is largely a consequence of the declarative nature of the speciﬁcation style sup-
ported by AnBx: being deﬁned as channel-level abstractions, the AnBx primitives
convey protection on all message components, not just on some components as in
the original iKP speciﬁcation, yielding stronger encapsulation mechanisms, and
consequently, stronger and more scalable security guarantees. As a byproduct of
our comparative analysis, we also found a (to the best of our knowledge) new
ﬂaw in the original speciﬁcation of {2,3}KP, and proposed an amended version
that rectiﬁes the problem.
Plan of the paper. In Section 2 we introduce the AnBx speciﬁcation language
together with our high-level security abstractions; in Section 3 we outline a

166
M. Bugliesi and P. Modesti
Table 1. AnBx communication modes
η
mode
semantics
(−, −)
plain
conveys no security guarantee
(A, −)
from A
a public, but authentic exchange which provides the
receiver with a guarantee on the origin of the message
(@A, −)
fresh from A
(A, −) with the additional guarantee that the message
is fresh, and may not be replayed
(−, B)
secret for B
a secret transmission, providing guarantees that only
the intended receiver B will be exposed to the message
payload: an intruder may become aware of the existence
of an output, but not of the message contents
(A, B)
from A,
secret for B
combines the guarantees of modes (A, −) and (−, B)
(@A, B)
fresh from A,
secret for B
combines the guarantees of modes (@A, −) and (−, B)
↑η0
forward
conveys some of the guarantees given by η0 and signals
that the message did not originate at the sender’s;
η0 can not be a forward mode itself
translation of the abstractions into a low-level, cryptographic language. In Sec-
tion 4 we present the AnBx compiler that implements that translation using the
AnB• cryptographic language supported by OFMC as a target, and discuss the
soundness of the translation. In Section 5 we show the abstractions at work on
the speciﬁcation of the iKP protocols, and discuss their properties. Section 6
concludes the presentation.
2
AnBx: Declarative Protocol Narrations
Looking at the existing protocols for electronic transactions, such as e-payment,
e-cash and e-voting, one notices that they are characterized by very speciﬁc
interaction patterns, expressed by few messaging primitives and data structures.
In this section, we isolate a core set of these primitives, and encode them in
terms of (i) diﬀerent modes for remote communication, and of (ii) a hiding
transformer on data. We inject these modes and the data transformer into an
extended version of the familiar AnB speciﬁcations for security protocols, whose
syntax may be deﬁned as follows:
A : α
local action performed by A
i. A →B, η : m
A sends a message m to B in mode η
(symmetrically ←receives from)
The action statements A : α may be employed to specify operations performed
by a principal, such as the generation of a new key or a test evaluation, as well
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
167
Table 2. AnBx forward modes
A →B, η0 : m B →C, η1 : m
mode
semantics
(−, C)
↑(−, C)
blind
- Secrecy for C is preserved
- B is not exposed to m
(A, C)
↑(A, C)
blind
- Authenticity from A is preserved
- Secrecy for C is preserved
- B is not exposed to m
(A, −)
(A, B)
(@A, −)
(@A, B)
⎫
⎪
⎪
⎬
⎪
⎪
⎭
↑(A, −)
sighted
- Authenticity from A is preserved
- Freshness (if present) is lost
- B is exposed to m
(A, −)
(A, B)
(@A, −)
(@A, B)
⎫
⎪
⎪
⎬
⎪
⎪
⎭
↑(A, C)
sighted
secret
- Authenticity from A is preserved
- Freshness (if present) is lost
- Secrecy for C is added to m, but C can not
make any assumption on secrecy at origin A
- B is exposed to m
as to declare the initial knowledge that we assume is available to the principal.
In the exchange statements i. A →B, η : m, i is an index labelling a protocol
step: the mode η and the format of the message m are discussed below.
Message formats. An important aspect of our abstractions is the choice of
the message formats. A message may either be a tuple of names (˜n), or a ref-
erence to a message exchanged at a previous protocol step (↑i), or a message
digest [ ˜m]. Notice that no explicit cryptographic operator is available for mes-
sage formation, and the only operation on data is the creation of digests (or
footprints) needed in most e-commerce and e-voting protocols: being able to
form [m] proves the knowledge of m without leaking it. We assume digests to
be resistant to chosen-plaintext attacks, hence presuppose an implementation
based on a hashing scheme that packages m together with a randomized quan-
tity known to the principals that possess m (the designated veriﬁer of [m]), and
is never leaked to any principals that do not have knowledge of m. To ease the
implementation, we allow digests to be tagged with an annotation that speciﬁes
the intended veriﬁer, as in [ ˜m : B].
Communication modes. Following [12,13], our abstractions encompass two
fundamental mechanisms for security, based on secrecy and authentication, and
include the communication modes that result from their possible combinations.
The mode η is encoded by a pair (source, destination) that qualiﬁes the security
guarantees conveyed at the two end-points of the remote communication. The
structure and the informal reading of the communication modes are described
in Table 1.
Forward modes. They can be used to model some three party exchanges, in
n-party protocols where n > 2. The statement B →C, η1 : m with η1 = ↑η

168
M. Bugliesi and P. Modesti
expresses the intention of B to forward a message m to C. The statement is
legal just in case B has been the target of a previous exchange A →B, η0 : m,
where m is the same in both statements (or else m = ↑i where i is the label
identifying the A →B exchange).
When this is the case, the mode η used in the forward statement will preserve,
and possibly extend, the security properties speciﬁed by η0. There are additional
coherence constraints that must be satisﬁed when specifying the mode of an ex-
change, to rule out unwanted eﬀects of impersonation, and other inconsistencies
that would result into unexecutable or broken protocols:
– the legal combination of η0 and η1 only are those in Table 2 and they specify
possible forward modes;
– in A →B, η : m, η may not specify a source other than A, unless η is a
forward mode;
– if η speciﬁes a destination other than B, then m can be forwarded only
blindly.
3
A Cryptographic Translation for AnBx
We outline a translation of the abstract speciﬁcation language we just introduced
into an intermediate representation based on few, well deﬁned building blocks:
keys, cryptographic operations and primitives for remote communication. The
intermediate notation, displayed in Table 3, is derived from the well known AnB
security protocol notation, and represents the low-level counterpart of the AnBx
abstract notation discussed in Section 2. The core resulting from the translation
can then be further compiled against a speciﬁc and concrete target (operating
system, programming language, cryptographic libraries, network protocols, etc)
to produce the running code.
This translation is based on public key cryptography, and presupposes the
existence of a Public Key Infrastructure (PKI) supporting a robust distribution
(and use) of public keys and certiﬁcates (including their veriﬁcation). We also
assume that the underling PKI supports dual key pairs, for encryption and digital
signatures [14]. Each AnBx principal may thus possess up to two pairs of certiﬁed
encryption keys: if it does posses both key pairs, we say that the principal is
certiﬁed.
We also use symmetric encryption schemes, which are notoriously computa-
tionally more eﬃcient than the asymmetric counterpart. Ideally perfect encryp-
tion is assumed and the hashing functions are expected to be collision-free and
non-invertible. Practically, the secrecy notion we expect is the standard com-
putational secrecy, i.e. all polynomial time adversaries have negligible success
probabilities. This allows us to use real cryptographic protocols for a concrete
implementation.
Communication Modes Translation. Table 4 summarizes the translation
of the diﬀerent AnBx communication modes into a sequence of AnB statements
and shows which principals must be certiﬁed. In a real implementation we could
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
169
Table 3. AnB-like Intermediate Syntax Notation
syntax
description
(PKA, PK−1
A )
long term (pub, priv) key pair used by principal A for encryption
(SKA, SK−1
A )
long term (pub, priv) key pair used by A for signature
{m}P KA
message m encrypted with the public key PKA of principal A
{m}P K−1
A
message m decrypted with the private key PK−1
A
by principal A
{m}K
{m}K−1
message m encrypted (decrypted) with a symmetric key K
{H(m)}SK−1
A
≡sigA(m) digital signature of the message m, signed by principal A, where
H is the hash function speciﬁed by the digital signature scheme
(m, sigA(m)) ≡SA(m)
message m digitally signed by principal A
nA
nonce generated by principal A
new K
fresh symmetric key, randomly generated
gx mod p
gy mod p
fresh half keys generated during a discrete logarithm based
agreement protocol (e.g. Diﬃe-Hellman),
where p is prime and g is primitive root mod p
gxy
fresh symmetric key, computed using a discrete logarithm based
agreement protocol (mod p is omitted in the notation)
HmacK(m)
HMAC of message m, computed with the key K
A : α
action α performed by principal A
A →B : m
A sends a message m to principal B (symmetrically ←)
include, for robustness, also some additional information (tags) such as protocol
name, version, step number, process identiﬁer, origin, destination, but at this
level of abstraction we can skip these details.
The authenticity property enforced by the “fresh from” mode corresponds
to Lowe’s injective agreement [15] while the property enforced by the “from”
mode corresponds to Lowe’s non-injective agreement. In the former case we say
that B authenticates A on m, while in the latter the authentication is only
weak. We use mostly standard techniques, hence here we will underline only few
additional points.
– in authentic exchanges the identity of the intended recipient B is included to
comply with the deﬁnition of the Lowe’s non-injective agreement, (injective
for fresh exchanges) [15];
– in secret exchanges we use a hybrid construction, with the fresh symmetric
key K acting as a confounder. If both, key and data encapsulation schemes,
are secure against adaptive chosen ciphertext attacks, then the hybrid scheme
inherits that property as well [16];
– combining authenticity and secrecy, we ﬁrst sign and then encrypt [17];

170
M. Bugliesi and P. Modesti
Table 4. Exchange Modes Translation of A →B, η : m
η
mode
translation
certiﬁed
(−, −)
plain
A →B : m
-
(A, −)
from A
A →B : SA(B, m)
A
(−, B)
secret for B
A : new K
A →B : {K}P KB , {m}K
B
(A, B)
from A,
secret for B
A : new K
A →B : {K}P KB , {SA(B, m)}K
A, B
(@A, −)
fresh from A A →B : A
A ←B : {nB, B}P KA
A →B : {nB, sigA(B, m)}P KB, SA(B, m)
A, B
(@A, −)
fresh from A
with DH*
A →B : gx
A ←B : {gy, nB, B}P KA
A →B : {nB, sigA(B, m)}gxy, SA(B, m)
A
(@A, B)
fresh from A,
secret for B
A →B : A
A ←B : {nB, B}P KA
A : new K
A →B : {nB, K}P KB , {SA(B, m)}K
A, B
* this translation is used only when the intended recipient B is not
certiﬁed, and therefore the standard fresh from A can not be applied
Table 5. Forward modes translation
η0
η1
mode
m1
certiﬁed
notes
(−, C)
↑(−, C)
blind
{K}P KC , {m}K
C
m0 = m1
(A, C)
↑(A, C)
blind
{K}P KC , {SA(C, m)}K
A, C
m0 = m1
(A, −)
(@A, −)

↑(A, −)
sighted SA(C, B, msg)
A
m1 ⊆m0
m = C, msg
(A, B)
(@A, B)

↑(A, −)
sighted SA(C, B, msg)
A, B
m1 ⊆m0
m = C, msg
(A, −)
(@A, −)

↑(A, C)
sighted
secret
{K}P KC , {SA(C, B, msg)}K
A, C
SA(C, B, msg) ⊆m0
m = C, msg
(A, B)
(@A, B)

↑(A, C)
sighted
secret
{K}P KC , {SA(C, B, msg)}K A, B, C
SA(C, B, msg) ⊆m0
m = C, msg
AnBx speciﬁcation: A →B, η0 : m; B →C, η1 : m
m1 ⊆m0 : message m1 is a component of message m0
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
171
Table 6. Message Formats Translation (digest formed by principal A)
digest
translation
certiﬁed
notes
[m : B] A : new K
A →B : HmacK(m), {K}P KB
B
Obfuscated veriﬁable digest
Can be veriﬁed only by B,
if B knows m
(besides the digest’s creator A)
[m : −] A : new K
A →B : HmacK(m)
-
Obfuscated unveriﬁable digest
Cannot be veriﬁed by anyone
(except the digest’s creator A)
[m]
H (m)
-
Plain digest
Can be veriﬁed by everyone knowing m
H cryptographic hash function
[m : C] p : new K
p →C : {H (mp)} , {Kp}P KC
C
Digest veriﬁable by proxy
p ∈{A, B}
C can notify to any principal if
H (mA) = H (mB) or not
– in fresh exchanges we use a challenge-response technique based on nonces. We
do not use timestamps, because they require synchronization (time servers),
and this introduces more complexity (attacks, failures, etc);
– in fresh from exchanges, when the intended recipient is not certiﬁed, we
use a combination of challenge-response and Diﬃe-Hellman key agreement
protocol;
Forward Modes Translation. Table 5 outlines the translation of the forward
modes. Each clause gives the translation of the statement B →C, η1, m with
reference to the statement A →B, η0, m that originated m: we only give the
translation for the legal combinations of η0, η1: in all other cases the translation
is undeﬁned.
Given that the forward modes in η1 will never include the freshness tag @,
the translation of B →C, η1, m amounts to just one AnB step, B →C, m1 with
m1 as deﬁned in Table 5.
Notice that m1 is always deﬁned in terms of m0, which is the closing AnB
message resulting from the translation of the reference AnBx statement A →
B, η0, m. In the sighted modes, the presence of the identities B and C is necessary
because both principals have to authenticate A on msg, which represents the real
high level payload of the three-party exchange.
Message Format Translation. (Table 6). As we observed, an implementa-
tion of a message digest [m] should guarantee that the only principals entitled
to verify that [m] matches m are those who already know m, either as initial
information, or provided explicitly by a legitimate source. In addition, a ro-
bust implementation should protect the digest against chosen-plaintext attacks,
and additionally guarantee that even in case m is disclosed to a non-legitimate

172
M. Bugliesi and P. Modesti
principal any time in the future, that principal should not be able to use m to
match [m].
These guarantees can be made by interpreting (and translating) the digest
formation in terms of a hashing scheme that packages m together with a ran-
domized quantity known to the principals that possess m, and never leaked to
any principals that do not have knowledge of m. A hashing scheme with these
properties is supported by what is known as keyed-Hash Message Authentication
Code (HMAC).
The subtlety in our translation is in the way the secret keys are made available
to the legitimate principals, as the structure of the digest [m] does not inform on
who is intended to act as the digest’s veriﬁer. In certain cases, that information
may be recovered by an analysis of the initial knowledge of the participants.
However, in general, it requires the designer’s intervention to tag each digest
occurrence [m] with an annotation [m : B] that signals that B is the designated
veriﬁer of [m] (a digest can also be always veriﬁed by its creator). Table 6 illus-
trates the translation scheme for digests based on these additional annotations
also proposing alternatives when the digest cannot directly be veriﬁed, due to
the lack of certiﬁcation of some participant. When and how to use these alter-
natives should be carefully evaluated since it could allow an intruder to perform
downgrade attacks.
4
Implementing the Translation: The AnBx Compiler
We have implemented the AnBx translation previously outlined using a subset
of the AnB cryptographic language supported by the OFMC model checker as
a target. In this section, we brieﬂy outline the implementation and discuss its
soundness.
An overview of OFMC. OFMC [1,2]) supports two protocol speciﬁcation lan-
guages, the Intermediate Format (IF) and a dialect of the AnB style of protocol
narrations, that we will refer to as OFMC-AnB. Details about of OFMC-AnB
can be found in [18] and in documentation included in the OFMC software pack-
age, so we will give a very short description. A protocol speciﬁcation comprises
various sections:
– Types: describes the entities (agents/principals) involved in the protocol, as
well as protocol data and data operators (constants, cryptographic functions,
. . . etc.)
– Knowledge: speciﬁes the initial knowledge of each principal
– Actions: speciﬁes the sequence of statements that constitute the ideal,
unattacked run of the protocol;
– Goals: speciﬁes the goals that the protocol is meant to convey.
A sample speciﬁcation is reported in Table 7, where we give the AnB narration
of the low-level translation for the “fresh-from” AnBx messaging primitive.
Msg is the message being authenticated, using N1 to complete the nonce-
exchange; pk and sk, generate the public keys for encryption and signing. Each
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
173
Table 7. AnB OFMC speciﬁcation of “fresh from” mode
Protocol : Fresh_From_A
Types:
Agent A,B;
Number
Msg ,N1;
Function
pk ,sk ,hash
Knowledge:
A: A,B,pk ,sk ,inv(pk(A)),inv(sk(A));
B: A,B,pk ,sk ,inv(pk(B)),inv(sk(B))
Actions :
A -> B: A
B -> A: {N1 ,B}pk(A)
A -> B: {N1 ,hash ({B,Msg}inv(sk(A)))}pk(B),{B,Msg}inv(sk(A))
Goals:
B authenticates A on Msg
principal is assumed to know its own and its partner’s identities, the public
key of all known principals and its own private keys. The speciﬁcation is com-
pleted with the authentication goal for the transmitted message Msg. This goal
already implies the freshness of the message, since in OFMC authenticity goals
correspond to the requirements of Lowe’s injective agreement.
AnBx and its compiler. AnBx is deﬁned as a variant of OFMC-AnB that sup-
ports the messaging modes discussed in §2 as well as few additional features
in the preamble sections. A sample AnBx speciﬁcation is reported in Table 8,
where we give the AnBx speciﬁcation of the “fresh from” messaging primitive.
Notice that while in the action section we use AnBx syntax to specify the trans-
mission modes, the protocol goals are expressed by means of OFMC-AnB goal
statements.
Given an AnBx speciﬁcation, the compiler generates a corresponding OFMC-
AnB protocol speciﬁcation that results from composing the translations of each
AnBx statement into a corresponding OFMC-AnB protocol, as outlined in §3.
Thus, while AnBx supports abstract messaging primitives for secrecy and au-
thentication, the code generated by the compiler only involves standard, “plain”
Table 8. AnBx speciﬁcation of “fresh from” mode
Protocol : Fresh_From_A Types:
Agent A,B;
Certified A,B;
Number M1 ,M2;
Function
id
Definitions
Msg: M1 ,M2
Knowledge:
A: A,B;
B: A,B
Actions :
A -> B,(@A ,-): Msg
Goals:
B authenticates A on Msg

174
M. Bugliesi and P. Modesti
message exchanges, and relies of the cryptographic constructions of §3 to achieve
the desired security guarantees. In addition to generating the AnB protocol steps,
the compiler creates the entries in the preamble sections (Types and Knowledge)
required to make a consistent OFMC-AnB speciﬁcation. To illustrate, the AnB
protocol in table 7 is, in fact, the result of compiling the AnBx in Table 8.
One may wonder why we did not take advantage of the bulleted OFMC-AnB
channels in our translation. The fact is that the combination of the three OFMC
basic channel types (authentic, conﬁdential, secure) does not allow one to express
communication patterns as those deﬁned by the forward modes.
This can be easily seen if we try to build the following sighted forward ex-
change: A →B, (@A, −) : m, B →C, ↑(A, −) : m . If we attempt to use a
bulleted channel to translate the ﬁrst instruction, as in A*->B:B,m, there is no
bulleted equivalent to express the second one. On the other hand, if we provide
a translation over a plain channel for the second instruction as the following,
B->C:{B,C,Msg}inv(sk(A)), OFMC will deem the protocol non-runnable. In
fact OFMC works at the symbolic level, and the term {B,C,Msg}inv(sk(A))
can be used in the second instruction only if it is has appeared earlier in the net-
work, because only principal A could have created such a term, being the only
one knowing inv(sk(A)). Therefore OFMC rejects a protocol where the term
{B,C,Msg}inv(sk(A)) appears for the ﬁrst time in an exchange originating from
B. In other words, there is no way to bind the bulleted and the unbulleted chan-
nels in order to satisfy the expected goals for the forward modes. Hence in order
to provide a coherent translation of all AnBx modes we need the plain channel
as target in all cases.
Soundness. To verify the soundness of the implementation, we coded in OFMC-
AnB all the AnBx primitives listed in §3, deﬁning the expected security goals. The
full list is given in Appendix (Table 13). Then we ran OFMC to verify the safety
of each protocol. We tested, successfully, all the resulting protocols with OFMC,
in one and two sessions (both typed and untyped) with the classical mode.
Most protocols were also veriﬁed in classical typed mode up to four sessions.
We also checked the new ﬁxpoint module for unbounded number of sessions, but
limited to secrecy and weak authenticity goals, which are the only goals currently
supported by OFMC 2009c.
A note on compositionality. Having validated the implementation of each
AnBx primitive against its expected goals does not in itself provide any validation
guarantee on the implementation of a structured AnBx speciﬁcation. In fact,
composing the implementation protocols resulting from the translation of each
AnBx step may, in principle, break the security guarantees provided by each of
the component protocols: this is an instance of the well-known compositionality
problems in security. While we do not yet have a formal compositionality proof
for our translations, we are conﬁdent that such result can be proved: in fact,
the implementation protocols satisfy all the static conditions, such as the use of
encryption keys from disjoint key-spaces that constitute the standard suﬃcient
conditions for compositionality.
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
175
In addition, although compositionality is certainly an interesting and useful
property, it does not represent a primary concern for our present endeavour. Our
main interest is in making AnBx interoperable with existing veriﬁcation tools
like OFMC rather than in deﬁning a new tool. Thus, as long as the security
goals of a given AnBx protocol narration provide an adequate speciﬁcation of the
security properties we expect of the protocol, we may safely content ourselves
with validating the AnB protocol resulting from our translation, rather than the
AnBx speciﬁcation itself.
A similar approach is gaining popularity in the literature on typed process
calculi targeted at the speciﬁcation of distributed protocols and systems (see,
e.g., [19,20]). In these papers, the typed calculi provide for idealized speciﬁ-
cations which are implemented by translations into low-level, but still typed,
cryptographic languages. Rather than showing the translations sound, the pro-
cess calculi speciﬁcations are shown secure by directly proving that the result of
the translations are secure, just as we propose here: the diﬀerence is that in a
typed calculus, security is by (well) typing, while in AnBx we prove security by
model-checking.
5
A Case Study: e-Payment Systems
We show AnBx at work on the speciﬁcation of systems for e-payment. We start by
outlining a general e-payment scheme which captures the essential ingredients of
most of the existing e-payment protocols, among which IBM’s iKP [10,11], SET
[21] and 3-D Secure [22] adopted by VISA.
Each principal has an initial knowledge shared with other participants. In par-
ticular, since most e-commerce protocols describe only the payment transaction,
we assume that customer and merchant have agreed on a contract, that includes
an order description (desc) and a price. We also assume that payments are based
on existing credit-card systems operated by an acquirer who shares with the cus-
tomer the customer’s account number (can) comprising the credit card number
and the associated PIN. In summary, the initial knowledge of the parties is the
following:
– Customer C: price, desc, can
– Merchant M : price, desc
– Acquirer A: can
To make each transaction univocally identiﬁed, the merchant generates a unique
transaction ID (tid) and associates the transaction also with a date (or any ap-
propriate time information). Both pieces of information must be communicated
to the other parties. Summarizing, the transaction is characterized by the the

176
M. Bugliesi and P. Modesti
tuple (price, tid, date, can, desc) which also constitutes the payment order in-
formation: if customer and merchant reach an agreement on this tuple, and they
can prove their agreement to the acquirer, then the transaction can be completed
successfully.
However, two security concerns arise here: on the one hand, customers typi-
cally wish to avoid leaking credit-card information to the merchant; on the other
hand, customers and the merchant would not let the acquirer know the details
of the order or the services involved in the transaction. Both these requirements
can be enforced by protecting the exchange of can and desc with the digests
we introduced in Section 2 and implemented as described in Table 6. The tuple
that represent the contract among the three parties may thus be represented as
(price, tid, date, [can][desc]), while auth is the transaction authorization result
returned by the acquirer.
Revisiting iKP. The iKP protocol family {i=1,2,3} was developed at IBM
Research [10,11,23] to support credit card-based transactions between customers
and merchants (under the assumption that payment clearing and authorization
may be handled securely oﬀ-line). All protocols in the family are based on public-
key cryptography, and vary in the number of parties that own individual public
key-pairs to generate digital signatures: this is reﬂected by the name of the
diﬀerent protocols – 1KP, 2KP, 3KP – which oﬀer increasing levels of security.
The structure of all the protocols can be speciﬁed by means of the AnBx messaging
primitives as follows:
1. C →M, η1 : [can : A], [desc : M]
2. C ←M, η2 : price, tid, date, [contract]
3. C →M, η3 : price, tid, can, [can : A], [contract]
4. M →A
(a) M →A, η4a : price, tid, can, [can : A], [contract]
(b) M →A, η4b : price, tid, date, [desc : M], [contract]
5. M ←A, η5 : auth, tid, [contract]
6. C ←M, η6 : auth, tid, [contract]
The digests [can] and [desc] are annotated with their intended veriﬁers, A and M
respectively (in 1KP [desc] deserves some more care, as we will discuss shortly).
Correspondingly, contract is the tuple price, tid, date, [can : A], [desc : M] and
its digests can be left as [contract] as all the sensitive data is already protected
against chosen plaintext attacks by the nested digests.
By instantiating the exchange modes ηi one may generate diﬀerent versions
of the protocol, achieving diﬀerent security guarantees. In Table 9 we report
the protocols resulting from the choice of the exchange modes ηi that oﬀer the
strongest level of security for a given number (i) of participants having public
key-pairs at their disposal. The resulting protocols constitute the AnBx counter-
part of the iKp protocol family.
Security Veriﬁcation. We veriﬁed the AnBx speciﬁcations of {1,2,3}KP by
compiling them into OFMC-AnB and running OFMC on the generated protocols
against the strongest possible goals, for each of the protocols.
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
177
Table 9. Exchange modes for the revisited iKP e-commerce protocol
mode/step
→
1KP
2KP
3KP
η1
C →M
(−, −)
(−, M)
(@C, M)
η2
C ←M
(−, −)
(@M, −)
(@M, C)
η3
C →M
(−, A)
(−, A)
(C, A)
η4a
M →A
↑(−, A)
↑(−, A)
↑(C, A)
η4b
M →A
(−, A)
(@M, A)
(@M, A)
η5
M ←A
(@A, −)
(@A, M)
(@A, M)
η6
C ←M
↑(A, −)
↑(A, −)
↑(A, C)
certiﬁed
A
M, A
C, M, A
Table 10. Security goals satisﬁed by Original and Revised iKP
1KP
2KP
3KP
Goal
O
R
O
R
O
R
can secret between C,A
-
-
-
-
+
+
C →∗a : can
+
+
+
+
+
+
A authenticates C on can
-
-
-
-
-
+
desc secret between C,M
+
+
+
+
+
+
Auth secret between C,M,A
-
-
-
-
-
+
M authenticates A on Auth
-
+
+*
+
+*
+
C authenticates A on Auth
-
+
+*
+
+*
+
tid secret between C,M,A
-
-
-
-
-
+
price secret between C,M,A
-
-
-
-
-
+
[contract] secret between C,M,A
-
-
-
-
-
+
a authenticates C on [contract]
-
-
-
+
+
+
a authenticates M on [contract]
-
-
-
+
+
+
* goal satisﬁed only ﬁxing the deﬁnition of SigA
We also carried out a corresponding analysis of the original speciﬁcations,
as reported in [11] and later amended in [24]. Below, we refer to this amended
version as the “original” iKP, to be contrasted with the “revised”, AnBx version
discussed above. In both versions, we run our tests assuming that the acquirer is
trusted (technically, modelled as a constant in the OFMC speciﬁcation). This ap-
pears reasonable in an e-commerce application, since the acquirer is always certi-
ﬁed. Furthermore, to compare the results of the analysis we treated the messages
common and contract, in the original and in the revised version, respectively, as
equivalent (indeed, they do provide the same abstraction conceptually).
On each protocol, we ran OFMC in classic mode with 1 and 2 sessions (typed
and untyped): with 2 sessions we were sometimes unable to complete the test
due to search space explosion. We also ran intensive tests limiting the depth of

178
M. Bugliesi and P. Modesti
the search space to remain within the available memory space (2Gb). Below, we
report on the results of such tests. For 3KP, the AnBx code for the revised and
the original versions is shown in the Appendix (Tables 11 and 12).
Main results. In general, our implementation of the iKP protocols outperforms
the original version, i.e. it satisﬁes more and stronger security goals, for all i’s.
This is largely due to the declarative nature of AnBx messaging primitives, which
being deﬁned as channel abstractions provide strong encapsulation mechanisms
on the messages they exchange. One of the design goals of iKP was scalability, to
support increasing security with the increase of the number of certiﬁed principals.
In the original version, scalability is achieved by including extra components (e.g.
signatures) in the messages exchanged in the diﬀerent instances of the protocols.
Conversely, the AnBx versions are naturally scalable, as the diﬀerent instances
of the protocol are simply encoded by tuning the exchange modes employed at
the diﬀerent steps (9). The only price to pay with respect to the original iKP is
that we need to split step 4 in two substeps, plus one additional step to return
the initiative to the merchant after step 4a. (Obviously the compiled AnB code
involves more additional steps in order to achieve the freshness when required).
As we mentioned earlier, the AnBx speciﬁcation are not just more scalable:
they provide stronger security guarantees (cf. Table 13). During the analysis
of the original 2KP and 3KP we found what, to the best of our knowledge, is
a new ﬂaw. It is related with the authenticity of the Authorization response
(auth) that is generated by the acquirer and then sent to the other principals at
step 5 and 6. The authenticity of the response is very important for the correct
termination of the transaction because otherwise, in case of controversy customer
and merchant could not rely on a robust proof of payment. The starred goals in
Table 13 are met only after ﬁxing this ﬂaw that is in relation with how injective
agreement is deﬁned (and to its notion of authenticity). The change we propose
is to add the identities of merchant and customer in SigA, adding the identities
of merchant and customer (in 2KP this can be done with an ephemeral identity
derived from the credit card number). Therefore in the original speciﬁcation
[11]: SigA : {hash(Auth, hash(Common))}SK−1
A
should be replaced by SigA :
{hash(C, M, Auth, hash(Common))} SK−1
A .
Our revisited 2KP performs almost as good the original 3KP. The only goal
not satisﬁed is can secret between C,A. This does not mean that the credit card
is leaked but only that is not strongly authenticated by the acquirer. In 3KP
the credit card number can be signed by C, whereas this is not possible in 2KP
and 1KP (neither in the original, nor in the revised versions). In this case, the
acquirer weakly authenticates the customer by means of the credit card number,
which is a shared secret among the two parties.
We mentioned earlier that some care must be taken in the revisited version
of 1KP to compute the digest [desc:M ]. In fact M is not certiﬁed, so we cannot
calculate the digest veriﬁable by M (Table 6). Therefore we must adopt one of
the possible alternatives and this requires a slight modiﬁcation of the protocol.
If we decide to use the unveriﬁable digest, as done by the original iKP, it is
advisable to have M generate the digest rather than C, because [can:A] is already
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
179
Table 11. AnBx speciﬁcation of revised 3KP
Protocol : Revised_3KP
Types:
Agent C,Me ,a;
Certified C,Me ,a;
Number
TID ,Auth ,Desc ,Price;
Function
can
Definitions:
Contract : Price ,TID ,dig(can(C),a),dig(Desc ,Me)
Knowledge:
C: C,Me ,a,can(C);
Me: C,Me ,a;
a: C,Me ,a
Actions :
# 0. Setup/ Initial
Knowledge
C *->*Me: Price ,Desc
Me -> C: empty
C -> Me ,(@C ,Me): dig(can(C),a),dig(Desc ,Me)
Me -> C,(@Me ,C): TID ,dig(Contract )
C -> Me ,(C,a): Price ,TID ,can(C),dig(can(C),a),dig(Contract )
Me -> a ,^(C,a): Price ,TID ,can(C),dig(can(C),a),dig(Contract )
a -> Me: empty
Me -> a,(@Me ,a): Price ,TID ,dig(Desc ,Me),dig( Contract )
a
-> Me ,(@a ,Me ): C,Auth ,TID ,dig(Contract )
Me -> C ,^(a,C): C,Auth ,TID ,dig(Contract )
Goals:
can(C) secret
between C,a
a authenticates C on can(C)
Desc secret
between C,Me
Auth secret
between C,Me ,a
Me
authenticates a on Auth
C authenticates a on Auth
TID secret
between C,Me ,a
Price
secret
between C,Me ,a
dig( Contract ) secret
between C,Me ,a
Table 12. Portion of the AnBx speciﬁcation of original 3KP
Protocol: Original_3KP
Types:
Agent C,Me ,A;
Certified C,Me ,A;
Number TID ,Auth ,empty ,Desc ,Price ,ID ,SALTC ,SALTMe ,V,VC ,NONCE ,RC;
Function pk ,sk ,hash ,hmac ,can
Definitions :
IDC: hmac(RC ,can(C));
Common: Price ,ID ,TID ,NONCE ,hmac(RC ,can(C)),hmac(SALTC ,Desc),hash(V),hash(VC);
Clear: ID ,TID ,NONCE ,hash(Common),hash(V),hash(VC);
Slip: Price ,hash(Common),can(C),RC ,SALTMe;
EncSlip: {Slip}pk(A);
SigMe: {hash(Common),EncSlip}inv(sk(Me ));
SigC: {hash(EncSlip ,hash(Common ))} inv(pk(C));
SigA: {C,Me ,hash(Auth ,hash(Common ))} inv(sk(A))
Knowledge: [...]
Actions:
C *->* Me: Price ,Desc
Me -> C: empty
C -> Me: SALTC ,IDC
Me -> C: Clear ,SigMe
C -> Me: EncSlip ,SigC
Me -> A: Clear ,hmac(SALTC ,Desc),EncSlip ,SigMe ,SigC
A -> Me: Auth , SigA
Me -> C: Auth ,SigA ,V,VC
Goals: [...]

180
M. Bugliesi and P. Modesti
Table 13. Map of AnBx communication and forward modes
exchange
mode
OFMC goals
Communication Modes - Tables 1,4
A →B, (−, −) : Msg
plain
-
A →B, (A, −) : Msg
from A
B weakly authenticates A on Msg
A →B, (@A, −) : Msg
fresh from A
B authenticates A on Msg
A →B, (−, B) : Msg
secret for B
A->*B: Msg
A →B, (A, B) : Msg
from A,
secret for B
B weakly authenticates A on Msg
Msg secret between A,B
A →B, (@A, B) : Msg
fresh from A,
secret for B
B authenticates A on Msg
Msg secret between A,B
Forward Modes - Tables 2,5
A →B, (−, C) : Msg
B →C, ↑(−, C) : Msg
blind
A->*C: Msg
A →B, (A, C) : Msg
B →C, ↑(A, C) : Msg
blind
Msg secret between A,C
C weakly authenticates A on Msg
A →B, (A, −) : C, Msg
B →C, ↑(A, −) : C, Msg
sighted
B weakly authenticates A on Msg
C weakly authenticates A on Msg
A →B, (A, B) : C, Msg
B →C, ↑(A, −) : C, Msg
sighted
B weakly authenticates A on Msg
C weakly authenticates A on Msg
A->*B: Msg
A →B, (@A, −) : C, Msg
B →C, ↑(A, −) : C, Msg
sighted
B authenticates A on Msg
C weakly authenticates A on Msg
A →B, (@A, B) : C, Msg
B →C, ↑(A, −) : C, Msg
sighted
B authenticates A on Msg
C weakly authenticates A on Msg
A->*B: Msg
A →B, (A, −) : C, Msg
B →C, ↑(A, C) : C, Msg
sighted
secret
B weakly authenticates A on Msg
C weakly authenticates A on Msg
A →B, (@A, −) : C, Msg
B →C, ↑(A, C) : C, Msg
sighted
secret
B authenticates A on Msg
C weakly authenticates A on Msg
A →B, (A, B) : C, Msg
B →C, ↑(A, C) : C, Msg
sighted
secret
B weakly authenticates A on Msg
C weakly authenticates A on Msg
Msg secret between A,B,C
A →B, (@A, B) : C, Msg
B →C, ↑(A, C) : C, Msg
sighted
secret
B authenticates A on Msg
C weakly authenticates A on Msg
Msg secret between A,B,C
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
181
generated by C, and since the digest generation implies the usage of fresh keys as
confounders, it is appropriate that those two values are generated by two diﬀerent
principals. In 2KP and 3KP, [desc:M ] is veriﬁable by M and this concern is not
present. Finally, we notice that in 1KP the only information really protected
is the credit card number (Goal C →∗a : can). Since this is one of the main
concerns of all e-commerce users, it is indeed good news.
6
Related Work and Conclusion
The beneﬁts of a programming discipline based on high-level abstractions for
security are increasingly being recognized as important in the design of dis-
tributed systems. We believe that the experience with AnBx we have reported
in this paper provides further evidence of the advantages of the approach: as
we have illustrated, using adequate abstractions results not only in simpler de-
sign, but also in a more robust code and stronger security. In fact, being deﬁned
as channel-level abstractions, the AnBx primitives convey protection on all mes-
sage components, yielding stronger encapsulation mechanisms, and consequently,
stronger and more scalable security guarantees.
Our approach shares motivations and ideas with various papers on design and
implementation of high-level process calculi for distributed protocols and sys-
tems [4,25,26,6,27]. While related in their initial motivations, these approaches
diﬀer signiﬁcantly from our present endeavour in their technical development.
In addition, the abstractions we discuss here capture more expressive security
mechanisms such as those needed in the design of structured e-commerce proto-
cols. Some research more closely related to ours has been carried out in [28,29,2].
Guttman [28] has proposed a protocol design process centered on the authenti-
cation tests, a method for protocol veriﬁcation based on the strand space theory.
Guttman, Herzog, Ramsdell, and Sniﬀe [29] attached trust management asser-
tions to protocol actions, constraining the behavior of a principal to be compat-
ible with its own trust policy, and proposed the CPPL language for design and
veriﬁcation. However their language still makes use of cryptographic primitives
while we avoid any reference to explicit cryptography.
M¨odersheim and Vigan`o [2] described security protocols with three basic kinds
of channels (authentic, conﬁdential, secure). An Ideal and a Cryptographic Chan-
nel Model, describe the ideal functionality of the channels and the concrete cryp-
tographic messages on insecure channels. The meaning of channels is deﬁned as
goals proving that, under certain restrictions, composing individually secure pro-
tocols results in a secure protocol. We used their OFMC tool [1,2] to verify the
AnBx compiled protocols. Our set of channel modes has a wider extension with
respect to the OFMC bulleted channels and in §4 we showed that our forward
modes (and their associated goals) cannot be speciﬁed only by means of the
bulleted channels.
Future plans include work along several directions. Speciﬁcally, we are in-
terested in developing a typed version of our primitives to provide static sup-
port for security, according to the design principles that are often referred to as

182
M. Bugliesi and P. Modesti
language-based security. The resulting typed abstractions could then be inte-
grated into a programming language. Meanwhile we are coding a Java library
to be used to experiment with real applications. Also, it would be interesting to
investigate the eﬀectiveness of our approach in expressing other properties, such
as anonymity and privacy, and work with the corresponding application domains
such as e-cash and e-voting.
References
1. Basin, D., M¨odersheim, S., Vigan`o, L.: OFMC: A symbolic model checker for secu-
rity protocols. International Journal of Information Security 4(3), 181–208 (2005)
2. M¨odersheim, S., Vigan`o, L.: The open-source ﬁxed-point model checker for sym-
bolic analysis of security protocols. In: Foundations of Security Analysis and Design
V, p. 194. Springer, Heidelberg (2009)
3. Dierks, T., Allen, C.: Rfc2246: The TLS protocol version 1.0. Internet RFCs (1999)
4. Abadi, M., Fournet, C., Gonthier, G.: Authentication primitives and their compi-
lation. In: Proceedings of the 27th ACM SIGPLAN-SIGACT symposium on Prin-
ciples of programming languages, pp. 302–315. ACM, New York (2000)
5. Bugliesi, M., Focardi, R.: Language Based Secure Communication. In: IEEE 21st
Computer Security Foundations Symposium, CSF 2008, pp. 3–16 (2008)
6. Adao, P., Fournet, C.: Cryptographically Sound Implementations for Communicat-
ing Processes. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP
2006. LNCS, vol. 4052, pp. 83–94. Springer, Heidelberg (2006)
7. Corin, R., D´enielou, P.M., Fournet, C., Bhargavan, K., Leifer, J.J.: Secure imple-
mentations of typed session abstractions. In: CSF 2007, pp. 170–186. IEEE, Los
Alamitos (2007)
8. Bhargavan, K., Corin, R., D´enielou, P.M., Fournet, C., Leifer, J.J.: Cryptographic
protocol synthesis and veriﬁcation for multiparty sessions. In: CSF 2009 (2009)
9. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Drielsma, P., H´eam, P., Kouchnarenko, O., Mantovani, J., et al.: The AVISPA
tool for the automated validation of internet security protocols and applications.
In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576, pp. 281–285.
Springer, Heidelberg (2005)
10. Bellare, M., Garay, J., Hauser, R., Herzberg, A., Krawczyk, H., Steiner, M., Tsudik,
G., Waidner, M.: iKP A Family of Secure Electronic Payment Protocols. In: Pro-
ceedings of the 1st USENIX Workshop on Electronic Commerce (1995)
11. Bellare, M., Garay, J., Hauser, R., Herzberg, A., Krawczyk, H., Steiner, M., Tsudik,
G., Van Herreweghen, E., Waidner, M.: Design, implementation, and deployment
of the iKP secure electronic payment system. IEEE Journal on selected areas in
communications 18(4), 611–627 (2000)
12. Maurer, U., Schmid, P.: A calculus for secure channel establishment in open net-
works. In: Gollmann, D. (ed.) ESORICS 1994. LNCS, vol. 875, p. 175. Springer,
Heidelberg (1994)
13. van Doorn, L., Abadi, M., Burrows, M., Wobber, E.: Secure network objects.
In: Secure Internet Programming, pp. 395–412 (1999)
14. Kelsey, J., Schneier, B., Wagner, D.: Protocol interactions and the chosen protocol
attack. LNCS, pp. 91–104. Springer, Heidelberg (1998)
15. Lowe, G.: A hierarchy of authentication speciﬁcations, pp. 31–43. IEEE Computer
Society Press, Los Alamitos (1997)
www.ebook3000.com

AnBx - Security Protocols Design and Veriﬁcation
183
16. Cramer, R., Shoup, V.: Design and analysis of practical public-key encryption
schemes secure against adaptive chosen ciphertext attack. SIAM Journal on Com-
puting(Print) 33(1), 167–226 (2004)
17. Abadi, M., Needham, R.: Prudent engineering practice for cryptographic proto-
cols. In: Proceedings of 1994 IEEE Computer Society Symposium on Research in
Security and Privacy, pp. 122–136 (1994)
18. M¨odersheim, S.: Algebraic properties in alice and bob notation. In: International
Conference on Availability, Reliability and Security, pp. 433–440 (2009)
19. Backes, M., Hritcu, C., Maﬀei, M.: Type-checking zero-knowledge. In: Ning, P.,
Syverson, P.F., Jha, S. (eds.) ACM Conference on Computer and Communications
Security, pp. 357–370. ACM, New York (2008)
20. Corin, R., Deni´elou, P.M., Fournet, C., Bhargavan, K., Leifer, J.J.: A secure com-
piler for session abstractions. Journal of Computer Security 16(5), 573–636 (2008)
21. Bella, G., Massacci, F., Paulson, L.: An overview of the veriﬁcation of SET. Inter-
national Journal of Information Security 4(1), 17–28 (2005)
22. Visa: Visa 3-D Secure Speciﬁcations. Technical report (2002)
23. O’Mahony, D., Peirce, M., Tewari, H.: Electronic payment systems for e-commerce.
Artech House Publishers (2001)
24. Ogata, K., Futatsugi, K.: Formal analysis of the iKP electronic payment protocols.
LNCS, pp. 441–460. Springer, Heidelberg (2003)
25. Abadi, M., Fournet, C., Gonthier, G.: Secure implementation of channel abstrac-
tions. Information and computation(Print) 174(1), 37–83 (2002)
26. Abadi, M., Fournet, C.: Private authentication. Theor. Comput. Sci. 322(3),
427–476 (2004)
27. Bugliesi, M., Giunti, M.: Secure implementations of typed channel abstractions.
In: Hofmann, M., Felleisen, M. (eds.) POPL, pp. 251–262. ACM, New York (2007)
28. Guttman, J.: Security protocol design via authentication tests. In: Proceedings
of 15th IEEE Computer Security Foundations Workshop. IEEE Computer, Los
Alamitos (2002)
29. Guttman, J., Herzog, J., Ramsdell, J., Sniﬀen, B.: Programming cryptographic
protocols. In: De Nicola, R., Sangiorgi, D. (eds.) TGC 2005. LNCS, vol. 3705,
pp. 116–145. Springer, Heidelberg (2005)

184
M. Bugliesi and P. Modesti
Appendix
Revised iKP Narration
The ideal narration of the revised iKP protocol given in Section 5 is, basically,
the following:
– Step 1 and 2: customer and merchant exchange data that let them build
independently the contract. They must tell their “own version of the story”
to the acquirer A. Customer C declares [can : C], the credit card will be
used in contract, sending the protected digest of can to the merchant M. The
customer also informs (and agrees with) the merchant on the digest of desc
they will use to deﬁne the contract. The merchant M generates a (fresh)
transaction ID (tid) and the date of transaction (C and M already had
agreed on price and desc, being part of their initial knowledge). C can verify
the integrity of [desc : M], form the contract, and then compute its digest.
The tuple (price, tid, date, [contract]) is sent to C which, upon receiving it,
can compute [contract] and verify that it matches the digest provided by M.
If the match succeeds, the protocol execution continues, otherwise it stops.
– Step 3: the customer prepares a secret message for the acquirer containing
the information necessary to complete the transaction: contract, credit card
number (can), amount of the transaction (price) and transaction ID (tid).
However this message is sent to the merchant and not to the acquirer, be-
cause this protocol does not allow direct interaction between customers and
acquirer, but only through merchant mediation. We assume that the mer-
chant M is cooperating in delivering messages. Hence M receives an opaque
message, and all it can do is to blindly forward it to the acquirer A (step 4a).
– Step 4: the merchant M sends the tuple price, tid, date, [desc : M], [contract]
to the acquirer (step 4b). This information is necessary to complete the pay-
ment. In particular date and [desc : M] are required by the acquirer to
compute independently the digest of contract. Upon reception of the two
versions of [contract] originating from the two other principals, the acquirer
can also compute the same value autonomously. If all three match, the trans-
action can be authorized, since this is the proof of the complete agreement
between the customer and the merchant.
– Step 5: the acquirer A sends the authorization response to the merchant,
within a fresh authentic message, containing also tid and [contract]. This
is done in order to bind all the information, and produce a proof that the
payment has been authorized for that speciﬁc transaction and that speciﬁc
contract.
– Step 6: the merchant M forwards the message received from the acquirer A
to the customer C. This is a notiﬁcation of the result of the transaction. In
this way C receives, via the merchant, a proof of payment from the acquirer.
Since the message is signed by the acquirer, the merchant cannot alter the
message without being discovered.
www.ebook3000.com

Author Index
Alvim, M´ario S.
111
Andr´es, Miguel E.
111
Bugliesi, Michele
23, 164
Caires, Lu´ıs
59
Calzavara, Stefano
23
Centenaro, Matteo
130
Costa, Gabriele
41
Degano, Pierpaolo
41
Dimkov, Trajce
112
Falcone, Andrea
77
Focardi, Riccardo
77, 95, 130
Gao, Han
1
Hartel, Pieter
112
Jaggard, Aaron D.
2
Kourjieh, Mounira
146
Kremer, Steve
146
Luccio, Flaminia L.
95
Macedonio, Damiano
23
Martinelli, Fabio
41
Meadows, Catherine
2
Mislove, Michael
2
Modesti, Paolo
164
Nielson, Flemming
1
Nielson, Hanne Riis
1
Palamidessi, Catuscia
111
Pieters, Wolter
112
Pires, M´ario
59
Ryan, Mark
146
Segala, Roberto
2
Smyth, Ben
146


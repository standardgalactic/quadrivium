-1
0
1
0
-1
-2
-3
2
0
-2
x
y
z
a
L a r r y  T u r y n
A D V A N C E D
ENGINEERING
MATHEMATICS   


CRC Press is an imprint of the
Taylor & Francis Group, an informa business
Boca Raton   London   New York
L a r r y  T u r y n
A D V A N C E D
ENGINEERING
MATHEMATICS   

MATLAB® is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not warrant the 
accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB® software or related products 
does not constitute endorsement or sponsorship by The MathWorks of a particular pedagogical approach or particular 
use of the MATLAB® software.
CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2014 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20130801
International Standard Book Number-13: 978-1-4822-1939-5 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid-
ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Preface..............................................................................................
xix
Acknowledgments ...............................................................................
xxv
1.
Linear Algebraic Equations, Matrices, and Eigenvalues ............................
1
1.1
Solving Systems and Row Echelon Forms ........................................
1
1.1.1
Matrices ........................................................................
2
1.1.2
Augmented Matrices .........................................................
5
1.1.3
Row Reduced Echelon Form (RREF) .......................................
6
1.1.4
Problems .......................................................................
13
1.2
Matrix Addition, Multiplication, and Transpose.................................
16
1.2.1
Special Kinds of Matrices....................................................
21
1.2.2
Powers of a Matrix............................................................
23
1.2.3
Transpose ......................................................................
24
1.2.4
Elementary Matrices .........................................................
24
1.2.5
Problems .......................................................................
25
1.3
Homogeneous Systems, Spanning Set, and Basic Solutions....................
26
1.3.1
Problems .......................................................................
32
1.4
Solutions of Nonhomogeneous Systems ..........................................
33
1.4.1
Problems .......................................................................
36
1.5
Inverse Matrix .........................................................................
37
1.5.1
Row Reduction Algorithm for Constructing the Inverse ...............
40
1.5.2
Inverse of a Partitioned Matrix .............................................
43
1.5.3
Problems .......................................................................
44
1.6
Determinant, Adjugate Matrix, and Cramer’s Rule .............................
47
1.6.1
Adjugate Matrix ..............................................................
53
1.6.2
Cramer’s Rule .................................................................
54
1.6.3
Problems .......................................................................
57
1.7
Linear Independence, Basis and Dimension ......................................
61
1.7.1
Linear Independence .........................................................
62
1.7.2
Vector Spaces and Subspaces ...............................................
65
1.7.3
Problems .......................................................................
69
Key Terms.....................................................................................
71
MATLAB® Commands.....................................................................
72
Mathematica Commands ....................................................................
72
References .....................................................................................
73
2.
Matrix Theory................................................................................
75
2.1
Eigenvalues and Eigenvectors ......................................................
75
2.1.1
The Adjugate Matrix Method for Finding an Eigenvector .............
79
2.1.2
Complex Numbers ...........................................................
82
2.1.3
Complex Eigenvalues and Eigenvectors ..................................
83
2.1.4
Eigenvalues and Eigenvectors of Triangular
and Diagonal Matrices .......................................................
85
iii

iv
Contents
2.1.5
MATLAB® and MathematicaTM.............................................
86
2.1.6
Problems .......................................................................
87
2.2
Basis of Eigenvectors and Diagonalization........................................
91
2.2.1
Diagonalizing a Matrix ......................................................
95
2.2.2
Deficient Eigenvalues ........................................................
98
2.2.3
Problems .......................................................................
99
2.3
Inner Product and Orthogonal Sets of Vectors ................................... 101
2.3.1
Orthogonal Set of Vectors ................................................... 105
2.3.2
The Gram–Schmidt Process ................................................. 106
2.3.3
Orthogonal Projections ...................................................... 109
2.3.4
Problems ....................................................................... 112
2.4
Orthonormal Bases and Orthogonal Matrices .................................... 114
2.4.1
Orthogonal Sets and Bases .................................................. 114
2.4.2
Orthogonal Matrices ......................................................... 116
2.4.3
Appendix ...................................................................... 118
2.4.4
Problems ....................................................................... 119
2.5
Least Squares Solutions .............................................................. 121
2.5.1
The Normal Equations ....................................................... 123
2.5.1.1
Least Squares Solution and Orthogonal Matrices ............ 127
2.5.2
Problems ....................................................................... 129
2.6
Symmetric Matrices, Definite Matrices, and Applications...................... 131
2.6.1
A Spectral Theorem .......................................................... 131
2.6.1.1
A Spectral Formula ............................................... 135
2.6.1.2
Positive Definite and Positive Semi-Definite Matrices ...... 136
2.6.1.3
Application to A2, A−1,
√
A ...................................... 137
2.6.1.4
Application to Least Squares Solutions ........................ 138
2.6.2
Further Study of Positive Definite Matrices .............................. 139
2.6.2.1
Vibrations and the Generalized Eigenvalue Problem ....... 140
2.6.2.2
Positive Definiteness and Determinants....................... 141
2.6.3
Problems ....................................................................... 146
2.7
Factorizations: QR and SVD......................................................... 148
2.7.1
QR Factorization .............................................................. 148
2.7.2
QR and Solving Systems..................................................... 150
2.7.3
QR and Least Squares Solutions............................................ 151
2.7.4
SVD ............................................................................. 151
2.7.5
SVD and L.S.S. ................................................................ 158
2.7.6
Moore–Penrose Generalized Inverse ...................................... 159
2.7.7
Problems ....................................................................... 163
2.8
Factorizations: LU and Cholesky ................................................... 165
2.8.1
LU Factorizations ............................................................. 165
2.8.2
Cholesky Factorizations ..................................................... 168
2.8.3
Problems ....................................................................... 169
2.9
Rayleigh Quotient..................................................................... 170
2.9.1
A Rayleigh Theorem ......................................................... 172
2.9.2
Problems ....................................................................... 174
2.10 Short Take: Inner Product and Hilbert Spaces.................................... 175
2.10.1 Linear Functionals and Operators ......................................... 177
2.10.2 Norm and Bounded Linear Operators .................................... 179
2.10.3 Convergence, Cauchy Completeness, and Hilbert Spaces ............. 183

Contents
v
2.10.4 Bounded Linear Functionals and Operator Adjoint..................... 186
2.10.5 Application to Signal Restoration .......................................... 187
2.10.6 Projection and Minimization................................................ 188
2.10.7 Weak Convergence and Compactness .................................... 189
2.10.8 Problems ....................................................................... 191
Key Terms..................................................................................... 192
MATLAB® Commands..................................................................... 194
Mathematica Commands .................................................................... 194
References ..................................................................................... 194
3.
Scalar ODEs I: Homogeneous Problems ................................................ 195
3.1
Linear First-Order ODEs............................................................. 195
3.1.1
Scalar ODEs ................................................................... 195
3.1.2
Linear First-Order ODEs .................................................... 196
3.1.3
Steady-State and Transient Solutions...................................... 202
3.1.4
Problems ....................................................................... 205
3.2
Separable and Exact ODEs .......................................................... 209
3.2.1
Separable ODEs ............................................................... 209
3.2.2
Exact ODEs .................................................................... 211
3.2.3
Existence of Solution(s) of an IVP .......................................... 215
3.2.4
Problems ....................................................................... 219
3.3
Second-Order Linear Homogeneous ODEs ....................................... 222
3.3.1
Spring–Mass–Damper Systems............................................. 222
3.3.2
Series RLC Circuit ............................................................ 225
3.3.3
The Underdamped Case ..................................................... 230
3.3.4
The Amplitude and Phase Form ........................................... 232
3.3.5
Figures of Merit in Graphs of Underdamped Solutions ................ 235
3.3.6
The Critically Damped Case ................................................ 237
3.3.7
The Wronskian Determinant................................................ 238
3.3.8
Problems ....................................................................... 239
3.4
Higher-Order Linear ODEs.......................................................... 244
3.4.1
The Zoo of Solutions of LCCHODEs ...................................... 250
3.4.2
Differential Operator Notation ............................................. 251
3.4.3
Shift Theorem ................................................................. 253
3.4.4
Problems ....................................................................... 254
3.5
Cauchy–Euler ODEs .................................................................. 255
3.5.1
Problems ....................................................................... 260
Key Terms..................................................................................... 261
Mathematica Command...................................................................... 262
Reference ...................................................................................... 262
4.
Scalar ODEs II: Nonhomogeneous Problems .......................................... 263
4.1
Nonhomogeneous ODEs ............................................................ 263
4.1.1
Special Case: RHS from the Zoo and Constant
Coefficients on LHS .......................................................... 264
4.1.2
The Method of Coefficients to Be Determined ........................... 265
4.1.3
Justification for the Method ................................................. 269
4.1.4
Using a Shift Theorem ....................................................... 270
4.1.5
Problems ....................................................................... 271

vi
Contents
4.2
Forced Oscillations.................................................................... 273
4.2.1
The Resonance Case .......................................................... 274
4.2.2
Steady-State Solution, Frequency Response, and
Practical Resonance........................................................... 276
4.2.3
Maximum Frequency Response ............................................ 281
4.2.4
Beats Phenomenon, Fast and Slow Frequencies, and
Frequency Response.......................................................... 283
4.2.5
Problems ....................................................................... 287
4.3
Variation of Parameters .............................................................. 291
4.3.1
Method of Variation of Parameters ........................................ 294
4.3.2
Problems ....................................................................... 298
4.4
Laplace Transforms: Basic Techniques ............................................ 299
4.4.1
Problems ....................................................................... 305
4.5
Laplace Transforms: Unit Step and Other Techniques .......................... 307
4.5.1
Writing a Function in Terms of Step Function(s) ........................ 308
4.5.2
Graph of a Solution of an ODE Involving a Step Function ............. 310
4.5.3
Convolution ................................................................... 312
4.5.4
Convolution and Particular Solutions ..................................... 314
4.5.5
Delta “Functions”............................................................. 318
4.5.6
Laplace Transform of a Periodic Function ................................ 321
4.5.7
Remarks ........................................................................ 321
4.5.8
Problems ....................................................................... 321
4.6
Scalar Difference Equations ......................................................... 323
4.6.1
General Solution and the Casorati Determinant ......................... 330
4.6.2
Nonhomogeneous Linear Difference Equation .......................... 334
4.6.3
The Method of Undetermined Coefficients............................... 334
4.6.4
Problems ....................................................................... 338
4.7
Short Take: z-Transforms ............................................................ 340
4.7.1
Sinusoidal Signals ............................................................ 344
4.7.2
Steady-State Solution......................................................... 345
4.7.3
Convolution and z-Transforms ............................................. 348
4.7.4
Transfer Function ............................................................. 348
4.7.5
Problems ....................................................................... 349
Key Terms..................................................................................... 349
References ..................................................................................... 351
5.
Linear Systems of ODEs ................................................................... 353
5.1
Systems of ODEs ...................................................................... 353
5.1.1
Systems of Second-Order Equations ....................................... 357
5.1.2
Compartment Models........................................................ 358
5.1.3
Problems ....................................................................... 360
5.2
Solving Linear Homogenous Systems of ODEs .................................. 362
5.2.1
Fundamental Matrix and etA ................................................ 368
5.2.2
Equivalence of Second-Order LCCHODE and LCCHS in R2 .......... 372
5.2.3
Maclaurin Series for etA ...................................................... 375
5.2.4
Nonconstant Coefficients .................................................... 376
5.2.5
Problems ....................................................................... 377
5.3
Complex or Deficient Eigenvalues ................................................. 381

Contents
vii
5.3.1
Complex Eigenvalues ........................................................ 381
5.3.2
Solving Homogeneous Systems of Second-Order Equations .......... 385
5.3.3
Deficient Eigenvalues ........................................................ 387
5.3.4
Laplace Transforms and etA ................................................ 390
5.3.5
Stability......................................................................... 391
5.3.6
Problems ....................................................................... 392
5.4
Nonhomogeneous Linear Systems ................................................. 395
5.4.1
Problems ....................................................................... 401
5.5
Nonresonant Nonhomogeneous Systems ......................................... 403
5.5.1
Sinusoidal Forcing ............................................................ 408
5.5.2
Problems ....................................................................... 411
5.6
Linear Control Theory: Complete Controllability................................ 412
5.6.1
Some Other Control Problems .............................................. 419
5.6.2
Problems ....................................................................... 421
5.7
Linear Systems of Difference Equations ........................................... 422
5.7.1
Color Blindness ............................................................... 423
5.7.2
General Solution and the Casorati Determinant ......................... 425
5.7.3
Complex Eigenvalues ........................................................ 427
5.7.4
Equivalence of Second-Order Scalar Difference Equation
and a System in R2 ........................................................... 428
5.7.5
Ladder Network Electrical Circuits ........................................ 429
5.7.6
Stability......................................................................... 434
5.7.7
Problems ....................................................................... 436
5.8
Short Take: Periodic Linear Differential Equations .............................. 439
5.8.1
The Stroboscopic, or “Return,” Map ....................................... 441
5.8.2
Floquet Representation ...................................................... 442
5.8.3
Stability......................................................................... 445
5.8.4
Hill’s Equation ................................................................ 447
5.8.5
Periodic Solution of a Nonhomogeneous
ODE System ................................................................... 448
5.8.6
Problems ....................................................................... 451
Key Terms..................................................................................... 454
MATLAB® Commands..................................................................... 455
References ..................................................................................... 455
6.
Geometry, Calculus, and Other Tools ................................................... 457
6.1
Dot Product, Cross Product, Lines, and Planes................................... 457
6.1.1
Dot Product and Cross Product ............................................ 457
6.1.2
Lines ............................................................................ 459
6.1.3
Planes........................................................................... 459
6.1.4
Problems ....................................................................... 461
6.2
Trigonometry, Polar, Cylindrical, and
Spherical Coordinates ................................................................ 463
6.2.1
Cylindrical Coordinates ..................................................... 465
6.2.2
Spherical Coordinates........................................................ 465
6.2.3
Right-Handed Orthogonal Bases for R3 ................................... 467
6.2.4
Orthonormal Basis in Spherical Coordinates ............................. 468
6.2.5
Relationships to the Standard o.n. Basis .................................. 470
6.2.6
Problems ....................................................................... 471

viii
Contents
6.3
Curves and Surfaces .................................................................. 471
6.3.1
Curves and Calculus ......................................................... 474
6.3.2
Zhukovskii Airfoil ............................................................ 477
6.3.3
Surfaces ........................................................................ 478
6.3.4
Problems ....................................................................... 482
6.4
Partial Derivatives .................................................................... 485
6.4.1
Linear Approximation ....................................................... 486
6.4.2
Multivariable Chain Rules .................................................. 489
6.4.3
Gradient Vector in R3 ........................................................ 492
6.4.4
Scalar Potential Functions ................................................... 493
6.4.5
Problems ....................................................................... 495
6.5
Tangent Plane and Normal Vector ................................................. 498
6.5.1
Problems ....................................................................... 504
6.6
Area, Volume, and Linear Transformations ...................................... 505
6.6.1
Linear Transformations ...................................................... 511
6.6.2
Linear Transformations, Area, and Volume .............................. 514
6.6.3
Change of Variables, Area, and Volume .................................. 516
6.6.4
Element of Surface Area ..................................................... 519
6.6.5
Problems ....................................................................... 520
6.7
Differential Operators and Curvilinear Coordinates ............................ 522
6.7.1
Properties of the Operators grad, div, and curl ........................... 524
6.7.2
Curvilinear Coordinates ..................................................... 525
6.7.3
Differential Operators in Curvilinear Coordinates ...................... 529
6.7.4
Summary of Operators in Cylindrical Coordinates ..................... 533
6.7.5
Summary of Operators in Spherical Coordinates........................ 533
6.7.6
Problems ....................................................................... 534
6.8
Rotating Coordinate Frames ........................................................ 537
6.8.1
ODEs Describing Rotation .................................................. 537
6.8.2
Velocity and Acceleration ................................................... 540
6.8.3
Velocity and Acceleration in a Rotating Frame Whose
Origin Is Moving.............................................................. 543
6.8.4
Problems ....................................................................... 544
Key Terms..................................................................................... 546
Mathematica Command...................................................................... 548
Reference ...................................................................................... 548
7.
Integral Theorems, Multiple Integrals, and Applications ........................... 549
7.1
Integrals for a Function of a Single Variable ...................................... 549
7.1.1
Improper Integrals............................................................ 553
7.1.2
Problems ....................................................................... 554
7.2
Line Integrals .......................................................................... 555
7.2.1
Line Integrals of Vector-Valued Functions ............................... 560
7.2.2
Fundamental Theorem of Line Integrals .................................. 563
7.2.3
Path Direction ................................................................. 565
7.2.4
Other Notations ............................................................... 565
7.2.5
Problems ....................................................................... 567
7.3
Double Integrals, Green’s Theorem, and Applications.......................... 570
7.3.1
Double Integral as Volume.................................................. 574
7.3.2
Polar Coordinates............................................................. 580

Contents
ix
7.3.3
Green’s Theorem.............................................................. 582
7.3.4
Comparison with Single Variable Calculus............................... 584
7.3.5
Green’s Theorem for an Annulus .......................................... 585
7.3.6
Green’s Theorem in Polar Coordinates.................................... 586
7.3.7
Problems ....................................................................... 589
7.4
Triple Integrals and Applications .................................................. 594
7.4.1
Cylindrical Coordinates ..................................................... 597
7.4.2
Spherical Coordinates........................................................ 602
7.4.3
Problems ....................................................................... 605
7.5
Surface Integrals and Applications................................................. 607
7.5.1
Surface Integral of a Scalar-Valued Function............................. 611
7.5.2
Surface Integral of a Vector Field........................................... 613
7.5.3
Problems ....................................................................... 617
7.6
Integral Theorems: Divergence, Stokes, and Applications ..................... 621
7.6.1
The Divergence Theorem in R2 ............................................. 623
7.6.2
Euler’s Conservation Equation ............................................. 624
7.6.3
Stokes’ Theorem .............................................................. 627
7.6.4
Problems ....................................................................... 632
7.7
Probability Distributions............................................................. 635
7.7.1
Joint Distribution ............................................................. 643
7.7.2
Problems ....................................................................... 645
Key Terms..................................................................................... 645
Mathematica Commands .................................................................... 647
Reference ...................................................................................... 647
8.
Numerical Methods I ....................................................................... 649
8.1
Solving a Scalar Equation ............................................................ 649
8.1.1
Newton–Raphson Method .................................................. 649
8.1.2
Modified Newton’s Method ................................................ 653
8.1.3
Secant Method ................................................................ 654
8.1.4
Fixed Point Problem Iteration .............................................. 655
8.1.5
Aitken’s δ2 Method ........................................................... 656
8.1.6
Newton’s Method versus Other Methods ................................ 657
8.1.7
Trouble ......................................................................... 658
8.1.8
Problems ....................................................................... 658
8.2
Solving a System of Equations ...................................................... 661
8.2.1
Newton’s Method in Rn ..................................................... 661
8.2.2
Newton–Kantorovich Theorem ............................................ 664
8.2.3
Fixed Point Problem Iteration .............................................. 666
8.2.4
Secant Method for Systems of Equations ................................. 666
8.2.5
Problems ....................................................................... 668
8.3
Approximation of Integrals ......................................................... 669
8.3.1
Approximation Error for Quadrature Rules.............................. 672
8.3.2
Iteration of Quadrature ...................................................... 674
8.3.3
Theory versus Practice ....................................................... 675
8.3.4
Problems ....................................................................... 676
8.4
Numerical Solution of Ax = b....................................................... 677

x
Contents
8.4.1
Partial Pivoting in the Gauss–Jordan Algorithm......................... 679
8.4.2
Iterative Methods for Solving Ax = b ..................................... 682
8.4.4
Problems ....................................................................... 686
8.5
Linear Algebraic Eigenvalue Problems ............................................ 687
8.5.1
Elementary Method .......................................................... 687
8.5.2
Power Methods ............................................................... 688
8.5.3
Deflation and Similarity ..................................................... 690
8.5.4
Using Similarity Transformations.......................................... 690
8.5.5
Background .................................................................... 691
8.5.6
QR Algorithm ................................................................. 693
8.5.7
Problems ....................................................................... 694
8.6
Approximations of Derivatives ..................................................... 695
8.6.1
Problems ....................................................................... 699
8.7
Approximate Solutions of ODE-IVPs .............................................. 699
8.7.1
Runge–Kutta Methods ....................................................... 704
8.7.2
Multistep Methods ........................................................... 705
8.7.3
Predictor–Corrector Methods............................................... 706
8.7.4
Systems of ODEs.............................................................. 706
8.7.5
Taylor’s Formula Method ................................................... 706
8.7.6
Numerical Instability and Stiffness ........................................ 707
8.7.7
Problems ....................................................................... 709
8.8
Approximate Solutions of Two Point BVPs....................................... 712
8.8.1
An ODE-BVP Eigenvalue Problem ........................................ 715
8.8.2
Using IVP Numerical Methods to Solve BVPs ........................... 716
8.8.3
Finding Periodic Solutions of Linear Problems .......................... 717
8.8.4
Problems ....................................................................... 718
8.9
Splines .................................................................................. 719
8.9.1
Cubic B-Splines ............................................................... 722
8.9.2
Nonuniform Splines.......................................................... 727
8.9.3
Spline Approximation of a Curve in Rn ................................... 729
8.9.4
Surface Splines ................................................................ 731
8.9.5
Triangular Surface Patches .................................................. 731
8.9.6
Problems ....................................................................... 733
Key Terms..................................................................................... 733
MATLAB® Commands..................................................................... 736
Mathematica Command...................................................................... 736
References ..................................................................................... 736
9.
Fourier Series ................................................................................ 737
9.1
Orthogonality and Fourier Coefficients ........................................... 737
9.1.1
Introduction ................................................................... 737
9.1.2
Convergence of Fourier Series .............................................. 739
9.1.3
Orthogonality and Calculating Fourier Coefficients .................... 741
9.1.4
Even and Odd Functions and Their Fourier
Series Coefficients ............................................................ 744
9.1.5
Finding the Fourier Series in a Special Case .............................. 748
9.1.6
Periodic Extension of a Function Given for 0 < x < 2L ................. 748
9.1.7
Other Kinds of Fourier Series ............................................... 751
9.1.8
Problems ....................................................................... 752

Contents
xi
9.2
Fourier Cosine and Sine Series ...................................................... 755
9.2.1
Fourier Cosine Series........................................................ 756
9.2.2
Fourier Sine Series ........................................................... 758
9.2.3
Fourier Analysis and Oscillations ......................................... 762
9.2.4
Problems ...................................................................... 765
9.3
Generalized Fourier Series........................................................... 767
9.3.1
Other Boundary Conditions ............................................... 770
9.3.2
Periodic Boundary Conditions and the Full Fourier Series ........... 775
9.3.3
Problems ...................................................................... 776
9.4
Complex Fourier Series and Fourier Transform.................................. 777
9.4.1
The Fourier Transform...................................................... 780
9.4.2
Convolution .................................................................. 785
9.4.3
Problems ...................................................................... 788
9.5
Discrete Fourier and Fast Fourier Transforms.................................... 790
9.5.1
Convolution and Auto-Correlation ....................................... 798
9.5.2
Fast Fourier Transform ..................................................... 799
9.5.3
Problems ...................................................................... 803
9.6
Sturm–Liouville Problems ........................................................... 805
9.6.1
Other Sturm–Liouville Problems.......................................... 808
9.6.2
A Composite Media Problem .............................................. 810
9.6.3
Fourth-Order ODE-BVP .................................................... 813
9.6.4
Problems ...................................................................... 815
9.7
Rayleigh Quotient..................................................................... 818
9.7.1
Problems ...................................................................... 822
9.8
Parseval’s Theorems and Applications ............................................ 824
9.8.1
Best Approximation by a Partial Sum of a Fourier Series ............. 827
9.8.2
Complex Fourier Series ..................................................... 829
9.8.3
Fourier Transforms .......................................................... 830
9.8.4
Problems ...................................................................... 831
Key Terms..................................................................................... 831
Mathematica Command...................................................................... 833
References ..................................................................................... 833
10. Partial Differential Equations Models .................................................. 835
10.1 Integral and Partial Differential Equations........................................ 835
10.1.1
Maxwell’s Equations of Electromagnetism .............................. 838
10.1.2
Continuum Mechanics...................................................... 840
10.1.3
Problems ...................................................................... 845
10.2 Heat Equations ........................................................................ 846
10.2.1
Steady-State Temperature .................................................. 849
10.2.2
Lower Dimensional Problems ............................................. 849
10.2.3
Composite Rod............................................................... 854
10.2.4
Problems ...................................................................... 855
10.3 Potential Equations ................................................................... 860
10.3.1
Magnetostatics ............................................................... 860
10.3.2
Boundary Conditions ....................................................... 861
10.3.3
Properties of Solutions ...................................................... 861
10.3.4
Problems ...................................................................... 865

xii
Contents
10.4 Wave Equations ....................................................................... 868
10.4.1
Guitar String.................................................................. 868
10.4.2
Vibrating String .............................................................. 869
10.4.3
Speed of Sound .............................................................. 872
10.4.4
Linear Elasticity .............................................................. 876
10.4.5
Linear Elastostatics .......................................................... 879
10.4.6
Problems ...................................................................... 880
10.5 D’Alembert Wave Solutions......................................................... 882
10.5.1
Zero Initial Velocity ......................................................... 883
10.5.2
Writing the Solution Using Step Functions .............................. 888
10.5.3
Support........................................................................ 888
10.5.4
Zero Initial Displacement .................................................. 889
10.5.5
Problems ...................................................................... 895
10.6 Short Take: Conservation of Energy in a Finite String .......................... 897
10.6.1
Problems ...................................................................... 900
Key Terms..................................................................................... 900
Reference ...................................................................................... 902
11. Separation of Variables for PDEs ........................................................ 903
11.1 Heat Equation in One Space Dimension........................................... 903
11.1.1
Easy Initial Conditions...................................................... 912
11.1.2
Composite Rod............................................................... 913
11.1.3
Time-Dependent Boundary Conditions.................................. 914
11.1.4
Problems ...................................................................... 918
11.2 Wave Equation in One Space Dimension ......................................... 924
11.2.1
Problems ...................................................................... 928
11.3 Laplace Equation in a Rectangle .................................................... 932
11.3.1
Using Clairvoyance to Choose Alternatives to cosh
 nπx
L

and sinh
 nπx
L

................................................................ 937
11.3.2
Contour Plot and 3D Plot Using Mathematica ........................... 940
11.3.3
Problems ...................................................................... 946
11.4 Eigenvalues of the Laplacian and Applications .................................. 948
11.4.1
Application to Time-Dependent Heat Flow in a Slab .................. 951
11.4.2
Special Case .................................................................. 952
11.4.3
Application to Transverse Vibrations of a
Rectangular Membrane ..................................................... 952
11.4.4
Application to Steady-State Temperature in a Slab with a
Steady Source or Sink ....................................................... 953
11.4.5
Application to Surface Waves ............................................. 954
11.4.6
Problems ...................................................................... 956
11.5 PDEs in Polar Coordinates .......................................................... 958
11.5.1
Laplace Equation in Polar Coordinates .................................. 958
11.5.2
Heat Equation in Polar Coordinates ...................................... 967
11.5.3
Problems ...................................................................... 969
11.6 PDEs in Cylindrical and Spherical Coordinates.................................. 973
11.6.1
Spherical Coordinates....................................................... 981
11.6.2
Polar Coordinates Again ................................................... 985
11.6.3
Problems ...................................................................... 986

Contents
xiii
Key Terms..................................................................................... 989
Mathematica Commands .................................................................... 989
References ..................................................................................... 989
12. Numerical Methods II ...................................................................... 991
12.1 Finite Difference Methods for Heat Equations ................................... 991
12.1.1
Incompatibility of Initial Condition with
Boundary Conditions ....................................................... 996
12.1.2
Time-Dependent Boundary Conditions.................................. 997
12.1.3
Other Boundary Conditions ............................................... 997
12.1.4
Nonlinearity .................................................................. 998
12.1.5
Problems ......................................................................1000
12.2 Numerical Stability ...................................................................1001
12.2.1
Crank–Nicholson Method..................................................1005
12.2.2
Problems ......................................................................1007
12.3 Finite Difference Methods for Potential Equations ..............................1007
12.3.1
Other Boundary Conditions ...............................................1010
12.3.2
Problems ......................................................................1011
12.4 Finite Difference Methods for the Wave Equation ...............................1014
12.4.1
Scalar Hyperbolic Problem.................................................1014
12.4.2
Lax Scheme ...................................................................1016
12.4.3
Problems ......................................................................1017
12.5 Short Take: Galerkin Method .......................................................1018
12.5.1
A Generalization of the Galerkin Method ...............................1021
12.5.2
The Galerkin Method for PDEs............................................1022
12.5.3
Nonlinear Problems .........................................................1023
12.5.4
Problems ......................................................................1024
Key Terms.....................................................................................1025
Reference ......................................................................................1025
13. Optimization .................................................................................1027
13.1 Functions of a Single Variable.......................................................1027
13.1.1
Global Optimization Result ................................................1029
13.1.2
Convex Functions and Optimization .....................................1029
13.1.3
Problems ......................................................................1030
13.2 Functions of Several Variables ......................................................1032
13.2.1
Global Optimization and Lagrange Multipliers ........................1038
13.2.2
Numerical Minimization and Steepest Descent Methods .............1045
13.2.3
Problems ......................................................................1046
13.3 Linear Programming Problems .....................................................1048
13.3.1
Slack Variables and Standard Form ......................................1050
13.3.2
Application: Structural Optimization ....................................1054
13.3.3
Problems ......................................................................1054
13.4 Simplex Procedure ....................................................................1055
13.4.1
Unit Cost Reduction.........................................................1056
13.4.2
Problems ......................................................................1060

xiv
Contents
13.5 Nonlinear Programming.............................................................1062
13.5.1
Dual LP Problem ............................................................1068
13.5.2
Application: Geometric Tolerancing......................................1071
13.5.3
Problems ......................................................................1072
13.6 Rayleigh–Ritz Method ...............................................................1073
13.6.1
Other Eigenvalues ...........................................................1075
13.6.2
Hilbert Space Eigenvalue Problems ......................................1077
13.6.3
Problems ......................................................................1079
Key Terms.....................................................................................1079
Mathematica Commands ....................................................................1080
References .....................................................................................1080
14. Calculus of Variations......................................................................1081
14.1 Minimization Problems ..............................................................1081
14.1.1
The Rayleigh–Ritz Method.................................................1085
14.1.2
Problems ......................................................................1087
14.2 Necessary Conditions ................................................................1088
14.2.1
Euler–Lagrange Equations .................................................1089
14.2.2
Natural Boundary Condition ..............................................1094
14.2.3
Hamilton’s Principle ........................................................1095
14.2.4
Hamilton’s Principle for Continuous Media ............................1097
14.2.5
Problems ......................................................................1098
14.3 Problems with Constraints ..........................................................1100
14.3.1
Differential Equation Constraints .........................................1103
14.3.2
Problems ......................................................................1105
14.4 Eigenvalue Problems .................................................................1106
14.4.1
An ODE-BVP .................................................................1106
14.4.2
An Eigenvalue Problem for the Laplacian ...............................1108
14.4.3
Sturm–Liouville Problem...................................................1109
14.4.4
Problems ......................................................................1110
14.5 Short Take: Finite Element Methods ...............................................1111
14.5.1
Mathematica Commands ....................................................1113
14.5.2
Rayleigh–Ritz, Galerkin, and Least Squares .............................1116
14.5.3
Finite Elements for PDEs ...................................................1118
14.5.4
Problems ......................................................................1118
Key Terms.....................................................................................1119
Mathematica Commands ....................................................................1120
References .....................................................................................1121
15. Functions of a Complex Variable ........................................................1123
15.1 Complex Numbers, Roots, and Functions ........................................1123
15.1.1
Polar Forms...................................................................1123
15.1.2
Roots...........................................................................1127
15.1.3
Functions......................................................................1130
15.1.4
Problems ......................................................................1133
15.2 Derivative and the Cauchy–Riemann Equations.................................1134
15.2.1
Derivatives....................................................................1138
15.2.2
Cauchy–Riemann Equations ...............................................1140

Contents
xv
15.2.3
Orthogonal Families of Curves and an Application
to Fluid Flow ..............................................................1143
15.2.4
Appendix...................................................................1145
15.2.5
Problems ...................................................................1145
15.3
Analyticity, Harmonic Function, and Harmonic Conjugate ..................1147
15.3.1
Harmonic Functions......................................................1150
15.3.2
Harmonic Conjugate .....................................................1152
15.3.3
Problems ...................................................................1155
15.4
Elementary Functions ...............................................................1157
15.4.1
Arg(z) .......................................................................1157
15.4.2
Exp(z) .......................................................................1159
15.4.3
Log(z) .......................................................................1160
15.4.4
Branches of Logarithms..................................................1164
15.4.5
Power Functions ..........................................................1164
15.4.6
Problems ...................................................................1169
15.5
Trigonometric Functions ...........................................................1171
15.5.1
Problems ...................................................................1174
15.6
Taylor and Laurent Series ..........................................................1175
15.6.1
Taylor Series ...............................................................1178
15.6.2
Laurent Series .............................................................1179
15.6.3
Product of Taylor Series .................................................1185
15.6.4
Problems ...................................................................1185
15.7
Zeros and Poles ......................................................................1186
15.7.1
Singularities ...............................................................1188
15.7.2
Problems ...................................................................1193
15.8
Complex Integration and Cauchy’s Integral Theorem .........................1194
15.8.1
Integration on a Closed Contour .......................................1198
15.8.2
Cauchy–Goursat Integral Theorem ....................................1200
15.8.3
Problems ...................................................................1204
15.9
Cauchy’s Integral Formulas and Residues.......................................1205
15.9.1
Use of a Dumb-Bell Contour ............................................1207
15.9.2
Integration of a Laurent Series ..........................................1210
15.9.3
Cauchy’s Residue Theorem .............................................1212
15.9.4
Problems ...................................................................1213
15.10 Real Integrals by Complex Integration Methods ...............................1214
15.10.1
Integration of Periodic Functions.......................................1214
15.10.2
Improper Integrals over (−∞, ∞), [0, ∞), or (−∞, 0] ...............1215
15.10.3
Cauchy Principal Value ..................................................1218
15.10.4
Hilbert Transform ........................................................1219
15.10.5
Problems ...................................................................1220
Key Terms.....................................................................................1221
16. Conformal Mapping ........................................................................1223
16.1
Conformal Mappings and the Laplace Equation ...............................1223
16.1.1
Linear Mappings ..........................................................1223
16.1.2
Harmonic Functions......................................................1226
16.1.3
Elementary Functions ....................................................1227
16.1.4
Möbius Transformations.................................................1228
16.1.5
Problems ...................................................................1229

xvi
Contents
16.2 Möbius Transformations.............................................................1230
16.2.1
Circles, Lines, and Möbius Transformations ............................1230
16.2.2
Mapping Two Given Circles to Two Concentric Circles...............1233
16.2.3
Some Useful Facts about Möbius Transformations.....................1240
16.2.4
Möbius Transformation to or from a Line ...............................1241
16.2.5
Problems ......................................................................1243
16.3 Solving Laplace’s Equation Using Conformal Maps ............................1245
16.3.1
Boundary Values on a Circle...............................................1248
16.3.2
The Joukowsky Map ........................................................1251
16.3.3
Zhukovskii Airfoils..........................................................1253
16.3.4
Lift on Zhukovskii Airfoils.................................................1255
16.3.5
Problems ......................................................................1259
Key Terms.....................................................................................1260
References .....................................................................................1261
17. Integral Transform Methods ..............................................................1263
17.1 Fourier Transform ....................................................................1263
17.1.1
Convolution ..................................................................1268
17.1.2
Problems ......................................................................1268
17.2 Applications to Partial Differential Equations....................................1270
17.2.1
Fourier Cosine and Sine Transforms .....................................1272
17.2.2
Problems ......................................................................1279
17.3 Inverse Laplace Transform ..........................................................1283
17.3.1
Solving a Wave Equation...................................................1288
17.3.2
Problems ......................................................................1292
17.4 Hankel Transforms ...................................................................1294
17.4.1
Problems ......................................................................1298
Key Terms.....................................................................................1300
References .....................................................................................1301
18. Nonlinear Ordinary Differential Equations ...........................................1303
18.1 Phase Line and Phase Plane .........................................................1303
18.1.1
Equilibria .....................................................................1306
18.1.2
Qualitative Study: The Phase Line ........................................1307
18.1.3
Qualitative Study: The Phase Plane for LCCHS ........................1309
18.1.4
Saddle Point Case............................................................1309
18.1.5
Problems ......................................................................1311
18.2 Stability of an Equilibrium Point ...................................................1312
18.2.1
Stability from Linearization ................................................1315
18.2.2
Using r(t)......................................................................1318
18.2.3
Problems ......................................................................1319
18.3 Variation of Parameters Using Linearization .....................................1321
18.3.1
Saddle Point Theorem ......................................................1321
18.3.2
Periodic Solutions ...........................................................1326
18.3.3
Problems ......................................................................1328
18.4 Liapunov Functions ..................................................................1328
18.4.1
Definite Functions ...........................................................1331
18.4.2
Liapunov Functions and Quadratic Forms ..............................1334
18.4.3
Instability .....................................................................1336

Contents
xvii
18.4.4
Stability of Another Equilibrium Point...................................1338
18.4.5
Problems ......................................................................1339
18.5 Short Take: LaSalle Invariance Principle ..........................................1341
18.5.1
Stability of a Set ..............................................................1343
18.5.2
Problems ......................................................................1343
18.6 Limit Cycles............................................................................1344
18.6.1
Periodic Linearization ......................................................1346
18.6.2
Linearization about a Periodic Solution..................................1346
18.6.3
Levinson–Smith Theorem ..................................................1347
18.6.4
Hopf Bifurcation .............................................................1349
18.6.5
Problems ......................................................................1353
18.7 Existence, Uniqueness, and Continuous Dependence...........................1354
18.7.1
Continuous Dependence ...................................................1359
18.7.2
Problems ......................................................................1362
18.8 Short Take: Horseshoe Map and Chaos ...........................................1364
18.9 Short Take: Delay Equations ........................................................1367
18.9.1
Characteristic Equation .....................................................1369
18.9.2
Euler’s Method...............................................................1370
18.9.3
Problems ......................................................................1370
Key Terms.....................................................................................1371
Mathematica Commands ....................................................................1372
Reference ......................................................................................1372
Appendix A: Partial Fractions ..................................................................1373
Appendix B: Laplace Transforms Definitions and Derivations .........................1379
Appendix C: Series Solutions of ODEs ......................................................1387
Index ............................................................................................... 1407


Preface
Purpose
Advanced Engineering Mathematics (AEM) can be used in a course for engineering
students who are at the beginning graduate or advanced undergraduate level. It could
also be used in a course for junior undergraduate engineering students who have under-
gone an elementary course on ordinary differential equations (ODEs) and matrices. In
addition, this book could be used by undergraduate engineering students in a variety of
their post-calculus mathematics courses and by undergraduate students pursuing applied
mathematics courses.
This book aims to (1) be comprehensive and self-contained, (2) be relatively “lean and
lively,” (3) have an appropriately varied pace, (4) be accessible and well written, and (5)
have a large choice of appropriate and varied homework problems. It is designed for a
heterogeneous group of engineering students in order to extend and enrich their knowl-
edge and to introduce them to new topics. Students who use this book will become well
prepared for their engineering courses.
Learning Features
This book successfully blends intuition and logical reasoning. It deals with more advanced
material than most textbooks of its kind but does so in a way that is accessible to advanced
undergraduate audiences. It helps students understand the basic “what” and “why”
questions and learn material at several levels, thus extending their capabilities. Software
packages evolve and are even replaced, but the “what” and “why” questions they address
are more constant. The habits needed to discern these questions will serve engineers well
as they progress through their careers.
For most engineering students a deductive, “theorem/proof/special case” style of expo-
sition is alien to their ways of learning things. But most people appreciate the need to
explain things that they are interested in. Sometimes, a plausibility argument is the most
accessible explanation. Most engineering graduate students need more practice with log-
ical arguments that explain why techniques are correct or at least plausible, and this will
enhance their problem-solving and communication skills.
Often an example leads to its standardization in a definition or a theorem with general
applicability. The style of exposition is usually inductive rather than deductive. Also, in
order to not overwhelm them, I show students the difficulties gradually and often begin
with analogies to familiar topics. It is precisely the students who are less well prepared
who need this book the most.
As Epsteen (1913) wrote, “The professor of engineering is certainly on firm ground when
he takes the stand that the mathematics taught to his students should not be too abstract
on the one hand nor too concrete on the other. If the subject matter is too abstract it is
unintelligible or uninteresting to the beginner; if it is too concrete the science degenerates
to the mere performing of certain mechanical operations to a common tool instead of a
valuable instrument.” This is still a very good guide to follow.
xix

xx
Preface
As Henderson (1997) wrote when discussing a survey of what businesses looked
for in hiring bachelor’s degree holders: “Although engineers may not often need to
develop novel mathematical techniques, an ability to read, interpret and implement such
techniques is still a vital part of engineering research and development.”
In this book, the role of theory is to organize results, provide solution techniques,
illuminate what to calculate, and to assure when it is best to calculate. Theorems allow
us to avoid reinventing the wheel” and thus are part of a style of establishing formulas
and other results that is analogous to the engineering style of standardization. In general,
theory in this book directly relates to methods. Usually, “theory” consists of derivations of
useful identities.
My choice of what to explain is driven by what I can reasonably expect the readers to
explain when they do the problems. What most readers learn from the text connects to
what they really learn from working on the problems.
A lot of what engineers do these days is to use software packages; some engineers in
research and development also help create software. As much as possible, software should
not be a “black box” for the user. While this book is not a book about software packages, I
want to give students the mathematical tools they need to understand what their software
hopes to do and sometimes even how the software does it. In context, I show students
useful MathematicaTM and MATLAB® commands in strategic places.
Examples
There are three kinds of examples in the book. Woven into the narrative are examples
that give background knowledge or develop the fundamentals of a subject or method.
Other examples serve as models for students’ work on homework problems. Occasionally,
examples point out the limitations of a method or indicate further directions in a topic
beyond the scope of this book.
Often an example, or sequence of examples of increasing depth, leads to its standardiza-
tion in definitions and theorems with general applicability.
Problems
One of the most important things about a textbook is the problems. Usually, students
will have reason to feel that they truly understand the material while or after they do
homework problems based on, or related to, the narrative of the book.
Most of the problems are based on what is discussed in the narrative; these may be called
“exercises.” A few problems provide a structured way of filling in some of the details in the
narrative. Other problems explore topics related to, but not directly based on, the material
in the narrative; these problems are referred to as “complements.”
I believe students will find that the problems vary in difficulty, point of view, and style.
This will help them learn thoroughly and assess their learning, and this will make the book
good preparation for their use of mathematics in engineering courses.
Some of the problems are derivations, usually requiring manipulations of formulas as
students do in science and engineering courses to get new, useful formulas. While this may
strike some as being “theory,” asking students to derive things is essential to measuring
their understanding of what they have learned and determining if they are likely to be able
to use that knowledge in future courses.

Preface
xxi
Appendices
There are three appendices. Appendix A develops the technique of partial fractions that
are useful (a) in solving problems using Laplace transforms in Sections 4.4 and 4.5 and
(b) in calculating contour integrals in the complex plane in Sections 15.8, 15.9, and 17.3.
Students will also be familiar with using partial fractions to evaluate integrals in calculus
courses.
Appendix B provides the definition of the Laplace transform and the derivations of its
properties that are used in Sections 4.4 and 4.5.
Appendix C discusses series solutions of ordinary differential equations. Bessel func-
tions and Legendre polynomials discussed in this appendix are used in solving partial
differential equations in Sections 11.5 and 11.6.
Ancillaries and Supplements
Supplemental resources for the book, including an answer key for odd-numbered prob-
lems, as well as complete solutions of selected homework problems, can be found at URL
http://www.crcpress.com/product/isbn/9781439834473.
Developmental Plan
Over the past 14 years, I have been either developing or teaching new courses of mathe-
matics for graduate students of engineering, in consultation with professors of mechanical
engineering. Also, I have taught much of that material in other courses, at both the
undergraduate and graduate levels, for decades.
Many colleagues and independent reviewers have helped me in developing this book.
I also appreciate help from my MTH 399/599, MTH 699, and MTH 304/504 and 305/605
students. Professors Antonio Mastroberardino and Peter Olszewski, both from Penn State
Erie, The Behrend College, assisted with accuracy checking.
Most of the book’s chapters underwent classroom testing by one or more professors,
including Professors Lop-Fat Ho, David Miller, and me at Wright State University, as well
as by Professor Paul Eloe at the University of Dayton.
Professors Vasilios Alexiades, University of Tennessee–Knoxville; Markus Bussmann,
University of Toronto; Paul Eloe, University of Dayton; Harry Hardee, New Mexico
State University; Allen Hunt, Wright State University; Thomas Pence, Michigan State
University; Allen Plotkin, San Diego State University; Carl Prather, Virginia Polytechnic
Institute; Scott Strong, Colorado School of Mines; Hooman Tafreshi, Virginia Common-
wealth University; Thad Tarpey, Wright State University; James T. Vance, Jr., Wright State
University; Aleksandra Vinogradov, Montana State University; and Dr. Glenn Stoops
reviewed and commented on early drafts of the chapters of the book.
Professors Yuqing Chen, Weifu Fang, Ann Farrell, Qingbo Huang, Terry McKee,
Munsup Seoh, and James T. Vance, Jr., all from Wright State University, helped me check
the first page proofs.
Scott Isenberg served as developmental editor for the project. The feedback he and the
reviewers gave me were essential in improving the book. It was helpful for me to get con-
structive criticism, and it was heartening for me to read some good reviews. I appreciate
early encouragement and help from Bill Stenquist.

xxii
Preface
Guided Tour
In order to be comprehensive, this book provides a wide spectrum of the mathemati-
cal tools beginning graduate engineering students need to use. The core chapters cover
ordinary differential equations, matrix/linear algebra, Fourier series and transforms,
numerical methods, and partial differential equations.
This book is similar to many AEM books in that it covers many more topics than indi-
vidual professors are likely to use. Professors will typically pick and choose from among
noncore chapters according to the needs of their students. Also, students can use the brief,
“Short takes” sections for projects or as introductions to more advanced topics.
In order to cover a comprehensive range of topics, these courses must have a rapid
pace. While a lot of the material covered is a review of mathematics coursework taken by
typical undergraduate engineering students, I have added many topics that are new to
most students. I have found that engineering students progress rapidly through many of
the basic topics but need more time and effort for the more highly enriched or new topics.
In general, I have tried to keep material in different chapters as independent as possible.
Occasionally, it made sense to refer the reader to an example or problem in a previ-
ous chapter, for example, Example 15.67 in Section 15.10 refers back to Example 7.3 in
Section 7.1. Similarly, the use of “clairvoyance” in Example 17.9 in Section 17.3 refers to
the discussion in Section 11.3.
Better prepared students can leave out almost all of Chapters 1, 3, 4, 6, and 7, which
review material most engineers have learned in the first two or three years of their under-
graduate education. Those chapters do contain some sections you may consider adding to
your syllabi because they are less likely to have been in students’ background or because
they may be particularly useful to review:
• Section 1.7: Linear Independence, Basis, and Dimension
• Section 3.2: Separable and Exact ODEs
• Section 3.5: Cauchy–Euler ODEs
• Section 4.2: Forced Oscillations
• Section 4.3: Variation of Parameters
• Section 4.6: Scalar Difference Equations
• Section 4.7: Short Take: z-Transforms
• Section 6.6: Area, Volume, and Linear Transformations
• Section 6.7: Differential Operators and Curvilinear Coordinates
• Section 6.8: Rotating Coordinate Frames
• Section 7.6: Integral Theorems: Divergence, Stokes, and Applications
• Section 7.7: Continuous Probability Distributions
In almost all cases, you may omit a “Short take” section and not worry that the omission
will prevent you from covering subsequent material. The only exception is that Section
12.5, “Short Take: Galerkin Method,” leads directly to Section 14.5, “Short Take: Finite
Element Methods.”
Notations
The symbol ⃝is written at the end of the work on an example problem, and the symbol
2 is written at the end of the derivation or other explanation of a theorem, lemma, or

Preface
xxiii
corollary. Some of the example problems are derivations or explanations, but I still use ⃝
to show their end.
The symbol ≜means “is defined by.” For example, Euler’s formula is stated as
eiθ ≜cos θ + i sin θ,
that is, eiθ is defined to be equal to cos θ + i sin θ. Usually, in a statement with ≜, the
left-hand side is defined by the right-hand side.
When reference is made to an equation it is inside parentheses. For example, in
Section 1.1 reference is made to (1.5).
Further, if reference is made to an equation, example, definition, theorem, lemma,
or corollary in another section, then that section number is mentioned. For example, in
Section 1.2.1 reference is made to “Example 1.5 in Section 1.1.”
Some Suggested Courses
A one-semester course for junior or senior undergraduate students whose background
knowledge is good could include all or parts of Chapters 2, 5, 9, 10, 11, and 15.
A one-semester course for undergraduates whose background knowledge is not as good
could include all of Chapters 1, 3, 4, 6, and 7, and parts of Chapters 2 and 5, particularly as
preparation for a second semester course.
A two-semester course sequence for beginning graduate students who need to fill a lot
of gaps in their background could be as follows:
1. First semester: All of Chapters 3 and 7, and parts of Chapters 1, 2, 4, 5, and 6
2. Second semester: Chapters 9, 10, and 11, and other sections and chapters
A one-semester course for advanced undergraduates or beginning graduate students
could consist of parts of Chapters 2 and 5, and all or parts of Chapters 9, 10, 11, and
other sections and chapters.
The “other sections and chapters” could include numerical methods in Chapters 8
and/or 12; optimization and calculus of variations in Chapters 13 and/or 14; complex vari-
ables and applications in Chapters 15, 16, and/or 17; and nonlinear ordinary differential
equations in Chapter 18.
In addition, you can use selective chapters of the book to cover the material in
courses such as “differential equations and matrix algebra,” “linear algebra and appli-
cations,” “ordinary differential equations,” “partial differential equations and Fourier
series,” “numerical methods,” “applied mathematics,” and “complex variables and
applications.”
MATLAB® is a registered trademark of The MathWorks, Inc. For product information,
please contact:
The MathWorks, Inc.
3 Apple Hill Drive
Natick, MA, 01760-2098 USA
Tel: 508-647-7000
Fax: 508-647-7001
E-mail: info@mathworks.com
Web: www.mathworks.com

xxiv
Preface
References
Epsteen, S. Minimum courses in engineering mathematics. Am. Math. Monthly 20 (1913) 47–52.
Henderson, K. Educating electrical and electronic engineers. Eng. Sci. Educ. J. 6 (1997) 95–98.

Acknowledgments
I appreciate the help I have had from all of the people mentioned earlier in the
“Developmental Plan” section. They have played a crucial role in improving the book
as well as influencing its conceptual basis. I also appreciate the help and support that
Buzz Reed gave me as well as the hard work put in by my editor, Jonathan Plant, along
with his excellent staff at CRC Press/Taylor & Francis Group.
I am profoundly grateful for the support and help of my family and friends, many of
whom were directly involved in the development of this book. I am also very grateful to
all my teachers and mentors for sharing their knowledge with me.
I have saved decades of files of problems I have used in courses. I have tried to give
credit to the distinctive work of others in creating good problems. I apologize in advance
if I have neglected to do so for some of the problems.
The publisher’s website will have what I hope will be only a short compendium of cor-
rections and other changes. Experience has taught me that it is very difficult to cleanse all
errors from a book. Please let me know if you find any error!
You may also have specific suggestions about what material should be added to the
book, or left out, or reorganized. You may find a particular explanation strange or
poorly presented, or you may think of a better explanation. Please give me those specific
suggestions for improvement!
Of course, I take full responsibility for the book as it is, and I look forward to your
comments.
Larry Turyn
Dayton, Ohio
xxv


1
Linear Algebraic Equations, Matrices,
and Eigenvalues
1.1 Solving Systems and Row Echelon Forms
We begin with solving systems of linear algebraic equations. You will use matrix methods
often to solve problems in your engineering courses and in the real world, although a
software package may hide this fact from you. Numerical approximations of differential
equations, optimization, data analysis, and applications to vibrations and circuits have
methods and algorithms that have at their core systems of linear algebraic equations and
matrix methods.
For the system of linear algebraic equations
⎧
⎨
⎩
x1
−
x2
+
x3
= 0
−2x1
+
2x2
−
x3
= 0
3x2
+
2x3
= 4
⎫
⎬
⎭
(1.1)
in unknowns x1, x2, x3, adding two times the first equation to the second equation gives an
“equivalent system,”
⎧
⎨
⎩
x1
−
x2
+
x3
= 0
x3
= 0
3x2
+
2x3
= 4
⎫
⎬
⎭.
The operation of adding two times the first equation to the second equation is an analogue
of what we will call an “elementary row operation” later in this section. By a solution of
(1.1), we mean a point (x1, x2, x3) whose values for x1, x2, and x3 make all of the equations
in system (1.1) true simultaneously. By equivalent system, we mean a system of linear
algebraic equations that has exactly the same solutions as the original system (1.1). From
now on, to save writing, we may write “system” when we mean “system of linear algebraic
equations.”
In general, a system in unknowns x1, x2, . . . , xn has the form
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
am1x1 + am2x2 + · · · + amnxn = bm
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(1.2)
1

2
Advanced Engineering Mathematics
1.1.1 Matrices
A matrix is a rectangular array, such as
	1
2
0
6
−5
7

.
In this example, the first row has entries 1, 2, 0, and the second row has entries 6, −5, 7.
We say that the first row is higher up than the second row. Also, in this example, the
first column has entries 1, 6; the second column has entries 2, −5; and the third column has
entries 0, 7. We say the first column is to the left of the second column, etc. The (i, j) entry
of a matrix is the entry in the ith row and the jth column. In the example, the (2, 3) entry is 7.
A matrix having exactly m rows and exactly n columns is said to be of size m × n. A
matrix is square if m = n.
A zero row has all entries being 0, and a nonzero row has at least one nonzero entry.
The leftmost nonzero entry of a nonzero row is called a leading entry. If a leading entry is
the number 1, we call it a leading 1.
Definition 1.1
A matrix is in a row echelon form if it satisfies all three of the following properties:
The higher up a leading entry is, the further to the left it is.
(1.3)
Any column with a leading entry has zeros below the leading entry.
(1.4)
Any zero row(s) is at the bottom of the matrix.
(1.5)
The pivot positions of a matrix are the locations of the leading entries in a row echelon
form of that matrix.
Here are examples of matrices in row echelon form:
	1
5
3
0
−2
−6

,
⎡
⎢⎢⎢⎣
2
0
...
0
⎤
⎥⎥⎥⎦,
and
⎡
⎣
√
2
3
0
0
−1
b
0
0
0
⎤
⎦,
where b is an unspecified constant. Here are examples of matrices that are not in row
echelon form:
⎡
⎣
0
1
1
−1
1
0
0
0
0
⎤
⎦,
⎡
⎢⎢⎢⎣
1
2
...
n
⎤
⎥⎥⎥⎦,
and
⎡
⎣
1
0
2
0
0
0
0
1
1
⎤
⎦.
Each of the latter three examples violates at least one of properties (1.3)–(1.5).
We listed the three properties (1.3)–(1.5) in that order because we will see that property
(1.3) influences our work the most.

Linear Algebraic Equations, Matrices, and Eigenvalues
3
If a matrix is in a row echelon form, every column having a leading entry is called a
pivot column. Often, we will circle the leading entries in, and thus identify, the pivot
columns, for example,
	
1⃝
5
−3
0
-2⃝
−6

.
Definition 1.2
If a matrix C is in a row echelon form, then the rank of C is the number of its pivot columns.
The definition of rank will be extended to all matrices later, in Definition 1.6.
Given an m × (n + 1) matrix
C =
⎡
⎢⎢⎢⎢⎣
c11
.
.
.
c1,n+1
.
.
.
.
.
.
.
.
.
cm1
.
.
.
cm.n+1
⎤
⎥⎥⎥⎥⎦
,
we can write C =
⎡
⎢⎣
C1∗
...
Cm∗
⎤
⎥⎦, where the rows of C are denoted by
C1∗=
c11
c12
...
c1,n+1

, C2∗=
c21
c22
...
c2,n+1

, . . . .
Cm∗=
cm1
cm2
...
cm,n+1

.
Definition 1.3
The elementary row operations are to
Exchange the ith and jth rows, where i ̸= j, notated Ri ↔Rj.
(1.6)
Add a multiple of the ith row into the jth row, notated kRi + Rj →Rj.
(1.7)
Multiply the ith row by a nonzero constant, notated kRi →Ri.
(1.8)
If any elementary row operation(s) is performed on matrix C to obtain matrix C, we say
C and C are row equivalent, and we write C ∼C. If matrix C is not already in a row echelon
form, we can perform a succession of elementary row operations on C to find matrix C that
is a row echelon form of C, that is, row equivalent to C and in a row echelon form.
The following is a method for choosing what sequence of row operations to do on C to
find a row echelon form of C. Essentially, the method discovers which of the columns in a
row echelon form should contain leading entries, that is, finds the pivot columns.

4
Advanced Engineering Mathematics
Here is a method: If the leftmost nonzero column of C is the jth column, then it will
contain a leading entry, and the first j −1 columns will not be pivot columns. If the (1, j)th
entry is zero, perform an interchange of rows to produce a row equivalent matrix, C1,
whose (1, j)th entry is not zero and is thus a leading entry.∗After that, add multiples of the
first row to the other rows to produce zeros below the leading entry.
Second, find the next pivot column, if possible: It should be the leftmost column that is
both to the right of the jth column and has a nonzero entry in the second or lower row.
This second pivot column will be, say, the kth column. If the (2, k)th entry is zero, perform
an interchange of rows to produce a row equivalent matrix, C2, whose (2, k)th entry is not
zero and is thus a leading entry. After that, add multiples of the second row to the other
rows to produce zeros below the leading entry.
After that, continue finding the pivot columns, producing leading entries in the third
row, fourth row, etc., if possible. Of course, eventually there will be no further pivot
columns to be found, and the matrix produced is an echelon form of C. If the total number
of pivot columns is ℓ, then the first ℓrows in an echelon form will have leading entries.
Any rows below the ℓth row will be zero rows.
In fact, if the original matrix C is a zero matrix, it is already in row echelon form, and
there are no pivot columns. Similarly, if the matrix
C2 =
⎡
⎢⎢⎣
0
0
2
−1
−3
0
0
0
5
7
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦
is produced by a succession of elementary row operations on C, then the algorithm stops
because C2 is already in a row echelon form.
Example 1.1
Use elementary row operations on matrix C as follows to find a matrix that is row equiv-
alent to C and is in a row echelon form. [Note: There can be more than one correct final
conclusion for this problem.]
Method:
C =
⎡
⎣
1
−1
2
0
−2
2
−1
0
−3
3
2
4
⎤
⎦
∼
2R1+R2 →R2
3R1+R3 →R3
⎡
⎣
1
−1
1
0
0
0
3
0
0
0
8
4
⎤
⎦
∼
−8
3R2+R3 →R3
⎡
⎣
1⃝
−1
1
0
0
0
3⃝
0
0
0
0
4⃝
⎤
⎦. ⃝
∗If the method is implemented on a computer, the basic advice is to get an entry in the (1, 1) position of greatest
absolute value, although the “implicit partial pivoting” technique is a little more complicated than that. The
reasons why will be discussed in Chapter 8.

Linear Algebraic Equations, Matrices, and Eigenvalues
5
1.1.2 Augmented Matrices
For system (1.1), that is,
⎧
⎨
⎩
x1
−
x2
+
x3
= 0
−2x1
+
2x2
−
x3
= 0
3x2
+
2x3
= 4
⎫
⎬
⎭,
we can assemble all of the coefficients that multiply the variables, as well as the right-hand
sides, in the augmented matrix
C =
⎡
⎣
1
−1
1
| 0
−2
2
−1
| 0
0
3
2
| 4
⎤
⎦.
(1.9)
We perform the elementary row operations 2R1 + R2 →R2, R2 ↔R3, in that order, giving
the augmented matrix
⎡
⎣
1⃝
−1
1
|
0
0
3⃝
2
|
4
0
0
1⃝
|
0
⎤
⎦
(1.10)
that corresponds to the system
⎧
⎨
⎩
x1
−
x2
+
x3
= 0
3x2
+
2x3
= 4
x3
= 0
⎫
⎬
⎭.
(1.11)
So, simple operations on system (1.1) exactly correspond to elementary row operations
on the corresponding augmented matrix in (1.9).
Theorem 1.1
A succession of elementary row operations does not change the set of solutions of a sys-
tem, that is, two row equivalent matrices are the augmented matrices of two equivalent
systems.
In general, given (1.2), a system of m equations in n unknowns, we can form the
corresponding m × (n + 1) augmented matrix
C =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
|
b1
.
.
.
|
.
.
.
.
|
.
.
.
.
|
.
am1
.
.
.
amn
|
bm
⎤
⎥⎥⎥⎥⎦
and then perform a succession of elementary row operations to obtain C, a matrix in a row
echelon form.

6
Advanced Engineering Mathematics
For the moment, we will discuss a special case of this situation: suppose that m = n and
that the first n columns of B are pivot columns. In this special case, B has the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
˜c11
˜c12
˜c13
.
.
.
˜c1n
|
˜c1,n+1
0
˜c22
˜c23
.
.
.
˜c2n
|
˜c2,n+1
0
0
˜c33
.
.
.
˜c3n
|
˜c3,n+1
.
.
.
.
.
|
.
.
.
.
.
.
|
.
.
.
.
.
.
|
.
0
0
0
.
.
.
˜cnn
|
˜cn,n+1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where ˜cnn ̸= 0. (Why?) We can use the last row of C to solve for xn, the next to last row to
solve for xn−1, etc. The latter process is called “back substitution” because, after solving the
last equation for xn, we substitute that into the next to last equation to solve for xn−1, etc.
This process of reduction to a row echelon form, followed by back substitution, is called
the Gaussian elimination method for solving a system of equations.
Example 1.2
Use the Gaussian elimination method to solve system (1.1).
Method: System (1.1) has augmented matrix (1.9), and elementary row operations reduce
it to (1.10), which corresponds to system (1.11), that is,
⎧
⎨
⎩
x1
−
x2
+
x3
= 0
3x2
+
2x3
= 4
x3
= 0
⎫
⎬
⎭.
Solving the last equation gives x3 = 0. Substituting x3 = 0 into the next to last equation
gives x2 = 1
3(4 −2 · 0) = 4
3. Substituting x2 = 4
3, x3 = 0 into the first equation gives
x1 = x2−x3 = 4
3−0 = 4
3. The system has exactly one solution: (x1, x2, x3) =

4
3, 4
3, 0

. ⃝
Now, instead of using back substitution, we could have continued row reducing the
augmented matrix.
1.1.3 Row Reduced Echelon Form (RREF)
If a leading entry is the number 1, we call it a leading 1.
Definition 1.4
If a matrix is in a row echelon form, that is, satisfies properties (1.3) through (1.5), and also
satisfies two other properties,
Each leading entry is a leading 1, and
(1.12)
any column with a leading 1 has zeros above the leading 1,
(1.13)
then we say that the matrix is in RREF.

Linear Algebraic Equations, Matrices, and Eigenvalues
7
The Gauss–Jordan method for solving a system of equations row reduces the aug-
mented matrix to RREF.
In MATLAB® there is a command, RREF, that reduces a given matrix to RREF.
Example 1.3
Use the Gauss–Jordan method to solve system (1.1).
Method: Continuing from (1.10),
⎡
⎣
1
−1
1
| 0
−2
2
−1
| 0
0
3
2
| 4
⎤
⎦∼
⎡
⎣
1
−1
1
| 0
0
3
2
| 4
0
0
1
| 0
⎤
⎦
∼
−2R3 + R2 →R2
−R3 + R1 →R1
⎡
⎣
1
−1
0
| 0
0
3
0
| 4
0
0
1
| 0
⎤
⎦
∼
1
3R2 →R2
R2 + R1 →R1
⎡
⎢⎢⎢⎢⎣
1⃝
0
0
|
4
3
0
1⃝
0
|
4
3
0
0
1⃝
|
0
⎤
⎥⎥⎥⎥⎦
= RREF(C),
(1.14)
which corresponds to the system
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
x1
= 4
3
x2
= 4
3
x3
= 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
The system has exactly one solution, namely, (x1, x2, x3) =

4
3, 4
3, 0

. ⃝
We could have read the solution directly from (1.14) without bothering to write the
corresponding system of equations.
Example 1.4
Solve the system of equations
⎧
⎨
⎩
x1
+
2x2
−
2x3
= −6
−2x1
−
4x2
+
5x3
= 17
x1
+
2x2
−
x3
= −1
⎫
⎬
⎭.
(1.15)
Method:
⎡
⎣
1
2
−2
|
−6
−2
−4
5
|
17
1
2
−1
|
−1
⎤
⎦
∼
2R1 + R2 →R2
−R1 + R3 →R3
⎡
⎣
1
2
−2
|
−6
0
0
1
|
5
0
0
1
|
5
⎤
⎦
∼
−R2 + R3 →R3
2R2 + R1 →R1
⎡
⎣
1⃝
2
0
|
4
0
0
1⃝
|
5
0
0
0
|
0
⎤
⎦,

8
Advanced Engineering Mathematics
which corresponds to the system
⎧
⎨
⎩
x1
+
2x2
= 4
x3
= 5
0
= 0
⎫
⎬
⎭.
This system has infinitely many solutions: The second equation gives x3 = 5, and the
first equation gives x1 = 4 −2x2. For any value of x2, we have a distinct solution of the
original system. The solutions are
(x1, x2, x3) = (4 −2c1, c1, 5),
where c1 is an arbitrary constant. ⃝
We recall from integral calculus and previous study of ordinary differential equations
that arbitrary constants, such as c1, often appear when we solve mathematical problems.
But why not make x1 be the arbitrary constant and solve for x2 in terms of x1, specifically
x1 + 2x2 = 4 gives x2 = 1
2(4 −x1)? Other than wanting to avoid a fraction in the solution,
it seems to be just as good. But we will see that to make the method more systematic, we
introduce two more definitions that will “standardize” our work and thus make it both
more reliable and more accessible to the reader. By the way, the engineering profession
favors establishing standards for exactly the same reasons.
Definition 1.5
Suppose an augmented matrix C is m × (n + 1) and has RREF(C) = CRR. For 1 ≤k ≤n,
(a) xk is a free variable if the kth column of CRR is not a pivot column.
(b) xk is a basic variable if the kth column of CRR is a pivot column.
Note that entries in the kth column of an augmented matrix CRR multiply xk in the
corresponding system.
We have an immediately useful result:
Theorem 1.2
The m × (n + 1) augmented matrix has the (n + 1)th column of CRR being a pivot column
if, and only if, the corresponding system of equations has no solution.
The (n + 1)th column of CRR is a pivot column exactly when the bottom nonzero row of
CRR is [0
0
· · ·
0| 1⃝].
Theorem 1.3
A system (1.2) has either no solution, exactly one solution, or infinitely many solutions.

Linear Algebraic Equations, Matrices, and Eigenvalues
9
x2
x1
x2
x1
x2
x1
(a)
(b)
(c)
FIGURE 1.1
Theorem 1.3. (a) No solution, (b) exactly one solution, (c) infinitely many solutions.
Why? Theorem 1.2 tells us when a system has no solution. On the other hand, if a system
has at least one solution, either there is no free variable, in which case there is exactly one
solution of the system, or there is a free variable, in which case there are infinitely many
solutions. 2
The set of points(x1, x2) that satisfy a single algebraic equation is a line in the x1x2 plane.
The points (x1, x2) that satisfy a system of two algebraic equations, that is, satisfy both
of the equations “simultaneously,” can be illustrated by drawing two lines in a plane.
Figure 1.1 illustrates Theorem 1.3.
From now on, we will add to both the Gauss–Jordan and the Gaussian elimination
methods the requirement that we will solve for all of the basic variables in terms of the
free variables and that we will replace the latter by arbitrary constants, just as we did in
Example 1.4.
Theorem 1.4
Any given matrix has exactly one RREF.
Why? See (Yuster 1984).
Because of this theorem, we refer to the RREF of a matrix, as opposed to a row echelon
form.
Associated with the RREF are many useful results.
Definition 1.6
Given any matrix C, we define rank(C) to be the rank of RREF(C), that is, its number of
pivot columns.
Theorem 1.5
If C is row equivalent to C, then (a) RREF(C) = RREF(C), and (b) rank(C) = rank(C).

10
Advanced Engineering Mathematics
Theorem 1.6
If A is m × n, then rank(A) ≤min{m, n}.
Why? The rank(A) is the number of leading ones in RREF(A). Each leading 1 is in one
column, so the number of leading ones is less than or equal to the number of columns of A,
that is, less than or equal to n. On the other hand, each row can have at most one leading 1,
so the number of leading ones is less than or equal to the number of rows, that is, less than
or equal to m. Because min{m, n} is either m or n, both of which are greater than or equal
to rank(A), the result follows. 2
Now, associated with a system (1.2), that is,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
am1x1 + am2x2 + · · · + amnxn = bm
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,
are five types of matrices. Denote
A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
,
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦,
and
b =
⎡
⎢⎢⎢⎢⎣
b1
·
·
·
bm
⎤
⎥⎥⎥⎥⎦
.
We call A the matrix of coefficients, x the vector of unknowns, and b the vector of right-
hand sides. The augmented matrix is
C = [A | b ] =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
|
b1
.
.
.
|
.
.
.
.
|
.
.
.
.
|
.
am1
.
.
.
amn
|
bm
⎤
⎥⎥⎥⎥⎦
,
and the latter is RREF(C).
The next result is called a “lemma” because it helps to establish the theorem after that.
Lemma 1.1
RREF([A | b ]) = [RREF(A) | cRR ], where cRR =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
cRR
1,n+1
cRR
2,n+1
·
·
·
cRR
m,n+1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

Linear Algebraic Equations, Matrices, and Eigenvalues
11
Why? The elementary row operations that reduce A to its RREF also reduce the augmented
matrix, [A | b ], to a matrix that is in reduced row echelon form, except for possibly its
(n + 1)th column.
Theorem 1.7
There are three cases for solutions of a linear system:
(a) If rank([A | b ]) > rank(A), then (1.2) has no solution.
(b) If rank([A | b ]) = rank(A) = n, then (1.2) has exactly one solution.
(c) If rank([A | b ]) = rank(A) < n, then (1.2) has infinitely many solutions, written in
terms of (n −rank(A)) arbitrary constants corresponding to the free variables.
Example 1.5
Study system (1.15) using Theorem 1.7.
Method: Example 1.4 gave the elementary row operations to find that
[A | b] =
⎡
⎣
1
2
−2
|
−6
−2
−4
5
|
17
1
2
−1
|
−1
⎤
⎦∼· · · ∼
⎡
⎣
1⃝
2
0
|
4
0
0
1⃝
|
5
0
0
0
|
0
⎤
⎦= [RREF(A) | cRR]
has rank([A | x]) = rank(A) = 2 < 3 = n. So, system (1.15) has infinitely many solutions
with one arbitrary constant, according to Theorem 1.7(c). ⃝
Example 1.6
Study the system
⎧
⎨
⎩
x1
+
2x2
−
2x3
= −6
−2x1
−
4x2
+
5x3
= 17
x1
2x2
−
x3
= 0
⎫
⎬
⎭
(1.16)
using Theorem 1.7.
Method: The coefficient matrix A is the same for system (1.16) as for system (1.15), so we
can use the same elementary row operations as in Example 1.4. We get
[A | x] =
⎡
⎣
1
2
−2
|
−6
−2
−4
5
|
17
1
2
−1
|
0
⎤
⎦∼· · · ∼
⎡
⎣
1⃝
2
0
|
0
0
0
1⃝
|
0
0
0
0
|
1⃝
⎤
⎦= [RREF(A) | cRR],
so rank([A|x]) = 3 > 2 = rank(A). According to Theorem 1.7(a), system (1.16) has no
solution. ⃝
Note that system (1.16) has no solution even though x2 is a free variable.
In Example 1.6, we could have instead used Theorem 1.2; the latter is the reason why
Theorem 1.7(a) is true.
Generally, computer methods use the Gaussian elimination method followed by back
substitution, not the Gauss–Jordan method, because for “large” matrices, back substitution
uses fewer operations than the alternative. Nevertheless, for small systems that we solve

12
Advanced Engineering Mathematics
by hand, the Gauss–Jordan method is perfectly fine. Also, we will see in Section 1.5 that
the Gauss–Jordan method will be used to find the “inverse” of a square matrix.
Example 1.7
For the direct current (DC) circuit shown in Figure 1.2, set up a system of three equations
for the loop currents I1, I2, I3; write the corresponding augmented matrix; and then find
the exact solution in terms of the unspecified voltages V1, V2.
Method: The Kirchhoff voltage law applied to the three loops, starting at the upper right
and proceeding clockwise, gives the three equations
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
2I1 + 3(I1 −I2) + 1 · I1 = V1,
3(I2 −I1) + 7I2 + 9I2 + 10(I2 −I3) = 0,
10(I3 −I2) + 5I3 + 6I3 = V2.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
The corresponding augmented matrix is
⎡
⎣
6
−3
0
|
V1
−3
29
−10
|
0
0
−10
21
|
V2
⎤
⎦.
If we do the row operation 1
2 R1 + R2 →R2 followed by 4
11R2 + R3 →R3, then we have
a row echelon form
⎡
⎢⎢⎢⎢⎢⎣
6
−3
0
|
V1
0
55
2
−10
|
1
2 V1
0
0
191
11
|
2
11 V1 + V2
⎤
⎥⎥⎥⎥⎥⎦
.
V1
V2
I2
I1
I3
1 Ω
10 Ω
6 Ω
7 Ω
2 Ω
3 Ω
9 Ω
5 Ω
FIGURE 1.2
Example 1.7.

Linear Algebraic Equations, Matrices, and Eigenvalues
13
Using back substitution gives, successively, the exact loop currents in terms of V1, V2:
I3 =
2
191V1 + 11
191V2,
I2 = 21
955V1 +
4
191V2,
and
I1 = 509
2865V1 +
2
191V2. ⃝
Computer methods may put in the pivot positions, entries whose relative magnitudes
are as large as possible, in order to reduce “numerical error.” Computer methods may
do “partial pivoting,” that is, row interchanges that move relatively larger numbers into
pivot positions, or even “complete pivoting,” which both exchanges rows and exchanges
columns. (Exchanging columns corresponds to exchanging the index of two variables, so
such a method must keep track of the variable indices and return them to their original
indices in the final conclusion.)
1.1.4 Problems
In problems 1–3, for each system, use a row reduction algorithm to find all solutions.
1.
 x
−
y
= −7
3x
−
4y
= 11

.
2.
⎧
⎨
⎩
x1
−
x2
+
2x3
= −1
2x1
+
x2
+
x3
= −1
−x1
+
3x2
−
4x3
=
5
3
⎫
⎬
⎭.
3.
⎧
⎨
⎩
−
x2
+
4x3
= 1
−x1
+
3x2
+
2x3
= 0
2x1
−
x3
= −2
⎫
⎬
⎭.
4. Find the RREF of the matrix
⎡
⎢⎢⎣
0
0
1
0
1
1
−1
0
0
1
1
1
2
0
0
1
⎤
⎥⎥⎦.
5. Suppose that [A | b] can be row reduced to
⎡
⎣
1
2
3
−1
|4
0
0
1
0
|5
0
0
0
0
|r
⎤
⎦.
(a) If r = 6, find all solutions of Ax = b.
(b) If r = 0, find all solutions of Ax = b.
6. A system Ax = b had its augmented matrix row reduced to
⎡
⎣
1
2
0
|r1
0
0
1
|r2
0
0
0
|r3
⎤
⎦.

14
Advanced Engineering Mathematics
(a) Find all solutions of Ax = b, if
⎡
⎣
r1
r2
r3
⎤
⎦=
⎡
⎣
−1
3
0
⎤
⎦.
(b) Find all solutions of Ax = b, if
⎡
⎣
r1
r2
r3
⎤
⎦=
⎡
⎣
−1
3
2
⎤
⎦.
7. Write down three different 3 × 4 matrices that are in RREF and whose rank is
exactly three.
8. Let A =
⎡
⎣
1
0
■
■
1
0
0
■
1
⎤
⎦.
(a) Replace the three ■s by three different positive integers, each ≥2. Write down
your A.
(b) For the matrix A you wrote in part (a), use the Gauss–Jordan or the Gaussian
elimination method to solve the system
Ax =
⎡
⎣
1
0
0
⎤
⎦.
9. The Chevford Company produces hybrid trucks and cars at three plants: In an
hour, Plant I produces 4 cars and 4 trucks, Plant II produces 4 cars and 1 truck,
and Plant III produces 2 cars and 3 trucks. Currently, Plant I runs 7 hours a day,
Plant II runs 6 hours a day, and Plant III runs 9 hours a day.
As part of a program of regular maintenance, Plant I must be shut down for a
week. The Company will make up for the shutdown of Plant I by adjusting the
number of hours per day that the two remaining plants will run. What should be
the total number of hours per day that Plants II and III should run while Plant I
is shut down, in order that at least the same total production should be achieved?
Turn this problem into a system of linear equations and then solve it.
10. The foods puffed rice, rolled oats, bran flakes, and corn flakes contain the
nutrients protein, carbohydrates, and calories. Specifically, one cup of puffed
rice contains∗1 g of protein, 13 g of carbohydrates, and 60 cal, one cup of rolled
oats contains 5 g of protein, 23 g of carbohydrates, and 130 cal, one cup of bran
flakes contain 4 g of protein, 28 g of carbohydrates, and 105 cal, and one cup of
corn flakes contain 2 g of protein, 20 g of carbohydrates, and 95 cal. Can the foods
puffed rice, rolled oats, and bran flakes be combined to yield a new cereal with
the same nutritional content as corn flakes? If so, give your answer in the form
106 cups of corn flakes =
cups of puffed rice +
cups of rolled oats+
cups of
bran flakes.
∗Nutritional information from the 1985 World Almanac.

Linear Algebraic Equations, Matrices, and Eigenvalues
15
Solve Problem 1.1.4.10∗by first turning it into a system of linear equations and
then solving it.
11. A dietician is planning a meal to supply certain quantities of calcium, potassium,
and magnesium. Three foods will be used. One hundred grams of food #1 contains
40 mg of calcium, 20 mg of potassium, and 40 mg of magnesium. One hundred
grams of food #2 contains 70 mg of calcium, 10 mg of potassium, and 30 mg of
magnesium. One hundred grams of food #3 contains 50 mg of calcium, 40 mg
of potassium, and 60 mg of magnesium. If the meal is to contain exactly 120 mg of
calcium, 30 mg of potassium, and 70 mg of magnesium, use a system of equations
to find the amounts of foods #1, #2, and #3 that should be consumed in a meal
to exactly meet the dietary requirements for calcium, potassium, and magnesium.
Do state clearly what quantities your variables represent.
12. Suppose Midwestern University is considering how much of the area of its central
quadrangle to turn into landscaped parking and how much to turn into a park-
ing garage. Landscaped parking spaces require 50 ft2 of surface area per car, and
parking garage spaces require 10 ft2 of surface area per car. The total quadrangle
area is 8000 ft2, and they want to have a total of 250 parking spaces. Turn this
problem into a system of linear equations and then solve it.
13. Assume A is an m×n matrix, the system Ax = b has infinitely many solutions and
m > n. For each of (a)–(e), decide whether it must be true, must be false, or may be
true or may be false.
(a) A has a row of zeros.
(b) [ A | b ] has a row of zeros.
(c) The RREF of A has a row of zeros.
(d) The RREF of [ A | b ] has a row of zeros.
(e) Replace the assumption m > n by the assumption m = n = 2. Then the system
Ax = b can be represented geometrically as two lines that are identical.
14. For the DC circuit shown in Figure 1.3, set up a system of three equations for the
loop currents I1, I2, I3; write the corresponding augmented matrix; and then find
the exact solution in terms of the unspecified voltages V1, V2. On your picture,
indicate which loop current is which.
15. Suppose that a system Ax = b has its augmented matrix row equivalent to
⎡
⎢⎢⎣
1
0
0
2
|
5
0
0
1
0
|
6
0
2
3
−4
|
7
0
4
6
−8
|
14
⎤
⎥⎥⎦.
(a) Use elementary row operations to further row reduce the aforementioned to
find RREF([ A | b ]) and RREF(A), and label which is which.
(b) Find all solutions of Ax = b.
(c) Find the ranks of RREF([ A | b ]) and RREF(A), and label which is which if they
are not equal.
∗This is a problem we wrote that appeared in Lederer (1989).

Linear Algebraic Equations, Matrices, and Eigenvalues
17
With this definition, we can write compactly the system of equations
⎧
⎪⎨
⎪⎩
a11x1 + a12x2 + · · · + a1nxn
= b1
...
am1x1 + am2x2 + · · · + amnxn
= bm
⎫
⎪⎬
⎪⎭
as
Ax = b.
In the definition of multiplication the n’s “match,” that is, A is m × n and x is n × 1.
We can write the matrix A in terms of its columns, specifically
A =

A∗1  A∗2  · · ·
 A∗n

,
(1.18)
where its columns are denoted by
A∗j =
⎡
⎢⎢⎢⎣
a1j
a2j
...
amj
⎤
⎥⎥⎥⎦, j = 1, . . . , n.
We write the columns in boldface type because they are vectors.
Equation (1.18) is an example of writing a matrix as a partitioned matrix.
The next result will be very useful in Section 1.7 for understanding what matrices do,
but the result here comes from simple calculations.
Lemma 1.2
Ax = x1A∗1 + x2A∗2 + · · · + xnA∗n.
(1.19)
Why? Because
Ax =
⎡
⎢⎢⎢⎣
a11x1 + a12x2 + · · · + a1nxn
a21x1 + a22x2 + · · · + a2nxn
...
am1x1 + am2x2 + · · · + amnxn
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
x1a11
x1a21
...
x1am1
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
x2a12
x2a22
...
x2am2
⎤
⎥⎥⎥⎦+ · · · +
⎡
⎢⎢⎢⎣
xna1n
xna2n
...
xnamn
⎤
⎥⎥⎥⎦
= x1
⎡
⎢⎢⎢⎣
a11
a21
...
am1
⎤
⎥⎥⎥⎦+ x2
⎡
⎢⎢⎢⎣
a12
a22
...
am2
⎤
⎥⎥⎥⎦+ · · · + xn
⎡
⎢⎢⎢⎣
a1n
a2n
...
amn
⎤
⎥⎥⎥⎦= x1A∗1 + x2A∗2 + · · · + xnA∗n. 2
Now that we have a definition for multiplication of an m × n matrix times an n-vector,
that is, multiplication of an m × n matrix times an n × 1 matrix, we can generalize to define
multiplication of an m × n matrix A by an n × p matrix B.

16
Advanced Engineering Mathematics
2 Ω
1 Ω
3 Ω
5 Ω
I3
I2
I1
V1
9 Ω
7 Ω
V2
FIGURE 1.3
Problem 1.1.4.14.
1.2 Matrix Addition, Multiplication, and Transpose
If A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
and C =
⎡
⎢⎢⎢⎢⎣
c11
.
.
.
c1n
.
.
.
.
.
.
.
.
.
cm1
.
.
.
cmn
⎤
⎥⎥⎥⎥⎦
are two matrices of the same
size, we can define their sum by A + C ≜
⎡
⎢⎢⎢⎢⎣
a11 + c11
.
.
.
a1n + c1n
.
.
.
.
.
.
.
.
.
am1 + cm1
.
.
.
amn + cmn
⎤
⎥⎥⎥⎥⎦
.
This defines matrix addition.
We’ve defined
A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
,
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦, and b =
⎡
⎢⎢⎢⎢⎣
b1
·
·
·
bm
⎤
⎥⎥⎥⎥⎦
.
The set of all such n-vectors x, having real number entries, is called Rn.
We define multiplication of a matrix times a vector, that is, Ax, by
Definition 1.7
Ax =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦≜
⎡
⎢⎢⎢⎢⎣
a11x1 + a12x2 + · · · + a1nxn
.
.
.
am1x1 + am2x2 + · · · + amnxn
⎤
⎥⎥⎥⎥⎦
.
(1.17)

18
Advanced Engineering Mathematics
Definition 1.8
Matrix multiplication is defined by
AB =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
b11
.
.
.
b1p
.
.
.
.
.
.
.
.
.
bn1
.
.
.
bnp
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
≜
⎡
⎢⎢⎢⎢⎣
a11b11+ a12b21+ · · ·+ a1nbn1
.
.
.
a11b1p+ a12b2p+ · · ·+ a1nbnp
.
.
.
.
.
.
.
.
.
am1b11+ am2b21+ · · ·+ amnbn1
.
.
.
am1b1p+ am2b2p+ · · ·+ amnbnp
⎤
⎥⎥⎥⎥⎦
.
Example 1.8
	1
2
3
4
5
6

 ⎡
⎣
7
10
8
11
9
12
⎤
⎦=
1 · 7 + 2 · 8 + 3 · 9
1 · 10 + 2 · 11 + 3 · 12
4 · 7 + 5 · 8 + 6 · 9
4 · 10 + 5 · 11 + 6 · 12

=

50
68
122
167

. ⃝
Recall that we can write a matrix in terms of its rows, for example,
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
A1∗
−−
A2∗
−−
...
−−
Am∗
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
has rows denoted by Ai∗= [ai1
ai2
· · ·
ain] , i = 1, . . . , m.
Similarly,
we
can
write
a
matrix
in
terms
of
its
columns,
for
example,
B =

B∗1 
 B∗2 
 · · ·

 B∗p

has columns denoted by B∗j =
⎡
⎢⎢⎢⎣
b1j
b2j
...
bnj
⎤
⎥⎥⎥⎦, j = 1, . . . , p.

Linear Algebraic Equations, Matrices, and Eigenvalues
19
The product of a row vector with n entries, that is, a 1 × n matrix, and a column vector
with n entries, that is, a n×1 matrix, is a special case of matrix multiplication, for example,
Ai∗• B∗j =
ai1
ai2
· · ·
ain

⎡
⎢⎢⎢⎣
b1j
b2j
...
bnj
⎤
⎥⎥⎥⎦=

ai1b1j + ai2b2j + · · · + ainbnj

.
We use a • to remind ourselves of the dot product used for vectors in physics. We may
consider a 1 × 1 matrix to be a scalar, that is, a number, so we may write
Ai∗• B∗j = ai1b1j + ai2b2j + · · · + ainbnj.
Theorem 1.8
If A is m × n and B is n × p,
AB =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
b11
.
.
.
b1p
.
.
.
.
.
.
.
.
.
bn1
.
.
.
bnp
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
A1∗• B∗1
.
.
.
A1∗• B∗p
.
.
.
.
.
.
.
.
.
Am∗• B∗1
.
.
.
Am∗• B∗p
⎤
⎥⎥⎥⎥⎦
.
In general, AB ̸= BA, that is, the left–right order of matrix multiplication matters. For
one thing, the sizes of A and B may allow only one of AB and BA to exist. And, even if they
both exist, AB and BA may be of different sizes, for example, reusing the result of Example
1.8, we note the following:
Example 1.9
For the matrices A =
	1
2
3
4
5
6

and B =
⎡
⎣
7
10
8
11
9
12
⎤
⎦, check to see if AB = BA.
Method: AB =
	1
2
3
4
5
6

 ⎡
⎣
7
10
8
11
9
12
⎤
⎦=
	 50
68
122
167

and
BA =
⎡
⎣
7
10
8
11
9
12
⎤
⎦
	1
2
3
4
5
6

=
⎡
⎣
47
64
81
52
71
90
57
78
99
⎤
⎦,
so AB ̸= BA. ⃝
In fact, AB = BA is only possible if A and B are both square and of the same size. But
having A and B both of size n × n doesn’t even come close to guaranteeing AB = BA. This
reminds me of a street sign I saw that read “Don’t even think of parking here.”

20
Advanced Engineering Mathematics
Example 1.10
For the matrices A =
	1
2
3
4

and B =
	5
6
7
8

, check to see if AB = BA.
Method:
AB =
	1
2
3
4

 	5
6
7
8

=
	19
22
43
50

̸=
	23
34
31
46

=
	5
6
7
8

 	1
2
3
4

, so AB ̸=
BA. ⃝
If we choose at “random” two n × n matrices A and B, there is as much chance of AB
equaling BA as there is of flipping an infinitesimally thin coin and having it land standing
on its side.
The following results are as amazingly useful as they are simple!
Theorem 1.9
If A is m × n and B is n × p,
A B = A

B∗1 
 B∗2 
 · · ·

 B∗p

=

AB∗1 
 AB∗2 
 · · ·

 AB∗p

,
(1.20)
and
A B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
A1∗
−−
A2∗
−−
...
−−
Am∗
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
A1∗B
−−−
A2∗B
−−−
...
−−−
Am∗B
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(1.21)
Why? The (i, j)th entry of AB is ai1b1j + ai2b2j + · · · + ainbnj. On the other side of (1.20), the
jth column of

AB∗1 
 AB∗2

 · · ·

 AB∗p

is
AB∗j =
⎡
⎢⎢⎢⎢⎢⎢⎣
a11b1j + a12b2j + · · · + a1nbnj
a21b1j + a22b2j + · · · + a2nbnj
...
am1b1j + am2b2j + · · · + amnbnj
⎤
⎥⎥⎥⎥⎥⎥⎦
,
whose ith entry down is also ai1b1j + ai2b2j + · · · + ainbnj. The explanation for (1.21) is
similar. 2

Linear Algebraic Equations, Matrices, and Eigenvalues
21
1.2.1 Special Kinds of Matrices
Definition 1.9
The n × n identity matrix is In ≜
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
.
.
.
0
0
0
1
.
.
.
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
.
.
.
1
0
0
0
.
.
.
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
, or I, for short.
Definition 1.10
The m × n zero matrix is Omn ≜
⎡
⎢⎢⎢⎢⎣
0
.
.
.
0
.
.
.
.
.
.
.
.
.
0
.
.
.
0
⎤
⎥⎥⎥⎥⎦
, or O, for short.
We have the following facts:
AIn = A, InB = B, AOnq = Omq, OqnB = Oqp
for any m × n matrix A and n × p matrix B. Because of these facts about multiplication, we
call In an “identity” matrix, and we call Omn a zero matrix.
It is useful to have another notation for a matrix:
A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
=

aij

1 ≤i ≤m
1 ≤j ≤n
.
Definition 1.11
D =

dij

1 ≤i ≤m
1 ≤j ≤n
is called a diagonal matrix if dij = 0 for all (i, j) with i ̸= j. If D is a diagonal
matrix, we may write D = diag(d11, d22, . . . , dnn).
So, for example, I = diag(1, . . . , 1) and O = diag(0, . . . , 0).

22
Advanced Engineering Mathematics
Example 1.11
⎡
⎣
1
0
0
0
0
0
0
0
−3
⎤
⎦is a diagonal matrix;
⎡
⎣
1
0
0
0
0
0
0
−3
0
⎤
⎦is not a diagonal matrix. ⃝
Theorem 1.10
If A =

A∗1 
 A∗2 
 · · ·

 A∗n

and D = diag(d11, d22, . . . , dnn), then
AD =

d11A∗1 
 d22A∗2 
 · · ·

 dnnA∗n

.
Why? You will explain why Theorem 1.10 is true in Problem 1.2.5.13.
Definition 1.12
(a) U =

uij

1 ≤i ≤m
1 ≤j ≤n
is called an upper triangular matrix if uij = 0 for all (i, j) with
i > j. For example, for m = n, an upper triangular matrix has the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
u11
u12
u13
.
.
.
u1n
0
u22
u23
.
.
.
u2n
0
0
u33
.
.
.
u3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
0
.
.
.
unn
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
For m > n, an upper triangular matrix has the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
u11
u12
u13
.
.
.
u1n
0
u22
u23
.
.
.
u2n
0
0
u33
.
.
.
u3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
0
.
.
.
unn
0
0
0
.
.
.
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
0
.
.
.
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(b) L =

ℓij

1 ≤i ≤m
1 ≤j ≤n
is called a lower triangular matrix if ℓij = 0 for all (i, j) with i < j.

Linear Algebraic Equations, Matrices, and Eigenvalues
23
For example, for m = n, a lower triangular matrix has the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
ℓ11
0
0
.
.
.
0
ℓ21
ℓ22
0
.
.
.
0
ℓ31
ℓ32
ℓ33
.
.
.
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ℓn1
ℓn2
ℓn3
.
.
.
ℓnn
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Example 1.12
⎡
⎣
1
2
0
0
0
1
0
0
0
⎤
⎦is an upper triangular matrix. Recall that we saw this matrix, in the role of
RREF(A), in Example 1.5 in Section 1.1. ⃝
We have a more general result:
Theorem 1.11
All matrices in row echelon form are upper triangular. In particular, all matrices in RREF
are upper triangular.
1.2.2 Powers of a Matrix
Definition 1.13
Given any square matrix A, A0 ≜I, A2 ≜AA, A3 ≜AA2, . . ., Ar+1 ≜AAr, . . . are called the
powers of A.
Fact: ArAs = Ar+s for any positive integers r and s.
Example 1.13
Does (A + B)2 = A2 + 2AB + B2 for all matrices A and B?
Method: No, because (A+B)2 = (A+B)(A+B) = AA+AB+BA+BB = A2+AB+BA+B2.
In general, (A + B)2 ̸= A2 + 2AB + B2 because, in general, BA ̸= AB. ⃝
There is one other property of matrix multiplication that we will use often:
Theorem 1.12
(Associativity): If the matrix multiplications CA and AB both make sense, that is, the
dimensions of the matrices allow those multiplications, then (CA)B = C(AB).

24
Advanced Engineering Mathematics
Because it doesn’t matter where the parentheses are, as long as the order of the matrices
stays the same, we often write CAB instead of (CA)B or C(AB).
1.2.3 Transpose
Definition 1.14
Given any m × n matrix A =

aij

1 ≤i ≤m
1 ≤j ≤n
, the transpose of A is the n × m matrix
AT =

aji

1 ≤j ≤n
1 ≤i ≤m
.
Example 1.14
	1
2
3
4
5
6

T
=
⎡
⎣
1
4
2
5
3
6
⎤
⎦. Note that the rows of A become the columns of AT and the
columns of A become the rows of AT. ⃝
Theorem 1.13
Assuming the sizes of A and B allow them to exist, then (a) (A + B)T = AT + BT and
(b) (AB)T = BTAT.
1.2.4 Elementary Matrices
It turns out that all three kinds of elementary row operations on a matrix can be
implemented by multiplication on the left by “elementary matrices.” For example,
	a
b
c
d

∼
2R1 + R2 →R2
	
a
b
2a + c
2b + d

=
	1
0
2
1

 	a
b
c
d

,
so multiplication on the left by
	1
0
2
1

implements the row operation 2R1 + R2 →R2.
By the way, a matrix such as
	1
0
2
1

and generalizations to 3 × 3 matrices are called
“shear transformations” and come up in many subjects, including solid mechanics and
computer graphics.
Here are some other elementary matrices and the corresponding elementary row they
implement:

Linear Algebraic Equations, Matrices, and Eigenvalues
25
	2
0
0
1

implements 2R1 →R1.
	0
1
1
0

implements R1 ↔R2.
1.2.5 Problems
1. Find another example of 2 × 2 matrices A and B for which AB ̸= BA.
2. Suppose A is n × n, (A −AT)(A −AT) = 0, and A2 = A. Why must (AAT)2 = AAT?
3. Find an example of 2 × 2 nonzero matrices A and B for which AB = 0.
4. Suppose A and B are 2 × 2 matrices, rank(A) = 1, and rank(B) = 1.
(a) Find a specific example of A and B for which rank(A + B) = 2.
(b) Find a specific example of A and B for which rank(A + B) = 1.
(c) Find a specific example of A and B for which rank(A + B) = 0.
5. For A =
⎡
⎣
−1
1
0
0
−1
1
0
0
2
⎤
⎦, find A2 and A3.
6. If A is m × n, B is n × p, and the jth column of B is zero, why is the jth column of
AB zero?
7. If A and B are both upper triangular matrices, why is AB also upper triangular?
8. If D = diag(d11, d22, . . . , dnn) is a diagonal matrix, why is Dk a diagonal matrix for
every positive integer k? Find a formula for Dk in terms of the d11, d22, . . . , dnn.
9. True or false: If a 2 × 2 matrix A has rank(A) = p, then it must be true that
rank(A2) = p also. If true, why? If false, give a specific counter example.
10. For the matrices of Example 1.8, verify that (AB)T = BTAT.
11. For each of (a), (b), and (c), give the 3 × 3 elementary matrix that implements the
elementary row operation: (a) −2R1 + R3 →R3, (b) 1
2 R2 →R2, and (c) R2 ↔R3.
12. Suppose L, D, U are lower triangular, diagonal, and upper triangular matrices,
respectively. Explain why (a) LD and DL are lower triangular and (b) UD and DU
are upper triangular, at least for those sizes of matrices for which the products
that exist.
While you may use specific examples to learn about this problem, your expla-
nations should be in general, that is, not for specific matrices or even specific sizes
of matrices. [Hint: Use Theorem 1.9, for example,
L

D∗1 
 D∗2 
 · · ·

 D∗n

=

LD∗1 
 LD∗2 
 · · ·

 LD∗n

,
and then use Lemma 1.2.]
13. Use Theorem 1.9 and Lemma 1.2 to explain why Theorem 1.10 is true.

26
Advanced Engineering Mathematics
1.3 Homogeneous Systems, Spanning Set, and Basic Solutions
Definition 1.15
A homogeneous system has all zeros on the right-hand sides, that is, has the form Ax = 0.
Example 1.15
Solve
⎧
⎨
⎩
2x1
+4x2
+x3
= 0
−x1
−3x2
+x3
= 0
x1
+2x2
= 0
⎫
⎬
⎭.
Method:
⎡
⎣
2
4
1
| 0
−1
−3
1
| 0
1
2
0
| 0
⎤
⎦
∼
1
2 R1 + R2 →R2
−1
2 R1 + R3 →R3
⎡
⎣
2⃝
4
1
| 0
0
-1
⃝
1.5
| 0
0
0
−1
2⃝
| 0
⎤
⎦.
Back substitution gives x3 = 0; substituting that gives x2 = 0, and then substituting
x3 = x2 = 0 gives x1 = 0. There is exactly one solution: (x1, x2, x3) = (0, 0, 0). ⃝
Example 1.16
Solve
⎧
⎪⎪⎨
⎪⎪⎩
x1
+x2
+x3
= 0
x1
−x3
= 0
2x1
−2x3
+x4
= 0
−3x1
−3x2
−3x3
= 0
⎫
⎪⎪⎬
⎪⎪⎭
.
(1.22)
Method:
⎡
⎢⎢⎣
1
1
1
0
| 0
1
0
−1
0
| 0
2
0
−2
1
| 0
−3
−3
−3
0
| 0
⎤
⎥⎥⎦
∼
−R1 + R2 →R2
−2R1 + R3 →R3
3R1 + R4 →R4
⎡
⎢⎢⎣
1
1
1
0
| 0
0
−1
−2
0
| 0
0
−2
−4
1
| 0
0
0
0
0
| 0
⎤
⎥⎥⎦
∼
−2R2 + R3 →R3
R2 + R1 →R1
−R2 →R2
⎡
⎢⎢⎣
1⃝
0
−1
0
| 0
0
1⃝
2
0
| 0
0
0
0
1⃝
| 0
0
0
0
0
| 0
⎤
⎥⎥⎦.

Linear Algebraic Equations, Matrices, and Eigenvalues
27
The latter is in RREF. We have circled the pivot positions, which are in the first, second,
and fourth columns, so x3 is the only free variable. Using the corresponding system of
equations,
⎧
⎪⎪⎨
⎪⎪⎩
x1
−x3
=
0
x2
+2x3
=
0
x4
=
0
0
=
0
⎫
⎪⎪⎬
⎪⎪⎭
,
we solve for the basic variables, x1, x2, and x4, in terms of the free variable:
x4 = 0, x2 =−2x3, and x1 = x3. The solutions are (x1, x2, x3, x4) = (x3, −2x3, x3, 0) =
(c1, −2c1, c1, 0), where c1 is an arbitrary constant. ⃝
We can also write the solutions in vector form:
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
c1
−2c1
c1
0
⎤
⎥⎥⎦= c1
⎡
⎢⎢⎣
1
−2
1
0
⎤
⎥⎥⎦
that is,
x = c1x(1),
where
x(1) ≜
⎡
⎢⎢⎣
1
−2
1
0
⎤
⎥⎥⎦
and
c1 is an arbitrary constant.
(1.23)
Theorem 1.14
(Solution of homogeneous systems): A homogeneous system Ax = 0 always has at least
x = 0 as a solution. A homogeneous system has either infinitely many solutions or 0 as the
only solution, the so-called trivial solution.
Theorem 1.15
(Linearity principle). If A is an m × n matrix, x and y are any n-vectors, and α is any
scalar, then
(a) A(x + y) = (Ax) + (Ay), and
(b) A(αx) = α(Ax).
Definition 1.16
The general linear combination of vectors v1, . . . , vℓis
v = c1v1 + · · · + cℓvℓ,
where c1, . . . , cℓare arbitrary constants.

28
Advanced Engineering Mathematics
Definition 1.17
The general solution of a homogeneous system of equations Ax = 0 has the form x =
c1x(1) + · · · + cℓx(ℓ) if for every solution x∗, there are values of scalars c1, . . . , cℓgiving
x∗= c1x(1) + · · · + cℓx(ℓ).
So, for example, the general solution of system (1.22) is given by (1.23). We see that a
general solution of a homogeneous system has the form of a general linear combination of
vectors. In Chapters 3, 5, and 11, we will see how this generalizes to solving linear ordinary
differential equations and even linear partial differential equations.
Example 1.17
Solve
x1
+x2
−x4
= 0
x3
+2x4
= 0

.
(1.24)
Method: The corresponding augmented matrix,
	
1⃝
1
0
−1
| 0
0
0
1⃝
2
| 0

, is already
in RREF and has its pivot positions circled. The basic variables are x1, x3, and
the free variables x2, x4 can be replaced by arbitrary constants c1, c2, respectively.
The solutions are
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−x2 + x4
x2
−2x4
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−c1 + c2
c1
−2c2
c2
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−c1
c1
0
0
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
c2
0
−2c2
c2
⎤
⎥⎥⎦
= c1
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦+ c2
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦,
that is,
x = c1
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦+ c2
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦≜c1x(1) + c2x(2),
(1.25)
where c1, c2 are arbitrary constants, which gives the general solution of system (1.24). ⃝
Definition 1.18
Suppose that a set of vectors, W, can be written as a general linear combination of vectors
v1, . . . , vℓ, that is, W = {v : v = c1v1 + · · · + cℓvℓ, arbitrary scalars c1, . . . , cℓ}. Then we say
{v1, . . . , vℓ} is a spanning set for W, and we write
W = Span{v1, . . . , vℓ}.

Linear Algebraic Equations, Matrices, and Eigenvalues
29
x2
x1
FIGURE 1.4
Line through the origin, spanned by one vector.
Shown in Figure 1.4 is a line spanned by a single vector in R2. Shown in Figure 1.5 is
the plane
W = Span
⎧
⎨
⎩
⎡
⎣
1
1
−1
⎤
⎦,
⎡
⎣
−1
1
1
⎤
⎦
⎫
⎬
⎭
in R3 with many linear combinations, of various lengths, shown.
10
5
x =0
–5 –10
10
5
z=0
–5
–10
–10
–5
y=0
5
10
FIGURE 1.5
Plane spanned by two vectors.

30
Advanced Engineering Mathematics
Example 1.18
(Example 1.17 again) Find a spanning set for the solution set of system (1.24).
Method: In Example 1.17, we saw that all solutions of (1.24) can be written in the form
(1.25). So,
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
is a spanning set for the solution set of system (1.24). ⃝
Example 1.19
(Example 1.16 again) Find a spanning set for the solution set of system (1.22).
Method: In Example 1.16, we saw that all solutions of (1.22) can be written in the form
(1.23). So, a spanning set for the solution set of system (1.22) is given by
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
1
−2
1
0
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
. ⃝
Definition 1.19
Suppose W is the set of all solutions of a homogeneous system Ax = 0. Row reduction of
the augmented matrix [ A | 0 ] to its RREF decides which are the free variables. Suppose
W has a spanning set

x(1), . . . , x(ℓ)
corresponding to those free variables. Then we call
the set

x(1), . . . , x(ℓ)
a complete set of basic solutions, and we call each of x(1), . . . , x(ℓ) a
basic solution.
So, the number of basic solutions we need to give a general solution of Ax = 0 is the same
as the number of free variables for that system of equations.
Notice that in Example 1.17, the solutions are
x = c1
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦+ c2
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦.
If we let c1 = 1 and c2 = 0, we have the first basic solution, x(1) =
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦.
If we let c1 = 0 and c2 = 1, we have the second basic solution, x(2) =
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦.

Linear Algebraic Equations, Matrices, and Eigenvalues
31
We have a general principle that we will see over and over again in this book, in a
variety of situations. This general principle will tie together many different mathematical
techniques!
Theorem 1.16
(Linear superposition principle):
If a homogeneous system Ax = 0 has solutions
x(1), . . . , x(ℓ), then
x = c1x(1) + · · · + cℓx(ℓ)
(1.26)
also solves Ax = 0, for any values of scalars c1, . . . , cℓ. Further, if those vectors x(1), . . . , x(ℓ)
are all of the basic solutions, then (a) {x(1), . . . , x(ℓ)} gives a spanning set for the set of
solutions, and (b) for the set of solutions, there is no spanning set with fewer than ℓvectors.
The explanation for Theorem 1.16 would follow from Theorem 1.41 in Section 1.7.
Definition 1.20
The nullity of a matrix A is the number of vectors needed to span the solution set of Ax = 0.
We denote the nullity of A by ν(A).
We note that ν(A) is the number of free variables for the system Ax = 0.
Theorem 1.17
(Rank and nullity): If A is m × n, then n = rank(A) + ν(A).
All this theorem says is that for the homogeneous system Ax = 0, the number of vari-
ables, n, equals the sum of the number of basic variables, rank(A), plus the number of free
variables, ν(A). Nevertheless, Theorem 1.17 is fundamental to the study of solving systems
of linear algebraic equations.
Corollary 1.1
If A is m × n, the homogeneous system Ax = 0 has a nontrivial solution for x if, and only if,
rank(A) < n.
Example 1.20
Find the nullity of the matrix A =
⎡
⎣
1
2
−2
−2
−4
5
1
2
−1
⎤
⎦and verify that the result of Theorem
1.17 holds true.

32
Advanced Engineering Mathematics
Method: Similar to our work on Example 1.4 in Section 1.1, we have
⎡
⎣
1
2
−2
| 0
−2
−4
5
| 0
1
2
−1
| 0
⎤
⎦
∼
2R1 + R2 →R2
−R1 + R3 →R3
⎡
⎣
1
2
−2
| 0
0
0
1
| 0
0
0
1
| 0
⎤
⎦
∼
−R2 + R3 →R3
2R2 + R1 →R1
⎡
⎣
1⃝
2
0
| 0
0
0
1⃝
| 0
0
0
0
| 0
⎤
⎦.
Because there are two basic variables, rank(A) = 2; because there is exactly one free vari-
able, the nullity of A is ν(A) = 1. The matrix A has three columns, so n = 3. Indeed,
n = 3 = 2 + 1 = rank(A) + ν(A), in agreement with the conclusion of Theorem 1.17. ⃝
In any specific example, we can discuss basic solutions of Ax = 0 and the nullity of A
only after performing row reduction on [ A | 0 ] (or row reduction on A, because the last
column of zeros in [ A | 0 ] never changes during the row reduction).
1.3.1 Problems
In problems 1–3, for each given matrix A, use the row reduction algorithm to find the
general solution of the system Ax = 0.
1. A =
	1
3
−1
1
2
4
0
3

.
2. A =
⎡
⎢⎢⎣
1
2
0
3
0
1
1
−1
1
0
−2
5
0
1
1
−1
⎤
⎥⎥⎦.
3. A =
⎡
⎣
0
0
1
−1
1
2
3
1
−1
3
2
4
⎤
⎦.
4. Suppose A is an m × n matrix and Ax = 0 has a nontrivial solution. Which of the
following must be true? For each one, explain why or why not.
(a) rank(A) < n, (b) rank(A) < m, (c) Ax = 0 has infinitely many solutions, (d) m ≥n,
and (e) n ≥m.
5. Suppose that (i) w is a linear combination of v1, v2; (ii) v1 is a linear combination
of u1, u2; and (iii) v2 is a linear combination of u1, u2. Why must w be a linear
combination of u1, u2?
6. Assume A is an m × n matrix and x is a vector that satisfies Ax = 0 and x ̸= 0 .
For each of (a)–(e), decide whether it must be true, must be false, or may be true
and may be false.
(a) rank(A) < n.
(b) rank(A) < m.
(c) Ax = 0 has infinitely many solutions.

Linear Algebraic Equations, Matrices, and Eigenvalues
33
(d) m > n.
(e) n > m.
7. (a) Write down an example of a 3 × 3 matrix A whose rank is exactly two, is in
RREF, and is not a diagonal matrix.
(b) For the matrix A you wrote in part (a), let B =
⎡
⎣
1
0
0
0
1
0
0
0
1
⎤
⎦−A. Find all
solutions of the system Bx = 0 and find all basic solutions of that system.
8. (a) Write down a 4 × 5 matrix A that satisfies all of the following properties: (i) A
is in RREF, (ii) A has exactly three pivot positions, and (iii) at least two entries
of A are 2 and at least two entries of A are −1.
(b) For the matrix A you wrote down in part (a), find the rank and nullity of A.
1.4 Solutions of Nonhomogeneous Systems
A non homogeneous system has the form
Ax = b,
(1.27)
where b ̸= 0.
Definition 1.21
xp is a particular solution of (1.27) if Axp = b.
Example 1.21
(Example 1.3 in Section 1.1 again) xp =
⎡
⎣
4/3
4/3
0
⎤
⎦is a particular solution of
⎡
⎣
1
−1
1
−2
2
−1
0
3
2
⎤
⎦x =
⎡
⎣
0
0
4
⎤
⎦. ⃝
Example 1.22
(Example 1.4 in Section 1.1 again) xp =
⎡
⎣
4
0
5
⎤
⎦is a particular solution of
⎡
⎣
1
2
−2
−2
−4
5
1
2
−1
⎤
⎦x =
⎡
⎣
−6
17
−1
⎤
⎦. ⃝
(1.28)

34
Advanced Engineering Mathematics
Recall from Theorem 1.15(a) in Section 1.3 the linearity principle that A(x + y) = (Ax) +
(Ay). Suppose xp is a solution of a nonhomogeneous system (1.27) and xh satisfies the
corresponding homogeneous system Ax = 0, that is, Axh = 0. By “corresponding,” we
mean that the matrix A is the same. We get
A(xp + xh) = (Axp) + (Axh) = b + 0 = b,
so x ≜xp + xh is a solution of the nonhomogeneous system Ax = b.
Recall from Definition 1.17 in Section 1.3 that x is a “general solution” for the system
Ax = 0 if x = c1x(1) + · · · + cℓx(ℓ) has the property that for every x∗that solves Ax = 0, there
are values for c1, . . . , cℓfor which x∗= c1x(1) + · · · + cℓx(ℓ). For nonhomogeneous systems,
similar to this, we have
Definition 1.22
x = xp + c1x(1) + · · · + cℓx(ℓ) is a general solution for Ax = b if for every x∗that solves
(1.27), there are values for c1, . . . , cℓfor which x∗= xp + c1x(1) + · · · + cℓx(ℓ).
Theorem 1.18
If xp is any particular solution of (1.27) and xh = c1x(1) + · · · + cℓx(ℓ) is a general solution of
the corresponding homogeneous system, then
x = xp + xh = xp + c1x(1) + · · · + cℓx(ℓ)
(1.29)
is a general solution of (1.27).
Example 1.23
(Example 1.22 again) xp =
⎡
⎣
4
0
5
⎤
⎦is a particular solution of system (1.28), and the cor-
responding homogeneous system,
⎡
⎣
1
2
−2
−2
−4
5
1
2
−1
⎤
⎦x =
⎡
⎣
0
0
0
⎤
⎦, has general solution
xh = c1
⎡
⎣
−2
1
0
⎤
⎦, as we can see from the row reduction done in Example 1.20 in Section 1.3.
The general solution of system (1.28) is
x =
⎡
⎣
4
0
5
⎤
⎦+ c1
⎡
⎣
−2
1
0
⎤
⎦,
(1.30)
where c1 is an arbitrary constant. ⃝
Figure 1.6 shows the line of solutions x∗= xp + c1x(1) =
⎡
⎣
x
y
z
⎤
⎦for Example 1.23.

Linear Algebraic Equations, Matrices, and Eigenvalues
35
10
10
5
x =0
–5
5
x*=xp+ c1x(1)
xp
z=0
–5
y= 0
x=0
5
FIGURE 1.6
Line solving nonhomogeneous system.
Note that (1.30) agrees with our conclusion for Example 1.22, as well as for Example 1.4
in Section 1.1. In fact, while Theorem 1.18 announces a great principle, the easiest way
to solve system (1.28) is, instead, to do the row reduction we did for Example 1.4 in
Section 1.1:
Example 1.24
(Example 1.4 in Section 1.1 again) Solve system (1.28) and write the solution in the
form (1.29).
Method: To summarize the work done for Example 1.4 in Section 1.1,
⎡
⎣
1
2
−2
|
−6
−2
−4
5
|
17
1
2
−1
|
−1
⎤
⎦
∼
2R1 + R2 →R2
−R1 + R3 →R3
−R2 + R3 →R3
2R2 + R1 →R1
⎡
⎣
1⃝
2
0
|
4
0
0
1⃝
|
5
0
0
0
|
0
⎤
⎦,
which gives solutions
x =
⎡
⎣
x1
x2
x3
⎤
⎦=
⎡
⎣
4 −2c1
c1
5
⎤
⎦=
⎡
⎣
4
0
5
⎤
⎦+ c1
⎡
⎣
−2
1
0
⎤
⎦= xp + c1x(1),
where c1 is an arbitrary constant. ⃝
So, when solving systems of linear algebraic equations, we can solve for both xp and xh
at the same time.
We will see that there is also a nonhomogeneous principle for solving ordinary differen-
tial equations. In fact, when solving ordinary differential equations, it will make sense to

36
Advanced Engineering Mathematics
solve separately for xp and xh and then add them together to have the general solution, as
Theorem 1.18 suggests. So, while the structure of solutions of linear algebraic equations is
the same as the structure of solutions of differential equations, the practical aspects of how
you go about solving for the solutions is simpler for systems of linear algebraic equations.
Theorem 1.19
A nonhomogeneous system that has at least one solution has more than one solution if,
and only if, the corresponding homogeneous system has a nontrivial solution.
1.4.1 Problems
In problems 1–3, solve the system and express the solutions in the form x = xp+xh. Clearly
label what is xp and what is xh.
1.
⎧
⎨
⎩
x1
+2x3
= −1
x1
+x2
+4x3
= 1
−x1
−2x3
= 1
⎫
⎬
⎭
2.
⎧
⎨
⎩
x1
+x2
+2x3
=
5
−x2
−3x3
=
−4
−2x1
+2x2
+8x3
=
6
⎫
⎬
⎭
3.
⎧
⎪⎪⎨
⎪⎪⎩
x1
+2x2
+3x4
= 4
x2
+x3
−x4
= −5
x1
−2x3
+5x4
= −6
x2
+x3
−x4
= 5
⎫
⎪⎪⎬
⎪⎪⎭
4. Suppose the system Ax = b(1) has a solution xp,1, the system Ax = b(2) has a solu-
tion xp,2, and the system Ax = 0 has general solution xh. Find the general solution
of the system Ax = b(1) + b(2).
5. Suppose the system Ax =
⎡
⎢⎢⎣
2
5
8
0
⎤
⎥⎥⎦has general solution x =
⎡
⎢⎢⎢⎣
4 + c1 −c2
1 −c1 −c2
c1
c2
⎤
⎥⎥⎥⎦, where
c1, c2 are arbitrary constants.
(a) Write the general solution in the form x = xp + xh. Clearly label what is xp and
what is xh.
(b) Use your result from part (a) to find the general solution of the system Ax = 0.
(c) Use your result from part (b) to find all of the basic solutions of the system
Ax = 0.
6. Assume A is an m × n matrix. Which of the following possibly may be true?
(a) The equation Ax = b has a solution if the augmented matrix [ A | b ] has a pivot
position in each row.
(b) The equation Ax = b has a solution for all b in Rm, if the matrix A has a pivot
position in each row.

Linear Algebraic Equations, Matrices, and Eigenvalues
37
(c) Every linear combination of vectors in Rm can be written in the form Ax for
some matrix A and vector x in Rn.
(d) The equation Ax = b can have a solution for all b in Rm and still the columns
of A do not span Rm.
7. [The Fredholm alternative] Suppose a nonhomogeneous problem Ax = b has a
solution x. Let z be any solution of the homogeneous system ATz = 0. Explain
why bTz = 0. [Hint: First, take the transpose of the equation Ax = b.]
8. (a) Write down a 3 × 4 matrix A that is in RREF, has rank equal to two, and has at
least two entries of “2” and at least one entry of “−3.”
(b) For the matrix that you wrote down in part (a), find all basic solutions of
Ax = 0.
(c) For the matrix that you wrote down in part (a), (i) find a vector b for which
Ax = b has at least one solution, and (ii) find all solutions of Ax = b.
9. Suppose A is a 6 × 5 matrix. Is it possible that for all vectors b in R6 the system
Ax = b has at least one solution for x? Why, or why not?
10. For the system
⎧
⎨
⎩
x1
+
x2
+
x3
= 0
x1
−
x2
+
x3
= 6
x1
+
2x3
= 0
⎫
⎬
⎭
find the general solution as well as the general solution of the corresponding
homogeneous system. Label which is which.
1.5 Inverse Matrix
We will see that the concept of an “inverse matrix” unifies many results concerning
systems of equations.
Example 1.25
For A =
	1
2
3
4

and B =
	−2
1
3
2
−1
2

, we have AB =
	1
0
0
1

= BA. ⃝
Example 1.26
For A = In and B = In, we have AB = In = BA. ⃝
The following definition works only for square matrices.
Definition 1.23
If A is n × n and there is an n × n matrix B with
AB = In = BA,
(1.31)

38
Advanced Engineering Mathematics
then we say A is invertible, B is the inverse of A, and we write B = A−1. We may also
say “A is non-singular” or “A−1 exists.” If a matrix does not have an inverse, we say the
matrix is singular, also known as non-invertible.
So, in each of Examples 1.25 and 1.26, the given matrix A is invertible and the given
B = A−1.
Theorem 1.20
The inverse of A is unique, that is, there can be at most one distinct matrix B with AB =
I = BA.
Why? Suppose B and C are both inverses of A, that is, AB = I = BA and AC = I = CA.
Then CAB = (CA)B = (I)B = B, and also CAB = C(AB) = C(I) = C, so B = C, that is, B
and C are not distinct. 2
So, we don’t have to look for more than one inverse of the same matrix. Further, if two
different numerical methods produce two distinct approximations of the inverse of a given
matrix, then one or both of the approximations are incorrect.
I’m all in favor of working less, even if it sometimes requires thinking more. The next
result does this.
Theorem 1.21
If A and B are n × n and either AB = In or BA = In, then A−1 exists and equals B.
So, we don’t have to check both AB = I and BA = I. In fact, Theorem 1.26, as follows,
will develop an algorithm that constructs B with AB = I, if possible.
The next result is among the most simple but useful results of mathematics for
engineers.
Theorem 1.22
If 2 × 2 matrix A =
	a
b
c
d

and ad −bc ̸= 0, then A is invertible and
A−1 =
1
ad −bc
	 d
−b
−c
a

.
(1.32)
As we will see in Section 1.6, specifically Theorem 1.30, if A is 2 × 2 and ad −bc = 0,
then A−1 does not exist. Further, in Section 1.6, we will see that the criterion ad −bc ̸= 0 is
generalized using the concept of the “determinant of a matrix.”

Linear Algebraic Equations, Matrices, and Eigenvalues
39
Example 1.27
(Example 1.25 again) Find the inverse of the matrix A =
	1
2
3
4

.
Method: A has ad −bc = (1)(4) −(2)(3) = −2 ̸= 0, so there exists
A−1 =
1
−2
	 4
−2
−3
1

=
	−2
1
3
2
−1
2

. ⃝
Example 1.28
Use the definition to explain why the matrix A =
	1
2
2
4

is not invertible.
Method: If there were a matrix B =
	w
x
y
z

with AB = I2, then
	1
0
0
1

= AB =
	1
2
2
4

 	w
x
y
z

=
	 w + 2y
x + 2z
2w + 4y
2x + 4z

;
hence, 1 = w + 2y, 0 = 2w + 4y, 0 = x + 2z, and 1 = 2x + 4z. Substituting the first of
the four equations into the second gives 0 = 2w + 4y = 2(w + 2y) = 2(1) = 2, giving a
contradiction. So, A has no inverse. ⃝
In the notation A−1, the −1 indicates the operation of “taking the inverse of,” the same
notation as for “inverse function” that you saw in algebra and calculus courses. In fact,
we have
Ax = b ⇐⇒x = A−1b.
(1.33)
This notation means both “if Ax = b, then x = A−1b” and “if x = A−1b, then Ax = b,” that
is, that Ax = b and x = A−1b are logically equivalent.
Note also that if B = A−1, then BA = I = AB, so A satisfies the requirements in the
definition to be B−1. In fact,
B = A−1 ⇐⇒A = B−1.
(1.34)
So, we often say “A and B are inverses of each other” rather than “B = A−1 ” or “A = B−1.”
We have some general results:
Theorem 1.23
If A and C are invertible and of the same size, then
(a) (A−1)−1 = A.
(b) (AT)−1 = (A−1)T.
(c) (AC)−1 = C−1A−1.
Why? For (a), we have just seen in (1.34) that A−1 = B yields B−1 = A; hence, (A−1)−1 =
(B)−1 = A.

40
Advanced Engineering Mathematics
For (b), let C = AT. Because I = AA−1, taking the transpose of both sides and using
Theorem 1.13 in Section 1.2 implies I = IT = (AA−1)T = (A−1)TAT = (A−1)TC. It follows
from Theorem 1.21 that (A−1)T = C−1, that is, that (A−1)T = (AT)−1. 2
For (c), explain this yourself in Problem 1.5.3.17.
Theorem 1.24
If n × n matrix A−1 exists, then for any n-vector b, the system Ax = b has exactly one
solution, namely, x = A−1b.
Why? b = Ax ⇐⇒A−1b = A−1(Ax) = (A−1A)x = (I)x = x. 2
Theorem 1.25
Suppose A is n × n. Then the following are logically equivalent statements:
(a) A−1 exists.
(b) rank(A) = n.
(c) RREF(A) = In.
(d) The only solution of Ax = 0 is x = 0.
(e) For every b, there is exactly one solution of Ax = b.
The reasons for these results will become apparent when we study the algorithm as
follows for finding A−1, if it exists.
1.5.1 Row Reduction Algorithm for Constructing the Inverse
We will see that to find a matrix B with AB = In, row reduce [A | In] ∼[In | B], if
A is invertible. This method is usually called the Gauss–Jordan method for find-
ing A−1.
Here’s why the method works:
recall Theorem 1.9 in Section 1.2,
that
is,
that AB =
AB∗1
AB∗2
· · ·
AB∗n

.
Next,
let’s define the columns of In by
e(1), e(2), . . . , e(n). So, solving AB = In is the same as solving [ AB∗1
AB∗2
· · ·
AB∗n ] =

e(1)
e(2)
· · ·
e(n)
, that is, solving all of the systems
AB∗1 = e(1),
AB∗2 = e(2), . . . ,
AB∗n = e(n)
for the columns of B. To solve for the columns of B, that is, B∗1, . . . , B∗n, we could
do row reduction individually on each of the corresponding augmented matrices

A | e(1) 
, . . . ,

A | e(n) 
. Instead, we can row reduce the “mega”- augmented matrix

A | e(1)
· · ·
e(n) 
:

Linear Algebraic Equations, Matrices, and Eigenvalues
41
[ A | In ] =

A | e(1)
· · ·
e(n) 
∼· · · ∼

RREF(A) | f(1)
· · ·
f(n) 
.
We can do row reduction instead on the “mega”- augmented matrix because the elemen-
tary row operations that reduce A to RREF(A) work just as well on the augmented matrices

A | e( j) 
, j = 1, 2, . . . . They “work just as well” because all elementary row operations can
be implemented by multiplication on the left by elementary matrices, as we mentioned at
the end of Section 1.2.
Now, if RREF(A) = In, then we have

A | e(1)
· · ·
e(n) 
∼· · · ∼

In|f(1)
· · ·
f(n) 
;
hence, we can immediately read off the solutions B∗1 = f(1), B∗2 = f(2), . . ., B∗n = f(n). This
gives A−1 = B =

f(1)
· · ·
f(n) 
.
On the other hand, if RREF(A) ̸= In, then because of property (1.5) in Section 1.1, RREF(A)
has at least its bottom row being an all zero row. But then, there are two cases concerning
the bottom row of the matrix

RREF(A) | f(1)
· · ·
f(n)
:
• If that bottom row is zero, then either there is no inverse matrix B or there
are infinitely many choices for inverse matrix B, but the latter is impossible by
Theorem 1.20.
• If that bottom row is nonzero, then by Theorem 1.2 in Section 1.1, at least one
of the equations AB∗1 = e(1),
AB∗2 = e(2), . . . , AB∗n = e(n) cannot be solved for
the corresponding column of the matrix B; hence, there is no matrix B satisfying
AB = In.
In either case, A is not invertible.
The preceding text explains why we have the following:
Theorem 1.26
(a) If RREF(A) = In and [ A | In ] ∼[ In | B ], then A−1 exists and equals B.
(b) If RREF(A) ̸= In, then A−1 does not exist.
(c) If at any point during the Gauss–Jordan row reduction process, there is a row of
the form [ 0 · · · 0 | α1 · · · αn ] with at least one of the αjs being nonzero, then we
can cut short the row reduction process and conclude that A−1 does not exist.
Example 1.29
(Example 1.25 again) For A =
	1
2
3
4

, use the Gauss–Jordan row reduction process to
determine whether A is invertible, and if it is, find A−1.

42
Advanced Engineering Mathematics
Method:
	1
2
|
1
0
3
4
|
0
1

∼
−3R1+R2→R2
	1
2
|
1
0
0
−2
|
−3
1

∼
R2 + R1 →R1
−1
2 R2 →R2
	
1⃝
0
|
−2
1
0
1⃝
|
3
2
−1
2

.
So, by Theorem 1.26(a), there exists A−1 =
	−2
1
3
2
−1
2

. ⃝
Example 1.30
For A =
⎡
⎣
1
−1
−2
2
−3
−5
−1
3
5
⎤
⎦, use the Gauss–Jordan row reduction process to determine
whether A is invertible, and if it is, find A−1.
Method:
⎡
⎣
1
−1
−2
|
1
0
0
2
−3
−5
|
0
1
0
−1
3
5
|
0
0
1
⎤
⎦
∼
−2R1 + R2 →R2
R1 + R3 →R3
⎡
⎣
1
−1
−2 |
1
0
0
0
−1
−1 |
−2
1
0
0
2
3 |
1
0
1
⎤
⎦
∼
2R2 + R3 →R3
−R2 + R1 →R1
−R2 →R2
⎡
⎣
1
0
−1 |
3
−1
0
0
1
1 |
2
−1
0
0
0
1 |
−3
2
1
⎤
⎦
∼
−R3 + R2 →R2
R3 + R1 →R1
⎡
⎣
1⃝
0
0
|
0
1
1
0
1⃝
0
|
5
−3
−1
0
0
1⃝
|
−3
2
1
⎤
⎦
So, by Theorem 1.26(a), there exists A−1 =
⎡
⎣
0
1
1
5
−3
−1
−3
2
1
⎤
⎦. ⃝
Example 1.31
For A =
⎡
⎣
1
2
3
4
5
6
1
−1
−3
⎤
⎦, use the Gauss–Jordan row reduction process to determine
whether A is invertible, and if it is, find A−1.

Linear Algebraic Equations, Matrices, and Eigenvalues
43
Method:
⎡
⎣
1
2
3 |
1
0
0
4
5
6 |
0
1
0
1
−1
−3 |
0
0
1
⎤
⎦
∼
−4R1 + R2 →R2
−R1 + R3 →R3
⎡
⎣
1
2
3 |
1
0
0
0
−3
−6 |
−4
1
0
0
−3
−6 |
−1
0
1
⎤
⎦
∼
−R2+R3→R3
⎡
⎣
1
2
3 |
1
0
0
0
−3
−6 |
−4
1
0
0
0
0 |
3
−1
1
⎤
⎦.
The bottom row is nonzero and has the form [0 0 0|α1 α2 α3], so Theorem 1.26(c) tells us
that A−1 does not exist. ⃝
1.5.2 Inverse of a Partitioned Matrix
In Section 1.2, we partitioned a matrix into its columns or rows. More generally, we can
partition a matrix into “blocks of rows” or “blocks or columns”; for example, an m × n
matrix A can be partitioned into two blocks containing its first r columns and its last n −r
columns:
A =

A∗1
· · ·
A∗r 
 A∗r+1
· · ·
A∗n

Even more generally, we could partition a matrix into blocks, each of which holds part of
the rows and part of the columns. For example,
A =
⎡
⎢⎢⎢⎢⎢⎢⎣
2
0

1
2
−4
−1
3

0
1
5
−
−

−
−
−
0
0

0
0
3
0
0

2
1
0
0
0

4
−2
3
⎤
⎥⎥⎥⎥⎥⎥⎦
≜
⎡
⎢⎢⎢⎢⎣
A11

A12

−−

−−

O

A22
⎤
⎥⎥⎥⎥⎦
,
where A11 is 2 × 2, A12 is 2 × 3, O is the 3 × 2 zero matrix, and A22 is 3 × 3.
We can define multiplication of block matrices:
⎡
⎢⎢⎢⎢⎣
C11
|
C12
−−
|
−−
C21
|
C22
−
|
−−
C31
|
C32
⎤
⎥⎥⎥⎥⎦
⎡
⎣
E11
|
E12
−−
|
−−
E21
|
E22
⎤
⎦≜
⎡
⎢⎢⎢⎢⎣
C11E11 + C12E21
|
C11E12 + C12E22
−−−−−
|
−−−−−
C21E11 + C22E21
|
C21E12 + C22E22
−−−−−
|
−−−−−
C31E11 + C32E21
|
C31E12 + C32E22
⎤
⎥⎥⎥⎥⎦
,
where the Cik, Ekj are themselves matrices, assuming all of the products of blocks exist.

44
Advanced Engineering Mathematics
Example 1.32
Suppose a matrix A can be written in block form
⎡
⎣
A11

A12
−−

−−
O

I
⎤
⎦,
where A11 is square and invertible. Explain why A is invertible and find its inverse.
Method: We’ll look for a matrix B in block form B =
⎡
⎣
B11

B12
−−

−−
B21

B22
⎤
⎦that we want to
satisfy AB = I. We calculate
⎡
⎣
I

O
−−
−−
O

I
⎤
⎦= I =? AB =
⎡
⎣
A11

A12
−−
−−
O

I
⎤
⎦
⎡
⎣
B11

B12
−−

−−
B21

B22
⎤
⎦
=
⎡
⎣
A11B11 + A12B21  A11B12 + A12B22
−−−−−

−−−−−
O · B11 + I · B21

O · B12 + I · B22
⎤
⎦=
⎡
⎣
A11B11 + A12B21  A11B12 + A12B22
−−−−−

−−−−−
B21

B22
⎤
⎦;
(1.35)
hence, we need O = B21 and I = B22. Substitute those into (1.35) to get
⎡
⎣
I

O
−−

−−
O

I
⎤
⎦= In =? AB =
⎡
⎣
A11B11

A11B12 + A12
−−−

−−−−
O

I
⎤
⎦.
So, we need I = A11B11 and O = A11B12+A12. The former is solvable because we assumed
A11 is invertible: B11 = A−1
11 . The latter is solvable by the same reason: O = A11B12 +
A12 ⇐⇒−A12 = A11B12 ⇐⇒−A−1
11 A12 = A−1
11 A11B12 = B12.
So, we conclude that A is invertible and A−1 has the partitioned form
A−1 =
⎡
⎣
A−1
11

−A−1
11 A12
−−

−−−
O

I
⎤
⎦. ⃝
Note that we didn’t specify the sizes of the matrix A or its blocks in Example 1.32. This
was because we wanted to emphasize that our explanations did not need us to work
with individual entries of the matrices, so we didn’t need to know how many of them
there were.
1.5.3 Problems
In problems 1–4, for each matrix, use the row reduction algorithm to either find the inverse
matrix or explain why the inverse does not exist.
1.
	 1
4
−2
3

2.
⎡
⎣
1
1
−1
2
0
−1
3
0
2
⎤
⎦

Linear Algebraic Equations, Matrices, and Eigenvalues
45
3.
⎡
⎣
1
−2
2
−2
0
−1
2
−1
0
⎤
⎦
4.
⎡
⎣
1
1
−1
3
0
2
−2
−1
0
⎤
⎦
5. Given that
⎡
⎣
1
0
−1
| 1
0
0
2
−2
1
| 0
1
0
−3
−2
1
| 0
0
1
⎤
⎦
is row equivalent to
⎡
⎣
1
0
−1
|
1
0
0
0
1
−1.5
|
1
−0.5
0
0
0
2.5
|
−5
2
0.5
−0.5
⎤
⎦,
use the row reduction algorithm to find
⎡
⎣
1
0
−1
2
−2
1
−3
−2
1
⎤
⎦
−1
.
6. If (AT)−1 =
	 1
2
−3
4

, find A−1, A, and A2.
7. If A is n × n and invertible and B = A−1, find a formula for (ATB)−1 in terms of A
and BT.
8. Find an example of 2 × 2 matrices A, B such that A, B, and A + B are all invertible
but (A + B)−1 ̸= A−1 + B−1.
9. If A is upper triangular and A−1 exists, must A−1 also be upper triangular? If so,
why? If not, give a specific counterexample.
In each of the problems 10–13 you are given information about Ay(i)’s, which you will use
to find a formula for A−1. Recall that e(1), e(1), . . . , e(n) are the columns of the n × n identity
matrix In. [Hint: Try to find, for example, a vector x satisfying Ax = e(3). Eventually, you
will use (1.20), in Section 1.2 which is part of Theorem 1.9 in Section 1.2.]
10. If A is a 4 × 4 matrix that satisfies
Ay(1) = e(1), Ay(2) = −e(3), Ay(3) = e(4), and Ay(4) = −e(2)
for some vectors y(1), . . . , y(4), find a formula for A−1 in terms of the y(1), . . . , y(4).

46
Advanced Engineering Mathematics
11. If A is a 4 × 4 matrix that satisfies
Ay(1) = e(1), Ay(2) = −e(3),
Ay(3) = 1
2

e(2) + e(4)
, and Ay(4) = 1
2

e(2) −e(4)
for some vectors y(1), . . . , y(4), find a formula for A−1 in terms of the y(1), . . . , y(4).
12. If A is a 4 × 4 matrix that satisfies
Ay(1) = e(2), Ay(2) = −e(4), Ay(3) = e(1), and Ay(4) = −e(3)
for some vectors y(1), . . . , y(4), find a formula for A−1 in terms of the y(1), . . . , y(4).
13. If A is a 3 × 3 matrix that satisfies
Ay(1) = e(1) + e(2), Ay(2) = −e(1), and Ay(3) = e(2) + e(3)
for some vectors y(1), . . . , y(3), find a formula for A−1 in terms of the y(1), y(2), y(3).
14. According to Theorem 1.21, if AB = In, then BA = In, also; hence, B = A−1. Find
an example of 2 × 2 matrices A and B for which AB = O but BA ̸= O.
15. Suppose C is an unspecified but invertible m × m matrix and A is an unspecified
m × n matrix. Explain why every solution of the system Ax = 0 is a solution of the
system CAx = 0, and vice versa.
16. If Cy = z, Bx = y, and B and C are inverses of each other, must there be a
relationship between x and z?
17. Explain why AC is invertible and (AC)−1 = C−1A−1 if A and C are invertible and
of the same size. [Hint: Let D = AC and B = C−1A−1 and explain why DB = I.]
18. Suppose A and B are n × n invertible matrices.
(a) If AB = BA, why must A−1B−1 = B−1A−1?
(b) If AB = BA, why must AB−1 = B−1A and A−1B = BA−1?
(c) If AB = BA, why must
A + B = (A−1 + B−1)AB = AB(A−1 + B−1)?
19. Assume A is an n × n matrix. Which of the following must be true?
If a system of equations Ax = 0 has a nontrivial solution, then
(a) A is invertible.
(b) A is singular.
(c) The columns of A span Rn.
(d) The set W ≜{x : Ax = 0} does not contain the zero vector.
20. Suppose that A is m × n, Ax = 0 has a nontrivial solution x(1), and n × n matrix B is
invertible.
(a) Find at least one nontrivial solution of ABx = 0.

Linear Algebraic Equations, Matrices, and Eigenvalues
47
(b) Suppose that in addition to the aforementioned assumptions, we assume that
m = n. What possible values can rank(A) have, and why? What possible values
can rank(AB) have, and why?
21. Suppose A, B, and C are n × n and ABC = In.
(a) Must A be invertible? If “yes,” why? If “no,” give an example.
(b) Must C be invertible? If “yes,” why? If “no,” give an example.
(c) Must B be invertible? If “yes,” why? If “no,” give an example.
22. If AC is invertible and C is invertible, must A be invertible? If so, find a formula
for A−1 in terms of (AC)−1 and C−1. [Hint: You cannot assume that AC invertible
implies that (AC)−1 = C−1A−1. Why not? Because Theorem 1.23(c) assumes that
A and C are invertible, and you can’t assume that A is invertible when trying to
explain why A is invertible. Nevertheless, the formula (AC)−1 = C−1A−1 gives you
a clue to find a formula for A−1 in terms of (AC)−1 and C−1.]
23. Suppose A, C are unspecified matrices and B ≜(I −A)−1 exists. For each of (a),
(b), (c), find a matrix X that satisfies the given equation. Give your conclusions in
terms of B, C.
(a) X = AX + C, (b) AX = X + C, and (c) XA = X + C.
24. Decide whether or not the partitioned matrix A ≜
⎡
⎣
I

O
−−

−−
A21

I
⎤
⎦is invertible; if
so, find its inverse in terms of O, I, and A21. Note that there is no reason to believe
that A21 is square.∗
25. Assume that A11 and A22 are invertible. Decide whether or not the partitioned
matrix A ≜
⎡
⎣
A11

O
−−

−−
A21

A22
⎤
⎦is invertible; if so, find its inverse in terms of
O, I, A11, A21, and A22.
26. The matrix A =
	 2
1
−3
−2

satisfies A−1 = A. Find two other examples of 2 × 2
matrices, other than I, each of which equals its inverse.
1.6 Determinant, Adjugate Matrix, and Cramer’s Rule
We saw in Theorem 1.22 in Section 1.5 that for 2 × 2 matrices A =
	a
b
c
d

, ad −bc being
nonzero versus zero completely determines whether A−1 exists or not. We will see that
this generalizes to n × n matrices using the concept of “determinant.” It turns out that
this concept applies to many things, including to (a) solving systems of linear algebraic
equations, later in this section; (b) eigenvalues and eigenvectors, in Section 2.1, which are
some of the most important engineering tools; (c) area and volume in Chapter 6; and (d)
multivariable integration, in Chapter 7.
∗Problems 1.5.3.24 and 1.5.3.25 are from Carlson (1993).

48
Advanced Engineering Mathematics
Definition 1.24
The determinant of A, denoted by det(A) or |A|, is defined for 2 × 2 matrices by
det
	a11
a12
a21
a22

≜a11a22 −a12a21.
(1.36)
Definition 1.25
The determinant of A, denoted by det(A) or |A|, is defined for 3 × 3 matrices by
det
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦≜a11

a22
a23
a32
a33
 −a12

a21
a23
a31
a33
 + a13

a21
a22
a31
a32
 .
(1.37)
The determinant of a 3×3 matrix is defined “recursively” in terms of determinants of 2×2
matrices.
The determinant of a 3 × 3 matrix is also given by

a11
a12
a13
a21
a22
a23
a31
a32
a33

= a11a22a33 + a12a23a31 + a13a21a32 −(a13a22a31 + a12a21a33 + a11a23a32).
It can also be found by the so-called crisscross method shown in Figure 1.7. We caution
that there is no crisscross method for matrices of size bigger than 3 × 3.
The determinant of A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
is defined using sub-matrix “minors”
Mij ≜the (n−1)×(n−1) matrix obtained from A by deleting its ith row and jth column
a11
a12
a13
a11
a12
a22
a21
a23
a22
a21
a31
a32
a33
a31
a32
= a11a22a33+ a12a23a31+ a13a21a32–(a13a22a31+ a12a21a33+ a11a23a32)
FIGURE 1.7
“Criss-cross” method for 3 × 3 determinant.

Linear Algebraic Equations, Matrices, and Eigenvalues
49
and
Aij = (−1)i+j det(Mij).
(1.38)
The Aij are called “co-factors.” The reason for this name will be given in Definition 1.26.
Example 1.33
For A =
⎡
⎢⎢⎣
1
2
3
4
5
6
7
8
7
6
5
4
3
2
−1
0
⎤
⎥⎥⎦, find A11, A12, and A23.
Method: We have A11 = (−1)1+1

6
7
8
6
5
4
2
−1
0

= −48, A12 = (−1)1+2

5
7
8
7
5
4
3
−1
0

=
72, and A23 = (−1)2+3

1
2
4
7
6
4
3
2
0

= 0. ⃝
Definition 1.26
The determinant of A, denoted by det(A) or |A|, is defined for n × n matrices by
det(A) ≜a11A11 + a12A12 + · · · + a1nA1n.
This definition is called “expansion of the determinant along the first row.”
Theorem 1.27
det(A) can be calculated by expansion along any row or column, that is,
(a) |A| = ai1Ai1 + ai2Ai2 + · · · + ainAin, any i for 1 ≤i ≤n, and
(b) |A| = a1jA1j + a2jA2j + · · · + anjAnj, any j for 1 ≤j ≤n.
So, we can expand a determinant along any row, as in (a), or along any column, as in (b).
Theorem 1.27 is amazing because it says that 2n different ways of calculating det(A) give
the same result.
Instead of Definition 1.26, there is an alternative way of defining det(A) using permuta-
tions of {1, . . . , n} and their parities. That alternative definition is very useful for theoretical
results, including many of the theorems in this section; however, rather than length-
ening the exposition considerably, we will refer the reader to other books that include
this topic.

50
Advanced Engineering Mathematics
Example 1.34
Expand

1
2
3
4
5
6
3
−4
5

along the first row, the third row, and the second column.
Method: Expansion along the first row, the third row, and the second column gives,
respectively,

1
2
3
4
5
6
3
−4
5

= 1 · (−1)1+1

5
6
−4
5
 + 2 · (−1)1+2

4
6
3
5
 + 3 · (−1)1+3

4
5
3
−4

= 49 −2(2) + 3(−31) = −48,

1
2
3
4
5
6
3
−4
5

= 3 · (−1)3+1

2
3
5
6
 + (−4) · (−1)3+2

1
3
4
6
 + 5 · (−1)3+3

1
2
4
5

= 3(−3) + 4(−6) + 5(−3) = −48, and

1
2
3
4
5
6
3
−4
5

= 2 · (−1)1+2

4
6
3
5
 + 5 · (−1)2+2

1
3
3
5
 + (−4) · (−1)3+2

1
3
4
6

= −2(2) + 5(−4) + 4(−6) = −48. ⃝
Here are a few important facts about determinants.
Theorem 1.28
If A and B are n × n, then
(a) |AT| = |A|.
(b) |AB| = |A| |B|.
(c) If A is an upper or lower triangular matrix, then |A| = a11a22 · · · ann, that is, the
product of the diagonal entries.
In particular, if A is a diagonal matrix, then |A| = a11a22 · · · ann.
Using Theorem 1.27, calculation of a determinant of an n × n matrix requires, in general,
calculation of n determinants of (n −1) × (n −1) matrices, each of which in turn requires
calculation of n −1 determinants of (n −2) × (n −2) matrices, etc. This could require mil-
lennia of computational time, after which time, the accumulated round-off errors would
invalidate the conclusion because the number n!, that is, n(n −1) · · · 2 · 1, quickly becomes
huge∗as n →∞, for example, 20! ≈2.6 × 1018. So, we want to have a better way to
evaluate a determinant of a “large” matrix. Theorem 1.28(c) gives us a clue: maybe we can
∗Stirling’s approximation gives n! ∼
√
2πn nn e−n.

Linear Algebraic Equations, Matrices, and Eigenvalues
51
take the determinant of an upper triangular matrix that is row equivalent to a given matrix
instead of taking the determinant of the given matrix.
Theorem 1.29
Suppose B is obtained from A by an elementary row operation:
(a) |A| = |B|, if kRi + Rj →Rj on A yields B.
(b) |A| = −|B|, if Ri ↔Rj on A yields B.
(c) |A| = k|B|, if 1
kRi →Ri on A yields B.
Example 1.35
Find the elementary row operation that gives the equalities
(a)

a
b
c
d
 =

a
b
2a + c
2b + d
, (b)

a
b
c
d
 = −

c
d
a
b
, and (c)

a
b
c
d
 = 2

a
2
b
2
c
d
.
Method:
(a) 2R1 + R2 →R2 operates on

a
b
c
d
 to produce

a
b
2a + c
2b + d
.
(b) R1 ↔R2 operates on

a
b
c
d
 to produce −

c
d
a
b
.
(c) This may look the strangest of the three examples. We can think of 1
2 R1 →R1 as
2R1 ←R1, so, for example,

4
6
1
5
 = 2

2
3
1
5
 , via2R1 ←R1, works by effectively
“factoring out 2 from the first row” of

4
6
1
5
. ⃝
If we combine Theorem 1.29 with Theorem 1.28(c), we have a row reduction algorithm
to evaluate determinants.
Example 1.36
Use elementary row operations to evaluate

1
2
3
4
5
6
3
−4
5

.
Method:

1
2
3
4
5
6
3
−4
5

=
−4R1 + R2 →R2
−3R1 + R3 →R3

1
2
3
0
−3
−6
0
−10
−4

=
−10
3 R2 + R3 →R3

1
2
3
0
−3
−3
0
0
16

= (1)(−3)(16) = −48. ⃝
In Example 1.36 we did not use a row exchange or multiplication of a row during the
row reduction process. A row exchange(s) may be needed to get to a row echelon form,

52
Advanced Engineering Mathematics
that is, a row equivalent matrix in upper triangular form. Also, a row exchange(s) may be
useful to do “partial pivoting” to control numerical error.
Example 1.37
Use elementary row operations to evaluate

0
2
3
0
4
5
1
−2
8

.
Method:

0
2
3
0
4
5
1
−2
8

=
R1 ↔R3
−

1
−2
8
0
4
5
0
2
3

=
−1
2 R2 + R3 →R3
−

1
−2
8
0
4
5
0
0
1
2

= −1 · 4 · 1
2 = −2. ⃝
This algorithm enables machines to find the determinant of a large matrix.
Because of the connection between row reduction and determinants, we have
Theorem 1.30
When A is n × n,
A−1 exists
⇐⇒rank(A) = n ⇐⇒
|A| ̸= 0.
Theorem 1.30 is one of the mathematical results most often used by engineers and
scientists.
Because of Theorem 1.30, we know the matrix A is invertible in each of Examples
1.36 and 1.37. Theorem 1.30 is an “existential result,” that is, it tells us whether A−1
exists but does not tell us how to find A−1. But Theorem 1.30 illustrates the concept
that if we study the explanation of a theorem, we can discern an algorithm for getting
numerical results. Here, the row reduction algorithm for determinants is what makes the
explanation “tick.”
Theorem 1.31
If A−1 exists, then |A−1| = 1
|A|.
Besides using elementary row operations to evaluate a determinant, we can also use
“elementary column operations” because of the identity |A| = |AT|. For example, the
elementary column operation of interchanging two columns given by Ci ↔Cj, multiplies
a determinant by (−1) because the operation of interchanging two rows of AT multiplies
its determinant by (−1). Similarly, the operation of adding a multiple of one column into
another column does not change the determinant, for example, 2C1 + C3 →C3 doesn’t
affect the determinant.

Linear Algebraic Equations, Matrices, and Eigenvalues
53
1.6.1 Adjugate Matrix
Definition 1.27
The adjugate ofA is defined by adj(A) ≜

Aji

1 ≤j ≤n
1 ≤i ≤n
=
 
Aij

1 ≤i ≤n
1 ≤j ≤n
 T
, that is, adj(A) is
the transpose of the matrix of cofactors of A.
By the way, many people call adj(A) the “adjoint of A” but use the same abbreviation,
“adj.” We prefer to not use the word adjoint because it means something entirely differ-
ent in the subject of operator theory, which has many applications to matrix theory and
differential equations.
Theorem 1.32
(a) A adj(A) = |A| In = adj(A) A.
(b) If |A| ̸= 0, then A−1 = 1
|A|adj(A).
(c) If |A| = 0, then A adj(A) = O = adj(A) A.
Theorem 1.32(b) gives a formula for A−1 that can be useful for very small matrices
and also for theoretical purposes. But we usually do not use that formula as a method
for computing A−1 because computing adj(A) requires computing n2 determinants of
(n −1) × (n −1) matrices.
Theorem 1.33
Given an n × n matrix A,
(a) rank(adj(A)) =
⎧
⎨
⎩
0,
if rank(A) < n −1
1,
if rank(A) = n −1
n,
if rank(A) = n
⎫
⎬
⎭.
(b) If the nullity of A is one, that is, ν(A) = 1, then every nonzero column of adj(A) is
a nontrivial solution of Ax = 0.
Why? An explanation of Theorem 1.33(a) can be found in Elementary Matrix Theory, by
H. Eves. As for Theorem 1.33(b), let adj(A) ≜B =
B∗1
B∗2
· · ·
B∗n

and note that
AB =
AB∗1
AB∗2
· · ·
AB∗n

by Theorem 1.9 in Section 1.2. Recall from Theorem 1.17
in Section 1.3 that rank(A) + ν(A) = n; hence, if ν(A) = 1, then rank(A) = n −1 and |A| = 0.
So, Theorem 1.32(c) gives O = AB =
AB∗1
AB∗2
· · ·
AB∗n

. So, every nonzero column
of adj(A) is a nontrivial solution of Ax = 0. We call this the “adjugate matrix method” for
finding a nontrivial solution of Ax = 0 when A is not invertible.

54
Advanced Engineering Mathematics
We will apply Theorem 1.33 to find an eigenvector in Section 2.1; this application was
inspired by an example of “mode shape” (actually, an eigenvector of an associated 3 × 3
matrix!) of a three-mass, three-spring mechanical model in Timoshenko et al. (1990). We’ll
discuss an example like Timoshenko et al.’s example in Chapter 5.
Like anything else having to do with the adjugate matrix, this “adjugate matrix method”
for solving Ax = 0 is practical only for very small matrices but is also useful for theoretical
purposes.
Example 1.38
Find the adjugate of A =
⎡
⎣
−5
2
4
2
−8
2
4
2
−5
⎤
⎦and use the result of Theorem 1.33 to find a
x ̸= 0 that solves Ax = 0.
Method:
adj(A) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

−8
2
2
−5

−

2
2
4
−5


2
−8
4
2

−

2
4
2
−5


−5
4
4
−5

−

−5
2
4
2


2
4
−8
2

−

−5
4
2
2


−5
2
2
−8

⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
T
=
⎡
⎣
36
18
36
18
9
18
36
18
36
⎤
⎦.
Adj(A) has rank equal to one, as we can see from doing the elementary row operations
−1
2R1 + R2 →R2, −R1 + R3 →R3 on adj(A).
Theorem 1.33(a) implies rank(A) = 2; hence, ν(A) = 1. Illustrating Theorem 1.33(b),
we see that
⎡
⎣
36
18
36
⎤
⎦is a nonzero solution of Ax = 0. ⃝
We can also define the determinant of a 1 × 1 matrix by det([ a11 ]) = a11. Note that we
should not use the | | notation for the determinant of a 1 × 1 matrix because it would be
confused with the absolute value notation. With this notation, the adjugate matrix of a
2 × 2 matrix A =
	a
b
c
d

is given by
adj(A) =
	 det([ d ])
−det([ c ])
−det([ b ])
det([ a ])

T
=
	 d
−c
−b
a

T
=
	 d
−b
−c
a

.
So, the formula for the inverse of a 2 × 2 matrix given in (1.32) in Section 1.5 is actually the
formula of Theorem 1.32(b) for n = 2.
1.6.2 Cramer’s Rule
If A−1 exists, that is, if |A| ̸= 0, then for every vector b, the system Ax = b has unique
solution given by

Linear Algebraic Equations, Matrices, and Eigenvalues
55
x = A−1b = 1
|A|
⎡
⎢⎢⎢⎢⎣
A11
.
.
.
A1n
.
.
.
.
.
.
.
.
.
An1
.
.
.
Amn
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
b1
b2
...
bn
⎤
⎥⎥⎥⎦;
hence,
x1 = 1
|A|(A11b1 + A21b2 + · · · + An1bn) = 1
|A|(b1A11 + b2A21 + · · · + bnAn1).
But, (b1A11 + b2A21 + · · · + bnAn1) is the determinant of a matrix expanded along its first
column:
(b1A11 + b2A21 + · · · + bnAn1) =

b1
a12
.
.
.
a1n
.
.
.
.
.
.
.
.
.
.
.
.
bn
an2
.
.
.
ann

.
The latter determinant is the same as |A| except for having the first column replaced by the
vector b. This suggests notations for new n × n matrices that mix A and b:
A1(b)
≜
[ b
A∗2
A∗3
· · ·
A∗n ]
A2(b)
≜
[ A∗1
b
A∗3
· · ·
A∗n ]
...
An(b)
≜

A∗1
A∗2
· · ·
A∗(n−1)
b

,
that is, Aj(b) is the matrix obtained from A by replacing its jth column by the vector b. We
have that the system Ax = b has unique solution given by
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
|A1(b)| / |A|
|A2(b)| / |A|
...
|An(b)| / |A|
⎤
⎥⎥⎥⎦.
(1.39)
Theorem 1.34
(Cramer’s rule) If |A| ̸= 0, then for every n vector b, the unique solution of Ax = b is given
by (1.39).
Again, this method for solving Ax = b is useful only for very small systems but is useful
for theoretical purposes.

56
Advanced Engineering Mathematics
Example 1.39
For the system
b = Ax =
⎡
⎣
1
2
3
4
5
6
3
−4
5
⎤
⎦
⎡
⎣
x1
x2
x3
⎤
⎦,
(a) Use Cramer’s rule to find the unique solution in terms of the unspecified parameters
b1, b2, b3.
(b) Use your result for part (a) to find A−1.
Method:
x1 =

b1
2
3
b2
5
6
b3
−4
5


1
2
3
4
5
6
3
−4
5

= −1
48

b1

5
6
−4
5
 −b2

2
3
−4
5
 + b3

2
3
5
6

 
,
x2 =

1
b1
3
4
b2
6
3
b3
5


1
2
3
4
5
6
3
−4
5

= −1
48

−b1

4
6
3
5
 + b2

1
3
3
5
 −b3

1
3
4
6

 
,
and
x3 =

1
2
b1
4
5
b2
3
−4
b3


1
2
3
4
5
6
3
−4
5

= −1
48

b1

4
5
3
−4
 −b2

1
2
3
−4
 + b3

1
2
4
5

 
.
For each of the calculations of x1, x2, and x3, we expanded along the column in which b
is found.
The unique solution is
x =
⎡
⎢⎢⎢⎢⎣
−49
48b1 + 22
48b2 + 1
16b3
1
24b1 + 1
12b2 −1
8b3
31
48b1 −5
24b2 + 1
16b3
⎤
⎥⎥⎥⎥⎦
.
(1.40)
(c) We can rewrite (1.40) as
x = 1
48
⎡
⎣
−49
22
3
2
4
−6
31
−10
3
⎤
⎦
⎡
⎣
b1
b2
b3
⎤
⎦,
(1.41)

Linear Algebraic Equations, Matrices, and Eigenvalues
57
and this is true for all vectors b =
⎡
⎣
b1
b2
b3
⎤
⎦. Because (1.41) is really x = A−1b, we can read
A−1 from (1.41):
A−1 = 1
48
⎡
⎣
−49
22
3
2
4
−6
31
−10
3
⎤
⎦. ⃝
Learn More About It
A good discussion of the definition of determinants using permutations is in Ele-
mentary Linear Algebra with Applications, 7th ed., by Bernard Kolman and David Hill,
Prentice-Hall, c⃝2000. A three-mass, three-spring mechanical model is found in Vibra-
tion Problems in Engineering, 4th edn. by S. Timoshenko, D. H. Young, and W. Weaver,
Jr., John Wiley & Sons, c⃝1990.
1.6.3 Problems
In problems 1–2, for each system, find the determinant by (a) expanding along a row
or column of your choice and (b) using the row reduction algorithm. In (a), state which
row or column you are expanding along. In (b), use only elementary row operations and
notate them.
1.
⎡
⎣
0
1
4
−1
3
2
2
0
1
⎤
⎦
2.
⎡
⎣
0
−1
2
1
5
3
−2
−1
1
⎤
⎦
3. Find an example of 2 × 2 matrices A, B for which |A + B| ̸= |A| + |B|.
4. If A and B are 3 × 3 matrices, |A| = −2, and |B| = 5, find
(a) |AB|, (b) |ATB2|, (c) | −A4B|, (d) |AB−1|.
5. You may assume that |A| = −132, where
A =
⎡
⎢⎢⎣
1
5
1
10
2
7
−1
8
3
9
−2
6
4
0
−3
4
⎤
⎥⎥⎦.
(a) Use |A| and an elementary row operation to find

1
5
1
10
3
12
0
18
3
9
−2
6
4
0
−3
4

.

58
Advanced Engineering Mathematics
(b) Use |A| and an elementary row operation to find

1
5
1
10
1
4
0
6
3
9
−2
6
4
0
−3
4

.
6. Find adj
⎛
⎝
⎡
⎣
1
1
2
−1
2
1
0
1
1
⎤
⎦
⎞
⎠.
7. Suppose a, b, c are unspecified but distinct scalars. Explain why the determinant
result

1
1
1
a
b
c
a2
b2
c2

is (c −a)(c −b)(b −a). [Note: This is an example of a Vandermonde determinant.]
8. Suppose a, b, c are unspecified but distinct scalars. For
A =
⎡
⎣
a
c
0
0
b
a
b
0
c
⎤
⎦,
(a) Find |A| in terms of a, b, c.
(b) Find adj(A) in terms of a, b, c.
(c) Find A adj(A).
9. Suppose the n × n matrix B = αA for some nonzero scalar α and A−1 exists.
(a) Why must B−1 exist? Find a formula for B−1 in terms of A−1 and α.
(b) Find a formula for adj(B) in terms of adj(A) and α.
10. Suppose A is 5 × 5 and AT = 2A. Find all possible values of |A|.
11. You may assume that

1
2
−1
2
0
1
−3
−2
1

= −4.
Use Cramer’s rule to find the exact solution of the system
⎧
⎨
⎩
x1
+
2x2
−
x3
= s
2x1
+
x3
= 5
−3x1
−
2x2
+
x3
= t
⎫
⎬
⎭
in terms of the unspecified parameters s and t.

Linear Algebraic Equations, Matrices, and Eigenvalues
59
12. Consider the system

sx1
+
x2
= 5
−2x1
+
3x2
= 4

.
(a) For which value(s) of the parameter “s” does the system have exactly one
solution?
(b) For the value(s) of “s” you found in part (a), explain how to use Cramer’s rule
to find the solution x =
	x1
x2

. Your final conclusion should be in terms of “s.”
13. Consider the system
sx1
+
x2
= 3
4x1
+
sx2
= −6

.
(a) For which value(s) of the parameter “s” does the system have exactly one
solution?
(b) For the value(s) of “s” you found in part (a), explain how to use Cramer’s rule
to find the solution x =
	x1
x2

. Your final conclusion should be in terms of “s.”
(c) For the all other value(s) of “s,” decide whether the system has a solution(s),
and if it does, find the solution(s).
14. Use Cramer’s rule to find x2 for the system
⎧
⎨
⎩
x1
−
2x3
= 3
2x1
+
x2
−
x3
= −2
−x1
+
2x2
+
x3
= 1
⎫
⎬
⎭.
15. Use Cramer’s rule to find x2 in terms of b1, b2, and b3 for the system
⎧
⎨
⎩
x1
+
x2
+
x3
= b1
x2
−
x3
= b2
−x1
+
x2
−
x3
= b3
⎫
⎬
⎭.
Do not substitute in specific values for b1, b2, and b3.
16. For the system as follows, use Cramer’s rule to find x3 in terms of b1, b2, and b3.
Do not substitute in specific values for b1, b2, and b3.
⎧
⎨
⎩
x1
+
x2
+
x3
= b1
−x1
+
2x2
−
x3
= b2
x2
−
2x3
= b3
⎫
⎬
⎭.
17. For what value(s) of k is
⎡
⎣
4
0
k
1
k
0
k
0
9
⎤
⎦invertible?

60
Advanced Engineering Mathematics
18. Find all value(s) of k for which the matrix A =
⎡
⎣
k
0
k
0
4
1
k
0
2
⎤
⎦is invertible. For those
values of k, find A−1 in terms of k.
19. (a) Why does having two equal rows imply that the determinant of a matrix must
be zero?
(b) Why does having two equal columns imply that the determinant of a matrix
must be zero?
(c) Why are these “alien cofactor identities” true?
0 = ai1Aj1 + ai2Aj2 + · · · + ainAjn, if i ̸= j,
0 = a1jA1i + a2jA2i + · · · + anjAni, if i ̸= j.
20. Suppose A is invertible and
A−1 =
⎡
⎣
1
−4
7
−2
5
−8
3
−6
2
⎤
⎦.
(a) If you had used Cramer’s rule to find the solution of Ax = b =
⎡
⎣
b1
b2
b3
⎤
⎦, what
would be the formula for |A2(b)|/|A| in terms of b1, b2, and b3? Do not
substitute in specific values for b1, b2, and b3.
(b) Find |A| and explain how you found it.
21. Suppose A =
	a
b
c
d

and B =
	e
f
g
h

. Recall a theorem that tells when a 2 × 2
matrix is invertible. Explain why (ae + bg)(cf + dh) −(af + bh)(ce + dg) ̸= 0 if
ad −bc ̸= 0 and eh −fg ̸= 0.
22. Suppose B and C are invertible matrices and AB = C and CA = B. Explain why
|A| is either 1 or −1.
23. Suppose A and B are n × n matrices and AB = −In. Which of the following must
be true? For each one, explain why, or why not.
(a) |A| ̸= 0, (b) B is not invertible, (c) A−1 = −B, (d) |AT| = 0.
24. Suppose A is a singular n × n matrix and b is in Rn.
(a) If Ax = b does have a solution x∗, why must adj(A)b = 0?
(b) If Ax = b does not have a solution, must adj(A)b = 0? If so, why? If not, give a
specific counterexample.
25. (a) Suppose A and B are invertible matrices. Why must adj(AB) = adj(B)adj(A)?
(b) If A and B are 2 × 2 matrices and either or both of A and B are singular, must
adj(AB) = adj(B)adj(A)? If so, why? If not, give a specific counterexample.
26. Let I3 = [ e(1)
e(2)
e(3) ]. Is it possible to have a 3 × 3 matrix A that satisfies
A(y(1) + y(2)) = e(1), A(2y(1) −y(2)) = e(2), and Ay(2) = e(3)
for some vectors y(1), . . . , y(3)? If so, find a specific example; if not, explain
why not.

Linear Algebraic Equations, Matrices, and Eigenvalues
61
27. Suppose an n × n matrix A can be written in the partitioned form A
≜
⎡
⎣
A11

O
−−

−−
O

I
⎤
⎦, where I is the (n −r) × (n −r) identity matrix. Explain why
|A| = |A11|. [Hints: The last n −r columns of A are the last n −r columns of the
n×n identity matrix (why?), so expand the determinant of A along the last column,
and then along the last column of a resulting (n −1) × (n −1) determinant, etc.]
28. Suppose an n × n matrix A can be written in the partitioned form A
≜
⎡
⎣
I

A12
−−

−−
O

A22
⎤
⎦, where I is the r × r identity matrix. Explain why |A| = |A22|.
[Hints: The first r columns of A are the first r columns of the n × n identity matrix
(why?), so expand the determinant of A along the first column and then along the
first column of a resulting (n −1) × (n −1) determinant, etc.]
29. Suppose an n × n matrix A can be written in the partitioned form A
≜
⎡
⎣
A11

A12
−−

−−
O

A22
⎤
⎦, where A11 is r × r. Suppose that A11 is invertible. Explain why
|A| = |A11| |A22|. [Hints: First, verify that we can factor A as
A =
⎡
⎣
A11

O
−−

−−
O

I
⎤
⎦
⎡
⎣
I

A−1
11 A12
−−

−−−
O

A22
⎤
⎦.
After that, use the results of Problems 1.6.3.27 and 1.6.3.28.]
1.7 Linear Independence, Basis and Dimension
Recall from Definition 1.18 in Section 1.3 that a set of vectors, W, has a spanning set,
{v1, . . . , vℓ}, if every vector v in W can be written as a linear combination of vectors
v1, . . . , vℓ, that is, there exists at least one choice of scalars c1, . . . , cℓsuch that v = c1v1 +
· · · + cℓvℓ. The context that we are most concerned with is when W is the set of solu-
tions of a homogeneous system. For example, in Example 1.18 in Section 1.3, the system
x1
+x2
−x4
= 0
x3
+2x4
= 0

has solution set spanned by
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
−1
1
0
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
1
0
−2
1
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
.
We recall from Theorem 1.16 in Section 1.3 that if x(1), . . . , x(ℓ) are all of the basic solu-
tions of a homogeneous system Ax = 0, then {x(1), . . . , x(ℓ)} is a spanning set for the set of
solutions, and no other spanning set has fewer than ℓvectors.

62
Advanced Engineering Mathematics
There is one more underlying concept that we will need, that of “linear independence”
of vectors. Intuitively, a set of vectors {v1, . . . , vℓ} is linearly independent if there are ℓ
“truly different” directions expressed by that set. We need a precise definition in order to
really pin down the concept of “truly different.”
Before we do so, here is a widely useful calculation, which we will use often again in
Chapter 5, as well as many times in this section.
Lemma 1.3
Given vectors v1 =
⎡
⎢⎢⎢⎣
v11
v21
...
vn1
⎤
⎥⎥⎥⎦, v2 =
⎡
⎢⎢⎢⎣
v12
v22
...
vn2
⎤
⎥⎥⎥⎦, . . . , vℓ=
⎡
⎢⎢⎢⎣
v1ℓ
v2ℓ
...
vnℓ
⎤
⎥⎥⎥⎦, form the n×ℓpartitioned matrix
A ≜

v1 
 v2 
 · · ·

 vℓ

and define α ≜
⎡
⎢⎣
α1
...
αℓ
⎤
⎥⎦. Then we have
Aα = α1v1 + · · · + αℓvℓ.
(1.42)
Why? This is really Lemma 1.2 in Section 1.2. 2
1.7.1 Linear Independence
Definition 1.28
(a) In a vector space V, a set of vectors {v1, . . . , vℓ} is linearly independent if there
is only one choice of scalars α1, . . . , αℓwith 0 = α1v1 + · · · + αℓvℓ, namely,
α1 = · · · = αℓ= 0.
(b) A set of vectors is defined to be linearly dependent if it is not linearly indepen-
dent.
Lemma 1.3 explains why a set of vectors {v1, . . . , vℓ} is linearly independent if, and only
if, the homogeneous system

v1 
 v2 
 · · ·

 vℓ

α = 0 has only the trivial solution for α.
Example 1.40
Determine whether the set of vectors
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
1
1
0
1
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
2
0
0
2
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
1
−1
0
1
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
is linearly independent or linearly dependent.

Linear Algebraic Equations, Matrices, and Eigenvalues
63
Method:
Using Lemma 1.2 in Section 1.2, we can rewrite 0 = α1v1 + α2v2 + αℓvℓas a
system whose augmented matrix is
⎡
⎢⎢⎣
1
2
1
| 0
1
0
−1
| 0
0
0
0
| 0
1
2
1
| 0
⎤
⎥⎥⎦
∼
−R1 + R2 →R2
−R1 + R4 →R4
R2 + R1 →R1
⎡
⎢⎢⎣
1⃝
0
−1
| 0
0
-2⃝
−2
| 0
0
0
0
| 0
0
0
0
| 0
⎤
⎥⎥⎦.
Because there is a free variable, this homogeneous system has infinitely many solutions;
hence, there is a nontrivial solution for α, for example, (α1, α2, α3) = (1, −1, 1), by setting
the free variable α3 = 1, hence the given set of three vectors is linearly dependent. ⃝
Theorem 1.35
A set of vectors {v1, . . . , vℓ} in a vector space V is linearly dependent if, and only if, there is
a choice of the index j so that vj is a linear combination of the other (ℓ−1) vectors, that is,
so that
vj is in the Span{v1, . . . , vj−1, vj+1, . . . , vℓ}.
Example 1.41
To illustrate Theorem 1.35, the set of three vectors
{v1, v2, v3} =
⎧
⎨
⎩
⎡
⎣
1
1
−1
⎤
⎦,
⎡
⎣
1
−1
1
⎤
⎦,
⎡
⎣
2
−1
−1
2
⎤
⎦
⎫
⎬
⎭
is linearly independent: In Figure 1.8a, the dashed multiples of the vector v3 are not in
the plane spanned by {v1, v2}. In Figure 1.8b, the dashed multiples of the vector v1 are
not in the plane spanned by {v2, v3}. In Figure 1.8c, the dashed multiples of the vector v2
are not in the plane spanned by {v1, v3}.
Because there is no j for which vj is a linear combination of the other two vectors, the
set of three vectors is linearly independent. ⃝
10
10 x =0 –5–10
5
z =0
–5
–10
–10
–5
y =0
5
10
10 x =0 –10
5
z =0
–5
–10
–5
y =0
5
10
10 x = 0 –10
5
z =0
–5
–10
–5
y =0
5
10
(a)
(b)
(c)
FIGURE 1.8
Linear independence. (a) v3 not in span {v1, v2}, (b) v1 not in span {v2, v3}, (c) v2 not in span {v1, v3}.

64
Advanced Engineering Mathematics
By the way, the planes in Figure 1.8 were plotted by MathematicaTM. For example, the
plane in Figure 1.8a was drawn by the command
ParametricPlot3D[{u + v, u - v, -u + v}, {u, -5, 5}, {v, -5, 5}],
and then we rotated the picture using the mouse. Note that
⎡
⎣
u + v
u −v
−u + v
⎤
⎦= u
⎡
⎣
1
1
−1
⎤
⎦+ v
⎡
⎣
1
−1
1
⎤
⎦
is a linear combination of the vectors [1
1
−1]T, [1
−1
1]T for all scalars u, v.
Fortunately, if our original problem is to find a general solution of a homogeneous sys-
tem and we want to decide if a set of solution vectors is a linearly independent set, the
next theorem says that we don’t have to do the work of row reduction as in Example 1.40.
Once again, we see that a reason we learn theorems and their applications is so that we
don’t have to do the same, tedious work over and over again.
Theorem 1.36
If x(1), . . . , x(ℓ) are all of the basic solutions of a homogeneous system Ax = 0, then the set
of vectors {x(1), . . . , x(ℓ)} is automatically linearly independent.
Corollary 1.2
If x(1), . . . , x(ℓ) are all of the basic solutions of a homogeneous system Ax = 0, then
{x(1), . . . , x(ℓ)} is linearly independent and spans the set W = {x : Ax = 0}.
Basis
We begin with a definition that is a special case of a general definition we will state soon.
Definition 1.29
Suppose A is an m × n matrix and W = {x : Ax = 0}. A finite set of vectors S ≜{v1, . . . , vℓ}
is a basis for W if S is linearly independent and spans W.
Example 1.42
Find a basis for W =
⎧
⎨
⎩x :
⎡
⎣
4
2
4
2
1
2
4
2
4
⎤
⎦x = 0
⎫
⎬
⎭.

Linear Algebraic Equations, Matrices, and Eigenvalues
65
Method: Row reduction gives
⎡
⎣
4
2
4
| 0
2
1
2
| 0
4
2
4
| 0
⎤
⎦
∼
−1
2 R1 + R2 →R2
−R1 + R3 →R3
1
4 R1 →R1
⎡
⎣
1⃝
1
2
1
| 0
0
0
0
| 0
0
0
0
| 0
⎤
⎦.
So, x1 is the only basic variable, and x2 ≜c1, x3 ≜c2 are free variables. Solving for x1 in
terms of x2, x3 will enable us to find all of the basic solutions:
x =
⎡
⎣
x1
x2
x3
⎤
⎦=
⎡
⎣
−1
2 x2 −x3
x2
x3
⎤
⎦=
⎡
⎣
−1
2 c1 −c2
c1
c2
⎤
⎦= c1
⎡
⎣
−1
2
1
0
⎤
⎦+ c2
⎡
⎣
−1
0
1
⎤
⎦≜c1x(1) + c2x(2).
By Corollary 1.2,
⎧
⎨
⎩
⎡
⎣
−1
2
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦
⎫
⎬
⎭is a basis for W. ⃝
1.7.2 Vector Spaces and Subspaces
A vector space is a nonempty set of objects, called vectors, equipped with two operations:
vector addition and multiplication of a scalar times a vector. The operations have to satisfy
certain “axioms.”
Example 1.43
R3 is a vector space when given the usual operations of vector addition and multiplica-
tion of a scalar times a vector:
⎡
⎣
x1
x2
x3
⎤
⎦+
⎡
⎣
y1
y2
y3
⎤
⎦≜
⎡
⎣
x1 + y1
x2 + y2
x3 + y3
⎤
⎦and α
⎡
⎣
x1
x2
x3
⎤
⎦≜
⎡
⎣
αx1
αx2
αx3
⎤
⎦. ⃝
So, vector addition and multiplication by a scalar are defined entry by entry.
Example 1.44
R3 can be generalized. The set Rn consists of all vectors x =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦and is a vector space
when given the usual operations of vector addition and multiplication of a scalar times
a vector:
x + y =
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
y1
y2
...
yn
⎤
⎥⎥⎥⎦≜
⎡
⎢⎢⎢⎣
x1 + y1
x2 + y2
...
xn + yn
⎤
⎥⎥⎥⎦,

66
Advanced Engineering Mathematics
and for any scalar α,
αx = α
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦≜
⎡
⎢⎢⎢⎣
αx1
αx2
...
αxn
⎤
⎥⎥⎥⎦. ⃝
The axioms are all very natural abstractions of properties of vector addition and scalar
multiplication on R3, as in Example 1.43. For example, commutativity of addition says that
for all vectors x, y,
x + y = y + x,
and distributivity of vector addition says that for all scalars α and vectors x, y,
α(x + y) = (αx) + (αy).
There are many other axioms, but the easy way to remember them is that in a vector space,
vector addition and scalar multiplication behave as they behave in R3. We will refer the
reader to other books for a more complete exposition of the axioms.
Definition 1.30
Given a vector space V, a vector subspace W is a nonempty subset of V such that when
vector addition and multiplication by a scalar are restricted to only the vectors in W, all of
the vector space axioms are verified.
The definition allows W = V, that is, that V is a vector subspace of itself. This sounds silly,
but allowing this will simplify certain results later.
Example 1.45
In R3, any plane or line that passes through the origin, that is, contains the point (0, 0, 0),
is a vector subspace. So, the line shown in Figure 1.4 and the planes shown in Figures
1.5 and 1.8 are vector subspaces of R3. The line in Figure 1.6 is not a vector subspace of
R3 because that line does not pass through the origin. ⃝
Caution: The only sets in R3 that are vector subspaces are {0}, lines through the origin,
planes through the origin, and R3 itself. So, a set can contain the origin but still not be a
vector subspace.
Using the definition of a vector subspace is, at best, tedious work, just as tedious as using
the definition of a vector space. Fortunately, there are theorems that save time and effort.
Theorem 1.37
If V is a vector space, W is a subset of vectors in V, and W satisfies the two “closure”
axioms

Linear Algebraic Equations, Matrices, and Eigenvalues
67
(CloseAdd)
If x and y are in W, then x + y is in W,
(CloseMult) If x is in W and α is a scalar, then αx is in W,
then W is a vector subspace of V.
Theorem 1.38
If V is a vector space, W is a subset of vectors in V, and W satisfies the “closure” axiom
(CloseLin) If x and y are in W and α and β are scalars, then αx + βy is in W,
then W is a vector subspace of V.
Theorem 1.39
Let V be the vector space Rn, that is, Example 1.44’s vector space of all real n vectors, with
its usual operations of addition and scalar multiplication. Suppose A is a real, m×n matrix.
Define W ≜{x in Rn : Ax = 0}. Then W is a vector subspace of Rn.
Why? Let x and y be any unspecified vectors in W and let α and β be any unspecified
scalars, that is, real numbers. We will explain why αx + βy being in W follows from x and
y being in W: Because x is in W, we have Ax = 0; similarly, Ay = 0. By linearity, that is,
Theorem 1.15 in Section 1.3, we have
A(αx + βy) = αA(x) + βA(y) = α0 + β0 = 0;
hence, αx + βy is in W. Theorem 1.38 explains why W is a vector subspace of Rn. 2
Definition 1.31
(a) A subspace W has a basis {v1, . . . , vℓ} if {v1, . . . , vℓ} is linearly independent and
Span{v1, . . . , vℓ} = W.
(b) The dimension of a vector subspace W is the number of vectors in a basis for W.
Because of Theorem 1.39, we see that Definition 1.29 is a special case of Definition 1.31
because Definition 1.29 refers to a vector subspace that is defined to be the set of solutions
of a homogeneous system.
Theorem 1.40
(a) For a given vector subspace W, every basis has the same number of vectors.
(b) If W is a subspace of Rn, then there exists a basis for W.

68
Advanced Engineering Mathematics
Because of this theorem, the definition of dimension makes sense: Theorem 1.40(a) says
we cannot get two different values for the dimension of a given vector subspace by using
two bases having different numbers of vectors. Theorem 1.40(b) says we can find a basis;
therefore, we can calculate the dimension of a given vector subspace.
Theorem 1.41
If a vector subspace W in Rn is spanned by a set of vectors {a1, . . . , aℓ}, form the n×ℓmatrix
A = [a1
· · ·
aℓ] and row reduce it to a row echelon form. If the pivot columns are the
i1, . . . , iℓcolumns, then {ai1, . . . , aiℓ} is a basis for W.
Theorem 1.42
(Goldilocks and the sets of vectors): Suppose W is a vector subspace and dim(W) = ℓ.
Suppose E is a set of ℓ−1 vectors in W and G is a set of ℓ+ 1 vectors in W. Then we know
for sure that neither E nor G can be a basis for W.
Why? Because every basis for W should have exactly ℓvectors. 2
Intuitively, E, a set of ℓ−1 vectors, is not a basis for W because E does not have enough
directions, that is, E cannot span W. Intuitively, the reason why G, a set of ℓ+ 1 vectors, is
not a basis for W is that G has too many directions, that is, G is not linearly independent.
It’s as if Goldilocks tried E, but that was too small, and then tried G, but that was too big.
In the fairy tale, “one bed was just right.”
But note that if F is a set of ℓvectors in W, F may or may not be a basis for W; even
though F has the right number of vectors, they may not be linearly independent or may
not span W.
As a special case, consider W = Rn = V, that is, the case where the vector subspace is the
whole space. We have two useful results. The first one is particularly beloved by students
because it can be easy to apply. Indeed, we will use it in Section 2.2 and often in Chapter 5.
Theorem 1.43
Suppose S = {v1, . . . , vn} is a set of n vectors in Rn. Then the following are logically
equivalent:
(a) S is a basis for Rn.
(b) S is linearly independent.
(c) S spans Rn.
(d) | v1
· · ·
vn | ̸= 0.
Why? From the given vectors v1, . . . , vn, form the matrix A ≜

v1
· · ·
vn

. Recall that
Theorem 1.30 says that |A| ̸= 0 ⇐⇒rank(A) = n ⇐⇒A−1 exists.

Linear Algebraic Equations, Matrices, and Eigenvalues
69
As to linear independence of the columns of A, Lemma 1.1 tells us that
0 = [ v1
· · ·
vn ] α = Aα
has its only solution being α = 0 if, and only if, rank(A) = n, if and only if |A| ̸= 0. The set
of vectors S is linearly independent if, and only if, |A| ̸= 0.
As to whether the set of vectors S spans Rn, that is, whether for every b, there exist
scalars c1, . . . , cn so that b = c1v1 + · · · + cnvn = Ac, we know there is a solution, c = A−1b,
when A−1 exists, that is, when |A| ̸= 0. But if |A| = 0, that is, rank(A) < n, then∗the system
Ac = b fails to have a solution, c, for at least one choice of the vector b; hence, S fails to
span Rn. 2
Corollary 1.3
A set of vectors S = {v1, . . . , vp} in Rn
(a) Is linearly dependent if p > n
(b) Fails to span Rn if p < n
Note that Corollary 1.3 gives more specific information than the (Goldilocks) Theo-
rem 1.42.
Theorem 1.44
For any m × n matrix A, rank(AT) = rank(A).
Learn More About It
A good discussion of Theorem 1.44 is in Section 1.15 of Theory of Matrices, by Peter
Lancaster, Academic Press, c⃝1969. Chapter 4 of Linear Algebra and Its Applications,
3rd ed., by David C. Lay, Pearson, c⃝2003, has a very good discussion of the axioms
of vector spaces, and the whole book is an excellent resource for matrix algebra.
1.7.3 Problems
1. Find a criterion satisfied by all vectors b =
⎡
⎣
b1
b2
b3
⎤
⎦that are in Span
⎧
⎨
⎩
⎡
⎣
2
1
0
⎤
⎦,
⎡
⎣
−2
2
3
⎤
⎦,
⎡
⎣
2
4
3
⎤
⎦
⎫
⎬
⎭.
∗Suppose Eq, Eq−1, . . . , E2, E1 are elementary matrices that row reduce A to its RREF, that is, EqEq−1 · · · E2E1A =
RREF(A). Let b = E−1
q
· · · E−1
1 e(n), so [ A | b ] ∼

RREF(A) | e(n) 
explains why Ax = b has no solution for this
vector b because RREF(A) ̸= In implies that at least the bottom row of RREF(A) is all zeros.

70
Advanced Engineering Mathematics
2. For what value(s) of k do the vectors
⎧
⎨
⎩
⎡
⎣
1
0
−1
⎤
⎦,
⎡
⎣
1
k
0
⎤
⎦,
⎡
⎣
0
−2
k
⎤
⎦
⎫
⎬
⎭
span R3?
3. Find the exact value(s) of t for which the set of vectors
⎧
⎨
⎩
⎡
⎣
1
−1
2
⎤
⎦,
⎡
⎣
t
2
3
⎤
⎦,
⎡
⎣
0
1
t
⎤
⎦
⎫
⎬
⎭
is linearly dependent.
4. Are the vectors
⎡
⎣
1
−1
1
⎤
⎦,
⎡
⎣
−2
3
−1
⎤
⎦,
⎡
⎣
0
1
1
⎤
⎦
linearly independent? If so, why? If not, explicitly write one of the three vectors
as a linear combination of the other two vectors.
5. For each of the sets of vectors as follows, determine whether the set is linearly
independent, spans R3, and/or is a basis for R3, and justify your conclusions.
(a)
⎧
⎨
⎩
⎡
⎣
1
1
1
⎤
⎦,
⎡
⎣
3
1
1
⎤
⎦,
⎡
⎣
1
−1
−1
⎤
⎦
⎫
⎬
⎭
(b)
⎧
⎨
⎩
⎡
⎣
1
1
0
⎤
⎦,
⎡
⎣
1
0
1
⎤
⎦,
⎡
⎣
0
1
1
⎤
⎦
⎫
⎬
⎭
(c)
⎧
⎨
⎩
⎡
⎣
1
1
0
⎤
⎦,
⎡
⎣
1
0
1
⎤
⎦
⎫
⎬
⎭
(d)
⎧
⎨
⎩
⎡
⎣
1
2
3
⎤
⎦,
⎡
⎣
0
1
4
⎤
⎦,
⎡
⎣
−1
0
5
⎤
⎦,
⎡
⎣
1
−3
0
⎤
⎦
⎫
⎬
⎭
6. Suppose A =

v(1)
v(2)
v(3)
v(4) 
is a 6 × 4 matrix and

v(2), v(3)
is a lin-
early independent set. Determine all possible values of rank(A) and state your
conclusions in the form
≤rank(A) ≤
. Explain how you arrived at your
conclusions.
7. Suppose A =

v(1)
v(2)
v(3)
v(4)
v(5) 
is a 4 × 5 matrix and

v(1), v(2)
is a
linearly independent set. Fill in the blanks as follows and explain how you arrived
at your conclusions.
(a) Determine all possible values of rank(A) and state your conclusions in the
form
≤rank(A) ≤
. Explain how you arrived at your conclusions.
(b) Determine all possible values of the nullity(A) and state your conclusions in
the form
≤ν(A) ≤
. Explain how you arrived at your conclusions.

Linear Algebraic Equations, Matrices, and Eigenvalues
71
8. Suppose A is an invertible 3 × 3 matrix and {b1, b2, b3} is a basis for R3.
(a) Why is B ≜[b1
b2
b3] invertible?
(b) Why is |AB| ̸= 0?
(c) Use parts (a) and (b) and Theorem 1.9 in Section 1.2 to explain why
{Ab1, Ab2, Ab3} is a basis for R3.
9. For the matrix A =
	1
−1
2
3
1
0

, find
(a) rank(A), (b) ν(A), and (c) a basis for W ≜{x in R3 : Ax = 0}.
10. Assume A is an 4 × 4 matrix that is not invertible. Which of the following must be
true?
(a) The columns of A are linearly independent.
(b) There is no 4 × 4 matrix B such that AB = I4.
(c) The system Ax = 0 has no solution.
(d) Two columns of A are identical.
(e) There is a vector x in R4 with x ̸= 0 and Ax = 0.
(f) A has four pivot positions.
11. (a) Find an example of a 2 × 3 matrix A such that the nullity of AT does not equal
the nullity of A.
(b) Why does A being square imply that the nullity of AT must equal the nullity
of A? [Hint: Theorem 1.44 is relevant.]
12. Explain why Theorem 1.35 is true.
Key Terms
adjugate: Definition 1.27 in Section 1.6
augmented matrix: before Lemma 1.1
basic solution: Definition 1.19 in Section 1.3
basic variable: Definition 1.5 in Section 1.1.3
basis: Definitions 1.29 and 1.31 in Section 1.7
complete set of basic solutions: Definition 1.19 in Section 1.3
Cramer’s rule: Theorem 1.34 in Section 1.6.2
determinant: Definitions 1.24 through 1.26 in Section 1.6
diagonal matrix: Definition 1.11 in Section 1.2
dimension: Definition 1.31 in Section 1.7
elementary column operations: after Theorem 1.31 in Section 1.6
elementary row operations: Definition 1.3 in Section 1.1
equivalent system: after (1.1) in Section 1.1
free variable: Definition 1.5 in Section 1.1.3
Gaussian elimination method: before Example 1.2 in Section 1.1
Gauss–Jordan method: after Definition 1.4 in Section 1.1
general linear combination: Definition 1.16 in Section 1.3
general solution: Definition 1.17 in Section 1.3, Definition 1.22 in Section 1.4
homogeneous system: Definition 1.15 in Section 1.3
identity matrix: Definition 1.9 in Section 1.2

72
Advanced Engineering Mathematics
inverse, invertible: Definition 1.23 in Section 1.5
leading entry: before Definition 1.1 in Section 1.1
leading 1: before Definition 1.1 in Section 1.1
linearly dependent: Definition 1.28 in Section 1.7
linearly independent: Definition 1.28 in Section 1.7
lower triangular: Definition 1.12 in Section 1.2
matrix: before Definition 1.1 in Section 1.1
matrix addition: beginning of Section 1.2
matrix multiplication: Definition 1.8 in Section 1.2
nonhomogeneous system: Section 1.4
non-invertible: Definition 1.23 in Section 1.5
non-singular: Definition 1.23 in Section 1.5
nonzero row: before Definition 1.1 in Section 1.1
nullity: Definition 1.20 in Section 1.3
particular solution: Definition 1.21 in Section 1.4
partition a matrix into blocks: Section 1.5.2; Example 1.32 in Section 1.5
partitioned matrix: before Lemma 1.2 in Section 1.2
pivot column: before Definition 1.2 in Section 1.1
pivot positions: Definition 1.1 in Section 1.1
powers: Definition 1.13 in Section 1.2
Rn: before Definition 1.7 in Section 1.2
rank: Definition 1.2 in Section 1.1
row echelon form: Definition 1.1 in Section 1.1
row echelon form of C: after Definition 1.3 in Section 1.1
row reduced echelon form (RREF): Definition 1.4 in Section 1.1.3
singular: Definition 1.23 in Section 1.5
solution: after (1.1) in Section 1.1
spanning set: Definition 1.18 in Section 1.3
square: before Definition 1.1 in Section 1.1
transpose: Definition 1.14 in Section 1.2
trivial solution: Theorem 1.14 in Section 1.3
upper triangular: Definition 1.12 in Section 1.2
Vandermonde determinant: Problem 1.6.3.7
vector space: Section 1.7.2
vector subspace: Definition 1.30 in Section 1.7
zero matrix: Definition 1.10 in Section 1.2.1
zero row: before Definition 1.1 in Section 1.1
MATLAB R⃝Commands
RREF: Definition 1.4 in Section 1.1
Mathematica Commands
ParametricPlot3D[{u + v, u - v, -u + v}, {u, -5, 5}, {v, -5, 5}]:
after Ex. 1.41

Linear Algebraic Equations, Matrices, and Eigenvalues
73
References
Carlson, D. Teaching linear algebra: Must the fog always roll in? College Mathematics Journal 24,
29–40, 1993.
Lederer, E.M. (ed.) Linear Algebra Exam File. Engineering Process, Inc., San Jose, CA, 1989.
Timoshenko, S., Young, D.H., and Kleaver, W., Vibration Problems in Engineering, 4th edn. John Wiley
& Sons, New York, 1990.
Yuster, T. The reduced row echelon form of a matrix is unique: A short proof. Mathematics Magazine
57, 93–94, 1984.


2
Matrix Theory
2.1 Eigenvalues and Eigenvectors
This topic is both at the heart of matrix theory and essential to the study of many topics
in engineering and science. We will see that Chapter 5 relies on this topic to solve systems
of linear ordinary differential equations. In Chapter 11, we will see how the same concept
applies to solving linear partial differential equations.
Example 2.1
Let A =
−4
−2
6
3

, x(1) =
 2
−3

, and x(2) =
1
2

. We have
Ax(1) =
−4
−2
6
3
  2
−3

=
−2
3

= (−1)
 2
−3

= (−1)x(1), and
Ax(2) =
−4
−2
6
3
  1
−2

=
0
0

= (0)
 1
−2

= (0)x(2).
If we denote λ1 = −1 and λ2 = 0, we have
Ax(1) = λ1x(1)
and
Ax(2) = λ2x(2). ⃝
These fit the following definition, which involves a universal concept and “tool” of
engineering, science, and mathematics:
Definition 2.1
A square matrix A has eigenvalue λ if the system Ax = λx has at least one non-trivial
solution for x, in which case we call such an x ̸= 0 a corresponding eigenvector.
So, in Example 2.1, A =
−4
−2
6
3

has eigenvalue λ1 = −1, with corresponding
eigenvector
 2
−3

, and eigenvalue λ2 = 0, with corresponding eigenvector
 1
−2

.
Geometrically, A has an eigenvector x if Ax is either parallel to x or is 0. If λ is the
corresponding eigenvalue, then |λ| is the magnification of Ax versus x; if λ is negative then
Ax points in the direction opposite to that of x. The results of Example 2.1 are summarized
pictorially in Figure 2.1.
75

76
Advanced Engineering Mathematics
Ax(1)
x(1)
x(2)
x2
x2
x1
x1
Ax(2)=0
FIGURE 2.1
Geometry of eigenvectors.
Theorem 2.1
λ is an eigenvalue of A if, and only if, 0 = |A −λI|.
Why? First, let’s rewrite Ax = λx:
Ax = λx ⇐⇒0 = Ax −λx = Ax −λInx = (A −λIn)x.
So,
λ is an eigenvalue of A ⇐⇒(A −λI)x = 0 has a solution x ̸= 0.
(2.1)
But, by Theorems 1.30 in Section 1.6 and 1.25 in Section 1.5, the homogeneous system
(A−λI)x = 0 has a solution x ̸= 0 if, and only if, the matrix (A−λI) has zero determinant. 2
Corollary 2.1
A is invertible if, and only if, 0 is not an eigenvalue of A.
Why? A is invertible ⇐⇒|A| ̸= 0 ⇐⇒|A −0 · I| ̸= 0 ⇐⇒λ = 0 is not an eigenvalue
of A. 2
Definition 2.2
The characteristic equation for an n × n matrix A is 0 = |A −λIn|, and the characteris-
tic polynomial for A is the function P(λ) ≜|A −λIn|.
Example 2.2
(Example 2.1 again) Find all of the eigenvalues of A =
−4
−2
6
3

.

Matrix Theory
77
Method:
0 = |A −λI2| =

−4
−2
6
3

−λ
1
0
0
1
 =

−4 −λ
−2
6
3 −λ

= (−4 −λ)(3 −λ) −(−2)(6) = λ2 + λ = λ(λ + 1).
The eigenvalues λ satisfy 0 = λ or 0 = λ + 1. The eigenvalues are λ = 0 and λ = −1. ⃝
Example 2.3
Find all of the eigenvalues of A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦.
Method:
0 = | A −λI3 | =

2 −λ
2
4
2
−1 −λ
2
4
2
2 −λ

=
−R1 + R3 →R3

2 −λ
2
4
2
−1 −λ
2
2 + λ
0
−2 −λ

=
R3 ←(2 + λ)R3
(2 + λ)

2 −λ
2
4
2
−1 −λ
2
1
0
−1

= (2 + λ)
	
1 ·

2
4
−1 −λ
2
 + (−1) ·

2 −λ
2
2
−1 −λ


, by expanding along R3
= (2 + λ)(4 −4(−1 −λ) −((2 −λ)(−1 −λ) −4)) = (2 + λ)(−λ2 + 5λ + 14)
= (2 + λ)(2 + λ)(7 −λ).
The eigenvalues are λ1 = −2, λ2 = −2, λ3 = 7. ⃝
You may wonder why we bothered to list −2 twice in Example 2.3. It will turn out
that we need the repetition of the eigenvalue both for a practical reason when solving
differential equations and for theoretical reasons.
Note that we used elementary row operations to simplify the determinant. There are
two primary reasons for this: expanding the determinant along a row or column gives the
cubic equation 0 = −λ3+3λ2+24λ+28, for which factoring could be a chore. (Admittedly,
we could use technology to find the roots of the characteristic equation pretty easily.) Sec-
ond, if we can simplify an algebraic expression, we usually make fewer algebraic errors.
You can test this yourself by seeing if you get the same characteristic equation by another
method.
How did we know that −R1+R3 →R3 would be so useful? We tried it because it created
a 0 in the (3, 2) entry, which we thought might help. To tell the truth, using that particular
elementary row operation turned out to be even more helpful than we imagined.
Similarly, if instead we had used the column operation C1 −C2 →C1, it also would have
been very helpful.

78
Advanced Engineering Mathematics
The beauty of the characteristic equation is that, in theory, it finds the exact eigenvalues.
But for “large” matrices, solving the characteristic equation can be a monster of a problem.
We will study numerical methods for finding eigenvalues in Section 8.5. Certainly, we
should solve the characteristic equation by hand only for very small matrices.
Nevertheless, exploring eigenvalues and eigenvectors of small matrices by hand is use-
ful for all but the most abstract thinkers. This is a case where, in the long run, it helps to
spend some effort “opening up the hood to see what’s really going on” and “getting your
hands dirty” with the details. There are quite a few concepts concerning eigenvalues and
eigenvectors that are more easily understood in the context of simple examples.
Given a matrix and its eigenvalues, how do we find the corresponding eigenvectors?
The answer is found in (2.1): Individually for each eigenvalue λ, solve the homogeneous
system (A −λIn)x = 0 for x.
Example 2.4
Find all of the eigenvectors of the matrix A =
−4
−2
6
3

:
Method: In Example 2.2, we found that λ1 = −1, λ2 = 0 gave all of the eigenvalues.

A −λ1I2 | 0

=

A −(−1)I2 | 0

=
−4 −(−1)
−2
| 0
6
3 −(−1)
| 0

=
−3
−2
| 0
6
4
| 0

∼
−1
3R1 →R1
−6R1 + R2 →R2

1⃝
2
3
| 0
0
0
| 0

.
The solutions are x =
x1
x2

=

−2
3c1
c1

= c1

−2
3
1

; this gives an eigenvector for any
c1 ̸= 0.
Likewise,

A −λ2I2 | 0

=

A −(0)I2 | 0

=
−4
−2
| 0
6
3
| 0

∼
−1
4R1 →R1
−6R1 + R2 →R2

1⃝
1
2
| 0
0
0
| 0

.
The solutions are x =
x1
x2

=

−1
2 c1
c1

= c1

−1
2
1

.
This gives an eigenvector for any c1 ̸= 0. ⃝
Some people prefer that eigenvectors be written as multiples of vectors without fractions
and with as few negative signs as possible. So, in Example 2.4, we might summarize the
eigenvectors as being
x = ˆc1x(1), where ˆc1 ̸= 0 and x(1) =
 2
−3

, corresponding to λ1 = −1,
and
x = ˆc1x(2), where ˆc1 ̸= 0 and x(2) =
 1
−2

corresponding to λ2 = 0.

Matrix Theory
79
For each eigenvalue λj, RREF(A −λjI) must have a row of zeros. Indeed, if RREF(A −λjI)
does not have a row of zeros, then an “alarm bell” should ring in our heads because the
system (A −λjI)x = 0 would have only the trivial solution for x, that is, there would be no
eigenvector, hence λj would not be an eigenvalue.
So, if we fail to get an eigenvector, what went wrong? Perhaps we made an error in
finding the eigenvalues, or in substituting them into (A −λjI)x = 0, or in row reducing

A −λjI | 0

.
2.1.1 The Adjugate Matrix Method for Finding an Eigenvector
Theorem 2.2
(Adjugate matrix method) Suppose an n × n matrix A has an eigenvalue λ and adj(A −λI)
is not the zero matrix. Then each nonzero column of adj(A −λI) is an eigenvector of A
corresponding to eigenvalue λ.
Why? Because A has an eigenvalue λ, the n × n matrix B ≜A −λI has |B| = 0. By Theorem
1.30 in Section 1.6, rank(B) < n. By Theorem 1.33b in Section 1.6, rank(adj(B)) ≤1. Because
adj(B) = adj(A −λI) is not the zero matrix, rank(adj(B)) ̸= 0. It follows that rank(adj(B)) = 1,
so Theorem 1.33(b) in Section 1.6 implies that every nonzero column of adj(B) is a nontrivial
solution of (A−λI)x = Bx = 0, hence is an eigenvector of A corresponding to eigenvalue λ. 2
Example 2.5
Find all of the eigenvectors of the matrix A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦.
Method:
In Example 2.3, we found that λ1 = −2, λ2 = −2, λ3 = 7 gave all of the
eigenvalues.

A −λ1I3 | 0

=

A −(−2)I3 | 0

=
⎡
⎣
2 −(−2)
2
4
| 0
2
−1 −(−2)
2
| 0
4
2
2 −(−2)
| 0
⎤
⎦=
⎡
⎣
4
2
4
| 0
2
1
2
| 0
4
2
4
| 0
⎤
⎦.
We studied this in Example 1.42 in Section 1.7 and found all solutions. The eigenvec-
tors are
x =
⎡
⎣
x1
x2
x3
⎤
⎦=
⎡
⎣
−1
2 c1 −c2
c1
c2
⎤
⎦= c1
⎡
⎣
−1
2
1
0
⎤
⎦+ c2
⎡
⎣
−1
0
1
⎤
⎦, any c1, c2 with |c1| + |c2| ̸= 0.
Note that we don’t have to repeat the above for λ2 because λ2 = −2 = λ1.
As for the last eigenvalue,

A −λ3I3 | 0

=

A −(7)I3 | 0

=
⎡
⎣
−5
2
4
| 0
2
−8
2
| 0
4
2
−5
| 0
⎤
⎦.

80
Advanced Engineering Mathematics
Rather than do row reduction, we will use the “adjugate matrix method” of Theorem
2.2: because λ3 = 7 is an eigenvalue of A, each nonzero column of adj(A −7I) is an eigen-
vector of A corresponding to eigenvalue λ = 7. We have from Example 1.38 in Section 1.6
that adj(A −7I) =
⎡
⎣
36
18
36
18
9
18
36
18
36
⎤
⎦, so x =
⎡
⎣
36
18
36
⎤
⎦is an eigenvector of A corresponding to
eigenvalue λ = 7. We can conclude that there are eigenvectors x = c1
⎡
⎣
2
1
2
⎤
⎦, any c1 ̸= 0. ⃝
How did we know that the adjugate matrix method would work, that is, that adj(A −7I)
would have a nonzero column? The truth is that we didn’t know it would work for sure
but we gave it a try. But, in Example 2.7 we will use a little more theory to be sure of
ourselves. In this textbook, the role of theory is to organize the results, provide methods
for solving things, know what to calculate, and know when it’s o.k. to go ahead and start
calculating.
Alternatively, we could have found an eigenvector corresponding to λ = 7 by using the
usual row reduction:

A −λ3I3 | 0

=

A −(7)I3 | 0

=
⎡
⎣
−5
2
4
| 0
2
−8
2
| 0
4
2
−5
| 0
⎤
⎦∼· · · ∼
⎡
⎣
1⃝
0
−1
| 0
0
1⃝
−1
2
| 0
0
0
0
| 0
⎤
⎦.
Multiplicities: First, we note that the characteristic polynomial of n × n matrix A,
P(λ) ≜| A −λIn | −

a11 −λ
a12
.
.
.
a1n
a21
a22 −λ
.
.
.
a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1
an2
.
.
.
ann −λ

,
is an n-th degree polynomial in λ. So,
P(λ) = 0 has exactly n roots λ1, λ2, . . . , λn, possibly complex
and possibly including repetition(s)
hence
P(λ) = (λ1 −λ)(λ2 −λ) · · · (λn −λ).
(2.2)
For example, in Example 2.3 we saw that A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦has
P(λ) = (−2 −λ)(−2 −λ)(7 −λ).

Matrix Theory
81
Definition 2.3
If λj is an eigenvalue of A, the algebraic multiplicity of λj is the number of times (λj −λ)
appears as a factor of P(λ) in (2.2). We will denote by αj the algebraic multiplicity of λj.
An eigenvalue whose algebraic multiplicity is one is called simple. Some people refer to
an eigenvalue whose algebraic multiplicity is greater than one as degenerate.
If μ1, μ2, . . . , μp are the distinct eigenvalues of A, then
P(λ) = (μ1 −λ)α1(μ2 −λ)α2 · · · (μp −λ)αp.
(2.3)
This way of writing the characteristic polynomial displays all of the eigenvalues and their
algebraic multiplicities.
Example 2.6
For the matrix A of Examples 2.3 and 2.5, P(λ) = (−2 −λ)2(7 −λ)1. ⃝
Definition 2.4
If μj is an eigenvalue of A, the geometric multiplicity of μj is the nullity of the matrix
A −μj I, that is, ν(A −μj I), namely, the number of basic solutions of the homogeneous
system (A −μj I)x = 0. The geometric multiplicity of μj is denoted by mj.
Theorem 2.3
Suppose n × n matrix A has distinct eigenvalues μ1, μ2, . . . , μp and corresponding alge-
braic multiplicities α1, α2, . . . , αp and geometric multiplicities m1, m2, . . . , mp.
Then,
(a) α1 + α2 + . . . + αp = n.
(b) 1 ≤mj ≤αj for each 1 ≤j ≤p.
(c) if αj = 1, then mj = 1.
We recall from Theorem 1.17 in Section 1.3 that for each 1 ≤j ≤p, rank(A −μjI) + ν(A −
μjI) = n. So, rank(A −μjI) = n −ν(A −μjI) = (the number of pivot columns of A −μjI).
Example 2.7
For A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦, that is, the matrix of Examples 2.3 and 2.5, find the algebraic
and geometric multiplicities of the eigenvalues.

82
Advanced Engineering Mathematics
Method:
From Example 2.6, we know that the distinct eigenvalues are μ1 = −2
and μ2 = 7 and that their algebraic multiplicities are α1 = 2, α2 = 1, respectively. From
Theorem 2.3(c), we have m2 = 1, that is, the geometric multiplicity of the eigenvalue
λ = 7 must be one, hence the rank of (A−7I) is 3−1 = 2. It follows from Theorem 2.2 that
the adjugate matrix method works here, that is, that adj(A −7I) has a nonzero column of
that is an eigenvector of A corresponding to eigenvalue λ = 7. ⃝
Example 2.8
Suppose x is an eigenvector for each of the matrices A, B, C,
corresponding to
eigenvalues λ, β, γ , respectively. If A−1B = C, find a relationship among λ, β, and γ .
Method:
First, writing C = A−1B implicitly tells us that A must be invertible. From
Corollary 2.1, it follows that λ ̸= 0. Next, by definition of eigenvalue and eigenvec-
tor, we were given that x ̸= 0 and Ax = λx, Bx = βx, and Cx = γ x. But, Ax = λx yields
x = A−1Ax = λA−1x, hence A−1x = λ−1x. So,
γ x = Cx = (A−1B)x = A−1(Bx) = A−1(βx) = β(A−1x) = β(λ)−1x.
Since x ̸= 0, it follows that γ = β
λ , which is a relationship among λ, β, and γ . ⃝
2.1.2 Complex Numbers
First, let’s note some facts about complex numbers: a number z is complex if z = x + iy,
where x and y are real and i ≜
√
−1. [If you’re an electrical engineer, use j instead of i.] For
example, both −1 + i2 and −1 + i0 are complex. So, all real numbers are also complex.
Given a complex number z = x+iy, where x and y are real, we call x the real part of z and
write x = Re(z), and we call y the imaginary part of z and write y = Im(z). If Im(z) = 0, we
say z is real. We denote z = x −iy and call it the complex conjugate of z. We have a basic
fact: wz = w z for all complex numbers w, z. Complex numbers are illustrated in Figure 2.2.
We have two useful facts:
Re(z) = z + z
2
and
Im(z) = z −z
2i .
Im(z)=y
z =x+ iy
x =Re(z)
z =x–iy
FIGURE 2.2
Complex numbers.

Matrix Theory
83
When we divide by a complex number, it helps to use the complex conjugate to
“rationalize the denominator.” For example,
1 −i2
3 + i4 = 1 −i2
3 + i4 · 3 −i4
3 −i4 = (1 −i2)(3 −i4)
(3 + i4)(3 −i4) = −5 −i10
32 −(i4)2 = −5 −i10
25
= −1
5 −i 2
5.
It helps to multiply by the complex conjugate of the denominator because if z = x + iy,
then zz = (x + iy)(x −iy) = x2 −(iy)2 = x2 + y2 is real.
2.1.3 Complex Eigenvalues and Eigenvectors
Example 2.9
For A =
1
−2
4
−3

, find all of the eigenvalues and the corresponding eigenvectors.
Method: 0 = | A −λI2 | =

1 −λ
−2
4
−3 −λ
 = (1 −λ)(−3 −λ) −(−2)(4) = λ2 + 2λ + 5,
so by the quadratic formula the eigenvalues are
λ = −2 ±

22 −4 · 1 · 5
2 · 1
= −2 ± i4
2
= −2
2 ± i4
2 = −1 ± i2,
To find the eigenvectors, use row reduction on

A −λI | 0

:

A −(−1 + i2)I2 | 0

=
2 −i2
−2
| 0
4
−2 −i2
| 0

∼
R1 ↔R2
1
4R1 →R1
−(2 −i2)R1 + R2 →R2

1⃝
−1
2 −i 1
2
| 0
0
0
| 0

.
Note that the last (2, 2) entry is −(2 −i2)

−1
2 −i 1
2

+ (−2) = −(−1 −1) −2 = 0. The
eigenvectors are x = c1
 1
2 + i 1
2
1

, for any complex number c1 ̸= 0.
Likewise,

A −(−1 −i2)I2 | 0

=
2 + i2
−2
| 0
4
−2 + i2
| 0

∼
R1 ↔R2
1
4R1 →R1
−(2 + i2)R1 + R2 →R2

1⃝
−1
2 + i 1
2
| 0
0
0
| 0

.
The eigenvectors are x = c1
 1
2 −i 1
2
1

, for any complex number c1 ̸= 0. ⃝

84
Advanced Engineering Mathematics
We notice that in Example 2.9, the eigenvalues came in a “complex conjugate pair”:
λ1 = −1 + i2, λ2 = −1 −i2. Not only that, the eigenvectors come in a “complex conjugate
pair”:
 1
2 + i 1
2
1

and
 1
2 −i 1
2
1

, that is,
 1
2
1

+ i
 1
2
0

and
 1
2
1

−i
 1
2
0

.
Here we define complex conjugation of a vector or matrix “entry by entry,” for example,
 1
2 + i 1
2
1

≜

1
2 + i 1
2
1

=
 1
2 −i 1
2
1

.
This can be generalized:
Theorem 2.4
If A is real, then its eigenvalues and eigenvectors come in complex conjugate pairs. That
is, if λ is an eigenvalue of A with corresponding eigenvector z, then λ is an eigenvalue of
A and z is a corresponding eigenvector.
Why? First, A being real means A = A, so
A −λI = A −λI = A −λI = A −λI.
If z is an eigenvector corresponding to A’s eigenvalue λ, then z ̸= 0 and 0 = (A −λI)z. It
follows that
z ̸= 0 and 0 = 0 = (A −λI)z = (A −λI) z = (A −λI)z,
hence z is an eigenvector of A corresponding to eigenvalue λ. 2
So, strangely enough, finding eigenvectors corresponding to complex eigenvalues of a
real matrix A needs about half of the work of finding eigenvectors corresponding to real
eigenvalues. That’s because Theorem 2.4 tells us that once we have found an eigenvector
corresponding to eigenvalue λ, we don’t need to do row reduction to find an eigenvec-
tor corresponding to eigenvalue λ. We will take advantage of this in Example 2.12 in
Section 2.2.

Matrix Theory
85
2.1.4 Eigenvalues and Eigenvectors of Triangular and Diagonal Matrices
It’s easy to find the eigenvalues and eigenvectors of a diagonal matrix. We’ll start with an
example.
Example 2.10
Find all eigenvalues and eigenvectors of A =
⎡
⎢⎢⎣
4
0
0
0
0
3
0
0
0
0
2
0
0
0
0
1
⎤
⎥⎥⎦.
Method:
0 = | A −λI4 | =

4 −λ
0
0
0
0
3 −λ
0
0
0
0
2 −λ
0
0
0
0
1 −λ

= (4 −λ)(3 −λ)
(2 −λ)(1 −λ) by Theorem 1.28(c) in Section 1.6, the fact that the determinant of a trian-
gular matrix is the product of the diagonal entries. To find corresponding eigenvectors,
row reduction is easy:
0 =

A −4I4 | 0

=
⎡
⎢⎢⎣
0
0
0
0
| 0
0
−1
0
0
| 0
0
0
−2
0
| 0
0
0
0
−3
| 0
⎤
⎥⎥⎦∼
⎡
⎢⎢⎣
0
1⃝
0
0
| 0
0
0
1⃝
0
| 0
0
0
0
1⃝
| 0
0
0
0
0
| 0
⎤
⎥⎥⎦,
so it’s easy to see that the eigenvectors corresponding to eigenvalue 4 are
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
c1
0
0
0
⎤
⎥⎥⎦= c1
⎡
⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎦, for any c1 ̸= 0.
Similarly, the eigenvectors corresponding to eigenvalue 3 are c1
⎡
⎢⎢⎣
0
1
0
0
⎤
⎥⎥⎦, for any c1 ̸= 0;
the eigenvectors corresponding to eigenvalue 2 are c1
⎡
⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎦, for any c1 ̸= 0; and the
eigenvectors corresponding to eigenvalue 1 are c1
⎡
⎢⎢⎣
0
0
0
1
⎤
⎥⎥⎦, for any c1 ̸= 0. ⃝
By the way, for any n × n diagonal matrix, the eigenvectors are the columns of the iden-
tity matrix, that is, the vectors e(1), e(2), . . . , e(n) used in the discussion of the row reduction
algorithm for constructing the inverse in Section 1.5.
Theorem 2.5
If A is an upper or lower triangular matrix, then the eigenvalues of A are the diagonal
entries of A.

86
Advanced Engineering Mathematics
Additionally, in the special case when A is an n × n upper or lower triangular matrix and
has n distinct eigenvalues, we can say a lot about the eigenvectors of A, as we will see in
Problem 2.1.6.21.
Caution: These results sometimes mislead people. It would be natural to ask, “If we
row reduce a matrix to a row echelon form, which is upper triangular, can we use that
to find the eigenvalues of the original matrix?” Unfortunately, the answer is “No!” The
eigenvalues of a matrix in RREF are all ones and zeros, so this idea of using a row echelon
form would seem to say that every matrix has all of its eigenvalues being only ones and
zeros, which is clearly nonsense. So, it’s good to have a degree of skepticism about things
that sound good but need further study and explanation.
2.1.5 MATLAB R⃝and MathematicaTM
The software package MATLAB was originally developed to apply robust, industrial-
strength algorithms for working with matrices, which is where the MAT part of its name
came from. Since then, the software package has developed in many directions.
The eig commands in MATLAB give approximate eigenvalues and eigenvectors. For
example, for the matrix in Problem 2.1.6.14, we entered
>A=[-10.07106781,-5.656854249,0;5.656854249, 4.071067812,0;0,0,3]
and >[V,D] = eig(A) and got
V =
−0.8944
0.4472
0
0.4472 −0.8944
0
0
0
1.0000
D =
−7.2426
0
0
0
1.2426
0
0
0
3.0000
This says that the approximate eigenvalues of A are -7.2426,1.2426,3.0000, and
corresponding approximate eigenvectors are
⎡
⎣
−0.8944
0.4472
0
⎤
⎦,
⎡
⎣
0.4472
−0.8944
0
⎤
⎦,
⎡
⎣
0
0
1.0000
⎤
⎦.
Eigenvectors produced by MATLAB are all normalized to have length one, for example,
(−0.8944)2 + 0.44722 + 02 ≈0.9999392. We can ask for more displayed accuracy by earlier
giving the command >format long, so
V =
−0.89442719098937
0.44721359552104
0
0.44721359552104 −0.89442719098937
0
0
0
1.00000000000000

Matrix Theory
87
D =
−7.24264068533334
0
0
0
1.24264068733334
0
0
0
3.00000000000000
The normalization we see is much more accurate:
(−0.89442719098937)2 + 0.447213595521042 + 02 ≈1.00000000000000.
Some versions of MATLAB also come with a symbolic manipulation toolbox, for
example, MAPLE, in which case we can get exact eigenvalues and eigenvectors.
In Mathematica, we entered A = {{−3−5
√
2, −4
√
2, 0}, 4
√
2, −3+5
√
2, 0}, {0, 0, 3}} and then
Eigenvalues[A], and got

3(−1 −
√
2), 3, 3(−1 +
√
2)

.
We entered Eigenvectors[A] and got

−2, 1, 0

,

0, 0, 1

,
−1
2 , 1, 0

.
The Mathematica command Eigenvalues[A,k] produces the first k eigenvalues of A,
where “first” means the largest in absolute value. For example, if a 4 × 4 matrix has eigen-
values −3, 2, 2, 1 then the first two eigenvalues are −3 and 2. The Mathematica command
Eigenvectors[A, k] produces the corresponding eigenvectors.
Each of the Mathematica commands Eigensystem[A] and Eigensystem[A,k] gives
both eigenvalues and eigenvectors.
Many calculators these days will produce approximate eigenvalues and eigenvectors,
and some more expensive models give exact results, that is, do symbolic manipulation.
2.1.6 Problems
In problems 1–9, for each matrix, find all exact eigenvalues and eigenvectors.
1.
−2
7
1
4

2.
1
2
3
2

3.
−1
4
1
1

4.
−2
−5
1
0

5.
⎡
⎣
1
1
2
−1
3
2
1
1
2
⎤
⎦
[Hint: the eigenvalues are 0, 2, and 4.]

88
Advanced Engineering Mathematics
6.
⎡
⎣
1
−3
6
0
−2
0
2
0
0
⎤
⎦
[Hint: −2 is an eigenvalue]
7.
⎡
⎣
−3
0
0
4
−4
−3
−1
1
0
⎤
⎦
8.
⎡
⎣
6
1
4
−4
1
−4
−1
−1
0
⎤
⎦
9.
⎡
⎣
1
0
0
2
3
1
−1
2
5
⎤
⎦
10. Suppose a, b, c are unspecified but distinct, nonzero scalars. For
⎡
⎣
a
b
0
0
a
b
0
0
c
⎤
⎦,
a. Find the characteristic equation.
b. Find the eigenvalues and their corresponding algebraic and geometric multi-
plicities.
11. For which values of λ does Ax = λBx have a nontrivial solution x, where
A =
⎡
⎣
−1
0
5
2
1
4
3
−2
3
⎤
⎦and B =
⎡
⎣
0
0
0
0
1
0
0
0
1
⎤
⎦?
12. Suppose A =
⎡
⎣
4
−1
2
2
1
4
−1
0
5
⎤
⎦has an eigenvector
⎡
⎣
2
4
1
⎤
⎦and has characteristic
polynomial P(λ) = −λ3 + 10λ2 −33λ + 36. Use all of the aforementioned infor-
mation to find all of the eigenvalues of A. [Hint: use the definition of the word
eigenvector.]
13. Suppose A =
⎡
⎣
4
0
10
−5
−6
−5
5
0
−1
⎤
⎦has eigenvectors
⎡
⎣
2
−1
1
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦,
⎡
⎣
0
1
0
⎤
⎦. Use these
eigenvectors to find three, not necessarily distinct, eigenvalues of A. State their
geometric and algebraic multiplicities and explain how you arrived at those mul-
tiplicities. Also, can A have another eigenvalue other than the ones you found?
Why or why not?
14. Suppose
A =
⎡
⎣
−3 −5
√
2
−4
√
2
0
4
√
2
−3 + 5
√
2
0
0
0
3
⎤
⎦.
Find all exact eigenvalues and eigenvectors of A.

Matrix Theory
89
15. a. Can a vector x be an eigenvector for two unequal eigenvalues λ1 and λ2
for the same matrix A? Answer “Yes” or “No” and justify that conclusion.
How? If your answer is “Yes,” give a specific example. If your answer is “No,”
explain why.
b. Can a nonzero vector x be an eigenvector for two unequal eigenvalues λ1 and
λ2 corresponding to two different matrices A and B, respectively? Answer
“Yes” or “No” and justify that conclusion. How? If your answer is “Yes,” give
a specific example. If your answer is “No,” explain why.
16. Suppose an n × n matrix A is invertible, B = 2In −A, B is invertible, C = B−1, λ is
an eigenvalue of A, and λ ̸= 2.
a. Find an eigenvalue γ for C, in terms of λ, and explain how you found it.
b. Can γ = 1
2? Why, or why not?
17. Suppose A and B are n × n matrices and C = A+B and x is an eigenvector for both
A and B, corresponding to λ and β, respectively.
a. Use x to find an eigenvalue γ for C, in terms of λ and β.
b. If, in addition, Cx = A2x, use part (a) to explain why either λ = 1
2(1 +

1 + 4β)
or λ = 1
2(1 −

1 + 4β).
18. Suppose x is an eigenvector for both of the n × n matrices A and B, correspond-
ing to eigenvalues λ and β, respectively. Must x be an eigenvector of AB? If so,
corresponding to what eigenvalue?
19. Suppose A is square and for some vector b, Ax = b has two distinct solutions. (a)
Must λ = 0 be an eigenvalue of A? Why, or why not? (b) If, in addition, b = 0,
must Ax = 0 have infinitely many solutions? Why, or why not?
20. Suppose A is an unspecified n × n matrix whose eigenvalues are λ1, λ2, . . . , λn.
Use (2.2) and the definition of the characteristic polynomial to explain why
|A| = λ1 · λ2 · · · · · λn, that is, the determinant is the product of the eigenvalues.
21. Suppose A is n × n, has n distinct eigenvalues, and is an upper or lower triangular
matrix. You’ll explain why you can say a lot about the eigenvectors of A: if
A =
⎡
⎢⎢⎢⎢⎢⎢⎣
a11
a12
.
.
.
a1n
0
a22
.
.
.
a2n
.
.
.
.
.
.
.
.
.
.
.
.
0
0
.
.
.
ann
⎤
⎥⎥⎥⎥⎥⎥⎦
,
first find all of the eigenvalues of A. After that, use the given information that all
of the eigenvalues are distinct to imply that
A −a11I =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
a12
.
.
.
a1n
0
a22 −a11
.
.
.
a2n
.
.
.
.
.
.
.
.
.
.
.
.
0
0
.
.
.
ann −a11
⎤
⎥⎥⎥⎥⎥⎥⎦

90
Advanced Engineering Mathematics
is row equivalent to B ≜
⎡
⎢⎢⎢⎢⎢⎢⎣
0
a12
.
.
.
a1n
0
1
.
.
.
0
.
.
.
.
.
.
.
.
.
.
.
.
0
0
.
.
.
1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
Use that to explain why e(1) is an eigenvector corresponding to eigenvalue
λ1 = a11 is e(1).
Continue by working with all of the other n −1 eigenvalues.
22. Suppose A is n × n, {x(1), x(2)} is linearly independent, Ax(1) = −3x(2), and
Ax(2) = 2x(1). Without using a specific matrix A, specific value of n, or specific
vector x(1) or x(2),
a. Find two eigenvectors of A2.
b. Explain why (−6 −λ)2 is a factor of the characteristic polynomial of A2.
c. Give a specific example of 2 × 2 matrix A and vectors x(1), x(2) which satisfy
all of the given hypotheses in the beginning of the problem, and verify that
conclusions (a) and (b) hold true for your example.
23. Suppose A is an n × n invertible matrix such that λ = 1 is not an eigenvalue.
Explain why
a. (I −A−1) is invertible.
b. A−1(I −A−1)−1 = (A −I)−1.
c. (I −A−1)−1A−1 = (A −I)−1.
24. Suppose A, B, C are n × n matrices, B = CTAC, C is invertible, and λ is an
eigenvalue of A with corresponding eigenvector x.
a. Why is |B −λCTC| = 0 ?
b. Find a nontrivial solution of the system (B −λCTC)y = 0.
25. Suppose A and B are invertible n × n matrices. AB = BA, λ is an eigenvalue of A
with corresponding eigenvector x, and y = Bx.
a. Explain why y ̸= 0.
b. Why is y an eigenvector of A? [Hint: Multiply AB = BA on the right by x.]
c. If, in addition to all of the aforementioned assumptions, A’s eigenvalue λ
has geometric multiplicity equal to one, why does it follow that x is also an
eigenvector of B?
26. Find all of the eigenvalues of
⎡
⎣
−1
1
0
0
−1
1
0
0
−1
⎤
⎦
and their algebraic and geometric multiplicities.
27. Suppose A is an unspecified n × n matrix.
a. Why is it true that A is invertible if, and only if, 0 is not an eigenvalue of A?
b. Suppose that all eigenvalues of A are greater than 2. Why must I −A be
invertible?

Matrix Theory
91
c. Suppose that all eigenvalues of A are greater than 2. Why must I −A−1 be
invertible?
28. Find an example of a 2 × 2 matrix A with such that the eigenvectors of AT are not
eigenvectors of A, and vice-versa. Why must your example have A ̸= AT?
29. Suppose A =
0
1
0
0

and B =
1
−1
1
1

.
a. Find all values of λ for which (A −λB)x = 0 has a nontrivial solution.
b. For each such value of λ, find a nontrivial solution for x.
Note: (A −λB)x = 0 is called a generalized eigenvalue problem. The Mathe-
matica commands Eigenvalues[A,B], Eigen-vectors[A,B], etc. can find
generalized eigenvalues and corresponding eigenvectors.
30. (Project) Investigate whether knowing that A can be partitioned into blocks can
help determined its eigenvalues. You may start with studying the eigenvalues of
matrices of the form
⎡
⎣
D11

O
−−

−−
O

A22
⎤
⎦,
where D11 is a diagonal matrix, or an upper triangular matrix, and A22 is square.
31. (Project) The paper Bryan and Leise (2006) discusses some of the mathematics
behind Google’s Page Rank Algorithm. Read the paper and do at least 8 of the 17
exercises contained in the paper.
The paper may be available at ⟨http://148.85.1.57/∼tleise/Math22Spring2007/
GoogleMath22.pdf⟩.
By the way, the $2.5 × 1010 in the title was the approximate “market value,”
that is, the value of all of the stock shares owned, of Google when the company
went public in 2004; more recently, the market value has been between $1 × 1011
and $2 × 1011.
32. Find an example of 2 × 2 matrices A and B for which no eigenvector of B is an
eigenvector of AB.
2.2 Basis of Eigenvectors and Diagonalization
Now that we’ve seen how a little theory can help us, we can use a little more abstraction.
When learning a theory, it’s always a good idea to refer back to a concrete example, for
example, Example 2.5 in Section 2.1, to see what the definitions and results would say
about an example.
Definition 2.5
If A has an eigenvalue μj whose geometric multiplicity is mj, then we call {x
:
(A −
μjI)x = 0} the eigenspace of A corresponding to eigenvalue μj and denote it by Ej or Eλ = μj.

92
Advanced Engineering Mathematics
Example 2.11
Find the eigenspaces for the matrix A of Example 2.5 in Section 2.1.
Method:
From the results of Example 2.5 in Section 2.1,
we know that A
has distinct eigenvalues μ1 = −2, with corresponding eigenvectors x = c1
⎡
⎣
−1
2
1
0
⎤
⎦+
c2
⎡
⎣
−1
0
1
⎤
⎦, any c1, c2 with |c1| + |c2| ̸= 0, and μ2 = 7, with corresponding eigenvectors
x=c1
⎡
⎣
2
1
2
⎤
⎦, any c1 ̸= 0. The eigenspaces are Eλ = −2 = Span
⎧
⎨
⎩
⎡
⎣
−1
2
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦
⎫
⎬
⎭and Eλ = 7 =
Span
⎧
⎨
⎩
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭. ⃝
Theorem 2.6
If n × n matrix A has distinct eigenvalues μ1, μ2, . . . , μp and corresponding eigenspaces
E1, . . . , Ep, then for each j = 1, . . . , p
(a) Ej is a vector subspace of Rn, if A and μj are real.
(b) Ej is a vector subspace of Cn, if A and/or μj is not real.
(c) dim(Ej) = mj and 1 ≤mj ≤αj ≤n.
(d) Ej has basis consisting of the basic solutions, {xj,1, . . . , xj,mj}, of the homogeneous
system (A −μjI)x = 0.
Why? Theorem 1.39 in Section 1.7 explains both (a) and (b). Definition 2.4 in Section 2.1
explains most of (c). Theorem 2.3 in Section 2.1 explains the 1 ≤mj ≤αj ≤n part of (c).
Corollary 1.2 in Section 1.7 explains (d). 2
So, adopting the notation found in Theorem 2.6(d), the eigenspaces E1, . . . , Ep have basis
vectors
x1,1, . . . , x1,m1; x2,1, . . . , x2,m2; . . . ; xp,1, . . . , xp,mp.
(2.4)
Theorem 2.7
Suppose n × n matrix A has distinct eigenvalues μ1, μ2, . . . , μp whose geometric multiplic-
ities are m1, m2, . . . , mp:
(a) If m1 + m2 + · · · + mp = n, then {x1,1, . . . , x1,m1, x2,1, . . . , x2,m2, . . . , xp,1, . . . , xp,mp}
is a basis for Rn when A and all its eigenvalues are real (respectively, Cn, if A
and/or at least one of its eigenvalues is not real);

Matrix Theory
93
(b) Any set of vectors chosen from {x1,1, . . . , x1,m1, x2,1, . . . , x2,m2, . . . , xp,1, . . . , xp,mp} is
linearly independent.
(c) If p = n, that is, A has n distinct eigenvalues, rewrite the corresponding eigenvec-
tors as x(1), x(2), . . . , x(n). Then {x(1), x(2), . . . , x(n)} is a basis for Rn(or Cn).
Why? We will explain only why (b) is true: by Theorem 1.43 in Section 1.7, it will
suffice to explain why {x(1), x(2), . . . , x(n)} is linearly independent. First, Ax(j) = λjx(j) implies
(A −λI)x(j) = (λj −λ)x(j). Now, suppose
0 = c1x(1) + c2x(2) + · · · + cnx(n).
If we multiply on the left by the matrix (A −λ2I)(A −λ3I) · · · (A −λnI), we have
0 = c1(A −λ2I)(A −λ3I) · · · (A −λnI)x(1) + c2(A −λ2I)(A −λ3I) · · · (A −λnI)x(2) + · · · +
+ cn(A −λ2I)(A −λ3I) · · · (A −λnI)x(n)
= c1(λ1 −λ2)(λ1 −λ3) · · · (λ1 −λn)x(1) + c2(λ2 −λ2)(λ2 −λ3) · · · (λ2 −λn)x(2) + · · · +
+ cn(λn −λ2)(λn −λ3) · · · (λn −λn)x(n)
= c1(λ1 −λ2)(λ1 −λ3) · · · (λ1 −λn)x(1) + 0 + · · · + 0.
Because the eigenvalues were assumed to be distinct, and x(1) ̸= 0, it follows that c1 = 0.
In a similar way (see Problem 2.2.3.24), we can conclude c2 = · · · = cn = 0. By the way, we
may call what we did an “annihilator method,” and we will use something like this when
solving nonhomogeneous differential equations in Section 4.1. 2
Example 2.12
For A =
−1
2
−2
−1

, find a basis for R2, or C2 consisting of eigenvectors of A.
Method:
0 = | A −λI2 | =

−1 −λ
2
−2
−1 −λ
 = (−1 −λ)2 + 4, so the eigenvalues λ
satisfy (−1 −λ)2 = −4, hence (−1 −λ) = ±i2, where i ≜
√
−1. The eigenvalues are
λ = −1 ± i2. We may call λ1 = −1 + i2 and λ2 = −1 −i2, although we could just as well
reverse the roles as long as we are consistent.
Because the 2 × 2 matrix A has two distinct eigenvalues, Theorem 2.7(b) guarantees
that C2 has a basis consisting of eigenvectors of A: {x(1), x(2)}. Let’s find them explicitly,
using row reduction on

A −λI | 0

:

A −(−1 + i2)I2 | 0

=
−i2
2
| 0
−2
−i2
| 0

∼
R1 ↔R2
−1
2 R1 →R1
i2R1 + R2 →R2

1⃝
i
| 0
0
0
| 0

.
The eigenvectors corresponding to eigenvalue −1 + i2 are x = c1
−i
1

, for any c1 ̸= 0.

94
Advanced Engineering Mathematics
Using Theorem 2.4 in Section 2.1 about the “complex conjugate pair” nature of
eigenvalues and eigenvectors, the eigenvectors corresponding to eigenvalue −1 −i2 are
x = c1
i
1

, for any c1 ̸= 0.
Because we have two distinct eigenvalues, Theorem 2.7(b) implies that
−i
1

,
 i
1

is a basis, consisting of eigenvectors of A, for C2. ⃝
Example 2.13
For A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦, that is, the matrix of Examples 2.3, 2.5, and 2.7 in Section 2.1,
find a basis for R3, or C3, consisting of eigenvectors of A.
Method: We can use Theorem 2.7(a), because in Example 2.5 in Section 2.1, we found
two basic solutions,
⎡
⎣
−1
2
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦, corresponding to eigenvalue μ1 = −2, and one basic
solution,
⎡
⎣
2
1
2
⎤
⎦, corresponding to eigenvalue μ2 = 7; in effect, these conclusions were
summarized in Example 2.11. It follows that
⎧
⎨
⎩
⎡
⎣
−1
2
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦,
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭
is a basis for R3 consisting of eigenvectors of A. ⃝
Here are further useful results.
Theorem 2.8
Suppose A has distinct eigenvalues μ1, μ2, . . . , μp
and corresponding eigenspaces
E1, E2, . . . , Ep:
(a) If B = A + γ I for some nonzero scalar γ , then B has distinct eigenvalues
μ1 + γ , μ2 + γ , . . . , μp + γ
and corresponding eigenspaces E1, E2, . . . , Ep, that is, the same eigenspaces as A.

Matrix Theory
95
(b) If B = γ A for some nonzero scalar γ , then B has distinct eigenvalues γ μ1, γ μ2, . . . ,
γ μp and corresponding eigenspaces E1, E2, . . . , Ep, that is, the same eigenspaces
as A.
(c) If B = AT, then B has distinct eigenvalues μ1, μ2, . . . , μp, that is, the same
eigenvalues as A, but not necessarily the same eigenspaces as A.
(d) If B = A−1 exists, then B has distinct eigenvalues μ1−1, μ2−1, . . . , μp−1, that is, the
reciprocals of the eigenvalues of A, and the same eigenspaces as A.
(e) If B = f(A) for some polynomial function, then B has eigenvalues
f(μ1), f(μ2), . . . , f(μp),
which are not necessarily distinct, and the same eigenspaces as A.
Parts (a) and (b) are special cases of part (e) but with more informative conclusions than
part (e) because the distinctiveness of the eigenvalues is maintained. As for part (e), the
reason why we must allow for the eigenvalues losing distinctiveness can be seen if 2 × 2
matrix A has eigenvalues −1 and 1 and f(A) = A2: By Theorem 2.8(e), A2 has eigenvalues
(−1)2 and 1, which are not distinct. In Problem 2.2.3.12, you will be asked to come up with
a specific example of such an A. 2
2.2.1 Diagonalizing a Matrix
Given a matrix A, we will see that it is sometimes useful to express it in terms of a diagonal
matrix: A = PDP−1, for some diagonal matrix D and invertible matrix P. But
A = PDP−1 ⇐⇒AP = (PDP−1)P = (PD)(P−1P) = (PD)(I) = PD
⇐⇒P−1(AP) = P−1(PD) = (P−1P)D = (I)D = D.
So, A = PDP−1 is logically equivalent to both AP = PD and to D = P−1AP.
Definition 2.6
Matrix A is diagonalizable if there is an invertible matrix P with P−1AP being a diagonal
matrix. In this case, we say that P diagonalizes A.
Theorem 2.9
A is diagonalizable if, and only if, AP = PD for some diagonal matrix D and invertible
matrix P.
Theorem 2.10
If Rn (respectively, Cn) has a basis {p(1), p(2), . . . , p(n)} consisting of eigenvectors of
A, then

96
Advanced Engineering Mathematics
(a) A is diagonalizable.
(b) the n × n matrix P =

p(1)
p(2)
. . .
p(n) 
diagonalizes A.
(c) P−1AP = D = diag(λ1, λ2, . . . , λn), where the λj’s are the n eigenvalues of A, possi-
bly including repetitions, corresponding to eigenvectors p(1), p(2), . . . , p(n).
Why? By Theorem 1.9 in Section 1.2,
AP = A
 
p(1)
p(2)
. . .
p(n)!
=
 
Ap(1)
Ap(2)
. . .
Ap(n)!
=
 
λ1p(1)
λ2p(2)
. . .
λnp(n)!
.
=
 
p(1)
p(2)
. . .
p(n) !
diag(λ1, λ2, . . . , λn) = PD.
by Theorem 1.10 in Section 1.2. 2
Example 2.14
Find an exact matrix that diagonalizes A =
−1
2
1
1

.
Method: First, let’s find the eigenvalues of A and then a basis for R2 or C2 that consists
of eigenvectors of A: 0 = | A−λI2 | =

−1 −λ
2
1
1 −λ
 =λ2 −3, so the eigenvalues are
λ = ±
√
3. We have

A −
√
3I2 | 0

=
−1 −
√
3
2
| 0
1
1 −
√
3
| 0

∼
R1 ↔R2
(1 +
√
3)R1 + R2 →R2

1⃝
1 −
√
3
| 0
0
0
| 0

.
Corresponding to eigenvalue λ1 =
√
3, we have eigenvectors x = c1

−1 +
√
3
1

, where
c1 ̸= 0. Similarly,

A −(−
√
3)I2 | 0

=
−1 +
√
3
2
| 0
1
1 +
√
3
| 0

∼

1⃝
1 +
√
3
| 0
0
0
| 0

.
Corresponding to eigenvalue λ2 = −
√
3, we have eigenvectors x = c1

−1 −
√
3
1

, where
c1 ̸= 0.
So, we can take p(1) =

−1 +
√
3
1

and p(2) =

−1 −
√
3
1

as the columns of the
diagonalizing matrix P, that is,
P =
 
p(1)
p(2)!
=

−1 +
√
3
−1 −
√
3
1
1

.
We only have one more thing to check: because |P| = (−1+
√
3)(1)−(−1−
√
3)(1) = 2
√
3 ̸=
0, P is invertible. According to Theorem 2.10(c), this matrix P should diagonalize A. ⃝

Matrix Theory
97
It doesn’t hurt to check that this works out correctly, that is, that P−1AP = D, the diagonal
matrix whose diagonal entries are the eigenvalues of A. First, we need the inverse of the
2 × 2 matrix P:
P−1 = 1
|P|
 1
1 +
√
3
−1
−1 +
√
3

=
1
2
√
3
 1
1 +
√
3
−1
−1 +
√
3

.
So,
P−1AP = P−1
−1
2
1
1
 
−1 +
√
3
−1 −
√
3
1
1

=
1
2
√
3
 1
1 +
√
3
−1
−1 +
√
3
 3 −
√
3
3 +
√
3
√
3
−
√
3

=
1
2
√
3
6
0
0
−6

=
√
3
0
0
−
√
3

= D =
λ1
0
0
λ2

,
the diagonal matrix predicted by Theorem 2.10(c). ⃝
Example 2.15
Find an exact matrix that diagonalizes A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦, and find the corresponding
diagonal matrix D.
Method: By Theorem 2.10(b), if we have a basis for R3 consisting of eigenvectors of A,
then we can use those eigenvectors as the columns of a matrix, P, that diagonalizes A.
But, we found such a basis in Example 2.13, so
P =
⎡
⎣
−1
2
−1
2
1
0
1
0
1
2
⎤
⎦,
diagonalizes A. As to the matrix D, we can either note that the three columns of P cor-
respond to eigenvalues −2, −2, 7 of A, hence according to Theorem 2.10(c), we should
have
D =
⎡
⎣
−2
0
0
0
−2
0
0
0
7
⎤
⎦,
or we could calculate that
P−1AP = 1
9
⎡
⎣
−2
8
−2
−4
−2
5
2
1
2
⎤
⎦
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦
⎡
⎣
−1
2
−1
2
1
0
1
0
1
2
⎤
⎦
= 1
9
⎡
⎣
−2
8
−2
−4
−2
5
2
1
2
⎤
⎦
⎡
⎣
1
2
14
−2
0
7
0
−2
14
⎤
⎦=
⎡
⎣
−2
0
0
0
−2
0
0
0
7
⎤
⎦= D. ⃝
In this example, it made sense to check that P−1AP = D even though the theorem
predicted what D should be.

98
Advanced Engineering Mathematics
Definition 2.7
Two matrices A and B are similar if there is an invertible matrix P with B = P−1AP.
So, a matrix is diagonalizable if it is similar to a diagonal matrix. Notice that the word
“similar” has nothing to do with the idea of “row equivalence.” We mention this because
some books write A ∼B to mean “A is similar to B,” while we have reserved the symbol
∼to mean “is row equivalent to.”
In the definition of similar matrices, either, both, or neither A and B may be diagonal
matrices.
Theorem 2.11
If A and B are similar, then B’s eigenvalues and their algebraic and geometric multiplicities
exactly equal those of A. Further, if B = P−1AP, then y is an eigenvector of B if, and only if,
Py is an eigenvector of A.
2.2.2 Deﬁcient Eigenvalues
Does every matrix have a set of eigenvectors that is a basis for Rn or Cn? Unfortunately, no.
Example 2.16
A =
 29
18
−50
−31

has only one linearly independent eigenvector.
Why? First, we find the eigenvalues: 0 = | A −λI2 | =

29 −λ
18
−50
−31 −λ
 = λ2 +
2λ + 1 = (λ + 1)2, so the only eigenvalue is λ = −1. So, to find all the corresponding
eigenvectors, we use

A −(−1)I2 | 0

=
 30
18
| 0
−50
−30
| 0

∼

1⃝
3
5
| 0
0
0
| 0

,
so the eigenvectors are x = c1

−3
5
1

, c1 ̸= 0. So, the dimension of the only eigenspace is
one, and A does not have a set of eigenvectors that is a basis for R2. ⃝
Definition 2.8
If an eigenvalue has its geometric multiplicity strictly less than its algebraic multiplicity,
we call that eigenvalue deficient.
So far, our only example of a deficient eigenvalue is λ = −1 in Example 2.16.

Matrix Theory
99
2.2.3 Problems
In problems 1–7, for each matrix, find an exact matrix that diagonalizes the given matrix,
if possible.
1.
5
−1
3
1

2.
 5
−2
−2
2

3.
 2
0
−1
−1

4.

−3
√
3
−
√
3
1

5.

−2
√
2
−
√
2
2

6.
⎡
⎣
−3
−1
2
0
−2
0
−1
−1
0
⎤
⎦
7.
⎡
⎣
6
7
7
−7
−8
−7
7
7
6
⎤
⎦
In problems 8–10, a matrix A and information about one or more of its eigenvalues is
given. Find an exact matrix, P, that diagonalizes A. Also, find P−1, for example by using a
calculator, and explicitly calculate D = P−1AP to check your work.
8. A =
⎡
⎣
1
5
−10
5
1
−10
0
0
−4
⎤
⎦, λ = 6
9. A =
⎡
⎣
3
0
−12
4
−1
−12
0
0
−1
⎤
⎦, λ = −1, 3
10. A =
⎡
⎣
−3
2
2
0
−1
0
−1
1
0
⎤
⎦, λ = −1
11. (a) Write down a 3 × 3 matrix A that satisfies all of the following properties: (i)
A is in RREF, (ii) A has exactly two pivot positions, and (iii) at least one entry
of A is 2 and at least one entry of A is −1.
(b) For the matrix A you wrote down in part (a), find a basis for R3 consisting of
eigenvectors of A.
12. Find an example of a 2 × 2 matrix A that has eigenvalues −1 and 1 and define
f(A) = A2. For your matrix A, explain why A2 has eigenvalues (−1)2 and 1, which
are not distinct. This illustrates Theorem 2.8.
13. Assume A is an n × n matrix, n ≥2, and x is an eigenvector of A. For each of (a)
through (g), decide whether it must be true, must be false, or may be true and
may be false.

100
Advanced Engineering Mathematics
a. x is in W ≜{x : (A −λIn)x = 0} for some scalar λ.
b. x is an eigenvector of AT.
c. x = 0
d. x is a factor of the characteristic polynomial.
e. x is an eigenvector of A2.
f. {x} is a basis of an eigenspace of A.
g. x is an eigenvector of B, if B is similar to A.
14. For the matrix A of Example 2.16, explain why there is no invertible matrix P that
diagonalizes A. [Hint: Consider the equation AP = PD; additionally, use Theo-
rems 2.5 in Section 2.1 and 2.11 to decide in advance what the diagonal entries of
D must be.]
15. Suppose A is a 3 × 3 matrix for which
[ A + 2I | 0 ] =
⎡
⎣
−2
−2
2
| 0
1
1
−1
| 0
0
0
0
| 0
⎤
⎦and [ A + 3I | 0] =
⎡
⎣
−1
−2
2
| 0
1
2
−1
| 0
0
0
1
| 0
⎤
⎦.
Find a set of three eigenvectors of A that is a basis for R3.
16. (a) Explain why matrices that are similar have equal determinant.
(b) Is the “converse” true? That is, if |A| = |B|, does it follow that A and B are
similar? If so, why? If not, give a specific counter-example.
17. Suppose A is an unspecified 3 × 3 matrix for which rank(A−2I3) = 2 and rank(A−
3I3) = 1. Find |A| either by using the result of Problem 2.1.6.20 or by using the
result of Problem 2.2.3.16.
18. Suppose A + I is row equivalent to
⎡
⎢⎢⎣
1
−2
3
5
0
0
1
4
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦.
Find a basis for the eigenspace Eμ = −1.
19. Suppose A is an n × n real matrix, all of whose eigenvalues are positive real num-
bers. Suppose A has a set of eigenvectors {p(1), p(2), . . . , p(n)} that is a basis for
Rn. Define, as usual, P =

p(1) p(2) . . . p(n)
, so P−1 A P = D ≜diag(λ1, . . . , λn).
Define S ≜P diag(√λ1, . . . , √λn) P−1. Explain why S2 = A, hence we may refer to
S as the “square root of the matrix” A.
20. If B = P−1AP, does it follow that A = PBP−1? If so, why? If not, give a specific
counterexample using 2 × 2 matrices.
21. (Designing a desired matrix) Find a 2 × 2 matrix A that has two distinct
eigenvalues and has eigenvectors
−5
1

,
4
1

.

Matrix Theory
101
22. For the matrix
A =
⎡
⎣
2
0
1
0
−5
0
3
0
2
⎤
⎦,
(a) Find all of the exact eigenvalues and eigenvectors and (b) explain why A has
a set of eigenvectors that is a basis for R3.
23. Suppose A is an unspecified 3 × 3 matrix that has eigenvalues 2, −2,
√
3 and
corresponding eigenvectors x1, x2, x3.
a. Find the characteristic polynomial of A.
b. Find a set of three linearly independent eigenvectors of A2.
c. Find the characteristic polynomial of A2.
24. In explaining why Theorem 2.7(c) was true, we used an “annihilator” to explain
why c1 must equal zero. What annihilator would you use to explain why c2
must equal zero? What annihilator would you use to explain why cn must
equal zero?
25. In explaining why Theorem 2.7(c) was true, we used the “annihilator” (A −
λ2I)(A −λ3I) · · · (A −λnI) to explain why c1 must equal zero. In an effort to
explain why Theorem 2.7(a) is true, with a similar annihilator using μ2, . . . , μp,
explain why
0 = c1,1x1,1 + · · · + c1,m1x1,m1 + c2,1x2,1 + · · · +
+ c2,m2x2,m2 + · · · + cp,1xp,1 + · · · + cp,mpxp,mp
would imply that c1,1 = · · · = c1,m1 = 0. Here, we’re using the notation of Theorem
2.7 that {x1,1, . . . , x1,m1} is the set of basic solutions that span the eigenspace E1.
26. Can R3 have a basis of eigenvectors all of which have 0 in their first components?
27. For A =
⎡
⎣
1
−1
−1
−1
1
−1
0
1
8
⎤
⎦, find all of the eigenvalues and use that information to
explain why A has a set of three linearly independent eigenvectors.
2.3 Inner Product and Orthogonal Sets of Vectors
We’re familiar with the dot product in R3 given by
⎡
⎣
x1
x2
x3
⎤
⎦•
⎡
⎣
y1
y2
y3
⎤
⎦≜x1y1 + x2y2 + x3y3.
For example, in physics, Work = Force•displacement, assuming Force is constant. The dot
product generalizes to Rn.

102
Advanced Engineering Mathematics
Definition 2.9
If x, y are in Rn, then
⟨x, y⟩≜x1y1 + x2y2 + · · · + xnyn,
(2.5)
which we call the inner product of x and y.
So, in R3, ⟨x, y⟩= x • y.
Theorem 2.12
(Properties of inner products) For all x, y, x1, x2 in Rn and α in R,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
(a) ⟨x, y⟩= ⟨y, x⟩
(b) ⟨αx, y⟩= α⟨x, y⟩
(c) ⟨x1 + x2, y⟩= ⟨x1, y⟩+ ⟨x2, y⟩
(d) ⟨x, x⟩≥0, with equality only if x = 0
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(2.6)
By the way, from properties (2.6)(a) and (b) it follows that
⟨x, αy⟩= α⟨x, y⟩.
Sometimes we will write
⟨x, y⟩= xTy,
(2.7)
that is, that the real number ⟨x, y⟩equals the real, 1 × 1 matrix xTy, in an abuse of notation.
Definition 2.10
If x, y are in Rn
|| x || ≜
#
x2
1 + x2
2 + . . . + x2n,
(2.8)
which we call the norm or magnitude or length of x.

Matrix Theory
103
The “direction vector” from the point (0, 0, . . . , 0), to the point (x1, x2, . . . , xn) is given by
x ≜−→
OP ≜
⎡
⎢⎢⎢⎣
x1
x2
...
xn
⎤
⎥⎥⎥⎦,
so || x || measures the distance from the origin, (0, 0, . . . , 0), to the point (x1, x2, . . . , xn).
Theorem 2.13
(Properties of norms) For all x, y in Rn and α in R,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(a) || x ||2 = ⟨x, x⟩
(b) ||x + y||2 = || x ||2 + || y ||2 + 2⟨x, y⟩
(c) || x || ≥0, with equality only if x = 0
(d) ||αx|| = |α| || x ||
(e) |⟨x, y⟩| ≤|| x || || y ||
(f) ||x + y|| ≤|| x || + || y ||
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(2.9)
Why? (a) and (c) are easily explained. As for (b), from (a) and then properties of ⟨, ⟩
we have
||x + y||2 = ⟨x + y, x + y⟩= ⟨x, x + y⟩+ ⟨y, x + y⟩= ⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩,
from which (2.9)(b) follows.
As for (d), you will explain this in Problem 2.3.4.14, using the fact that for real numbers
α, we have
√
α2 = |α|, for example,

(−5)2 = 5 = |(−5)|.
Part (e) is called the Cauchy–Schwarz inequality. We will explain this result in the
appendix at the end of this section.
Part (f) is known as the triangle inequality and is illustrated in Figure 2.3: In the plane
spanned by the vectors x and y, here visualized as the plane R2, we know that ||x + y||,
the distance from O to P, is the length of the shortest curve from O to P, hence is less than
or equal to || x || + || y ||, the sum of the distance from O to Q and the distance from Q to P.
To summarize, the triangle OQP illustrates that
||x + y|| ≤|| x || + || y ||.

104
Advanced Engineering Mathematics
Q
||x+ y||
||y||
P
O
||x||
FIGURE 2.3
Triangle inequality.
x
y
θ
O
FIGURE 2.4
Angle between vectors.
To explain why (f) follows from (e), note that
||x + y||2 = || x ||2 + || y ||2 + 2⟨x, y⟩≤|| x ||2 + || y ||2 + 2|⟨x, y⟩|
≤|| x ||2 + || y ||2 + 2|| x || || y || =
$
|| x || + || y ||
%2 ,
by the Cauchy–Schwarz inequality. It follows that ||x + y|| ≤|| x || + || y ||. 2
Definition 2.11
The angle θ, with 0 ≤θ ≤π, between two nonzero vectors x, y is defined implicitly by
cos θ ≜
⟨x, y⟩
|| x || || y ||.
(2.10)
and the requirement that 0 ≤θ ≤π. This is illustrated in Figure 2.4.
Note that the Cauchy inequality guarantees that

⟨x, y⟩
|| x || || y ||
 ≤1, that is, that | cos θ | ≤1.
We can rewrite (2.10) as
⟨x, y⟩= || x || || y || cos θ.
(2.11)
Some people prefer to define ⟨x, y⟩via (2.11) and then explain why ⟨x, y⟩satisfies the
equality in Definition 2.9 as a result rather than as a definition.

Matrix Theory
105
Theorem 2.14
Equality in the Cauchy–Schwarz inequality, that is, |⟨x, y⟩| = || x || || y ||, holds if and only
if the set of vectors {x, y} is linearly dependent.
This result makes sense because when x and y are nonzero, cos θ = ± 1 is equivalent
to θ = 0 or θ = π, that is, equivalent to the vectors x, y being parallel. Note also that if x = 0,
then {0, y} is linearly dependent.
By the way, given two nonzero vectors x, y, the quantity
r ≜
⟨x, y⟩
||x|| ||y||
is called the correlation coefficient in statistics. When r = ± 1, the nonzero vectors x, y are
linearly dependent, hence there is a scalar k with y = kx. If we think of the components of
the vectors x, y as pairing off in data points (x1, y1), (x2, y2), . . . , (xn, yn) in the plane, then
r = ±1 implies that those data points lie on a common line. We’ll see more about fitting
a line to data points in Section 2.5.
Definition 2.12
x, y are orthogonal if ⟨x, y⟩= 0.
The notation “x ⊥y” means that “x, y are orthogonal.”
Theorem 2.15
(Pythagoras) ||x + y||2 = || x ||2 + || y ||2 holds if, and only if, x ⊥y.
2.3.1 Orthogonal Set of Vectors
Definition 2.13
(a) {x, y} is an orthogonal set of vectors if x ⊥y.
(b) {x1, . . . , xn} is an orthogonal set of vectors if xi ⊥xj for all i ̸= j.
Example 2.17
Here are three examples of orthogonal sets of vectors:
(a)
⎧
⎨
⎩
⎡
⎣
1
0
−1
⎤
⎦,
⎡
⎣
1
−4
1
⎤
⎦,
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭.

106
Advanced Engineering Mathematics
(b)
⎧
⎨
⎩
1
√
2
⎡
⎣
1
0
−1
⎤
⎦,
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦, 1
3
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭.
(c)
⎧
⎨
⎩
⎡
⎣
1
0
−1
⎤
⎦,
⎡
⎣
0
0
0
⎤
⎦,
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭.
Example 2.17(b) is very similar to Example 2.17(a). Each of the vectors in Example 2.17(b)
has length one, that is, is a unit vector and is a normalized version of a vector in Example
2.17(a). Given a nonzero vector x, we can “divide” it by its length to get a unit vector, &x,
that points in the same direction:
&x ≜
1
|| x ||x.
(2.12)
We use a conventional notation that a hat, &, over a vector signifies that it is a unit vector.
Definition 2.14
S is an orthonormal set of vectors if S is an orthogonal set of vectors and each vector in S
is a unit vector. We may abbreviate “orthonormal set of vectors” by writing “o.n. set.”
So, Example 2.17(b) gives an o.n. set of three vectors.
Based on Example 2.17(a) and (b), we note that an orthogonal set of nonzero vectors will
give us an o.n. set of vectors after normalizing each of the vectors. If 0 is in an orthogonal
set of vectors, then we can’t normalize to get an o.n. set of vectors.
A common notation is the Kronecker delta defined by
δij ≜
1,
if i = j
0,
if i ̸= j

.
Using this symbol, we note that {x1, . . . , xn} is an o.n. set if ⟨xi, xj⟩= δij for all i, j. We also
note that the n × n identity matrix
In =

δij

1 ≤i ≤n
1 ≤j ≤n
.
2.3.2 The Gram–Schmidt Process
This is a very useful technique both for producing o.n. sets and, as we will see in
Section 2.7, factoring matrices.
Theorem 2.16
(The Gram–Schmidt process) From a linearly independent set of vectors {a1, . . . , an} in Rm
we can construct vectors q1, . . . , qn in Rm with the properties that

Matrix Theory
107
(a) {q1, . . . , qn} is an o.n. set.
(b) Span{q1, . . . , qn} = Span{a1, . . . , an}.
(c) For each i ≤n, qi is a linear combination of a1, . . . , ai.
Before explaining the process in general, it’s useful to do an example.
Example 2.18
Demonstrate the Gram–Schmidt process for the set of vectors
⎧
⎨
⎩
⎡
⎣
1
0
−1
⎤
⎦,
⎡
⎣
1
−2
0
⎤
⎦,
⎡
⎣
0
1
5
⎤
⎦
⎫
⎬
⎭.
(2.13)
Method: Denote the vectors listed in (2.13) as a1, a2, a3. To start a “recursive” process, let
(1a)
v1 ≜a1,
r11 ≜||v1|| =
√
2,
and
(1b)
q1 = r−1
11 v1 =
1
√
2
⎡
⎣
1
0
−1
⎤
⎦.
Next, let
(2a)
v2 ≜a2 −(a2 • q1)q1 =
⎡
⎣
1
−2
0
⎤
⎦−
⎛
⎝
⎡
⎣
1
−2
0
⎤
⎦• 1
√
2
⎡
⎣
1
0
−1
⎤
⎦
⎞
⎠1
√
2
⎡
⎣
1
0
−1
⎤
⎦
=
⎡
⎣
1
−2
0
⎤
⎦−
	 1
√
2

 1
√
2
⎡
⎣
1
0
−1
⎤
⎦=
⎡
⎣
0.5
−2
0.5
⎤
⎦, r22 ≜||v2|| =
3
√
2
,
and
(2b)
q2 = r−1
22 v2 =
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦.
Finally, let
(3a)
v3 ≜a3 −(a3 • q1)q1 −(a3 • q2)q2
=
⎡
⎣
0
1
5
⎤
⎦−
⎛
⎝
⎡
⎣
0
1
5
⎤
⎦• 1
√
2
⎡
⎣
1
0
−1
⎤
⎦
⎞
⎠1
√
2
⎡
⎣
1
0
−1
⎤
⎦−
⎛
⎝
⎡
⎣
0
1
5
⎤
⎦•
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦
⎞
⎠
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦
=
⎡
⎣
0
1
5
⎤
⎦−
	 −5
√
2

 1
√
2
⎡
⎣
1
0
−1
⎤
⎦−
	
1
3
√
2

1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦= · · · = 11
9
⎡
⎣
2
1
2
⎤
⎦,
r33 ≜||v3|| = 11
3 ,
and
(3b)
q3 = r−1
33 v3 = 1
3
⎡
⎣
2
1
2
⎤
⎦.
It is clear by construction that the conclusion in Theorem 2.16 (b) is true and that the
qi’s are unit vectors; to see that they are orthogonal, we have, for example, that
⟨q2, q1⟩= r−1
22 ⟨a2 −(a2 • q1)q1, q1⟩= r−1
22 (⟨a2, q1⟩−(a2 • q1)⟨q1, q1⟩)
= r−1
22 (⟨a2, q1⟩−⟨a2, q1⟩· 1) = 0.

108
Advanced Engineering Mathematics
To summarize the conclusions, the o.n. set
{q1, q2, q3} =
⎧
⎨
⎩
1
√
2
⎡
⎣
1
0
−1
⎤
⎦,
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦, 1
3
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭
satisfies the conclusions of Theorem 2.16 for the given set of three vectors {a1, a2, a3}. ⃝
As an aside, the order in which we list the given vectors may affect what vectors are
produced by the Gram–Schmidt process. Also, in Section 2.7, we will see why we denote
the “r’s” as rii instead of more simply ri.
The general description of the Gram–Schmidt process is an algorithm, that is, a compu-
tational procedure: given a linearly independent set of vectors {a1, . . . , an}, the initial step
is to let
v1 ≜a1, r11 ≜||v1||, and q1 = r−1
11 v1.
Here’s the “inductive step”: Having already constructed an o.n. set {q1, . . . , qj−1} that
has the property that
for each i ≤j −1, qi is a linear combination of a1, . . . , ai
and the property that
Span{q1, . . . , qj−1} = Span{a1, . . . , aj−1},
we let
vj ≜aj −(aj • q1)q1 −· · · −(aj • qj−1)qj−1,
rjj ≜||vj||,
and
qj ≜r−1
jj vj.
By construction, qj is a unit vector. To see that qj is orthogonal to q1, . . . , qj−1, we note that
for i ≤j −1,
⟨qj, qi⟩= r−1
jj
+
aj −(aj • q1)q1 −· · · −(aj • qj−1)qj−1, qi
,
= r−1
jj
$
⟨aj, qi⟩−⟨aj, q1⟩⟨q1, qi⟩−· · · −⟨aj, qj−1⟩⟨qj−1, qi⟩
%
.
By orthonormality, all of the subtracted terms are zero except for the ith term, which is
−⟨aj, qi⟩⟨qi, qi⟩= −⟨aj, qi⟩· 1.
So,
⟨qj, qi⟩= r−1
jj (⟨aj, qi⟩−⟨aj, qi⟩· 1) = r−1
jj
· 0 = 0.
Because {a1, . . . , aj} is a linearly independent set of vectors, we can’t write aj as a linear
combination of a1, . . . , aj−1. After a little further reasoning, it follows that vj ̸= 0.
As for conclusion (c) of Theorem 2.16, q1 is a linear combination of a1, and q2 is a linear
combination of a1, a2. Continuing in this way, qj is a linear combination of a1, . . . , aj−1.

Matrix Theory
109
It follows that vj, and hence qj, is a linear combination of aj and a1, . . . , aj−1, hence is a
linear combination of a1, . . . , aj. This explains (c) and also explains why
Span{q1, . . . , qn} ⊆Span{a1, . . . , an},
that is, every vector in Span{q1, . . . , qn} must also be in Span{a1, . . . , an}.
As for conclusion (b) of Theorem 2.16, a1 is a linear combination of q1 and a2 = v2 + (a2 •
q1)q1 = r22q2 + (a2 • q1)q1 is a linear combination of q1, q2. Continuing in this way, the
inductive step explains why
aj = rjjqj + (aj • q1)q1 + · · · + (aj • qj−1)qj−1
(2.14)
so aj is a linear combination of q1, . . . , qj−1, qj. It follows that
Span{a1, . . . , an} ⊆Span{q1, . . . , qn}.
This, along with the other subset relationship at the end of the previous paragraph,
explains why Span{a1, . . . , an} = Span{q1, . . . , qn}. 2
We will use (2.14) again when we study the “QR factorization” in Section 2.7.
2.3.3 Orthogonal Projections
In physics, work is defined as −−→
Force • −−−−−−−−→
displacement, assuming −−→
Force is constant. In effect, we
are interested in the “component” of force in the direction of the displacement vector. This
concept is formalized in the concept of “orthogonal projection.”
Definition 2.15
An orthogonal projection on Rn is a real, n × n, matrix P that satisfies both P2 = P and
PT = P.
Theorem 2.17
If P is an orthogonal projection on Rn then (a) for every x, y in Rn
Px ⊥(I −P)y,
(2.15)
and (b) every x in Rn can be decomposed into a sum of orthogonal vectors:
x = Ix = (P + I −P)x = Px + (I −P)x.
Why? Concerning (a), for any x in Rn,
⟨Px, (I −P)y⟩= (Px)T(I −P)y = xTPT(I −P)y = xTP(I −P)y = xT(P −P2)y
= xTOy = 0. 2

110
Advanced Engineering Mathematics
x
(I– P)x
x
q
(a)
Px
(b)
FIGURE 2.5
Projection onto a line: (a) x is to be projected onto q, (b) Decompose x = Px + (I −P)x.
Theorem 2.17(b) is illustrated in Figure 2.5b.
Lemma 2.1
If q is a unit vector in Rn then
P ≜qqT
is a projection.
You will explain why P ≜qqT satisfies the two requirements of an orthogonal projection
in Problem 2.3.4.17.
Theorem 2.18
If q is a unit vector and P = qqT, then for every x in Rn
Px = ⟨x, q⟩q.
(2.16)
Why?
Px = (qqT)x = q(qTx) = (qTx)q = ⟨q, x⟩q = ⟨x, q⟩q.2
In the latter calculation, we again used the “abuse of notation” of considering (qTx) to be
both a 1 × 1 matrix and a scalar.
Geometrically, P = qqT is a projection onto the vector subspace spanned by the (unit)
vector q, that is, a line through the origin, as illustrated in Figure 2.5a.
Theorem 2.19
Suppose {q1, . . . , qn} is an o.n. set in Rm and we define
Pn = q1qT
1 + · · · + qnqT
n.

Matrix Theory
111
x
Px
FIGURE 2.6
Projection onto a plane.
Then, (a) Pn is an orthogonal projection and (b) Pnqi = qi for every i ≤n.
Geometrically, Pn is a projection onto the vector subspace Vn = Span{q1, . . . , qn}. For the
case n = 2, V2 is a plane, and the projection P2 is illustrated in Figure 2.6.
Using the concept of orthogonal projection and the notation for Pn we can restate the
inductive step in the Gram–Schmidt process as
vj ≜(I −Pj−1)aj,
rjj = ||vj||,
and
qj ≜r−1
jj vj.
In this clean notation, it’s even easier to see that qj is orthogonal to each of q1, . . . , qj−1,
using Theorem 2.19(b).
Appendix
To explain Theorem 2.13(e), that is, (2.9)(e), the “Cauchy–Schwarz” inequality, let’s
consider two cases: if y = 0, then (2.9)(e) would follow from
|⟨x, 0⟩| = | 0 | = 0 = || x || · 0 = || x || ||0||.
On the other hand, suppose y ̸= 0. Our subsequent work will essentially be a calculation.
Let α be a scalar; later we will choose a useful value for it. We calculate
0 ≤||x −αy||2 = || x ||2 + ||αy||2 −2⟨x, αy⟩= || x ||2 + |α|2|| y ||2 −2α⟨x, y⟩.
If we think of the RHS as a polynomial in α, we can complete the square:
0 ≤α2|| y ||2 −2α⟨x, y⟩+ || x ||2 = || y ||2
	
α2 −
	
2 ⟨x, y⟩
|| y ||2

α

+ || x ||2
= || y ||2
-
α2 −
	
2 ⟨x, y⟩
|| y ||2

α +
	 ⟨x, y⟩
|| y ||2

2
−
	 ⟨x, y⟩
|| y ||2

2.
+ || x ||2
= || y ||2
-
α2 −
	
2 ⟨x, y⟩
|| y ||2

α +
	 ⟨x, y⟩
|| y ||2

2.
−|| y ||2
	 ⟨x, y⟩
|| y ||2

2
+ || x ||2

112
Advanced Engineering Mathematics
that is,
0 ≤|| y ||2
	
α −⟨x, y⟩
|| y ||2

2
−
$
⟨x, y⟩
%2
|| y ||2
+ || x ||2
(2.17)
Now, if we choose
α = ⟨x, y⟩
|| y ||2 ,
then (2.17) becomes
0 ≤0 −
$
⟨x, y⟩
%2
|| y ||2
+ || x ||2.
that is,
0 ≤
⟨x, y⟩
2
|| y ||2
≤|| x ||2.
Multiply both sides of the inequality by || y ||2 to get
⟨x, y⟩
2 ≤|| x ||2|| y ||2
and take the square root of both sides to get |⟨x, y⟩| ≤|| x || || y ||. 2
2.3.4 Problems
In problems 1–4, find an o.n. set whose span equals the span of the given set of vectors.
Use exact values, that is, do not make decimal approximations.
1.
1
1

,
2
0

2.
⎧
⎨
⎩
⎡
⎣
1
0
1
⎤
⎦,
⎡
⎣
0
1
1
⎤
⎦,
⎡
⎣
1
1
1
⎤
⎦
⎫
⎬
⎭
3.
⎧
⎨
⎩
⎡
⎣
−1
2
0
⎤
⎦,
⎡
⎣
2
1
−1
⎤
⎦,
⎡
⎣
1
0
2
⎤
⎦
⎫
⎬
⎭
4.
⎧
⎨
⎩
⎡
⎣
1
−1
1
⎤
⎦,
⎡
⎣
1
1
−1
⎤
⎦,
⎡
⎣
−1
1
1
⎤
⎦
⎫
⎬
⎭
5. Suppose a1 =
 3
−4

and a2 =

1
√
2

. Find an o.n. set S = {q1, q2} such that
Span({a1, a2}) = Span(S).

Matrix Theory
113
6. Let
a1 =
⎡
⎣
2
1
1
⎤
⎦, a2 =
⎡
⎣
1
1
0
⎤
⎦, a3 =
⎡
⎣
1
0
2
⎤
⎦.
a. Use the Gram–Schmidt (G.–S.) process on {a1, a2, a3}.
b. Let w1 = a2, w2 = a3, w3 = a1.
Use the Gram–Schmidt (G.–S.) process on
{w1, w2, w3}.
7. If you use the Gram–Schmidt (G.–S.) process on a set of vectors, the conclusion
may depend on the order in which you listed the original vectors. Find an example
of two vectors x, y in R2 so that G.–S. on {x, y} and G.–S. on {y, x} give two different
o.n. sets.
8. Suppose {a1, a2} is a linearly independent set of vectors in R137 and ⟨a1, a2⟩= 1
3.
Find, in terms of {a1, a2}, the set of vectors {q1, q2} produced by the Gram–Schmidt
process.
9. Is it true that for all vectors x, y, z
⟨x, x + y −z⟩+ ⟨y, y + z −x⟩+ ⟨z, z + x −y⟩= || x ||2 + || y ||2 + ||z||2?
If so, why? If not, give a specific counterexample.
10. Suppose x, y, z are nonzero vectors in Rn, x ⊥y, and y ⊥z. For two different
values of n and for each of the three conclusions (i), (ii), (iii), state whether it is
true or false. If true, explain why; if false, give a specific counterexample.
(i) (x + z) ⊥y, (ii) x ∥z, (iii) x ⊥(−y).
a. if n = 2
b. if n = 3
11. Suppose ai, i = 1, 2, 3 are unspecified vectors in R3. Suppose that all we know
about them is that
⟨a1, a1⟩= 2, ⟨a1, a2⟩= 3, ⟨a1, a3⟩= 4,
⟨a2, a2⟩= 5, ⟨a2, a3⟩= 6, ⟨a3, a3⟩= 9.
a. Construct from {a1, a2, a3} a set of three vectors that is a basis for R3 and is an
orthogonal (not necessarily o.n.) set.
b. Construct from {a1, a2, a3} an o.n. basis for R3. [Hint: it may help to use the
identity (2.9)(b).]
12. Explain why for all vectors x, y
||x + y||2 + ||x −y||2 = 2(|| x ||2 + || y ||2).
[Hint: Use (2.9)(b).]
13. Use Theorem 2.13 (b) to explain why Theorem 2.15, that is, the Pythagorean
Theorem, is true.
14. Explain why ||αx|| = |α| || x || for all scalars α and vectors x in Rn.

114
Advanced Engineering Mathematics
15. Suppose that {a1, . . . , an−1} is a linearly independent set of vectors in Rm and
we have used the Gram–Schmidt process to construct an o.n. set {q1, . . . , qn−1}.
Suppose an is a linear combination of {a1, . . . , an−1}. What happens if we try to
continue the Gram–Schmidt process, that is, we try to construct qn?
16. Suppose A has a real eigenvalue λ and corresponding unit eigenvector u. In terms
of λ, find the exact values of (a) ⟨u, Au⟩, (b) ⟨u, A2u⟩, (c) ||Au||2.
17. Assume q is a unit vector and define P ≜qqT. Explain why it is an orthogonal
projection, that is, satisfies the two properties of an orthogonal projection.
18. Suppose {q1, q2} is an o.n. set of vectors in Rm. Let Pi ≜qiqT
i for i = 1, 2. Let
A = γ1P1 + γ2P2 where γi are unspecified scalars γ1, γ2.
a. If w = α1q1 + α2q2 for unspecified scalars α1, α2,
find Aw in terms of
α1, α2, γ1, γ2, q1, q2.
b. If, in addition, m = 2 and γ1 ̸= γ2, find all of the eigenvalues and eigenvectors
of A in terms of γ1, γ2.
19. Suppose P1 and P2 are orthogonal projections and P1P2 = P2P1. Explain why P1P2
is an orthogonal projection, too.
2.4 Orthonormal Bases and Orthogonal Matrices
2.4.1 Orthogonal Sets and Bases
Theorem 2.20
Every o.n. set is linearly independent.
Why? Suppose {q1, . . . , qn} is an o.n. set. To explain why it is linearly independent, we will
explain why the equation
0 = c1q1 + · · · + cnqn
(2.18)
has only the trivial solution for c1, . . . , cn. To see this, for any i with 1 ≤i ≤n, operate with
P ≜qiqT
i on both sides of (2.18) to get
0 = P0 = P(c1q1 + · · · + cnqn) = c1Pq1 + · · · + cnPqn = ciqi,
hence, ci = 0. Since this is true for each i, (2.18) has only the trivial solution, and so
{q1, . . . , qn} is linearly independent. 2
Corollary 2.2
If S is an orthogonal set of nonzero vectors, then S is a basis for Span(S).

Matrix Theory
115
Corollary 2.3
If {a1, . . . , an} is a linearly independent set of vectors in Rm and {q1, . . . , qn} is the o.n. set
produced by the Gram–Schmidt process (and thus satisfies the conclusions of Theorem
2.16) in Section 2.3, then {q1, . . . , qn} is an o.n. basis for Span{a1, . . . , an}.
Given a real, m × n matrix, define
Col(A) ≜{Ax : x in Rn},
which is called the column space of A or range of A. The MATLAB command
colspace(A) gives a basis for the column space of A.
Write A in terms of its columns, that is,
A =
 
a1 
 . . .

 an
!
.
By Lemma 1.3 in Section 1.7, for every
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦.
we have
Ax = x1a1 + · · · + xnan,
so every vector in Col(A) is a linear combination of the columns of A, that is,
Col(A) = Span({a1, . . . , an}).
Corollary 2.4
If the columns of a real, m × n matrix A are linearly independent then the following are
bases for Col(A):
(a) {a1, . . . , an}, and
(b) {q1, . . . , qn}, the o.n. set produced by the Gram–Schmidt process.
Corollary 2.5
If a real, m × n matrix A has rank(A) = ℓthen use Theorem 1.41 in Section 1.7 to construct
a basis {ai1, . . . , aiℓ} for Col(A). Then use the Gram–Schmidt process in Theorem 2.16 in
Section 2.3 to construct an o.n. basis {q1, . . . , qℓ} for Col(A). From that o.n. basis we can use
Theorem 2.19 in Section 2.3 to construct PA, an orthogonal projection onto Col(A).

116
Advanced Engineering Mathematics
Corollary 2.6
If {q1, . . . , qn} is an o.n. set in Rn and x, y are in Rn then
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
(a) x = ⟨x, q1⟩q1 + · · · + ⟨x, qn⟩qn
(b) || x ||2 = |⟨q1, x⟩|2 + · · · + |⟨qn, x⟩|2
(c) ⟨x, y⟩= ⟨x, q1⟩⟨q1, y⟩+ · · · + ⟨x, qn⟩⟨qn, y⟩
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(2.19)
Note that these results discuss the special case of an o.n. set of n vectors in Rn.
Why? By Theorems 1.43 in Section 1.7 and 2.20 in Section 2.4, {q1, . . . , qn} is a basis for Rn,
so there are scalars c1, . . . , cn such that
x = c1q1 + · · · + cnqn.
(2.20)
If we take the inner product of both sides of (2.20) with qj we get
⟨x, qj⟩= c1⟨q1, qj⟩+ · · · + cn⟨qn, qj⟩= c1δ1j + · · · + cnδnj = cj,
from which (2.19)(a) follows. Because the Pythagorean theorem explains why
|| x ||2 = ||c1q1||2 + · · · + ||cnqn||2 = |c1|2||q1||2 + · · · + |cn|2||qn||2 = |c1|2 + · · · + |cn|2,
(2.19)(a) implies (2.19)(b). As for (2.19)(c), you will do that calculation in Problem
2.4.4.15. 2
By the way, (2.19)(b) and (c) are known as Parseval identities. Both (2.19)(a) and the
conclusion of the next result are known as “Expansion” theorems.
Corollary 2.7
Suppose v1, . . . , vℓis an orthogonal set of nonzero vectors in Rm and y is a vector in
Span{v1, . . . , vℓ}. Then
y =
k
/
i=1
1
||vi||2 ⟨y, vi⟩vi.
2.4.2 Orthogonal Matrices
Definition 2.16
A square matrix Q is an orthogonal matrix if it satisfies QTQ = I.

Matrix Theory
117
Partitioning any matrix Q in terms of its columns, that is, Q =
 
q1 
 . . .

 qn
!
, we have
QTQ =
⎡
⎢⎢⎢⎢⎢⎣
qT
1
−−
...
−−
qT
n
⎤
⎥⎥⎥⎥⎥⎦
 
q1 
 . . .

 qn
!
=
 
qT
i qj
!
i = 1, . . . , n
j = 1, . . . , n
.
Remarks
(1) For an n × n matrix to be a real, orthogonal matrix, its columns must be an o.n. set
in Rn, hence must be an o.n. basis for Rn.
(2) If Q is an orthogonal matrix then it is invertible and Q−1 = QT. You will explain
the following related result in Problem 2.4.4.22.
Corollary 2.8
Suppose Q =
 
q1 
 . . .

 qn
!
is an orthogonal matrix. Then the unique solution of
Qx = b is
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦=
⎡
⎢⎣
⟨b, q1⟩
...
⟨b, qn⟩
⎤
⎥⎦=
n
/
i=1
⟨b, qj⟩e(j).
Theorem 2.21
If {u1, . . . , ur} is an o.n. set in Rm and r < m, then we can “complete the o.n. set to get an
o.n. basis for Rm,” that is, construct (m −r) vectors {ur+1, . . . , um} in Rm so that
{u1, . . . , ur; ur+1, . . . , um}
is an o.n. set in Rm.
We will explain this theorem in the appendix later.
Corollary 2.9
Suppose x(1) is a unit vector in Rn and z is a vector in Rn that has the property that every
vector that is orthogonal to x(1) must also be orthogonal to z. Then z is a scalar multiple
of x(1).

118
Advanced Engineering Mathematics
Why? In R3 this seems to be geometrically obvious: A unit vector x(1) is, by definition,
normal to the plane  ≜{x(1)}⊥, so if z ̸= 0, then z being orthogonal to every vector that
is orthogonal to x(1) says that z is also normal to the plane . Because x(1) and z are both
normal to the plane , geometrical intuition tells us that z must be a scalar multiple of x(1).
To explain the result in general and without using intuition, we apply Theorem 2.21:
because x(1) is a unit vector in Rn, there must be vectors x(2), . . . , x(n) so that {x(1), . . . , x(n)}
is an o.n. basis for Rn. It follows from Corollary 2.6 (a) that
z = ⟨z, x(1)⟩x(1) + ⟨z, x(2)⟩x(2) + · · · + ⟨z, x(n)⟩x(n).
But x(2), . . . , x(n), being orthogonal to x(1), must be orthogonal to z, as we assumed. So,
z = ⟨z, x(1)⟩x(1) + 0 + · · · + 0 = (⟨z, x(1)⟩)x(1),
that is, z is a multiple of x(1). 2
2.4.3 Appendix
We will explain Theorem 2.21, that is, why a given o.n. set {u1, . . . , ur} in Rm can be “com-
pleted” to give an o.n. basis for Rm. We will use a version of the Gram–Schmidt process on
the set of vectors

u1, . . . , ur; e(1), . . . , e(m)
,
where e(j), j = 1, .., m are the columns of the m × m identity matrix. The vectors {u1, . . . , ur}
are assumed to be an o.n. set, so the Gram–Schmidt process starts with the next vector,
e(1): We calculate
vr+1 = e(1) −
r
/
j=1
(e(1) • uj)uj.
(2.21)
There are two possibilities regarding (2.21):
1. First, suppose vr+1 ̸= 0, that is, that e(1) is not in the Span{u1, . . . , ur}. In this case,
we let ur+1 =
1
||vr+1||vr+1, so now we have an o.n. set of r + 1 vectors. For example,
in Example 2.30 in Section 2.7, we will have r = 2 < 3 = m and we will be able to
construct u3 using the Gram–Schmidt process on e(1).
The next step would be to try to find the (r + 2)th vector, using
vr+2 = e(2) −
r+1
/
j=1
(e(2) • uj)uj
2. Second, suppose instead that vr+1 = 0. Then our o.n. set is stuck at still having only
the original r vectors, {u1, . . . , ur}. We try to find the (r + 1)th vector in the o.n. set
using

Matrix Theory
119
vr+1 = e(2) −
r
/
j=1
(e(2) • uj)uj.
Continuing in this way, eventually we will find an (r + 1)th vector. Why? Because it can-
not be true that each of e(1), . . . , e(m) are in Span{u1, . . . , ur}, because that would say that
Span{u1, . . . , ur} is Rm, which would violate the initial assumption that r < m and the fact
that the dimension of Rm is m.
So, eventually we will find a vector ur+1 to put in our o.n. set. Likewise, eventually we
will find an (r + 2)th vector ur+1 to put in our o.n. set, etc. Continuing in this way, we use
as many of e(1), . . . , e(m) as we need to construct ur+1, . . . , um. 2
To increase our intuition about the aforementioned method, consider a special situation
when all of the vectors u1, . . . , ur have zeros in their bottom m−r rows. Then clearly each of
e(r+1), . . . , e(m) would be in
$
Span{u1, . . . , ur}
%⊥so we could take uℓ= e(ℓ), ℓ= r + 1, . . . , m.
This would give us a shortcut to completing the basis.
2.4.4 Problems
1. Can we use the set of vectors
⎧
⎨
⎩
⎡
⎣
1
2
−1
⎤
⎦,
⎡
⎣
1
−1
0
⎤
⎦,
⎡
⎣
3
0
−1
⎤
⎦
⎫
⎬
⎭
to construct an o.n. basis for R3 using the G.-S. process? If so, do that; if not,
why not?
2. Find three different real, 2 × 2, orthogonal matrices.
3. Find all values of a, b for which
A =
⎡
⎢⎢⎢⎢⎣
a
0
1/
√
2
1/
√
2
0
b
0
1
0
⎤
⎥⎥⎥⎥⎦
is a real, orthogonal matrix.
4. Find a 3 × 3 real, orthogonal matrix Q that has no zero entry, that is, qij ̸= 0 for all
i, j. Show how you found your Q.
5. Find a 3 × 3 real, orthogonal matrix Q whose first column is
1
√
14
⎡
⎣
1
2
3
⎤
⎦.
6. Is it possible to find a 3 × 3 real, orthogonal matrix Q that has exactly three entries
of zero? Why, or why not?
7. Find a 3 × 3 real, orthogonal matrix Q that has exactly four entries of zero.

120
Advanced Engineering Mathematics
8. Explain why for all angles φ, θ the matrix
A =
⎡
⎣
sin φ cos θ
−sin θ
cos φ cos θ
sin φ sin θ
cos θ
cos φ sin θ
cos φ
0
−sin φ
⎤
⎦
is orthogonal.
9. Suppose Q1 and Q2 are both orthogonal matrices. Explain why Q ≜Q1Q2 is also
an orthogonal matrix.
10. Suppose Q is a real, m × n, matrix whose set of columns is an o.n. set.
a. If m = n, explain why rank(Q) = n.
b. If m > n, explain why rank(Q) = n.
c. Why do we not need to discuss the case m < n?
11. If Q is a real, orthogonal matrix, explain why ⟨Qx, Qy⟩= ⟨x, y⟩for all x, y. [Hint:
Rewrite the left-hand side as a matrix multiplication involving a transpose.]
By the way, sometimes this property of real, orthogonal matrices is stated as
“(multiplication by) a real, orthogonal matrix preserves angles between vectors.”
12. If Q is a real, orthogonal matrix, explain why
a. || Qx || = || x || for all x. [Hint: First, rewrite || Qx ||2 = ⟨Qx, Qx⟩.]
b. Every eigenvalue λ of Q satisfies |λ| = 1. [Hint: If x is a unit eigenvector of Q
corresponding to λ, then 1 = || x ||2 = || Qx ||2 = ||λx||2.] By the way, sometimes
the property of orthogonal matrices in part (a) is stated as “(multiplication by)
an orthogonal matrix preserves lengths of vectors.”
13. Let B =
⎡
⎢⎢⎢⎢⎢⎣
1
√
3
1
√
2
1
√
6
1
√
3
−1
√
2
1
√
6
1
√
3
0
−2
√
6
⎤
⎥⎥⎥⎥⎥⎦
. Is it true that ||Bx||2 = || x ||2 for all x in R3?
14. Let A =
⎡
⎢⎢⎢⎢⎢⎣
1
2
−1
2
1
√
2
1
√
2
1
√
2
0
1
2
−1
2
−1
√
2
⎤
⎥⎥⎥⎥⎥⎦
. Is it true that || Ax ||2 = || x ||2 for all x in R3?
15. Use a calculation, that is, derivation, to explain why (2.19)(c) is correct.
16. Explain why the determinant of every real, orthogonal matrix is ±1. [Hints: Use
QTQ = I and Theorem 1.28(a) and (b) in Section 1.6.]
17. A Householder matrix is of the form Q = I −2qqT, where q is any unit vector.
Explain why each such matrix satisfies QT = Q and is an orthogonal matrix.
18. Suppose A = I −2q1qT
1 −2q2qT
2 , where {q1, q2} is any o.n. set of vectors. Explain
why each such matrix satisfies AT = A and is an orthogonal matrix.
In problems 19–21, find an orthogonal projection onto the column space of the given
matrix.

Matrix Theory
121
19. A =
1
−1
2
1

20. A =
1
−1
2
−2

21. A =
⎡
⎣
1
1
0
0
−1
1
2
0
2
⎤
⎦
22. Explain why Corollary 2.8 is true.
2.5 Least Squares Solutions
If A is an invertible square matrix, then the system of linear algebraic equations Ax = b has
a unique solution x = A−1b. This is a good theoretical result.
But what if A is not invertible or not square? In the real world, we might still need
to find some kind of approximate solution, or, if there are infinitely many solutions, we
might need to find the “best” solution. Now we will study these situations, which go back
in history at least as far back as Gauss’s “day job” of using data from geodesic surveys.
Suppose A is a real, m × n matrix and b is in Rm. The first fundamental problem we want
to solve is to find an x⋆that minimizes the squared error
E(x) ≜||Ax −b||2 =
m
/
i=1
$
(Ax)i −bi
%2 .
(2.22)
E(x) is the square of the distance between Ax, an “arrow”, and b, our “target.” A perfect
“bullseye” would be when E(x) = 0, that is, Ax = b. Note that Ax)i = (Ax) • e(i) is the ith
component of the vector Ax.
We call such a minimizer x⋆a least squares solution (l.s.s.) of Ax = b. The vector
r ≜b −Ax
is called the residual, so the goal of finding a least squares solution is the goal of
minimizing the norm of the residual.
Geometrically, the goal of this problem is to find a vector bA in the vector subspace
Col(A) ≜{Ax : x in Rn} such that bA comes closest to b, as shown in Figure 2.7.
We recall that this is basically the same picture as the illustration of projection onto a
plane in Figure 2.6. This picture suggests that if PA is the orthogonal projection (recall
Corollary 2.5) in Section 2.4 for the vector subspace Col(A), then bA will be the vector in
Col(A) that comes closest to b. Let’s explain why this is true: Given any vector subspace W
contained in Rm, we can define
W⊥= {y in Rm : y ⊥v for all v in W},
(2.23)

122
Advanced Engineering Mathematics
b
bA
Col(A)
FIGURE 2.7
Least squares problems: Projection PA.
w
w
FIGURE 2.8
W⊥.
as illustrated in Figure 2.8, and define P to be the projection onto W. Then every vector b
in Rm can be written uniquely as a sum of a vector in W and a vector in W⊥, namely,
b = Pb + (I −P)b,
according to Theorem 2.17(b) in Section 2.3.
If we define PA to be the projection onto the vector subspace Col(A), then indeed PAb
will be the vector in Col(A) that is closest to b. Why? By the Pythagorean Theorem, if v is
any vector in Col(A), then
||b −v||2 = ||(PAb + (I −PA)b) −v||2
= ||(PAb −v) + (I −PA)b||2 = ||PAb −v||2 + ||(I −PA)b||2,
(2.24)
by the facts that (1) Col(A) is a vector subspace, hence closed under vector addition and
scalar multiplication and (2) both PAb and v are in Col(A). Clearly, (2.24) tells us that the
closest v can get to b is to have v = PAb ≜bA, in order to have ||PAb −v|| = 0.
By the way, the aforementioned says that bA = PAb solves the best approximation
problem for the vector subspace Col(A).
That potentially leaves open at least one more theoretical issue: How do we find a solu-
tion(s) for x⋆so that Ax⋆= bA? We’ll see that there is a very satisfying theoretical answer
to this: “By solving the ‘normal equations.’ ”
Because bA is in Col(A), we are certain to be able to find at least one least squares
solution, that is, an x⋆with Ax⋆= bA. Later, in this section, we will discuss the second
fundamental problem: If there are infinitely many least squares solutions, which should
we choose?

Matrix Theory
123
2.5.1 The Normal Equations
Recall that we denote the columns of In by e(1), . . . , e(n), each of which is a vector in Rn.
Because bA = PAb,
b −bA = (I −PA)b
is automatically orthogonal to every vector in Col(A), that is, (b −bA) is in Col(A)⊥, by
Theorem 2.17(b) in Section 2.3. In particular, for i = 1, . . . , n, Ae(i) is in Col(A), so
(b −bA) ⊥Ae(i), i = 1, . . . , n,
that is,
0 =

Ae(i)T
(b −bA) = (e(i))T 
AT(b −bA)

, i = 1, . . . , n.
This being true for every i = 1, . . . , n, we know that
AT(b −bA) = 0,
that is,
ATb = ATbA = AT $
Ax⋆%
.
So, we have derived the basic theoretical method for finding least squares solutions:
Theorem 2.22
Every least squares solution x⋆for Ax = b must satisfy the normal equations
ATAx = ATb.
(2.25)
The normal equations may have exactly one solution or infinitely many solutions.
Corollary 2.10
If ATA is invertible, then there is exactly one least squares solution,
x⋆=

ATA
−1
ATb.
(2.26)
Example 2.19
Find a l.s.s. of
⎡
⎣
4
0
−1
1
0
2
⎤
⎦
x1
x2

=
⎡
⎣
0
3
5
⎤
⎦.

124
Advanced Engineering Mathematics
Method: With A =
⎡
⎣
4
0
−1
1
0
2
⎤
⎦and b =
⎡
⎣
0
3
5
⎤
⎦, we have that
ATA =
4
−1
0
0
1
2
 ⎡
⎣
4
0
−1
1
0
2
⎤
⎦=
 17
−1
−1
5

is invertible. There is exactly one l.s.s.:
x⋆=

ATA
−1
ATb = 1
84
5
1
1
17
 4
−1
0
0
1
2
 ⎡
⎣
0
3
5
⎤
⎦= · · · = 1
42
 −1
109

. ⃝
Example 2.20
For the system
 1
−1
−2
2
 x1
x2

=
 2
−2

, (a) find (all of) the least squares solution(s), and
(b) find the l.s.s. whose norm is minimized.
Method: With A =
 1
−1
−2
2

and b =
 2
−2

, we have that
ATA =
 5
−5
−5
5

is not invertible. The normal equations are
 5
−5
−5
5

x = ATAx = ATb =
 6
−6

.
The Gauss–Jordan method gives
 5
−5
|
6
−5
5
|
−6

∼

1⃝
−1
|
6/5
0
0
|
0

after R1 + R2 →R2, 1
5R1 →R1. So, there are infinitely many l.s.s.
x⋆=
1.2
0

+ c1
1
1

=
1.2 + c1
c1

,
(2.27)
where c1 is an arbitrary scalar.
(b) To find the minimum norm solution, we complete the square to calculate
||x⋆||2 = (1.2 + c1)2 + c2
1 = 2c2
1 + 2.4c1 + 1.44 = 2(c2
1 + 1.2c1) + 1.44
= 2(c2
1 + 1.2c1 + 0.36 −0.36) + 1.44 = 2(c1 + 0.6)2 + 0.72.
So, ||x⋆|| is minimized by taking c1 = −0.6. The minimum norm l.s.s. is
x⋆⋆≜
 0.6
−0.6

. ⃝
Geometrically, in Example 2.20, we have that
Col(A) =

t
−2t

: −∞< t < ∞


Matrix Theory
125
(a)
(b)
–0.5
1
b2
b1
–1
–1
–1
x2
x1
–2
1
2
–2
–2
(1.2, –2.4)
(0.6, –0.6)
(2, –2)
–3
–4
0.5
1.0
1.5
2.0 2.5
FIGURE 2.9
Example 2.20. (a) In b space and (b) in x space.
is a line, as depicted in Figure 2.9a, which illustrates what is happening in the space where
b = [b1
b2]T and Ax live. Figure 2.9b shows a picture of all of the least squares solutions
x⋆and the l.s.s. of minimum norm, x⋆⋆, in particular.
Write the matrix A = [ a1 
 . . .

 an] in terms of its columns. Then the n × n matrix
ATA =
⎡
⎢⎢⎢⎢⎢⎣
aT
1
−−
...
−−
an
⎤
⎥⎥⎥⎥⎥⎦
[ a1 
 . . .

 an ] =
⎡
⎢⎢⎢⎢⎣
a1 • a1
.
.
.
a1 • an
.
.
.
.
.
.
.
.
an • a1
.
.
.
an • an
⎤
⎥⎥⎥⎥⎦
= [ai · aj]1 ≤i ≤m
1 ≤j ≤n
is called the Gramian or Gram matrix.
Example 2.21
Find the regression line y = f(x) ≜μx + β for the data given in Table 2.1.
Method: Denote by (xi, yi) the data points, for i = 1, 2, 3. Our linear fit for the data will be
in the form y = f(x) ≜μx + β, where constants μ and β are to be chosen to minimize the
total squared error
TABLE 2.1
Data for Example 2.21
x
1
2
3
y
0.5
1.0
2.0

126
Advanced Engineering Mathematics
2.5
2.0
1.5
1.0
0.5
1
2
3
4
f (x)
x
FIGURE 2.10
Regression line for Example 2.21.
E ≜
3
/
i=1
$
yi −f(xi)
%2 =
3
/
i=1
(yi −μxi −β)2.
To understand the concept of regression line, it helps to look at Figure 2.10.
Ideally, we could find μ, β that satisfy
μxi + β = yi, i = 1, 2, 3,
which can be written as a system of three linear equations in the two unknowns μ, β:
⎧
⎨
⎩
μ · 1 + β
=
0.5
μ · 2 + β
=
1.0
μ · 3 + β
=
2.0
⎫
⎬
⎭,
that is,
A
μ
β

≜
⎡
⎣
1
1
2
1
3
1
⎤
⎦
μ
β

=
⎡
⎣
0.5
1.0
2.0
⎤
⎦.
With A =
⎡
⎣
1
1
2
1
3
1
⎤
⎦, the normal equations,
14
6
6
3
 μ
β

= ATA
μ
β

= ATAx = ATb =
1
2
3
1
1
1
 ⎡
⎣
0.5
1.0
2.0
⎤
⎦=
8.5
3.5

,
have unique solution
μ
β

=
14
6
6
3
−1 8.5
3.5

= 1
6
 3
−6
−6
14
 8.5
3.5

=

3
4
−1
3

.
The regression line is
y = 3
4 x −1
3. ⃝

Matrix Theory
127
Figure 2.10 shows the data and the regression line for Example 2.21.
In
general,
we
want
to
find
a
regression
line
y = μx + β
for
data
points
(x, y) = (xi, yi), i = 1, . . . , m, where at least two of the xi’s are distinct. Trying to solve
yi = μxi + β, i = 1, . . . , m can be restated as A
μ
β

= y. The normal equations are
⎡
⎢⎢⎢⎢⎢⎢⎣
m
/
i=1
x2
i
m
/
i=1
xi
m
/
i=1
xi
m
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
μ
β
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
m
/
i=1
xiyi
m
/
i=1
yi
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(2.28)
Some averages commonly used in statistics and mechanics are defined by
x ≜1
m
m
/
i=1
xi,
x2 ≜1
m
m
/
i=1
x2
i ,
xy ≜1
m
m
/
i=1
xiyi,
and
y ≜1
m
m
/
i=1
yi.
System (2.28) is
⎡
⎣
m x2
m x
mx
m
⎤
⎦=
⎡
⎣
μ
β
⎤
⎦=
⎡
⎣
m xy
m y
⎤
⎦.
Divide both sides by m to see that this is
⎡
⎣
x2
x
x
1
⎤
⎦
⎡
⎣
μ
β
⎤
⎦=
⎡
⎣
xy
y
⎤
⎦.
(2.29)
The matrix ATA is invertible because its determinant, x2−
$
x
%2, is guaranteed to be positive.
(In Problem 2.5.2.9 you will explain why.) The l.s.s. is
⎡
⎣
μ
β
⎤
⎦=

ATA
−1
⎡
⎣
xy
y
⎤
⎦=
1
x2 −
$
x
%2
⎡
⎣
1
−x
−x
x2
⎤
⎦
⎡
⎣
xy
y
⎤
⎦.
(2.30)
The regression line is given by
y =
1
x2 −
$
x
%2

( xy −x y )x + (−x xy + x2 y)

.
(2.31)
2.5.1.1 Least Squares Solution and Orthogonal Matrices
Suppose the set of columns of A is already an o.n. set {a1, . . . , an} in Rm. By using Lemma 1.3
in Section 1.7, we can rewrite the system satisfied by least squares solutions, that is,
Ax⋆= bA, as

128
Advanced Engineering Mathematics
x⋆
1a1 + · · · + x⋆
nan = bA.
(2.32)
Because {a1, . . . , an} is an o.n. set, the projection onto Col(A) is given by
PAb = ⟨b, a1⟩a1 + · · · + ⟨b, an⟩an.
Because bA = PAb, we can rewrite (2.32) as
x⋆
1a1 + · · · + x⋆
nan = (aT
1 b)a1 + · · · + (aT
nb)an.
So, if the set of columns of A is an o.n. set, then there is exactly one l.s.s.:
x⋆=
⎡
⎢⎣
x⋆
1...
x⋆
n
⎤
⎥⎦=
⎡
⎢⎣
aT
1 b
...
aT
nb
⎤
⎥⎦=
⎡
⎢⎣
aT
1...
aT
n
⎤
⎥⎦b = ATb.
(2.33)
What if the set of columns of the given m × n matrix A is not o.n.? Can we use the
Gram–Schmidt process to get a general result, as long as the columns of A are lin-
early independent? We will see the answers when we discuss the QR factorization in
Section 2.7.
So, what if we’re trying to solve a system Ax = b and it happens that A is a real, orthog-
onal matrix? Then A−1 = AT, so the unique solution of Ax = b is given by x = ATb, which
equals the unique l.s.s. given by (2.33). Actually, this should not be a great surprise:
Equation (2.33) holds when the set of columns of the m × n matrix A is an o.n. set, which is
a generalization of the special case when A is square and is a real, orthogonal matrix.
Suppose the set of columns of A is an o.n. set but A is m × n with m ̸= n. Because
the columns of A are linearly independent, we must have m > n, by the (Goldilocks)
Theorem 1.42 in Section 1.7. So, what are ATA and AAT in this case? Because the set of
columns of A is an o.n. set, we are guaranteed to have ATA = In. But we will not be able to
say much about AAT.
Example 2.22
(a) Let A =
1
√
2
⎡
⎣
1
1
0
0
1
−1
⎤
⎦. Then
AAT =
1
√
2
⎡
⎣
1
1
0
0
1
−1
⎤
⎦1
√
2
1
0
1
1
0
−1

=
⎡
⎣
1
0
0
0
0
0
0
0
1
⎤
⎦,
which would seem to suggest that we could get something like “part of the identity
matrix.”
(b) Let
A =
1
√
6
⎡
⎣
√
2
−2
√
2
1
√
2
1
⎤
⎦.

Matrix Theory
129
Then
AAT =
1
√
6
⎡
⎣
√
2
−2
√
2
1
√
2
1
⎤
⎦1
√
6
√
2
√
2
√
2
−2
1
1

=
⎡
⎣
1
0
0
0
0.5
0.5
0
0.5
0.5
⎤
⎦.
2.5.2 Problems
In problems 1–6, find all least squares solutions.
1.
⎧
⎨
⎩
x1
−
x2
=
0
x1
+
2x2
=
1
x2
=
2
⎫
⎬
⎭
2.
⎧
⎨
⎩
x1
+
2x2
=
3
3x1
+
4x2
=
5
5x1
−
x2
=
1
⎫
⎬
⎭
3.
⎡
⎣
4
−1
4
−3
2
−1
⎤
⎦x =
⎡
⎣
2
−1
1
⎤
⎦
4.
⎡
⎣
2
−1
2
−2
−1
3
⎤
⎦x =
⎡
⎣
2
−1
1
⎤
⎦
5.
⎡
⎣
1
1
1
−2
0
1
⎤
⎦x =
⎡
⎣
3
1
4
⎤
⎦
6.
⎡
⎣
1
0
−1
1
−1
0
0
1
−1
⎤
⎦x =
⎡
⎣
1
2
0
⎤
⎦
7. If an n × n matrix Q satisfies QTQ = In, explain why QQT = In.
8. If the set of columns of an m × n matrix Q is an o.n. set, explain why QQT is an
orthogonal projection.
9. Assuming that at least two of the xi’s are distinct, explain why x2 −
$
x
%2 is guar-
anteed to be positive. [Hint: Apply the Cauchy–Schwarz inequality in Rm using
y ≜
 
1
m
1
m
. . .
1
m
!T
. As part of the explanation, explain why x, y are linearly
independent.]
In problems 10–12, find the regression line for the data given in the table.
10. Table 2.2
11. Table 2.3
12. Table 2.4
13. Table 2.5 has course grades on a 4.0 scale for students of Dynamics and Circuits.
Using a least squares regression line for the data,

130
Advanced Engineering Mathematics
TABLE 2.2
Data for Problem 2.5.2.10
x
1
2
3
y
1.5
2.0
3.0
TABLE 2.3
Data for Problem 2.5.2.11
x
0.9
2.1
2.8
y
1.0
2.0
3.0
TABLE 2.4
Data for Problem 2.5.2.12
x
0.6
0.7
1.3
1.8
y
0.9
1.1
2.0
3.0
TABLE 2.5
Data for Problem 2.5.2.13
Student#
Dynamics
Circuits
1
3
2.3
2
3.7
2.7
3
2.3
3
4
2.3
2.7
5
1
0
6
2.7
3.3
a. Predict the grade in Circuits for students who receive a B−, that is, 2.7, in
Dynamics, and
b. Predict the grade in Dynamics for students who receive a B−, that is, 2.7, in
Circuits.
14. We wish to fit data (xi, yi),
for i = 1, . . . , m,
to a function of the form
f(x) = αx + βx2. Find the normal equations and find the best such function f(x).
15. If m > n, that is, there are more equations than unknowns, can there be infinitely
many l.s.s.’s?
16. Suppose A is invertible. Explain why there is exactly one least squares solution
for Ax = b and it is given by x⋆= A−1b. Explain also why this is the only solution
of the normal equations.
17. We wish to fit a function of the form f(x) = Aeαx to the data in Table 2.6. Define
z = ln y and fit the data in z = ln f(x).

Matrix Theory
131
TABLE 2.6
Data for Problem 2.5.2.17
x
1.00
2.00
3.00
4.00
y
1.65
2.70
4.50
7.35
2.6 Symmetric Matrices, Definite Matrices, and Applications
We will see that if A is a real matrix and AT = A, then many wonderful things are true,
particularly concerning eigenvalues and eigenvectors. Not only that, but these results help
greatly in using all matrices, as we will see in the next section.
Definition 2.17
A real, square matrix A is symmetric if A = AT.
Note that a real matrix is symmetric if and only if aij = aji for all i, j.
Example 2.23
A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦is symmetric.
Definition 2.18
Two square matrices A and B are orthogonally similar if there is an orthogonal matrix Q
such that B = QTAQ.
2.6.1 A Spectral Theorem
Another name for eigenvalues is spectra.
Theorem 2.23
(Spectral theorem) If A is a real, symmetric, n × n matrix, then A has real eigenvalues λ1 ≥
λ2 ≥. . . ≥λn and there is a choice of corresponding eigenvectors x(1), . . . , x(n) satisfying
(a) {x(1), . . . , x(n)} is an o.n. set, and (b) the matrix Q =
 
x(1) 
 . . .

 x(n) !
is orthogonal and
diagonalizes A.
Why? Here we will explain all of these conclusions except that we will omit the expla-
nation why there are no deficient eigenvalues, that is, why each eigenvalue’s geometric

132
Advanced Engineering Mathematics
multiplicity equals its algebraic multiplicity. (The latter property will be discussed in
Section 13.6.) It follows that we can find a linearly independent set of exactly n eigen-
vectors.
We have a choice concerning the order in which the eigenvalues are listed, so there are
many choices for the diagonal matrix D = QTAQ. Nevertheless, the diagonal entries of D
are the eigenvalues of A, including multiplicities. And, even if we specify D, we may have
many choices for the matrix Q, as we will see in the explanation of Theorem 2.23 which
follows a lemma.
First, Lemma 2.2 will establish orthogonality for eigenvectors corresponding to dis-
tinct eigenvalues. After that we will explain why we can choose a real, orthogonal set
of eigenvectors corresponding to a repeated eigenvalue.
Lemma 2.2
If A is a real, symmetric matrix, then (a) eigenvectors corresponding to distinct eigenvalues
are automatically orthogonal and (b) all of its eigenvalues are real.
Why? First, notice that we didn’t bother to state the size of A because we want to
emphasize that the derivation doesn’t need to be explicit about that size.
(a) If A has distinct eigenvalues λ, μ with corresponding eigenvectors x, y, then we
calculate
yT(Ax) = yT(λx) = λyTx.
Also, using the assumption that A = AT and the fact that every 1 × 1 matrix is
symmetric,
yTAx = (yTAx)T = xTATy = xT(Ay) = xT(μy) = μxTy = μyTx.
Putting these two results together, we get
μyTx = yT(Ax) = λyTx,
hence
(μ −λ)yTx = 0.
Because μ and λ are distinct, we can divide both sides by (μ −λ) ̸= 0 to get
yTx = 0, that is, that x, y are orthogonal.
(b) If A has an eigenvalue λ with corresponding eigenvector x, then we can explain
why λ must be real: let
denote complex conjugation, as in Section 2.1. We saw
in Section 2.1 that we may need complex eigenvectors for real matrices, so we
should not (accidentally) assume that x is real. Define a scalar z by
z ≜xT (Ax) = xT A x.
Using the given information that A is real and the fact that the complex conjugate applied
twice cancels out, we calculate that

Matrix Theory
133
z = xT A x = xT A x = xT A x.
Also, the latter is a 1 × 1 matrix, so it equals its transpose. It follows that
z =

xT A x
T
= xT AT xTT
Using the assumption that A is symmetric and the fact that the transpose applied twice
cancels out, we have
z = xT A x = z.
To summarize, z = z. It follows that z is real. But z = xT (Ax) = xT(λx) = λ xT x, and in
Problem 2.6.3.11 you will explain why xTx is real. It follows that λ must be real. 2
Explanation for Theorem 2.23:
Lemma 2.2 tells us that the real, symmetric, n × n matrix A has only real eigenvalues.
Because the eigenvectors x satisfy real, homogeneous systems of the form (A −λI)x = 0,
we may choose real eigenvectors. (We could choose them to be complex, for example, if
[1 −3]T is an eigenvector, then [(2 + i) −3(2 + i)]T is also an eigenvector, too, but let’s
keep things as simple and real as possible.)
Are eigenvectors corresponding to the same eigenvalue automatically orthogonal? No.
But, we can choose them to be: If, for example, λ1 = · · · = λℓ> λℓ+1 ≥· · · ≥λn and we
can find a complete set of basic solutions v1, . . . , vℓfor the system (A −λ1I)x = 0, then
{v1, . . . , vℓ} would be a basis for the ℓdimensional eigenspace Eλ = λ1. Being a basis, it’s
linearly independent, so the Gram–Schmidt process (Theorem 2.16) in Section 2.3 can be
used to produce an o.n. set {x(1), . . . , x(ℓ)} with
Span({x(1), . . . , x(ℓ)}) = Span({v1, . . . , vℓ}) = Eλ=λ1.
So, if we assume that no eigenvalues of a real, symmetric matrix are deficient, then we
can put together o.n. sets of eigenvectors that span the eigenspaces that correspond to the
distinct eigenvalues of A. By Lemma 2.2, eigenvectors corresponding to distinct eigenval-
ues are automatically orthogonal, so this explains why Theorem 2.23(a) is true, that is,
there is an o.n. set of eigenvectors {x(1), . . . , x(n)} of A. By Theorem 2.20 in Section 2.4, that
set is linearly independent, hence is a basis for Rn, by Theorem 1.43 in Section 1.7. By The-
orem 2.10 in Section 2.2, the matrix P =

x(1)
· · ·
x(n)
is orthogonal and diagonalizes
A. This completes the explanations for Theorem 2.23, except we omit the explanation why
real, symmetric matrices do not have deficient eigenvalues. 2
If there is an eigenvalue of multiplicity ℓ≥2, we have many choices for the order in
which we list v1, . . . , vℓ. The vectors q1, . . . , qℓproduced by the Gram–Schmidt process
may depend on that order. That is why there may be many choices for the orthogonal
matrix Q even for the same diagonal matrix D = QTAQ.
Example 2.24
Find a real, orthogonal matrix Q that diagonalizes A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦.
Method: From Example 2.5 in Section 2.1 (and work done in examples that led up to it),
the three eigenvalues of this matrix are 7 > −2 = −2 and the corresponding eigenspaces
have bases

134
Advanced Engineering Mathematics
Eλ1=7 = Span
⎛
⎝
⎧
⎨
⎩
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭
⎞
⎠, Eλ2=λ3=−2 = Span
⎛
⎝
⎧
⎨
⎩
⎡
⎣
−0.5
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦
⎫
⎬
⎭
⎞
⎠.
The G.–S. process for the linearly independent set
⎧
⎨
⎩
⎡
⎣
2
1
2
⎤
⎦
⎫
⎬
⎭is easy: let x(1) = 1
3
⎡
⎣
2
1
2
⎤
⎦.
Here is the G.–S. process for the linearly independent set
⎧
⎨
⎩
⎡
⎣
−0.5
1
0
⎤
⎦,
⎡
⎣
−1
0
1
⎤
⎦
⎫
⎬
⎭:
x(2) =
1
√
1.25
⎡
⎣
−0.5
1
0
⎤
⎦=
1
√
5
⎡
⎣
−1
2
0
⎤
⎦,
v3 =
⎡
⎣
−1
0
1
⎤
⎦−
⎛
⎝
⎡
⎣
−1
0
1
⎤
⎦• 1
√
5
⎡
⎣
−1
2
0
⎤
⎦
⎞
⎠1
√
5
⎡
⎣
−1
2
0
⎤
⎦= · · · = 1
5
⎡
⎣
−4
−2
5
⎤
⎦, and
r33 =
√
45
5
, x(3) =
5
√
45
· 1
5
⎡
⎣
−4
−2
5
⎤
⎦.
The matrix
Q =
⎡
⎣1
3
⎡
⎣
2
1
2
⎤
⎦
1
√
5
⎡
⎣
−1
2
0
⎤
⎦
1
√
45
⎡
⎣
−4
−2
5
⎤
⎦
⎤
⎦,
that is,
Q =
1
3
√
5
⎡
⎢⎣
2
√
5
−3
−4
√
5
6
−2
2
√
5
0
5
⎤
⎥⎦,
is a real, orthogonal matrix that diagonalizes A. ⃝
Because Q is an orthogonal matrix, Q−1 = QT, so the diagonalization of A is given by
QTAQ = D.
It is very good to avoid needing to find the inverse of a matrix, even if it means relying
on theory. That’s because, as we will see better in Chapter 8, finding an inverse is likely to
introduce numerical error.
As usual, it is good to check that QTAQ = D actually is true.

Matrix Theory
135
2.6.1.1 A Spectral Formula
Suppose A is a real, symmetric matrix and Q is a real, orthogonal matrix that diagonalizes
A, specifically A = QDQ−1 = QDQT, where D = diag(λ1, . . . , λn). We calculate that
A = QDQT =
 
q(1) 
 . . .

 q(n) !
diag(λ1, . . . , λn)
⎡
⎢⎣
qT
1...
qT
n
⎤
⎥⎦=
 
q(1) 
 . . .

 q(n) !
⎡
⎢⎣
λ1qT
1
...
λnqT
n
⎤
⎥⎦
= q1λ1qT
1 + · · · + qnλnqT
n
that is,
Theorem 2.24
(Spectral decomposition) If A is a real, symmetric, n × n invertible matrix whose eigen-
values, including multiplicities, are λ1, λ2, . . . , λn and whose corresponding set of real
eigenvectors {q(1), . . . , q(n)} is an o.n. basis for Rn, then
A =
n
/
i=1
λiqiqT
i .
(2.34)
This says that a real, symmetric matrix can be decomposed into a “weighted sum” of
projections qiqT
i . We saw this in action in Problem 2.3.4.18. We will see something similar
when we discuss the Singular Value Decomposition in Section 2.7.
Example 2.25
For the matrix A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦of Example 2.5 in Section 2.1 state explicitly the
spectral decomposition.
Method: Using the eigenvalues of Example 2.5 in Section 2.1, and set of corresponding
eigenvectors that is an o.n. basis for R3, the spectral decomposition of A is
A = λ1q1qT
1 + λ2q2qT
2 + λ3q3qT
3
= 7
⎡
⎢⎢⎢⎢⎣
2
3
1
3
2
3
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
2
3
1
3
2
3
⎤
⎥⎥⎥⎥⎦
T
+ (−2)
⎡
⎢⎢⎢⎢⎢⎣
−1
√
5
2
√
5
0
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
−1
√
5
2
√
5
0
⎤
⎥⎥⎥⎥⎥⎦
T
+ (−2)
⎡
⎢⎢⎢⎢⎢⎢⎣
−
4
√
45
−
2
√
45
5
√
45
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
−
4
√
45
−
2
√
45
5
√
45
⎤
⎥⎥⎥⎥⎥⎥⎦
T
= 7
9
⎡
⎣
4
2
4
2
1
2
4
2
4
⎤
⎦−2
5
⎡
⎣
1
−2
0
−2
4
0
0
0
0
⎤
⎦−2
45
⎡
⎣
16
8
−20
8
4
−10
−20
−10
25
⎤
⎦. ⃝

136
Advanced Engineering Mathematics
2.6.1.2 Positive Deﬁnite and Positive Semi-Deﬁnite Matrices
Definition 2.19
A real, square matrix A is
(a) Positive definite if xTAx > 0 for all x ̸= 0
(b) Positive semi-definite if xTAx ≥0 for all x
(c) Negative definite if xTAx < 0 for all x ̸= 0, that is, if −A is positive definite
Example 2.26
Suppose A is a positive definite matrix and P is invertible. Explain why PTAP is also
positive definite.
Method: While the problem did not say so, implicitly we should assume that the sizes of
A and P agree so that the matrix product PTAP exists. For all x ̸= 0,
xT 
PTAP

x =

xTPT
A (Px) = (Px)T A (Px) > 0,
because x ̸= 0 and P invertible together imply Px ̸= 0. (Why?) ⃝
Theorem 2.25
Suppose A is a real, symmetric matrix and λ1, λ2, . . . , λn and {q(1), . . . , q(n)} are as in
Theorem 2.24. Then,
(a) All of its eigenvalues are positive if, and only if, A is positive definite.
(b) All of its eigenvalues are nonnegative if, and only if, A is positive semi-definite.
Why? By Corollary 2.6(a) and (b) in Section 2.4, we have
x = c1q1 + · · · + cnqn and || x ||2 = |c1|2 + · · · + |cn|2.
Using the spectral decomposition of A, we have
Ax =
- n
/
i=1
λiqiqT
i
.
(c1q1 + · · · + cnqn) =
n
/
i=1
λiciqi.
Corollary 2.6(c) in Section 2.4, a Parseval identity, implies
xTAx = ⟨x, Ax⟩=
0 n
/
i=1
ciqi,
n
/
j=1
λjcjqj
1
=
n
/
i=1
λi|ci|2.
(2.35)
The desired conclusions (a) and (b) follow from (2.35) and Definition 2.19. 2

Matrix Theory
137
2.6.1.3 Application to A2, A−1,
√
A
Not only is (2.34) a nice formula, it will give similar, nice formulas for A2 and A−1 and
even to a concept of
√
A, if the latter two exist.
If A is a real, symmetric, n × n, and invertible (the latter is equivalent to all of its eigenval-
ues being nonzero), we can calculate A−1: if Q is a real, orthogonal matrix that diagonalizes
A, then the set of columns of Q is a basis for Rn. To solve Ax = b, we use
n
/
i=1
(qT
i b)qi = b = Ax =
- n
/
i=1
λiqiqT
i
.
x =
n
/
i=1
λi(qT
i x)qi.
The solution x is easily found to have
qT
i b = λiqT
i x, i = 1, . . . , n,
so
A−1b = x =
n
/
i=1
(qT
i x)qi =
n
/
i=1
λ−1
i
(qT
i b)qi =
- n
/
i=1
λ−1
i
qiqT
i
.
b.
(2.36)
This gives us
Theorem 2.26
If A is a real, symmetric, n × n invertible matrix whose set of eigenvectors q(1), . . . , q(n) is
an o.n. basis for Rn, then
A−1 =
n
/
i=1
λ−1
i
qiqT
i .
(2.37)
and, similarly,
A2 =
n
/
i=1
λ2
i qiqT
i .
(2.38)
Theorem 2.27
If A is a real, symmetric, n × n matrix all of whose eigenvalues are nonnegative and whose
set of eigenvectors q(1), . . . , q(n) is an o.n. basis for Rn, then
√
A ≜
n
/
i=1

λi qiqT
i
(2.39)
satisfies
√
A
2
= A.
It would be interesting for you to compare this result with that of Problem 2.2.3.19.

138
Advanced Engineering Mathematics
2.6.1.4 Application to Least Squares Solutions
If A is a real, symmetric, n × n invertible matrix, then (2.36) gives a nice formula for x, the
unique solution of Ax = b. In these circumstances, this is also a formula for the unique least
squares solution (l.s.s.), by Problem 2.5.2.16.
But what if A is a real, symmetric, n × n matrix that is not invertible? Say that λ = 0 is an
eigenvalue of algebraic multiplicity (n −r) for A and the other eigenvalues, λ1, . . . , λr, are
nonzero; for convenience, we are no longer ordering the eigenvalues as λ1 ≥λ2 ≥· · · ≥λn.
Using the assumption that A = AT, the normal equations become
A2x = ATAx = ATb = Ab.
(2.40)
The spectral decomposition (2.34) and the assumption that λr+1 = · · · = λn = 0 give
A =
n
/
i=1
λiqiqT
i =
r
/
i=1
λiqiqT
i +
n
/
i=r+1
0 · qiqT
i =
r
/
i=1
λiqiqT
i
and (2.38) gives
A2 =
r
/
i=1
λ2
i qiqT
i .
The normal equations, that is, (2.40), become
- r
/
i=1
λ2
i qiqT
i
.
x =
- r
/
i=1
λiqiqT
i
.
b.
By the usual sort of manipulations, for example, (qiqT
i )x = (qT
i x)qi, the solutions of the
normal equations, that is, all least squares solutions, are given by
x⋆=
- r
/
i=1
λ−1
i
qiqT
i
.
b +
n
/
i=r+1
ciqi,
where the ci are arbitrary constants.
If we ask for the l.s.s. of minimum norm, Parseval identity (2.19)(b) in Section 2.4 implies
that this is
x⋆⋆=
- r
/
i=1
λ−1
i
qiqT
i
.
b.

Matrix Theory
139
Definition 2.20
Suppose A is a real, symmetric, n × n matrix having λ = 0 as an eigenvalue of algebraic
multiplicity (n −r) and all other eigenvalues of A being λ1, . . . , λr. Then we define
A+ ≜
r
/
i=1
λ−1
i
qiqT
i ,
(2.41)
called a “partial inverse” or the “Moore–Penrose generalized inverse.”
In these special circumstances, the minimum norm l.s.s. of Ax = b is given by
x⋆⋆= A+b.
2.6.2 Further Study of Positive Deﬁnite Matrices
Theorem 2.28
Suppose A is a real, symmetric matrix. Then scalars η < ∞and ξ > −∞can be chosen so
that η ≥xTAx ≥ξ for all unit vectors x. The values of η, ξ may depend upon A.
We postpone the explanation to the appendix at the end of this section. 2
Example 2.27
Explain why for any given any real matrix A we can choose a scalar γ so that (A + γ I) is
positive definite.
Method: Given a real, n × n, matrix A, choose ξ as in conclusion (b) of Theorem 2.28 and
any γ > −ξ. Then, for all unit vectors x,
xT(A + γ I)x = xTAx + γ xTx = xTAx + γ || x ||2 ≥ξ + γ · 1 > 0. ⃝
Theorem 2.29
Suppose W is a real, symmetric positive definite n × n matrix. Define
⟨x, y⟩W ≜⟨x, Wy⟩= xT WTy
and
|| x ||W =

⟨x, x⟩W.
Then all of the conclusions of Theorems 2.12 and 2.13 in Section 2.3 are true for ⟨x, y⟩W and
|| x ||W replacing ⟨x, y⟩and ||x||, respectively.
You will explain the results of Theorem 2.29 in Problem 2.6.3.17.

140
Advanced Engineering Mathematics
2.6.2.1 Vibrations and the Generalized Eigenvalue Problem
In Section 5.3, we will study vibrations of spring–mass systems modeled by systems of
differential equations;
¨x = Ax.
In fact, those systems can also be written in the form
M¨x + Kx = 0,
(2.42)
where the n × n mass matrix (also known as an inertia matrix) M is real, symmetric, and
positive definite and the stiffness matrix K is also real, symmetric, and positive def-
inite. If we try solutions of linear constant coefficients homogeneous system of ODEs
(LCCHS) (2.42) in the form x = eλtv where v is a constant vector, then we would get the
generalized eigenvalue problem
(K + μM)x = 0,
(2.43)
where μ = λ2.
Theorem 2.30
For the generalized eigenvalue problem (2.43), all eigenvalues μj are negative and there is a
set eigenvectors that is an o.n. basis for Rn. Correspondingly, system (2.42) has frequencies
of vibration ω ≜−μj.
Why? We can rewrite (2.43) as
0 = (K + μ
√
M 2) x =
√
M (
√
M −1K
√
M −1 + μI )
√
M x.
Defining y =
√
Mx, (2.43) is equivalent to
(
√
M −1K
√
M −1 + μI ) y = 0.
Defining C ≜
√
M −1K
√
M −1 and γ = −μ, (2.43) is equivalent to
Cx = γ x.
(2.44)
In problem 2.6.3.20, you will explain why the facts that both M and K are real, symmetric,
and positive definite imply that C is also real, symmetric, and positive definite. It fol-
lows from Theorems 2.23 and 2.25 that all eigenvalues γ = −μ are positive and Rn has an
o.n. basis of eigenvectors. This implies the conclusions of this theorem. 2.

Matrix Theory
141
2.6.2.2 Positive Deﬁniteness and Determinants
Suppose A =
a
b
b
c

is a real, 2 × 2, symmetric, positive definite matrix. Then for all
x = [x1
x2]T ̸= 0,
0 < xTAx = [x1
x2]T
a
b
b
c
 x1
x2

= · · · = ax2
1 + 2bx1x2 + cx2
2.
In particular, for x = [1
0]T, we get 0 < ax2
1, so necessarily a > 0. Similarly, x = [0
1]T
yields c > 0. So, the diagonal entries of A must be positive.
Further, by completing a square, we get
0 < xTAx = ax2
1 + 2bx1x2 + cx2
2 = a
	
x2
1 + 2
	bx2
a

x1

+ cx2
2
= a
-
x2
1 + 2
	bx2
a

x1 +
	bx2
a

2
−
	bx2
a

2.
+ cx2
2 = a
	
x1 + bx2
a

2
−b2x2
2
a
+ cx2
2
= a
	
x1 + bx2
a

2
+

ac −b2
x2
2.
Positivity for all x = [x1
x2]T ̸= 0 yields a > 0, which we already knew, and ac −b2 > 0,
that is, |A| > 0.
Now, if A =

aij

1 ≤i ≤n
1 ≤j ≤n
is n × n a real, symmetric, positive definite matrix, then for all
x ̸= 0 of the special form
x = [x1
x2
. . .
xn]T = [x1
x2
0
. . .
0]T,
we have, using aij = aji,
0 < xTAx = a11x2
1 + 2a12x1x2 + a22x2
2 = [x1
x2]
a11
a12
a12
a22
 x1
x2

.
By the result in the previous paragraph and again noting that aij = aji, we conclude that
necessarily
a11 > 0, a22 > 0, and

a11
a12
a21
a22
 > 0.
This motivates.

142
Advanced Engineering Mathematics
Definition 2.21
If A is n × n, then its principal minors are defined by A1 ≜det([a11]), A2 ≜

a11
a12
a12
a22
,. . . ,
Aj ≜

a11
.
.
.
a1j
.
.
.
.
.
.
.
.
.
aj1
.
.
.
ajj

, . . . .
Theorem 2.31
Suppose A is a real, symmetric n × n matrix. If A is positive definite, then its principal
minors A1, A2, . . . , Aj, . . . , An all must be positive. In particular, a11 > 0 and |A| > 0.
Conversely, if all of the principal minors are positive, then A is positive definite.
Note that Aj is the determinant of the matrix obtained from A by deleting all except the
rows 1, 2, . . . , j and the columns 1, 2, . . . , j. This corresponds to only considering x1, . . . , xj
to be allowed to be nonzero in xTAx. Now, if we change the order in which the variables
x1, . . . , xn are listed, then we can define a corresponding minor
Ai1,...,ij
to be the determinant of the matrix obtained from A by deleting all except the rows
i1, . . . , ij and columns i1, . . . , ij. Comparing this notation with the principal minors notation,
we have
A1,...,j = Aj
Theorem 2.32
Suppose A is a real, symmetric, positive definite n × n matrix. Then for all 1 ≤j ≤n and
choices of i1, . . . , ij, we must have Ai1,...,ij > 0; in particular, all diagonal entries of A must
be positive.
Example 2.28
Suppose A is 3 × 3 and has the form A =
⎡
⎣
a
−1
b
−1
c
2
b
2
4
⎤
⎦. If A is required to be positive
definite, reach as many conclusions as possible about the parameters a, b, c, d.
Method: First, the diagonal entries must be positive, so a > 0 and c > 0. Using 2 × 2
minors, all of the following must be positive:
A1,2 =

a
−1
−1
c
 = ac −1, A1,3 =

a
b
b
4
 = 4a −b2, A2,3 =

c
2
2
4
 = 4c −4 = 4(c −1).

Matrix Theory
143
6
4
2
2
0
–2
2
4
6
a
b
c
–4 0
FIGURE 2.11
Example 2.28: Solid in parameter space.
Further,
|A| = A1,2,3 = 4ac −4a −4 −4b −b2c.
The conclusions we reach are a > 0, c > 1, ac > 1, 4a−b2 > 0, and (2a−b)c−2(a+b) > 2. ⃝
Figure 2.11 shows part of the region in parameter space for which the matrix A of
Example 2.28 is positive definite, specifically those (a, b, c) satisfying the inequalities and
contained in the box {(a, b, c) : 0 < a < 6, −5 < b < 2, 1 < c < 5}. We used the Mathematica
command
RegionPlot3D[ac > 1 && 4a −b2 > 0 && 4ac −4a −4b −b2c > 4, {a, 0, 6}, {b, −4, 3},
{c, 1, 6}]
and then rotated the picture using the mouse.
Appendix
First, we need a basic calculation:
Theorem 2.33
For any x ̸= 0 and any scalar β ̸= 0,
xTAx
xTx = (βx)TA(βx)
(βx)T(βx) .
(2.45)

144
Advanced Engineering Mathematics
Why?
(βx)TA(βx)
(βx)T(βx) = 
β2(xTAx)

β2(x)Tx)
= xTAx
xTx . 2
Corollary 2.11
2
xTAx
|| x ||2 : x ̸= 0
3
= {xTAx : || x || = 1}.
Why? If || x || = 1, then
xTAx = xTAx
1
= xTAx
|| x ||2 .
(2.46)
If α is any number in the set
S1 ≜{xTAx : || x || = 1}
that is, α = xTAx, for some x satisfying || x || = 1, then
α = xTAx
1
= xTAx
|| x ||2
is also in the set
S2 ≜
2
xTAx
|| x ||2 : x ̸= 0
3
.
This being true for any number α in S1 explains why S1 ⊆S2.
Now, suppose α is any number in the set S2, that is,
α = xTAx
|| x ||2
for some x ̸= 0. If we denote
&x =
1
|| x || x,
then ||&x|| = 1. Theorem 2.33, with β =
1
|| x ||, explains why
α = xTAx
|| x ||2 = (βx)TA(βx)
||βx||2
= &xTA&x
||&x||2 = &xTA&x
1
= &xTA&x.

Matrix Theory
145
This explains why α is in the set S1. This being true for any number α in S2 explains why
S2 ⊆S1.
Because S1 ⊆S2 and S2 ⊆S2, the sets are equal, that is, S1 = S2. 2.
The explanation for Theorem 2.28 is a little bit sophisticated: although it involves calcula-
tions, often it uses logical explanations. Also, using set inclusions adds to the sophistication
of the explanations.
Here we explain the lower inequality in Theorem 2.28, that is, the existence of ξ as
desired; the explanation of the upper inequality, that is, the existence of η, is similar. By
Corollary 2.11,
˘m ≜min
2
xTAx
xTx : x satisfying x ̸= 0
3
= min{xTAx : x satisfying || x || = 1} ≜&m.
The explanations for why these minimums are “achieved,” for example, why there is a
specific vector&x ̸= 0 for which the minimum defined to be &m =&xTA&x, is more sophisticated
than we choose to discuss here.∗
Given a real, n×n, matrix A, of the n2 numbers aij, there is a positive number υ such that
|aij| ≤υ. For any x satisfying || x || = 1, we have
xTAx =
n
/
i=1
n
/
j=1
aijxixj.
To get a lower bound on xTAx, use the triangle inequality to see that
xTAx
 =

n
/
i=1
n
/
j=1
aijxixj

≤
n
/
i=1
n
/
j=1
aijxixj
 =
n
/
i=1
n
/
j=1
aij
 |xi|
xj
 ≤
n
/
i=1
n
/
j=1
υ |xi|
xj
 .
Now, for any two real numbers x, y, we have 0 ≤1
2 (|x|−|y| )2 = 1
2 x2−|x| |y|+ 1
2 y2, hence,
|x| |y| ≤1
2

x2 + y2
.
Replacing x by xi and y by xj gives
xTAx
 ≤
n
/
i=1
n
/
j=1
υ · 1
2

x2
i + x2
j

= υ
2
n
/
i=1
n
/
j=1

x2
i + x2
j

.
Noting that
n
/
i=1
n
/
j=1
x2
i = n(x2
1 + · · · + x2
n) =
n
/
i=1
n
/
j=1
x2
j , we have
xTAx
 ≤υ
2 · 2n(x2
1 + · · · + x2
n) = nυ|| x ||2 = nυ · 1 = nυ.
∗See Theorem 6.10.13 of Naylor and Sell (1982) mentioned in the “Read more about it” at the end of Section 2.10.

146
Advanced Engineering Mathematics
If b > 0 and a is real, then |a| ≤b is equivalent to b ≥a ≥−b. So,
nυ ≥xTAx ≥−nυ ≜ξ. 2
2.6.3 Problems
In problems 1–10, find a real, orthogonal matrix that diagonalizes the given matrix. Use
exact values, that is, do not make decimal approximations.
1.
1
2
2
4

2.

0
−
√
3
−
√
3
0

3.
⎡
⎣
0
1
0
1
0
0
0
0
2
⎤
⎦
4.
⎡
⎣
1
0
−5
0
−2
0
−5
0
1
⎤
⎦[Hint: 6 is an eigenvalue.]
5.
⎡
⎣
−2
4
4
4
7
−5
4
−5
7
⎤
⎦[Hint: 12 is an eigenvalue.]
6.
⎡
⎣
1
−3
0
−3
1
0
0
0
−2
⎤
⎦
7.
⎡
⎣
−2
0
0
0
3
−1
0
−1
3
⎤
⎦
8.
⎡
⎣
5
3
0
3
5
0
0
0
5
⎤
⎦
9.
⎡
⎣
7
4
−4
4
−8
−1
−4
−1
−8
⎤
⎦
10.
⎡
⎣
4
2
−2
2
1
−1
−2
−1
1
⎤
⎦[Hint: 0 and 6 are eigenvalues.]
11. Explain why xTx is real for every vector x in Cn.
12. Suppose A is a real, m × n matrix, ATA is invertible, m > n, and we define
B = A(ATA)−1AT. Explain why B is symmetric.
13. Suppose A is a real, m × n matrix, ATA is invertible, m > n, and we define
B = A(ATA)−1AT. Explain why B2 = B.

Matrix Theory
147
14. Explain why (2.38) is correct.
15. Explain why (2.39) is correct.
16. Find all values of the scalar α for which A ≜
1
α
α
1

is (a) positive definite and
(b) positive semi-definite.
17. Suppose W is a real, symmetric, positive definite n × n matrix. Define
⟨x, y⟩W ≜⟨Wx, y⟩≜xT WTy
and
|| x ||W ≜

⟨x, x⟩W.
Explain why all of the conclusions of Theorem 2.29 are true. [Hint: Since ⟨x, y⟩W is
defined using ⟨x, y⟩, you can use the results of Theorem 2.12 in Section 2.3 giving
conclusions about ⟨x, y⟩to get conclusions about ⟨x, y⟩W.]
18. (Small Project) Suppose W is a real, symmetric, positive definite n × n matrix
and ⟨x, y⟩W and || x ||W are defined as in Problem 2.6.3.17. Suppose real, m × n
matrix A and vector b in Rm are given. Consider the problem of finding x⋆that
minimizes ||Ax −b||2
W. We call this a generalized, or weighted least squares
problem, with the letter “W” standing for “weighted.” This problem allows us to
study situations where some errors are more important than others. [for example,
if W =
2
0
0
1

, then the total squared error is E(x) = 2|(Ax)1 −b1|2 +|(Ax)2 −b2|2.]
Explain why x⋆should satisfy the generalized normal equations
AT WTAx = AT WTb.
[Hints: We can still think of Col(A) as a vector subspace of Rm. The orthogonal
projection PA should now be understood as being with respect to the inner prod-
uct ⟨x, y⟩W and the concept of orthogonality similarly understood. For example,
(b −bA) ⊥Ae(i) means
0 = ⟨Ae(i), b −bA⟩W =

Ae(i)T
WT(b −bA) = (e(i))TAT WT(b −bA).]
19. Suppose that instead of wanting to minimize the squared error, ||Ax −b||2, that
is, 4m
i = 1
$
(Ax)i −bi
%2, we want to minimize the relative squared error,
m
/
i=1
	(Ax)i −bi
bi

2
,
assuming all of the bi ̸= 0. Explain why this can be written as a weighted least
squares problem.

148
Advanced Engineering Mathematics
20. Suppose both M and K are real, symmetric, and positive definite. Explain why
C =
√
M −1K
√
M −1 is also real, symmetric, and positive definite.
21. Find two different examples of real, 2 × 2 matrices that are both symmetric and
orthogonal.
22. Suppose Q is a real, symmetric, orthogonal n × n matrix. Use the result of
Problem 2.4.4.12(b) to explain why only ±1 can be eigenvalues of Q.
2.7 Factorizations: QR and SVD
In this section and the next we will explain why we can factor a matrix A in various ways
that are useful foundations for practical, “industrial strength” methods for solving systems
and least squares problems. These factorizations are
A = QR,
A = UVT,
EA = LU,
and
A = LLT
(2.47)
2.7.1 QR Factorization
Theorem 2.34
If m ≥n, A is m × n, and its set of columns is a linearly independent set of vectors in Rm,
then A = QR where Q is an m × n matrix whose set of columns is an o.n. set and R is an
invertible, upper triangular n × n matrix. In addition, if m = n, then Q is a real, orthogonal
matrix.
Note that if m < n, then the columns of A cannot be linearly independent.
Why is Theorem 2.34 true? The Gram–Schmidt process used in explaining Theorem 2.16
in Section 2.3 can be summarized by (2.14) in Section 2.3, that is, for j = 1, . . . , n,
aj = rjjqj + (aj • q1)q1 + · · · + (aj • qj−1)qj−1,
where rjj = ||vj|| and {q1, . . . , qn} is an o.n. set. For example, for j = 1, 2, 3, (2.14) in Section
2.3 says
a1 = r11q1,
a2 = r22q2 + (a2 • q1)q1,
and
a3 = r33q3 + (a3 • q1)q1 + (a3 • q2)q2.

Matrix Theory
149
Define rij = aj • qi for j > i. From (2.14) in Section 2.3, we know that
A = [a1 
 a2 
 . . .

 an] = [r11q1 
 r22q2 + r12q1 
 . . .

 rnnqn + r1nq1 + · · · + rn−1,nqn−1]
= [r11q1 
 r12q1 + r22q2 
 · · ·

 r1nq1 + · · · + rn−1,nqn−1 + rnnqn].
By a calculation similar to Theorem 1.10 in Section 1.2 (see Problem 1.2.5.13),
A = [q1 
 q2 
 . . .

 qn]
⎡
⎢⎢⎢⎢⎢⎢⎣
r11
r12
.
.
.
r1n
0
r22
.
.
.
r2n
.
.
.
.
.
.
.
.
.
.
.
.
0
0
.
.
.
rnn
⎤
⎥⎥⎥⎥⎥⎥⎦
≜QR.
(2.48)
The properties of Q and R follow from the Gram–Schmidt process, that is, from the
conclusions of Theorem 2.16 in Section 2.3. ⃝
If m > n then the matrix Q is not square and thus is not a real, orthogonal matrix, but it
is still true that QTQ = In.
Example 2.29
(Example 2.18 in Section 2.3 again) Find the QR factorization of A =
⎡
⎣
1
1
0
0
−2
1
−1
0
5
⎤
⎦.
Method: Denoting the columns of A by a1, a2, a3, the calculations we did when solving
Example 2.18 in Section 2.3 explain why
q1 =
1
√
2
⎡
⎣
1
0
−1
⎤
⎦, r11 =
√
2 .
After that, we had
q2 =
1
3
√
2
⎡
⎣
1
−4
1
⎤
⎦, r22 =
3
√
2
, r12 = a2 • q1 =
1
√
2
,
and then
q3 = 1
3
⎡
⎣
2
1
2
⎤
⎦, r33 = 11
3 , r13 = a3 • q1 = −5
√
2
, r23 = a3 • q2 =
1
3
√
2
.
It follows that
Q =
 
q1

 q2

 q3
!
=
⎡
⎢⎢⎢⎢⎢⎢⎣
1
√
2
1
3
√
2
2
3
0
−
4
3
√
2
1
3
−1
√
2
1
3
√
2
2
3
⎤
⎥⎥⎥⎥⎥⎥⎦

150
Advanced Engineering Mathematics
and
R =
⎡
⎢⎢⎢⎢⎣
r11
r12
r13
0
r22
r23
0
0
r33
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
√
2
1
√
2
−5
√
2
0
3
√
2
1
3
√
2
0
0
11
3
⎤
⎥⎥⎥⎥⎥⎦
. ⃝
2.7.2 QR and Solving Systems
Suppose an m × n matrix A = QR where Q, R are as in the conclusions of Theorem 2.34.
Then we have a convenient method for solving the system Ax = b, that is,
QRx = b.
(2.49)
Note that the set of columns of Q is an o.n. set of vectors in Rm, so QTQ = In.
If (2.49) has a solution, then multiplication on the left by QT gives
Rx = RIx = (QTQ)Rx = QT(QRx) = QTb.
(2.50)
Because R is upper triangular and invertible, we can solve the augmented matrix
 
R | QTb
!
by back substitution: xn = r−1
nn (QTb)n, xn−1 = r−1
n−1,n−1
$
(QTb)n−1 −rn−1,nxn
%
, etc. So, if x is
a solution, then it satisfies (2.50) and can be computed using back substitution.
MATLAB can do a very good job of using back substitution to solve (2.50) using the
command
x = R\QTb.
A natural question is the converse: “If x satisfies (2.50), will it solve the original system,
Ax = b?” Let’s substitute it in and see: Because R is invertible, x = R−1QTb follows from
(2.50). Does
b = Ax = QRx = QRR−1QTb = QQTb ?
(2.51)
Unfortunately, if m > n then the m × m matrix QQT ̸= Im, as we saw in Example 2.22 in
Section 2.5 and will see in Problem 2.7.7.13. So, no, in answer to the question posed in
(2.51), we have
Theorem 2.35
If A is m × n and its columns are a linearly independent set of vectors, let A = QR where
Q is an m × n matrix whose set of columns is an o.n. set and R is an invertible, upper
triangular n × n matrix. Suppose x satisfies (2.50), that is, Rx = QTb. Then x satisfies Ax = b
if m = n.

Matrix Theory
151
2.7.3 QR and Least Squares Solutions
So, what if we used the QR factorization of A, as in the conclusions of Theorem 2.34, and x
satisfies (2.50), that is, Rx = QTb. If m > n, then what good does that do for us?
Theorem 2.36
If A is m × n and its columns are a linearly independent set of vectors, let A = QR where
Q is an m × n matrix whose set of columns is an o.n. set and R is an invertible, upper
triangular n × n matrix. Suppose x⋆satisfies (2.50), that is, Rx = QTb. Then x is a l.s.s. for
Ax = b.
Why? We will explain why x⋆satisfies the normal equations. We know that R is invertible
and QTQ = In, so x⋆= R−1QTb. We substitute this into the normal equations:
ATAx⋆= (QR)T(QR)x⋆= RTQTQRx⋆=
$
RTInR
%
x⋆=
$
RTR
%
R−1QTb = RTInQTb
= (QR)Tb = ATb.
So, yes, x⋆= R−1QTb is a l.s.s. of Ax = b. 2
The explanation of Theorem 2.36 is a nice example of a derivation because it is a
string of equalities that review our knowledge of both least squares solutions and the QR
factorization.
2.7.4 SVD
Definition 2.22
A matrix  is pseudo-diagonal if it can be written in one of the forms D, [ D 
 O ], or
D
O

for some diagonal matrix D and some zero matrix O.
Equivalently,  is pseudo-diagonal if its entries σij are zero for all i ̸= j.
Theorem 2.37
If A is m × n, then it has a singular value decomposition (SVD)
A = UVT,
where
U is a real, m × m, orthogonal matrix
V is a real, n × n orthogonal matrix
 is an m × n pseudo-diagonal matrix.
Also,  is in the form D if m = n, [ D 
 O ] if n > m, or
D
O

if m > n.

152
Advanced Engineering Mathematics
Note that we do not have to assume that the columns of A are linearly independent.
Here is an outline of the steps used to construct the SVD:
1. Define the real, symmetric, positive semi-definite n × n matrix B ≜ATA and find
its eigenvalues and an o.n. set of eigenvectors.
2. Use the r eigenvalues of B that are positive to construct an invertible, diagonal r × r
matrix S, and use the corresponding eigenvectors of B to construct an n × r matrix
V1 whose columns are an o.n. set of vectors. The columns of the n × n orthogonal
matrix V = [ V1 
 V2 ] consist of all of the eigenvectors.
3. Construct the m × r matrix U1 ≜AV1S−1, whose columns are an o.n. set of vectors.
4. If r < m, then the set of r columns of U1 can be completed to give an o.n. basis for
Rm, for example, using the method of the appendix to Section 2.4. That basis will
be the columns of the m × m orthogonal matrix U = [ U1 
 U2 ].
Before explaining why an SVD exists by “construction,” we need some further back-
ground that is itself useful. Also, we will apply this factorization to least squares
problem.
Recall from Section 2.6 that if a real matrix is symmetric (hence square), then it has only
real eigenvalues and has a basis that is an o.n. set of real eigenvectors. Given any real, m × n
matrix A, define
B = ATA.
Then B is real, n × n, and symmetric. In addition, B is positive semi-definite because
for all x,
xT(Bx) = xT(ATAx) = (Ax)T(Ax) = || Ax ||2 ≥0.
By Theorem 2.25 in Section 2.6, all of B’s eigenvalues μ1, . . . , μn are automatically
nonnegative. By Theorem 2.23 in Section 2.6, we may choose corresponding real eigen-
vectors v1, . . . , vn that form an o.n. basis for Rn.
Even though A may not be symmetric and/or A may not be positive semi-definite, by
constructing B = ATA, we can take advantage of the results of Section 2.6 applied to B.
Also, recall from Section 2.5 that ATA played a significant role in studying least squares
solutions.
It follows that the eigenvalues of B can be denoted by
σ 2
1 , . . . , σ 2
r , 0, . . . , 0
assuming that μ1 ≥. . . ≥μr > 0 = μr+1 = · · · = μn.
Our motivation for separating the positive eigenvalues of B from the zero eigenvalues
is our recollection that when studying nonhomogeneous systems of linear algebraic equa-
tions, we used the solutions of the corresponding homogeneous system, that is, solutions
of Bx = 0, among which are all of the eigenvectors of B corresponding to eigenvalue μ = 0.
Also, as we will see later, the zero eigenvalues of B will play a role in solving least squares
problems.

Matrix Theory
153
To discover how it is possible to factor A as UVT for some real, orthogonal matrices
U, V and pseudo-diagonal matrix , let’s assume that we could and see what that would
tells us about  and V:
B = ATA = (UVT)T(UVT) = (VTUT)(UVT) = V(UTU)VT = V2VT.
(2.52)
But we know from Theorem 2.23 in Section 2.6 that a real, symmetric matrix such as B can
be diagonalized by the real, orthogonal matrix
P =
 
v1 
 . . .

 vn
!
,
because {v1, . . . , vn} is an o.n. set of eigenvectors for B. So,
B = P diag(σ 2
1 , . . . , σ 2
r , 0, . . . , 0) PT.
(2.53)
Comparing (2.52) and (2.53), we have
V
$
2%
VT = B = P diag(σ 2
1 , . . . , σ 2
r , 0, . . . , 0) PT.
So, choose
V = P =
 
v1 
 . . .

 vn
!
and pseudo-diagonal matrix
 = [ σij ]1 ≤i ≤m
1 ≤j ≤n
with σii ≜σi for i = 1, . . . , r, and all other entries of  being zero.
After that, it’s a little more work to discover what U should be. To do that, it helps to
use block matrices introduced in Sections 1.5 and 1.6. For example, we can partition  by
writing it as
 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
σ1
0
.
.
.
0
|
0
.
.
.
0
0
σ2
.
.
.
0
|
0
.
.
.
0
.
.
.
.
|
.
.
.
.
.
.
.
|
.
.
.
.
.
.
.
|
.
.
.
0
0
.
.
.
σr
|
0
.
.
.
0
−
−
−
−
−
−
|
−
−
−
−
0
0
.
.
.
0
|
0
.
.
.
0
.
0
.
.
|
.
.
.
.
0
.
.
|
.
.
.
0
0
.
.
.
0
|
0
.
.
.
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
|
|
Sr × r
|
Or × (n−r)
|
|
|
−−−
|
−−−−
|
|
O(m−r) × r
|
O(m−r) × (n−r)
|
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(2.54)
In (2.54), S = diag(σ1, . . . , σr) is r × r, the upper right zero matrix, O, is r × (n −r), the
bottom left O is (m −r) × r, and the bottom right O is (m −r) × (n −r). Note that S is
invertible because it is a diagonal matrix and all of its diagonal entries are nonzero.

154
Advanced Engineering Mathematics
If m = r then  = [ D 
 O ] or  = D. If n = r then  =
D
O

or  = D.
We recall from Section 1.5 the definition of multiplication of block matrices:
⎡
⎣
C11
|
C12
−−
|
−−
C21
|
C22
⎤
⎦
⎡
⎣
E11
|
E12
−−
|
−−
E21
|
E22
⎤
⎦≜
⎡
⎣
C11E11 + C12E21
|
C11E12 + C12E22
−−−−−
|
−−−−−
C21E11 + C22E21
|
C21E12 + C22E22
⎤
⎦,
where the Cik, Ekj are themselves matrices.
Next, we want to find a real, orthogonal matrix U satisfying A = UVT. Write U and V
as block matrices appropriate for multiplications involving block matrix  as in (2.54):
A = UVT =
 
U1 
 U2
!
⎡
⎣
S
|
O
−−
|
−−
O
|
O
⎤
⎦
⎡
⎣
VT
1
−−
VT
2
⎤
⎦=
 
U1 
 U2
!
⎡
⎣
SVT
1
−−
O
⎤
⎦= U1SVT
1 .
(2.55)
The fact that
A = U1SVT
1
(2.56)
is sometimes called the thin SVD or reduced SVD factorization.
Recall that V =
 
v1 
 . . .

 vn
!
, where {v1, . . . , vn} is an o.n. set of eigenvectors for
ATA. So,
V1 =
 
v1 
 . . .

 vr
!
,
V2 =
 
vr+1 
 . . .

 vn
!
.
As an aside, if r = n, then we don’t need to write V in blocks, that is, V = V1.
Because {v1, . . . , vn} is an o.n. set, so is its subset {v1, . . . , vr}, thus the n × r matrix V1
satisfies VT
1 V1 = Ir. Also, to be of the correct sizes for the block multiplications to make
sense, U1 must be m × r and U2 must be m × (m −r). As an aside, note that if r = m then we
don’t need to write U in blocks, that is, U = U1.
Because of (2.56) and the fact that VT
1 V1 = Ir, we want
AV1 = (U1SVT
1 )V1 = U1S(VT
1 V1) = U1SI = U1S.
(2.57)
It follows that we want
U1 ≜AV1S−1
(2.58)
because we began by assuming we could find U, , V to satisfy A = UVT and then we
proceeded to discover what they should be. If we choose U, , V such that (2.54) and (2.58)
are true, then we have almost all of the desired conclusions about the SVD.
The remaining thing to discuss is why U is a real, orthogonal matrix. First, we can
explain why the set of columns of the block matrix U1 is an o.n. set in Rm:
UT
1 U1 = (AV1S−1)T(AV1S−1) = (S−1)TVT
1
$
ATA
%
V1S−1 = (S−1)TVT
1 BV1S−1

Matrix Theory
155
But S is a diagonal matrix so (S−1)T = S−1 so
UT
1 U1 = S−1VT
1 BV1S−1,
But, by (2.52) and (2.54),
B = V2VT =
 
V1 
 V2
!
⎡
⎣
S
|
O
−−
|
−−
O
|
O
⎤
⎦
2  
V1 
 V2
!
=
 
V1 
 V2
!
⎡
⎣
S2VT
1
−−
O
⎤
⎦= V1S2VT
1 .
It follows that
UT
1 U1 = S−1VT
1 BV1S−1 = S−1VT
1

V1S2VT
1

V1S−1 = S−1 
VT
1 V1

S2 
VT
1 V1

S−1
= S−1Ir S2Ir S−1 = Ir,
hence the set of columns of U1 is an o.n. set in Rm. In the appendix at the end of Section 2.4
we explained why having defined U1 ≜AV1S−1 and having explained why the set of
columns of U1 is an o.n. set, it follows that we can find an m × (m −r) matrix U2 so that
U ≜
 
U1 
 U2
!
is a real, orthogonal matrix.
This completes the explanation of why we can find the SVD of any real, m × n matrix A. 2
On uniqueness of SVD, just by choosing a different order of listing the eigenvalues of B,
we can get a different SVD. We can get more than one significantly different SVD if B = ATA
has an eigenvalue with multiplicity greater than one. Also, if r < m, then there can be more
than one choice for the matrix U2 and thus for U.
Example 2.30
Find an SVD factorization of A =
⎡
⎣
1
1
1
0
0
1
⎤
⎦.
Method: Here m = 3 and n = 2, so U should be 3 × 3,  should be 3 × 2, and V should be
2 × 2.
B = ATA =
1
1
0
1
0
1
 ⎡
⎣
1
1
1
0
0
1
⎤
⎦=
2
1
1
2

has eigenvalues satisfying 0 = |B−μI| = (μ−2)2 −1. So μ1 = 3, μ2 = 1, so σ1 =
√
3, σ2 = 1.
Because B has two positive eigenvalues, r = 2 and we don’t need to decompose V into
blocks, that is, V = V1.
Note that it is μjs that are the eigenvalues of B, not the σjs. The eigenvectors of B are
found by row reduction on

B −μjI | 0

, for example,
[ B −μ1I | 0 ] =
−1
1
| 0
1
−1
| 0

∼

1⃝
−1
| 0
0
0
| 0

,
so we can use
v1 =
1
√
2
1
1


156
Advanced Engineering Mathematics
as a unit eigenvector in the eigenspace of B corresponding to eigenvalue μ1 = 3. We will
omit the similar details which find that
v2 =
1
√
2
 1
−1

as a unit eigenvector in the eigenspace of B corresponding to eigenvalue μ2 = 1.
So, we may take the orthogonal matrix V to be
V = V1 = [ v1

 v2 ] =
1
√
2
1
1
1
−1

.
Because B has no zero eigenvalue, it is not necessary to write V in block form.
Also, because m = 3 and n = 2,
 =
⎡
⎢⎢⎣
σ1
0
0
σ2
−−
−−
0
0
⎤
⎥⎥⎦=
⎡
⎣
S
−−
O
⎤
⎦, where S =
√
3
0
0
1

.
Because m −r = 3 −2 > 0, it is necessary to write U in block form. To find U1, we use
A = U1SVT
1 = U1SVT to imply
U1 = AV1S−1 =
⎡
⎣
1
1
1
0
0
1
⎤
⎦1
√
2
1
1
1
−1
 
1/
√
3
0
0
1

= · · · =
1
√
6
⎡
⎣
2
0
1
√
3
1
−
√
3
⎤
⎦.
To find the third column of U =
 
U1

 U2
!
=
 
u1
u2

 u3
!
, that is, to find U2 = [u3],
we need to find a unit vector u3 that is orthogonal to both of the columns of U1. In the
appendix of Section 2.4 we explained, using a process akin to the Gram–Schmidt process,
how to “complete” an o.n. set to get an o.n. basis. Based on that appendix, using
e(1) =
⎡
⎣
1
0
0
⎤
⎦,
calculate
w3 = e(1) −⟨e(1), u1⟩u1 −⟨e(1), u2⟩u2 = · · · = 1
3
⎡
⎣
1
−1
−1
⎤
⎦,
||w3|| = 1/
√
3, and finally
u3 =
1
√
3
⎡
⎣
1
−1
−1
⎤
⎦.
So, putting it all together, an SVD for A is given by
A = UVT =
⎛
⎝1
√
6
⎡
⎣
2
0
|
√
2
1
√
3
|
−
√
2
1
−
√
3
|
−
√
2
⎤
⎦
⎞
⎠
⎛
⎜⎜⎝
⎡
⎢⎢⎣
√
3
0
0
1
−−
−−
0
0
⎤
⎥⎥⎦
⎞
⎟⎟⎠
	 1
√
2
1
1
1
−1

. ⃝

Matrix Theory
157
Example 2.31
Find an SVD factorization of A =
⎡
⎣
1
1
2
−1
2
√
2
0
0
−1
1
2
−1
2
⎤
⎦.
Method: We will skim over routine details in order to highlight the steps of the method.
In this example, as opposed to Example 2.30, it will turn out that the matrix V must be
written in blocks.
B = ATA = · · · =
⎡
⎣
4
0
0
0
0.5
−0.5
0
−0.5
0.5
⎤
⎦
has eigenvalues μ1 = 4, μ2 = 1, μ3 = 0, so σ1 = 2, σ2 = 1, σ3 = 0. Because B has two positive
eigenvalues, r = 2. The eigenvectors of B are found by row reduction on

B −μjI | 0

, for
example,
[ B −μ1I | 0 ] =
⎡
⎣
0
0
0
| 0
0
−3.5
−0.5
| 0
0
−0.5
−3.5
| 0
⎤
⎦∼· · · ∼
⎡
⎣
0
1⃝
0
| 0
0
0
1⃝
| 0
0
0
0
| 0
⎤
⎦
so we can use
v1 =
⎡
⎣
1
0
0
⎤
⎦
as a unit eigenvector in the eigenspace of B corresponding to eigenvalue μ1 = 4. We will
omit the similar details which find that
v2 =
1
√
2
⎡
⎣
0
1
−1
⎤
⎦
and
v3 =
1
√
2
⎡
⎣
0
1
1
⎤
⎦
are unit eigenvectors in the eigenspaces of B corresponding to eigenvalues μ2 = 1, μ3 = 0.
So, we have the orthogonal matrix
V = [ v1

 v2

 v3 ] =
1
√
2
⎡
⎣
√
2
0
0
0
1
1
0
−1
1
⎤
⎦
and the corresponding diagonal matrix is
 = diag(σ1, σ2, σ3) =
⎡
⎣
2
0
0
0
1
0
0
0
0
⎤
⎦=
⎡
⎣
S
|
O
−−
|
−−
O
|
0
⎤
⎦.
Because n−r = 3−2 > 0, it is necessary to write V in block form. Because m−r = 3−2 > 0,
it is necessary to write U in block form. Here,
V1 = [ v1

 v2 ] =
1
√
2
⎡
⎣
√
2
0
0
1
0
−1
⎤
⎦,
V2 = [ v3 ] =
1
√
2
⎡
⎣
0
1
1
⎤
⎦, and S = diag(σ1, σ2) =
2
0
0
1

.

158
Advanced Engineering Mathematics
To find U1, we use A = U1SVT
1 to find
U1 = AV1S−1 =
⎡
⎢⎢⎣
1
1
2
−1
2
√
2
0
0
−1
1
2
−1
2
⎤
⎥⎥⎦
1
√
2
⎡
⎢⎢⎣
√
2
0
0
1
0
−1
⎤
⎥⎥⎦
⎡
⎣
1
2
0
0
1
⎤
⎦= · · · = 1
2
⎡
⎢⎢⎣
1
√
2
√
2
0
−1
√
2
⎤
⎥⎥⎦.
To find the third column of U, that is, U2 = [u3], we need to find a unit vector u3 that
is orthogonal to both of the columns of U1. As in Example 2.30, we use the method of
the appendix of Section 2.4: First, calculate
w3 = e(1) −⟨e(1), u1⟩u1 −⟨e(1), u2⟩u2 = · · · = 1
4
⎡
⎣
1
−
√
2
−1
⎤
⎦,
||w3|| = 1/2, and finally
u3 = 1
2
⎡
⎣
1
−
√
2
−1
⎤
⎦.
So, putting it all together, a SVD for A is given by
A = UVT =
⎛
⎝1
2
⎡
⎣
1
√
2
1
√
2
0
−
√
2
−1
√
2
−1
⎤
⎦
⎞
⎠
⎛
⎝
⎡
⎣
2
0
0
0
1
0
0
0
0
⎤
⎦
⎞
⎠
⎛
⎝1
√
2
⎡
⎣
√
2
0
0
0
1
−1
0
1
1
⎤
⎦
⎞
⎠. ⃝
2.7.5 SVD and L.S.S.
By (2.56), that is, A = U1SVT
1 , the normal equations are
V1S2VT
1 x = · · · = ATAx = ATb = V1SUT
1 b.
(2.59)
Example 2.32
Use (2.59) to find a l.s.s. of Ax = b.
Method: Multiply (2.59) on the left by VT
1 and recall that VT
1 V1 = Ir. After that, multiply
on the left by S−1 to get
SVT
1 x = UT
1 b.
Taking advantage of the fact that VT
1 V1 = I, we try to find a solution of this in the form
x = V1y. When we substitute this in, after exchanging the right and left sides, we get
UT
1 b = SVT
1 V1y = S(VT
1 V1)y = SIry,
whose solution is y = S−1UT
1 b, hence a l.s.s. is given by
x⋆= V1S−1UT
1 b. ⃝
If we had any doubts before, this tells us that

Matrix Theory
159
Theorem 2.38
Ax = b always has at least one l.s.s.
Analogous to the spectral decomposition of (2.34) in Section 2.6, we have
Theorem 2.39
If A = UVT is an SVD, then
A =
r
/
i=1
σiuivT
i ,
(2.60)
where ui, i = 1, . . . , r, are the columns of U1.
Why? By the thin SVD (2.56), that is, A = U1SVT
1 , Theorem 1.10 in Section 1.2 implies
A =
 
u1 
 . . .

 ur
!
diag(σ1, . . . , σr)
⎡
⎢⎣
vT
1...
vT
r
⎤
⎥⎦=
 
σ1u1 
 . . .

 σrur
!
⎡
⎢⎣
vT
1...
vT
r
⎤
⎥⎦
= σ1u1vT
1 + · · · + σrurvT
r . 2
2.7.6 Moore–Penrose Generalized Inverse
We saw a formula for A+, where x⋆⋆= A+b gives the minimum norm l.s.s. of Ax = b, as
long as A is real and symmetric. Let’s get a formula for A+ in general.
We saw, using the SVD of any matrix A, that
x⋆= V1S−1UT
1 b
is a l.s.s. of Ax = b. In Problem 2.7.7.21, you will explain why (2.60) explains why we can
rewrite this as
x⋆=
- r
/
i=1
σ −1
i
viuT
i
.
b.
(2.61)
We will see why (2.61) gives the l.s.s. of minimum norm: rewriting this as
x⋆=
r
/
i=1
(σ −1
i
uT
i b)vi

160
Advanced Engineering Mathematics
shows that x⋆is a linear combination of the columns of V1. But {v1, . . . , vr, vr+1, . . . , vn} is
an o.n. basis for Rn, so every l.s.s. has the form
x =
r
/
i=1
αivi +
n
/
i=r+1
civi
(2.62)
for some scalars αi, i = 1, . . . , r and scalars ci, i = r + 1, . . . , n. We substitute (2.62) into the
normal equations by first calculating
ATAx = (V1S2VT
1 )
⎛
⎝
r
/
i=1
αivi +
n
/
i=r+1
civi
⎞
⎠= · · · =
⎛
⎝
r
/
i=1
σ 2
i αivi +
n
/
i=r+1
0 · civi
⎞
⎠=
r
/
i=1
σ 2
i αivi
(2.63)
and
ATb = VT
1 SUT
1 b =
r
/
i=1
(σiuT
i b)vi.
(2.64)
Equating (2.63) and (2.64), we need to have
αi = σ −1
i
uT
i b,
so in order to be a l.s.s. we must have x to be of the form
x =
r
/
i=1
(σ −1
i
uT
i b)vi +
n
/
i=r+1
civi = x⋆+
n
/
i=r+1
civi
(2.65)
for some scalars ci, i = r + 1, . . . , n. By Parseval identity (2.19)(b) in Section 2.4 and the
orthonormality of {v1, . . . , vr, vr+1, . . . , vn},
|| x ||2 = ||x⋆||2 +
n
/
i=r+1
|ci|2.
The l.s.s. of minimum norm has cr+1 = · · · = cn = 0 and thus is x⋆, as we wanted to
explain. 2
Because of this, we have a formula for the Moore–Penrose generalized inverse for any
matrix A:
A+ =
r
/
i=1
σ −1
i
viuT
i = V1S−1UT
1 .
(2.66)
The Moore–Penrose generalized inverse satisfies many properties:

Matrix Theory
161
Theorem 2.40
If A is real, X ≜A+ satisfies
⎧
⎪⎪⎨
⎪⎪⎩
(a) AXA = A
(b) XAX = X
(c) (AX)T = AX
(d) (XA)T = XA
⎫
⎪⎪⎬
⎪⎪⎭
.
(2.67)
Example 2.33
For the system of Example 2.20 in Section 2.5, that is, Ax ≜
 1
−1
−2
2

x =
 2
−2

= b,
a. Find the Moore–Penrose generalized inverse, A+, and
b. Use it to find the l.s.s. of minimum norm.
Method: (a) We will skim over routine details in order to make clear the method. Begin
to calculate the SVD of A by finding the eigenvalues σ 2
j and corresponding eigenvectors
vj of B ≜ATA =
 5
−5
−5
5

: σ1 =
√
10 > 0 = σ2, so r = 1, with corresponding o.n. basis
of eigenvectors
{v1, v2} =
 1
√
2
 1
−1

, v2

.
After that,
we have V = [V1


V2] with V1 = v1,
the 1 × 1 diagonal matrix
S = diag(σ1) = [
√
10], and so U = [U1

 U2] with
U1 ≜AV1S−1 = · · · =
1
√
5
 1
−2

= [u1].
In order to construct the Moore–Penrose inverse, A+, it is not necessary to know U2
or V2:
A+ =
1
/
i=1
σ −1
i
viuT
i = (
√
10)−1
	 1
√
2
 1
−1

-
1
√
5
 1
−2
T.
= 1
10
 1
−1

[1 −2] = 1
10
 1
−2
−1
2

.
(b) The l.s.s. solution of minimum norm is
x⋆⋆= A+b = 1
10
 1
−1
−2
2
  2
−2

=
 0.6
−0.6

. ⃝
It is a relief to find that this agrees with conclusion (b) of Example 2.20 in Section 2.5.

162
Advanced Engineering Mathematics
Example 2.34
For the system
Ax ≜
⎡
⎢⎢⎣
1
√
2
−1
√
2
0
1
√
2
1
√
2
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
3
0
0
0
2
0
0
0
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
1
√
2
1
√
2
0
−1
√
2
1
√
2
⎤
⎥⎥⎦x =
⎡
⎢⎢⎣
2
−1
1
⎤
⎥⎥⎦= b,
a. Find all least squares solutions.
b. Find the l.s.s. of minimum norm.
c. Find the Moore–Penrose generalized inverse of A.
Method: (a) Writing, as in (2.56),
A = UVT =
 
u1  u

2  u3
!
⎡
⎢⎢⎣
3
0
|
0
0
2
|
0
−
−
|
−−
0
0
|
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
vT
1
vT
2
−−
vT
3
⎤
⎥⎥⎦,
(2.65) implies that all least squares solutions are of the form
x = (σ −1
1 uT
1 b)v1 + (σ −1
2 uT
2 b)v2 + c3v3,
where c3 is an arbitrary scalar. Note that σ1 = 3, σ2 = 2, and σ3 = 0. Here we have that all
least squares solutions are given by
x = 3−1
⎛
⎜⎜⎝
⎡
⎢⎢⎣
1
√
2
1
√
2
0
⎤
⎥⎥⎦•
⎡
⎢⎢⎣
2
−1
1
⎤
⎥⎥⎦
⎞
⎟⎟⎠
⎡
⎢⎢⎣
1
0
0
⎤
⎥⎥⎦+ 2−1
⎛
⎜⎜⎝
⎡
⎢⎢⎣
−1
√
2
1
√
2
0
⎤
⎥⎥⎦•
⎡
⎢⎢⎣
2
−1
1
⎤
⎥⎥⎦
⎞
⎟⎟⎠
⎡
⎢⎢⎣
0
1
√
2
1
√
2
⎤
⎥⎥⎦+ c3
⎡
⎢⎢⎣
0
−1
√
2
1
√
2
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
1
3
√
2
−3
4 −c3 ·
1
√
2
−3
4 + c3 ·
1
√
2
⎤
⎥⎥⎦,
where c3 is an arbitrary scalar.
(b) Because {v1, v2, v3} is an o.n. set, Parseval identity (2.19)(b) in Section 2.4 implies that
|| x ||2 = ||(σ −1
1 uT
1 b)v1 + (σ −1
2 uT
2 b)v2 + c3v3||2 = |σ −1
1 (uT
1 b)|2 + |σ −1
2 (uT
2 b)|2 + |c3|2
is minimized by taking c3 = 0. The l.s.s. of minimum norm is
x⋆⋆= 1
12
⎡
⎢⎢⎣
2
√
2
−9
−9
⎤
⎥⎥⎦.

Matrix Theory
163
(c) By (2.66)
A+ = σ −1
1 v1uT
1 + σ −1
2 v2uT
2 = 3−1
⎡
⎣
1
0
0
⎤
⎦
 1
√
2
1
√
2
0

+ 2−1
⎡
⎢⎣
0
1
√
2
1
√
2
⎤
⎥⎦

−1
√
2
1
√
2
0

= · · · =
⎡
⎢⎢⎣
1
3
√
2
1
3
√
2
0
−1
4
1
4
0
−1
4
1
4
0
⎤
⎥⎥⎦. ⃝
Learn More About It
A good reference for much of the material in this chapter is Matrix Analysis for Sci-
entists and Engineers, by Alan J. Laub, SIAM (Society for Industrial and Applied
Mathematics), c⃝2005. Indeed, we use the notations of that book as much as possible.
2.7.7 Problems
In problems 1–4, find the QR factorization of the given matrix. Use exact values, that is, do
not make decimal approximations. Check that A = QR.
1.
⎡
⎣
1
0
1
1
0
1
⎤
⎦
2.
⎡
⎢⎢⎣
1
0
−1
1
1
2
0
1
⎤
⎥⎥⎦
3.
⎡
⎣
1
0
1
−2
1
1
⎤
⎦
4.
⎡
⎢⎢⎣
1
0
0
−1
1
0
0
−1
1
0
0
−1
⎤
⎥⎥⎦
5. Suppose A is a real, m × n matrix whose set of columns is an orthogonal set of
nonzero vectors. Find the QR factorization of A.
6. Suppose A is a real, m × n matrix whose set of columns is an o.n. set. Find the QR
factorization of A.
7. Suppose A is a real, invertible, upper triangular matrix. What is the QR
factorization of A?
8. Suppose A = QR is the QR factorization of a real, m × n matrix. From (2.51) we
see that if x satisfies Rx = QTb it might still be true that Ax ̸= b. (Why? Because
QQTb may not equal b.) Find an example where x satisfies Rx = QTb and Ax ̸= b.

164
Advanced Engineering Mathematics
9. Suppose A = QR is the QR factorization of a real, m × n matrix. We saw that if
x satisfies Rx = QTb it might still be true that Ax ̸= b. Now, suppose addition-
ally that b = Qc for some n × 1 vector c. Show that this is enough to guarantee
that Ax = b if Rx = QTb. In your explanation(s), do not use specific values for
m, n, A, Q, R, b, or c.
In problems 10 and 11, note the QR factorization and use that to find all least squares
solutions.
10.
⎡
⎢⎢⎢⎢⎣
2/3
1/
√
2
2/3
−1/
√
2
1/3
0
⎤
⎥⎥⎥⎥⎦
⎡
⎣
6
−3
0
√
2
⎤
⎦x =
⎡
⎣
2
−1
1
⎤
⎦
11.
⎡
⎢⎢⎣
2/3
1/
√
5
2/3
0
−1/3
2/
√
5
⎤
⎥⎥⎦
⎡
⎣
3
−3
0
√
5
⎤
⎦x =
⎡
⎣
2
−1
1
⎤
⎦
12. For the system Ax = b given by
⎡
⎢⎢⎢⎣
1/
√
2
0
1/
√
2
0
1
0
−1/
√
2
0
1/
√
2
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
3
0
0
0
2
0
0
0
0
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
1
0
0
0
1/
√
2
1/
√
2
0
−1/
√
2
1/
√
2
⎤
⎥⎥⎥⎥⎦
x
=
⎡
⎢⎣
2
−1
1
⎤
⎥⎦,
(a) Find all least squares solutions, (b) find the l.s.s. of minimum norm, and
(c) find the Moore–Penrose generalized inverse of A.
13. Why did we say, “If m > n” before Theorem 2.36; in particular, why did we not
mention the case m < n?
14. Suppose A is a real, m × n matrix and A = QR is its QR factorization. Explain why
|| Ax ||2 = || Rx ||2 for all x in Rn.
15. Suppose A = [a1
a2] is a real, m × 2 matrix. Suppose we don’t know the entries
of A but we do know that ⟨a1, a1⟩= 2, ⟨a1, a2⟩= −1, and ⟨a2, a2⟩= 2. Without
specifying the entries of A, find the QR factorization of A. Your Q should be
written in terms of a1, a2.
16. Suppose the columns of a real, m × n matrix A are linearly independent. Use
the QR factorization to explain why there is only one l.s.s. of Ax = b, no matter
what b is.
In problems 17–20, find an SVD of the given matrix. Use exact values, that is, do not make
decimal approximations. Check that A = UVT.

Matrix Theory
165
17.
 1
1
−1
1

18.
⎡
⎣
1
0
−5
0
−2
0
−5
0
1
⎤
⎦
19.
⎡
⎣
2
1
4/
√
5
−8/
√
5
2
1
⎤
⎦
20.
1
3
√
2
⎡
⎣
3
√
5
√
5
2
√
10
0
−16
4
√
2
3
√
5
√
5
2
√
10
⎤
⎦
21. Recall from the explanation of the SVD that x⋆= V1S−1UT
1 b is a l.s.s. of Ax = b.
Now, use (2.60) to explain why we can rewrite x⋆= V1S−1UT
1 b as (2.61).
22. Verify that the Moore–Penrose inverse A+ we found in Example 2.33 is a solution
for X that satisfies properties (2.67)(a), (b), (c), and (d).
23. Verify that the Moore–Penrose inverse A+ we found in Example 2.34 is a solution
for X that satisfies properties (2.67)(a), (b), and (d).
24. Suppose AT A is invertible. Then the unique l.s.s. of Ax = b is given by
x = (ATA)−1ATb. Because the minimum norm l.s.s. has to be the only l.s.s. in
this situation, find a formula for the Moore–Penrose inverse A+. Also, verify that
this A+ is a solution for X that satisfies properties (2.67)(a) and (c).
25. If A is a real, symmetric, positive definite matrix, find its SVD and relate it to the
results of the spectral theories of Section 2.6.
26. Suppose A is a real, symmetric, n × n matrix having eigenvalues λ1, . . . , λn and
corresponding set of eigenvectors {q(1), . . . , q(n)} is an o.n. basis for Rn. Assume
also that |λ1| ≥|λ2| ≥· · · ≥|λn|. Find an SVD of A and relate it to the results of
the spectral theories of Section 2.6.
27. (a) Write down an example of a 3 × 3 matrix A whose nine entries are −1, 0, 0, 0,
1, 1, 1, 1, 1, and (b) find the SVD of the matrix A you wrote in part (a).
28. Suppose a square matrix A has QR factorization QR. Explain why A is orthogo-
nally similar to the matrix RQ, the reverse multiplication. This result will be used
by the QR—algorithm mentioned in Section 8.5. [Hint: Multiply A = QR on the
left by QT and, after that, on the right by Q.]
29. (a) Write down an example of a 3 × 3 matrix A that is in RREF, which has exactly
one entry of 2 and exactly one entry of −1, and (b) find the SVD factorization of
the matrix A you wrote in part (a).
2.8 Factorizations: LU and Cholesky
2.8.1 LU Factorizations
One drawback to using the SVD factorization is that it assumes we can find the eigenvalues
and eigenvectors of the symmetric matrix B. As we will see in Chapter 8, for a “large”

166
Advanced Engineering Mathematics
matrix, finding these things accurately can be a difficult problem from the practical point of
view; even finding eigenvectors by solving homogeneous linear systems can have practical
difficulties. Here we will briefly mention two other types of factorizations that are very
useful. In addition to studying these methods further in Chapter 8, we will see how the LU
factorization is a building block for many methods, including for solving finite difference
approximations of partial differential equations in Chapter 12.
Recall that in Section 1.2, we mentioned that all elementary row operations can be imple-
mented by multiplication on the left by elementary matrices. For example, to get a row
echelon form of the matrix A below, the operations R1 ↔R2, followed by −1
5R1+R3 →R3,
followed by R2 ↔R3 on
A =
⎡
⎣
0
0
1
4
10
0
15
10
2
1
2
2
⎤
⎦
can be implemented, respectively, by
E1 ≜
⎡
⎣
0
1
0
1
0
0
0
0
1
⎤
⎦,
E2 ≜
⎡
⎣
1
0
0
0
1
0
−1
5
0
1
⎤
⎦,
and
E3 ≜
⎡
⎣
1
0
0
0
0
1
0
1
0
⎤
⎦.
You can check that
E3E2E1A =
⎡
⎣
10
0
15
10
0
1
−1
0
0
0
1
4
⎤
⎦≜U,
(2.68)
where U is an upper triangular matrix and a row echelon form for A.
The matrix E2 is lower triangular and the matrices E1 and E3 are neither upper nor
lower triangular. E1 and E3 are “permutation matrices,” so called because they change
the sequence of rows. For example, E1 changes the list of rows R1, R2, R3 to R2, R1, R3.
To reduce a matrix to row echelon form, we may need to do row interchanges, that is,
we may need to do “pivoting.” In fact, as we will see in Section 8.4, it often improves
the accuracy of the Gauss–Jordan algorithm to do “implicit partial pivoting,” that is, row
interchanges that move numbers of larger relative magnitude into pivot positions.
Theorem 2.41
Every matrix A has a general LU factorization
EA = LU
(2.69)
where
E is a product of permutation matrices
L is a lower triangular matrix
U is an upper triangular matrix

Matrix Theory
167
We can rewrite (2.69) as
L−1 EA = U
(2.70)
In (2.70), all of the permutation matrices operate on A first, after which a lower triangular
matrix L−1 operates on EA to get U.
Unfortunately, (2.68) does not fit the form (2.70), because the lower triangular matrix E2
is stuck in between the permutation matrices E1 and E3. But, always we can operate on A
with all of the permutations matrices first.
In the above example,
E3E1A = E3
⎡
⎣
10
0
15
10
0
0
1
4
2
1
2
2
⎤
⎦=
⎡
⎣
10
0
15
10
2
1
2
2
0
0
1
4
⎤
⎦
At this point, the operation E2 does no good. Instead, we need to operate using
E4 ≜
⎡
⎣
1
0
0
−1
5
1
0
0
0
1
⎤
⎦.
In fact,
E4(E3E1A) = E4
⎡
⎣
10
0
15
10
2
1
2
2
0
0
1
4
⎤
⎦=
⎡
⎣
10
0
15
10
0
1
−1
0
0
0
1
4
⎤
⎦≜U.
Denote E = E3E1. If we define L−1 ≜E4, then
L = E−1
4
=
⎡
⎣
1
0
0
1
5
1
0
0
0
1
⎤
⎦.
Putting everything together, we have in this example
EA = LU =
⎡
⎣
1
0
0
1
5
1
0
0
0
1
⎤
⎦
⎡
⎣
10
0
15
10
0
1
−1
0
0
0
1
4
⎤
⎦,
where
E is a product of permutation matrices
L is lower triangular
U is upper triangular
If no row interchanges were used, then we would have
A = LU,
which is a special case of the LU factorization but which is not always possible to achieve.
L is not only lower triangular but it is also invertible because all of its diagonal entries
are nonzero.

168
Advanced Engineering Mathematics
To solve a system Ax = b, we note that all permutation matrices are invertible, so we can
use (2.68) to rewrite the system as
E−1LUx = b,
or
LUx = Eb,
(2.71)
If we define y ≜Ux, then solving Ax = b is equivalent to solving
y = Ux
(2.72)
for x in terms of y and then solving
Ly = Eb
(2.73)
2.8.2 Cholesky Factorizations
In the special case that A is real, symmetric, and positive semi-definite, we can find a lower
triangular matrix L so that U = LT gives an LU factorization.
Theorem 2.42
(Cholesky factorization) If A is real, symmetric, and positive semi-definite, we can find
a lower triangular matrix L such that A = LLT.
Why? The next, simple example will suggest how to explain the general result. For the
general derivation, please see the book by John Rice mentioned in the “Read more about
it” at the end of this section.
A matrix that can be written in the form LLT must be symmetric and positive semi-
definite.
Example 2.35
Find a Cholesky factorization of A =
 2
−1
−1
2

.
Method: We want to find a lower triangular matrix L =
ℓ11
0
ℓ21
ℓ22

satisfying A = LLT,
that is,
 2
−1
−1
2

= LLT = · · · =

ℓ2
11
ℓ11ℓ21
ℓ11ℓ21
ℓ2
21 + ℓ2
22

.
The (1, 1) entry of A requires ℓ2
11 = 2, so one Cholesky factorization can use ℓ11 =
√
2.
After that, both the (1, 2) and (2, 1) entries of A require that −1 = ℓ11ℓ21, hence ℓ21 = −1
√
2.
Finally, the (2, 2) entry of A requires 2 = ℓ2
21 + ℓ2
22 = 1
2 + ℓ2
22. One Cholesky factorization
of A is given by

Matrix Theory
169
A =
 √
2
0
−1
√
2
√
3
√
2
  √
2
0
−1
√
2
√
3
√
2
T
. ⃝
Another Cholesky factorization instead would use ℓ11 = −
√
2 and ℓ21 =
1
√
2.
Theorem 2.43
(Positive definiteness) Suppose A is a real, symmetric matrix. Then A is positive definite
if, and only if, A has a Cholesky factorization A = LLT where L is lower triangular and its
diagonal entries are all positive.
Learn More About It
A good reference for this chapter is Computations and Mathematical Software, by John
R. Rice, McGraw-Hall, Inc., 1981. In particular, that book has “pseudo-code” for algo-
rithms to implement the LU factorization, for example, by the Crout algorithm, and
the Cholesky factorization. Another good reference for much of the material in this
chapter is Matrix Analysis for Scientists and Engineers, by Alan J. Laub, mentioned at the
end of Section 2.7.
2.8.3 Problems
In problems 1 and 2, find an LU factorization of the given tri-diagonal matrix. Use exact
values, that is, do not make decimal approximations.
1. A3 =
⎡
⎣
−2
1
0
1
−2
1
0
1
−2
⎤
⎦
2. A4 =
⎡
⎢⎢⎣
−2
1
0
0
1
−2
1
0
0
1
−2
1
0
0
1
−2
⎤
⎥⎥⎦
3. [Small project] For all k ≥3, find an LU factorization of the k × k tri-diagonal
matrix
Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
−2
1
0
.
.
.
0
1
−2
1
.
0
1
−2
.
.
.
.
.
.
.
.
.
.
.
.
.
1
−2
1
0
.
.
.
0
1
−2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Your conclusions for L and U should be k × k matrices, with unspecified k.

170
Advanced Engineering Mathematics
In problems 4–7, find a Cholesky factorization of the given matrix.
4. A =
 3
−1
−1
3

5. A =
 3
−2
−2
2

6. A =
⎡
⎣
2
−1
0
−1
2
1
0
1
1
⎤
⎦
7. A =
⎡
⎣
2
−1
0
−1
2
−1
0
−1
2
⎤
⎦
[Hint: If A is 3 × 3, partition
L =
⎡
⎣
L11
|
O
−
−
|
−−
ℓ31
ℓ32
|
ℓ33
⎤
⎦,
where
L11 is 2 × 2,
and correspondingly
A =
⎡
⎣
A11
|
A12
−−
|
−−
A21
|
A22
⎤
⎦.
When you calculate LLT, you will see that you need A11 = L11LT
11.]
2.9 Rayleigh Quotient
The Rayleigh quotient gives a method for approximating eigenvalues. This method has
been used in vibration problems for systems with a finite number of masses, also known
as discrete systems. The method is particularly useful for estimating the lowest and high-
est eigenvalues of a real, symmetric matrix. Also, we will see in Section 9.7 that there is
a powerful generalization of the Rayleigh quotient to boundary value problems, that is,
continuous systems.
Suppose λ is an eigenvalue of a real, symmetric matrix A with corresponding eigenvector
x. Then,
xT(Ax) = xT(λx) = λ(xTx) = λ|| x ||2.
This motivates

Matrix Theory
171
Definition 2.23
The Rayleigh quotient of a real matrix A is defined by
RA(x) ≜⟨x, Ax⟩
⟨x, x⟩= xTAx
|| x ||2 , for x ̸= 0.
(2.74)
From the discussion preceding the definition, we have
Remark
If a real, symmetric matrix A has eigenvalue λ and corresponding eigenvector x, then
RA(x) = λ.
This explains why the Rayleigh quotient can produce eigenvalues. Turning theory into
practice requires more work, some of which we will do in this section, some of which will
be in Chapter 8, and some of which will be beyond the scope of this book.
Example 2.36
It is known that x ≜[−1
0
1]T is an eigenvector of the matrix
A =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦.
Verify that the Rayleigh quotient produces an eigenvalue of A.
Method: We calculate
RA(x) =
1
||x||2 xTAx = 1
2 [−1
0
1]
⎡
⎢⎣
2
2
4
2
−1
2
4
2
2
⎤
⎥⎦
⎡
⎣
−1
0
1
⎤
⎦
= 1
2 [−1
0
1]
⎡
⎣
2
0
−2
⎤
⎦= 1
2 · (−4).
Direct calculations show that Ax = −2x, that is, RA(x) = −2 is an eigenvalue of A for
which this x is a corresponding eigenvector. ⃝
If A is n × n, then
RA(x) ≜
4n
i=1
4n
j=1 aijxixj
4n
i=1 x2
i
.

172
Advanced Engineering Mathematics
Theorem 2.44
The Rayleigh quotient has the property that for any x ̸= 0 and any scalar β ̸= 0,
RA(β x) = RA(x).
Corollary 2.12
{RA(x) : x ̸= 0} = {RA(x) : || x || = 1}.
These are the same as Theorem 2.33 in Section 2.6 and Corollary 2.11 in Section 2.6
written in the notation of a Rayleigh quotient.
2.9.1 A Rayleigh Theorem
If A is a real, symmetric matrix, then it has eigenvalues λ1 ≥· · · ≥λn and a corresponding
set of eigenvectors {q(1), . . . , q(n)} that is an o.n. basis for Rn. So, the real, orthogonal matrix
Q ≜[q(1)
. . .
q(n)] diagonalizes A, that is, A = QDQT, where D = diag(λ1, . . . , λn). For
any vector x ̸= 0, we define&x =
1
|| x || x and calculate
RA(&x ) =
1
||&x ||2 &xTA&x = 1
1 &xTQDQT&x = (QT&x )TD(QT&x ).
Defining
y = QT&x =
⎡
⎢⎣
y1
...
yn
⎤
⎥⎦,
we have
RA(&x ) = ( QT&x )TD( QT&x ) = yTDy = · · · =
n
/
i=1
λiy2
i
(2.75)
Because y is a unit vector, −1 ≤yi ≤1 for i = 1, . . . , n. It follows that the largest value
that RA(&x ) can be is the largest of the numbers λ1, . . . , λn and the smallest that RA(&x) can
be is the smallest of the numbers λ1, . . . , λn. Equation (2.75) establishes
Theorem 2.45
Suppose A is a real, symmetric n × n matrix. Then the values
λ1 ≜max{RA(x) : x satisfying x ̸= 0}
and
λn ≜min{RA(x) : x satisfying x ̸= 0}

Matrix Theory
173
exist, are eigenvalues of A, and there are eigenvectors x(1), x(n), respectively, that “achieve”
the values λ1 and λn, that is,
λ1 = RA(x(1))
and
λn = RA(x(n)).
Example 2.37
Use the Rayleigh quotient to estimate the minimum and maximum eigenvalues of the
matrix
A =
⎡
⎣
−4
2
3
2
−5
1
3
1
−8
⎤
⎦.
Method: It follows from Theorem 2.37 that
λ1 = max{RA(x) : x satisfying x ̸= 0}.
We use the Mathematica command
FindMaximum[{f[x,y,z], 1 ≥x ≥−1&&1 ≥y ≥−1&&1 ≥z ≥−1}, {x,y,z}],
where we replace the vector x by [x
y
z]T, and calculate that
f(x, y, z) ≜RA(x) = −4x2 + 4xy −5y2 + 6xz + 2yz −8z2
x2 + y2 + z2
.
Mathematica gives output
{−1.10996, {x →0.258129, y →0.167868, z →0.136756},
so the maximum eigenvalue of A is λ1 ≈−1.10996. Similarly, the FindMinimum
command gives us that the minimum eigenvalue of A is λn ≈−9.60653. ⃝
In Section 5.3, we will study vibrations of spring–mass systems modeled by systems of
differential equations ¨x = Ax. In that context, the matrix A will be negative definite. The
frequencies of vibration will turn out to be ω = √−λ, where λ is an eigenvalue of A. So, the
minimum frequency of vibration will be equal to √−λ1, where λ1 is the maximum value
of the Rayleigh quotient of A. For the matrix A of Example 2.37, the lowest frequency of
vibration is ω1 ≈1.05354 and highest frequency of vibration ω3 ≈
√
9.60653 . . . ≈3.09944.
Example 2.38
Use the Rayleigh quotient to estimate the minimum and maximum eigenvalues of the
matrix
A =
⎡
⎢⎢⎣
0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0
⎤
⎥⎥⎦.
Method: The Rayleigh quotient is
RA(x) = 2(x1x2 + x1x3 + x1x4 + x2x3 + x2x4 + x3x4)
x2
1 + x2
2 + x2
3 + x2
4
≜f(x1, x2, x3, x4).
The function f(x1, x2, x3, x4) is symmetrical in the sense that interchanging the roles of xi
and xj would have no effect on RA(x). For example, f(x1, x2, x3, x4) = f(x2, x4, x3, x1).

174
Advanced Engineering Mathematics
This suggests that a vector with maximum symmetry, such as x+ = [1
1
1
1]T,
and a vector with maximum “disorder,” such as x−= [1
−1
1
−1]T, might give
good estimates for eigenvalues. We calculate RA(x+) = 12
4 = 3 and RA(x−) = −4
4 = −1.
This suggests guessing that the eigenvalues of A are λ1 = 3 ≥λ2 ≥λ3 ≥λ4 = −1. ⃝
By the way, for the matrix A of Example 2.38, Mathematica calculates that the eigenvalues
are 3, −1, −1, −1, so the Rayleigh quotient and intuition did really well at guessing the
eigenvalues.
Learn More About It
Below, Problems 2.9.2.7 and 2.9.2.8 are adapted from Theory of Matrices, by Peter
Lancaster, Academic Press, 1969, specifically its Exercises 3.2.2 and 3.2.3. Lancaster’s
book is a very useful reference for the whole subject of matrices. In particular, the book
discusses the “mini-max” technique and its relationship to the eigenvalues between
the minimum and maximum eigenvalues of a real, symmetric matrix.
2.9.2 Problems
In problems 1 and 2, use the Rayleigh quotient to find the exact minimum and maximum
eigenvalues of the 2 × 2 matrix. If possible, reduce the problem to a Calculus problem of
finding the minimum and maximum values of a function of a single variable.
1. A =
2
1
1
−1

2. A =
1
3
3
4

In problems 3 and 4, use the Rayleigh quotient to find the approximations of the minimum
and maximum eigenvalues of the 3 × 3 matrix. You may use Mathematica or MATLAB as
in Example 2.37.
3. A =
⎡
⎣
2
√
3
0
√
3
0
0
0
0
−1
⎤
⎦
4. A =
⎡
⎣
0
1
1
1
0
0
1
0
2
⎤
⎦
In problems 5 and 6, use the Rayleigh quotient and intuition, as in Example 2.38, to find
estimates of the minimum and maximum eigenvalues of the matrix.
5. A =
⎡
⎢⎢⎢⎢⎣
0
1
1
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1
0
1
1
1
1
1
0
⎤
⎥⎥⎥⎥⎦

Matrix Theory
175
6. A =
⎡
⎢⎢⎢⎢⎣
0
1
−1
1
−1
1
0
1
−1
1
−1
1
0
1
−1
1
−1
1
0
1
−1
1
−1
1
0
⎤
⎥⎥⎥⎥⎦
7. Use the vector x = [0
. . .
0 1 0
. . .
0]T = e(i), the ith column of the identity
matrix In, to explain why λn ≤aii ≤λ1 for each of the diagonal elements of a real,
symmetric matrix A.
8. Use the vector x ≜[1
1
. . .
1]T, the “ones” vector, to explain why λn ≤
1
n
4n
i = 1
4n
j = 1 aij ≤λ1 for the elements of a real, symmetric matrix A.
9. Suppose A is a real, symmetric matrix. Let q1 and qn be unit eigenvectors
of A corresponding to λ1 and λn, respectively, the maximum and minimum
eigenvalues of A. Define x(t) = tq1 + (1 −t)qn for 0 ≤t ≤1.
a. Use the Pythagorean theorem to explain why ||x(t)||2 = t2 + (1 −t)2. Explain
why that guarantees that x(t) ̸= 0 for 0 ≤t ≤1.
b. Define a function of a single variable by f(t) ≜RA
$
x(t)
%
. Explain why f(t) is a
continuous function.
c. For a real, symmetric matrix such as A, the interval W = [λn, λ1] is called the
numerical range. Use the Intermediate Value Theorem of Calculus I to explain
why for every number w in the interval W there is a nonzero vector x for which
w = RA(x). This explains why we refer to it as the numerical “range.”
10. Suppose A is a real, symmetric n × n matrix, C is a real, m × n matrix, and B = A+
CTC. Explain why
a. The maximum eigenvalue of B is greater than or equal to the maximum
eigenvalue of A
b. The minimum eigenvalue of B is greater than or equal to the minimum
eigenvalue of A
2.10 Short Take: Inner Product and Hilbert Spaces
Suppose V is a vector space and we have an operation that produces a scalar, ⟨x, y⟩, from
every choice of vectors x, y in V.
Definition 2.24
(a) We say ⟨·, ·⟩is an inner product if it satisfies the properties
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
(i) ⟨x, y⟩= ⟨y, x⟩
(ii) ⟨αx, y⟩= α⟨x, y⟩
(iii) ⟨x1 + x2, y⟩= ⟨x1, y⟩+ ⟨x2, y⟩
(iv) ⟨x, x⟩≥0, with equality only if x = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(2.76)

176
Advanced Engineering Mathematics
for all x, y, x1, x2 in V and all scalars α. In (i), ¯ denotes complex conjugation, as
in Section 2.1.
(b) If ⟨·, ·⟩is an inner product on V then we say (V, ⟨·, ·⟩) is an inner product space,
or “i.p. space” for short. If the scalars are the real numbers, then we call it a
real i.p. space; if the scalars are the complex numbers, then we call it a com-
plex i.p. space. There are other choices for the set of scalars besides the real or
complex numbers, but we won’t need those other choices.
We have already seen examples of an i.p. space in Section 2.3. Because of Theorem 2.12
in Section 2.3, we have:
Example 2.39
$
Rn, ⟨·, ·⟩
%
is a real i.p. space when given the usual inner product ⟨x, y⟩≜xTy. ⃝
We have seen in Example 2.9 in Section 2.1 that eigenvalues and eigenvectors of a real
matrix may be complex. So, we also need the next example.
Example 2.40
$
Cn, ⟨·, ·⟩
%
is a complex i.p. space when given the inner product
⟨x, y⟩≜xT y =
n
/
j=1
xjyj,
where
denotes the complex conjugate. ⃝
Because of Examples 2.39 and 2.40, it is common to define a more universal notation:
xHy ≜
2
xTy,
if V = Rn
xT y,
if V = Cn
3
.
(2.77)
Recall that in Theorem 2.29 in Section 2.6 and Problems 2.6.3.17 and 2.6.3.18, we defined
an alternative inner product on Rn by
⟨x, y⟩W ≜⟨Wx, y⟩= xT WTy.
Example 2.41
If W is a real, positive definite, symmetric matrix, then
$
Rn, ⟨·, ·⟩W
%
is a real i.p. space.
It is because a vector space may have more than one inner product that Definition 2.24(b)
adopted the notation (V, ⟨·, ·⟩) that pairs a vector space with an inner product.

Matrix Theory
177
Example 2.42
Let Pn be the set of all polynomials of degree less than or equal to n with real coefficients,
that is, functions of the form
p(x) = a0 + a1x + · · · + anxn,
where a0, a1, . . . , an are real constants. If p(x), q(x) are in Pn, define
⟨p, q⟩≜
1
−1
p(x)q(x) dx.
(2.78)
Then (Pn, ⟨·, ·⟩) is a real i.p. space.
To be brief and to avoid mathematical reasoning where possible, we will omit the expla-
nation for property (d) in Example 2.42, specifically, why the only real polynomial of
degree less than or equal to n for which 0 = ⟨p, p⟩=
 1
−1 p(x)p(x)dx =
 1
−1 |p(x)|2dx is the
zero polynomial, that is, the polynomial whose coefficients are a0 = a1 = · · · = an = 0.
Properties (a), (b), and (c) follow from linearity of the operation of integration.
2.10.1 Linear Functionals and Operators
Definition 2.25
Suppose V is an i.p. space.
(a) A functional on V is a function f whose inputs are vectors in V and whose outputs
are scalars. The scalars are real (or complex numbers) if the i.p. space V is real (or
complex, respectively). The “machine picture” is in Figure 2.12.
(b) A functional f is linear if
f(αx + βy) = αf(x) + βf(y),
for all vectors x, y and scalars α, β.
Example 2.43
Explain why a linear functional f must have f(0) = 0.
Why? For α = 0, β = 0 and any vectors x, y, linearity of f implies that
f(0) = f(0 · x + 0 · y) = 0 · f(x) + 0 · f(y) = 0 + 0 = 0. ⃝
f
f (x)=scalar
x
FIGURE 2.12
Functional as a machine.

178
Advanced Engineering Mathematics
Example 2.44
Suppose V is an i.p. space and z is a fixed vector in V. Then,
f(x) ≜⟨x, z⟩
defines a linear functional on V.
Why? For all x, y in V and scalars α, β,
f(αx + βy) = ⟨αx + βy, z⟩= ⟨αx, z⟩+ ⟨βy, z⟩= α⟨x, z⟩+ β⟨y, z⟩= αf(x) + βf(y). ⃝
Definition 2.26
Suppose V is an i.p. space. A linear operator on V is a function A whose inputs are vectors
in V and whose outputs are also vectors in V and satisfies
A(αx + βy) = αA(x) + βA(y)
for all vectors x, y and scalars α, β.
Example 2.45
Suppose V is Rn (or Cn), and is given the usual inner product defined in (2.77). If A is
a real (or complex), n × n matrix, then
A(x) ≜Ax
defines a linear operator on Rn (respectively, Cn.)
Because of this example, usually we will write the output of a linear operator on a vector
x as Ax rather than as A(x).
Remark
In Example 2.43 we saw that every linear functional f has f(0) = 0. A similar thing is true
concerning linear operators: if A is any linear operator, then A0 = 0.
Example 2.46
Suppose q is a unit vector in Rn. Then
A(x) ≜

qTx

q
defines a linear operator on Rn.
Why? For all x, y in Rn and scalars α, β,
A
$
αx + βy
%
=

qT $
αx + βy
%
q =

α

qTx

+ β

qTy

q = α

qTx

q + β

qTy

q
= αAx + βAy. ⃝

Matrix Theory
179
Orthogonal projection matrices, P, that we studied in Section 2.4 and used throughout
Chapter 2, are examples of linear operators on Rn.
2.10.2 Norm and Bounded Linear Operators
Suppose (V, ⟨·, ·⟩) is an i.p. space.
Definition 2.27
The corresponding norm on V is given by || x || ≜√⟨x, x⟩.
Theorem 2.46
Suppose (V, ⟨·, ·⟩) is an i.p. space. Then the corresponding norm ||·|| satisfies the properties
in (2.9) in Section 2.3, that is, that for all x, y in V and scalars α,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(a) ||αx|| = |α| || x ||
(b) ||x + y|| ≤|| x || + || y ||
(c) || x || ≥0, with equality only if x = 0
(d) || x ||2 = ⟨x, x⟩
(e) ||x + y||2 = || x ||2 + || y ||2 + 2⟨x, y⟩
(f) |⟨x, y⟩| ≤|| x || || y ||
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(2.79)
Note that Theorem 2.46(f) is the “Cauchy–Schwarz” inequality stated previously as
Theorem 2.13(e) in Section 2.3, that is, (2.9)(e) in Section 2.3.
Example 2.47
If V is Rn (or Cn) and is given the usual inner product ⟨x, y⟩≜
xHy, then the
corresponding norm satisfies
|| x ||2 = xHx =
n
/
j=1
|xj|2.
Example 2.48
For all x, y in an i.p. space,
|| x || −|| y ||
 ≤|| x −y || .
Why? First, note that a ≜|| x ||−|| y || is a real number. If b is a nonnegative real number,
an inequality of the form |a| ≤b is equivalent to a ≤b and −b ≤a. So, we want to explain
both || x || −|| y || ≤|| x −y || and −|| x −y || ≤|| x || −|| y ||.

180
Advanced Engineering Mathematics
First, by the triangle inequality, that is, (2.9)(b) in Section 2.3, with x replaced by x −y,
we have
||x|| = ||(x −y) + y|| ≤||x −y|| + || y ||.
Subtract ||y| from the left- and right-hand sides to get
||x|| −|| y || ≤||x −y||.
(2.80)
Second, in that inequality, swap the roles of x and y to get
||y|| −|| x || ≤||y −x||.
But, using (2.9)(a) in Section 2.3, we have
||y −x|| = ||(−1)(x −y)|| = | −1| ||x −y|| = ||x −y||,
so
||y|| −|| x || ≤||x −y||.
Multiply through by (−1), which changes the direction of the inequality, to get
||x|| −|| y || = (−1)
$
||y|| −|| x ||
%
≥(−1)||x −y||,
that is,
−||x −y|| ≤||x|| −|| y ||.
This and (2.79) explain the result. ⃝
Definition 2.28
Suppose A is a linear operator on an i.p. space (V, ⟨·, ·⟩) and ||·|| denotes the corresponding
norm. A is bounded if there is a constant M ≥0 satisfying
|| Ax || ≤M|| x ||, for all nonzero x in V.
(2.81)
If A is bounded, then the smallest such number M is denoted || A ||, called the (“induced”)
norm of A.
Remark
1. || Ax || ≤|| A || || x ||, for all x in V.
2. || A || = 0 only if A = O, the zero operator that satisfies Ax = 0 for all x in V.
3. || A || is the smallest number M for which it is true that
|| Ax || ≤M|| x ||, for all unit vectors x.

Matrix Theory
181
Theorem 2.47
If A is a real (or complex), n × n matrix, then it is automatically a bounded linear operator
on Rn (or Cn, respectively) and
|| A || ≤|| A ||F ≜
7
8
8
9
n
/
j=1
n
/
k=1
|ajk|2.
(2.82)
|| ||F is called the Frobenius norm.
Theorem 2.48
Give Rn its usual norm || x || =
#
x2
1 + · · · + x2n.
(a) If Q is an n × n real, orthogonal matrix then considered as a linear operator on Rn
it has || Q || = 1.
(b) If P is a nonzero, orthogonal projection on Rn then || P || = 1.
Why? For (a), do Problem 2.4.4.12. For (b), suppose P is any nonzero, orthogonal projec-
tion. First, let us explain why there must exist a nonzero x for which Px = x: because P is
nonzero, there must be at least one y for which Py ̸= 0. After that, define x ≜Py. Because
P is a projection, P2 = P. It follows that
Px = P(Py) = P2y = Py = x.
From this we can conclude M ≥1 in order for (2.81) to be true. Why? Because x = Px
implies that || x || = || Px || ≤M|| x ||, hence M ≥1. It follows that || P ||, the least such M,
must have || P || ≥1.
Second, let us explain why || P || ≤1: for all vectors x in V, x = Px+(I−P)x is a decompo-
sition into a sum or orthogonal vectors, by Theorem 2.17 in Section 2.3. By the Pythagorean
theorem, that is, Theorem 2.15 in Section 2.3,
|| x ||2 = || Px ||2 + || (I −P)x ||2 ≥|| Px ||2 + 0 = || Px ||2
(2.83)
because Theorem 2.46(c) implies that || (I −P)x ||2 ≥0. Take the square root of both sides
of (2.83) to get || x || ≥|| Px ||. Because this is true for all x in V, the number M = 1 makes
(2.81) true. So, P is bounded and has || P || ≤1.
So, || P || ≥1 and || P || ≤1, so we must have || P || = 1. 2
Example 2.49
If A is a bounded linear operator on an i.p. space and has an eigenvalue λ, explain why
|λ| ≤|| A ||.

182
Advanced Engineering Mathematics
Method: Let q be a unit eigenvector of A corresponding to eigenvalue λ, that is, Aq = λq
and || q || = 1. We calculate, using (2.79)(a),
| λ | = | λ | · 1 = | λ | || q || = || λq || = || Aq || ≤|| A || || q || = || A || · 1 = || A ||. ⃝
Example 2.49 is an example of “soft analysis.” It explains why we can still say a lot
in general even if we don’t specify much about the particular space and linear operator.
“Hard analysis” gets into the nitty-gritty details of a particular situation and can often
produce more useful results.
Corollary 2.13
If A is a real, symmetric n × n matrix whose eigenvalues, including multiplicities, are λ1 ≥
λ2 ≥· · · ≥λn, then || A || = max1≤j≤n |λj| = max{|λ1|, |λn|}. Note that λ1 and/or λn may be
negative.
Why? The fact that max1≤j≤n |λj| = max{|λ1|, |λn|} follows from the assumption that
λ1 ≥λ2 ≥· · · ≥λn. Note that λ1 and/or λn may be negative.
By the result of Example 2.49, |λj| ≤|| A || for j = 1, . . . , n. It follows that
|| A || ≥max
1≤j≤n |λj|.
Here’s how to explain why there is equality instead of inequality in the previous line:
Using a corresponding set of eigenvectors {q(1), . . . , q(n)} that is an o.n. basis for Rn, we
can expand any x in Rn as x = c1q(1) + · · · + cnq(n). We calculate, using Parseval’s identity
Corollary 2.6(b) in Section 2.4, that
|| Ax ||2 = ||A(c1q(1) + · · · + cnq(n))||2 = ||λ1c1q(1) + · · · + λncnq(n)||2 =
n
/
i=1
|λici|2.
Use Parseval’s identity a second time to get || x ||2 = 4n
i = 1 |ci|2, so
|| Ax ||2 =
n
/
i=1
|λi|2|ci|2 ≤
n
/
i=1
	
max
1≤i≤n

|λi|

2
|ci|2 =
	
max
1≤i≤n

|λi|

2
n
/
i=1
|ci|2,
(2.84)
hence
|| Ax ||2 ≤
	
max
1≤i≤n

|λi|

2
|| x ||2 .
Take the square root of both sides of (2.84) to conclude that || A || ≤max{|λ1|, |λn|}.
Because || A || ≥max{|λ1|, |λn|} and || A || ≤max{|λ1|, |λn|}, the equality holds and the
desired result follows. 2

Matrix Theory
183
Definition 2.29
Suppose V is an i.p. space and A is a linear operator.
a. We say A is one-to-one if whenever x1 ̸= x2, it follows that Ax1 ̸= Ax2. [Note:
because A is linear, this is the same as saying that the equation Ax = 0 has only the
trivial solution x = 0.]
b. Suppose A is one-to-one. The algebraic inverse of A is the linear operator B
defined implicitly by By =(the unique x such that Ax = y).
c. If the algebraic inverse is also bounded, we write B = A−1 and we say that A is
invertible.
In (2.23) in Section 2.5, we defined the concept of
⊥for a vector subspace of Rn. This
generalizes to any i.p. space.
Definition 2.30
Suppose V is an i.p. space and W is a vector subspace of V. The orthogonal complement
of W is defined by
W⊥≜{x in V : ⟨w, x⟩= 0 for all w in W}.
2.10.3 Convergence, Cauchy Completeness, and Hilbert Spaces
Recall that in Calculus we studied sequences {xk}∞
k = 1 of real numbers and we were particu-
larly interested in convergence of sequences. For example, if xk = 1−10−k for k = 1, 2, 3, . . .,
then we see that the sequence, that is,
.9, .99, .999, .9999, . . .
converges to 1. Sometimes we write this as xk →1, or, more explicitly, as
xk →1 as k →∞.
We are not saying that the sequence ever arrives at 1 but rather that the terms in the
sequence get arbitrarily closer and closer to 1 the further we go along the sequence.
Suppose xk is the diameter, in cm, of ball bearings produced by a machine and that
ideally we want our bearings to have diameter 1 cm. If the machine’s production run
makes bearings whose diameters in cm. are successively,
.9, .99, .999, .9999, . . .
then we would say that after a few bad bearings the machine settles down to produce bet-
ter and better bearings. Define a positive number ε to be an “error tolerance” we demand

184
Advanced Engineering Mathematics
of our product. We want to find a Kε, possibly dependent on ε, so that
|xk −1| < ε,
for all k ≥Kε.
For example, we may choose K.01 = 3 because .999, .9999, . . . are all within .01 of 1. Simi-
larly, K.05 = 3 will work, but K.001 has to be chosen to be at least 4. Note that (Kε −1) may
be the number of bearings rejected by an error tolerance of ε.
Definition 2.31
A sequence

xk
∞
k = 1 in an i.p. space is
a. Bounded if there is an M ≥0 such that ||xk|| ≤M for all k
b. Converges to x∞if for all values of ε > 0, we can choose Kε such that
|| xk −x∞|| < ε, for all k ≥Kε.
If

xk
∞
k = 1, converges to x∞, then we write xk →x∞. Also, we say

xk
∞
k = 1 is convergent if
there exists some x∞to which the sequence converges.
So, convergence of a sequence is the same as our being able to impose an arbitrarily small
error tolerance. Of course, the number of objects rejected may become arbitrarily large as
we impose an arbitrarily fine error tolerance.
In the real world, we don’t demand mathematical convergence of the sequence of bear-
ings produced by an infinite production run, for two reasons: First, we are not interested
in producing an infinite number of bearings, and second we trade off the economic costs
of “achieving” perfection, or getting arbitrarily close to it, versus the goal of producing
reliable, high-quality bearings.
Nevertheless, the mathematical concept is relevant because people design and improve
machines that are capable of greater and greater production and of making finer and finer
quality product. The mathematical, ideal concept orients us to do better and better.
Theorem 2.49
xk →x∞in an i.p. space V if, and only if, the sequence of real numbers {||xk −x∞||}∞
k = 1
converges to 0 in R1.
Example 2.50
If xk →x∞and A is a bounded linear operator, then Axk →Ax∞.
Why? || Axk −Ax∞|| = || A(xk −x∞) || ≤|| A || | xk −x∞|| →0. ⃝

Matrix Theory
185
Definition 2.32
Suppose V is an i.p. space.
a. If S ⊆V, that is, S is a subset of V, and every sequence

xk
∞
k = 1 in S that is
convergent has its limit in S, then we say S is closed.
b. A vector subspace W in V is a closed subspace if W is a closed subset of V.
Related to the definition of convergence is another concept.
Definition 2.33
A sequence

xk
∞
k = 1 satisfies the Cauchy criterion, or is Cauchy, for short, if for all values
of ε > 0 we can choose Kε such that
|| xk −xℓ|| < ε, for all k and ℓ≥Kε.
Theorem 2.50
A sequence of real numbers is convergent if, and only if, it is Cauchy.
Now we have enough background knowledge to see a crucial definition.
Definition 2.34
An i.p. space is Cauchy complete if every Cauchy sequence in the space is convergent. A
Hilbert space is an i.p. space that is Cauchy complete.
Often we use the symbol H to denote a Hilbert space.
Theorem 2.51
Rn and Cn, when given the usual inner product ⟨x, y⟩≜xHy, are Hilbert spaces.
Theorem 2.51 is a generalization of Theorem 2.50.
Unfortunately, not all i.p. spaces are Hilbert spaces. For example, P, the space of all
polynomials when given the inner product in (2.78), is not Cauchy complete because we
can explain why the sequence of polynomials pk(t) ≜4k
j = 0
1
j!tj, the partial sums of the
Maclaurin series for et, is Cauchy but does not converge to a polynomial. Intuitively, an

186
Advanced Engineering Mathematics
i.p. space that is not Cauchy complete has “holes,” for example, we’re missing the place in
P where et would be if it were a polynomial.
2.10.4 Bounded Linear Functionals and Operator Adjoint
Definition 2.35
Suppose f is a linear functional on an i.p. space (V, ⟨·, ·⟩) and ||· || denotes the corresponding
norm. f is bounded if there is a constant M ≥0 satisfying
|f(x)| ≤M|| x ||, for all nonzero x in V.
If f is bounded, then the smallest such number M is called ||f||.
In a real i.p. space, | | denotes absolute value and in a complex i.p. space | | denotes
modulus of a complex number.
Theorem 2.52
(Riesz representation theorem) If f is a bounded linear functional on a Hilbert space H,
then there exists a vector z such that
f(x) = ⟨x, z⟩, for all z in H.
Definition 2.36
Suppose A is a bounded linear operator on a Hilbert space, (H, ⟨·, ·⟩).
a. If there is a linear operator A∗that satisfies
⟨Ax, y⟩= ⟨x, A∗y⟩, for all x, y in V,
then we call A∗the adjoint of A.
b. If A = A∗then we say A is self-adjoint or Hermitian.
Although we won’t really give the explanation here, it is the Riesz representation the-
orem, that is, Theorem 2.52, that explains why the next result is true for any Hilbert
space.
Theorem 2.53
If A is a bounded linear operator on a Hilbert space, then A∗exists.

Matrix Theory
187
Example 2.51
Suppose H = Rn is given the usual inner product xTy. If A is a real, n × n matrix then,
considered as a bounded linear operator on H, A∗= AT.
Why? For all x, y in Rn,
xTATy = (Ax)T y = ⟨Ax, y⟩= ⟨x, A∗y⟩= xTA∗y.
So, we need A∗= AT. ⃝
Example 2.52
Suppose H = Cn is given the usual inner product xHy. If A is a (possibly) complex n × n
matrix then, considered as a bounded linear operator on H, A∗= AT, where
denotes
the complex conjugate.
Why? You will explain this result in Problem 2.10.8.5. ⃝
2.10.5 Application to Signal Restoration
Example 2.53
On the Hilbert space H = Cn with the usual inner product ⟨x, y⟩= xHy = xTy, suppose W
is a Hermitian matrix, that is, satisfies W = W∗. If W is positive definite, that is, ⟨Wx, x⟩>
0 for all x ̸= 0, then
⟨x, y⟩W ≜⟨Wx, y⟩= xT WTy
defines an inner product on H.
This is a generalization of Example 2.41.
One example of a “signal restoration” problem (Cadzow 1997) deals with the equation
x = Aa + w,
where
x is a measurement signal
A is a known m × n complex matrix
w is a Gaussian (random) noise vector. The mathematical problem is to find a vector of
the form Aa, where a is in Cn, so that the squared error functional
(x −Aa)H WT (x −Aa)
is minimized. Here, WT is the matrix inverse of the covariance matrix of w.
This is a generalized least squares problem similar to the one you studied in Problem
2.6.3.18, the only difference being that the vectors and matrices may be complex. Never-
theless, a similar conclusion holds, specifically that a should satisfy the normal equations
A∗WAa = A∗Wx.

188
Advanced Engineering Mathematics
If it happens that A∗WA is invertible, then the unique l.s.s. would be
a = (A∗WA)−1A∗Wx.
This and much further applications of Hilbert space to signal processing are in the article
by Cadzow.
2.10.6 Projection and Minimization
Definition 2.37
A bounded linear operator P on a Hilbert space H is a projection if it satisfies P2 = P = P∗.
Theorem 2.54
Suppose q1, . . . , qn is an o.n. set in a Hilbert space H. For all x in H, define
Px ≜
n
/
j=1
⟨x, qj⟩qj.
Then
a. P is a bounded linear operator and a projection on H
b. For all x, y, ⟨Px, (I −P)y⟩= 0
c. For all x, Px is the vector in W ≜Span{q1, . . . , qn} that is closest to x, that is,
min{||x −v|| : v in W} = ||x −Px||.
Why? (a) Linearity of P follows from properties (ii) and (iii) of Definition 2.24. To explain
why P2 = P, that is, P2x = Px for all x, using linearity we calculate
P2x = P(Px) = P
⎛
⎝
n
/
j=1
⟨x, qj⟩qj
⎞
⎠=
n
/
j=1
⟨x, qj⟩Pqj =
n
/
j=1
⟨x, qj⟩qj = Px,
because for ℓ= 1, 2, . . .
Pqℓ=
n
/
j=1
⟨qℓ, qj⟩qj =
n
/
j=1
δℓjqj = qℓ.
As for explaining why P = P∗, for all x.y, we have
⟨x, P∗y⟩≜⟨Px, y⟩=
0 n
/
j=1
⟨x, qj⟩qj , y
1
=
n
/
j=1
⟨x, qj⟩
+
qj, y
,
,
(2.85)
by definition of P∗.

Matrix Theory
189
Using properties (i) and (ii) of Definition 2.24, we have ⟨v, αw⟩= α⟨v, w⟩, so
⟨x, Py⟩=
0
x,
n
/
j=1
⟨y, qj⟩qj
1
=
n
/
j=1
⟨y, qj⟩⟨x, qj⟩=
n
/
j=1
⟨x, qj⟩⟨qj, y⟩= ⟨Px, y⟩.
(2.86)
Using (2.85) and (2.86), ⟨x, P∗y⟩= ⟨x, Py⟩for all x, y. So, P = P∗.
The boundedness of P follows from the explanation of Theorem 2.48.
(b) The explanation is the same as for Theorem 2.17 in Section 2.3.
(c) The explanation is the same as for the best approximation property of PA
explained in Section 2.5, particularly using (2.24) in Section 2.5. 2
2.10.7 Weak Convergence and Compactness
Definition 2.38
A sequence

xk
∞
k = 1 in a Hilbert space H is weakly convergent if there is some x∞such that
for all fixed y in H, ⟨xk, y⟩→⟨x∞, y⟩,
in which case we write xk ⇀x∞.
In conversational English, we can think of weak convergence as “convergence in every
direction y.”
Theorem 2.55
If xk →x∞then xk ⇀x∞.
Weak convergence really is different from convergence. For example, in Problem
2.10.8.6, you will explain why xk →x∞implies that ||xk|| →||x∞||. Because some Hilbert
spaces have infinite dimension, convergence in every direction does not imply convergence.
A weakly convergent sequence∗could have xk ⇀0, but ||xk|| = 1 for all k = 1, 2, . . . implies

xk
∞
k = 1 cannot converge to 0.
Definition 2.39
If

xk
∞
k = 1 is a sequence, then a subsequence is a partial, ordered list

xkℓ
∞
ℓ= 1 of items
chosen from the original sequence.
∗For example, if

e(k)∞
k = 1 is an o.n. set, let xk = e(k).

190
Advanced Engineering Mathematics
For example, the sequence
1, 1
2, 2, 2
3, 3, 3
4, 4, 4
5, . . .
has a subsequence
1
2, 2
3, 3
4, 4
5, . . .
In this example, the subsequence is convergent even though the original sequence is not.
Definition 2.40
(a) A set S in a Hilbert space is compact if for every sequence in S there is a
convergent subsequence whose limit is in S.
(b) A set S in a Hilbert space is weakly compact if every sequence in S has a weakly
convergent subsequence whose weak limit is in S.
Theorem 2.56
(Banach-Alaoglu)
(a) Suppose H is a Hilbert space and r > 0 is a constant. Then the sets
{x : || x || ≤r} and {x : || x || = r}
are weakly compact.
(b) Every bounded sequence in a Hilbert space has a weakly convergent subsequence.
Theorem 2.56 is powerful, particularly because of the concept defined next and the
theorem that follows.
Definition 2.41
An operator A on a Hilbert space H is compact if for every weakly convergent sequence

xk
∞
k = 1 the sequence {Axk}∞
k = 1 is convergent.
Theorem 2.57
(Spectral theory) Suppose A is a bounded, linear, compact, self-adjoint operator on a
Hilbert space H. Then A has real eigenvalues λk and corresponding eigenvectors qk such

Matrix Theory
191
that {qk} is an o.n. basis for H and for all x in H
Ax =
/
j
λj⟨x, qj⟩qj.
In the summation notation, we just wrote 4
j rather than 4∞
j = 1. This is because a Hilbert
space may be finite dimensional or infinite dimensional. Theorem 2.57 can be considered
as a generalization of Theorem 2.24 in Section 2.6 (Spectral decomposition of a real, sym-
metric matrix) to (possibly) infinite dimensional space. Because of Example 2.50, a linear
operator on Rn is self-adjoint if it corresponds to an n × n real, symmetric matrix.
Learn More About It
Very good expositions of higher mathematics useful for science and engineering,
including Hilbert spaces and the generalization known as Banach spaces, are found in
(1) Elements of Applicable Functional Analysis, by Charles W. Groetsch, Marcel Dekker,
Inc., 1980, and (2) Linear Operator Theory in Engineering and Science, (v. 40 of Applied
Mathematical Sciences), by Arch W. Naylor and George R. Sell, Springer-Verlag, 1982.
2.10.8 Problems
1. Suppose p is a polynomial and define the average value of p by p = 1
2
 1
−1 p(x) dx
and the root mean square of p by prms =
	
1
2
 1
−1
p(x)
2 dx

1/2
. Use the Cauchy–
Schwarz inequality for the inner product defined in (2.78) and the function q(x) ≡
1 to explain why
p
 ≤prms.
2. Suppose A and B are bounded linear operators on an i.p. space V. Define BA by
(BA)(x) ≜B(Ax). You may assume that BA is a linear operator on V. Explain why
BA is also a bounded linear operator and why || BA || ≤|| B || || A ||.
3. Suppose V is an i.p. space and A is a one-to-one, bounded, linear operator. If B is
the algebraic inverse of A and B is also a bounded linear operator, then || B || ≥
(|| A ||)−1. [Hints: Use the fact that BA = I, along with || I || = 1 and the result of
Problem 2.10.8.2.]
4. Suppose V is an i.p. space and A is a one-to-one, bounded linear operator. Define
||| x ||| ≜|| Ax ||. Explain why ||| x ||| also defines a norm on V.
5. Suppose H = Cn is given the usual inner product xHy. If A is a (possibly) complex
n × n matrix then, considered as a bounded linear operator on H, explain why
A∗= AT, where
denotes the complex conjugate.
6. Explain why xk →x∞implies that ||xk|| →||x∞||. [Hint: Use Example 2.48.]
7. Assume xk ⇀x∞and yk →y∞. Explain why the sequence of scalars ⟨xk, yk⟩→
⟨x∞, y∞⟩.
8. Find the values of the constants c1 and c2 so that
 1
−1 |x2−c1−c2x|2dx is minimized.
9. Suppose {u1, . . . , un} is an orthonormal set of vectors in an inner product space
V and λ1, . . . , λn are complex numbers with |λ1| ≥|λ2| ≥. . . ≥|λn|. Define

192
Advanced Engineering Mathematics
an operator A by Ax ≜4n
j = 1 λj⟨x, u(j)⟩u(j). You may assume that A is a linear
operator. Explain why A is bounded and find || A ||.
10. Suppose V is an i.p. space and W1 and W2 are vector subspaces of V with W1 ⊆
W2, that is, every vector w1 in W1 is also in W2. Explain why W⊥
2 ⊆W⊥
1 .
11. Suppose A is a bounded linear operator on a Hilbert space H. Suppose A is
invertible. Explain why
$
A−1%∗= (A∗)−1.
12. Suppose A is a bounded linear operator on a Hilbert space H. Suppose there is a
positive number α such that for all x in H we have || Ax || ≥α|| x ||. Explain why
A is invertible and ||A−1|| ≤α−1.
13. Use the Cauchy–Schwarz inequality to derive Theorem 2.47, that is, || A || ≤
|| A ||F.
14. Consider the functions f0(x) ≡1, f1(x) = x, and f2(x) = x2 defined on the interval
[0, 1]. Define V = Span{ f0(x), f1(x), f2(x)}. Find an orthonormal basis for V consid-
ered as a vector subspace of L2(0, 1) with the scalars being R, with its usual inner
product ⟨f(x), g(x)⟩≜
 1
0 f(x) g(x) dx.
Key Terms
adjoint: Definition 2.36 in Section 2.10
algebraic inverse: Definition 2.29 in Section 2.10
algebraic multiplicity: Definition 2.3 in Section 2.1
algorithm: during explanation of Theorem 2.16 in Section 2.3
best approximation: (2.24) in Section 2.5
bounded linear functional: Definition 2.35 in Section 2.10
bounded linear operator: Definition 2.28 in Section 2.10
bounded sequence: Definition 2.31 in Section 2.10
Cauchy complete: Definition 2.34 in Section 2.10
Cauchy criterion, Cauchy: Definition 2.33 in Section 2.10
Cauchy–Schwarz: Theorem 2.13(e) in Section 2.3
characteristic equation: Definition 2.2 in Section 2.1
characteristic polynomial: Definition 2.2 in Section 2.1
Cholesky factorization: Theorem 2.42 in Section 2.8
closed set, closed subspace: Definition 2.31 in Section 2.10
column space: after Corollary 2.3 in Section 2.4
compact: Definition 2.40 in Section 2.10
compact operator: Definition 2.41 in Section 2.10
complex conjugate: after Example 2.7 in Section 2.1; Figure 2.2
complex i.p. space: Definition 2.24 in Section 2.10
complex number: after Example 2.7 in Section 2.1
sequence converges, convergent: Definition 2.31 in Section 2.10
correlation coefficient: after Theorem 2.14 in Section 2.3
deficient: Definition 2.8 in Section 2.2
degenerate eigenvalue: Definition 2.3 in Section 2.1
diagonalizable, diagonalizes: Definition 2.6 in Section 2.2
eigenspace: Definition 2.5 in Section 2.2
eigenvalue, eigenvector: Definition 2.1 in Section 2.1

Matrix Theory
193
Frobenius norm: Theorem 2.47 in Section 2.10
functional, linear: Definition 2.25 in Section 2.10
generalized eigenvalue problem: Problem 2.1.6.29, (2.43) in Section 2.6
geometric LU factorization: Theorem 2.41 in Section 2.8
geometric multiplicity: Definition 2.4 in Section 2.1
generalized normal equations: Problem 2.6.3.18
generalized, weighted, least squares problem: Problem 2.6.3.18
Gram matrix, Grammian: before Example 2.21 in Section 2.5
Gram–Schmidt process: Theorem 2.16 in Section 2.3
Hermitian: Definition 2.36 in Section 2.10
Hilbert space: Definition 2.34 in Section 2.10
Householder matrix: Problem 2.4.4.17
imaginary part: after Example 2.7 in Section 2.1; Figure 2.2
inertia matrix: after (2.42) in Section 2.6
inner product: Definition 2.9 in Section 2.3, Definition 2.24 in Section 2.10
inner product space: Definition 2.24 in Section 2.10
invertible: Definition 2.29 in Section 2.10
Kronecker delta: after Definition 2.14 in Section 2.3
least squares solution (l.s.s.): after (2.22) in Section 2.5
length: Definition 2.10 in Section 2.3
linear operator: Definition 2.26 in Section 2.10
LU factorization: (2.69) in Section 2.8
magnitude: Definition 2.10 in Section 2.3
mass matrix: after (2.42) in Section 2.6
MATLAB and Mathematica: after Theorem 2.5 in Section 2.1
measurement signal: Example 2.53 in Section 2.10
Moore–Penrose generalized inverse: (2.66) in Section 2.7
norm: Definitions 2.10 in Section 2.3, (2.27) in Section 2.10
normal equations: Theorem 2.22 in Section 2.5
normalized: after Theorem 2.5 in Section 2.1, after Example 2.17 in Section 2.3
numerical range: Problem 2.9.2.9
one-to-one: Definition 2.29 in Section 2.10
orthogonal: Definition 2.12 in Section 2.3
orthogonal complement: Definition 2.30 in Section 2.10
orthogonal matrix: Definition 2.16 in Section 2.4
orthogonal projection: Definition 2.15 in Section 2.3
orthogonal set of vectors: Definition 2.13 in Section 2.3
orthogonally similar: Definition 2.18 in Section 2.6
orthonormal set of vectors: Definition 2.14 in Section 2.3
Parseval identities: Corollary 2.6 in Section 2.4
positive definite: Definition 2.19 in Section 2.6
positive semi-definite: Definition 2.19 in Section 2.6
principal minors: Definition 2.21 in Section 2.6
projection: Definition 2.36 in Section 2.10
pseudo-diagonal: Definition 2.22 in Section 2.7
real: after Example 2.7 in Section 2.1; Figure 2.2
range: after Corollary 2.3 in Section 2.4
Rayleigh quotient: Definition 2.23 in Section 2.9
real i.p. space: Definition 2.24 in Section 2.10

194
Advanced Engineering Mathematics
real part: after Example 2.7 in Section 2.1; Figure 2.2
reduced SVD: (2.56) in Section 2.7
regression line: Example 2.21 in Section 2.5
residual: after (2.22) in Section 2.5
self-adjoint: Definition 2.37 in Section 2.10
similar: Definition 2.7 in Section 2.2
simple eigenvalue: Definition 2.3 in Section 2.1
singular value decomposition (SVD): Theorem 2.37 in Section 2.7
spectral formula, (Spectral decomposition): Theorem 2.24 in Section 2.6
Spectral Theorem, spectra: Theorem 2.23 in Section 2.6
stiffness matrix: after (2.42) in Section 2.6
subsequence: Definition 2.40 in Section 2.10
symmetric: Definition 2.17 in Section 2.6
thin SVD: (2.56) in Section 2.7
triangle inequality: (2.9)(f) in Section 2.3
unit vector: after Example 2.17 in Section 2.3
weakly compact: Definition 2.41 in Section 2.10
weakly convergent: Definition 2.39 in Section 2.10
MATLAB R⃝commands
colspace(A): after Corollary 2.3 in Section 2.4
eig: after Theorem 2.5 in Section 2.1
>[V, D] = eig(A): after Theorem 2.5 in Section 2.1
>format long: after Theorem 2.5 in Section 2.1
Mathematica Commands
Eigenvalues[A],Eigenvectors[A]: after Theorem 2.5 in Section 2.1
Eigenvalues[A,k],Eigenvectors[A,k]: after Theorem 2.5 in Section 2.1
Eigensystem[A],Eigensystem[A,k]: after Theorem 2.5 in section 2.1
RegionPlot3D[ac > 1 && 4a −b2 > 0 && 4ac −4a −4b −b2c > 4, {a, 0, 6}, {b, −4, 3},
{c, 1, 6}]: after Example 2.28 in Section 2.6
FindMaximum[{ f [x,y,z], 1 ≥x ≥−1&&1 ≥y ≥−1 &&1 ≥z ≥−1}, {x,y,z}]: Example
2.37 in Section 2.9
References
Bryan, K. and Leise, T. The $25,000,000,000* eigenvector: The linear algebra behing Google. SIAM
Review, 48, 569–581, 2006.
Cadzow, J.A. Signal restoration, in R.C. Dorf (editor-in-chief), The Electrical Engineering Handbook,
2nd edn. CRC Press/IEEE Press, Boca Raton, FL, 1997, Section 14.4.
Naylor, A.W. and Sell, G.R. Linear Operator Theory in Engineering and Science, Vol. 40, Applied
Mathematical Sciences, Springer-Verlag, New York, 1982.

3
Scalar ODEs I: Homogeneous Problems
3.1 Linear First-Order ODEs
3.1.1 Scalar ODEs
A scalar ordinary differential equation (ODE) is an equation that relates the derivative(s)
of a single function of one variable to possibly the independent variable and the function
itself. For example,
dy
dt (t) = ky(t)
is the ODE for exponential growth or decay that you saw in calculus.
In this section and chapter, we will be most concerned with methods for solving ODEs.
At the end of Section 3.2, we will also present three basic “existential results” that give a
firm foundation for all of the techniques we will learn.
The order of the ODE is the highest derivative of that function in the equation. We will
study first-order ODEs that can be written in the form
dy
dt (t) = f

t, y(t)

,
for some function f. Often, our notation will suppress the dependence on the independent
variable and write the ODEs as
dy
dt = f(t, y).
(3.1)
Having the derivative with respect to t tells us that y is a function of t, so y = y(t) and
˙y = ˙y(t). More generally, ODEs could also take the form F(t, y, ˙y) = 0, that is, involve ˙y
implicitly. For example, we might try to solve (˙y)2 = 2(1 −cos y) after rewriting it in the
forms of ˙y = ±

2(1 −cos y).
Definition 3.1
A solution of an ODE (3.1) is a function y(t) defined on an open interval I for which the
derivative also exists on I and satisfies (3.1) on I.
195

196
Advanced Engineering Mathematics
Open intervals can be of the form (a, b) = {t : a < t < b}, where a < b, (−∞, ∞) =
{t :−∞<t<∞}, (−∞, b) = {t : −∞< t < a}, or (a, ∞) = {t : a<t<∞}.
A solution has to satisfy the ODE on an open interval. A practical way to find the interval
is to substitute the supposed solution into the ODE and during, or after, that, choose the
open interval I. It is usually safe to treat the issue of the open interval as an afterthought
as long as you don’t ignore it completely.
Along with a single first-order differential equation, we may also have an initial con-
dition, for example, y(0) = 1. The combination of a differential equation and an initial
condition is called an initial value problem (IVP). Usually, when solving an IVP, first
we should find all of the solutions of the differential equation and then satisfy the initial
condition by solving for an arbitrary constant.
Example 3.1
Find the solution of the ODE ˙y + 10y = −2 sin 2t that also satisfies the IC y(0) = 1. You
may assume that the ODE has solutions
y(t) = Ce−10t + 1
52 (2 cos 2t −10 sin 2t) ,
for any value of the constant C, as we will derive in Example 3.6.
Method: Substitute the solutions into the IC to get
1 = y(0) = C · 1 + 1
52(2 · 1 −0) = C + 1
26,
so we should choose C = 25
26. The solution of the IVP is
y(t) = 25
26e−10t −1
26 (cos 2t −5 sin 2t) . ⃝
By the way, the ODE in Example 3.1 can come from a series RC circuit with an alternating
current (AC) source, where y(t) is the loop current. You will see an example of this in
Problem 4.1.5.16.
There is an additional notion of a solution of an ODE: An implicit solution is a curve, for
example, x2 + y2 = 4, on which dy
dx = f(x, y) except possibly at finitely many points. If such
a curve passes through a given point (x0, y0), then we say that the implicit solution also
satisfies the initial condition y(x0) = y0. If asked for a solution, we should find a solution
rather than an implicit solution, if at all possible.
3.1.2 Linear First-Order ODEs
The standard form of linear first-order ODEs is
˙y + p(t)y = f(t).
(3.2)
ODE (3.2) is called homogeneous if f(t) ≡0. Physically, this can occur in the simplest
model for friction acting to slow an object, ˙v + pv = 0.

Scalar ODEs I
197
A little algebra may be needed to put a linear first-order ODE into standard form, for
example, the ODE
1
2 t2˙y + ty = e−2t
can be multiplied through by 2
t2 to rewrite it as
˙y + 2
t y = 2
t2 e−2t.
The integrating factor method allows us to find a formula for solutions of ODE (3.2). First,
let’s see how it works in an example.
Example 3.2
Find all solutions of
˙y + 2y = 5e−3t.
(3.3)
Method: If we could somehow combine the two terms on the left-hand side (LHS) of the
ODE, then hopefully we could solve for y. The product rule says that the derivative of
a product yields two terms. As they are now, the two terms on the LHS of (3.3) cannot
be combined to be the derivative of one term. But, if we multiply through (3.3) by e2t,
we have
e2t ˙y + 2e2ty = 5e2te−3t.
(3.4)
The two terms on the LHS add up to d
dt

e2ty

, so (3.4) can be rewritten as
d
dt

e2ty

= 5e2te−3t = 5e−t.
Indefinite integration with respect to t of both sides yields
e2ty = −5e−t + c1,
and then multiply through by e−2t to have
y = −5e−3t + c1e−2t,
where c1 is an arbitrary constant. This gives all∗of the solutions. ⃝
Now let’s return to the standard linear first-order differential (3.2), ˙y + p(t)y = f(t). In
Example 3.2, multiplication of both sides of the ODE by e 2t appeared “out of the blue.”
While the terms ˙y + 2y cannot be combined using the product rule, e 2t ˙y + 2e 2ty can be.
How did we know that multiplying through by e 2t would be so useful?
∗The algebraic steps and the step of integrating neither created spurious solutions nor lost valid solutions, at least
on an interval of t on which e2ty and 5e−t are differentiable and nonzero. This is because of Rolle’s theorem,
which states that dw
dt = 0 is equivalent to w = c1, where c1 is an arbitrary constant.
More generally, in ODE (3.2), p(t) and f(t) being continuous will be useful technical assumptions for finding all
of the solutions.

198
Advanced Engineering Mathematics
The factor e 2t is called an integrating factor. In general, if multiplication by μ(t) of the
two terms on the LHS of (3.2) leads to the two terms being a single derivative, then we say
μ(t) is an integrating factor:
μ(t)
˙y + p(t)y

= μ(t) ˙y + μ(t)p(t)y = d
dt

μ(t)y

.
But, according to the product rule,
d
dt

μ(t)y

= μ(t) ˙y + dμ(t)
dt
y,
so we need dμ
dt = μp(t). This is a differential equation whose solutions are μ(t) = Ce

p(t)dt,
where C is any constant. We could find all such solutions for μ(t), but we need only one
integrating factor,
μ(t) = e

p(t) dt,
(3.5)
to multiply through (3.2) so as to be able to use the product rule. So,
Example 3.3
Find an integrating factor for ˙y + 2y = 5e−3t.
Method: The ODE is in standard form: μ(t) = e

p(t)dt = e

2dt = e 2t is an integrating
factor. ⃝
Example 3.4
Find all solutions of ODE t˙y + 2y = 5e−3t.
Method: To find an integrating factor, first put the PDE into standard form (3.2):
˙y + 2
t y = 5 1
t e−3t.
(3.6)
Now that the equation is in standard form, we can use (3.5):
μ(t) = e

p(t)dt = e
 2
t dt = e2 ln(t) = eln(t2) = t2.
Multiply through (3.6) by μ(t) to get
t2˙y + 2ty = 5te−3t,
use the product rule to rewrite it as
d
dt

t2y

= 5te−3t,
and then integrate both sides with respect to t to get
t2y = 5

te−3t dt.
The latter can be done using the method of integration by parts: Let u = t and dv =
e−3tdt, so du ≜du
dt dt = 1dt = dt and v =

dv =

e−3tdt = −1
3e−3t. So,

Scalar ODEs I
199

te−3tdt =

udv = uv −

vdu = −1
3te−3t −
 
−1
3e−3t

dt.
So,
t2y = 5

−1
3te−3t −1
9e−3t + c1

,
where c1 is an arbitrary constant. So, all solutions of t˙y + 2y = 5e−3t are given by
y = −5
9
	
3t−1 + t−2
e−3t + c1t−2, where c1 is an arbitrary constant. ⃝
Analogous to Definitions 1.17 and 1.19 in Section 1.3 for homogeneous systems of linear
algebraic equations, we have
Definition 3.2
The general solution of a first-order linear homogeneous ODE
˙y + p(t)y = 0
(3.7)
has the form
yh = c1y1(t),
if for every solution y∗(t) of (3.7) there is a value of the scalar c1 giving y∗(t) = c1y1(t). In
this case, we call the set of one function {y1(t)} a complete set of basic solutions, and we
call the function y1(t) a basic solution of (3.7).
Unlike for systems of linear algebraic equations, for first-order linear homogeneous
ODEs, we don’t have a concept of “free variables” or “basic variables.” But also unlike for
systems of linear algebraic equations, we have a nice formula for a basic solution: denote
y1(t) = e−
p(t)dt.
Because we have such a formula for a basic solution, first-order linear homogeneous
ODEs are simpler than systems of linear algebraic equations, at least in this way.
Again, analogous to Definitions 1.21 and 1.22 in Section 1.4 for nonhomogeneous
systems of linear algebraic equations, we have
Definition 3.3
(a) A particular solution of a first-order linear nonhomogeneous ODE (3.2) is any
function yp(t) that satisfies (3.2).
(b) y(t) = yp(t) + c1y1(t) is the general solution of ODE (3.2) if for every solution y∗(t)
of (3.2), there is a value of the scalar c1 giving y∗(t) = yp(t) + c1y1(t).

200
Advanced Engineering Mathematics
Analogous to Theorem 1.18 in Section 1.4, we have
Theorem 3.1
If yp(t) is any particular solution of first-order linear nonhomogeneous ODE (3.2) and y1(t)
is any basic solution of the corresponding first-order linear homogeneous ODE (3.7), then
y(t) = yp(t) + c1y1(t) is the general solution of ODE (3.2).
This result will follow from the theory found at the end of Section 3.2.
The integrating factor method produces in one process all of the solutions, including
both a particular solution and the solutions of the corresponding homogeneous problem.
Additionally, there is an unusually helpful aspect of the method: After multiplying the
standard form of the ODE through by the integrating factor, the product rule acts as an
automatic check that most of the preceding work has been done correctly. I advise you to
always check that the product rule works out in the problems you do. If the product rule
doesn’t seem to be working, then it should act as an alarm bell to alert you to an error
having happened. Unfortunately, like some error messages a computer might give you,
this alarm bell doesn’t tell you specifically what the error was.
The likely sources of error are (1) an algebra error when putting the ODE into standard
form, (2) misidentifying the function p(t), for example, by ignoring a minus sign, (3) an
error in doing the integral

p(t) dt, (4) an error in using the exponential function and pos-
sibly a logarithm, (5) an error in multiplying both sides of the ODE in standard form by
μ(t), (6) an error in doing the integral

μ(t)p(t)dt, and (7) an algebra error in dividing both
sides of μ(t)y = · · · by μ(t).
As we’ve seen, along with a single first-order differential equation, we may also have an
initial condition. When solving an IVP involving a linear differential equation, usually we
should first find all solutions of the differential equation and, only then, satisfy the initial
condition by solving for the arbitrary constant.
Example 3.5
Solve the IVP ˙y +
5
10 −2ty = 4, y(0) = 23.
Method: The ODE is in standard form and has an integrating factor
e

5
10−2t dt = e−5
2 ln |10−2t| = eln

|10−2t|−5/2
= |10 −2t|−5/2.
But, because
|a| =
 a,
if a > 0
−a,
if a < 0

,
we don’t need the absolute value in the integrating factor: μ(t) = (10 −2t)−3/2 will do.∗
(Remember that we only need to find one integrating factor.) We multiply μ(t) through
∗Because we’re solving on an interval containing t = 0 and (10 −2t)−5/2 doesn’t involve the square root of a
negative number near t = 0.

Scalar ODEs I
201
the ODE and use the product rule to get
d
dt

(10 −2t)−5/2y

= (10 −2t)−5/2˙y + (10 −2t)−5/2
5
10 −2t y = 4(10 −2t)−5/2.
Integrate both sides with respect to t to get
(10 −2t)−5/2y =

4(10 −2t)−5/2dt = 4
3 (10 −2t)−3/2 + c1;
hence, all solutions of the ODE are given by
y(t) = 4
3 (10 −2t) + c1(10 −2t)5/2.
Substitute in the initial condition to get
23 = y(0) = 40
3 + c1 · 105/2,
so c1 = 29
3

10−5/2
. The solution of the IVP is
y(t) = 4
3 (10 −2t) + 29
3
	10 −2t
10

5/2
,
that is,
y(t) = 4
3 (10 −2t) + 29
3
	5 −t
5

5/2
. ⃝
Note that this solution only exists on the time interval −∞< t ≤5.
Example 3.6
For ODE ˙y + 10y = −2 sin 2t, find the general solution and a particular solution.
Method: It’s easy to see that e10t is an integrating factor. Multiply through by it to get
d
dt

e10ty

= e10t ˙y + 10e10ty = −2e10t sin 2t,
so the solutions are given by
e10ty = −

2e10t sin 2tdt.
(3.8)
To do this kind of integral, use integration by parts twice followed by the “recursion”
technique, or consult a book of formulas that includes

eat cos bt dt =
eat
a2 + b2 ·

b sin bt + a cos bt

+ c
(3.9)
and

eat sin bt dt =
eat
a2 + b2 ·

a sin bt −b cos bt

+ c,
(3.10)
where c is an arbitrary constant, or use a symbolic manipulation capable calculator or
software package. In our problem here, we have
e10ty = −2

e10t sin 2tdt = −2e10t
104
(10 sin 2t −2 cos 2t) + c1.

202
Advanced Engineering Mathematics
Using this in (3.8), the general solution of the ODE is
y = c1e−10t −1
52 (10 sin 2t −2 cos 2t) .
Since y = c1e−10t is a solution of the corresponding homogeneous ODE,
yp(t) = −1
52 (10 sin 2t −2 cos 2t)
is a particular solution of the original, nonhomogeneous ODE. ⃝
3.1.3 Steady-State and Transient Solutions
We saw in Example 3.6 that the solutions of ˙y + 10y = −2 sin 2t are
y(t) = c1e−10t −1
52 (10 sin 2t −2 cos 2t) .
For any constant c1, we know that limt→∞c1e−10t = 0, that is, c1e−10t →0 as t →∞. This
inspires a definition that is fundamental to engineering:
Definition 3.4
A transient solution of a linear ODE is a function z(t) that (a) is either a particular solution
or a solution of the corresponding homogeneous ODE and (b) has limt→∞z(t) = 0.
Let yT(t) denote a transient solution. Intuitively, a transient solution is that part of a
solution that becomes insignificant eventually.
The definition does not say that a transient solution must be a solution, by itself, of the
ODE. As we will see, often, but not always, a transient solution turns out to be a solution
of the corresponding linear homogeneous ODE. But in Example 4.7 in Section 4.1, we will
see that the transient solution may not be a homogeneous solution. On the other hand, in
Example 3.6, yT(t) = c1e−10t is both a homogeneous solution and a transient solution.
There is another part to the solutions of ˙y + 10y = −2 sin 2t:
yS(t) ≜y(t) −yT(t) = −1
52 (10 sin 2t −2 cos 2t) .
Note that limt→∞yS(t) does not exist, due to oscillation.
Definition 3.5
A function f(t) is bounded as t →∞if there is no sequence of times tn for which
limn→∞| f(tn)| = ∞.
For example, f(t) ≜et cos 2t is not bounded as t →∞because f(nπ) = |enπ cos 2nπ| =
enπ →∞as n →∞.

Scalar ODEs I
203
Definition 3.6
(a) If a solution of a linear ODE can be written as y(t) = yT(t) + yS(t), where
• yT(t) is a transient solution, and
• yS(t) is bounded as t →∞and does not have limt→∞yS(t) = 0,
then we say yS(t) is a steady-state solution.
(b) Alternatively, if a solution y(t) of a linear ODE is bounded as t →∞and does
not have limt →∞y(t) = 0, then we say that y(t) = yS(t) is itself a steady-state
solution.
So, in Example 3.6,
yS(t) = −1
52 (10 sin 2t −2 cos 2t)
is a steady-state solution.
The definition does not say that a steady-state solution must be a solution, by itself, of
the ODE. As we will see, often, but not always, a steady-state solution turns out to be a
particular solution of the linear nonhomogeneous ODE. But in Example 4.7 in Section 4.1,
we will see that the steady-state solution is not a particular solution.
Figure 3.1 illustrates a steady-state solution for the solution of Example 3.6 with c1 = 1.5,
that is, the solution of ˙y + 10y = −2 sin 2t, y(0) = 20
13. Notice that as t increases, the transient
solution, 1.5e−10t, becomes insignificant, leaving the solution, y(t), looking like the steady-
state oscillation yS(t).
Example 3.7
Consider the IVP ˙y = −by+t, y(0) = 3. Let b be an unspecified positive constant. (a) Solve
the IVP, and (b) find the transient solution and the steady-state solution, if they exist.
1.5
y
1.0
0.5
2
4
6
8 t
FIGURE 3.1
Example 3.6.

204
Advanced Engineering Mathematics
Method: (a) The only thing unusual about this example is that we don’t know the value
of the positive constant b. But our usual method works well: first, put the ODE into the
standard form
˙y + by = t.
Using integrating factor μ(t) = e

bdt = ebt, we have
d
dt

ebty

= ebt ˙y + by

= tebt,
so the solutions are given implicitly by
ebty =

tebtdt = t · ebt
b −
 ebt
b dt = ebt
b2 (bt −1) + c1.
Multiplying through by e−bt, we find that the solutions are
y = c1e−bt + bt −1
b2
,
where c1 is an arbitrary constant.
(b) By definition, yT ≜c1e−bt is a transient solution because (a) it is a solution of the cor-
responding homogeneous ODE and (b) limt→∞c1e−bt = 0 because the problem assumed
b > 0. This agrees with our intuition that a transient solution should be that part of
the solution that becomes insignificant eventually. But there is no steady-state solution
in this example, because w(t) ≜y(t) −yT = bt−1
b2
is not bounded as t →∞, because we
assumed that b is nonzero. ⃝
Example 3.8
Find a formula for the solutions of ˙y + p(t)y = f(t). (Such a formula may also be called a
“closed form” solution.)
Method: Because we were not given formulas for p(t) and f(t), the best that we can hope
for is a formula in terms of an integrating factor μ(t). Multiplying through ˙y+p(t)y = f(t)
by μ(t), we have
μ(t)˙y + p(t)μ(t)y = μ(t)f(t).
Because μ(t) satisfies ˙μ(t) = p(t)μ(t), the product rule explains why this ODE is
equivalent to
d
dt[μ(t)y] = μ(t) f(t).
So, either
y(t) =
1
μ(t)
	
c1 +

μ(t)f(t) dt

or
y(t) =
1
μ(t)
⎛
⎝c1 +
t
t0
μ(s)f(s) ds
⎞
⎠,
where t0 is a constant, is a formula for all solutions of ˙y + p(t)y = f(t), where c1 is an
arbitrary constant. ⃝

Scalar ODEs I
205
3.1.4 Problems
In problems 1–4, find all solutions of the ODE.
1. ˙y + y = e−2t
2. t˙y + y = e−2t
3. t˙y −3y = t4
4. ˙y −
1
t+1y = (t2 −t −2)
5. Suppose y(t) = t2 ln(t) solves t˙y −2y = t2. Solve the IVP t˙y −2y = t2, y(1) = 3.
In problems 6–17, solve the IVP.
6. ˙y + 3y = e−t, y(0) = −1
7. t˙y + 3y = e−2t
t2 , y(1) = −1
8. x dy
dx −3y = x4, y(1) = −1
9. t˙y + (t −1)y + t2 = 0, y(1) = −2
10. t˙y = −y −3t, y(1) = 1
11. ˙A = 4 −
6A
100−2t, A(0) = 10
12. ˙y + 3y = 5te−t, y(0) = 1
13. ˙y = −3y + 2t, y(1) = 0
14. ˙y = 5t −3
t y, y(2) = −4
15. ˙y +
2t
1+t2 y = 1
t , y(2) = −1
16. t˙y + y = 4, y(1) = 3
17. ˙y + t+1
t y = 1
t e−2t, y(1) = 0
In problems 18–21, find the steady-state solution.
18. ˙y + y
t = cos t, y( π
2 ) = 0
19. ˙y = −y + sin t, y(0) = 1
20. ˙y = −1
2 y −1 + 2 sin t, y(0) = 0
21. ˙y + y = cos 2t, y(0) = 0
22. Let α be an unspecified parameter. Find the solution of the IVP ˙y−αy = t, y(0) = 3.
Your final conclusion(s) should be in terms of α. [Caution: α = 0 may be a special
case.]
23. Let α be an unspecified positive parameter. For the IVP ˙y + αy = 2, y(0) = 1,
(a) Find the solution, in terms of α.
(b) Find an approximate value(s) of α for which y(1) = 2. Because y(1) is a func-
tion of α, setting 2 equal to y(1) could give a difficult equation to be solved
for α. Technology, for example, a graphing calculator, may be useful for that.
24. The loop current I in a series RL circuit with constant voltage source E0 satisfies
L˙I + RI = E0, by Kirchhoff’s voltage law. Assume that R and L are, as usual,
constants. Assume that initially there is no current in the circuit. Find the current

206
Advanced Engineering Mathematics
as a function of time, and find the steady-state solution. When will the current be
(1 −e−1) times the steady-state current? [This time is called the “rise time.”]
Newton’s law of cooling says that a hot object in cool surroundings will lose heat
at a rate proportional to the difference between the temperature of the object and
the medium that surrounds it. Assuming the specific heat is roughly constant
during this process, the temperature of the object, T, satisfies an ODE of the form
˙T = −α(T −M),
where
M is the constant temperature of the medium
α is a positive constant
25. At 1:00 pm. a hot object was brought into a room whose temperature is kept at a
constant 20◦C. Suppose that the object’s temperature was 250◦C at 1:03 pm. and
200◦C at 1:04 pm. What was the object’s temperature at 1:00 pm.?
26. When a cake was removed from an oven, the cake’s temperature was 360◦F.
Fifteen minutes later, its temperature was 80◦F. The kitchen’s temperature is a
constant 65◦F. When was the cake’s temperature 100◦F?
27. [CSI-ODE] Suppose a coroner found the temperature of a dead person was 34.8◦C
at 11 am. and was 34.3◦C a half an hour later. The police noted the room’s tem-
perature seemed to be a constant 21.1◦C. Give an estimate for the person’s time of
death, assuming that living people usually have a temperature of about 36.95◦C.
28. The situation is basically the same as in Problem 3.1.4.27 except that we note
that living people usually have a temperature that lies in the interval of about
36.6◦C–37.2◦C, that is, 36.95◦C, is not completely reliable. Give an interval esti-
mate for the person’s time of death, for example, something such as “The person
died sometime between about 9:05 am and about 9:55 am.”
29. Suppose that a 0.5 kg falling object experiences an air resistance force whose mag-
nitude in newtons is 4 times its speed in m/s. The object is released from rest, and
assume g ≈9.81 m/s2. Be careful to state a physical coordinate system for this
situation and then write down an IVP for the velocity and solve it. What is the
steady-state velocity?
30. [Redheffer and Port] Suppose that you’re in a canoe that passed the finish line in a
race at time t = 0. Although you stopped paddling, your canoe continues drifting
forward, going a distance yT by time t = T and a total distance y2T by time t = 2T.
Assuming that the water exerts a resistive force proportional to the speed of the
canoe but that there is no wind or water current, explain why if you wait long
enough the position of the canoe will be arbitrarily close to the number
y∞=
y2
T
2yT −y2T
.
31. Suppose that the number of acres occupied by a certain plant satisfies two assump-
tions: (i) Goats are consuming the plant at a rate of 10 acres per year, and (ii) in

Scalar ODEs I
207
b
G
FIGURE 3.2
Problem 3.1.4.32.
the absence of goats, the acreage occupied by the plant would increase at a rate
proportional to the current acreage.
(a) Write down an ODE to model this situation. Carefully define your variables.
(b) Find all solutions of the ODE that you wrote in part (a).
32. Glucose is dripping into the bloodstream of a patient at a constant rate of b g/m.
At the same time, the patient’s body uses up its glucose at a rate proportional
to the amount of glucose in the bloodstream, with constant of proportionality k.
An illustration of this situation is shown in Figure 3.2. We call this situation a
one-compartment model.
Define your variables and set up an ODE for the amount of glucose in the
patient’s bloodstream.
Assuming that initially there is no glucose in the patient, find the steady-state
amount of glucose in the patient’s bloodstream, in terms of b and k. How long will
it take for the amount of glucose in the bloodstream to reach (1 −e−5) times the
steady-state amount? Again, your conclusion should be in terms of k and b.
33. A rocket burns fuel to generate thrust and this also causes the rocket to continu-
ally decrease in mass. Newton’s second law of motion says that d
dt[mv] = Forces,
where m is the mass of the rocket and v is the velocity of the rocket, assuming its
motion is along a straight line. From this we can explain why the ODE describ-
ing the motion of the rocket motion is −mg = m˙v + u ˙m, where u is a constant, and
we assume the gravitational force is constant and there are no resistive forces.
Assume v(0) = v0, g = 32 ft/s2, m = m0

1 −
t
200

, 0 ≤t ≤190, and m0 is a constant.
Find the velocity as a function of time. What is the velocity when the rocket stops
burning, that is, when t = 190 s, assuming the mass of the payload is negligible?
34. Table 3.1 has hypothetical data for the temperature, T, of an object.
(a) Which of the following models is better for this data? Why?
Model #1: ˙T = −αT
Model #2: ˙T = −α(T −M)
TABLE 3.1
Choosing a Model
Time (s)
0
20
40
60
80
100
120
180
300
Temperature (◦C) 140
80
50
45
35
33
32
30
28

208
Advanced Engineering Mathematics
(b) For the model you chose in part (a), estimate the numerical values of the
physical parameters. Include the units in your conclusions.
Suppose we can find a formula for the indefinite integral for μ(t), but we can’t
find a formula for the indefinite integral

μ(t)p(t) dt. Then, in the method of inte-
grating factor, instead of doing an indefinite integral of both sides of d
dt

μ y

=
μ(t)f(t), we could do a definite integral of both sides with respect to t. Note that
d
dt[z] = f(t) is equivalent to z(t) = z(0) +
 t
0 f(s) ds.
35. Solve the IVP ˙y + 2ty = 1, y(0) = 3.
36. Solve the IVP y′ = −3x2y −2x, y(1) = 0, where ′ = d
dx.
Suppose there is no explicit formula for

p(t)dt. That would seem to doom
finding an integrating factor, but this is not true. We want to find an integrating
factor, μ(t), that is, a function that satisfies the differential equation dμ
dt = μp(t).
Separation of variables gives us
dμ
μ = p(t) dt.
Instead of doing an indefinite integration of both sides of this, do a definite
integration: For example,
μ
1
dν
ν =
t
0
p(s) ds,
that is,
ln |μ| −ln |1| =
t
0
p(s) ds.
Raise e to both sides of
ln |μ| −0 =
t
0
p(s) ds
to get |μ| = e
 t
0 p(s)ds; hence, ±μ = e
 t
0 p(s)ds. Since we need only one integrating
factor, let
μ = e
 t
0 p(s)ds.
37. Find all solutions of the ODE ˙y + et2y = 1.

Scalar ODEs I
209
3.2 Separable and Exact ODEs
3.2.1 Separable ODEs
An ODE that can be written in the form
dy
dt = f(t)g(y),
(3.11)
is called separable. Usually, we can use integrals to find most of the solutions of an
ODE (3.11).
Example 3.9
Find all solutions of dy
dt = 2ty2
t2 + 1.
Method: Recall from Calculus I that the differential dy is defined by dy ≜dy
dt dt. It follows
that dy
dt can be written as the ratio of differentials:
dy
dt =
dy
dt
.
So, we can rewrite the ODE as
dy
dt
= y2
2t
t2 + 1.
If y ̸= 0, divide both sides by y2 and multiply both sides by dt to get
dy
y2 =
2t
t2 + 1dt.
Now we see why it is called the ODE “separable”: All of the y dependence, including
the dy, is on one side, and all of the t dependence, including the dt, is on the other side of
the equation. [By the way, the use of differentials is a historical reason why we call this
subject “differential” equations rather than “derivative” equations.]
We will see later that it can be justified to integrate both sides of this differential
equation:
 dy
y2 =

2t
t2 + 1 dt.
This gives
−y−1 + c = ln |t2 + 1| + ˜c,
that is,
y =
−1
ln(t2 + 1) + c1
, where −∞< c1 < ∞.
Even though there appeared to be two arbitrary constants c and ˜c, they can be combined
into a single arbitrary constant, c1 = ˜c −c. In future problems, we will use a single

210
Advanced Engineering Mathematics
arbitrary constant rather than having to “reinvent the wheel” in the future by combining
two arbitrary constants each time.
But did we find all of the solutions? No. Recall that one of the algebraic steps was
enabled by “If y ̸= 0...” What if y = 0? Then we have to start from scratch and find
another solution technique. But this turns out to be even easier than the many steps of
algebra and calculus we had to do to get the aforementioned solutions involving c1.
If y = 0, the right-hand side (RHS) of the differential equation becomes ˙y = 02
2t
t2+1 ≡0,
and that is solvable by letting y(t) ≡0, that is, y(t) is identically equal to 0, that is, y(t) = 0
for all t. In fact, this completes our work, that is, we have found all of the solutions:
⎧
⎨
⎩
y(t) ≡0
and
y(t) = −

ln |t2 + 1| + c1
−1 , where c1 is an arbitrary constant
⎫
⎬
⎭. ⃝
(3.12)
We will see why it makes sense to integrate both sides of dy
g(y) = f(t) dt to get solutions
satisfying

dy
g(y) =

f(t) dt.
It seems very strange to equate two integrals, one integrating with respect to y and the
other integrating with respect to t, but we will explain why it makes sense:
Theorem 3.2
Suppose y(t) is a solution of (3.11), that is, ˙y = f(t)g(y), on some open interval I = (a, b)
and g(y(t)) ̸= 0 for all t in I. It follows that

dy
g(y) =

f(t) dt.
Why? Because g(y(t)) ̸= 0 for all t in I, divide dy
dt (t) = f(t)g(y(t)) through by g(y(t)) to get
1
g(y(t))
dy
dt (t) = f(t). Next, the indefinite integral with respect to t of both sides gives

1
g(y(t))
dy
dt (t)dt =

f(t) dt.
According to the method of substitution, the LHS of the preceding equation is

1
g(y(t))
dy
dt (t) dt =

1
g(y)dy,
so the result follows. 2
Theorem 3.2 does not address the case where a solution y(t) has g(y(t)) = 0 for some
t in I. As we saw in Example 3.9, we can have constant solution(s) that is not studied in

Scalar ODEs I
211
Theorem 3.2. In general, if for some constant c, y(t) ≡c solves ODE (3.11), then the ODE
says that 0 ≡dy
dt = f(t)g(y(t)) ≡f(t)g(c), for all t in I. Unless f(t) ≡0 on interval I, we need
to have g(c) = 0. [And if f(t) ≡0 on I, then our differential equation is dy
dt = f(t)g(y(t)) ≡
0 · g(y(t)) ≡0 on I, which again says that y(t) ≡constant on I.]
Example 3.10
Find all constant solutions of ˙y = 2t(y2 −3y).
Method: If y(t) ≡c is a constant solution, then 0 ≡˙y = 2t(c2 −3c). It follows that
0 = c2 −3c, that is, 0 = c(c −3). The constant solutions are y(t) ≡0 and y(t) ≡3. ⃝
3.2.2 Exact ODEs
Definition 3.7
An ODE in the form
M(t, y) + N(t, y)dy
dt = 0
(3.13)
is called exact if there is a continuously differentiable function φ(t, y) satisfying
M(t, y) = ∂φ
∂t
and
N(t, y) = ∂φ
∂y .
(3.14)
Example 3.11
Verify that the ODE
2t cos y −t2 sin y dy
dt = 0
(3.15)
is exact.
Method:
The ODE is exact because φ(t, y) ≜t2 cos y satisfies
2t cos y = ∂
∂t

t2 cos y

and −t2 sin y = ∂
∂y

t2 cos y

. ⃝
Recall that when taking the partial derivative with respect to t, we treat y as if it were a
constant, and similarly, when taking the partial derivative with respect to y, we treat t as
if it were a constant.
Is having such a function φ a curiosity, or is it useful in a general way?
Theorem 3.3
If ODE (3.13) is exact, then the curves φ(t, y) = C satisfy the ODE.

212
Advanced Engineering Mathematics
Why? First, here’s a fact about partial derivatives: If y = y(t) is differentiable, then
d
dt[ φ(t, y(t)) ] = ∂φ
∂t (t, y(t)) + ∂φ
∂y (t, y(t)) · dy
dt (t).
(3.16)
This chain rule fact makes sense because, if we define △y ≜y(t+ △t) −y(t),
lim
△t→0
φ(t+ △t, y(t+ △t)) −φ(t, y(t))
△t
= lim
△t→0
φ(t+ △t, y(t+ △t)) −φ(t, y(t+ △t))
△t
+ lim
△t→0
φ(t, y(t+ △t)) −φ(t, y(t))
△t
= lim
△t→0
φ(t+ △t, y(t+ △t)) −φ(t, y(t+ △t))
△t
+

lim
△y→0
φ(t, y(t)+ △y) −φ(t, y(t))
△y

lim
△t→0
△y
△t

= ∂φ
∂t (t, y(t)) + ∂φ
∂y (t, y(t)) · dy
dt (t).
So, using that fact and the exactness, we can rewrite the ODE as
0 = M(t, y(t)) + N(t, y(t))dy
dt (t) = ∂φ
∂t (t, y(t)) + ∂φ
∂y (t, y(t))dy
dt (t) = d
dt[ φ(t, y(t))].
Taking the indefinite integral of both sides with respect to t gives C = φ(t, y(t)), where C is
an arbitrary constant. 2
For example, in Example 3.11, the curves
t2 cos y = C
satisfy ODE (3.15). Indeed, ODE (3.15) is also separable, and the method of separation
of variables gives the same solution curves t2 cos y = C. In fact, as you will explain in
Problem 3.2.4.22, all separable ODEs can be rewritten in a form that is exact.
If an ODE M(t, y) + N(t, y) dy
dt = 0 is exact, then Clairaut’s theorem says we must have
∂
∂y

M(t, y)

= ∂
∂y
∂φ
∂t

= ∂2φ
∂y∂t = ∂2φ
∂t∂y = ∂
∂t
∂φ
∂y

= ∂
∂t

N(t, y)

.
Theorem 3.4
If ODE (3.13) is exact, then it must be true that
∂
∂y

M(t, y)

= ∂
∂t

N(t, y)

,
(3.17)
which is called the exactness criterion.

Scalar ODEs I
213
The theorem follows as earlier from Clairaut’s theorem. The exactness criterion is a neces-
sary condition because if ODE (3.13) is exact, then necessarily the ODE satisfies the exactness
criterion. On the other hand, if M and N satisfy the exactness criterion in a nice region in the
ty-plane, why is there a function φ as desired by the definition of “exact ODE?” An expla-
nation for that would involve mathematics that is more advanced than we care to give
here. But this result is related to “potential flow” for ideal fluids, existence of potentials
in multivariable calculus, and existence of a “harmonic conjugate” in complex variable
theory. So, even though an explanation involves more advanced mathematics, the result
being explained may be of great significance to engineers.
Example 3.12
Decide whether the ODE as follows is exact, and if so,
(a) Find all solutions
(b) Find the solution passing through the point (1, 2) in the ty-plane:
y3 −2t2 +
	
3ty2 + y

 dy
dt = 0.
(3.18)
Method: (a) First, we check the exactness criterion. [If this fails, then the ODE is not exact
and there is nothing left to do in the problem because of the “if so.” If the instructions
said to solve under all circumstances, then we would have to find some other method!]
3y2 = ∂
∂y

y3 −2t2 
= ∂
∂y

M(t, y)

=? ∂
∂t

N(t, y)

= ∂
∂t

3ty2 + y

= 3y2,
so, yes, ODE (3.18) is exact.
We want to find a “potential function” φ(t, y) satisfying (3.14). So, we need to have
y3 −2t2 = M(t, y) = ∂
∂t

φ(t, y)

;
hence,
φ(t, y) =
 	
y3 −2t2
∂t = ty3 −2
3t3 + f(y),
where f( y) is an arbitrary function of only y. Our symbol

...∂t is shorthand for the
operation of anti-partial differentiation with respect to t.
The reason we have an arbitrary function f(y) instead of an arbitrary constant is
because ∂
∂t[f(y)] ≡0. Note also that because f(y) is a function of y alone, ∂
∂y[f(y)] = df
dy.
From (3.14), φ(t, y) must also satisfy
	
3ty2 + y

= N(t, y) = ∂
∂y[φ(t, y)] = ∂
∂y

ty3 −2
3t3 + f(y)

= 3ty2 + df
dy,
so
y = df
dy.
We have f(y) = 1
2 y2; we could add an arbitrary constant, but it would turn out to be
redundant because our solutions are the curves φ(t, y) = C.
Putting everything together, we have that the solutions of ODE (3.18) are the curves
C = φ(t, y) = ty3 −2
3t3 + 1
2 y2,
where C is an arbitrary constant.

214
Advanced Engineering Mathematics
(b) Saying the curve passes through the point (1, 2) in the ty-plane means that at t = 1,
y = 2. We can satisfy this initial condition by solving for C:
C =

ty3 −2
3t3 + 1
2 y2
 
(t,y)=(1,2) = 8 −2
3 + 2.
The solution of the IVP is the curve
28
3 = ty3 −2
3t3 + 1
2 y2. ⃝
While it would be nice to find an explicit solution, that is, y given as an explicit function
of t, and while it would be possible by using formulas for the roots of a cubic equation, we
will skip that here because the formula is a little complicated and is not very informative.
Instead, we used MathematicaTM to get a graph, by using the command
ContourPlot

ty3 −2
3t3 + 1
2y2, {t, −4, 2},{y, −10, 10}, Contours →
28
3

,
PerformanceGoal →′′Quality′′, ContourShading →False

to get Figure 3.3. The lower branch of the curve is not part of the solution of the IVP because
it doesn’t pass through the point (1, 2). We also plotted a zoomed-out view, in Figure 3.3b,
to feel more confident that the two branches don’t connect in the ty-plane.
Caution:
The solutions of an exact ODE are the curves φ(t, y) = C, where C is an
(arbitrary) constant, not the function φ(t, y).
40
20
y
0
0
20
40
t
t
–20
–20
–40
10
5
y
0
–5
–4
–3
–2
–1
0
1
2
–10
–40
(a)
(b)
FIGURE 3.3
Example 3.12: (a) solutions curves and (b) zoomed out view of solution curves.

Scalar ODEs I
215
3.2.3 Existence of Solution(s) of an IVP
Unlike a system of linear algebraic equations, it is not obvious how to decide whether a
given IVP has a solution or how to find the solution(s). We will explain some of the basic
results on existence of solution(s) and present some examples. Unfortunately, the expla-
nations for the theorems are more mathematically advanced than we choose to give, but
we will refer the reader to other books or articles. Once again, even though an explanation
involves a lot of pure mathematics, the results being explained are of great significance to
engineers. While an engineer might use a numerical method (see Chapter 8) to approx-
imate the solution of an IVP, it would be good to know that there is a solution to be
approximated!
Theorem 3.5
(Peano’s existence theorem) The IVP
⎧
⎪⎨
⎪⎩
dy
dt = f(t, y)
y(t0) = y0
⎫
⎪⎬
⎪⎭
(3.19)
has at least one solution as long as f(t, y) is continuous on a closed rectangle
Rα,β ≜{(t, y): t0 −α ≤t ≤t0 + α, y0 −β ≤y ≤y0 + β},
for some positive scalars α, β.
As far as it goes, this is a good result. However, it doesn’t say how many solutions
there are for the IVP. In science and engineering, we prefer that a mathematical model of a
physical system should give only one prediction about future behavior. Also, the theorem
does not tell us for how long a time interval the solution exists. If it turns out that the
solution only exists for 10−40 s in the future, that would probably not be very useful for
making predictions in a physical problem.
Example 3.13
For the IVP
⎧
⎪⎨
⎪⎩
dy
dt = −3
2 y1/3
y(1) = 0
⎫
⎪⎬
⎪⎭
,
(3.20)
(a) explain why the method of separation of variables fails because, by itself, it produces
no solution, and (b) explain why there are infinitely many solutions.
Method: (a) Separation of variables gives

dy
y1/3 = −
 3
2 dt,

216
Advanced Engineering Mathematics
so it seems that the solutions are given by
3
2 y2/3 = −3
2 t + c,
where c is an arbitrary constant. Multiplying through by 2
3 and renaming ¯c = 2
3c, we have
y2/3(t) = −t + c.
The initial condition gives 0 = y(1) = −1 + ¯c, so we have ¯c = 1. We get
y2/3(t) = 1 −t.
(3.21)
Now, because

y(t)
2/3 =
	
y(t)
1/3
2
≥0, the implicit solution given by (3.21) exists
only for t ≤1. But according to our definition in Section 3.1, a solution is a function y(t)
defined on an open interval I that satisfies the ODE on I and the IC y(1) = 0. So, a solu-
tion has to be defined on an open interval I containing t = 1 inside. Possible intervals I
are (a, b), where a < 1 < b, or (−∞, b), where b > 1, or (a, ∞), where a < 1, or (−∞, ∞).
But our alleged solution doesn’t exist for t > 1. A solution of the IVP has to exist on
an open interval containing t = 1, so we reach a contradiction. The method of separation
of variables, by itself, doesn’t produce a solution!
This problem seems to be a bit “unfair” or “tricky.” And it seems at first as if the
difficulty is just that mathematicians messed up the definition of “solution.” But the
subject would be meaningless without a serious definition of “solution,” and there is
a very good reason to require solutions to exist both forward and backward in time:
Engineering applications of physics should be able to predict what will happen in the
future. Even “predicting” what happened in the past can be difficult.
But once you see the solution for part (b), it will appear to be more reasonable.
Mathematics is like that sometimes.∗
(b) The only problem with what we got from separation of variables, y(t) = (1 −t)3/2,
is that it doesn’t exist for t > 1. The trick is to find some way to “extend” the solution to
the right of t = 1. Here’s the inspiration: The initial condition is y(1) = 0, and y(t) ≡0 is
a constant solution of this ODE. Let’s define y(t) = 0 for t > 1, that is,
y(t) =
⎧
⎨
⎩
(1 −t)3/2 ,
t < 1
0,
t ≥1
⎫
⎬
⎭.
(3.22)
This is, indeed, an honest to goodness solution. Not only that, but (3.22) can be
generalized to give infinitely many solutions, one for every value of the constant γ ≤1:
y(t) =
⎧
⎨
⎩
(γ −t)3/2 ,
t < γ
0,
t ≥γ
⎫
⎬
⎭.
∗Here’s an apocryphal story about the American eccentric mathematician Norbert Weiner, who invented cyber-
netics: Once, during a lecture on the Fourier transform, a subject for which he made fundamental contributions,
he was asked a question. He silently pondered it for a few minutes, abruptly left the lecture hall, and reappeared
about an hour later. He said, “It’s obvious,” without further comment on it, and then resumed his lecture at
the point where he had been interrupted. Apparently, after one of the greatest minds of the twentieth century
had thought about it for an hour, he arrived at a conclusion but decided it was so trivial as to not be worth
mentioning.

Scalar ODEs I
217
y(t)
t
FIGURE 3.4
Example 3.13.
A typical solution graph is shown in Figure 3.4. This y(t) is differentiable every-
where, with
dy
dt (t) =
⎧
⎨
⎩
−3
2 (γ −t)1/2 ,
t < γ
0,
t ≥γ
⎫
⎬
⎭,
which does equal the RHS of the ODE,
−3
2 y(t)1/3 = −3
2 ·
⎧
⎪⎨
⎪⎩
	
(t −γ )3/2
1/3
,
t < γ
0,
t ≥γ
⎫
⎪⎬
⎪⎭
=
⎧
⎨
⎩
−3
2 · (t −γ )1/2 ,
t < γ
0,
t ≥γ
⎫
⎬
⎭,
for all t.
In addition, there is a constant solution y(t) ≡0. ⃝
Theorem 3.6
(Picard’s existence and uniqueness theorem) Suppose that both f(t, y) and ∂f
∂y(t, y) are con-
tinuous on a closed rectangle Rα,β, for some positive scalars α, β. Then there is an open
time interval containing t0 on which IVP (3.19) has exactly one solution.
This version of Picard’s existence theorem addresses the “future predictions” issue but
at the cost of demanding more “smoothness” of the function f(t, y) on the RHS of the ODE,
compared to Theorem 3.5.
Theorem 3.7
(Picard’s theorem with interval of existence) Suppose that both f(t, y) and ∂f
∂y(t, y) are con-
tinuous on a closed rectangle Rα,β ≜{(t, y) : t0 −α ≤t ≤t0 + α, y0 −β ≤y ≤y0 + β},
for some positive scalars α, β and where t0 and y0 are the same as in the initial condition

218
Advanced Engineering Mathematics
y(t0) = y0. Suppose that there are positive constants M and K such that for all (t, y) in Rα,β,
we have
|f(t, y)| ≤M and

∂f
∂y(t, y)
 ≤K.
If we choose ¯α and ¯β sufficiently small that
0 < ¯α ≤α, 0 < ¯β ≤β, M¯α ≤¯β, and K ¯α < 1,
then IVP (3.19) has exactly one solution on the time interval I¯α ≜[t0 −¯α, t0 + ¯α] and the
points

t, y(t)

remain in the closed rectangle R¯α, ¯β for all t in the interval I¯α.
Theorem 3.7 adds more information beyond the result of Theorem 3.6 by giving a
specific time interval of existence. Most books don’t explicitly state Theorem 3.7 but do
include that information in their explanations for Theorem 3.6. Theorem 3.7 is illustrated
in Figure 3.5.
The condition that M¯α ≤¯β has a good physical interpretation:
dy
dt
=| f

t, y(t)

| is the
speed of an object whose position, y(t), satisfies ODE (3.19), so M is an upper bound on
that speed. So, M¯α ≤¯β says, by distance = speed × time, that the object can’t go further
away from y(t0) than a distance of ¯β, that is, |y(t) −y0| ≤¯β. So, M¯α ≤¯β implies that the
graph of y(t) versus t stays inside the rectangle R¯α, ¯β.
Both Peano’s existence Theorem 3.5 and Picard’s existence and uniqueness Theorem 3.6
have what are known as “sufficient conditions.” What this means is that, for example, in
Peano’s theorem, “if f(t, y) is continuous...then the IVP has a solution” says that if we can
explain why f(t, y) is continuous, then that will be sufficient to get the conclusion that the
IVP has a solution.
But a sufficient condition may not be “necessary.”
y0+ β
y0+ β–
–
y0– β
(t0, y0)
Rα, β
y
y0– β
t
t0–α
–
t0–α
–
t0+ α
t0+ α
FIGURE 3.5
Picard’s Theorem 3.7.

Scalar ODEs I
219
Example 3.14
Explain why the IVP ˙y = y
t , y(0) = 0 has a solution even though f(t, y) is not continuous.
Method: In fact, separation of variables produces infinitely many solutions y(t) = Kt,
for any constant K, even though f(t, y) = y
t is continuous on no rectangle that contains
(t0, y0) = (0, 0) strictly inside. ⃝
Learn More About It
To see explanations of an improved version of Picard’s existence Theorem 3.6, hence
also an explanation for Theorem 3.7, see An Introduction to Ordinary Differential Equa-
tions, Earl A. Coddington, Dover publications, 1989, c⃝1961. For a more advanced
point of view of Peano’s existence theorem, see Ordinary Differential Equations, Jack K.
Hale, Robert E. Krieger Publishing Company, c⃝1980, or Metric Spaces, E. T. Copson,
Cambridge University Press, c⃝1988.
3.2.4 Problems
1. Solve the ODE
	
t −t2
2y + e−3y
˙y = e 2t −y + t ln(y).
2. Find the solution of dy
dt =
1
2(1 + t2)y that passes through (−1, 1) in the (t, y)-plane.
3. For the ODE ˙y = t(y −1)
t2 + 1 ,
(a) Find the solution, y2(t), that passes through the point (t, y) = (1, 3).
(b) Find the solution, y1(t), that passes through the point (t, y) = (1, 1).
(c) Graph y1(t) and y2(t) on the same set of axes and label which one is which.
The amount, A, of a radioactive element decays at a rate proportional to the amount
remaining, so ˙A = −αA where α is a constant, called the decay constant. The half-life,
denoted by t⋆, of a radioactive substance is the time it takes for its amount to be reduced
to one-half of the initial amount.
4. Find a formula for t⋆in terms of the constant α.
5. The wood of an Egyptian sarcophagus (burial case) is found to contain 63% of the
carbon-14 that would be in a present-day sample. What is the approximate age of
the sarcophagus, assuming that the half-life of carbon-14 is about 5730 years?
6. A certain isotope of radium decays radioactively. After 44.5 years, 2% of it has
decayed. What is its half-life?
The effect of uncertainty in initial data:
In real life, we cannot measure quantities with absolute precision.
The goal of problems 8–10 is to explore how this fact of life can alter, sometimes
significantly, conclusions drawn from ODE models.

220
Advanced Engineering Mathematics
Suppose that the earth’s human population, P(t), satisfies the differential
equation
(⋆) ˙P = kP,
where k is a constant that measures the rate of growth of the population and t is
the time, in years. (This model can give accurate predictions over a short period
of time but is probably not good over a “long” time interval; that leaves open the
question just how long is a long time interval!)
7. Suppose that the earth’s population on July 1, 1980 was 4, 473, 000, 000 and that
on July 1, 1987, it was 5, 055, 000, 000. Assuming that P satisfies model (⋆), when
would the earth’s human population reach 10 billion?
Suppose that in real life, we can’t really measure P with absolute precision. If T
is the time when the earth’s population reaches 10 billion, then we would have a
range of values for T. For each of problems 8–10, before starting your symbol- and
number-crunching, use your common sense, assisted by sketching solution(s) for
P versus t, to predict how the value of T should change due to the ± uncertainty
in the data.
8. Suppose that the earth’s population on July 1, 1980, was (4, 473, 000, 000 ±
50, 000, 000) and that on July 1, 1987, it was 5, 055, 000, 000. Assuming that P satis-
fies model (⋆), when would the earth’s human population reach 10 billion? Your
answer should be an interval of time predictions. Check your answer using com-
mon sense: which should be bigger, the T corresponding to P(0) = 4.523 billion or
to P(0) = 4.423 billion?
9. Suppose that the earth’s population on July 1, 1980, was (4, 473, 000, 000 ± 50, 000,
000) and that on July 1, 1987, it was (5, 055, 000, 000 ±50, 000, 000). Assuming that
P satisfies model (⋆), when would the earth’s human population reach 10 billion?
10. Suppose that the earth’s population on July 1, 1980, was (4, 473, 000, 000 ± 10, 000,
000) and that on July 1, 1987, it was (5, 055, 000, 000 ±10, 000, 000). Assuming that
P satisfies model (⋆), when would the earth’s human population reach 10 billion?
11. A particle moves on the y-axis in such a way that its velocity is proportional to the
square of its distance from the origin. At time t = 0, the particle is located at y = 2.
At time t = 3, the particle is located at y = 4.
(a) Find the position of the particle as a function of time t.
(b) At what time does the particle reach position y = 8?
(c) At what time does the particle reach y = 1000?
(d) How long does the particle “live?”
12. For ODE 2xy dy
dx +y2 −1 = 0, find the solutions passing through the points (a) (1, 0)
and (b) (0, 1) in the xy-plane.
13. Solve the IVP
dy
dx =
2x + y
3 + 3y2 −x, y(0) = 1.
[Hint: First rewrite the ODE in the form M(x, y) + N(x, y) dy
dx = 0.]

Scalar ODEs I
221
14. Solve the IVP
dy
dx = −x + cos y
2 + x sin y , y
	π
2

= −π
2 .
[Hint: First rewrite the ODE in the form M(x, y) + N(x, y) dy
dx = 0.]
15. Solve the ODE
˙y = −sin y + y cos t −4
sin t + t cos y + y .
[Hint: First rewrite the ODE in the form M(t, y) + N(t, y)dy
dt = 0.]
16. Solve the ODE (1 + x cos(xy)) dy
dx + y cos(xy) = 0.
17. Here’s a model for the pharmacokinetics of alcohol: Let x = x(t) be the con-
centration of alcohol in a person, so 0 ≤x ≤1, t be the time in hours, and
A, k be constants. The model has x(t) satisfying the ODE (⋆)
˙x = −kx
A+x after a
person has stopped absorbing alcohol. Assume that initially the person’s alcohol
concentration is 0.024, three times the legal limit in many states.
(a) How long does it take for the person’s concentration to fall within the legal
limit of 0.008, assuming A = 0.005 and k = 0.01?
(b) Choose new values for the constants A, k and discuss how the conclusion
changes. Hypothesize some personal characteristics, for example, gender,
body mass index (BMI), and age that could affect the conclusions and thus
could be modeled by the values of A, k.
(c) What term(s) would you add to ODE (⋆) to model the situation where a person
is still absorbing alcohol at a constant rate?
18. Solve the ODE (x sin(xy) −2y + cos x) dy
dx = −y sin(xy) + x + y sin x. [Hint: First
rewrite the ODE in the form M(x, y) + N(x, y) dy
dx = 0.]
19. Find two distinct solutions of the IVP ˙y =

y −1, y(2) = 1.
20. Find two distinct solutions of the IVP ˙y = y1/5, y(2) = 0.
21. Find two distinct solutions of the IVP ˙y = y2/3, y(0) = 0.
22. Explain why all separable ODEs can be rewritten in a form that is exact!
23. For the IVP
⎧
⎪⎨
⎪⎩
dy
dt = y2
y(0) = 3
⎫
⎪⎬
⎪⎭
,
(a) Explain why the method of separation of variables produces a solution that
exists only on a time interval of the form −∞< t < δ.
(b) Explain why Picard’s theorem guarantees the existence and uniqueness of a
solution on an interval of the form −¯α ≤t ≤¯α.

222
Advanced Engineering Mathematics
(c) Compare the largest value of ¯α that Picard’s theorem produces with the value
of δ. Compare Picard’s theorem theoretical guarantee of a time interval of
existence with the actual time interval of existence.
24. Find an IVP of the form
˙y = f(t, y), y(0) = 0,
other than that of Example 3.14, which has a solution even though f(t, y) is not
continuous on any rectangle that contains (0, 0) strictly inside.
3.3 Second-Order Linear Homogeneous ODEs
Second-order, linear, constant coefficients homogeneous ordinary differential equations
(LCCHODEs) have the standard form
¨y + p˙y + qy = 0,
(3.23)
where ˙ = d
dt and “p” and “q” are constants. We will find the general solution by first
guessing solutions and then turning that guessing process into a method.
3.3.1 Spring–Mass–Damper Systems
But first, a word from our “sponsors”: Why should we care about (3.23)? For one thing,
(3.23) is a model for vibrations basic to engineering. Figure 3.6 shows an unforced oscillator
y=0
y
m
y >0
b
mg
–k(y+ ℓ)
k
.by
FIGURE 3.6
Vertical spring–mass–damper system.

Scalar ODEs I
223
y =–ℓ
y=0
y
ℓ
m
k
y>0
FIGURE 3.7
Vertical spring–mass system.
system with positive spring constant k, a positive constant mass m at the end of the spring,
and a nonnegative constant damping coefficient b.
Let y = y(t) be the displacement downward from the equilibrium position. Newton’s
second law of motion implies
m¨y + b˙y + ky = 0.
(3.24)
Why? First, the spring–mass–damper system has vertical motion, so our model includes
the force of gravity.
The basic differential equation is Newton’s second law of motion
mdv
dt =  Forces.
As shown in Figure 3.7, there may be a damping device attached to the mass and
spring.
There are three forces on the mass: (1) the downward force of gravity on the mass, (2) the
spring’s restoring force, which points toward the equilibrium position, and (3) (possibly)
the damper’s resisting force, which points opposite to the velocity vector. According to
Hooke’s law, the amount the spring is stretched by an object whose mass is m is ℓ= mg/k.
We should explicitly state the coordinate system. We usually use the coordinate system
where y > 0 is downward and y = 0 is the equilibrium position of the spring–mass–damper
system. The free-body diagram has the spring force pointing up because the spring has
been stretched beyond the equilibrium length. The damping force points up as long as the
velocity points down.
From Newton’s second law of motion,
mdv
dt =  Forces = Fgravity + Frestoring + Fresisting = mg −k(y + ℓ) −bv.

224
Advanced Engineering Mathematics
Because kℓ= mg, that is, the force of gravity cancels the part of the spring force that
opposes the stretch of the spring by a length ℓ, so we get
mdv
dt = −ky −b˙y,
hence (3.24).
If y = 0 is located where the end of the spring was before the damper and mass were
attached, then Frestoring =−ky. In this coordinate system, the force of gravity is not canceled
out, that is, the differential equation would be m¨y + b˙y + ky = mg, which we won’t know
how to solve until Section 4.1. So it is convenient to use a coordinate system with y = 0
located at the equilibrium position of the spring–mass–damper system.
Note also that if y > 0 is upward, then Fgravity = −mg, not +mg, and Frestoring = −k(y−ℓ),
so that again, when in equilibrium, the force of gravity cancels part of the spring force. So,
for spring–mass–damper problems, the direction of y > 0 is not crucial, as long as we are
consistent!
As to units of measurement, again, we should be consistent. For example, suppose that
a problem says, “Assume a resistance whose magnitude in newtons is 20 times the magni-
tude of the instantaneous velocity v, in meters per second.” It’s good to “follow the lead”
of the narrative of the problem. Here, the magnitude of the resistance force is in terms
of velocity in meters per second, so it would be best to measure all lengths in meters.
Since |Fresistive| = b|v|, where b is measured in units of newtons/(m/s), this says b = 20 if
forces are measured in newtons, lengths are measured in meters, and time is measured in
seconds.
Example 3.15
Assume that a mass of m kg is attached to the end of a vertical spring, and assume that
a weight of (m g) N would stretch the spring by 3 m, where g ≈9.81 m/s2. Assume a
damping force whose magnitude in newtons is 5 times the magnitude of the instanta-
neous velocity v, in meters per second. If the system is released from rest from a point
1
3 m above the equilibrium position of the spring–mass–damper system, write down an
IVP to model this physical problem.
Method: Assume the coordinate system is “y > 0 is downward and y = 0 is the equilib-
rium position of the object” and that y is measured in meters. At equilibrium, a weight of
m g N stretches the spring ℓ= 3 m beyond its natural length. So m g = 3k, where k is the
spring constant; hence, k = m g
3 . The third sentence tells us that 5|v| = |Fresistive| = b|v|, so
b = 5. It follows that the ODE is m¨y + 5˙y + m g
3 y = 0, that is, ¨y + 5
m ˙y + g
3y = 0.
As for the initial conditions, “...released from rest...” says that ˙y(0) = 0. Because “...
the system is released from rest from a point 1
3 m above the equilibrium position of the
spring–mass–damper system...” and “y > 0 is downward,” y(0) = −1
3.
To summarize, the IVP is

¨y + 5
m ˙y + g
3y = 0
y(0) = −1
3, ˙y(0) = 0

. ⃝
From experience with the physical situation modeled by (3.24), we know that it is appro-
priate to specify also two pieces of initial data: the initial position, y(t0) = y0, and the initial
velocity v(t0) = v0. Because v(t) = ˙y(t), the second datum can be written as ˙y(t0) = ˙y0.

Scalar ODEs I
225
An IVP for a second-order ODE consists of the ODE along with the two initial conditions
y(t0) = y0, ˙y(t0) = ˙y0.
3.3.2 Series RLC Circuit
A second example well known to engineers is a DC series “RLC” circuit with an inductor
of strength L henrys, a capacitor of strength C farads, and a resistor of strength R ohms
(). Assume that R, L, and C are, as usual, constants. It has loop current I, as shown in
Figure 3.8, satisfying
L¨I + R˙I + 1
C I = 0.
(3.25)
Why? The voltage drop across the resistor is RI, the voltage drop across the capacitor is
1
C q, where q is the charge on the capacitor, and the voltage drop across the inductor is L˙I.
Kirchhoff’s voltage law yields
LdI
dt + RI + 1
C q = V0.
Differentiate both sides of this equation with respect to t and use the fact that I = ˙q
to get (3.25).
This points to the power of mathematics: If we learn something about (3.23), then we
will have learned something about both all unforced damped oscillators and all DC series
electrical RLC circuits.
Figure 3.9 shows the shapes of some typical graphs of solutions of (3.24). The labels
“overdamped,” “critically damped,” and “underdamped” will be explained shortly. By
the way, some graphs are labeled by both overdamped and critically damped because
those two cases can produce roughly the same graph, to within the discernment of the
human visual system.
L
I
R
V0
C
FIGURE 3.8
DC RLC series circuit.

226
Advanced Engineering Mathematics
(a)
(b)
(c)
(d)
t
y(t)
t
t
t
y(t)
y(t)
y(t)
FIGURE 3.9
Shapes of solutions of mass–spring–damper system. (a) Critically damped or overdamped, (b) critically damped
or overdamped, (c) critically damped or overdamped, and (d) underdamped.
To get started guessing solutions, recall that in Calculus I or II we studied the basic
exponential growth or decay model
˙y = −ay,
where “a” is a constant. Rewrite the ODE as
˙y + ay = 0,
and guess solutions of the form
y = est,
where later, the constant s will be chosen so as to be useful. Substitute y = est and ˙y = sest
into the ODE to get
0 = sest + aest = (s + a)est.

Scalar ODEs I
227
So, if s = −a, we have a solution of the form y = e−at. Moreover, we can check that for any
constant C,
y(t) = Ce−at
will also be a solution. You saw these solutions in a Calculus I or II course and in Problems
3.2.4.4 through 3.2.4.10.
For (3.23), let’s try the same thing: Substitute y = est into that ODE to get
0 = ¨y + p¨y + qy = s2est + psest + qest = (s2 + ps + q)est.
The characteristic equation
s2 + ps + q = 0
has two solutions, counting multiplicity, for the constant s:
s = −p
2 ±

p2 −4q
2
.
Those solutions are also called the roots of the characteristic polynomial, P(s) ≜s2+ps+q.
For the vertical spring–mass–damper system modeled by (3.24), the characteristic
equation is ms2+bs+k = 0. This has two solutions for the constant s, counting multiplicity:
s = −b
2m ±

b2 −4mk
2m
.
Those solutions are also called the roots of the characteristic polynomial, P(s) ≜ms2+bs+k.
There are three cases for the roots of a quadratic polynomial in s:
• Distinct real roots, s1, s2, called the overdamped case for the mass–spring–damper
ODE (3.24) [corresponding to b2 > 4mk]
• Only one real distinct root, s1, called the critically damped case for (3.24)
[corresponding to b2 = 4mk]
• A complex conjugate pair of roots, α ± iν, where ν ̸= 0, called the underdamped
case for (3.24) [corresponding to b2 < 4mk].
It is useful to define the critical damping by bcrit =
√
4mk, that is, the strength of damp-
ing that produces the critically damped case. Surprisingly, the critical damping definition
is also useful in the context of forced vibrations of a spring–mass–damper system, as we
will see in Section 4.2.
For an ODE, the time constant indicates how long it takes for a solution to decay to 1
e of
its initial value. For the damped harmonic oscillator, all of the solutions are transient. The
time constant τ for the ODE can be defined by
τ ≜
1
rmin
,

228
Advanced Engineering Mathematics
where rmin is the slowest “decay rate.” For the overdamped case, rmin = b−√
b2−4mk
2m
. For
the underdamped and critically damped cases, rmin =
b
2m.
Because each solution y(t) may include many different decaying exponential functions,
“weighted” by constants, we can’t guarantee that y(τ) = 1
e y(0). Nevertheless, for physical
intuition, it is still useful to think of the time constant as being about how long it takes for
the solution to decay in a standard way.
You may be wondering about the phrase “characteristic equation,” which we first saw in
Section 2.1, where it allowed us to find the eigenvalues of a matrix. In fact, as we will see in
Section 5.2, the “characteristic equation” of a LCCHODE and the “characteristic equation”
of a matrix are deeply related, so the solutions of those equations are related!
Example 3.16
Solve an ODE that models a spring–mass–damper system where the mass is 0.5 kg, that
mass would stretch the spring by 9.81
15 m, and the damping device exerts a force whose
magnitude in newtons is 4 times the magnitude of the instantaneous velocity v, in meters
per second. Assume the acceleration of gravity is 9.81 m/s2.
Method: In the MKS system of units, we identify m = 0.5. Because of Hooke’s Law, mg =
kℓ, where ℓis the stretch of the spring in equilibrium. So, k = mg
ℓ= (0.5)(9.81)
9.81/15
= 7.5 N/m.
The damping coefficient is b = 4, in N/(m/s). The ODE is 0.5¨y + 4˙y + 7.5y = 0, that is,
¨y + 8˙y + 15y = 0.
(3.26)
The characteristic equation is 0 = s2 + 8s + 15 = (s + 3)(s + 5), so y = e−3t and y = e−5t
are both solutions of the ODE. Moreover, we can multiply each of those by arbitrary
constants and then add to have what we hope are solutions:
y(t) = c1e−3t + c2e−5t.
(3.27)
Let’s check that these are solutions: For any values of the constants c1, c2, substitute (3.27)
into (3.26) to see that
¨y + 8˙y + 15y = d2
dt2
	
c1e−3t + c2e−5t
+ 8 d
dt

c1e−3t + c2e−5t
+ 15
	
c1e−3t + c2e−5t
= (9 −24 + 15) c1e−3t + (25 −40 + 15) c2e−5t = 0 · c1e−3t + 0 · c2e−5t = 0. ⃝
Analogous to solving a homogeneous system of linear algebraic equations, we see that
in Example 3.16, the solutions were in the form y = c1y1(t) + c2y2(t), that is, the general
linear combination of solutions y1(t), y2(t).
Theorem 3.9 will tell us we have found all of the solutions in Example 3.16. Again,
analogous to Definition 3.2 in Section 3.1 for first-order linear ODEs, we have
Definition 3.8
The general solution of a second-order linear homogeneous ODE
¨y + p(t)˙y + q(t)y = 0
(3.28)

Scalar ODEs I
229
has the form
yh = c1y1(t) + c2y2(t)
if for every solution y∗(t) of (3.28),
there are values of constants c1, c2 giving
y∗(t) = c1y1(t) + c2y2(t). In this case, we call the set of functions {y1(t), y2(t)} a complete
set of basic solutions. Each of the functions y1(t), y2(t) is called a basic solution of (3.28).
These definitions do not require that the coefficients p, q be constants.
Unlike for systems of linear algebraic equations, for second-order linear homogeneous
ODEs we don’t have a concept of “free variables” or “basic variables.” But as we have
begun to see using our guessing method, for constant coefficients linear homogeneous
ODEs, there will be formulas for the complete set of basic solutions.
Theorem 3.8
(Existence and uniqueness) Suppose p(t), q(t), and f(t) are continuous on an open interval
I. Then for all values of initial time t0 inside I and initial data y0 and ˙y0, the IVP
⎧
⎨
⎩
¨y + p(t)˙y + q(t)y = f(t)
y(t0) = y0, ˙y(t0) = ˙y0
⎫
⎬
⎭
(3.29)
has exactly one solution y(t) on the interval I.
Theorem 3.9
(Existence of a complete set of basic solutions of a linear homogeneous second-order ODE)
Suppose p(t) and q(t) are continuous on an open interval I. Then the linear homogeneous
second-order ODE
¨y + p(t)˙y + q(t)y = 0
(3.30)
has a complete set of basic solutions {y1(t), y2(t)}.
Why? Theorem 3.9 follows from Theorem 3.8: Pick any t0 inside I. By Theorem 3.8, each of
the two IVPs
¨y1 + p(t)˙y1 + q(t)y1 = 0
y1(t0) = 1, ˙y1(t0) = 0

,
¨y2 + p(t)˙y2 + q(t)y2 = 0
y2(t0) = 0, ˙y2(t0) = 1

has a solution. Suppose y∗(t) is any solution of ¨y+p(t)˙y+q(t)y = 0 on I and define constants
c1 = y∗(t0) and c2 = ˙y∗(t0). Let
z(t) = y∗(t) −

c1y1(t) + c2y2(t)

.

230
Advanced Engineering Mathematics
It’s easy to see that (1) z(t) satisfies ODE ¨y + p(t)˙y + q(t)y = 0 on I, (2) z(t0) = 0, and (3)
˙z(t0) = 0. For example, to see that (2) is true, recall that y1(t0) = 1 and y2(t0) = 0, so
z(t0) = y∗(t0) −

c1y1(t0) + c2y2(t0)

= y∗(t0) −

y∗(t0) · 1 + ˙y∗(t0) · 0

= y∗(t0) −y∗(t0) = 0.
But, the unique solution of the IVP
¨z + p(t)˙z + q(t)z = 0
z(t0) = 0, ˙z(t0) = 0

is easy to find: z(t) ≡0! So, y∗(t) −

c1y1(t) + c2y2(t)

≡0, that is, y∗(t) = c1y1(t) + c2y2(t). 2
3.3.3 The Underdamped Case
Example 3.17
For the ODE
¨y + 2˙y + 10y = 0.
(3.31)
find as many solutions as possible.
Method: The characteristic equation is 0 = s2 + 2s + 10 = (s + 1)2 + 9, so the roots of the
characteristic polynomial are
s = −1 ± i3.
This would seem to say that the solutions of the ODE are
˜y1(t) = e(−1+i3)t, ˜y2(t) = e(−1−i3)t.
The presence of the symbol i ≜
√
−1 in a complex conjugate pair is reminiscent of the
complex conjugate pair of eigenvalues and eigenvectors that we saw in Section 2.1. [If
you’re an electrical engineer, use j instead of i.]
In a sense, we have gotten ahead of ourselves, because the ˜’s over the y’s will later
signify that we are dealing with complex-valued quantities. So far, we don’t know how
these functions ˜y1(t), ˜y2(t) behave. Also, since our original ODE had only real coef-
ficients, 2 and 10, it is probably not appropriate to have solutions that involve the
symbol i.
Euler’s formula is
eiθ ≜cos θ + i sin θ.
We can take this to be a definition of a shorthand notation, or instead, we could define∗
the exponential function in terms of a power series and then derive† Euler’s formula
using the power series. We’ll assume that the laws of exponents are true even for
complex exponents
∗ez ≜1 + z + 1
2! z2 + 1
3! z3 + · · ·
† eiθ ≜1+iθ + 1
2!(iθ)2 + 1
3! (iθ)3 +· · · =
	
1 −1
2! (θ)2 + 1
4! (θ)4 ± · · ·

+ i
	
θ −1
3! (θ)3 + 1
5! (θ)5 ± ...

= cos θ +i sin θ,
using the MacLaurin series for the cosine and sine functions. These series converge for all real θ.

Scalar ODEs I
231
Using Euler’s formula and the law for addition of exponents, we can rewrite the
solutions of Example 3.17:
y(t) =c1y1(t) +c2y2(t) =c1e(−1+i3)t +c2e(−1−i3)t =c1e−t+i3t +c2e−t−i3t
=c1e−tei3t +c2e−te−i3t = e−tc1 (cos 3t + i sin 3t) +c2 (cos(−3t) + i sin(−3t))

= e−tc1 (cos 3t + i sin 3t) +c2 (cos 3t −i sin 3t)

= e−t
(c1 +c2) cos 3t + i (c1 −c2) sin 3t

≜e−t (c1 cos 3t + c2 sin 3t) ,
where c1 ≜c1 +c2 and c2 ≜i (c1 −c2). In the future, we will not have to go through the
aforementioned process again.
So the solutions of Example 3.17 can be written in the form
y(t) = c1y1(t) + c2y2(t),
where
y1(t) ≜e−t cos 3t = Re(e(−1+i3)t), y2(t) ≜e−t sin 3t = Im(e(−1+i3)t).
Because y1(t) and y2(t) are real-valued functions, they don’t have’s over them. ⃝
So that we don’t have to “reinvent the wheel” in the future, we state
Theorem 3.10
Consider ODE ¨y + p˙y + q = 0. Suppose p, q are real constants and the characteristic poly-
nomial has a complex conjugate pair of roots s = α ± iν, where α and ν are real and ν ̸= 0.
Then all solutions of the ODE are given by
y(t) = c1eαt cos νt + c2eαt sin νt = eαt
c1 cos νt + c2 sin νt

,
where c1, c2 are arbitrary constants. So, {eαt cos νt, eαt sin νt} is a complete set of basic
solutions.
Why? The result follows from substituting each of the proposed basic solutions into
the ODE to explain why they are indeed solutions. Theorem 3.13 will explain why
y1(t) ≜eαt cos νt and y2(t) ≜eαt sin νt give a complete set of basic solutions. 2
Example 3.18
For ODE (3.24), that is, m¨y+b˙y+ky = 0, assume we have the underdamped case, that is,
the characteristic polynomial has a complex conjugate pair of roots s = α ± iν where α, ν
are real and ν ̸= 0. Find the solutions of the ODE, in terms of the physical parameters
m, b, k.
Method: The characteristic polynomial is P(s) = ms2 + bs + k, whose roots are
s = −b
2m ±

b2 −4mk
2m
.

232
Advanced Engineering Mathematics
But, to be in the underdamped case, we need to have b2 −4mk < 0, so we can rewrite

b2 −4mk =

−(4mk −b2) = i

4mk −b2 .
The roots of our characteristic polynomial are
s = α ± iν ≜−b
2m ± i

4mk −b2
2m
.
By Theorem 3.10, the solutions in the underdamped case are given by
y(t) = e−bt/(2m)
c1 cos νt + c2 sin νt

,
where we define the quasi-frequency by
ν ≜

4mk −b2
2m
=

k
m −
 b
2m
2
=

ω2
0 −
 b
2m
2
,
and we define the “undamped natural frequency” to be ω0 ≜

k
m. ⃝
Surprisingly, the quantity defined to be the quasi-frequency reappears when studying
the “frequency response” of forced vibrations of a spring–mass–damper system, as we will
see in Section 4.2.
3.3.4 The Amplitude and Phase Form
Example 3.19
For the undamped oscillator ODE
m¨y + ky = 0,
(3.32)
(a) Find all solutions, and
(b) Rewrite them in the amplitude and phase form
y(t) = A cos(ω0t −δ),
(3.33)
where the amplitude is A, a nonnegative constant; the natural frequency is ω0, a
positive constant; and the phase is δ, a constant with −π < δ ≤π.
Method: (a) The characteristic polynomial is P(s) = ms2 + k, so its roots are s = ±iω0,
where
ω0 =
√
4mk
2m
=
 
k
m .
(3.34)
According to Theorem 3.10, the solutions of the ODE are
y(t) = e0·t
c1 cos ω0t + c2 sin ω0t

= c1 cos ω0t + c2 sin ω0t,
where c1, c2 are arbitrary constants.
(b) We equate the form of the solutions we have with the desired, new form (3.33) and
use the trigonometric identity for the cosine of a difference:
y(t) = c1 cos ω0t + c2
: sin ω0t = A cos(ω0t −δ) = A

cos ω0t cos δ + sin ω0t sin δ

= A cos δ cos ω0t + A sin δ
::::: sin ω0t.

Scalar ODEs I
233
A
δ
(c1, c2)
c2
c1
FIGURE 3.10
Amplitude and phase picture.
So, we need
c1 = A cos(δ) and c2 = A sin(δ).
(3.35)
We recognize this as both how the trigonometric functions are defined and how polar
coordinates are defined. A picture, such as Figure 3.10, “says a thousand words.”∗
We have
A =

c2
1 + c2
2
and
tan(δ) = c2
c1
,
(3.36)
where the quadrant in which δ is found is the quadrant in which the point (c1, c2) is
found. For future reference, we note that
δ =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
arctan(c2/c1),
if c1 > 0
π + arctan(c2/c1),
if c1 < 0
π
2 ,
if c1 = 0 and c2 > 0
−π
2 ,
if c1 = 0 and c2 < 0
D.N.E.,
if (c1, c2) = (0, 0)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
(3.37)
Example 3.20
Find the exact solution of the IVP

¨y + ˙y + y = 0
y(0) = −5, ˙y(0) = 3

.
(3.38)
and express the solution in the exact form of
y(t) = Aeαt cos(νt −δ),
(3.39)
where A, ν > 0 and −π < δ ≤π.
Method: The characteristic polynomial s2 + s + 1 has roots
s = −1 ±

12 −4 · 1 · 1
2 · 1
= −1
2 ± i
√
3
2 ,
∗“The mathematician Fred Almgren‘s son Robert is also a mathematician. He says, ‘A lot of fathers give life
advice to their sons. My father used to tell me, ‘Son, when you’re in doubt, when you don’t know which way to
turn or what to do, I want you to remember two things. First, draw a picture. Second, integrate by parts,” from
p. 182 of Mathematical Aprocrypha, by Steven George Krantz.

234
Advanced Engineering Mathematics
so the solutions of the ODE are
y(t) = c1e−t/2 cos
!√
3 t
2
"
+ c2e−t/2 sin
!√
3 t
2
"
.
The first IC requires −5 = y(0) = c1 · 1 + c2 · 0 = c1, so
y(t) = −5e−t/2 cos
!√
3 t
2
"
+ c2e−t/2 sin
!√
3 t
2
"
.
Before applying the second IC, we calculate
˙y(t) = 5
2 e−t/2cos
	√
3 t
2

+ 5
√
3
2
e−t/2 sin
	√
3 t
2

−1
2 c2e−t/2sin
	√
3 t
2

+
√
3
2 c2e−t/2cos
	√
3 t
2

.
The second IC requires 3 = ˙y(0) = 5
2 +
√
3
2 c2, so c2 =
1
√
3. The solution to the IVP is
y(t) = −5e−t/2 cos
	√
3 t
2

+ 1
√
3
e−t/2 sin
	√
3 t
2

= e−t/2
!
5 cos
	√
3 t
2

+ 1
√
3
sin
	√
3 t
2

"
.
The two terms in the parentheses can be combined into amplitude and phase form, with
the decay factor, e−t/2 coming along for the ride:
−5 cos
!√
3 t
2
"
+ 1
√
3
sin
!√
3 t
2
"
= A cos(νt −δ),
so we must choose ν =
√
3
2 . A and δ are found by drawing the picture in Figure 3.11
and using the formulas for amplitude and phase: A =
 
(−5)2 +
	
1
√
3

2
=

76
3 and
tan(δ) = 1/
√
3
−5 , with c1 < 0, so δ = π + arctan
	
−1
5
√
3

. Using arctan(−x) = −arctan(x), the
exact solution, in the desired final form, is
y(t) =
 
76
3 e−t/2 cos
!√
3
2 t −π + arctan

1
5
√
3
"
. ⃝
c2
A
δ
1
(c1, c2)=(–5,      )
c1
√3
FIGURE 3.11
Amplitude and phase in Example 3.20.

Scalar ODEs I
235
y
y1
y2
t1
t2
t3
t
2π
ν
——
FIGURE 3.12
Typical underdamped solution.
3.3.5 Figures of Merit in Graphs of Underdamped Solutions
In the underdamped case, solutions have the form y(t) = Aeαt cos(νt −δ). Think of this as a
decaying, time-dependent amplitude, Aeαt, multiplying a sinusoidal function, cos(νt −δ).
A typical solution graph is shown in Figure 3.12, where the envelope consists of the
two dashed curves y = ±eαt: We can explain [see Problem 3.3.8.47] why the rela-
tive (also known as “local”) maxima occur at times t = t1, t2, t3, . . ., which occur every
2π
ν
units of time; this explains why we refer to ν as the quasi-frequency. Even though
the relative maxima do not occur exactly at the times when cos(νt −δ) = 1, we have
cos(νt1 −δ) = cos(νt2 −δ) = cos(νt3 −δ) = · · · . Because of this, we have
y(t2)
y(t1) = y(t3)
y(t2) · · · = Aeαt2 cos(νt2 −δ)
Aeαt1 cos(νt1 −δ) = eα(t2−t1) = e2πα/ν.
The logarithmic decrement is defined to be D ≜ln

y(t2)/y(t1)

= 2πα
ν
, so
ν = 2πα
D .
Example 3.21
Assuming mass m = 1, find the damping coefficient and the spring constant that can
produce the solution graphed in Figure 3.13.
Method: First, for an ODE ¨y + b˙y + ky = 0, the roots of the characteristic polynomial
s2 + bs + k are
s = −b
2 ±

b2 −4k
2
.
In order to have infinitely many oscillations, albeit of decaying amplitude, our ODE
must be in the underdamped case. So, as in Example 3.18, the roots of the characteristic
polynomial have to be
s = α ± iν .

236
Advanced Engineering Mathematics
y(t)
4
2
–2
2
4
6
8
10
12
t
FIGURE 3.13
Example 3.21.
Here, m = 1 yields
α = −b
2,
ν =

k −
 b
2
2
.
(3.40)
On the graph given in Figure 3.13, we cannot tell for sure that the first relative maximum
occurs at t = 0, but we can estimate that there are relative maxima y1 ≈2.5, at t1 ≈4.0,
and y2 ≈1.2, at t2 ≈8.0, to two significant digits. The quasi-period is
T ≜2π
ν
≈8.0 −4.0 = 4.0;
hence,
ν ≈2π
4.0 ≈1.570796327,
and the logarithmic decrement is
2πα
ν
= D = ln
y2
y1

≈ln
1.2
2.5

≈−0.7339691751.
So,
−b
2 = α = νD
2π = D
T ≈−0.1834922938;
thus, the damping coefficient is b ≈0.3669845875. It follows from (3.40) that the spring
constant is
k = ν2 +
 b
2
2
≈2.501070523.
To summarize,
b ≈0.3669845875,
k ≈2.501070523.
Rounded off to the two significant digits of our graphical data, b ≈0.37, k ≈2.5. ⃝

Scalar ODEs I
237
3.3.6 The Critically Damped Case
Example 3.22
For the IVP
¨y + 8˙y + 16y = 0,
(3.41)
find as many solutions as possible.
Method: The characteristic polynomial s2 + 8s + 16 = (s + 4)2 has roots s = −4, −4, so it
seems that the solutions of the ODE are
y(t) = c1e−4t + c2e−4t = ce−4t,
where c = c1+c2 is only one arbitrary constant. But our experience with the overdamped
and underdamped cases says that we need two arbitrary constants if we are to solve
for two initial conditions, and the general Theorem 3.9 says that we have two different
solutions y1(t) and y2(t), where, for example, y2(0) = 0, ˙y2(0) = 1. No such function y2(t)
appears from using the roots of the characteristic polynomial. What can we do?
Here’s some general advice: if we don’t know what to do, try using what we do know!
That’s vague. In this situation, we know we have a solution y(t) = e−4t. What can we do
with it? Multiplying it by a constant doesn’t help, so try multiplying it by a function to
be determined:
y(t) = v(t)e−4t.
Substitute this into the ODE, after finding
˙y = ˙ve−4t −4ve−4t,
¨y = ¨ve−4t −8˙ve−4t + 16ve−4t,
to get
0 = ¨y + 8˙y + 16y = (¨ve−4t −8˙ve−4t +

16ve−4t) + 8(˙ve−4t −
4ve−4t) +

16ve−4t = ¨v.
The solutions of 0 = ¨v are v(t) = c1t + c2, so we get solutions
y = v(t)e−4t = (c1t + c2)e−4t = c1e−4t + c2te−4t,
where c1, c2 are arbitrary constants. By “accident” we got the solutions, ce−4t, that we
started with. ⃝
As an aside, when we tried y(t) = v(t)e−4t, we were using the method of “reduction
of order.”
Rather than “reinventing the wheel” in the future, it’s useful to state
Theorem 3.11
(Solutions in the critically damped case) Consider the ODE ¨y + p˙y + q = 0. Suppose p, q
are constants and the characteristic polynomial has a repeated real root s = α. Then all
solutions of the ODE are given by
y(t) = c1eαt + c2teαt,
where c1, c2 are arbitrary constants.
So, {eαt, teαt} is a complete set of basic solutions.

238
Advanced Engineering Mathematics
3.3.7 The Wronskian Determinant
Definition 3.9
Suppose y1(t), y2(t) are any two functions. Their Wronskian determinant is defined to be
W(y1, y2)(t) ≜

y1(t)
y2(t)
˙y1(t)
˙y2(t)
 .
Example 3.23
W(eαt cos νt, eαt sin νt) =

eαt cos νt
eαt sin νt
αeαt cos νt −νeαt sin νt
αeαt sin νt + νeαt cos νt

= eαt cos νt
	
αeαt sin νt + νeαt cos νt

−eαt sin νt
	
αeαt cos νt −νeαt sin νt

= · · · = νe2αt. ⃝
Theorem 3.12
(Abel’s theorem) Suppose y1, y2 are two solutions of the same ODE (3.28), that is, ¨y+p(t)˙y+
q(t)y = 0, on an open interval I. Then
W(y1, y2)(t) = exp
	
−
t
t0
p(τ)dτ

W(y1, y2)(t0)
(3.42)
for any t0, t inside I.
Why? We will see why W(t) ≜W(y1, y2)(t) satisfies the first-order homogeneous ODE
dW
dt = −p(t)W(t),
and then the rest will be in Problem 3.3.8.46. We have
˙W = d
dt

y1(t)˙y2(t) −y2(t)˙y1(t)

=
˙y1(t)˙y2(t) + y1(t)¨y2(t)

−
˙y2(t)˙y1(t) + y2(t)¨y1(t)

=
˙y1(t)˙y2(t) −˙y2(t)˙y1(t)

+

y1(t)¨y2(t) −y2(t)¨y1(t)

=

˙y1(t)
˙y2(t)
˙y1(t)
˙y2(t)
 +

y1(t)
y2(t)
¨y1(t)
¨y2(t)
 .
The first determinant is zero because it has two equal rows.

Scalar ODEs I
239
Both of y1, y2 solve the same ODE (3.28), that is, ¨y1 + p(t)˙y1 + q(t)y1 = 0, ¨y2 + p(t)˙y2 +
q(t)y2 = 0 on I. So, using the effects of elementary row operations on determinants,
˙W = 0 +

y1(t)
y2(t)
−p(t)˙y1 −q(t)y1
−p(t)˙y2 −q(t)y2

=
−q(t)R1+R2→R2

y1(t)
y2(t)
−p(t)˙y1(t)
−p(t)˙y2(t)

=
R2←−p(t)R2
−p(t)

y1(t)
y2(t)
˙y1(t)
˙y2(t)

= −p(t)W(t). 2
Theorem 3.13
Suppose y1(t), y2(t) are solutions of the same linear homogeneous ODE (3.28), that is,
¨y + p(t)˙y + q(t)y = 0, on an open interval I, and p(t), q(t) are continuous on I. Then
W(y1, y2)(t) ̸= 0 for all t in I if, and only if, {y1(t), y2(t)} is a complete set of basic solutions
of ODE (3.28) on I.
Why? The explanation is similar to that of Theorem 3.9: Given any y∗(t) that solves ODE
(3.28) on I, we can choose constants c1, c2 such that
⎡
⎣
y∗(t0)
˙y∗(t0)
⎤
⎦=
⎡
⎣
y1(t0)
y2(t0)
˙y1(t0)
˙y2(t0)
⎤
⎦
⎡
⎣
c1
c2
⎤
⎦
after using Theorem 3.12 to see that the invertibility of the 2 × 2 matrix follows from the
fact that its Wronskian determinant W(y1, y2)(t0) ̸= 0 for all t0 in I. After that, Lemma 1.3
in Section 1.7 and the uniqueness in Theorem 3.8 explain why y∗(t) = c1y1(t) + c2y2(t). 2
3.3.8 Problems
In problems 1–6, solve the ODE. If all solutions have limt→∞y(t) = 0, determine the time
constant.
1. ¨y + 8˙y + 15y = 0
2. 2¨y + 12˙y + 18y = 0
3. ¨y + ˙y −15
4 y = 0
4. ¨y + 2˙y + 65y = 0
5. ¨y + 8˙y + 18y = 0
6. ¨y + 4˙y + 7y = 0
In problems 7–9, solve the IVP.
7. ¨y −2˙y −3y = 0, y(0) = 5, ˙y(0) = 7
8. ¨y + 3
2 ˙y + 1
2 y = 0, y(0) = 0, ˙y(0) = 1
9. ¨y + ˙y + 9
4y = 0, y(0) = 7, ˙y(0) = 0

240
Advanced Engineering Mathematics
In problems 10–13, solve the IVP.
10. ¨y + 8˙y −20y = 0, y(0) = 1, ˙y(0) = 2
11. ¨y + 3˙y −10y = 0, y(0) = 1, ˙y(0) = −3
12. ¨y + 8˙y + 16y = 0, y(0) = −1, ˙y(0) = 2
13. ¨y + ˙y + 1
4y = 0, y(0) = −1, ˙y(0) = 2
In problems 14–21, solve the IVPs and express the solution in the amplitude-phase form
(3.39). Give conclusions using exact values, as simply as possible.
14. ¨y + ˙y + y = 0, y(0) = −2, ˙y(0) = 4
15. ¨y + ˙y + y = 0, y(0) = −2, ˙y(0) = −2
16. ¨y + ˙y + 17
4 y = 0, y(0) = −1, ˙y(0) = 2
17. ¨y + 2˙y + 5y = 0, y(0) = −2, ˙y(0) = 6
18. ¨y + 4˙y + 5y = 0, y(0) = 2, ˙y(0) = 0
19. ¨y + 4˙y + 5y = 0, y(0) = −1, ˙y(0) = 2 +
√
3
20. ¨y + 4˙y + 10y = 0, y(0) = 2, ˙y(0) = 0
21. 5¨y + 20˙y + 60y = 0, y(0) = −2, ˙y(0) = 0
22. For each of the ODEs following, indicate which case it’s in with abbreviations “O”
for overdamped, “C” for critically damped, or “U” for underdamped.
(a) ¨y + 4˙y + 2y = 0
(b) ¨y + 4˙y + 4y = 0
(c) 4¨y + 4˙y + 2y = 0
(d) 4¨y + 4˙y + 1
2 y = 0
(e) 4¨y + 8˙y + 8y = 0
23. Suppose y(t) satisfies a spring–mass–damper ODE ¨y + b˙y + ky = 0. For each of
parts (a), (b), and (c), the behavior of y(t) is described and you are asked to give
a specific numerical value for each of the physical parameters b, k that can produce
such a solution y(t).
(a) y(t) is in the overdamped case
(b) y(t) = Aeαt cos(νt −δ), where α > 0
(c) y(t) is in the critically damped case
24. Shown in Figure 3.14 are graphs of solutions of two different ODEs of the form
¨y + b˙y + y = 0. Decide which of the pairs of parameter values could conceivably
give the graphs, and explain why. More than one pair may be correct.
(a) b1 = 1 and b2 = 2
(b) b1 = 0.5 and b2 = 2
(c) b1 = 1 and b2 = 0.8
(d) b1 = 1 and b2 = 1.5
(e) b1 = 1.2 and b2 = 1.5
25. Shown in Figure 3.15 are graphs of solutions of two different ODEs of the form
¨y + 2˙y + k1y = 0 and ¨y + 2˙y + k2y = 0. Decide which of the pairs of parameter

Scalar ODEs I
241
y
y
t
t
y+b1y + y =0
..
.
y + b2y+ y =0
..
.
FIGURE 3.14
Problem 3.3.8.24.
y
y
t
t
y+2y + k1y =0
..
.
y+2y+ k2y =0
..
.
FIGURE 3.15
Problem 3.3.8.25.
values could conceivably give the graphs, and explain why. More than one pair
may be correct.
(a) k1 = 1 and k2 = 2
(b) k1 = 0.5 and k2 = 2
(c) k1 = 1 and k2 = 0.8
(d) k1 = 1 and k2 = 1.5
(e) k1 = 1.2 and k2 = 1.5
26. An object, whose mass is 4 kg, stretches a spring 0.784 m. The mass is released
from a point 0.3 m above the equilibrium position with a downward velocity of
10 m/s. Assuming there is no damping or other frictional forces, find the position
of the mass as a function of time and find all times when the mass reaches the
maximum displacement below the equilibrium position.
27. An object, whose mass is 2 kg, stretches a spring 0.392 m. At all times, the damping
device gives a resisting force whose magnitude in newtons is 20 times the mag-
nitude of the velocity in m/s. The mass is released from the equilibrium position
with an upward velocity of 5 m/s. How far from the equilibrium position does the
object get?

242
Advanced Engineering Mathematics
28. A mass on a vertical spring has position y(t) = e−t 
cos( t
2) + sin( t
2)

.
(a) When is the first time t ≥0 when the mass passes through the equilibrium
position?
(b) Find an ODE for which y(t) is a solution.
29. Suppose y(t) = 2e−3t cos(t −π
4 ) solves an ODE of the form ¨y + b˙y + ky = 0. Find
the exact values of b, k. Of course, show all your work that explains how you
found b, k.
30. Assume that the vertical position of a wheel in an automobile suspension system
is given by y(t) = −e−t cos 3t +
√
3e−t sin 3t.
(a) When is the first time t ≥0 when the wheel passes through the equilibrium
position?
(b) What is the maximum absolute deviation of the wheel from the equilibrium
position?
(c) Find an ODE for which y(t) is a solution.
31. Suppose that a series RLC circuit has current I satisfying 1
2 ¨I+10˙I+ 1
C I = 0. Assume
that C is, as usual, a constant. Give a criterion on C that guarantees that the current
I does not oscillate infinitely often.
32. Suppose that I(t) = 5e−2t cos(
√
3 t −π
3 ) is a current that solves an ODE of the form
5¨I + R˙I + 1
C I = 0 for a series RLC circuit. Assume that R and C are, as usual,
constants. Find the exact values of the parameters R, in ohms, and C, in farads.
Assume the “5” in the ODE is short for 5 H.
33. Suppose that y(t) satisfies an ODE of the form ¨y + p˙y + qy = 0, where p, q are
constants. Suppose the graph of y(t) has successive local maxima at the points
(t, y) = (2.0000, 5.1234) and (4.0000, 3.9876). Find the values of p, q, correct to four
decimal please.
34. Assuming mass m = 1, find the damping coefficient and the spring constant that
can produce the solution graphed in Figure 3.16.
35. Given that y1(t) and y2(t) solve the same ODE (⋆) ¨y+p(t)˙y+q(t)y = 0 on an interval
I and that p(t) and q(t) are continuous on that interval.
0.6
y
0.4
0.2
–0.2
2
4
6
8
10
12
t
–0.4
FIGURE 3.16
Problem 3.3.8.34.

Scalar ODEs I
243
V0
I1
I2
R
L
C
υ(t)
FIGURE 3.17
Problem 3.3.8.37.
(a) Which of the following is true?
(a1) 2y1(t) −y2(t) is a solution of (⋆) on I
(a2) 2y1(t) −y2(t) is not a solution of (⋆) on I
(a3) Without more information, we cannot tell whether (a1) or (a2) is true.
(b) Which of the following is true?
(b1) c1y1(t) + c2y2(t) is a general solution of (⋆) on I
(b2) c1y1(t) + c2y2(t) is not a general solution of (⋆) on I
(b3) Without more information, we cannot tell whether (b1) or (b2) is true.
36. For the DC series RLC circuit shown in Figure 3.8 and modeled by ODE (3.25),
define the natural frequency by ω0 =

1
LC and the neper “frequency” by α = R
2L.
Explain why the roots of the characteristic polynomial are −α ±

α2 −ω2
0.
37. (Small project) For the DC parallel RLC circuit shown in Figure 3.17, assume that
L, R, and C are, as usual, constants.
(a) Explain why the voltage across the capacitor, v(t), satisfies the ODE C¨v+ 1
R ˙v+
1
Lv = 0.
(b) Define the natural frequency by ω0 =

1
LC and the neper “frequency” by α =
1
2RC. Explain why the roots of the characteristic polynomial are −α±

α2 −ω2
0.
(c) Conclude that the natural frequency is the same whether the RLC circuit
is series or parallel but that the formula for the neper “frequency” varies
according to whether it is a series or parallel circuit.
38. Suppose y(t) solves the IVP ¨y + ˙y −6y = 0, y(0) = a, ˙y(0) = 0, where “a” is
an unspecified positive number. (Do not substitute in a specific value for “a.”)
Explain why limt→−∞y(t) = ∞= limt→∞y(t).
39. Suppose that a critically damped spring–mass–damper system is released from
rest. Explain why the maximum deviation from equilibrium occurs at the initial
time. This is a completely general result!
40. Suppose that an overdamped spring–mass–damper system is released from rest.
Explain why the maximum deviation from equilibrium occurs at the initial time.
This is a completely general result!

244
Advanced Engineering Mathematics
41. For ODE
(1−2t)¨y + (1+ 4t2)˙y + (−2 + 2t−4t2)y = 0,
(a) Explain why et and et2 are solutions.
(b) Find a complete set of basic solutions on some interval.
(c) Try to solve the IVP consisting of that ODE and the ICs y( 1
2) = 5, ˙y
	
1
2

= −3.
(d) Does your difficulty in part (c) contradict the existence and uniqueness
conclusion of Theorem 3.8?
42. Suppose ω and L are positive constants. Find all solutions of ¨y −ω2y = 0 that also
satisfy the “boundary condition” y(L) = 0.
43. Suppose ω and L are positive constants. Find all solutions of ¨y −ω2y = 0 that also
satisfy the “boundary condition” ˙y(L) = 0.
44. Suppose a second-order linear homogeneous ODE has coefficients that are contin-
uous on an interval I and that {y1(t), y2(t)} is a compete set of basic solutions on I.
Use Theorem 3.13 to decide for which values of α the set of {αy1(t) + y2(t), y1(t) +
2αy2(t)} is also a compete set of basic solutions on I.
45. Suppose a second-order linear homogeneous ODE has coefficients that are contin-
uous on an interval I and that {y1(t), y2(t)} is a compete set of basic solutions on I.
Use Theorem 3.13 to decide for which values of α the set of {αy1(t)−3y2(t), y1(t)−
αy2(t)} is also a compete set of basic solutions on I.
46. Using the idea and explanation that preceded Problem 3.1.4.37,
explain
why the solution of
dW
dt = −p(t)W(t) can be written in the form W(t)
=
exp

−
 t
t0 p(τ)dτ

W(t0).
47. Explain why the relative maxima of y(t) = Aeαt cos(νt −δ) occur at times t =
t1, t2, t3, . . ., which occur every 2π
ν units of time. [This is why we call T = 2π
ν the
quasi-period.]
3.4 Higher-Order Linear ODEs
An IVP for an n-th order linear ODE has the standard form
⎧
⎨
⎩
y(n) + p1(t)y(n−1) + · · · + pn−1(t)˙y + pn(t)y = f(t),
y(t0) = y0, ˙y(t0) = y(1)
0 , . . . , y(n−1)(t0) = y(n−1)
0
⎫
⎬
⎭.
(3.43)
Note that n initial conditions are imposed in (3.43).
Theorem 3.14
Suppose the functions p1(t), ..., pn(t), f(t) are continuous on an open interval I. Then IVP
(3.43) has exactly one solution on interval I.

Scalar ODEs I
245
Example 3.24
Find as many solutions of ...y −9¨y + 23˙y −15y = 0 as possible.
Method: As in Section 3.3, we try solutions in the form y = est. Substitute that into the
homogeneous ODE to get
0 = s3est −9s2est + 23sest −15est = (s3 −9s2 + 23s −15)est.
So, we get a characteristic equation in s:
s3 −9s2 + 23s −15 = 0.
It’s possible to factor the third-degree polynomial: s3 −9s2 + 23 −15 = (s −1)(s −3)
(s −5). So, we know the ODE has solutions et, e3t, e5t. Because the equation is linear and
homogeneous, we can multiply each of these functions by an arbitrary constant and add
to get solutions
y(t) = c1et + c2e3t + c3e5t, where c1, c2, c3 are arbitrary constants. ⃝
By the way, here’s some information about factoring: Denote the polynomial by P(s) =
1s3 −9s2 + 23 −15. Standard advice when finding roots of a polynomial says that here we
should try
s = ±All possible factors of 15
all possible factors of 1 = ±1, ±3, ±5, ±15.
Since P(1) = 0, (s −1) must be a factor of P(s). Long division then gives P(s) = (s −1)
(a second-degree polynomial), and the latter can be factored using the quadratic formula.
Did we find all of the solutions in Example 3.24? To answer that, we need a little more
theory.
Definition 3.10
The general solution of an n-th order linear homogeneous ODE
y(n) + p1(t)y(n−1) + · · · + pn−1(t)˙y + pn(t)y = 0
(3.44)
has the form
yh(t) = c1y1(t) + c2y2(t) + · · · + cnyn(t)
if for every solution y∗(t) of (3.44), there are values of constants c1, c2, . . . , cn giving y∗(t) =
c1y1(t)+c2y2(t)+· · ·+cnyn(t). In this case, we call the set of functions {y1(t), y2(t), . . . , yn(t)} a
complete set of basic solutions. Each of functions y1(t), . . . , yn(t) is called a basic solution
of (3.44).

246
Advanced Engineering Mathematics
V0
I1
I2
1 Ω
3
4— H
2
3— F
2 F
υ2(t)
υ1(t)
FIGURE 3.18
Example 3.25.
Example 3.25
Find the solution for the voltage v2(t) across the second capacitor in the DC circuit shown
in Figure 3.18.
Method: The voltage, v, across a capacitor and current, I, through it satisfy ˙v = C−1I.
Here,
˙v1 = 1
2 I1, and ˙v2 = 3
2 I2.
It follows that
2
3 ¨v2 = ˙I2.
(3.45)
Kirchhoff’s voltage law in the first loop states that V0 = I1 + 3
4(˙I1 −˙I2) + v1. Take the
derivative with respect to t of both sides to get
0 = ˙I1 + 3
4(¨I1 −¨I2) + ˙v1 .
(3.46)
Kirchhoff’s voltage law in the second loop states that
0 = v2 + 3
4(˙I2 −˙I1)
(3.47)
The latter yields v2 = 3
4 ˙I1 −3
4 ˙I2 = 3
4 ˙I1 −1
2 ¨v2; hence,
˙I1 = 4
3 v2 + 2
3 ¨v2.
(3.48)
It follows that
2
3 ¨v2 = ˙I1 −4
3 v2.
(3.49)
Take the derivative with respect to t of both sides of (3.47) to get
0 = ˙v2 + 3
4(¨I2 −¨I1).
(3.50)

Scalar ODEs I
247
Add to that (3.46) to get
0 = ˙v2 + ˙I1 + ˙v1,
so,
˙I1 = −˙v2 −˙v1.
Substitute that into (3.49) to get
2
3 ¨v2 = −˙v2 −˙v1 −4
3v2 .
But
˙v1 = 1
2 I1,
so,
2
3 ¨v2 = −˙v2 −1
2 I1 −4
3 v2.
(3.51)
Take the derivative with respect to t of both sides of (3.51) to get
2
3
...v 2 = −¨v2 −1
2
˙I1 −4
3 ˙v2.
(3.52)
Use (3.48), that is, ˙I1 = 2
3 ¨v2 + 4
3 v2, in (3.52) to get
2
3
...v 2 = −¨v2 −
1
3 ¨v2 + 2
3 v2

−4
3 ˙v2
So,
2
3
...
v2 + 4
3 ¨v2 + 4
3 ˙v2 + 2
3 v2 = 0.
After multiplying through by 3
2, the characteristic equation is s3 + 2s2 + 2s + 1 = 0. The
standard advice says to try s = ±1. Here the advice works and gives root s = −1, so we
can factor to get 0 = (s + 1)(s2 + s + 1). The roots are s = −1, −1
2 ± i
√
3
2 , so the general
solution is
v2(t) = c1e−t + c2e−t/2 cos
!√
3t
2
"
+ c3e−t/2 sin
!√
3t
2
"
,
where c1, c2, c3 are arbitrary constants. ⃝
Usually a characteristic equation will not be so simple and easy to solve, but we cooked
this one up to be convenient and demonstrate the concept of general solution.
Theorem 3.15
Suppose the functions p1(t), . . . , pn(t) are continuous on an open interval I. Then the linear
homogeneous n-th order ODE (3.44) has a general solution, that is, a complete set of n
basic solutions, on interval I.

248
Advanced Engineering Mathematics
Why? Pick any t0 inside I. By Theorem 3.14, each of the n IVPs
⎧
⎨
⎩
y(n)
1 +p1(t)y(n−1)
1
+· · ·+pn(t)y1 =0
y1(t0)=1, ˙y1(t0)=0,· · ·, y(n−1)(t0)=0
⎫
⎬
⎭,
⎧
⎨
⎩
y(n)
2 +p1(t)y(n−1)
2
+· · ·+pn(t)y2 =0
y2(t0)=0, ˙y2(t0)=1,· · ·, y(n−1)
2
(t0)=0
⎫
⎬
⎭, · · ·
⎧
⎨
⎩
y(n)
n +p1(t)y(n−1)
n
+· · ·+pn(t)yn =0
yn(t0)=0, ˙yn(t0)=0,· · ·, y(n−1)
n
(t0)=1
⎫
⎬
⎭
has a solution. The rest of the explanation is similar to what we gave for Theorem 3.9 in
Section 3.3. 2
Unfortunately, in Example 3.24, the solutions et, e3t, e5t that we found are not the basic
solutions y1(t), y2(t), y3(t) because, for example, et doesn’t satisfy the initial conditions that
y1(t), y2(t), or y3(t) satisfies in the explanation for Theorem 3.15. But fortunately, we will
have another tool in our toolbox.
Definition 3.11
Suppose y1(t), y2(t), . . . , yn(t) are any n functions. Their Wronskian determinant is the n×n
determinant defined by
W

y1(t), y2(t), . . . , yn(t)

≜

y1(t)
.
.
.
yn(t)
˙y1(t)
.
.
.
˙yn(t)
.
.
.
.
.
.
.
.
.
y(n−1)
1
(t)
.
.
.
y(n−1)
n
(t)

.
Example 3.26
W(et, e3t, e5t) =

et
e3t
e5t
et
3e3t
5e5t
et
9e3t
25e5t

= · · · = 16e9t.
Theorem 3.16
(Abel’s theorem) Suppose y1(t), y2(t), . . . , yn(t) are n solutions of the same n-th order linear
homogeneous ODE (3.44) on an open interval I on which the functions p1(t), . . . , pn(t) are
all continuous. Then,
W(y1, y2, . . . , yn)(t) = exp
⎛
⎝−
t
t0
p1(τ)dτ
⎞
⎠W(y1, y2, . . . , yn)(t0)
(3.53)
for any t0, t inside I.

Scalar ODEs I
249
Why? Similar to the explanation of Theorem 3.12 in Section 3.3, but with need for greater
use of determinants, we can explain why W(t) ≜W(y1, y2, . . . , yn)(t) satisfies the first-order
homogeneous ODE
dW
dt = −p1(t)W(t).
The rest of the explanation is the same as in Section 3.3, specifically as in Problem
3.3.8.46. 2
Theorem 3.17
Suppose y1(t), y2(t), . . . , yn(t) are solutions of the same n-th order linear homogeneous
ODE (3.44) on an open interval I and p1(t), . . . , pn(t) are all continuous on I. Then
W(y1, y2, . . . , yn)(¯t) ̸= 0 for all ¯t in I if, and only if, {y1(t), y2(t), . . . , yn(t)} is a complete
set of basic solutions of ODE (3.44) on I.
Why? The explanation is similar to what we gave for Theorem 3.13 in Section 3.3. 2
Example 3.27
Solve the IVP

...y −9¨y + 23˙y −15y = 0
y(0) = −1, ˙y(0) = 2, ¨y(0) = 5

.
Method: In Example 3.24, we used a characteristic equation to find three solutions, et, e3t,
e5t, for the third-order LCCHODE. In Example 3.26, we calculated their Wronskian, so
Theorem 3.15 guarantees that {et, e3t, e5t} is a complete set of basic solutions for the ODE.
By definition,
y(t) = c1et + c2e3t + c3e5t
gives all the solutions of the ODE. To find constants c1, c2, c3 so that y(t) satisfies the
three ICs, we can write the ICs in vector form as
⎡
⎣
−1
2
5
⎤
⎦=
⎡
⎣
c1 + c2 + c3
c1 + 3c2 + 5c3
c1 + 9c2 + 25c3
⎤
⎦=
⎡
⎣
1
1
1
1
3
5
1
9
25
⎤
⎦
⎡
⎣
c1
c2
c3
⎤
⎦.
The 3 × 3 matrix is the matrix whose determinant gives a Wronskian value of 16, at least
at t = 0. It follows that we can invert the matrix, so
⎡
⎣
c1
c2
c3
⎤
⎦=
⎡
⎣
1
1
1
1
3
5
1
9
25
⎤
⎦
−1 ⎡
⎣
−1
2
5
⎤
⎦=
⎡
⎣
−3.25
3
−0.75
⎤
⎦,
the last step using a calculator. While calculators do not necessarily give exact conclu-
sions, the facts that (1) the determinant of the matrix is 16 = (a power of 2) and (2) the
values for c1, c2, c3 are fractions whose denominators are powers of 2 give us confidence
that these are the exact values. So, the solution of the IVP is
y(t) = −13
4 et + 3e3t −3
4e5t. ⃝

250
Advanced Engineering Mathematics
3.4.1 The Zoo of Solutions of LCCHODEs
Suppose an n-th order homogeneous linear ODE has constant coefficients, that is, is a
LCCHODE, so it is in the form
y(n) + p1y(n−1) + · · · + pn−1˙y + pny = 0,
(3.54)
where p1, . . . , pn are constants. Substituting in y(t) = est yields the corresponding charac-
teristic polynomial
P(s) ≜sn + p1sn−1 + · · · + pn−1s + pn,
(3.55)
which has n roots, including multiplicities.
If among those roots there is a real root, s1, of multiplicity m1, that is, P(s) = (s−s1)m1Q(s)
where Q(s1) ̸= 0, then correspondingly LCCHODE (3.54) has solutions
es1t, . . . , tm1−1es1t.
Since the ODE is linear and homogeneous, the principle of linear superposition tells us
that the ODE has solutions
y(t) = c1es1t + · · · + cm1tm1−1es1t,
(3.56)
where c1, . . . , cm1 are arbitrary constants. So, any positive integer power of t times an expo-
nential function eαt is in the zoo of all possible solutions of LCCHODEs. Of course, a
characteristic polynomial’s n roots may include other roots, so (3.56) may be only part
of the whole story for solutions of that n-th order LCCHODE.
For example, in Example 3.22 in Section 3.3, ODE ¨y + 8˙y + 16y = 0 had characteristic
polynomial P(s) = s2 + 8s + 16 = (s + 4)2 having a double root s = −4, −4; hence, s1 has
m1 = 2, and the ODE had solutions e−4t, te−4t.
A special case of the preceding text is when s1 = 0 is a root of multiplicity m1. Because
es1t = e0·t = e0 ≡1, the ODE has among its solutions
y(t) = c1 + c1t + · · · + cm1tm1−1.
(3.57)
So, any polynomial in t is in the zoo of all possible solutions of LCCHODEs.
For example, ODE y(m1)(t) = 0 has solutions (3.57).
Suppose that among the n roots of the characteristic polynomial P(s), there is a complex
conjugate pair α ± iν, where α, ν are real and ν ̸= 0, of multiplicity m, that is, P(s) =

(s −α)2 + ν2mQ(s), where Q(α ± iν) ̸= 0. Then LCCHODE (3.54) has among its solutions
eαt cos νt, eαt sin νt, . . . , tm−1eαt cos νt, tm−1eαt sin νt.
By the principle of linear superposition, among the solutions of the LCCHODE are
y(t) = (c1 + c2t + · · · + cmtm−1)eαt cos νt + (d1 + d2t + · · · + dmtm−1)eαt sin νt,
where c1, . . . , cm; d1, . . . , dm are arbitrary constants. The special case of α = 0 is included.

Scalar ODEs I
251
Example 3.28
Suppose the characteristic polynomial of a LCCHODE has list of roots s = −1, −1,
−5, −3 ± i2, −3 ± i2, ±i4, ±i4. Find all solutions of the ODE.
Method: By the principle of linear superposition, the solutions are
y(t) = c1e−t + c2te−t + c3e−5t + c4e−3t cos 2t + c5te−3t cos 2t + d4e−3t sin 2t + d5te−3t sin 2t
+ c6 cos 4t + c7t cos 4t + d6 sin 4t + d7t sin 4t,
where c1, . . . , c7 and d4, . . . , d7 are arbitrary constants. ⃝
For an ODE, the time constant indicates how long it takes for a solution to decay to 1
e of
its initial value. Suppose all solutions of a LCCHODE are transient. The time constant τ
for that ODE can be defined by
τ =
1
rmin
,
where rmin is the slowest decay rate. For physical intuition, it is useful to think of the time
constant as being about how long it takes for the solution to decay in a standard way.
3.4.2 Differential Operator Notation
Define the symbol D by
D[ y ] = ˙y,
that is, D is the operator of differentiation. We can think of D as a machine whose input is
a differentiable function, y(t), and whose output is ˙y(t), its derivative with respect to t, as
illustrated in Figure 3.19.
D is a linear operator, because we know from Calculus I that
D[c1y1(t) + c2y2(t)] = c1D[y1(t)] + c2D[y2(t)],
for all constants c1, c2 and all differentiable functions y1(t), y2(t).
We can define higher-order derivatives by
D2[y(t)] ≜D[D[y(t)]] = ¨y(t), D3[y(t)] ≜D[D2[y(t)]] = ...y(t), · · ·
and we can define L, an n-th order linear differential operator (LD-Op), by
L[y(t)] ≜Dn[y(t)] + p1(t)Dn−1[y(t)] + · · · + pn−1(t)D[y(t)] + pn(t)y(t).
D
y
y
FIGURE 3.19
D Operator as a machine.

252
Advanced Engineering Mathematics
A special case is when the coefficients are constant so that we get a linear constant
coefficients differential operator (LCCD-Op) L defined by
L[y(t)] ≜Dn[y(t)] + p1Dn−1[y(t)] + · · · + pn−1D[y(t)] + pny(t)
= (Dn + p1Dn−1 + · · · + pn−1D + pn)[y(t)].
What’s nice about LCCD-Ops is that they can be factored just like polynomials, for
example,
L = D2 + 4D + 3 = (D + 1)(D + 3)
and the order of the factors does not matter:
(D + 1)(D + 3) = (D + 3)(D + 1).
Unfortunately, if an LD-Op L has at least one coefficient that is not constant, then we may
not be able to factor L, and if an LD-Op can be factored, then the order of factors matters
because of the product rule. For example,
(D(D + t)) [y(t)] ≜D[(D + t)[y(t)]] = D[˙y(t) + ty(t)] = D[˙y(t)] + D[ty(t)]
= ¨y(t) +

1 · y(t) + t˙y(t)

= ¨y(t) + t˙y(t) + y(t),
versus
((D + t)D) [y(t)] ≜(D + t)[D[y(t)]] = (D + t)[˙y(t)] = ¨y(t) + t˙y(t).
So, D(D + t) ̸= (D + t)D. The order of operation matters just as the order of matrix
multiplication matters. Fortunately, most of our study will be on LCCD-Ops!
Suppose L = P(D) ≜Dn + p1Dn−1 + · · · + pn−1D + pn is an LCCD-Op, where P is an n-th
degree polynomial. When we substitute y(t) = est into LCCHODE 0 = L[y(t)] = P(D)[y(t)],
we get
0 = P(D)[est] = (Dn + p1Dn−1 + · · · + pn−1D + pn)[est]
= snest + p1sn−1est + · · · + pn−1sest + pnest = est 	
sn + p1sn−1 + · · · + pn−1s + pn

.
The characteristic polynomial is P(s) = sn + p1sn−1 + · · · + pn−1s + pn, that is, the same
polynomial in s rather than D. The characteristic polynomial, P(s), has exactly n roots,
counting multiplicities.
Example 3.29
Find all solutions of the ODE
(D + 1)4(D + 3)
	
(D −1)2 + 32
2
[y(t)] = 0.
(3.58)

Scalar ODEs I
253
Method: The characteristic polynomial is (s + 1)4(s + 3)

(s −1)2 + 322, which has roots
s = −1, −1, −1, −1, −3, 1 ± i3, 1 ± i3. The solutions of LCCHODE (3.58) are
y(t) = (c1 + c2t + c3t2 + c4t3)e−t + c5e−3t + (c6 + c7t)et cos 3t + (d6 + d7t)et sin 3t,
where c1, . . . , c7; d6, d7 are arbitrary constants. ⃝
3.4.3 Shift Theorem
We saw in Example 3.22 in Section 3.3 that ODE (D2 + 8D + 16)[y(t)] = ¨y + 8˙y + 16y = 0
has a solution y1(t) = e−4t, and we saw that substituting in y = e−4tv(t) helped us to find a
second solution, y2(t) = te−4t, after seeing that ¨v = 0. Let’s rewrite what we saw as
(D + 4)2[e−4tv(t)] = 0 yielded D2[v] = 0.
In fact, this is a special case of a general and useful result.
Theorem 3.18
(Shift theorem) For any positive integer k and constant α,
(D −α)k[eαtv(t)] = eαtDk[v].
Why? We’ll just explain why the result is true for k = 1; for other positive integers k, we
could use an inductive process. Notice that Theorem 3.18 is not about solving an equation
but just states a property of differential operators and exponential functions.
We calculate, using the product and chain rules of Calculus I, that
(D−α)[eαtv(t)]=D[ eαtv(t)]−α

eαtv(t)

=


αeαtv(t)+eαt ˙v(t)

−
αeαtv(t)
= eαt ˙v(t)= eαtD[v(t)] . 2
Table 3.2 summarizes the zoo of solutions of LCCHODEs.
Note that the first line is a special case of the second line, because eαt ≡1 when α = 0.
Likewise, the third line includes the functions cos νt and sin νt when α = 0.
TABLE 3.2
The Zoo of Solutions of Linear Homogeneous Constant Coefficients ODEs
Roots of Characteristic
Polynomial
Solutions y(t)
0, 0, . . . , 0 [m times]
1, t, . . . , tm−1
α, α, . . . , α [m times]
eαt, teαt, . . . , tm−1eαt
α ± iν, . . . , α ± iν [m times]
eαt cos νt, teαt cos νt, . . . , tm−1eαt cos νt
and
eαt sin νt, teαt sin νt, . . . , tm−1eαt sin νt

254
Advanced Engineering Mathematics
3.4.4 Problems
In problems 1–3, solve the ODE.
1. ...y +¨y−2y = 0. [Hint: Guess a simple-looking root of the characteristic polynomial.]
2. y(6) −y(4) −2¨y = 0.
3. (D + 1)3[ y ] = 0.
In problems 4 and 5, solve the IVP. Determine the time constant if all solutions have
limt→∞y(t) = 0.
4. (D2 −9)[ y ] = 0, y(0) = 3, ˙y(0) = −6.
5. ...y −2¨y −15˙y = 0, y(0) = 0, ˙y(0) = 0, ¨y(0) = 1.
In problems 6 and 7, solve the IVP. [Hint: To factor the characteristic polynomial, substitute
s2 = r to get a quadratic equation in r.]
6. (D4 + 2D2 + 1)[ y ] = 0, y(0) = 0, ˙y(0) = 0, ¨y(0) = 0, ...y(0) = −2.
7. (D4 −2D2 −3)[ y ] = 0, y(0) = 0, ˙y(0) = 0, ¨y(0) = 0, ...y(0) = −2.
In problems 8–10, you are given a solution of the ODE. Use that information to help solve
the ODE.
8. y(t) = cos t is a solution of ODE ...y + 3¨y + ˙y + 3y = 0.
9. y(t) = sin t is a solution of ODE y(4) −8...y + 17¨y −8˙y + 16y = 0.
10. y(t) = sin t is a solution of ODE y(4) + 2...y + 2¨y + 2˙y + y = 0.
11. Note that (z ± a)(z2 ∓az + a2) = z3 ± a3. Find the exact general solutions of
(a) ...y + 8y = 0.
(b) ...y −2y = 0.
12. Find the solution for the voltage v2(t) across the second capacitor in the DC circuit
shown in Figure 3.20.
V0
I1
I2
7 Ω
16
7
–— H
1
16
–— F
1
5— F
υ2(t)
υ1(t)
FIGURE 3.20
Problem 3.4.4.12.

Scalar ODEs I
255
13. Explain why Abel’s Theorem 3.16 is true for k = 3. First, expand W(t) along its
first row, and then take its time derivative. After that, explain ˙W = −p1(t)W(t) by
work similar to the explanation of Theorem 3.12 in Section 3.3.
3.5 Cauchy–Euler ODEs
Cauchy–Euler ODEs of the second-order have the standard form
r2y′′ + pry′ + qy = 0,
(3.59)
where p, q are given real constants and, throughout this section, ′ denotes d
dr. For this ODE,
instead of guessing solutions of the form y = esr, it turns out that we will get solutions of
the form
y(r) = rn,
where n is to be chosen appropriately later.
When we substitute y(r) = rn into (3.59), we get
0 = r2 	
n(n −1)rn−2
+ pr
	
nrn−1
+ qrn = rn 
n(n −1) + pn + q

.
So, we get a characteristic equation to be satisfied by n:
n(n −1) + pn + q = 0.
(3.60)
Example 3.30
Find as many solutions of
r2y′′ −4ry′ + 6y = 0
(3.61)
as possible.
Method: The characteristic equation is 0 = n(n−1)−4n+6 = n2−5n+6 = (n−2)(n−3), so
we get solutions y1(r) = r2 and y2(r) = r3. Because the ODE is linear and homogeneous,
we get solutions
y(r) = c1r2 + c2r3, where c1, c2 are arbitrary constants.
In fact, this gives all of the solutions on any open interval I, as long as 0 is not in
I, using Theorem 3.13 in Section 3.3: W(y1, y2)(r) =

r2
r3
2r
3r2
 = r4 ̸= 0 as long as
r ̸= 0. ⃝
So, we see how to handle second-order Cauchy–Euler ODEs when the characteristic
polynomial has two distinct real roots. The other two cases, when the characteristic poly-
nomial has exactly one real root or a complex conjugate pair of roots, are trickier. To work
on these cases, we need to study where the idea for trying y = rn came from.

256
Advanced Engineering Mathematics
Substitute t = ln(r), that is, r = et: if y = y(r), denote Y(t) ≜y(et). Using the chain rule
for a function of a single variable, we have
˙Y ≜d
dt

Y(t)

= d
dr

y(r)

· dr
dt = dy
dr · et = dy
dr · r,
and, using the product rule and the chain rule, we have
¨Y ≜d2
dt2

Y(t)

= d
dt
 ˙Y

= d
dt
 dy
dr

·

et 
=
 d
dr
dy
dr

· dr
dt

· (et) +
dy
dr

·
 d
dt

et 
that is,
¨Y =
!
d2y
dr2
"
· (et)2 +
dy
dr

· et =
!
d2y
dr2
"
· r2 +
dy
dr

· r = r2 d2y
dr2 + ˙Y.
When we substitute
r2 d2y
dr2 = ¨Y −˙Y and r dy
dr = ˙Y
(3.62)
into the original Cauchy–Euler ODE (3.59), we get
 ¨Y −˙Y

+ p ˙Y + qY = 0,
that is,
¨Y(t) + (−1 + p) ˙Y(t) + qY(t) = 0.
When we substitute Y(t) = ent into this LCCHODE, we get its characteristic equation
0 = n2 + (−1 + p)n + q = n(n −1) + pn + q,
the same characteristic equation that we got from the original Cauchy–Euler ODE (3.59).
So, we can reuse everything about second-order LCCHODEs that we learned in Section
3.3. There, for the critically damped case, we got general solution Y(t) = c1eαt + c2teαt. But,
t = ln(r) and r = et, so
y(r) = Y(t) = c1eαt + c2teαt = c1

etα + c2 ln(r)

etα = c1rα + c2 ln(r)rα.
So, our two solutions are y1(r) = rα and y2(r) = rα ln(r), where α is real and α is the only
root of the characteristic polynomial in (3.60).
Indeed, we calculate the Wronskian:
W(y1, y2)(r) =

rα
rα ln(r)
αrα−1
αrα−1 · ln(r) + rα · 1
r
 = · · · = r2α−1 ̸= 0,
as long as r ̸= 0. So, we have

Scalar ODEs I
257
Theorem 3.19
If the characteristic polynomial of the homogeneous second-order Cauchy–Euler ODE
(3.59) has only one root, n = α, then on any open interval I not containing r = 0, the
general solution of the ODE is
y(r) = c1rα + c2 ln(r)rα,
that is, {rα, rα ln(r)} is a complete set of basic solutions on I.
Similarly, we can reuse what we learned about the complex solutions case for second-
order LCCHODEs in Section 3.3. There, for the underdamped case, we got general solution
Y(t) = c1eαt cos νt + c2teαt sin νt. But t = ln(r) and r = et, so
y(r) = Y(t) = c1

etα cos(ν ln(r)) + c2

etα sin(ν ln(r)),
that is,
y(r) = c1rα cos(ν ln(r)) + c2rα sin(ν ln(r)).
So, our two solutions are y1(r) = rα cos(ν ln(r)) and y2(r) = rα sin(ν ln(r)), assuming α ± iν,
where α, ν are real and ν ̸= 0, is a complex conjugate pair of roots of the characteristic
polynomial in (3.60). In Problem 3.5.1.10, you will calculate the Wronskian, W

y1(r), y2(r)

,
and explain why it is nonzero at all r ̸= 0. So, we have
Theorem 3.20
If the characteristic polynomial of the homogeneous second-order Cauchy–Euler ODE
(3.59) has a complex conjugate pair of roots α ± iν, where α, ν are real and ν ̸= 0, then
on any open interval I not containing r = 0, the general solution is
y(r) = c1rα cos(ν ln(r)) + c2rα sin(ν ln(r)),
that is, {rα cos(ν ln(r)), rα sin(ν ln(r))} is a complete set of basic solutions on I.
Example 3.31
Solve the IVP

r2y′′ + 3ry′ + y = 0
y(1) = −2, y′(1) = 5

.
Method: The characteristic polynomial is n(n −1) + 3n + 1 = n2 + 2n + 1 = (n + 1)2, so
the characteristic polynomial has only one root, n = −1. By Theorem 3.19, the general
solution of the ODE is
y(r) = c1r−1 + c2r−1 ln(r),

258
Advanced Engineering Mathematics
where c1, c2 are arbitrary constants. Before substituting in the ICs, first calculate
y′(r) = d
dr
	
c1r−1 + c2r−1 ln(r)

= −c1r−2 + c2

−r−2 ln(r) + r−1 · 1
r

= r−2 
−c1 + c2(1 −ln(r))

.
Substitute in the ICs to get
−2 = y(1) = c1 · 1 + c2 · 0
and
5 = y′(1) = −c1 + c2,
so c1 = −2 and c2 = 3. The solution of the IVP is
y(r) = −2r−1 + 3r−1 ln(r) = r−1 
−2 + 3 ln(r)

. ⃝
Example 3.32
Solve the IVP

r2y′′ + 3ry′ + 10y = 0
y(1) = −2, y′(1) = 5

.
Method: The characteristic equation is 0 = n(n−1)+3n+10 = n2+2n+10 = (n+1)2+9,
so the characteristic polynomial has roots, n = −1±i3. The general solution of the ODE is
y(r) = c1r−1 cos(3 ln(r)) + c2r−1 sin(3 ln(r)),
where c1, c2 are arbitrary constants. Before substituting in the ICs, use the product rule
and the chain rule to calculate
y′(r) = d
dr

c1r−1 cos(3 ln(r)) + c2r−1 sin(3 ln(r))

= c1

−r−2 cos(3 ln(r)) + r−1 −sin(3 ln(r)) · 3
r

+ c2

−r−2 sin(3 ln(r)) + r−1 cos(3 ln(r)) · 3
r

= r−2 
c1

−cos(3 ln(r)) −3 sin(3 ln(r))

+ c2

−sin(3 ln(r)) + 3 cos(3 ln(r))

.
Substitute in the ICs to get
−2 = y(1) = c1 · 1 + c2 · 0
and
5 = y′(1) = −c1 + 3c2,
so c1 = −2 and c2 = 1. The solution of the IVP is
y(r) = r−1 
−2 cos(3 ln(r)) + sin(3 ln(r))

. ⃝
Example 3.33
(Pressure in a thick-walled cylinder) ∗Let E be Young’s modulus of elasticity, ν be
Poisson’s ratio, σr be stress in the radial direction, and p be pressure in a thick-walled
cylinder. Then we can assume that
σr =
E
1 −ν2
du
dr + ν u
r

,
∗(See Ugural and Fenster 2003, specifically Section 8.2).

Scalar ODEs I
259
where we have assumed there is no dependence on the angular variable, θ, and u =
u(r) is the deformation, also known as the displacement, from the reference state, for
a ≤r ≤b.
Solve the boundary value problem (BVP)
⎧
⎨
⎩
r2u′′(r) + ru′(r) −u = 0
σr(a) = −pi, σr(b) = −po
⎫
⎬
⎭,
where the subscripts i, o refer to the inner and outer walls of the cylinder.
Method: Each of the conditions σr(a) = −pi and σr(b) = −po are called boundary condi-
tions. The Cauchy–Euler ODE has characteristic equation 0 = n(n −1) + n −1 = n2 −1,
so the characteristic polynomial has roots, n = ±1. The general solution of the ODE is
u(r) = c1r + c2r−1,
where c1, c2 are arbitrary constants. Before substituting in the boundary conditions, we
calculate that
σr(r) =
E
1 −ν2
	
c1 −c2r−2 + νc1 + νc2r−2
=
E
1 −ν2
	
(1 + ν)c1 + (ν −1)c2r−2
.
Substitute in the boundary conditions to get
⎧
⎪⎨
⎪⎩
−1−ν2
E pi = 1−ν2
E σr(a) = (1 + ν)c1 + a−2(ν −1)c2
−1−ν2
E po = 1−ν2
E σr(b) = (1 + ν)c1 + b−2(ν −1)c2
⎫
⎪⎬
⎪⎭
.
Using the inverse of a 2 × 2 matrix, we get
c1
c2

= −1 −ν2
E
 ν + 1
a−2(ν −1)
ν + 1
b−2(ν −1)
−1 pi
po

= −1 −ν2
E
1
(ν2 −1)(b−2 −a−2)

b−2(ν −1)
−a−2(ν −1)
−(ν + 1)
ν + 1
 pi
po

=
−a2b2
E(b2 −a2)
⎡
⎣
(ν −1)(b−2pi −a−2po)
(ν + 1)(−pi + po)
⎤
⎦=
1
E(b2 −a2)
⎡
⎣
(1 −ν)(a2pi −b2po)
(1 + ν)a2b2(pi −po)
⎤
⎦.
It follows that the radial stress is
σr =
E
1 −ν2
	
(1 + ν)c1 + (ν −1)c2r−2
=
1
(1 −ν2)(b2 −a2)
	
(1 + ν)(1 −ν)(a2pi −b2po) + (ν −1)(1 + ν)a2b2(pi −po)r−2
=
1
(b2 −a2)
	
(a2pi −b2po) −a2b2(pi −po)r−2
and
u(r) = c1r + c2r−1 =
1
E(b2 −a2)
	
(1 −ν)(a2pi −b2po)r + (1 + ν)a2b2(pi −po)r−1
.⃝

260
Advanced Engineering Mathematics
3.5.1 Problems
In problems 1–5, solve the ODE, where ′ denotes d
dr.
1. r2y′′ + 5ry′ −2y = 0.
2. r2y′′ + 1
4y = 0.
3. r2y′′ + ry′ + 4y = 0.
4. r2y′′ + 3ry′ + 3y = 0.
5. r2y′′ + 5ry′ + 4y = 0.
In problems 6–9, solve the IVP. Let ′ denote d
dr.
6. r2y′′ + 6ry′ + 6y = 0, y(2) = 0, y′(2) = 1.
7. r2y′′ −2y = 0, y(e) = 0, y′(e) = 11.
8. r2y′′ −3ry′ + 4y = 0, y(e) = 2, y′(e) = −3.
9. r2y′′ −ry′ + 5y = 0, y(1) = −2, y′(1) = 0.
10. Explain why the Wronskian,
W(rα cos(ν ln(r)), rα sin(ν ln(r))
is nonzero at all r ̸= 0, assuming ν ̸= 0.
11. In a quarter of a circular plate, a partial differential equation for steady-state heat
conduction can lead to the ODE
r2y′′ + ry′ −(2m)2y = 0,
where m is a nonnegative integer. Find all of the solutions, in terms of m and r.
[Hint: Consider separately the cases m = 0 and m ≥1.]
12. In a circular plate, the “biharmonic” partial differential equation of solid mechan-
ics can lead to the fourth order ODE
r4y(4) + 2r3y(3) −(2m2 + 1)r2y′′ + (2m2 + 1)ry′
+ m2(m2 −4)y = 0,
where m is a nonnegative integer. This is a fourth-order ODE of Cauchy–Euler
type.
(a) Find the characteristic equation. [Hint: Mathematica may help when doing
this.]
(b) Find all of the solutions of the ODE, for m = 0, m = 1, and m = 2.
13. [Small Project] For the ODE of Problem 3.5.1.12, find all of the solutions in terms
of m and r. [Hint: Mathematica may help when doing this.]

Scalar ODEs I
261
Key Terms
amplitude: (3.33) in Section 3.3
amplitude and phase form: Example 3.19 in Section 3.3
basic solution: Definition 3.2 in Section 3.1, Definition 3.8 in Section 3.3, Definition 3.10 in
Section 3.4
bounded as t →∞: Definition 3.5 in Section 3.1
boundary conditions: Example 3.33 in Section 3.5
Cauchy–Euler ODE: (3.59) in Section 3.5
characteristic equation: before Example 3.16 in Section 3.3, Example 3.24 in Section 3.4
characteristic polynomial: (3.54) in Section 3.4
complete set of basic solutions: Definitions 3.2 in Section 3.1, (3.8) in Section 3.3, (3.10) in
Section 3.4
critically damped: before Example 3.16 in Section 3.3
decay constant: Problems 3.2.4.4 through 3.2.4.6
differential operator: Section 3.4.2
envelope: before Example 3.21 in Section 3.3
Euler’s Formula: Example 3.17 in Section 3.3
exact: Definition 3.7 in Section 3.2
exactness criterion: Theorem 3.4 in Section 3.2
explicit solution: after Example 3.12 in Section 3.2
general solution: Definitions 3.2 in Section 3.1, (3.3) in Section 3.1, (3.8) in Section 3.3,
(3.10) in Section 3.4
half-life: Problems 3.2.4.4 through 3.2.4.6
homogeneous: after (3.2)
implicit solution: after Example 3.1 in Section 3.1
initial condition: before Example 3.1 in Section 3.1
initial value problem (IVP): before Example 3.1 in Section 3.1, before (3.25) in Section 3.3
integrating factor: before (3.5) in Section 3.1
logarithmic decrement: before Example 3.21 in Section 3.3
natural frequency: after (3.33) in Section 3.3
neper “frequency”: Problems 3.3.8.36 and 3.3.8.37
one compartment model: Problem 3.1.4.32
order: beginning of Section 3.1
overdamped: before Example 3.16 in Section 3.3
particular solution: Definition 3.3 in Section 3.1
phase: (3.33) in Section 3.3
quasi-frequency: Example 3.18 in Section 3.3
quasi-period: after (3.40) in Section 3.3
scalar ordinary differential equation: beginning of Section 3.1
separable: after (3.11) in Section 3.2
series RLC circuit: after Theorem 3.13 in Section 3.3
Shift Theorem: Theorem 3.18 in Section 3.4
solution: Definition 3.1 in Section 3.1
spring–mass–damper systems: Section 3.3.1
standard form: (3.2) in Section 3.1
steady-state solution: Definition 3.6 in Section 3.1
time constant: before Example 3.16 in Section 3.16
transient solution: Definition 3.4 in Section 3.1

262
Advanced Engineering Mathematics
underdamped: before Example 3.16 in Section 3.3
undamped: Example 3.19 in Section 3.3
Wronskian determinant: Definition 3.9 in Section 3.3, Definition 3.11 in Section 3.4
Zoo of Solutions of LCCHODEs: before (3.54) in Section 3.4
Mathematica Command
ContourPlot[ty3 −2
3t3 + 1
2y2,{t,−4, 2},{y,−10, 10}, Contours →
28
3

,
PerformanceGoal →′′Quality′′, ContourShading →False]
Reference
Ugural, A.C. and Fenster, S.K. Advanced Strength and Applied Elasticity. Pearson, Upper Saddle River,
NJ, 2003.

4
Scalar ODEs II: Nonhomogeneous Problems
4.1 Nonhomogeneous ODEs
The standard form of the n-th order linear ODE is, again,
y(n) + p1(t)y(n−1) + · · · + pn−1(t)˙y + pn(t)y = f(t).
(4.1)
Definition 4.1
(a) A particular solution of an n-th order linear nonhomogeneous ODE (4.1) is any
function yp(t) that satisfies (4.1).
(b) y(t) = yp(t)+c1y1(t)+c2y2(t)+· · ·+cnyn(t) is the general solution of ODE (4.1) if for
every
solution y∗(t) of (4.1), there are values for scalar constants c1, . . . , cn giving y∗(t) =
yp(t) + c1y1(t) + c2y2(t) + · · · + cnyn(t).
Analogous to Theorem 3.1 in Section 3.1, we have the following:
Theorem 4.1
If yp(t) is any particular solution of n-th order linear nonhomogeneous ODE (4.1) and
{y1(t), . . . , yn(t)} is any complete set of basic solutions of the corresponding linear homoge-
neous ODE
y(n) + p1(t)y(n−1) + · · · + pn−1(t)˙y + pn(t)y = 0,
(4.2)
then y(t) = yp(t) + c1y1(t) + c2y2(t) + · · · + cnyn(t) is a general solution of ODE (4.1), where
c1, . . . , cn are arbitrary constants.
Example 4.1
Suppose ODE ¨y + y = cos t has a particular solution yp(t) = 1
2 t sin t. [We will see later in
this section how to find this particular solution.] Solve the IVP

¨y + y = cos t
y(0) = 1, ˙y(0) = 5

.
263

264
Advanced Engineering Mathematics
Method: Theorem 4.1 says that
y(t) = 1
2 t sin t + c1y1(t) + c2y2(t)
is a general solution of the ODE, where {y1(t), y2(t)} is a complete set of basic solutions of
the corresponding linear homogeneous ODE ¨y + y = 0 and c1, c2 are arbitrary constants.
We saw how to solve this homogeneous ODE in Section 3.3: The characteristic poly-
nomial, s2 + 1, has a complex conjugate pair of roots, s = ±i, so the general solution of
¨y+y = 0 is y(t) = c1 cos t+c2 sin t: The general solution of the original, nonhomogeneous
ODE ¨y + y = cos t is
y(t) = 1
2 t sin t + c1 cos t + c2 sin t,
where c1, c2 are arbitrary constants. Before using the ICs to solve for c1 and c2, calculate
˙y(t) = 1
2 sin t + 1
2 t cos t −c1 sin t + c2 cos t.
Substituting y(t) and ˙y(t) into the ICs gives
1
= y(0)
= 0 + c1 · 1 + c2 · 0,
5
= ˙y(0)
= 0 −c1 · 0 + c2 · 1

,
so c1 = 1 and c2 = 5. The solution of the IVP is
y(t) = 1
2 t sin t + cos t + 5 sin t. ⃝
4.1.1 Special Case: RHS from the Zoo and Constant Coefﬁcients on LHS
Suppose a nonhomogeneous linear ODE has the special form
y(n) + p1y(n−1) + · · · + pn−1˙y + pny = f(t),
(4.3)
where the coefficients p1, . . . , pn are constants and the right-hand side (RHS), f(t), is a linear
combination of functions that are in the zoo of solutions of linear, constant coefficients
homogeneous ordinary differential equations (LCCHODEs). In this special case, we will
see there is an efficient method for solving nonhomogeneous ODE (4.3).
For example,
f(t) = −cos t + 17e−2t sin 5t + 4t3 −1
3e2t
is such an RHS function, that is, is a linear combination of functions that are in the zoo.
Example 4.2
Find all solutions of ODE ¨y + 2˙y + 5y = e−t.
Method: Define the differential operator L by L[y] ≜¨y + 2˙y + 5y, that is, the left-hand
side (LHS) of the nonhomogeneous ODE we’re trying to solve. Let’s guess a particular
solution of the form yp(t) = Ae−t, where A is a constant to be determined later. Why? For
this form of particular solution, no matter how many derivatives we take of Ae−t, the
result will be a multiple of Ae−t, so L[Ae−t] will be a multiple of Ae−t like the RHS, e−t.

Scalar ODEs II
265
We substitute yp(t) into the ODE to get
e−t = L[yp] = L[Ae−t] = (D2 + 2D + 5)[Ae−t] = Ae−t −2Ae−t + 5Ae−t = 4Ae−t,
so A = 1
4 and yp(t) = 1
4e−t. By Theorem 4.1, the general solution of the nonhomogeneous
ODE has the form y = yp + yh =
1
4e−t + c1y1(t) + c2y2(t). The characteristic poly-
nomial of the corresponding LCCHODE, 0 = L[yh] = ¨yh + 2 ˙yh + 5yh, is s2 + 2s + 5.
Because the roots are s = −1 ± i2, the general solution of the original nonhomogeneous
ODE is
y = yp + yh = 1
4e−t + c1e−t cos 2t + c2e−t sin 2t,
where c1, c2 are arbitrary constants. ⃝
4.1.2 The Method of Coefﬁcients to Be Determined
This method is usually called the method of undetermined coefficients, but that makes
it sound like we still have coefficients that were not determined after we finish. In
fact, the whole point of the method is to figure out the correct form of a particular
solution and then determine the values of the coefficients in that form, as we saw in
Example 4.2.
Suppose L = P(D) is an n-th order linear constant coefficients differential operator and
f(t) is a linear combination of functions in the “zoo.” To solve L[y] = f(t), here’s the method:
Step 1. Find all n of the roots of the corresponding LCCHODE characteristic polynomial,
P(s), and write them in a list, L1. The list may include repetitions.
Step 2. Find all roots that correspond to the zoo functions that are in f(t), and write them
in a list L2. The list may include repetitions. Suppose L2 has m roots, including repetitions.
Step 3. Amalgamate the two lists to get a “superlist,” L, which is organized so that all of
its repetitions are next to each other. L has m + n roots, including repetitions.
Step 4. Write down the general solution, y(t), of an (m + n)-th order LCCHODE whose
characteristic polynomial has roots identical to the superlist, L, including repetitions. (We
don’t need to actually write down that (m + n)-th order LCCHODE; we only need its
solutions that we get from the superlist.)
Step 5. Let yp(t) = y(t) −yh(t), where yh(t) is the general solution of the LCCHODE
L[y] = 0. Note that yp(t) should have m coefficients that are constants to be determined
in step 6.
Step 6. Substitute yp(t) into the original, nonhomogeneous ODE L[y] = f(t), sort by
common functions to get m equations for the m coefficients in yp(t), and solve to get yp(t).
Step 7. Write the general solution of the original, nonhomogeneous ODE in the form
y(t) = yp(t) + yh(t).
Step 8. If this is an IVP, that is, if there are initial conditions (ICs) to be satisfied, only
after assembling the general solution y = yh + yp do we substitute that into the ICs. Do NOT
substitute a particular solution or a homogeneous solution into the ICs. 2

266
Advanced Engineering Mathematics
Example 4.3
(Example 4.2 again) Find the form of a particular solution of ODE ¨y + 2˙y + 5y = e−t.
Method: Characteristic polynomial s2 + 2s + 5 gives list L1 = −1 ± i2, and RHS f(t) = e−t
gives list L2 = −1, so the superlist is L = −1 ± i2, −1. This gives y(t) = c1e−t cos 2t +
c2e−t sin 2t + c3e−t, so yh(t) = c1e−t cos 2t + c2e−t sin 2t gives yp(t) = c3e−t. The correct
form of a particular solution is yp(t) = Ae−t, where A is a constant to be determined later
after substituting yp into ¨y + 2˙y + 5y = e−t. ⃝
Example 4.4
Find the form of a particular solution of ODE ¨y + 3˙y + 2y = e−t.
Method: Characteristic polynomial s2 + 3s + 2 gives list L1 = −1, −2, and RHS f(t) = e−t
gives list L2 = −1, so the superlist is L = −1, −1, −2. This gives y(t) = c1e−t + c2te−t +
c3e−2t, so yh(t) = c1e−t + c3e−2t gives yp(t) = c2te−t. The correct form of a particular
solution is yp(t) = Ate−t, where A is a constant to be determined later after substituting
yp into ¨y + 4˙y + 3y = e−t. ⃝
There can be an interaction(s) between the differential operator, L, on the LHS of the
nonhomogeneous ODE and the function f(t) on the RHS. Some textbooks ask the reader to
learn many special rules about how to choose the form of a particular solution. We prefer
to learn one method that does not require remembering special cases
Example 4.5
Find the form of a particular solution of ODE ¨y + 4y = e−t −cos 2t.
Method: Characteristic polynomial s2 + 4 gives list L1 = ±i2, and RHS f(t) = e−t −cos 2t
gives list L2 = −1, ±i2, so the superlist is L = −1, ±i2, ±i2. This gives y(t) = c1e−t +
c2 cos 2t + c3 sin 2t + c4t cos 2t + c5t sin 2t, so yh(t) = c2 cos 2t + c3 sin 2t gives yp(t) =
c1e−t + c4t cos 2t + c5t sin 2t. The correct form of a particular solution is
yp(t) = Ae−t + Bt cos 2t + Ct sin 2t,
where A, B, C are constants to be determined. ⃝
Example 4.6
Solve the IVP
 ¨y + 3˙y + 2y = e−t
y(0) = −1, ˙y(0) = 2

.
Method: From Example 4.4, the correct form of a particular solution is yp(t) = Ate−t,
where A is a constant to be determined. First, we calculate derivatives
˙yp = A(1 · e−t −te−t),
¨yp = A(−2e−t + te−t).
Substituting these into the original nonhomogeneous ODE gives
1 · e−t = ¨yp + 3 ˙yp + 2yp = A

(−2e−t +
te−t) + 3(e−t −
te−t) +

2te−t
= Ae−t.
(4.4)
This determines that A = 1, so yp(t) = te−t. While solving Example 4.4, we found the
solution of the corresponding LCCHODE, yh(t) = c1e−t + c3e−2t. The general solution of
the ODE is

Scalar ODEs II
267
y(t) = te−t + c1e−t + c3e−2t.
While substituting yp into the nonhomogeneous ODE, we calculated ˙yp = A(1−t)e−t, so
˙y = (1 −t)e−t −c1e−t −2c2e−2t. We substitute the general solution into the ICs:
−1
= y(0)
= 0 + c1 + c2
2
= ˙y(0)
= 1 −c1 −2c2

.
This can be written as
 1
1
−1
−2
 c1
c2

=
−1
1

.
Using the inverse of the 2 × 2 matrix gives
c1
c2

=
1
−1
−2
−1
1
1
 −1
1

=
−1
0

,
so the solution of the IVP is
y(t) = te−t −e−t = (t −1)e−t. ⃝
We notice that in (4.4), several terms canceled. This was not a coincidence, as we will see
in Example 4.9.
Example 4.7
For the AC–LC circuit shown in Figure 4.1, find the current as a function of time, assum-
ing the initial charge on the capacitor is 3 C and the initial current is 1 A. Also, find the
steady-state current and its amplitude.
Method: Let q = q(t) be the charge on the capacitor, in coulombs. Kirchhoff’s voltage law
yields ODE L¨q + 1
C q = V(t). The picture shows L = 1 H, C = 1
9 F, and V(t) = e−2t volts.
The IVP is

¨q + 9q = e−2t
q(0) = 3, ˙q(0) = I(0) = 1

.
The corresponding LCCHODE ¨q + 9q = 0 has s = ±i3 as the roots of its character-
istic polynomial’s, so qh(t) = c1 cos 3t + c2 sin 3t, where c1, c2 are arbitrary constants.
V(t)=e–2t volts
I
1 H
—F
1
9
FIGURE 4.1
AC LC series circuit in Example 4.7.

268
Advanced Engineering Mathematics
The superlist is L = −2, ±i3, so q(t) = c1 cos 3t + c2 sin 3t + c3e−2t, and thus, qp(t) =
Ae−2t is the correct form of a particular solution. Substituting that into the original,
nonhomogeneous ODE gives
1 · e−2t = ¨qp + 9qp = 4Ae−2t + 9Ae−2t = 13Ae−2t;
hence, the general solution of the nonhomogeneous ODE is
q(t) = 1
13e−2t + c1 cos 3t + c2 sin 3t.
Substitute that into the ICs to get
⎧
⎨
⎩
3 = q(0) = 1
13 + c1
1 = ˙q(0) = −2
13 −0 · c1 + 3c2
⎫
⎬
⎭.
This yields c1 = 38
13 and c2 = 5
13, and the solution of the IVP is
q(t) = 1
13e−2t + 38
13 cos 3t + 5
13 sin 3t.
Because limt→∞e−2t = 0, the transient solution is qT =
1
13e−2t. Because, q(t) −qT =
38
13 cos 3t + 5
13 sin 3t is bounded as t →∞, the steady-state charge is
qS(t) = 38
13 cos 3t + 5
13 sin 3t.
The steady-state current is
IS(t) = ˙qS(t) = 3
13 (−38 sin 3t + 5 cos 3t) ,
whose amplitude is
3
13 ·

(−38)2 + 52 = 3
13

382 + 52 = 3
13
√
13 · 113 = 3 ·

113
13 . ⃝
In Example 4.7, the steady-state solution is not a particular solution, and the transient
solution is not a homogeneous solution.
Example 4.8
For the IVP
¨y + 2˙y + 9y = sin 2t
y(0) = 3, ˙y(0) = −2

, find the steady-state solution and its amplitude.
Method: The corresponding LCCHODE ¨y+2˙y+9y = 0 has its characteristic polynomial’s
roots being −1±i
√
8, so yh(t) = c1e−t cos(
√
8 t)+c2e−t cos(
√
8 t), where c1, c2 are arbitrary
constants. Because limt→∞yh(t) = 0, the steady-state solution can only possibly be yp(t).
The superlist is L =−1±i
√
8, ±i2, so y(t) = yh(t)+c3 cos 2t+c4 sin 2t, and thus, yp(t) =
A cos 2t + B sin 2t is the correct form of a particular solution, where A, B are constants to
be determined. Substitute that into the original, nonhomogeneous ODE to get
sin 2t = ¨yp + 2 ˙yp + 9yp
= (−4A cos 2t −4B sin 2t) + 2(−2A sin 2t + 2B cos 2t) + 9(A cos 2t + B sin 2t)
= (5A + 4B) cos 2t + (−4A + 5B) sin 2t.

Scalar ODEs II
269
To match the LHS with the RHS, we sort by functions in common to get
0 · cos 2t + 1
: · sin 2t = (5A + 4B) cos 2t + (−4A + 5B)
::::::::: sin 2t;
hence,
 0 = 5A + 4B
1 = −4A + 5B

.
Using the inverse of a 2 × 2 matrix, we get
A
B

=
 5
4
−4
5
−1 0
1

= 1
41
5
−4
4
5
 0
1

= 1
41
−4
5

.
The steady-state solution of the IVP is
yS(t) = yp(t) = 1
41(−4 cos 2t + 5 sin 2t),
and its amplitude is 1
41

(−4)2 + 52 =
1
√
41. ⃝
We were only asked for the steady-state solution, so in this problem, we didn’t need to
use the ICs to solve for the constants c1, c2.
4.1.3 Justiﬁcation for the Method
Suppose we want to solve a nonhomogeneous ODE L[y] = f(t), where L is an n-th order
linear constant coefficients ordinary differential operator and f(t) is a linear combination
of functions in the zoo of solutions of LCCHODEs.
Corresponding to f(t) we have a list L2 of m roots. From that list, we can construct an
m-th order linear constant coefficients ordinary differential operator L that “annihilates”
f(t), in the sense that L[ f(t)] = 0. For example, in Example 4.5, for f(t) = e−t −cos 2t,
we had list of roots L2 = −1, ±i2, so L ≜(D + 1)(D −i2)(D + i2) = (D + 1)(D2 + 4)
annihilates f(t). This is just a short way of saying that (D + 1)(D2 + 4)[ f(t)] = 0, that is, f(t)
is a solution of LCCHODE (D + 1)(D2 + 4)[y] = 0. [Note that f(t), the RHS of the original,
nonhomogeneous ODE L[y] = f, need not be a general solution, just some solution, of
L[y] = 0.]
We will explain why yp(t) is a solution of the (n + m)-order LCCHODE LL[y] = 0 if yp(t)
is a particular solution of the original nonhomogeneous L[y] = f(t). This is because
L[yp] = f implies LL[yp] ≜L[L[yp]] = L[ f] = 0,
because L was chosen so as to annihilate f.
Moreover, we might as well let the form of a particular solution be yp(t) ≜y(t) −yh(t),
where y(t) is the general solution of LL[y] = 0 and yh(t) is the general solution of the
corresponding n-th order LCCHODE L[y] = 0. Why? Adding yh(t) to yp(t) does not help
to find yp(t), because if we did, we would wind up solving f = L[yp + yh] = L[yp] + L[yh] =
L[yp] + 0. 2

270
Advanced Engineering Mathematics
4.1.4 Using a Shift Theorem
Recall from Theorem 3.18 in Section 3.4, a shift theorem, that for any positive integer n,
(D −α)n[eαtg(t)] = eαtDng(t) = eαtg(n)(t).
For example, (D −α)[eαtg(t)] = eαt ˙g(t).
In Example 4.4, we found that yp(t) = Ate−t is the correct form of a particular solution of
ODE ¨y + 3˙y + 2y = e−t. In Example 4.4, we substituted yp into the nonhomogeneous ODE
and then solved for the constant A. Here we will see an alternative way to substitute in yp
that takes advantage of a shift theorem.
Example 4.9
(Part of Example 4.6 again, done in a new way) Find a particular solution of ¨y + 3˙y +
2y = e−t.
Method: Substitute yp(t) = Ate−t into the ODE and use the (shift) Theorem 3.18 in Section
3.4 with α = −1 and f(t) = t to get
e−t = L[ yp(t)] = (D2 + 3D + 2)[ Ate−t] = A(D + 2)(D + 1)[ te−t]
= A(D + 2)[(D + 1)[ te−t]] = A(D + 2)[ e−tD[ t] ] = A(D + 2)[ e−t] = Ae−t,
so A = 1, and yp(t) = te−t is a particular solution. ⃝
Each person can decide for himself or herself whether it is better/easier/more reliable to
use a shift theorem or just take two derivatives of yp(t) = Ate−t before substituting into the
nonhomogeneous ODE. In more complicated examples, for example, higher-order ODEs,
a shift theorem may become more advantageous in terms of accuracy and ease of use.
In any case, we can use a shift theorem as a way to check our work by using a different
technique. In real life, any methods we have for checking our work are useful for reducing
the number and severity of analytical errors.
Theorem 4.2
(Nonhomogeneous superposition principle) If L[y1,p] = f1(t) and L[y2,p] = f2(t), then
y(t) = y1,p + y2,p is a particular solution of the nonhomogeneous ODE L[ y ] = f1(t) + f2(t).
Example 4.10
Solve ¨y + 3˙y + 2y = 3e−t + 4e−4t.
Method: First, rewrite our ODE using differential operators:
(D + 1)(D + 2)[yp] = 3e−t + 4e−4t.
(4.5)
Using Theorem 4.2, we write a particular solution of the ODE in the form yp(t) = y1,p +
y2,p where (D + 1)(D + 2)[y1,p] = 3e−t and (D + 1)(D + 2)[y2,p] = 4e−4t.
For (D + 1)(D + 2)[y1,p] = 3e−t, our lists are L1 = −1, −2 and L2 = −1, so the superlist
is L = −1, −1, −2. We get y(t) = c1e−t + c2te−t + c3e−2t and yh(t) = c1e−t + c3e−2t, so
y1,p(t) = Ate−t,

Scalar ODEs II
271
where A is a constant to be determined. Substitute that into the first nonhomogeneous
problem and use a shift theorem to calculate
3e−t = (D + 1)(D + 2)[ Ate−t] = (D + 2)[(D + 1)[ Ate−t]] = (D + 2)[ Ae−tD[ t]]
= (D + 2)[ Ae−t] = (−1 + 2)Ae−t = Ae−t;
hence, A = 3.
For the second nonhomogeneous problem, (D + 1)(D + 2)[y2,p] = 4e−4t, our lists are
L1 = −1, −2 and L2 = −4, so the superlist is L = −1, −2, −4. We get y(t) = c1e−t +
c2e−2t + c4e−4t and yh(t) = c1e−t + c2e−2t, so
y2,p(t) = Be−4t,
Substitute that into the second nonhomogeneous problem and calculate
4e−4t = (D + 1)(D + 2)[Be−4t] = (−4 + 1)(−4 + 2)Be−4t = 6Be−4t;
hence, B = 2
3.
So, yp(t) = y1,p + y2,p = 3te−t + 2
3e−4t. The general solution of the original ODE is
yp(t) = e−4t + 3te−t + c1e−t + c2e−2t,
where c1, c2 are arbitrary constants. ⃝
4.1.5 Problems
In problems 1–8, solve the ODE.
1. ¨y + 5˙y + 6y = 3e−t
2. ¨y + 4˙y + 6y = t + 3e−2t
3. ¨y + 5˙y + 6y = 2e−3t
4. y′′ + 2y = xe−x, where ′ denotes d
dx
5. ¨y −y = e−t + 5e−2t
6. ¨y + 4˙y + 5y = sin 2t
7. ¨y + ˙y −12y = 5e−4t
8. ¨y + 4y = e−t cos 2t
9. ODE (⋆) ¨y + 3˙y + 2y = cos(et) has a particular solution y(t) = −e−2t cos(et). Solve
the IVP consisting of ODE (⋆) and ICs y(0) = 0, ˙y(0) = 0.
10. Suppose that y1(x) = sin x solves ODE (⋆) y′(x) + 2xy = cos x + 2x sin x,
where ′ = d
dx.
(a) Find the general solution of ODE (⋆).
(b) Solve the IVP consisting of ODE (⋆) and IC y(0) = 5.
11. Given that ODE y(t) = −1
9tet + 1
6t2et solves (⋆) ¨y + ˙y −2y = tet, find the solution of
the IVP consisting of ODE (⋆) and ICs y(0) = 0, ˙y(0) = −2.
12. Suppose α is a nonzero constant. Find all solutions of ˙y + αy = e−t in as simple
form as possible. [Caution: α = 1 may be a special case.] Your final conclusion(s)
should be in terms of α.

272
Advanced Engineering Mathematics
I
~
0.02 sin(120πt)
R
C
FIGURE 4.2
AC–RC series circuit in Problem 4.1.5.16.
In problems 13–15, solve the IVP.
13. ¨y −y = e−2t, y(0) = ˙y(0) = 0
14. ¨y −y = et, y(0) = ˙y(0) = 0
15. ¨y + ˙y + 5y = 10, y(0) = ˙y(0) = 0
16. For the AC–RC−series circuit shown in Figure 4.2, find the solution of the IVP
R˙q+ 1
C q = 0.02 sin(120πt), q(0) = 0.001. Let q = q(t) be the charge on the capacitor,
in units of coulombs, and measure R in ohms and C in farads. Note that ω = 120π
is a frequency of 60 Hz, commonly used in electrical systems. As usual, assume
that R and C are constants.
(a) Using the method of undetermined coefficients
(b) Using the integrating factor method of Section 3.1
In problems 17–24, find the steady-state solution and its amplitude.
17. ¨y + 2˙y + 5y = sin 2t
18. ¨y + 4˙y + 5y = sin t
19. ¨y + 4˙y + 5y = cos t
20. ¨y + 3˙y + 5y = 6 cos 4t
21. ¨y + 2˙y + 5y = 3, y(π) = 7, ˙y(π) = 0
22. ¨y + 9y = 3e−2t, y(0) = 0, ˙y(0) = −4
23. 2¨y + 4y = f0e−2t, y(0) = ˙y(0) = 0
24. m¨y + ky = f0e−2t, y(0) = 2, ˙y(0) = −1
Your final conclusion should be in terms of the unspecified positive constant
parameters m, k, f0.
In problems 25 and 26, solve the IVP.
25. ¨y −2˙y + 2y = t, y(π) = 0, ˙y(π) = 0

Scalar ODEs II
273
26. ¨y + 2˙y = e−2t, y(0) = 0, ˙y(0) = −5
27. Given that cos t is one solution of ...y + 2¨y + ˙y + 2y = 0, find the general solution of
...y + 2¨y + ˙y + 2y = 4 −3e−2t.
28. Suppose that y3(t) solves ODE m¨y + b˙y + ky = g(t) and y0(t) solves ODE m¨y + b˙y +
ky = 0. Let y(t) = 2y0(t) −y3(t). What ODE does y(t) satisfy, and why?
29. Suppose that y1(t) =−2t + 3 solves ODE ¨y + 2˙y + 5y =−10t + 11 and that
y2(t) =−cos 2t −4 sin 2t solves ODE ¨y + 2˙y + 5y =−17 cos 2t. Find the general
solution of ¨y + 2˙y + 5y = t −11
10 + 2 cos 2t.
30. Given that y1(t) and y2(t) solve the same ODE (⋆) ¨y + p(t)˙y + q(t)y = et on an
interval I and that p(t) and q(t) are continuous on that interval.
(a) Which of the following is true? (a1) 2y1(t) −y2(t) is a solution of (⋆) on I, (a2)
2y1(t) −y2(t) is not a solution of (⋆) on I, or (a3) without more information, we
cannot tell whether (a1) or (a2) is true.
(b) Which of the following is true? (b1) c1y1(t) + c2y2(t) is a general solution of (⋆)
on I, (b2) c1y1(t) + c2y2(t) is not a general solution of (⋆) on I, or (b3) without
more information, we cannot tell whether (b1) or (b2) is true.
In problems 31 and 32, a function y(t) is given. Find at least two of the possible second-
order ODEs for which y(t) is one of the solutions.
31. y(t) = e−t + e−2t −e−3t
32. y(t) = e−t + te−t + 2e−2t
4.2 Forced Oscillations
A series RLC circuit with AC voltage source, V(t) = V0
ω sin ωt, where V0 is a constant, has
loop current I(t) satisfying Kirchhoff’s voltage equation,
LdI
dt + RI + 1
C q = V(t),
(4.6)
where the charge on the capacitor, q, satisfies ˙q = I. Assume that L, R, and C are, as usual,
constants. Differentiate (4.6) to get
L¨I + R˙I + 1
C I = V0 cos ωt.
(4.7)
As illustrated in Figure 4.3, essentially the same ODE can appear in vibrations where
there is a sinusoidal external force:
m¨y + b˙y + ky = f0 cos ωt,
(4.8)
where f0 is a constant.
We will see that the graphs of solutions of (4.8) take one of the shapes shown in
Figure 4.4. We will use the method of undetermined coefficients to solve (4.8) or (4.7).

274
Advanced Engineering Mathematics
y=0
y
m
y >0
b
f
k
FIGURE 4.3
Forced spring–mass–damper system.
y(t)
y(t)
y(t)
t
t
t
(a)
(b)
(c)
FIGURE 4.4
Forced oscillator examples: (a) pure resonance, (b) steady state, and (c) beats phenomenon.
4.2.1 The Resonance Case
Example 4.11
Solve the IVP
 ¨y + 4y = 5 sin 2t
y(0) = 0, ˙y(0) = 0

.
(4.9)
Method: The RHS function 5 sin 2t is in the zoo, corresponding to list of roots L2 = ±i2.
The corresponding LCCHODE, ¨y + 4y = 0, gives list of roots L1 = ±i2. The superlist
is L = ±i2, ±i2, so y(t) = c1 cos 2t + c2 sin 2t + c3t cos 2t + c4t sin 2t. Because yh(t) =
c1 cos 2t + c2 sin 2t, the correct form of a particular solution is
yp(t) = At cos 2t + Bt sin 2t,
where A, B are constants to be determined. Using the product and the chain rules, we
calculate
˙yp(t) = A(−2t sin 2t + cos 2t) + B(2t cos 2t + sin 2t)

Scalar ODEs II
275
and
¨yp(t) = A(−4t cos 2t −4 sin 2t) + B(−4t sin 2t + 4 cos 2t).
Substitute them into the original, nonhomogeneous ODE in (4.9) to get
0 · cos 2t + 5
: · sin 2t = ¨yp + 4yp
= A(((((
−4t cos 2t −4 sin 2t) + B(((((
−4t sin 2t + 4 cos 2t) + 4(((((
At cos 2t +
Bt sin 2t)
= 4B cos 2t + (−4A
::: ) sin 2t,
so 0 = 4B and 5 = −4A. The general solution of ODE (4.9) is
y(t) = yp(t) + yh(t) = −5
4 t cos 2t + c1 cos 2t + c2 sin 2t,
where c1, c2 are arbitrary constants. It’s useful to calculate that
˙y(t) = 5
2 t sin 2t −5
4 cos 2t −2c1 sin 2t + 2c2 cos 2t.
Substituting into the ICs gives 0 = y(0) = c1, 0 = ˙y(0) = −5
4 + 2c2. The solution of the
IVP is
y(t) = −5
4 t cos 2t + 5
8 sin 2t. ⃝
The graph of the solution is shown in Figure 4.5. The dashed lines y = ± 5
4 t give an
approximate “envelope” for the graph of the solution; we say “approximate” because the
graph escapes the envelope less and less as t →∞.
We can think of the particular solution yp(t) = −5
4 t cos 2t as being in the form
yp(t) = A(t) cos 2t,
where the time-dependent, growing amplitude is A(t) = 5
4 t.
y(t)
10
5
–5
20
40
60
80
t
–10
FIGURE 4.5
Example 4.11.

276
Advanced Engineering Mathematics
Note also that there is no transient solution, according to Definition 3.4 in Section 3.1,
even though the term
5
8 sin 2t becomes insignificant compared to the term y(t) =
−5
4 t cos 2t, for more and more values of t, as t →∞. This problem also does not have
a steady-state solution, according to Definition 3.6 in Section 3.1, because (i) there is no
transient solution and (ii) y(t) is not bounded as t →∞, due to unbounded oscillation.
In Example 4.11, there was no damping, that is, b = 0, and there was a perfect match of
the forcing frequency, ω = 2, and the natural frequency, ω0 =

k
m = 2. We call this the
resonance case. In general, this can be summarized as
The resonance case : b = 0 and ω = ω0.
In retrospect, when we did Example 4.1 in Section 4.1, we could have immediately known
that this was in the resonance case by looking at the ODE and noticing that b = 0 and
ω = ω0 = 1.
Physically, in the resonance case, the mechanical system has no friction that dissipates
energy, contrary to our experience in the real world, and the external force is continually
pumping in energy at just the right frequency to match the unforced, natural frequency.
An apocryphal story is that resonance was discovered when a Roman legion, marching
in perfect formation, set a bridge into vibration of sufficient amplitude to send it crashing
down.
In the real world, a mechanical system, and all but some superconducting electrical
systems, always have some dissipation of energy due to friction or electrical resistance.
Nevertheless, as we will see shortly, the system can have “practical resonance” solutions
that are mathematically bounded as t →∞but that get so large as to crash the system.
4.2.2 Steady-State Solution, Frequency Response, and Practical Resonance
As long as there is some damping, that is, b > 0, there cannot be pure resonance. Instead,
there can be a steady-state oscillation.
Example 4.12
Let m, b, k, f0 be unspecified constants, with m, b, k being positive. Find the steady-state
solution and its amplitude for the IVP
m¨y + b˙y + ky = f0 cos ωt
y(0) = y0, ˙y(0) = ˙y0

,
(4.10)
where ω is a positive constant.
Method: This example is general, that is, we don’t substitute in specific values for the
constant parameters m, b, k, f0. Nevertheless, we can use the method of undetermined
coefficients.
The RHS function f0 cos ωt is in the zoo, corresponding to list of roots L2 = ±iω. The
corresponding LCCHODE, m¨y + b˙y + ky = 0, gives list of roots L1 = s1, s2 where
s1, s2 = −b
2m ±

b2 −4mk
2m
.
The superlist is L = ±iω, s1, s2.

Scalar ODEs II
277
No matter whether the two roots s1, s2 are in the overdamped, critically damped, or
underdamped case, the assumption that b is positive implies that the solutions of the
corresponding homogeneous ODE go to zero as t →∞. Why? Specifically, in the over-
damped case, both of s1, s2 are real and negative. In the underdamped case, the solutions
are of the form y(t) = Aeαt cos(νt −δ), where A is a constant and α = −b
2m < 0. In the
critically damped case, solutions are of the form y(t) = (c1 +c2t)eαt, where α = −b
2m < 0,
so L’Hôpital’s rule implies that solutions go to zero as t →∞.
It follows that yh(t) →0 as t →∞, that is, yh(t) is a transient solution, according
to Definition 3.4 in Section 3.1. It also follows that the correct form of the particular
solution is
yp(t) = A cos ωt + B sin ωt,
where A, B are constants to be determined. We calculate
˙yp(t) = ω(−A sin ωt + B cos ωt)
and
¨yp(t) = −ω2(A cos ωt + B sin ωt).
Substitute them into the original, nonhomogeneous ODE in (4.10) to get
f0 · cos ωt + 0
: · sin ωt
= −mω2A cos ωt −mω2B sin ωt −bωA sin ωt + bωB cos ωt + kA cos ωt + kB sin ωt
=

(k −mω2
A + bωB) cos ωt +

(k −mω2
B −bωA)
::::::::::::::::: sin ωt,
so f0 = (k−mω2)A+bωB and 0 = (k−mω2)B−bωA. This can be written as a 2×2 system
⎧
⎨
⎩
(k −mω2)A
+
bωB
= f0
−bωA
+
(k −mω2)B
= 0
⎫
⎬
⎭,
whose solution is
A
B

=
k −mω2
bω
−bω
k −mω2
−1 f0
0

=
1
(k −mω2)2 + (bω)2
k −mω2
−bω
bω
k −mω2
 f0
0

.
So, a particular solution of the IVP is given by
yp(t) =
f0
(k −mω2)2 + (bω)2

(k −mω2) cos ωt + (bω) sin ωt

.
Earlier discussion explained that this is also the steady-state solution because yh(t) is a
transient solution, and the initial data y0, ˙y0 don’t affect this! The steady-state solution is
yS(t) = yp(t),
which has
Amplitude =

(k −mω2)f0
(k −mω2)2 + (bω)2
2
+

bωf0
(k −mω2)2 + (bω)2
2

278
Advanced Engineering Mathematics
=





(k −mω2)2 + (bω)2

(k −mω2)2 + (bω)22 f 2
0 ,
that is,
Amplitude =
| f0|

(k −mω2)2 + (bω)2 . ⃝
(4.11)
Again, the amplitude of the steady-state solution does not depend on the IC.
In retrospect, when we did Example 4.8 in Section 4.1, just from looking at the ODE,
we could have immediately known that this was in the steady-state solution case because
b = 2 > 0. It’s good to be able to know something about the solutions before finding them
in full detail.
In fact, let’s double-check our amplitude formula using that old example: m = 1, b = 2,
k = 9, f0 = 1, and ω = 2 should give
Amplitude =
1

(9 −4)2 + (2 · 2)2 =
1
√
41
,
(4.12)
which agrees with the steady-state solution found in that problem.
We can think of the forcing function, f0 cos ωt, as being the “input” for the mechanical
system, and the steady-state solution, yS(t), as being the “output,” depicted in Figure 4.6.
The ratio
G ≜Amplitude of the steady state
| f0|
=
1

(k −mω2)2 + (bω)2
(4.13)
can be called the frequency response, which measures the ratio of the strength of the
output to the strength of the input.
Shown in Figure 4.7a is a plot of the frequency response versus frequency for ODE
¨y + b˙y + 4y = cos ωt
(4.14)
for two values of b, specifically b = 0.1 and 0.4. Shown in Figure 4.7b is another plot of the
frequency response for ODE (4.14) for the values of b = 1, 4, 4
√
2, 8.
Example 4.13
Suppose that y(t) = 2e−t + 3 cos 2t −4 sin 2t is a solution of an IVP of the form
⎧
⎨
⎩
m¨y + b˙y + ky = f0 cos ωt
y(0) = y0, ˙y(0) = ˙y0
⎫
⎬
⎭.
(4.15)
f0 cos(ωt)
ys(t)
Mechanical system
FIGURE 4.6
Forced vibrations input–output relationship.

Scalar ODEs II
279
5
4
3
2
1
0
1
2
3
4
5
6
0.6
0.5
0.4
0.3
0.2
0.1
0
1
b = 8
b = 4
b = 1
2
ω
ω
G
G
3
4
5
6
(a)
(b)
FIGURE 4.7
Frequency response (a) b = 0.1, 0.4 and (b) b = 1, 4, 4
√
2, 8.
Assuming b > 0, find exact values of the constants m, b, k, f0, ω, y0, ˙y0.
Method: The initial data are easily found by substituting t = 0 into the given solution
to get
y0 = y(0) = 2 + 3 −0 = 5.
Substitute t = 0 into the derivative of given solution, ˙y(t) = −2e−t −6 sin 2t −8 cos 2t,
to get
˙y0 = ˙y(0) = −10.
The assumption that b is a positive constant implies that the solutions of the corre-
sponding homogeneous ODE are transient and that the parts of the given solution that
aren’t transient, that is, 3 cos 2t −4 sin 2t, must be a particular solution due to the RHS of
ODE (4.15), that is, f0 cos ωt. It follows that ω = 2.
While we could find the amplitude of the steady-state solution, yS(t) = 3 cos 2t −
4 sin 2t, and compare that with the formula for the amplitude given by (4.11), this would
not be the best thing to try: There would be too many constant parameters to solve for,
that is, m, b, k, f0, at the same time, and the formula is nonlinear in m, b, k.
Instead, we can first substitute the transient solution, yT(t) = 2e−t, into the corre-
sponding homogeneous ODE, m¨y + b˙y + ky = 0, to get
0 = m d2
dt2

yT

+ b d
dt

yT

+ kyT = m d2
dt2

2e−t 
+ b d
dt

2e−t 
+ k2e−t = 2e−t(m −b + k);
hence, we have a linear algebraic equation:
m −b + k = 0.
(4.16)
Also, use our first inference that ω = 2 and substitute the steady-state solution, yS(t) =
3 cos 2t −4 sin 2t, into the original, nonhomogeneous ODE m¨y + b˙y + ky = f0 cos ωt to get
f0 cos 2t + 0
: · sin 2t = m ¨yS + b ˙yS + kyS
= −4m(3 cos 2t −4 sin 2t) + 2b(−3 sin 2t −4 cos 2t) + k(3 cos 2t −4 sin 2t)
= (−12m −8b + 3k) cos 2t + (16m −6b −4k)
::::::::::::: sin 2t;

280
Advanced Engineering Mathematics
hence,
−12m −8b + 3k = f0
(4.17)
and
16m −6b −4k = 0.
(4.18)
Equations (4.16) through (4.18) are three equations in four unknowns, m, b, k, f0. On gen-
eral principle, that would seem to be not enough equations, and that is true! If we look
at the ODE in the IVP, we see that letting m = 1 wouldn’t lose all of our ability to find a
solution for the parameters. We don’t need to find all the ODEs, just one.
We can rewrite the three (4.16) through (4.18), after substituting in m = 1, as
⎧
⎨
⎩
−b
+k
=
−1
−8b
+3k
−f0
=
12
−6b
−4k
=
−16
⎫
⎬
⎭.
(4.19)
We could use the inverse of a 3 × 3 matrix to solve for
⎡
⎣
b
k
f0
⎤
⎦,
but it is easier and more reliable to first find the solution for b, k to satisfy the first and
third equations of (4.19), which don’t involve f0:
b
k

=
−1
1
−6
−4
−1  −1
−16

= 1
10
−4
−1
6
−1
  −1
−16

=
2
1

;
hence, b = 2, k = 1. After that, substitute into the second of the equations in (4.19), to get
f0 = −12 −8b + 3k = −12 −8(2) + 3(1) = −25.
To summarize, the exact values are, assuming m = 1,
m = 1, b = 2, k = 1, f0 = −25, ω = 2, y0 = 5, and ˙y0 = −10. ⃝
Define ζ ≜|ω −ω0|/2, a so-called de-tuning parameter. We see from formula (4.13) that
we get a very large frequency response, that is, ratio of the steady-state output magnitude
to the forcing function input magnitude, when 0 < b ≪1, that is, b is positive but very
small, and ζ is also very small. This can be called practical resonance.
The practical resonance case : 0 < b ≪1 and 0 ≤ζ ≪1.
Example 4.14
If the damping is very small, the solution can look a lot like the pure resonance case for
quite a long time! We illustrate this with Figure 4.8’s graph of the solution of the IVP
¨y + 0.04 ˙y + 4y = cos 2t
y(0) = 0, ˙y(0) = 0

.
(4.20)

Scalar ODEs II
281
10
5
–5
20
y
40
60
80
100
t
–10
FIGURE 4.8
Example 4.14.
4.2.3 Maximum Frequency Response
In the apocryphal story, when the Roman legion marched in perfect formation over a
bridge and the resulting vibrations sent the bridge crashing down, there was damping or
friction in the system. The oscillations do not become unbounded, but when the magnitude
of the solution becomes sufficiently large, then the system exceeds its design specifications
enough to crash it.
Similarly, an airplane rudder can fail if its oscillations become sufficiently large, even
though there is some damping in the system. But, to be honest, in the real world, the mod-
els are much more complicated than we have discussed so far, because there is nonlinearity
in the ODEs. We will return to this issue in Chapter 18.
Also, the models at least start with nonlinear partial differential equations, although
mathematical analysis may reduce the model to nonlinear ODEs.
Engineers interested in design problems care about optimization, that is, maximizing
or minimizing some design objective. In this situation, one natural objective would be to
enforce a limit on how large the steady-state oscillation can be, for example, in order to
avoid having a bridge fall down, an electrical device “burn” out, or an airplane rudder
jam in one position. While real-world problems often involve nonlinearity, sometimes we
can get some insight into these phenomena by seeing the maximum magnitude of the
frequency response of the steady-state oscillation.
Recall from (4.13) that the frequency response is
G = G(ω) =
1

(k −mω2)2 + (bω)2 .
Some of the graphs of frequency response in Figure 4.7 showed G(ω) achieved a global
maximum value.
In calculus I, we learned how to find the global maximum value of G = G(ω) as a
function of ω in the interval I ≜(0, ∞), that is, where 0 < ω < ∞.

282
Advanced Engineering Mathematics
Theorem 4.3
Suppose a function g = g(x) is defined and differentiable for x in the interval (0, ∞). If g′(x)
is positive for 0 < x < x⋆and g′(x) is negative for x⋆< x < ∞, then the global maximum
value of g on (0, ∞) is
gmax = g(x⋆).
In our situation, it is easier to deal with the derivative of the square of G(ω) rather than
G(ω) itself. We had another relevant result in calculus I:
Theorem 4.4
Suppose a function g = g(x) is defined and differentiable for x in an interval I. If g(x) ≥0
for all x in the interval I and f(x) ≜

g(x)
2 has global maximum value f(x∗) on I, then
g(x) has global maximum value g(x∗) on I.
Define
f(ω) ≜(G(ω))2 =
1
(k −mω2)2 + (bω)2 .
Using the chain rule, we calculate
f ′(ω) = df
dω =
−1

(k −mω2)2 + (bω)22 · d
dω

(k −mω2)2 + (bω)2
= −4mω(k −mω2) + 2b2ω

(k −mω2)2 + (bω)22 .
In order to know where f ′ is positive or negative, it would help to know where f ′ = 0, that
is, where the numerator of f ′ is zero:
0 = −4mω(k −mω2) + 2b2ω = 2ω

−2m(k −mω2) + b2
= 2ω

2m2ω2 −2mk + b2
.
So, f ′ = 0 at
ω = 0, ω = ±

k
m −b2
2m2 ,
where we assume that k
m −b2
2m2 > 0, that is, b2 < 2mk. Recall from Section 3.3 that the critical
damping is bcrit ≜
√
4mk for the corresponding unforced system. Note that b2 < 2mk is
equivalent to the damping being less than 1
√
2
times the critical damping.

Scalar ODEs II
283
We are only concerned with ω > 0. Recall the definition ω0 ≜

k
m. Define
ω∗≜

ω2
0 −b2
2m2 .
The numerator of f ′, that is,
2ω

2m2ω2 −2mk + b2
= 4m2ω

ω2 −k
m + b2
2m2
 
= 4m2ω

ω2 −(ω∗)2
,
is positive for 0 < ω < ω∗and is negative for ω < ω∗< ∞, as long as b2 < 2mk. It follows
from Theorem 4.3 that fmax = f(ω∗), so Theorem 4.4 implies that the maximum frequency
response is
Gmax = G(ω∗) =
1

(k −m(ω∗)2)2 + (bω∗)2 .
In Problem 4.2.5.27, you will substitute ω∗≜

ω2
0 −
b2
2m2 into Gmax and get the next result.
Theorem 4.5
Suppose that the damping satisfies b2 < 2mk, that is, 0 < b <
1
√
2 bcrit. Then the maximum
frequency response is
Gmax = 1
νb,
where we recall that
ν ≜

4mk −b2
2m
=

k
m −b2
4m2
is the quasi-frequency for the corresponding unforced system.
On the other hand, if b is greater than or equal to
1
√
2 times the critical damping for the
corresponding unforced system, then you will explain in Problem 4.2.5.28 why G′(ω) < 0
for all ω > 0; hence, limω→0+ G(ω) = 1
k is the maximum frequency response.
4.2.4 Beats Phenomenon, Fast and Slow Frequencies, and Frequency Response
The third basic phenomenon occurs when there is zero damping, that is, b = 0, but the two
frequencies do not match, that is, ω ̸= ω0.

284
Advanced Engineering Mathematics
Example 4.15
Solve the IVP
¨y + 4y = 3 cos(
√
5 t)
y(0) = 0, ˙y(0) = 0

(4.21)
and write the solution in the form of y(t) = A(t)· (a sinusoidal function).
Method: The RHS function 3 cos(
√
5 t) is in the zoo, corresponding to list of roots L2 =
±i
√
5. The corresponding homogeneous ODE, ¨y + 4y = 0, gives list of roots L1 = ±i2
and homogeneous solution y(t) = c1 cos 2t + c2 sin 2t. The superlist is L = ±i
√
5, ±i2, so
y(t) = c1 cos 2t+c2 sin 2t+c3 cos(
√
5 t)+c4 sin(
√
5 t), and the correct form of a particular
solution is
yp(t) = A cos(
√
5 t) + B sin(
√
5 t),
where A, B are constants to be determined.
Substitute ¨yp(t) into the original, nonhomogeneous ODE to get
3 · cos(
√
5 t) + 0
: · sin(
√
5 t) = ¨yp + 4yp =
= −5

A cos(
√
5 t) + B sin(
√
5 t)

+ 4

A cos(
√
5 t) + B sin(
√
5 t)

= −A cos(
√
5 t) + (−B
:: ) sin(
√
5 t);
hence, −A = 3, −B = 0 and
yp(t) = −3 cos(
√
5 t).
The general solution of ODE (4.21) is
y(t) = yp(t) + yh(t) = −3 cos(
√
5 t) + c1 cos 2t + c2 sin 2t,
where c1, c2 are arbitrary constants. Substitute this and its derivative into the ICs to get
0 = y(0) = −3 + c1
and
0 = ˙y(0) = 0 + 2c2.
The solution of the IVP is
y(t) = −3 cos(
√
5 t) + 3 cos 2t.
Using one of the less well-known trigonometric identities, namely, the difference of
cosine functions identity
−cos α + cos β = 2 sin
α −β
2

sin
α + β
2

,
we can rewrite the solution of the IVP in the form
y(t) = 6 sin
√
5 t −2t
2
 
sin

2t +
√
5 t
2
 
= A(t) sin

(
√
5 + 2)t
2
 
,
(4.22)
where the slowly varying amplitude is
A(t) ≜6 sin

(
√
5 −2)t
2
 
. ⃝

Scalar ODEs II
285
2
1
–1
20
y(t)
40
60
80
100
t
–2
FIGURE 4.9
Example 4.15.
Another correct final conclusion for Example 4.15 would be
y(t) = ¯A(t) sin

(
√
5 −2)t
2
 
, where ¯A(t) = 6 sin

(
√
5 + 2)t
2
 
.
(4.23)
We prefer (4.22) to (4.23) for the graphical reason that the former has a slowly varying
amplitude rather than the quickly varying amplitude in the latter.
The dashed curves y = ±A(t) = ±6 sin

(
√
5−2)t
2

in Figure 4.9 give an “envelope” for the
graph of the solution.
The solution (4.22) has two frequencies, a slow-frequency
ζ ≜
√
5 −2
2
,
which is the same de-tuning parameter we mentioned in the steady-state oscillation case
before Example 4.14, and a fast-frequency
β ≜
√
5 + 2
2
.
The beats phenomenon consists of fast oscillations, sin βt, within an envelope of slow
oscillations, for example, y = ±6 sin ζt. The slow frequency is sometimes called the “fre-
quency of the beats.” It is this slow frequency that a piano tuner uses to estimate how
much to raise or lower the tension on a piano string in order to have it calibrated with a
standard frequency, for example, that of a tuning fork.
More generally, the solution of an IVP
⎧
⎨
⎩
¨y + ω2
0y = f0 cos ωt
y(0) = y0, ˙y(0) = ˙y0
⎫
⎬
⎭

286
Advanced Engineering Mathematics
can be written in the form
y(t) = a0 cos(ω0t −δ0) + a1 cos(ωt −δ1).
(4.24)
In general, it is not possible to use a trigonometric identity to rewrite y(t) as a product
of a fast oscillation with an envelope of slow oscillations. In general, the solution given by
(4.24) is either a “quasiperiodic” function or a “periodic” function, depending upon the
relationship between the forcing frequency, ω, and the natural frequency, ω0.
Definition 4.2
(a) A function y(t) is periodic if there is a positive constant T such that y(t + T) ≡y(t)
at all t in the domain of y. [Pictorially, a function is periodic if its graph is not
changed when you shift it horizontally by T units.]
(b) A function y(t) is quasiperiodic if it is not periodic, but it is the sum of two (or any
finite number of) periodic functions.
A quasiperiodic function must involve more than one frequency; otherwise, it would
automatically be periodic.
Unfortunately, there is no simple graphical interpretation of the concept of quasiperi-
odicity, although there are sophisticated interpretations involving all the horizontal shifts
of a quasiperiodic function. There is an even more general concept of so-called “almost
periodic” functions, and they show up in the general theory of ODEs but involve more
advanced mathematics that we won’t discuss.
A function of the form
y(t) = a0 cos(ω0t −δ0) + a1 cos(ωt −δ1)
(4.25)
is periodic if the ratio ω
ω0 is a rational number p
q, that is, the ratio of two integers, in which
case, T ≜2πp
ω is a period. For example,
y(t) = cos

3
√
2 t −7
4

+ 8 cos(5
√
2t)
is periodic with period T = 2π5
5
√
2 = π
√
2 because ω
ω0 = 5
√
2
3
√
2 = 5
3 = p
q and
y(t + π
√
2) = cos

3
√
2 (t + π
√
2) −7
4

+ 8 cos

5
√
2(t + π
√
2)

= cos

3
√
2 t + 6π −7
4

+ 8 cos

5
√
2t + 10π

= cos

3
√
2 t −7
4

+ 8 cos(5
√
2t) ≡y(t).
While it is not possible to say much that is exactly correct about solutions with the beats
phenomenon in the quasiperiodic case, a solution in the form (4.25) has crude bounds on
its amplitude: Because | cos θ | ≤1 for all θ and there is an inequality for real numbers
called the triangle inequality, that is, |a + b| ≤|a| + |b|, we have
|y(t)| ≤|a0| + |a1|.

Scalar ODEs II
287
In addition, for some time interval, the solution will behave as if it has slow-frequency
ζ ≜|ω −ω0|
2
and fast-frequency
β ≜ω + ω0
2
,
say for a time interval of length three of the slow periods.
4.2.5 Problems
1. Solve ¨y + 4y = −3 cos 2t.
In problems 2–4, solve the IVP.
2. ¨y + 9y = 5 sin 3t, y(0) = ˙y(0) = 0
3. ¨y + 16y = 4 cos 4t, y(0) = −1, ˙y(0) = 3
4. ¨y + 8y = 5 sin 3t, y(0) = ˙y(0) = 0
In problems 5–8, find the steady-state solution, express it in the amplitude-phase form,
and state its amplitude.
5. ¨y + 2˙y + 2y = sin t
6. ¨y + ˙y + 5y = cos 2t
7. ¨y + 2˙y + 3y = sin 2t
8. ¨y + 2˙y + y = 4 sin 3t
In problems 9–12, find the solution and the steady-state solution, and express the latter in
the amplitude-phase form and state its amplitude.
9. ¨y + 2˙y + 10y = 74 cos 3t, y(0) = −1, ˙y(0) = 2
10. ¨y + 4˙y + 5y = sin πt, y(0) = ˙y(0) = 0
11. ¨y + 2˙y + 6y = sin 2t, y(0) = −3, ˙y(0) = 3 −3
√
5
12. ¨y + ˙y + 3y = sin 2t, y(0) = 1, ˙y(0) = 0
In problems 13 and 14, for the IVP, find (a) the frequency of the beats and (b) the maximum
amplitude of the motion.
13. ¨y + 9y = 5 cos(
√
8 t), y(0) = ˙y(0) = 0
14. ¨y + 5y = 3 cos 2t, y(0) = ˙y(0) = 0
15. Find the steady-state solution for m¨y+b˙y+ky = f0 sin ωt. Show all work, and leave
your final conclusion in terms of the unspecified physical parameters m, b, k, f0, ω,
that is, do not substitute in specific values for them.

288
Advanced Engineering Mathematics
–3
3
2
1
–1
2
y(t)
4
6
8
10
12
t
–2
FIGURE 4.10
Problem 4.2.5.16
2
1
2
y(t)
4
6
8
10
12
t
–1
–2
FIGURE 4.11
Problem 4.2.5.17.
16. Shown in Figure 4.10 is the graph of a solution of ¨y + 6˙y + ky = 22 cos ωt.
Determine (a) ω, (b) the amplitude of the steady-state solution, and (c)
√
k, the
natural frequency of the system, using parts (a) and (b). For each part, cite specific
information from the graph to help justify your conclusion(s).
17. Shown in Figure 4.11 is the graph of a solution of ¨y + 4˙y + ky = 11 cos ωt.
Determine (a) ω, (b) the amplitude of the steady-state solution, and (c)
√
k, the
natural frequency of the system, using parts (a) and (b). For each part, cite specific
information from the graph to help justify your conclusion(s).
18. Shown in Figure 4.12 are graphs of solutions of two different ODEs of the form ¨y+
b˙y + y = cos(ω2t). Decide which of the pairs of parameter values could conceivably
give the graphs, and explain why. More than one pair may be correct. (a) b1 = 0.5
and ω1 =
√
2, (b) b2 = 0.5 and ω2 =
√
2, (c) b1 = 0.5 and ω1 = 1, (d) b2 = 0 and
ω2 = 1, (e) b1 = 1 and ω1 =
√
2, and (f) b2 = 0 and ω2 =
√
2.

Scalar ODEs II
289
y(t)
t
y(t)
t
y+ b1y+ y = cos(ω1t)
..
.
y+ b2y + y = cos(ω2t)
..
.
FIGURE 4.12
Problem 4.2.5.18.
19. For ODE ¨y+2d˙y+y = cos t, let d be a positive, but adjustable, parameter. Describe
the behavior of the steady-state solution as d →0+. Interpret this in regard to the
concepts of pure versus practical resonance.
20. The loop current in a series LC circuit with a voltage source satisfies L¨I + 1
C I =
f0 sin(120πt). If L = 10−3 H and the constant C is measured in farads, for which
value(s) of C does the circuit exhibit
(a) Pure resonance?
(b) The beats phenomenon? In that case, find the slow, beats frequency in terms
of C.
21. A front-loading washing machine is mounted on a rubber pad that acts like a
spring; the weight of the machine depresses the pad by exactly 0.1 in. When the
machine spins at
ω
2π revolutions per second, the rotor exerts a vertical force of
f0 cos ωt pounds on the machine. At what speed, in revolutions per second, will
resonance vibrations occur, assuming we can neglect damping and g ≈32 ft/s2?
22. An oscillator problem ¨y + b˙y + ky = f0 cos ωt has a solution y(t) = e−t cos 3t +
3 cos 2t+2 sin 2t. State the exact values of b, k, ω, and explain the reasoning process
you used to find them.∗
23. An oscillator problem m¨y + b˙y + ky = f0 cos ωt has a solution y(t) = 2t sin 2t +
5 cos(2t −δ), for some constant δ. Assume the spring constant k is 6 N/m. State the
exact values of m, b, f0, ω, and explain the reasoning process you used to find them.
24. An oscillator problem ¨y+b˙y+ky = f0 cos ωt has a solution y(t) = 2 sin 3t−cos 3t−
1
4t sin 3t. State the exact values of b, k, f0, ω, and explain the reasoning process you
used to find them.
25. Suppose y(t) satisfies a spring, mass, and possibly damped, ODE of the form
¨y+b˙y+9y = f0 cos ωt. For each of parts (a), (b), (c), the behavior of y(t) is described,
and you are asked to give a specific numerical value for each of the physical param-
eters b, f0, ω that can produce such a solution y(t). For convenience, assume f0 ≥0
and ω > 0.
(a) y(t) = (steady-state solution) + (transient solution).
(b) y(t) exhibits pure resonance.
(c) y(t) exhibits the beats phenomenon.
∗Problems 22–24 were inspired by problems in Farlow et al. (2007).

290
Advanced Engineering Mathematics
26. The following is a model (Bearman, 1984) for oscillations of a rigid 2D bluff body
placed normal to fluid flow and mounted on springs. The body is subject to a
transverse fluid force due to the body’s shedding of vortices and viscous-type
damping force associated with the springs and their mounting:
m¨y + 4πmn0δs˙y + 4π2mn2
0y = 1
2CyϱU2D sin(2πnνt + ϕ)
where ϱ is the fluid density, the fluid speed is U, and Cy is a coefficient mea-
suring the transverse force of the vortices. D is a characteristic dimension of the
body, usually the body width; k is the spring constant; m is the mass of the body;
β = 4πδsmn0 is the damping coefficient; δs is the fraction of critical damping; and
the natural (unforced, undamped) frequency in Hz is n0 =
1
2π

k
m.
Substitute into the ODE a steady-state solution in the form yp = y sin(2πnνt)
without the phase angle ϕ because it is assumed that the fluid force “leads” the
oscillation. Use the trigonometric identity sin(α + ϕ) = sin α cos ϕ + cos α sin ϕ on
the RHS of the ODE to help explain why
n0
nν
=

1 −Cy
4π2 cos ϕ

ϱD2
2m
  U
n0D
2y
D
−1 −1/2
and
y
D = Cy
8π2 sin ϕ

ϱD2
2mδs
  U
n0D
2 n0
nν
.
This explains why y, the amplitude of the response, depends on the phase angle
ϕ, which in turn depends on the forcing frequency, nν, in Hz.
27. Suppose that the damping b2 < 2mk. Substitute ω∗≜

ω2
0 −
b2
2m2 into Gmax and
simplify it to find the nicer looking result that the maximum frequency response
is Gmax = 1
νb, the conclusion of Theorem 4.5.
28. Suppose that the damping b2 ≥2mk. (a) Explain why (a) dG
dω < 0 for all ω > 0, (b)
calculate limω→0+ G(ω), and (c) explain why the maximum frequency response is
1
k but is not achieved for any ω > 0.
29. Throughout this problem, suppose that y(t) = 1.5 + 0.75 cos(t −π
3 ) −0.5e−t/5 and
ω0, δ, γ , α, ε, and η are constants.
(a) Find a nonhomogeneous ODE of the form ¨y + ω2
0y = δ + γ e−αt that has y(t) as
a solution.
(b) Find a nonhomogeneous ODE of the form (D2 + ω2
0)(D + ε)[ y ] = η that has
y(t) as a solution.
(c) Find a homogeneous ODE that has y(t) as a solution.
30. Throughout this problem, suppose that the graph of y(t) is drawn in Figure 4.13
and ω0, δ, γ , ε, and η are constants. Assume y(t) satisfies the ICs y(0) =
˙y(0) = 1.

Scalar ODEs II
291
3.0
y(t)
t
2.5
2.0
1.5
1.0
0.5
5
10
15
20
FIGURE 4.13
Problem 4.2.5.30.
(a) Find a nonhomogeneous ODE of the form ¨y + ω2
0y = δ + γ e−t/2 that has y(t) as
a solution.
(b) Using the y(t) you found to help in solving part (a), find a nonhomogeneous
ODE of the form (D2 + ω2
0)(D + ε)[ y ] = η that has y(t) as a solution.
(c) Find a homogeneous ODE that has y(t) as a solution.
31. Find the steady-state solutions for (a) ˙y + δy = f0 cos ωt, (b) ˙y + δy = f0 sin ωt, and
(c) ˙y + δy = f0 · (a cos ωt + b sin ωt).
For each of (a), (b), and (c), show all work, and leave your final conclusion in terms
of the unspecified parameters δ, f0, ω, and, for (c), a, b, that is, do not substitute in
specific values for them.
32. [Small project] Find the frequency response, function for ˙y + δy = f0 cos ωt in
terms of the unspecified parameters δ, f0, ω. Get a result(s) concerning maximum
frequency response such as we did in work preceding Theorem 4.5.
33. [Small project] Find the frequency response function for ˙y + δy = f0 sin ωt in
terms of the unspecified parameters δ, f0, ω. Get a result(s) concerning maximum
frequency response such as we did in work preceding Theorem 4.5.
4.3 Variation of Parameters
The method of variation of parameters will be our second method for solving ODEs that
are nonhomogeneous. Suppose our ODE can be written in the form
L[ y ] = f(t),
where L is a second-order linear differential operator (LD-op). It is legitimate to use Section
4.1’s method of undetermined coefficients if L is a constant coefficients LD-op and f is a

292
Advanced Engineering Mathematics
linear combination of functions in the zoo of solutions of LCCHODEs. In this section, we
will learn a more general method.
In this section, we will work only with second-order nonhomogeneous ODEs. If an ODE
is of higher order, it is solvable using a generalization of the method to systems of ODEs
that we will develop in Chapter 5.
By the way, in the original, 1951, version of the movie “The Day the Earth Stood
Still,” Klaatu, an extra terrestrial from a civilization far more advanced than ours, finds
a scientist, Prof. Jacob Barnhardt, he can confide in. Klaatu helps the professor with a
tough problem and finishes his comments with, “With variation of parameters, this is the
answer.”∗
Let’s start with an example we could have done in Section 4.1 but would have been
tedious if solved by the method of undetermined coefficients. After we do this example,
we will develop the new method in general.
Example 4.16
Solve the ODE by the method of variation of parameters developed as follows:
¨y + 5˙y + 6y = t2e−t.
(4.26)
Method: The corresponding homogeneous ODE, ¨y + 5˙y + 6y = 0, has characteristic
polynomial s2 + 5s + 6 = (s + 2)(s + 3), so the general solution is
y(t) = c1e−2t + c2e−3t.
Let’s try to find a solution of the nonhomogeneous ODE (4.26) in the form
y(t) = v1(t)e−2t + v2(t)e−3t,
(4.27)
where v1(t), v2(t) are functions to be determined later. The idea of replacing constants
c1, c2 by functions v1(t), v2(t) is what we mean by variation of parameters, also known
as variation of constants. The motivation for trying this comes from success we had in
finding a second solution of the form y2(t) = v(t)eαt in the critically damped case in
Section 3.3.
To substitute (4.27) into ODE (4.26), we will need to take two derivatives of y(t). Using
the product rule, we get
˙y(t) = ˙v1(t)e−2t + ˙v2(t)e−3t −2e−2tv1(t) −3e−3tv2(t).
If we were to take another derivative, we would get both ¨v1(t) and ¨v2(t) appearing in
what we would need to solve, and it sounds like a bad idea to replace one ODE involving
one second derivative with a problem involving two second derivatives.
A couple of centuries ago, Joseph Louis Lagrange figured out how to make things
work better. His idea was to make sure that ¨v1(t), ¨v2(t) wouldn’t be needed, by later
enforcing the requirement that
˙v1(t)e−2t + ˙v2(t)e−3t ≡0.
(4.28)
Assuming that ˙v1, ˙v2(t) can be made to satisfy (4.28), we have
˙y(t) = −2e−2tv1(t) −3e−3tv2(t),
∗Between the 41st and the 42nd minutes of the film.

Scalar ODEs II
293
and thus,
¨y(t) = −2e−2t ˙v1(t) −3e−3t ˙v2(t) + 4e−2tv1(t) + 9e−3tv2(t).
Substituting all of that into the original, nonhomogeneous ODE (4.26), we get
t2e−t = ¨y + 5˙y + 6y
= −2e−2t ˙v1(t) −3e−3t ˙v2(t) +

4e−2tv1(t) +
9e−3tv2(t) + 5(
−2e−2tv1(t) −

3e−3tv2(t))
+ 6(
v1(t)e−2t +
v2(t)e−3t),
that is,
−2e−2t ˙v1(t) −3e−3t ˙v2(t) = t2e−t.
(4.29)
It turns out that having so many terms cancel is no accident, as we will see when we
discuss the method in general.
So, ˙v1(t), ˙v2(t) should satisfy both (4.28) and (4.29), that is, should satisfy the system of
linear equations
⎧
⎨
⎩
e−2t ˙v1(t) + e−3t ˙v2(t)
= 0
−2e−2t ˙v1(t) −3e−3t ˙v2(t)
= t2e−t
⎫
⎬
⎭.
(4.30)
Using the inverse of a 2 × 2 matrix, we get
⎡
⎣
˙v1
˙v2
⎤
⎦=
⎡
⎣
e−2t
e−3t
−2e−2t
−3e−3t
⎤
⎦
−1 ⎡
⎣
0
t2e−t
⎤
⎦=
1
−e−5t
⎡
⎣
−3e−3t
−e−3t
2e−2t
e−2t
⎤
⎦
⎡
⎣
0
t2e−t
⎤
⎦=
⎡
⎣
t2et
−t2e2t
⎤
⎦.
We obtain, using integration by parts twice,
v1(t) =

˙v1(t)dt =

t2etdt = · · · = t2et −2tet + 2et + c1,
and similarly,
v2(t) =

˙v2(t)dt = −

t2e2tdt = · · · = −1
2t2e2t + 1
2te2t −1
4e2t + c2,
where c1, c2 are arbitrary constants.
Putting everything together, we get solutions
y(t) = v1(t)e−2t + v2(t)e−3t =

(t2 −2t + 2)et + c1

e−2t +

−1
2t2 + 1
2t −1
4

e2t + c2

e−3t
=

t2 −2t + 2 −1
2t2 + 1
2t −1
4

e−t + c1e−2t + c2e−3t.
By Theorem 4.1 in Section 4.1, the general solution of nonhomogeneous ODE (4.26) is
y(t) =
1
2 t2 −3
2 t + 7
4

e−t + c1e−2t + c2e−3t,
(4.31)
where c1, c2 are arbitrary constants. ⃝
In (4.31), we have both yp(t), a particular solution, as well as yh(t), a general solution of
the corresponding homogeneous ODE. The method of variation of parameters automati-
cally gives the general solution all at once, that is, we don’t need to add yh(t) to a yp(t).

294
Advanced Engineering Mathematics
Also, if we are only interested in a particular solution, we can substitute c1 = c2 = 0 into
(4.31) to find a yp(t).
In Problem 4.3.2.21, you will use the method of Section 4.1, that is, the method of unde-
termined coefficients, to solve (4.26). This will both check our conclusion in (4.31) and
also give us an opportunity to decide whether, in this example, the method of undeter-
mined coefficients would have been easier or more reliable than the method of variation
of parameters.
In general, we should use the method of undetermined coefficients if it is legitimate to
do so and the RHS of the nonhomogeneous ODE looks simple.
In general, to solve
L[y] ≜¨y + p(t)˙y + q(t)y = f(t),
that is, where L is a second-order LD-op in standard form, here is the
4.3.1 Method of Variation of Parameters
Step 1. Find {y1(t), y2(t)}, a complete set of basic solutions of ¨y + p(t)˙y + q(t)y = 0, that is,
the corresponding linear homogeneous ODE in standard form.
Step 2. Let
y(t) = y1(t)v1(t) + y2(t)v2(t),
(4.32)
where v1(t), v2(t) are functions to be found later. Assume that
y1(t)˙v1(t) + y2(t)˙v2(t) ≡0,
(4.33)
and use that to calculate ˙y(t) and then ¨y(t) from (4.32).
Step 3. Substitute y, ˙y, ¨y into the original nonhomogeneous ODE ¨y + p(t)˙y + q(t)y = f(t),
which is in standard form. Almost all of the terms cancel, leaving
˙y1(t)˙v1(t) + ˙y2(t)˙v2(t) = f(t).
(4.34)
Step 4. Solve the system consisting of (4.33) and (4.34), that is,
⎧
⎨
⎩
y1(t)˙v1(t) + y2(t)˙v2(t)
= 0
˙y1(t)˙v1(t) + ˙y2(t)˙v2(t)
= f(t)
⎫
⎬
⎭,
(4.35)
for ˙v1(t), ˙v2(t).
Step 5. Integrate ˙v1(t) and ˙v2(t) with respect to t to get v1(t) and v2(t), including arbitrary
constants c1, c2, and then substitute them into (4.32) to find the general solution of the
nonhomogeneous ODE, ¨y + p(t)˙y + q(t)y = f(t). ⃝

Scalar ODEs II
295
To solve system (4.35), we use the inverse of a 2 × 2 matrix in
˙v1
˙v2

=
y1(t)
y2(t)
˙y1(t)
˙y2(t)
−1  0
f(t)

,
(4.36)
so we need that the Wronskian determinant, W(y1, y2)(t), never be zero. But Theorem 3.13
in Section 3.3 says that it would follow from the assumption we made that {y1(t), y2(t)} is a
complete set of basic solutions of the corresponding linear homogeneous ODE, ¨y + p(t)˙y +
q(t)y = 0!
We claim that if we assume that both y1(t) and y2(t) are solutions of L[y] = 0 and we
assume that y1(t)˙v1(t) + y2(t)˙v2(t) ≡0, then almost all of the terms cancel, leaving (4.34).
Why? Because
L[v1(t)y1(t) + v2(t)y2(t)]
d2
dt2

v1(t)y1(t) + v2(t)y2(t)

+ p(t) d
dt

v1(t)y1(t) + v2(t)y2(t)

+ q(t)

v1(t)y1(t) + v2(t)y2(t)

= ˙y1(t)˙v1(t) + ˙y2(t)˙v2(t) + ¨y1(t)˙v1(t) + ¨y2(t)˙v2(t) + p(t)
˙y1(t)v1(t) + ˙y2(t)v2(t)

+ q(t)(v1(t)y1(t) + v2(t)y2(t))
= ˙y1(t)˙v1(t) + ˙y2(t)˙v2(t) + v1(t)
¨y1(t) + p(t)˙y1(t) + q(t)y1(t)

+ v2(t)
¨y2(t) + p(t)˙y2(t) + q(t)y2(t)

= ˙y1(t)˙v1(t) + ˙y2(t)˙v2(t) + 0 + 0,
that is, (4.34).
Example 4.17
(Rotating disks of constant thickness) (Ugural and Fenster, 2003). Solve the ODE
r2 d2u
dr2 + r du
dr −u = −(1 −ν2)ϱ ω2r3
E
,
(4.37)
where u = u(r) is the deformation, also known as the displacement, from the refer-
ence state, and mass density ϱ, angular speed ω, Young’s modulus of elasticity E, and
Poisson’s ratio ν are constants.
Method: First, we put the ODE into the standard form by dividing through by r2:
L[u] ≜d2u
dr2 + r−1 du
dr −r−2u = −η r,
where the constant η ≜(1 −ν2)ϱω2/E. The method of undetermined coefficients, in
Section 4.1, doesn’t apply to this ODE because the LD-op L does not have constant
coefficients. The only method we can use is the method of variation of parameters.
The corresponding homogeneous ODE, d2u
dr2 + r−1 du
dr −r−2u = 0, is the same one
we studied in Example 3.33 in Section 3.5: Substituting in u(r) = rn yields characteristic

296
Advanced Engineering Mathematics
polynomial n(n−1)+n−1 = n2−1 = (n+1)(n−1), so the functions {u1(r) = r, u2(r) = r−1}
are a complete set of basic solutions. The method of variation of parameters says to try
solutions in the form u(r) = rv1(r)+r−1v2(r), where v1(r), v2(r) should satisfy the system
(4.35). Denoting d
dr =′, this is
⎧
⎨
⎩
r v′
1(r) + r−1 v′
2(r)
= 0
v′
1(r) −r−2 v′
2(r)
= −η r
⎫
⎬
⎭.
Using the inverse of a 2 × 2 matrix, we get
v′
1
v′
2

=
r
r−1
1
−r−2
−1 
0
−η r

=
1
−2r−1

−r−2
−r−1
−1
r
 
0
−η r

=
−1
2η r
1
2η r3

.
We get
v1(r) =

v′
1(r)dr =
 
−1
2η r

dr = −1
4η r2 + c1
and
v2(r) =

v′
2(r)dr =
 1
2η r3dr = 1
8η r4 + c2,
where c1, c2 are arbitrary constants.
Putting everything together, we get solutions
u(r) = rv1(r) + r−1v2(r) = · · · = −η
8 r3 + c1r + c2r−1.
Putting back in the definition of η, the general solution of the nonhomogeneous ODE
(4.37) is
u(r) = −(1 −ν2)ϱ ω2
8E
r3 + c1r + c2r−1,
where c1, c2 are arbitrary constants. ⃝
Example 4.18
Solve the IVP
 ¨y + 2˙y + y =
√
t e−t
y(0) = −4, ˙y(0) = 5

.
(4.38)
Method: The corresponding homogeneous ODE, ¨y + 2˙y + y = 0, has characteristic poly-
nomial s2 + 2s + 1 = (s + 1)2, so {y1(t) = e−t, y2(t) = te−t} is a complete set of basic
solutions of that homogeneous ODE. Let’s try to find a solution of the nonhomogeneous
ODE (4.38) in the form
y(t) = v1(t)e−t + v2(t)te−t.
The functions v1(t), v2(t) should satisfy system (4.35), which here is
⎧
⎨
⎩
e−t ˙v1(t) + te−t ˙v2(t) = 0
−e−t ˙v1(t) + (1 −t)e−t ˙v2(t) =
√
t e−t
⎫
⎬
⎭.

Scalar ODEs II
297
We get
˙v1
˙v2

=
 e−t
te−t
−e−t
(1 −t)e−t
−1 
0
√
t e−t

=
1
e−2t
(1 −t)e−t
−te−t
e−t
e−t
 
0
√
t e−t

;
hence,
˙v1
˙v2

=
−t3/2
t1/2

.
So,
v1(t) =

˙v1(t) dt =

(−t3/2) dt = −2
5 t5/2 + c1,
and
v2(t) =

˙v2(t) dt =

t1/2 dt = 2
3 t3/2 + c2,
where c1, c2 are arbitrary constants.
Putting everything together, we get solutions
y(t) = v1(t)e−t + v2(t)te−t = · · · = 4
15 t5/2 e−t + c1e−t + c2te−t.
After calculating that
˙y = 2
3 t3/2e−t −4
15 t5/2e−t −c1e−t + c2(1 −t)e−t,
we address the ICs:
−4 = y(0) = 0 + c1 + 0
and
5 = ˙y(0) = 0 −c1 + c2;
hence, c1 = −4, c2 = 1. The solution of the IVP is
y(t) = 4
15 t5/2e−t −4e−t + te−t =
 4
15 t5/2 −4 + t

e−t. ⃝
Example 4.19
Find a formula for the solutions of ¨y + p(t)˙y + q(t)y = f(t). (Such a formula may also be
called a “closed form” solution.)
Method: The best that we can hope for is a formula in terms of {y1(t), y2(t)}, a complete set
of basic solutions of the corresponding linear homogeneous ODE, ¨y + p(t)˙y + q(t)y = 0.
Assume p(t) and q(t) are continuous on an open interval containing t = 0. Continuing
from (4.36), we have
˙v1
˙v2

=
y1(t)
y2(t)
˙y1(t)
˙y2(t)
−1  0
f(t)

=
1
W(y1, y2)(t)

˙y2(t)
−y2(t)
−˙y1(t)
y1(t)
  0
f(t)

=
1
W(y1, y2)(t)
−y2(t)f(t)
y1(t)f(t)

.
Because, for example, v1(t) = c1 +
 t
0 ˙v1(s) ds, where c1 is an arbitrary constant,
y(t) =
⎛
⎝c1 +
t
0
−
y2(s)f(s)
W(y1, y2)(s) ds
⎞
⎠y1(t) +
⎛
⎝c2 +
t
0
y1(s)f(s)
W(y1, y2)(s) ds
⎞
⎠y2(t),
(4.39)
where c1, c2 are arbitrary constants. Equation (4.39) gives a formula for all solutions of
¨y + p(t)˙y + q(t)y = f(t). ⃝

298
Advanced Engineering Mathematics
4.3.2 Problems
In problems 1–12, solve the ODE.
1. y′′ + 4y =
1
sin 2x, where ′ = d
dx
2. ¨y + y = sec(t) csc(t).
3. ¨y + y = sec2 t.
4. ˙y + 4˙y + 4y = e−2t
t−1
5. x2y′′ −2xy′ + 2y = x3, where ′ = d
dx
6. x2y′′ −2xy′ + 2y = x3e−3x, where ′ = d
dx
7. x2y′′ −5xy′ + 8y = x3e−x, where ′ = d
dx
8. r2y′′ −4ry′ + 6y = r4 cos(r), where ′ = d
dr
9. r2y′′ −6ry′ + 12y = r5 sin 2r, where ′ = d
dr
10. r2y′′ + 4ry′ + 2y = e−r, where ′ = d
dr
11. r2y′′ −4ry′ + 6y = r2, where ′ = d
dr
12. ¨y + 4˙y + 5y = e−2t sec(t)
13. Use the method of variation of parameters to solve ¨y + ˙y = e−t.
14. Use the method of variation of parameters to solve ¨y + 8˙y + 16y = e−4t.
In problems 15–17, solve the IVP.
15. ¨y + 4˙y + 4y =
√
t e−2t, y(1) = −1, ˙y(1) = 0
16. y′′ + y = sec(x), y(0) = −1, y′(0) = 0, where ′ = d
dx
17. r2y′′ + ry′ + y = r2, y(1) = 3, y′(1) = −1, where ′ = d
dr
18. For ODE x2y′′ −4xy′ + 6y = 0, find the solutions satisfying the given ICs, if
possible.
(a) y(1) = 0, y′(1) = −2
(b) y(0) = 0, y′(0) = −2
(c) Why does your difficulty in part (b) not contradict the existence and unique-
ness conclusion of Theorem 3.8 in Section 3.3?
19. Solve xy′′ −(x + 2)y′ + 2y = x3, where ′ =
d
dx, given that the corresponding
homogeneous ODE has among its solutions y = ex and y = 1 + x + 1
2x2.
20. Another way to find a particular solution of the ODE in Example 4.18 is to rewrite
it as (D + 1)2[y] =
√
t e−t, look for a solution in the form y(t) = e−tv(t), and use
the shift theorem 3.18 in Section 3.4. [This method was shown to me by Daniel
Schepler in the 1990s.]
21. Use the method of undetermined coefficients to solve (4.26), that is,
¨y +
5˙y + 6y = t2e−t.
For this problem,
give your opinion as to you would
rather use undetermined coefficients or variation of parameters, as we did in
Example 4.16.

Scalar ODEs II
299
22. Use the Wronskian and the inverse of a 3 × 3 matrix to find a formula for all
solutions of the third-order linear ODE ...y +p1(t)¨y+p2(t)˙y+p3(t)y = f(t) in terms of
f(t) and a complete set of basic solutions of the corresponding linear homogeneous
ODE.
4.4 Laplace Transforms: Basic Techniques
We will see how “Laplace transforms” can turn many ODE problems into algebra prob-
lems. Engineers especially prize Laplace transforms because they give techniques to
solve ODEs involving terms that are switched on or off. In addition, they give engi-
neers convenient language and techniques for solving control problems “in the frequency
domain.”
Here, our emphasis is on using Laplace transforms rather than how they are defined or
calculated using an improper integral, which will be discussed in Appendix B near the end
of this book. For our purpose now, the Laplace transform of a function f(t) is
L[ f(t)](s),
a function only of s. As depicted in Figure 4.14, L can be thought of as a machine, just as
we thought of D, the operator of differentiation.
We need to know what L does so that we can use L to solve ODEs; justifications for the
properties will be in Appendix B.
Theorem 4.6
Assuming the Laplace transforms exist, they satisfy the following properties:
(1) L[ f(t) + g(t)](s) = L[ f(t)](s) + L[ g(t)](s).
(2) L[cf(t)](s) = c L[ f(t)](s), any constant c.
(3) L[eat](s) =
1
s−a.
(4) L[˙y](s) = sL[y(t)](s) −y(0).
Usually, we denote by an uppercase letter, the Laplace transform of a lower case letter, for
example, F(s) ≜L[ f(t)](s).
Properties (1)−(4) suffice for solving some ODEs and IVPs.
f (t)
[ f (t)](s)
FIGURE 4.14
Laplace transform as a machine.

300
Advanced Engineering Mathematics
Example 4.20
Solve
˙y + 2y = 3e−4t
y(0) = 5

.
(4.40)
Method: Take L of both sides of ODE (4.40) and use properties (1) and (2) of Theorem 4.6
to get
L[ ˙y] + 2L[ y] = 3L[ e−4t].
Use properties (3) and (4) and the IC y(0) = 5 to get

sL[ y] −5

+ 2L[ y] = 3 ·
1
s −(−4).
At this point, we have an algebra problem: solve
(s + 2)Y(s) = 5 +
3
s + 4
for Y(s) ≜L[y(t)](s) in terms of s:
Y(s) =
5
s + 2 +
3
(s + 2)(s + 4).
But, to solve the IVP means to find the solution y(t), not L[y](s).
Using property (3), we could infer what y(t) is if we could rewrite Y(s) as a linear com-
bination of terms of the form
1
s−a. To do this, we use the techniques of partial fractions
expansions from first-year calculus:
3
(s + 2)(s + 4) =
A
s + 2 +
B
s + 4,
(4.41)
where constants A, B are to be determined.
To find A, B, multiply (4.41) through by the denominator of its LHS to get
3 = A(s + 4) + B(s + 2).
(4.42)
Substitute s = −4 into (4.42) to get
3 = A(−4 + 4) + B(−4 + 2) = −2B,
so B = −3
2. Similarly, substitute s = −2 into (4.42) to get
3 = A(−2 + 4) + B(−2 + 2) = 2A,
so A = 3
2.
We have
Y(s) =
5
s + 2 +
 3/2
s + 2 −3/2
s + 4

= 13/2
s + 2 −3/2
s + 4.
Thinking backward, we infer from L[ y(t)](s) and property (3) that the solution is
y(t) = 13
2 e−2t −3
2 e−4t. ⃝
By the way, the function Y(s) is called the solution in the frequency domain.
Just from this example, we observe many things about the method of Laplace transforms
for solving ODEs:

Scalar ODEs II
301
Remarks
1. The ODE and IC were used together to find the solution. In previous methods, we first
found the general solution of the ODE before using the IC.
2. After finding Y(s), we had to “think backward” to find y(t). This will be formalized as
follows by working with the “inverse Laplace transform.”
3. We used algebraic manipulations, including partial fractions, to get Y(s) in a form suit-
able for finding y(t). For some simple IVPs, the method of Laplace transform may
involve more work than techniques we learned before, such as the method of
undetermined coefficients.
Recall that if an n × n matrix A is invertible, then
Ax = b ⇐⇒x = A−1b.
Similarly, we define the inverse Laplace transform, L−1[ ], by
L[ f(t)](s) = F(s) ⇐⇒f(t) = L−1[ F(s)](t).
For example, property (3) and the corresponding inverse property say that
L[ eat](s) =
1
s −a ⇐⇒L−1

1
s −a

= eat.
Now using this notation, in Example 4.20, we could have written the solution as
y(t) = L−1[Y(s)] = L−1
 13/2
s + 2 −3/2
s + 4

= 13
2 e−2t −3
2 e−4t.
For every property of Laplace transforms, there is a corresponding property for inverse
Laplace transforms.
In order to solve a wider variety of ODEs and IVPs, we need more facts about Laplace
transforms.
Theorem 4.7
Laplace transforms exist with the following properties:
(5) L[tn](s) = n!
sn , n = 0, 1, 2, . . ..
(6) L[cos ωt](s) =
s
s2 + ω2 .
(7) L[sin ωt](s) =
ω
s2 + ω2 .
(8) L[eatf(t)](s) = F(s −a), where F(s) = L[ f(t)](s).
(9) L[¨y](s) = s2L[y](s) −sy(0) −˙y(0).

302
Advanced Engineering Mathematics
Property (8) can also be stated as
L[ eatf(t)](s) = L[ f(t)]
%%%
s→(s−a),
which is equivalent to the property
L−1[ F(s −a)] = eat · L−1[ F(s)].
Properties (4) and (9) can be generalized to
L[ y(n)](s) = snL[ y(s)] −sn−1y(0) −sn−2˙y(0) −· · · −sy(n−2)(0) −y(n−1)(0).
Example 4.21
Find the Laplace transforms of the following given functions: (a) 2t3 −5t + 7,
(b) 5 cos 2t −sin 3t,
and (c) e−2t cos t + e−t sin 2t + te−3t.
Method:
(a)
L[ 2t3 −5t + 7] = 2L[ t3] −5L[ t] + 7L[ t0] = 2 3!
s4 −5 1!
s2 + 7 0!
s1 = 12
s4 −5
s2 + 7
s .
(b)
L[ 5 cos 2t −sin 3t] = 5
s
s2 + 22 −
3
s2 + 32 =
5s
s2 + 4 −
3
s2 + 9.
(c)
L[ e−2t cos t + e−t sin 2t + te−3t]
= L[ cos t]
%%%
s→(s−(−2)) + L[ sin 2t]
%%%
s→(s−(−1)) + L[ t]
%%%
s→(s−(−3))
=
s
s2 + 1
%%%
s→(s+2)) +
2
s2 + 4
%%%
s→(s+1)) + 1
s2
%%%
s→(s+3))
=
s + 2
(s + 2)2 + 1 +
2
(s + 1)2 + 4 +
1
(s + 3)2 . ⃝
Example 4.22
Find the inverse Laplace transforms of the following:
(a) L−1
3s + 4
s2 + 2

,
(b) L−1
 (s + 3) + 5
(s + 3)2 + 4

,
and
(c) L−1

3s + 8
s2 + 4s + 5

.
Method:
(a)
L−1
3s + 4
s2 + 2

= 3L−1

s
s2 + 2

+ 4L−1

1
s2 + 2

= 3 cos(
√
2 t) + 4 1
√
2
L−1
&
√
2
s2 + 2
'
= 3 cos(
√
2 t) + 2
√
2 sin(
√
2 t).

Scalar ODEs II
303
(b)
L−1
 (s + 3) + 5
(s + 3)2 + 4

= L−1 [F(s −(−3))] = e−3tL−1 [F(s)] = e−3tL−1
 s + 5
s2 + 4

= e−3t

cos 2t + 5
2 sin 2t

.
(c) The denominator is not of the form s2+ω2 but can be rewritten in the form (s−a)2+ω2
after completing the square:
s2 + 4s + 5 =

s2 + 4s + 4 −4

+ 5 =

s2 + 4s + 4

−4 + 5 = (s + 2)2 + 1.
After that, we need to rewrite the numerator to be in terms of (s + 2) so that we can use
property (8) backward:
L−1

3s + 8
s2 + 4s + 5

= L−1

3s + 8
(s + 2)2 + 1

= L−1
3 ((s + 2) −2) + 8
(s + 2)2 + 1

= L−1
3(s + 2) + 2
(s + 2)2 + 1

= e−2tL−1
3s + 2
s2 + 1

= e−2t (3 cos t + 2 sin t) . ⃝
Example 4.23
Solve
 ¨y + 4˙y + 5y = e−t
y(0) = −3, ˙y(0) = 7

.
(4.43)
Method: Using property (9) of Theorem 4.7, take L of both sides of ODE (4.43) to get
s2L[y](s) −s(−3) −7 + 4

sL[y](s) −(−3)

+ 5L[y](s) =
1
s + 1.
For convenience, denote Y(s) = L[y](s). Combining all terms that involve Y(s) and
moving all other terms to the RHS, we have
(s2 + 4s + 5)L[y](s) = −3s −5 +
1
s + 1.
So,
y(t) = L−1
 −3s −5
s2 + 4s + 5 +
1
(s + 1)(s2 + 4s + 5)

.
To handle the second term, use the partial fractions expansion
1
(s + 1)(s2 + 4s + 5) =
A
s + 1 +
Bs + C
s2 + 4s + 5,
where A, B, C are constants to be determined. Multiply through by the denominator of
the LHS:
1 = A(s2 + 4s + 5) + (Bs + C)(s + 1).
(4.44)
Substitute s = −1 into (4.44) to get
1 = A

(−1)2 + 4(−1) + 5

+ (B(−1) + C) · 0 = 2A.

304
Advanced Engineering Mathematics
Substitute A = 1
2 into (4.44) and move terms to the LHS to get
1 −1
2(s2 + 4s + 5) = (Bs + C)(s + 1).
After dividing through by (s + 1), we have
Bs + C = 1 −1
2s2 −2s −5
2
s + 1
= −1
2s2 −2s −3
2
s + 1
= −1
2 · s2 + 4s + 3
s + 1
= −1
2

(s + 1)(s + 3)

(s + 1)
= −1
2(s + 3).
So,
y(t) = L−1
&
−3s −5
s2 + 4s + 5 +
1
2
s + 1 + −1
2(s + 3)
s2 + 4s + 5
'
= 1
2

L−1

1
s + 1

+ L−1
 −7s −13
s2 + 4s + 5

= 1
2

e−t + L−1
−7 ((s + 2) −2) −13
(s + 2)2 + 1

= 1
2

e−t + L−1
−7(s + 2) + 1
(s + 2)2 + 1

= 1
2

e−t + e−2tL−1
−7s + 1
s2 + 1

= 1
2

e−t + (−7 cos t + sin t)e−2t
. ⃝
Remark
In Example 4.23, after we divided through by (s+1), it was good that the polynomial Bs+C
turned out to be equal to a polynomial such as −1
2(s + 3). If we had not been able to cancel
out factors of (s+1), hence we had seemed to get Bs+C not equal to a polynomial, then an
alarm bell should go off. Like some computer error messages, an alarm bell may not tell
us exactly where we made an error but merely that we’ve made an error somewhere.
Example 4.24
Find the steady-state solution of
¨y + 4˙y + 5y = 6 cos 2t.
(4.45)
Method: Note that ICs are not given in this problem. Take L of both sides of ODE (4.45)
to get
s2L[ y](s) −sy(0) −˙y(0) + 4

sL[ y](s) −y(0)

+ 5L[ y](s) =
6s
s2 + 4.
For convenience, denote Y(s) = L[ y](s). Solve for Y(s) in terms of s:
Y(s) = sy(0) + ˙y(0) + 4y(0)
s2 + 4s + 5
+
6s
(s2 + 4)(s2 + 4s + 5).
Because s2 + 4s + 5 = (s + 2)2 + 1, the first terms in y(t) = L−1[Y(s)] will be of the form
e−2t times cosine and sine functions and hence will be part of the transient solution. So,
the ICs play no role in the steady-state solution in this example.
We use the partial fraction expansion
6s
(s2 + 4)(s2 + 4s + 5) = As + B
s2 + 4 +
Cs + E
s2 + 4s + 5,
(4.46)

Scalar ODEs II
305
where A, B, C, E are constants to be determined. Multiply through by the denominator
of the LHS of (4.46) to get
6s = (As + B)(s2 + 4s + 5) + (Cs + E)(s2 + 4).
(4.47)
Unfortunately, substituting in real values of s will not wipe out any of the coeffi-
cients A, B, C, E. We could get four equations in A, B, C, E by sorting (4.47) by powers of
s. Alternatively, we will substitute four convenient values of s into (4.47), specifically
s = 0, 1, −1, −2 to get, respectively,
⎧
⎪⎪⎨
⎪⎪⎩
@s = 0 :
0 =
5B + 4E
@s = 1 :
6 =
10(A + B) + 5(C + E)
@s = −1 :
−6 = 2(−A + B) + 5(−C + E)
@s = −2 : −12 = (−2A + B) + 8(−2C + E).
⎫
⎪⎪⎬
⎪⎪⎭
.
The solution of this system of four equations in four unknowns is
⎡
⎢⎢⎣
A
B
C
E
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
0
5
0
4
10
10
5
5
−2
2
−5
5
−2
1
−16
8
⎤
⎥⎥⎦
−1 ⎡
⎢⎢⎣
0
6
−6
−12
⎤
⎥⎥⎦.
Only A = 6
65, B = 96
65 figure into the steady-state solution, which is
yS(t) = 6
65L−1
s + 16
s2 + 4

= 6
65 (cos 2t + 8 sin 2t) . ⃝
Table 4.1 has a short summary of facts concerning Laplace transforms.
4.4.1 Problems
In problems 1–5, find the Laplace transform of the given function.
1. −5e3t + sin 2t
2. cos 3t + sin( t
2)
3. 1 + at + 1
2!(at)2 + 1
3!(at)3, where a is an unspecified constant
4. t3e−2t
5. et/2 cos
 √
3
2 t

In problems 6–11, find the inverse Laplace transform of the given function.
6.
5
s3 −3s−1
s2+4
7.
s−2
(s+2)(s2+1)
8.
s+1
s2−4s+5
9.
−s+4
s2+4s+7
10.
2s2+4s+8
s3−4s
11.
s+6
(s+3)2(s2+2s+2)

306
Advanced Engineering Mathematics
TABLE 4.1
First Table of Laplace Transforms, Where c, ω Are Positive Constants
Formula
f(t)
F(s) = L[ f(t)]
L1.1
f(t) + g(t)
F(s) + G(s)
L1.2
cf(t)
cF(s)
L1.3
eat
1
s−a
L1.4
˙y
sY(s) −y(0)
L1.5
tn
n!
sn+1 , integer n ≥0
L1.6
sin ωt
ω
s2+ω2
L1.7
cos ωt
s
s2+ω2
L1.8
eatf(t)
F(s −a)
L1.9
¨y
s2Y(s) −sy(0) −˙y(0)
L1.10
step(t −c)
1
s e−cs
L1.11
g(t)step(t −c)
e−csL[ g(t + c)](s)
L1.12
(f ∗g)(t)
F(s)G(s)
L1.13
t sin ωt
2ωs
(s2+ω2)2
L1.14
t cos ωt
s2−ω2
(s2+ω2)2
L1.15
δ(t −c)
e−cs
12. Find y(t) = L−1 +
s
s2+4s+13
,
and express it in an amplitude-phase form Aeαt
cos(νt −δ).
In problems 13–16, use Laplace transforms to solve the given IVP. If the problem has a
steady-state solution, then note what it is.
13. ˙y −2y = 3e4t, y(0) = −1
14. ˙y + 2y = cos 4t, y(0) = −1
15. ¨y + 3˙y −10y = 0, y(0) = 1, ˙y(0) = −3
16. ¨y + 9y = 5 sin 2t, y(0) = 1, ˙y(0) = −3
17. For the IVP ¨y + 9y = 10te−t, y(0) = 0, ˙y(0) = 0, find the solution using (a) the
method of undetermined coefficients, and (b) the method of Laplace transforms.
Which method do you prefer for this problem?
18. For the IVP ¨y + 4y = e−t cos 2t, y(0) = 0, ˙y(0) = 0, find the solution using (a) the
method of undetermined coefficients and (b) the method of Laplace transforms.
Which method do you prefer for this problem?
19. For ODE ¨y + 2˙y + 2y = sin t, use Laplace transforms to find the steady-state
solution.
20. Suppose Y(s) =
s−2
(s2+1)(s2+2s+5) is the Laplace transform of a solution of an IVP.
(a) Find the steady-state solution of the IVP.
(b) Find an IVP for which the given function Y(s) is the Laplace transform of a
solution.
In each of problems 21 and 22, suppose a forced oscillator problem has a solution, y(t).
Without finding the inverse Laplace transform, decide whether the ODE fits the case of

Scalar ODEs II
307
resonance, beats phenomenon, steady-state oscillation, or none of the above. Of course,
give at least a brief explanation for how you reached your conclusion. In addition, if the
solution is in the steady-state oscillation case, find the steady-state solution.
21. L[ y(t)] = s+1
s2+3 −s+2
s2+1
22. L[ y(t)] =
s−1
s2+4s+10 −s+2
s2+9
23. The Taylor polynomial
pn(t) ≜1 + t + 1
2!(t)2 + · · · + 1
n!(t)n
is an approximation of et. Find L

pn(t)

and explain, using partial sums of a
geometric series, why
L

pn(t)

→L

et
,
as
n →∞, for s > 1.
4.5 Laplace Transforms: Unit Step and Other Techniques
As mentioned earlier, Laplace transforms are particularly useful for solving ODEs involv-
ing terms that are switched on or off, for example, in electrical circuits. In this section, we
will study and apply “unit step functions,” as well as “convolution” and its relationship
to the “transfer function” and “delta functions.” In addition, we will work with periodic
functions often used by engineers to analyze systems.
Definition 4.3
The unit step function switched on at t = c is
step(t −c) ≜
0,
t < c
1,
t ≥c

,
(4.48)
where c is a nonnegative constant.
As in the previous section, our emphasis is on using Laplace transforms rather than how
they are defined or calculated using an improper integral. But we definitely need to know
what the Laplace transform does.
Theorem 4.8
(Properties of unit step functions): if c is a nonnegative constant, then
(10) L[ step(t −c)](s) = 1
s e−cs.
(11) L[ g(t)step(t −c)](s) = e−cs · L[ g(t + c)](s).

308
Advanced Engineering Mathematics
We refer to property (11) as the horizontal shift theorem because of its shift of the function
g’s input on the RHS.
Example 4.25
Find the Laplace transforms of the given functions:
(a) L[ step(t −3)](s),
(b) L[ (t −3)step(t −3)](s),
and (c) L[ t step(t −3)](s).
Method:
(a) L[step(t −3)](s) = 1
s e−3s.
(b) L[(t −3)step(t −3)](s) = e−3sL[ g(t + 3)], where g(t) = (t −3). So,
L[(t −3)step(t −3)](s) = e−3sL
+
(t −3)
%%%
t→(t+3)
,
= e−3sL[(t + 3) −3] = e−3sL[t] = 1
s2 e−3s.
(c) L[t step(t −3)](s) = e−3sL[h(t + 3), where h(t) = t. So,
L[t step(t −3)](s) = e−3sL
+
t
%%%
t→(t+3)
,
= e−3sL[ t + 3] =
 1
s2 + 3
s

e−3s. ⃝
As earlier, corresponding to every property of Laplace transforms is a property of the
inverse Laplace transforms. Corresponding to property (11) is the next result.
Corollary 4.1
If g(t) = L−1[ G(s)], then
L−1 
e−csG(s)

= g(t −c)step(t −c) = g(t)
%%%
t→(t−c)step(t −c).
Example 4.26
Find the inverse Laplace transforms of the given functions:
(a) L−1
s
s2+4 e−s
and (b) L−1 1
s3 e−5s
.
Method:
(a) L−1 +
s
s2+4 e−s,
= L−1+
s
s2+4
,%%%
t→(t−1)step(t −1) = cos 2t
%%%
t→(t−1)step(t −1)
= cos (2(t −1)) step(t −1).
(b) L−1 +
1
s3 e−5s ,
= L−1 +
1
2! · 2!
s3
, %%%
t→(t−5)step(t −5) = 1
2t2|t→(t−5)step(t −5)
= 1
2(t −5)2step(t −5). ⃝
4.5.1 Writing a Function in Terms of Step Function(s)
Example 4.27
Write f(t) =
⎧
⎨
⎩
0,
0 ≤t < 2
1,
2 ≤t < 5
0,
5 ≤t < ∞
⎫
⎬
⎭in terms of step functions.

Scalar ODEs II
309
Method: Concerning only the time interval 0 ≤t < 5, f(t) has a “1” switched on at t = 2.
So, for 0 ≤t < 5, f(t) = step(t −2). But then at t = 5, the “1” is switched off, or,
equivalently, a “−1” is switched on. So,
f(t) = 1 · step(t −2) + (−1) · step(t −5) = step(t −2) −step(t −5). ⃝
Example 4.28
Write f(t) =
⎧
⎨
⎩
0,
0 ≤t < 2
1,
2 ≤t < 5
t −4,
5 ≤t < ∞
⎫
⎬
⎭in terms of step functions.
Method 1: Just as for Example 4.27, on the interval 0 ≤t < 5, f(t) = step(t −2). At t = 5,
the “1” is switched off, or, equivalently, a “−1” is switched on, and also “(t −4)” is
switched on. So,
f(t) = 1 · step(t −2) + ((−1) + (t −4)) · step(t −5) = step(t −2) + (t −5) · step(t −5). ⃝
Method 2: We can rewrite f(t) as
f(t) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
0 ≤t < 2
1,
2 ≤t < 5
1 −1 + (t −4),
5 ≤t < ∞
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
0 ≤t < 2
1,
2 ≤t < 5
1,
5 ≤t < ∞
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
+
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
0 ≤t < 2
0,
2 ≤t < 5
t −5,
5 ≤t < ∞
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
=
⎧
⎨
⎩
0,
0 ≤t < 2
1,
2 ≤t < ∞
⎫
⎬
⎭+(t −5)
⎧
⎨
⎩
0,
0 ≤t < 5
1,
5 ≤t < ∞
⎫
⎬
⎭= step(t −2) + (t −5)step(t −5). ⃝
We presented two slightly different methods, one that is essentially graphical and the
other algebraic, so that you may choose which method you find to be the more comfort-
able and accurate. Of course, you have the option to use both methods to check your
conclusions.
By the way, other authors use other notations for the unit step function such as
H(t −c), uc(t), or u(t −c). The H stands for Oliver Heaviside, a great electrical engineer,
and the u stands for “unit step.”
Example 4.29
Solve the IVP
˙y + 1
2y = 3step(t −4)
y(0) = 5

.
(4.49)
The ODE models the RC series circuit with R = 2 , C = 1 F, and voltage source V(t) =
6step(t −4) volts, a DC source switched on at t = 4, shown in Figure 4.15.
Method: Denote Y(s) = L[y(t)](s). Take the Laplace transform of both sides and use
properties (4) and (10):

s + 1
2

Y(s) = 5 + 3
s e−4s.
To find
y(t) = L−1[Y(s)] = L−1
⎡
⎣
5
s + 1
2
+
3
s

s + 1
2
 e−4s
⎤
⎦,

310
Advanced Engineering Mathematics
2 Ω
6 step(t–4)
volts
I
1 F
FIGURE 4.15
RC circuit with step-function voltage source.
use the partial fractions expansion
3
s(s + 1
2)
= A
s +
B
s + 1
2
,
where A, B are constants to be determined.
Caution: The e−4s factor is not part of the partial fractions work.
Multiply through by the denominator of the LHS to get
3 = A

s + 1
2

+ Bs.
Substitute s = 0 in to get A = 6; substitute s = −1
2 in to get B = −6.
So,
y(t) = L−1
&
5
s + 1
2
+

6
s −
6
s + 1
2
 
e−4s
'
= 5e−t/2 +

6 −6e−t/2 %%%
t→(t−4)step(t −4)
= 5e−t/2 + 6

1 −e−(t−4)/2
step(t −4). ⃝
4.5.2 Graph of a Solution of an ODE Involving a Step Function
Recall that the graph of y = g(t −c) is the same as the graph of y = g(t) but shifted c units
to the right. For example, the graph of Figure 4.16b, 6

1 −e−(t−4 )/2
step(t −4), is the same
as the graph of Figure 4.16a, 6

1 −e−t/2
step(t −0) shifted 4 units to the right.
To understand the graph of the solution of Example 4.29, that is,
y(t) = 5e−t/2 + 6

1 −e−(t−4)/2
step(t −4),
it helps to rewrite it as
y(t) = 5e−t/2 + 6

1 −e−(t−4)/2
·
0,
0 ≤t < 4
1,
t ≥4

= 5e−t/2 +
⎧
⎨
⎩
0,
t < 4
6

1 −e−(t−4)/2
,
t ≥4
⎫
⎬
⎭

Scalar ODEs II
311
6
5
4
3
2
1
5
10
t
t
5
10
6
5
4
3
2
1
(a)
(b)
FIGURE 4.16
Horizontal shift: (a) 6 (1 −e−t/2) step(t −0) and (b) 6 (1 −e−(t−4)/2) step(t −4).
6
5
4
3
2
1
y(t)=5e–t/2+ 6(1– e–(t–4)/2) step (t –4)
t
2
4
6
8
10
12
14
FIGURE 4.17
Solution of Example 4.29.
=
⎧
⎨
⎩
5e−t/2,
t < 4
5e−t/2 + 6

1 −e−(t−4)/2
,
t ≥4
⎫
⎬
⎭.
The graph of that solution is in Figure 4.17. We see clearly that the effect of the ODEs
forcing term 3step(t −4) on the solution of the ODE is to switch on at t = 4 a horizontal
shift of the function 6

1 −e−t/2
.
Notice that the graph of y(t) is continuous at t = 4, but its derivative has a finite jump
there. This makes sense because the ODE tells us that
lim
t→4± ˙y(t) = lim
t→4± −1
2y(t) + 3 step(t −4) =
⎧
⎨
⎩
−5
2e−2,
as t →4−
−5
2e−2 + 3,
as t →4+
⎫
⎬
⎭. ⃝
Besides switching on or off a source in an electrical circuit, another physical example of
a step function occurs if we are driving, and for a certain time interval [t0, t1], we drive

312
Advanced Engineering Mathematics
over “rumble strips” warning us to slow down or to stay off the shoulder of the road. The
vibrations of the suspension system could be modeled by the ODE
m¨y + b˙y + ky = f(t),
where f(t) is a sum of pulse terms f0 ·

step(t −t0) −step (t −t1)).
4.5.3 Convolution
Definition 4.4
The “half-line” convolution, or “convolution,” for short, of functions f and g is defined
to be
(f ∗g)(t) ≜
t
0
f(t −u)g(u) du.
(4.50)
Note that (f ∗g)(t) is a function only of the variable t, because the u variable gets “inte-
grated out” of the definite integral. Note also that t appears twice in the formula: as the
upper limit of integration and inside the f function evaluation.
Note for Chapter 9 that in the subject of Fourier transforms, convolutions are integrals
over the whole real line, that is,
 ∞
−∞.
Theorem 4.9
Properties of convolution
(a) (f ∗g)(t) = (g ∗f)(t).
(b)

(f1 + f2) ∗g

(t) = (f1 ∗g)(t) + (f2 ∗g)(t).
(c)

(cf) ∗g

(t) = c(f ∗g)(t), if c is a constant.
Theorem 4.10
Property of Laplace transform of a convolution
(12) L[( f ∗g)(t)](s) = L[ f(t)](s) · L[g(t)](s).
Corresponding to property (12) is a property of inverse Laplace transforms:
L−1[F(s)G(s)] =

L−1[F(s)] ∗L−1[G(s)]

(t).
Example 4.30
Find the convolution of et ∗t by two methods: (a) by using the definition of convolution
and (b) using Theorem 4.10.

Scalar ODEs II
313
Method 1:
(a) Using a law of exponents and then integration by parts,
et ∗t ≜
t
0
et−u u du =
t
0
ete−u u du = et
t
0
e−u u du = et 
−ue−u −e−ut
0
= et 
−te−t −e−t −(0 −1)

= −t −1 + et.
(b)
et ∗t = L−1+
L[et ∗t]
,
= L−1+
L[ et] · L[t]
,
= L−1

1
s −1 · 1
s2

.
Partial fractions gives
1
(s −1)s2 =
A
s −1 + B
s + C
s2 ,
where A, B, C are constants to be determined. Multiply through by the denominator of
the LHS:
1 = As2 + Bs(s −1) + C(s −1).
(4.51)
Substitute in s = 0 to get 1 = −C, so C = −1, and substitute in s = 1 to get 1 = A.
Substitute the values A = 1, C = −1 into (4.51) to get
1 = s2 + Bs(s −1) −(s −1).
Solve for B by putting all other terms on the LHS and then dividing through by s(s −1):
B = 1 −s2 + (s −1)
s(s −1)
= −s2 + s
s(s −1) = −

s(s −1)


s(s −1) = −1.
So, again,
et ∗t = L−1

1
s −1 · 1
s2

= L−1

1
s −1 −1
s −1
s2

= et −1 −t. ⃝
Remark
One very quick “reality check” you can make on a calculation of a convolution is that
(f ∗g)(0) should be zero because (f ∗g)(0) =
 0
0 f(0 −u)g(u) du = 0 if the functions f and g
are reasonable.
Example 4.31
Find L−1

1
(s + 1)(s2 + 4)

using a convolution.
Method: L−1

1
(s + 1)(s2 + 4)

= e−t ∗1
2 sin 2t =
 t
0 e−(t−u) 1
2 sin 2u du
=
 t
0 e−teu 1
2 sin 2u du = 1
2 e−t  t
0 eu sin 2u du = 1
2 e−t ·

eu
12 + 22 (sin 2u −2 cos 2u)
t
0

314
Advanced Engineering Mathematics
= 1
10e−t 
et (sin 2t −2 cos 2t) + 2

= 1
5e−t + 1
10 (sin 2t −2 cos 2t) . ⃝
Using convolution, we can get the next results.
Theorem 4.11
Properties of Laplace transforms of solutions of problems with resonance:
(13) L[ t sin ωt](s) =
2ωs
(s2+ω2)2.
(14) L[ t cos ωt](s) =
s2−ω2
(s2+ω2)2 .
Example 4.32
Find (a) L−1

9
(s2 + 9)2

, and (b) L−1
&
s2
(s2 + 9)2
'
.
Method: Using algebra work on numerators, we have
(a)
L−1

9
(s2 + 9)2

= L−1
& 1
2(s2 + 9) −1
2(s2 −9)
(s2 + 9)2
'
= 1
2L−1

1
s2 + 9

−1
2L−1
&
s2 −9
(s2 + 9)2
'
= 1
6 sin 3t −1
2t cos 3t.
(b)
L−1
&
s2
(s2 + 9)2
'
= L−1
& 1
2(s2 + 9) + 1
2(s2 −9)
(s2 + 9)2
'
= 1
2L−1

1
s2 + 9

+ 1
2L−1
&
s2 −9
(s2 + 9)2
'
= 1
6 sin 3t + 1
2t cos 3t.⃝
4.5.4 Convolution and Particular Solutions
Suppose p, q are constants. We want to find a formula for a particular solution, yp(t), of
¨y + p˙y + qy = f(t).
(4.52)
Take the Laplace transform of the ODE:
(s2 + ps + q)Yp(s) −syp(0) −˙yp(0) −pyp(0) = F(s),
where Yp(s) = L[ yp(t)](s) and F(s) = L[ f(t)](s). Because we are interested only in a partic-
ular solution, we may ignore the −sy(0) −˙y(0) −py(0) terms. So, let
Yp(s) =
1
s2 + ps + q · F(s).

Scalar ODEs II
315
So,
yp(t) = L−1[Yp(s)] = L−1

1
s2 + ps + q · F(s)

= L−1

1
s2 + ps + q

∗L−1 [F(s)]
= L−1

1
s2 + ps + q

∗f(t).
So,
yp(t) =
t
0
y2(t −u)f(u) du,
where y2(t) is the solution of the IVP
⎧
⎨
⎩
¨y + p˙y + qy = 0
y(0) = 0
˙y(0) = 1
⎫
⎬
⎭. Why? Because Y2(s) = L[ y2(t)]
satisfies
(s2 + ps + q)Y2(s) −s · 0 −1 −p · 0 = 0,
that is, Y2(s) =
1
s2 + ps + q .
By the way, in Theorem 3.9 in Section 3.3, the function y2(t) was one of the two solutions
in a complete set of basic solutions of the corresponding homogeneous ODE, ¨y+p˙y+qy = 0.
Definition 4.5
If p, q are constants,
T(s) ≜
1
s2 + ps + q
is called the transfer function for the problem ¨y + p˙y + qy = f(t).
The zeros of the transfer function, that is, s satisfying s2 + ps + q = 0, are called the poles
of the system. The poles are, not coincidentally, the zeros of the characteristic polynomial
of the corresponding LCCHODE.
Example 4.33
Express a particular solution of ¨y + ω2y = f(t) using a convolution. Assume ω ̸= 0.
Method: Take the Laplace transform of the ODE to get
(s2 + ω2)Yp(s) −sy(0) −˙y(0) = F(s),

316
Advanced Engineering Mathematics
where Yp(s) = L[ yp(t)](s) and F(s) = L[ f(t)](s). Because we are interested only in a
particular solution, we may ignore the −sy(0) −˙y(0) terms. We have
yp(t) = L−1[Yp(s)] = L−1

1
s2 + ω2 F(s)

= L−1

1
s2 + ω2

∗f(t).
So, a particular solution of the ODE is given by
yp(t) =
t
0
1
ω sin(ω(t −u))f(u) du. ⃝
Recall that a function f(t) is periodic if there is a positive constant T such that
f(t + T) ≡f(t) at all t in the domain of f. Pictorially, a function is periodic if its graph
is not changed when you shift it horizontally by T units. Periodic functions are commonly
found in engineering as inputs to, and/or outputs from, machinery.
Define a square wave over one period by
f(t) =
⎧
⎨
⎩
1,
0 < t < T
2
−1,
T
2 < t < T
⎫
⎬
⎭.
(4.53)
For T = 2π, the graph of the periodic function f is shown in Figure 4.18.
Example 4.34
Solve the IVP
˙y + 2y = f(t)
y(0) = 0

, where f(t) is the square wave defined in (4.53).
Method: Take the Laplace transform to get
Y(s) =
1
s + 2 · F(s),
1.0
0.5
–0.5
–5
5
y(t)
10
t
–1.0
FIGURE 4.18
Square wave function.

Scalar ODEs II
317
where Y(s) = L[ y(t)](s) and F(s) = L[ f(t)](s). The solution can be expressed in terms of a
convolution:
y(t) = L−1[Y(s)] = L−1

1
s + 2

∗F(s) =
t
0
e−2(t−u)f(u)du = e−2t ·
t
0
e2uf(u) du. ⃝
This gives a form of the solution, but we can get even more information about the solu-
tion by evaluating the definite integral. Unfortunately, to do so requires a lot of tedious
work by examining different intervals of t values. First, if 0 ≤t ≤T
2, then
I(t) ≜
t
0
e2uf(u) du =
t
0
e2u · 1 du =
&
e2u
2
't
0
= 1
2

e2t −1

.
Next, if T
2 ≤t ≤T, then a property of definite integration yields
t
0
e2uf(u) du =
T
2
0
e2u · 1 du +
t
T
2
e2u · (−1) du = I
T
2

+
&
−e2u
2
't
T
2
= 1
2

(eT −1) −(e2t −eT)

= 1
2

−e2t −1 + 2eT
.
If T ≤t ≤3T
2 , then
t
0
e2uf(u) du =
T
0
e2uf(u)du +
t
T
e2u · 1 du = I (T) +
&
−e2u
2
't
T
= 1
2

−e2T −1 + 2eT + (e2t −e2T)

= 1
2

e2t −1 + 2eT −2e2T
.
If 3T
2 ≤t ≤2T, then
t
0
e2uf(u)du =
3T
2
0
e2uf(u)du +
t
3T
2
e2u · (−1)du = I
3T
2

+
&
−e2u
2
't
3T
2
= 1
2

e3T −1 + 2eT −2e2T −(e2t −e3T)

= 1
2

−e2t −1 + 2eT −2e2T + 2e3T
, . . . .

318
Advanced Engineering Mathematics
0.4
0.2
–0.2
–0.4
5
10
15
t
y(t)
FIGURE 4.19
Solution of Example 4.34.
The solution of the IVP is
y(t) = 1
2 e−2t ·
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
e2t −1,
0 ≤t ≤T
2
−e2t −1 + 2eT,
T
2 ≤t ≤T
e2t −1 + 2eT −2e2T,
T ≤t ≤3T
2
−e2t −1 + 2eT −2e2T + 2e3T,
3T
2 ≤t ≤2T
...
...
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
In general, for k T
2 ≤t ≤(k + 1) · T
2,
y(t) = 1
2e−2t
⎛
⎝(−1)ke2t −1 −2
k
-
j=1
(−1)jejT
⎞
⎠.
For T = 2π, the solution is graphed in Figure 4.19. By the way, the solution graphed
in Figure 4.19 is not a periodic solution, as we will see in Example 5.41 in Section 5.8.
Nevertheless, we can see a steady-state solution hiding in Figure 4.19.
4.5.5 Delta “Functions”
While step(t −c) is used to model turning on or off a function at t = c, there is another
related but more exotic “function,” δ(t −c), called a delta function. We assume c is a
positive constant.

Scalar ODEs II
319
δn(t)
n
t
1
2n
c–——
1
2n
c+——
c
FIGURE 4.20
Approximate delta function.
Intuitively, a delta function is an “infinitesimally brief but infinitely strong input.” We
can think of δ(t −c) as the limit, as n →∞, of “approximate delta functions” defined by
δn(t) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
t < c −1
2n
n,
c −1
2n ≤t < c + 1
2n
0,
t ≥c + 1
2n
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
= n ·

step

t −

c −1
2n

−step

t −

c + 1
2n

.
Physically, an approximate delta function can model a strong but brief input to a system.
Figure 4.20 shows an approximate delta function. For each n, the area under the graph of
δn(t) is 1.
If f(t) is continuous at t = c, for n large enough that c −1
2n > 0, that is, n > 1
2c, then
∞

0
f(t)δn(t)dt =
c+ 1
2n

c−1
2n
f(t) · ndt ≈nf(c) ·

length of interval

c −1
2n, c + 1
2n

= nf(c) · 1
n = f(c).
Definition 4.6
δ(t −c) is defined to satisfy
 ∞
0
f(t)δ(t −c) dt = f(c) for any function f that is continuous at
t = c.
Theorem 4.12
(Property of Laplace transform of a delta function) Assume c is a nonnegative constant.
Then
(15) L[ δ(t −c)](s) = e−cs.

320
Advanced Engineering Mathematics
Example 4.35
Solve the IVP ˙y + 4y = 2δ(t −1), y(0) = 3.
Method: Take the Laplace transform of both sides of the ODE to get
(s + 4)Y(s) −3 = 2e−s,
where Y(s) = L[ y(t)](s). Using property (11) backward, the solution of the IVP is
given by
y(t) = L−1

3
s + 4 +
2
s + 4 e−s

= 3e−4t + L−1

2
s + 4
 %%%
t→(t−1) · step(t −1)
= 3e−4t + (2e−4t)
%%%
t→(t−1) · step(t −1) = 3e−4t + 2e−4(t−1)step(t −1). ⃝
The graph of the solution,
y(t) = 3e−4t +
0,
0 ≤t < 1
2e−4(t−1),
t ≥1

=
3e−4t,
0 ≤t < 1
3e−4t + 2e−4(t−1),
t ≥1

,
in Figure 4.21 has a vertical jump, by two units, at t = 1: a horizontal translate of 2e−4t is
switched on at t = 1.
In some sense, the derivative of a step function is a delta function because of properties
(4), (10), and (15):
L
 d
dt

step(t −c)

= sL

step(t −c)

−step(0 −c) = s · 1
s e−cs −0
= e−cs = L[δ(t −c)].
3.0
y(t)
2.5
2.0
1.5
1.0
0.5
t
0.5
1.0
1.5
2.0
2.5
3.0
FIGURE 4.21
Example 4.25.

Scalar ODEs II
321
4.5.6 Laplace Transform of a Periodic Function
Theorem 4.13
Property of Laplace transform of a periodic function
(16) L[ f(t)](s) =
1
1−e−sT
 T
0 f(t)e−st dt.
Example 4.36
Find the Laplace transform of the square wave defined in (4.53) over one period,
0 < t < T, and simplify the result.
Method:
L[ f(t)](s) =
1
1 −e−sT
T
0
f(t)e−st dt =
1
1 −e−sT
⎛
⎝
T/2

0
f(t)e−stdt +
T
T/2
f(t)e−stdt
⎞
⎠
=
1
1 −e−sT
⎛
⎝
T/2

0
(1)e−stdt +
T
T/2
(−1)e−stdt
⎞
⎠=
1
1 −e−sT
 e−st
−s
T/2
0
−
 e−st
−s
T
T/2
 
=
1
1 −e−sT · 1
s ·

−e−sT/2 + 1

+

e−sT −e−sT/2
= e−sT −2e−sT/2 + 1
s(1 −e−sT)
=
(1 −e−sT/2)2
s(1 −(e−sT/2)2) =
(1 −e−sT/2)2
s(1 −e−sT/2)(1 + e−sT/2) =
1 −e−sT/2
s(1 + e−sT/2). ⃝
4.5.7 Remarks
1. The result of Example 4.36 could be used in an attempt to solve Example 4.34;
however, finding L−1[Y(t)] would be difficult.
2. Another way to approach Example 4.34 would be to rewrite the periodic function
f(t) as an infinite series of step functions, specifically
f(t) = 1 −2 step

t −T
2

+ 2 step (t −T) −2 step

t −3T
2

± · · · .
This would turn out to be a fairly reasonable approach even though the solution, y(t),
would also be an infinite series involving step functions. It would be reasonable because
at any finite time, only finitely many of the terms in y(t) would be nonzero, so convergence
of the infinite series would not be an issue.
4.5.8 Problems
In problems 1–5, find the Laplace transform of the given function.
1. 7 + 3e−2t −t step(t −4)
2. (t −2)step(t −2) −5t step(t −3)

322
Advanced Engineering Mathematics
3. f(t) =
−t + 4,
0 ≤t < 2
t,
t ≥2

4. f(t) =
⎧
⎨
⎩
t,
0 ≤t < 1
1,
1 ≤t < 3
−1,
t ≥3
⎫
⎬
⎭
5. h(t) =
 t
0 e−(t−u) cos(2u)du
6. Which of the following, if any, is the Laplace transform of t step(t −3)?
(a)
1
s2 e−3s
(b)
1
s2 e3s
(c) ( 1
s2 + 3
s )e3s
(d) ( 1
s2 + 3
s )e−3s
(e) ( 1
s2 + 1
s )e−3s
7. Which of the following, if any, is the Laplace transform of
1
√
3e−t/2 sin(
√
3t
2 )? It
might be true that more than one is correct.
(a)
1
2(s2+s+1)
(b)
1
2(s2−s+1)
(c)
√
3/2
(s+ 1
2 )(s2+ 3
4 )
(d)
√
3/2
(s−1
2 )(s2+ 3
4 )
(e)
1
2((s−1
2 )2+ 3
4 )
In problems 8–13, find the inverse Laplace transform of the given function.
8.
e−πs/4
s2+9
9.
e−s
s2−2s
10.
e−2s
s3+4s
11.
s
(s2+4)2
12.
s2
(s2+4)2
13.
4
(s2+4)2
14. Express L−1 +
s
(s2+6)(s2+4)
,
as a specific definite integral. Do use specific functions
and variables in your integral, but do not evaluate or simplify the integral.
In problems 15–21, use Laplace transforms to solve the given IVP. Also, graph the
solution.
15. ˙y + 3y = 2 −step(t −1), y(0) = 4
16. ¨y + 9y = 2 −step(t −2), y(0) = −1, ˙y(0) = 6

Scalar ODEs II
323
3.0
2.5
2.0
1.5
1.0
0.5
g(t)
t
2
4
6
8
FIGURE 4.22
Sawtooth function with period T = π.
17. ¨y + 4y = step(t −c), y(0) = 0, ˙y(0) = −5, where c is an unspecified constant
18. ¨y + y = f(t), y(0) = 1, ˙y(0) = 0, where f(t) =
 t,
0 ≤t < π
0,
t ≥π

19. ˙y + 3y = δ(t −2), y(0) = −1
20. ¨y + 4y = δ(t −c), y(0) = 0, ˙y(0) = 0, where c is an unspecified constant
21. ¨y + y = cos t
In problems 22–24, use Laplace transforms to solve the given IVP and to express the solu-
tion in the form of a convolution. Do not replace f(t) by a specific function, that is, your
final conclusion should be in terms of f(t).
22. ˙y + 5y = f(t), y(0) = 0
23. ¨y + 4y = f(t), y(0) = 0, ˙y(0) = 0
24. ¨y + 4˙y + 5y = f(t), y(0) = 0, ˙y(0) = 0
Definition 4.7
Define a sawtooth function over one period by g(t) ≜t, 0 < t < T.
In problems 25 and 26, use Laplace transforms to solve the given IVP and to express
the solution in the form of a convolution. Here, g(t) is the sawtooth function shown in
Figure 4.22 and has period T = π. Analyze the solution as in Example 4.34.
25. ˙y + 5y = g(t), y(0) = 0
26. ¨y + 4y = g(t), y(0) = 0, ˙y(0) = 0
4.6 Scalar Difference Equations
A “difference equation” is a relationship involving a sequence of values, say {yk}N
k=0. For
example, yk = ryk−1 says that we have a geometric sequence, y0, ry0, r2y0, . . . .

324
Advanced Engineering Mathematics
A difference equation can have its origins in the time evolution of a system. Suppose we
partition an interval [ a, b] into N equal subintervals, that is,
a = t0,
a + h = t1,
a + 2h = t2,
· · · ,
a + Nh = b = tN
and define
yk ≜y(tk) = y(a + kh), k = 0, 1, 2, . . . , N.
You saw this when approximating definite integrals in calculus I or II, for example, using
the trapezoidal rule. Another notation for h is t, the “time-step size” or “grid size.”
The difference equation
yk+2 = yk+1 + yk
(4.54)
is famous because it produces the “Fibonacci numbers”: starting with initial data y0 = 0,
y1 = 1, the sequence is
{yk}∞
k=0 = 0, 1, 1, 2, 3, 5, 8, 13, 21, . . . .
Fibonacci numbers show up in an amazing variety of scientific problems, including a sim-
ple model for the growth of a population of rabbits, where yk, k = 1, 2, 3, . . ., is the number
of female rabbits at successive, discrete times.
Difference equations also naturally show up in digital computation, for example,
discrete-time signal theory, discrete-time control theory, and in approximating solutions
of differential equations.
In general, an n-th order difference equation is given by
yk+n = f(k, yk, yk+1, . . . , yk+n−1), k = 0, 1, 2, . . . .
(4.55)
So, for example, (4.54) is a second-order difference equation. A solution of an n-th order
difference equation is a sequence {yk}N
k=0 or {yk}∞
k=0. We may also refer to yk, without the
curly brackets, as a solution. Substituting in k = 0, (4.55) yields
yn = f(0, y0, y1, . . . , yn−1);
substituting in k = 1, (4.55) implies
yn+1 = f(1, y1, y2, . . . , yn);
etc. So, in order to solve an n-order difference equation, we need initial data
y0, y1, . . . , yn−1,
(4.56)
after which we can “recursively” calculate yn, then yn+1, then yn+2, etc. Of course, how far
ahead in time we can calculate the successive values in the recursive sequence depends
upon the computing machinery, upon n, and upon the function f. Sometimes the function
f is not known explicitly as a formula in k, yk, yk+1, . . . , yk+n−1 but is instead a subroutine
that itself involves recursive processes of length not initially known.

Scalar ODEs II
325
For an n-th order linear constant coefficients homogeneous difference equation (also
known as LCCHE)
yk+n = a1yk+n−1 + a2yk+n−2 + · · · + anyk,
(4.57)
we will assume that, in addition, an ̸= 0, so that the LCCHE is truly of n-th order.
Fortunately, we will see that there is a solution method like the solution methods for
LCCHODEs and for Cauchy–Euler equations: try solutions of the form
yk = rk,
where r is a constant to be determined. When we substitute this into (4.57), we get
rn+k = a1rn+k−1 + a2rn+k−2 + · · · + anrk,
that is,
rkrn = rk(a1rn−1 + a2rn−2 + · · · + anr0),
that is,
rk(rn −a1rn−1 −a2rn−2 −· · · −an−1r −an) = 0.
So, r should satisfy a characteristic equation,
rn −a1rn−1 −a2rn−2 −· · · −an−1r −an = 0.
(4.58)
Example 4.37
Solve the Fibonacci LCCHE (4.54) with ICs y0 = 0, y1 = 1.
Method: With yk = rk, this second-order difference equation has characteristic equation
r2 −r −1 = 0,
whose solutions are
r = 1 ±

(−1)2 −4(1)(−1)
2
= 1 ±
√
5
2
.
The Fibonacci difference (4.54) has solutions
yk = c1

1 +
√
5
2
 k
+ c2

1 −
√
5
2
 k
,
(4.59)
where c1, c2 are arbitrary constants.
We substitute (4.59) into the ICs to get
0 = y0 = c1 + c2
and
1 = y1 =

1 +
√
5
2
 
c1 +

1 −
√
5
2
 
c2.

326
Advanced Engineering Mathematics
So, we get
⎡
⎣
c1
c2
⎤
⎦=
⎡
⎣
1
1
1+
√
5
2
1−
√
5
2
⎤
⎦
−1 ⎡
⎣
0
1
⎤
⎦=
1
−
√
5
⎡
⎢⎣
1−
√
5
2
−1
−1+
√
5
2
1
⎤
⎥⎦
⎡
⎣
0
1
⎤
⎦=
⎡
⎢⎣
1
√
5
−1
√
5
⎤
⎥⎦.
The Fibonacci sequence is given by
yk =
1
√
5
⎛
⎝

1 +
√
5
2
 k
−

1 −
√
5
2
 k⎞
⎠. ⃝
(4.60)
Because it looks overly complicated to have all of those
√
5s, let’s check by substituting
in, say, k = 4:
3 = y4 =?
1
√
5
⎛
⎝

1 +
√
5
2
 4
−

1 −
√
5
2
 4⎞
⎠
=
1
16
√
5

1 + 4
√
5 + 6(5) + 4(5
√
5) + 1(25) −1 + 4
√
5 −6(5) + 4(5
√
5) −1(25)

=
1
16
√
5
(8
√
5 + 40
√
5) =
1
16
√
5
48
√
5 = 3. ⃝
For any second-order LCCHE
yk+2 = a1yk+1 + a2yk,
(4.61)
we will assume the coefficients a1, a2 are real. It is possible to allow them to be complex,
but that would alter some of the results that follow. In addition, we assume that a2 ̸= 0 so
that the LCCHE is truly of second order. The characteristic polynomial is
r2 −a1r −a2,
(4.62)
whose roots are
r =
a1 ±

a2
1 −4a2
2
.
If the roots are real, distinct r1, r2, then the solutions of (4.61) are given by
yk = c1rk
1 + c2rk
2,
where c1, c2 are arbitrary constants.

Scalar ODEs II
327
Analogously to the critically damped case of Section 3.3, it turns out that if the roots of
the characteristic polynomial are real but equal, that is, r1 = r2, then the solutions of (4.61)
are given by
yk = c1rk
1 + c2krk
2,
where c1, c2 are arbitrary constants.
If the roots of the characteristic polynomial are a complex conjugate pair r = α ± iν,
where α, ν are real and ν > 0, then the solutions of (4.61),
yk = ˜c1(α + iν)k + ˜c2(α −iν)k,
can be rewritten in real form: First, we need to mention the “polar form” of a complex
number:
α + iν = ρeiω = ρ(cos ω + i sin ω),
(4.63)
where ρ is real and nonnegative and −π < ω ≤π. This is just another example of using
polar coordinates/trigonometry because
α + iν = ρ cos ω + iρ sin ω
is solvable for real, nonnegative ρ and real ω in terms of α, ν:
ρ =

α2 + ν2,
tan ω = ν
α .
As usual, it helps to draw a picture, Figure 4.23, in the (α, ν)-plane. Note that ν > 0
implies that ω ̸= 0.
We see that with −π < ω ≤π, α + iν = ρ cos ω + iρ sin ω = ρeiω; the latter is called the
polar form of the complex number. This implies that
α −iν = ρ cos ω −iρ sin ω = ρ cos(−ω) + iρ sin(−ω) = ρe−iω.
ρ
ν
ω
α
FIGURE 4.23
ρ and ω picture.

328
Advanced Engineering Mathematics
So, we can rewrite the solutions of (4.61) in the complex roots case:
yk = ˜c1(α + iν)k + ˜c2(α −iν)k = ˜c1(ρeiω)k + ˜c2(ρe−iω)k = ρk 
˜c1eiωk + ˜c2e−iωk
= ρk ˜c1(cos ωk + i sin ωk) + ˜c2(cos ωk −i sin ωk)

= ρk(c1 cos ωk + c2 sin ωk).
So, when there is a complex conjugate pair of roots, the solution is
yk = c1ρk cos ωk + c2ρk sin ωk,
(4.64)
where c1, c2 are arbitrary constants.
Example 4.38
For k ≥3, let Ak be the k × k “tri-diagonal” matrix
Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1.9
1
0
.
.
.
0
1
−1.9
1
0
.
0
1
−1.9
1
.
.
.
.
.
.
.
.
.
.
.
.
.
−1.9
1
0
1
−1.9
1
0
.
.
.
0
1
−1.9
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Find a formula for |Ak|, the determinant of Ak in terms of k.
Method: At first, this problem seems to have nothing to do with difference equations,
except perhaps the “subscript k” reminds us of the solution of a difference equation.
But, as we will see, we can find an LCCHE satisfied by
yk ≜|Ak|.
But first, it will help if we look at some small-sized examples of the matrix Ak and their
determinants.
For k = 3,
A3 =
⎡
⎣
−1.9
1
0
1
−1.9
1
0
1
−1.9
⎤
⎦.
By expanding along the first row,
y3 = |A3| = (−1.9)
%%%%
−1.9
1
1
−1.9
%%%% −(1)
%%%%
1
1
0
−1.9
%%%% + (0)
%%%%
1
−1.9
0
1
%%%% = −3.059.
For k = 4,
y4 = |A4| =
%%%%%%%%
−1.9
1
0
0
1
−1.9
1
0
0
1
−1.9
1
0
0
1
−1.9
%%%%%%%%

Scalar ODEs II
329
= (−1.9)
%%%%%%
−1.9
1
0
1
−1.9
1
0
1
−1.9
%%%%%%
−(1)
%%%%%%
1
1
0
0
−1.9
1
0
1
−1.9
%%%%%%
+ (0) + (0) = −1.9y3 −
%%%%
−1.9
1
1
−1.9
%%%% .
This suggests that it would be good to also define
A2 ≜
−1.9
1
1
−1.9

, y2 ≜|A2| = 2.61.
Then we see that
y4 = −1.9y3 −y2.
In fact, in general, by expanding the determinant along the first row, we have
yk =
%%%%%%%%%%%%%%%%
−1.9
1
0
0
.
.
.
0
1
−1.9
1
0
.
0
1
−1.9
1
.
.
.
.
.
.
.
.
.
.
.
.
.
−1.9
1
0
1
−1.9
1
0
.
.
.
0
1
−1.9
%%%%%%%%%%%%%%%%
= (−1.9)
%%%%%%%%%%%%%%
−1.9
1
0
.
.
.
0
1
−1.9
1
.
0
1
−1.9
.
.
.
.
.
.
.
.
.
.
.
.
1
−1.9
1
0
.
.
.
0
1
−1.9
%%%%%%%%%%%%%%
−
%%%%%%%%%%%%%%
1
1
0
.
.
.
0
0
−1.9
1
.
1
−1.9
.
.
.
.
.
.
.
.
.
.
.
.
−1.9
1
0
.
.
.
1
−1.9
%%%%%%%%%%%%%%
.
Now, expand the second term along its first column to get
yk = −1.9yk−1 −(1)
%%%%%%%%%%%%
−1.9
1
0
.
.
.
0
1
−1.9
1
.
.
.
.
.
.
.
.
.
.
1
−1.9
1
0
.
.
.
0
1
−1.9
%%%%%%%%%%%%
= −1.9yk−1 −yk−2.
So, yk satisfies the second-order difference equation
yk = −1.9yk−1 −yk−2, for k ≥3,
that is,
yk+2 = −1.9yk+1 −yk, for k ≥1,

330
Advanced Engineering Mathematics
whose characteristic polynomial, r2 + 1.9r + 1, has a complex conjugate pair of roots
r = −0.95 ± i
√
0.0975.
According to our previous theoretical result in (4.64), here ρ =

(−0.95)2 + (0.0975)2 = 1
and tan ω =
√
0.0975
−0.95 and ω in the second quadrant of the (α, ν)-plane give
yk = c1 cos ωk + c2 sin ωk,
as the solutions of the difference equation, where c1, c2 are arbitrary constants. Just as
we did for the amplitude-phase form of solutions in Section 3.3, we get
ω = π + tan−1
√
0.0975
−0.95
 
= π −tan−1
√
0.0975
0.95
 
.
To satisfy the IC,
⎧
⎨
⎩
2.61 = y2 = c1 cos 2ω + c2 sin 2ω
−3.059 = y3 = c1 cos 3ω + c2 sin 3ω
⎫
⎬
⎭,
we get
⎡
⎣
c1
c2
⎤
⎦=
⎡
⎣
cos 2ω
sin 2ω
cos 3ω
sin 3ω
⎤
⎦
−1 ⎡
⎣
2.61
−3.059
⎤
⎦
=
1
cos 2ω sin 3ω −sin 2ω cos 3ω
⎡
⎣
sin 3ω
−sin 2ω
−cos 3ω
cos 2ω
⎤
⎦
⎡
⎣
2.61
−3.059
⎤
⎦
= · · · =
⎡
⎣
1
−0.95/
√
0.0975
⎤
⎦.
So,
det(Ak) = cos ω −
0.95
√
0.0975
sin ωk, k ≥3. ⃝
By the way, we can conclude that for all k ≥3,
%%%det(Ak)
%%% =
%%%% cos ωk +

−
0.95
√
0.0975

sin ωk
%%%% ≤| cos ωk| +
%%%% −
0.95
√
0.0975
sin ωk
%%%%
≤1 +
%%%%−
0.95
√
0.0975
%%%% ≈4.042434922.
4.6.1 General Solution and the Casorati Determinant
Analogous to the differential operator, D =
d
dt, is the forward difference operator 
defined by
(y)k ≜yk+1 −yk,
for
k ≥0.
(4.65)

Scalar ODEs II
331
TABLE 4.2
The Zoo of Solutions of Linear Homogeneous Constant Coefficients
Difference Equations
Roots of Characteristic Polynomial
Solutions yk
1, 1, . . . , 1 [m times]
1, k, . . . , km−1
α, α, . . . , α [m times]
αk, k αk, . . . , km−1αk
α ± iν, . . . , α ± iν [m times]
ρk cos ωk, k ρk cos ωk, . . . , km−1ρk cos ωk
and
ρk sin ωk, k ρk sin ωk, . . . , km−1ρk sin ωk
Recall from calculus I that if yk = y(tk), then
˙y(tk) ≈yk+1 −yk
tk+1 −tk
= yk
tk
.
We can also define higher order difference operators by
(2y)k ≜((y))k = (yk+1 −yk) = (yk+2 −yk+1) −(yk+1 −yk) = yk+2 −2yk+1 + yk,
for k ≥0,
that is, 2 ≜. Similarly, we can define 3 ≜2, etc.
In Table 4.2, the first line is a special case of the second line because αk ≡1 when α = 1.
In the last line, the point (α, ν) = ρ · (cos ω, sin ω), where ρ =

α2 + ν2 > 0.
If {y(1)
k , y(2)
k , . . . , y(n)
k } is a set of n sequences, the Casorati determinant plays a role
analogous to that of the Wronskian determinant for linear ODEs. We define the Casorati by
C

y(1)
k , y(2)
k , . . . , y(n)
k

=
%%%%%%%%%%%%%
y(1)
k
y(2)
k
.
.
.
y(n)
k
y(1)
k+1
y(2)
k+1
.
.
.
y(n)
k+1
.
.
.
.
.
.
.
.
.
.
.
.
y(1)
k+n−1
y(2)
k+n−1
.
.
.
y(n)
k+n−1
%%%%%%%%%%%%%
.
For example,
C

rk
1, rk
2

=
%%%%%%
rk
1
rk
2
rk+1
1
rk+1
2
%%%%%%
= rk
1rk+1
2
−rk
2rk+1
1
= rk
1rk
2(r1 −r2) ̸= 0,
as long as r1 ̸= r2.
In Problem 4.6.4.20, you will explain why the Casorati determinant can be rewritten in a
form that looks more analogous to the Wronskian determinant.

332
Advanced Engineering Mathematics
An n-th order linear homogeneous difference equation has the form
yk+n = a1,kyk+n−1 + a2,kyk+n−2 + · · · + an,kyk ≜(Ly)k, for k ≥0,
(4.66)
where we assume that
an,k ̸= 0,
for all
k ≥0.
(4.67)
We assume (4.67) so that (4.66) is truly of order n.
Definition 4.8
The general solution of an n-th order linear homogeneous difference (4.66) has the form
y(h)
k
= c1y(1)
k
+ c2y(2)
k
+ · · · + cny(n)
k
if for every solution y∗
k of (4.66), there are values of constants c1, c2, . . . , cn giving y∗
k =
c1y(1)
k
+ c2y(2)
k
+ · · · + cny(n)
k . In this case, we call the set of sequences {y(1)
k , y(2)
k , . . . , y(n)
k } a
complete set of basic solutions for linear homogeneous difference (4.66). Each of the n
sequences y(1)
k , y(2)
k , . . . , y(n)
k
is called a basic solution of (4.66).
Theorem 4.14
The linear homogeneous n-th order linear homogeneous difference (4.66) has a general
solution, that is, a complete set of n basic solutions.
Why? The explanation is similar to that for Theorem 3.15 in Section 3.4: Each of the n IVPs
⎧
⎪⎨
⎪⎩
y(1)
k+n = (Ly(1))k
y(1)
0
= 1, y(1)
1
= 0, . . . , y(1)
n−1 = 0
⎫
⎪⎬
⎪⎭
,
⎧
⎪⎨
⎪⎩
y(2)
k+n = (Ly(2))k
y(2)
0
= 0, y(2)
1
= 1, . . . , y(2)
n−1 = 0
⎫
⎪⎬
⎪⎭
, . . . ,
⎧
⎪⎨
⎪⎩
y(n)
k+n = (Ly(n))k
y(n)
0
= 0, y(n)
1
= 0, . . . , y(n)
n−1 = 1
⎫
⎪⎬
⎪⎭
has a solution. The rest of the explanation is also similar to that given for Theorem 3.9 in
Section 3.3. 2
Theorem 4.15
(Abel’s theorem) Suppose {y(1)
k , y(2)
k , . . . , y(n)
k } is a set of n solutions of the same n-th order
linear homogeneous difference (4.66). Then the Casorati determinant satisfies

Scalar ODEs II
333
C

y(1)
k , y(2)
k , . . . , y(n)
k

=
⎛
⎝
k−1
.
ℓ=0

(−1)n−1an,ℓ

⎞
⎠C

y(1)
0 , y(2)
0 , . . . , y(n)
0

(4.68)
for any k ≥1.
Why? Analogous to the explanation of Theorem 3.12 in Section 3.3, first, we claim that
Ck ≜C

y(1)
k , y(2)
k , . . . , y(n)
k

satisfies the first-order linear homogeneous difference equation
Ck+1 = (−1)n−1an,kCk.
(4.69)
Here we will give an explanation of (4.69) for n = 2. In Problem 4.6.4.21, you will use
the same techniques to explain (4.69) for n = 3 or for all n ≥2. For n = 2, using the effects of
elementary row operations on determinants, we have
Ck+1 =
%%%%%%%
y(1)
k+1
y(2)
k+1
y(1)
k+2
y(2)
k+2
%%%%%%%
=
%%%%%%%
y(1)
k+1
y(2)
k+1
a1,ky(1)
k+1 + a2,ky(1)
k
a1,ky(2)
k+1 + a2,ky(2)
k
%%%%%%%
=
−a1,kR1+R2→R2
%%%%%%%
y(1)
k+1
y(2)
k+1
a2,ky(1)
k
a2,ky(2)
k
%%%%%%%
=
R2←a2,kR2
a2,k
%%%%%%%
y(1)
k+1
y(2)
k+1
y(1)
k
y(2)
k
%%%%%%%
=
R1↔R2
−a2,k
%%%%%%%
y(1)
k
y(2)
k
y(1)
k+1
y(2)
k+1
%%%%%%%
= −a2,kCk,
as we desired.
It is easy to solve (4.69). Continuing in the case n = 2,
Ck+1 =
⎛
⎝
k
.
ℓ=0

(−1)2−1an,ℓ

⎞
⎠C0 = (−1)k+1
⎛
⎝
k
.
ℓ=0
an,ℓ
⎞
⎠C0,
as desired. 2
Theorem 4.16
Suppose y(1)
k , y(2)
k , . . . , y(n)
k
are solutions of the same n-th order linear homogeneous
LCCHE (4.66).
Then C(y(1)
k , y(2)
k , . . . , y(n)
k )
̸=
0 for all k
≥
0 if,
and only if,
{y(1)
k , y(2)
k , . . . , y(n)
k } is a complete set of basic solutions of LCCHE (4.66).
Why? In order for this to be truly an n-th order linear homogeneous difference equa-
tion (4.66), we assumed in (4.67) that an,k ̸= 0, for all k ≥0. It follows that for all k ≥1,
/k
ℓ=0 an,ℓ

̸= 0; hence, the Casorati determinant Ck+1 is never zero as long as C0 ̸= 0. 2

334
Advanced Engineering Mathematics
4.6.2 Nonhomogeneous Linear Difference Equation
Definition 4.9
(a) A particular solution of an n-th order nonhomogeneous linear difference
equation
yk+n = fk + a1,kyk+n−1 + a2,kyk+n−2 + · · · + an,kyk, for k ≥0,
(4.70)
is any sequence y(p)
k
that satisfies (4.70).
(b) yk = y(p)
k +c1y(1)
k +c2y(2)
k +· · ·+cny(n)
k , for k ≥0, is the general solution of nonhomo-
geneous linear difference (4.70) if for every solution y∗
k of (4.70), there are values
for scalar constants c1, . . . , cn giving y∗(t) = y(p)
k
+ c1y(1)
k
+ c2y(2)
k
+ · · · + cny(n)
k ,
for k ≥0.
Analogous to Theorem 3.1 in Section 3.1, we have
Theorem 4.17
If y(p)
k
is any particular solution of n-th order linear nonhomogeneous difference (4.70)
and {y(1)
k , y(2)
k , . . . , y(n)
k } is any complete set of basic solutions of the corresponding linear
homogeneous difference (4.66), that is,
yk+n = a1,kyk+n−1 + a2,kyk+n−2 + · · · + an,kyk, for k ≥0,
then yk = y(p)
k
+ c1y(1)
k
+ c2y(2)
k
+ · · · + cny(n)
k , for k ≥0, is a general solution of the
nonhomogeneous linear difference (4.70), where c1, . . . , cn are arbitrary constants.
4.6.3 The Method of Undetermined Coefﬁcients
It turns out that for difference equations, the method of undetermined coefficients is
completely analogous to that for differential equations. Suppose an n-th order linear
constant coefficients nonhomogeneous difference (4.70) has forcing term fk that is a linear
combination of solutions from the zoo of solutions of LCCHEs. Construct the list L1 of
roots of the corresponding LCCHEs characteristic polynomial, rn−a1rn−1+a2rn−2+· · ·+
an, and the list L2 of roots corresponding to the forcing term(s) in fk. The superlist L ≜L1, L2
will show us the form of a particular solution, y(p)
k , involving constants to be determined
after we substitute it into the original, nonhomogeneous difference equation. The general
solution of the nonhomogeneous difference equation will be yk = y(h)
k
+ y(p)
k , where y(h)
k
is
the general solution of the corresponding linear homogeneous difference equation.

Scalar ODEs II
335
Example 4.39
Find the form of a particular solution of the difference equation
yk+2 = 2(1 −γ )yk+1 −yk + 3k −δk,
where δ and γ are real constants, δ ̸= 0, and 0 < γ < 2.
Method: The corresponding linear homogeneous difference equation has characteristic
polynomial r2 −2(1 −γ )r + 1 = (r −(1 −γ ))2 + (1 −(1 −γ )2), which has roots
r = (1 −γ ) ± i

1 −(1 −γ )2 = α ± iν = ρ cos ω ± iρ sin ω.
We have
ρ =

α2 + ν2 =

(1 −γ )2 +

1 −(1 −γ )2
2
= · · · = 1
and
tan ω =

1 −(1 −γ )2
1 −γ
.
The forcing term(s) in fk ≜3k −rk is (a) 3k, which comes from roots r = 1, 1, and
(b) δk, which comes from root r = δ. We have list L2 = 1, 1, δ, so the superlist is 1 ·
e±iω, 1, 1, δ. This gives yk = c1 cos ωk + c2 sin ωk + c31k + c4k · 1k + c5δk, so the correct form
of a particular solution is
y(p)
k
= A + Bk + Cδk,
where A, B, C are constants to be determined. ⃝
Example 4.40
Professor Sitsin Ivory Tower contemplates taking out a loan in the amount of P dollars.
The letter P stands for the “principal” of the loan. The First Federal Bank would charge
her interest of 100R % per month, and The Bank will take an automatic loan repayment
of M dollars per month from her checking account.
(a) Derive a linear constant coefficients nonhomogeneous difference equation IVP
model for this situation.
(b) Solve that model.
(c) If the professor wishes the loan to be exactly paid off after the Nth monthly payment,
find formulas for
(i) The monthly payment amount in terms of the loan principal, the monthly
interest rate, and the number for payments.
(ii) The number of payments, in terms of the loan principal, the monthly payment,
and the monthly interest rate.
(d) Suppose the annual percentage interest rate on her “subprime” loan is 11%. If the
professor can only afford to make monthly payments of $123.45 and needs to finish
paying off the loan in seven years, what is the largest loan that she can afford to take
out?
Method:
(a) Let yk be the monthly outstanding balance on her loan, which satisfies the initial con-
dition y0 = P. The Bank charges interest on the outstanding balance before crediting
her monthly payment, so

336
Advanced Engineering Mathematics
yk+1 = yk + Ryk −M,
that is, yk+1 = (1 + R)yk −M.
(b) The corresponding linear homogeneous difference equation has characteristic poly-
nomial (r −(1 + R)), so y(h)
k
= c1(1 + R)k. We have list L2 = 1, so the superlist is
L = L1, L2 = 1 + R, 1, and the correct form of a particular solution is
y(p)
k
= A · (1)k ≡A,
where A is a constant to be determined after substituting y(p)
k
into the original, linear
nonhomogeneous difference equation:
A = y(p)
k+1 = (1 + R)y(p)
k
−M = (1 + R)A −M.
We get A = M
R , so the general solution of the difference equation is
yk = y(p)
k
+ y(h)
k
= M
R + c1(1 + R)k,
where c1 is an arbitrary constant. Substitute this into the IC to get
P = y0 = M
R + c1(1 + R)0 = M
R + c1;
hence, c1 = P −M
R , and the solution of the model is
yk = M
R +

P −M
R

(1 + R)k.
(c) Having the loan exactly paid off after the Nth-monthly payment means that
yN = 0.
This is exactly like having a boundary condition for an ODE, except here we are
dealing with discrete time measured in months. Substitute the general solution of
the difference equation into the boundary condition to get
0 = yN = M
R +

P −M
R

(1 + R)N.
or, after multiplying through by R,
0 = M + (PR −M)(1 + R)N.
(4.71)
(i) To solve for M in terms of the other parameters, put all terms involving M on one side
of (4.71) to get

(1 + R)N −1

M = PR(1 + R)N;
hence, the desired monthly payment is
M = P ·
R(1 + R)N
(1 + R)N −1.

Scalar ODEs II
337
(ii) To solve, instead, for N in terms of the other parameters, put all terms involving N on
one side of (4.71) to get
(1 + R)N =
−M
PR −M =
M
M −PR.
(4.72)
The monthly payment, M, must exceed PR, the interest charged during the first month
of the loan; otherwise it is impossible to pay of the loan by making monthly payments
of M dollars! So, M −PR > 0.
So, taking the natural logarithm of both sides of (4.72), we get
N ln(1 + R) = ln

(1 + R)N
= ln

M
M −PR

;
hence, the number of monthly payments is
N =
ln

M
M−PR

ln(1 + R) .
(d) The annual percentage interest rate is 11%, that is, 0.11, so the monthly interest rate
is R = 0.11
12 . Substituting R, N = 84, and M = $123.45 into (4.71), we get
0 = 123.45 +

P · 0.11
12 −123.45
 
1 + 0.11
12
84
;
hence,
P = 123.45

1 −
1
(1 + (0.11/12))84
00.11
12

≈$7209.838417.
Because each month the bank may do some rounding off (or up) to the nearest (or higher)
cent, to be on the safe side, she should limit her loan to, say, $7208.99. ⃝
Over the life of the loan, she made 84 equal payments of $123.45, that is, she gave the
Bank a total of $10369.80, of which about $10369.80−7208.99 = $3160.81 was interest. We
say approximately because the 84th, last loan payment will be a little less than $123.45.
This is because the loan amount was hopefully lower than the actual amount paid when
allowing for some cents lost due to rounding.
Learn More About It
Some good references for scalar difference equations are Introduction to Difference
Equations, by Samuel Goldberg, Dover Publications, c⃝1986; Finite Difference Equa-
tions, by H. Levy and F. Lessman, Dover Publications,
c⃝1992; and Calculus of
Finite Differences and Difference Equations, by Murray R. Spiegel, Schaum’s Outline
Series, McGraw-Hill Book Company,
c⃝1971. In addition, there is an interesting
chapter on difference equations in Lectures on Applications-Oriented Mathematics, by
Bernard Friedman, edited by Victor Twersky, Holden-Day, Inc., c⃝1969 [reprinted
by Wiley-Interscience, c⃝1991].

338
Advanced Engineering Mathematics
4.6.4 Problems
In problems 1–7, solve the difference equation.
1. yk+2 = yk
2. yk+2 = −5yk+1 −6yk
3. yk+2 = −yk
4. yk+2 = 2yk+1 −10yk
5. yk+3 = yk
6. yk+1 = yk +

1
2
k
7. yk+2 = yk −k
In problems 8–12, solve the IVP.
8. yk+2 = 4yk, k ≥0; y0 = 2, y1 = 1
9. yk+2 = −yk+1 −2yk, k ≥0; y0 = 2, y1 = 1
10. yk+2 = −9yk, k ≥0; y0 = −1, y1 = 2
11. yk+2 = −4yk+1 −13yk, k ≥0; y0 = −2, y1 = 1
12. yk+2 = yk+1 + yk +

1
2
k
, k ≥0; y0 = −2, y1 = 1
In problems 13–17, the k × k matrix Ak is given. Find a formula for det(Ak) in terms of k.
13. Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
.
.
.
0
1
1
1
0
.
0
1
1
1
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1
0
1
1
1
0
.
.
.
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
14. Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1.8
1
0
.
.
.
0
1
−1.8
1
0
.
0
1
−1.8
1
.
.
.
.
.
.
.
.
.
.
.
.
.
−1.8
1
0
1
−1.8
1
0
.
.
.
0
1
−1.8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
15. Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−2
1
0
.
.
.
0
1
−2
1
0
.
0
1
−2
1
.
.
.
.
.
.
.
.
.
.
.
.
.
−2
1
0
1
−2
1
0
.
.
.
0
1
−2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

Scalar ODEs II
339
[Note: The Ak of Problem 15 is among the most famous of matrices. It appears
repeatedly when studying numerical methods for differential equations, as we
will see in Chapter 8.]
16. Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
2
1
0
.
.
.
0
1
2
1
0
.
0
1
2
1
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1
0
1
2
1
0
.
.
.
0
1
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
17. Ak =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−3
2
0
.
.
.
0
2
−3
2
0
.
0
2
−3
2
.
.
.
.
.
.
.
.
.
.
.
.
.
−3
2
0
2
−3
2
0
.
.
.
0
2
−3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
18. Find a formula for yk if yk+2 = 4yk+1 + yk = 0, k = 0, 1, 2, . . . ; y0 = 3; and limk→∞
yk = 0.
19. Suppose that uranium hexafluoride comes in two forms, U238F6 and U235F6. Pass-
ing a mixture of U238F6 and U235F6 through a single centrifuge can increase the
fraction that is U235F6 from f to rf, where r = 1.0014. Let yk be the fraction of
U235F6 after the mixture has been run through the kth-centrifuge. If the initial frac-
tion is 1%, how enriched is the mixture after running through such a cascade of
1024 centrifuges in series?
20. Use Theorem 1.29(a) in Section 1.6 to explain why the Casorati determinant can be
rewritten as
C

y(1)
k , y(2)
k , . . . , y(n)
k

=
%%%%%%%%%%%%
y(1)
k
y(2)
k
.
.
.
y(n)
k
(y(1))k
(y(2))k
.
.
.
(y(n))k
.
.
.
.
.
.
.
.
.
.
.
.
(n−1y(1))k
(n−1y(2))k
.
.
.
(n−1y(n))k
%%%%%%%%%%%%
.
21. Use the effects of elementary row operations on determinants to explain why
(4.69) is true:
(a) For n = 3, that is, Ck+1 = a3,kCk.
(b) For all n, that is, Ck+1 = (−1)n−1an,kCk.
22. For the difference equation zk+1 = zk + fk, we get
z1 = z0 + f0,
z2 = z1 + f1 = z0 + (f0 + f1),
etc.
Come up with a formula for the solution, zk in terms of z0, and a sum.

340
Advanced Engineering Mathematics
23. Project: Come up with a general solution method for first-order linear difference
equations, that is, of the form yk+1 −pkyk = fk. Hint: Define μk = (p0 · p1 · · · pk−1)
and zk = μkyk. An equation of the form zk = fk, that is, zk+1 = zk + fk, is solvable
using sums.
24. Explain why the method of undetermined coefficients works for linear constant
coefficients nonhomogeneous difference equations (4.70) whose forcing term fk is
a linear combination of solutions from the zoo of solutions of LCCHEs.
25. Project: Come up with a variation of parameters solution method for second-order
linear difference equations, that is, of the form yk+2 = pk+1yk+1 + qkyk + fk.
4.7 Short Take: z-Transforms
Suppose x = x(t) is a function of time, for example, a voltage measurement. An analog-to-
digital converter (A/D or ADC) may sample the voltage at times t = . . . , −T, 0, T, 2T, . . ..
In other words, the A/D produces a sequence x =
1
xn
2∞
n=−∞defined by
xn ≜x(nT), for n = . . . , −1, 0, 1, 2, . . . .
(4.73)
The sequence
1
xn
2∞
n=−∞is called the discrete-time signal, or signal, for short. Another
notation commonly used by electrical engineers is
x[n] ≜xn for n = . . . , −1, 0, 1, 2, . . . .
Note that x[n] is x(nT). For example, if the voltage comes from a microphone used to
record an audio compact disk, T =
1
44,100 s, that is, the sampling is done 44, 100 times each
second.∗
By definition, the Laplace transform uses x(t) for t ≥0 to produce
F(s) = L[ f(t)](s) ≜
∞

0
f(t)e−st dt,
an improper integral that is a function of s. We have seen in Sections 4.4 and 4.5 many
examples of Laplace transforms, which usually involve rational functions of s such as
F(s) =
s
(s+1)(s2+4).
The discrete-time version of this is called the unilateral or one-sided z-transform and is
defined by
X(z) = Z

x[n]

≜
∞
3
n=0
xnz−n.
(4.74)
∗This sampling rate was chosen according to the Nyquist Shannon sampling theorem so as to allow reproduction
of sounds with frequencies up to 20, 000 Hz.

Scalar ODEs II
341
Note that, like the Laplace transform, X(z) is calculated using only measurements at times
t ≥0, even if the discrete-time signal x[n] is defined for n < 0.
We will usually be brief by omitting the words “unilateral” or “one sided” and refer to
X(z) as the z-transform of the signal x[n]. There is another kind of z-transform, using all
of the signal values including x[n] for n < 0, called the “bilateral z-transform,” but we will
not study it.
The z- transform sum is like the left endpoints Riemann sum approximating the Laplace
transform:
L[ f(t)](s) ≜
∞

0
f(t)e−st dt ≈△t
∞
3
n=0
f(tn)e−stn = T
∞
3
n=0
f(tn)z−n.
This uses sampling points 0, T, 2T, 3T, . . . and with
e−stn = e−s(nT) = e(−n)(sT) =

esT−n
= z−n,
after defining
z = esT.
We will see how the z- transform can be used to solve the linear difference equations we
saw in Section 4.6. So, all solutions in the zoo of solutions of linear difference equations
give particularly relevant discrete-time signals. Also, we will need results concerning oper-
ations on signals similar to the operations we saw in the study of Laplace transforms. For
example, instead of taking the Laplace transform of a time derivative, d
dt, of a function, we
could take the z-transform of a time difference, x[n + 1] −x[n], of a signal.
We will follow the conventional notation that signals have index n instead of using
Section 4.6’s conventional index k.
Clearly, the signal x[n] ≜αn will be relevant because it is in the zoo of solutions of
LCCHEs. Also, x[n] ≜αn is the discrete-time signal for the function x(t) = eβt/T because
x(nT) = eβn =

eβn = αn, if β = ln α and α > 0. Using the sum of a geometric series, we
calculate
Z

x[n]

≜
∞
3
n=0
x[n]z−n =
∞
3
n=0
αnz−n =
∞
3
n=0
α
z
n
=
1
1 −(α/z) =
z
z −α = z ·
1
z −α .
(4.75)
Implicitly, we are assuming |z| > |α| so that the series converges. In this section, we will
ignore this technicality.
After the transform of an exponential, the next ingredients are the transforms of a time
shift and a forward difference. Recall from (4.65) in Section 4.6 the notation that
(x)[n] ≜x[n + 1] −x[n],
for all
n.
(4.76)
This suggests that we also need the shift operator S defined by
(Sx)[n] ≜x[n + 1],
for all
n.
(4.77)

342
Advanced Engineering Mathematics
Using the change of index n′ = n + 1 and denoting X(z) ≜Z

x[n]

, we calculate
Z

Sx[n]

= Z

x[n + 1]

≜
∞
3
n=0
x[n + 1]z−n =
∞
3
n=1
x[n′]z−(n′−1) =
∞
3
n=1
z · x[n′]z−n′;
hence,
Z

x[n + 1]

= z
∞
3
n=1
x[n′]z−n′ = z
⎛
⎝−x[0] +
∞
3
n=0
x[n′]z−n′
⎞
⎠.
So,
Z

x[n + 1]

= zX(z) −z x[0].
(4.78)
This is reminiscent of the Laplace transforms property (4) in Theorem 4.6 in Section 4.4,
namely, that L[ ˙y](s) = sY(s) −y(0).
Based on (4.78), we calculate that
Z

x[n + 2]

= Z

S2x[n]

= zZ

Sx[n]

(z) −z(Sx)[0] = z (zX(z) −z x[0]) −z x[0 + 1];
hence,
Z

x[n + 2]

= z2X(z) −z2x[0] −z x[1].
(4.79)
The usual linearity properties apply to z-transforms:
(1) Z

x[n] + y[n]

(z) = X(z) + Y(z).
(2) Z

cx[n]

(z) = c X(z), any constant c.
Example 4.41
Solve the LCCHE x[n + 1] = x[n] + αn, with IC x[0] = 3, assuming the constant α ̸= 1.
Method: Take the z-transforms of both sides of the difference equation Sx[n] = x[n] + αn
and denote X(z) ≜Z

x[n]

. Using (4.78), we calculate
zX(z) −z x[0] = Z

Sx[n]

(z) = Z

x[n] + αn 
(z) = X(z) + z ·
1
z −α ;
hence,
X(z) = z ·
3
z −1 + z ·
1
(z −1)(z −α).
Use partial fractions in the form∗
1
(z −1)(z −α) =
A
z −1 +
B
z −α ,
∗Note that we leave off the factor of z·

Scalar ODEs II
343
where A and B are constants to be determined. Multiply through by (z −1)(z −α) to get
1 = A(z −α) + B(z −1).
Substitute in z = 1 and z = α to get 1 = A(1 −α) and 1 = B(α −1), respectively. So,
X(z) = z

3
z −1 +
1
α −1

−
1
z −1 +
1
z −α

= z ·

3 −
1
α −1

1
z −1 +
1
α −1 ·
1
z −α

.
Using (4.75) in reverse, that is, taking x[n] = Z−1
X(z)

, gives that the solution is
x[n] =

3 −
1
α −1

· 1n +
1
α −1 · αn,
for n ≥0,
that is,
x[n] = 3 +
1
α −1 · (αn −1) = 3 + 1 −αn
1 −α ,
for n ≥0. ⃝
We could have found the solution by direct work with the difference equation to get the
pattern of a geometric series, but the method of Example 4.41 was useful for illustrating
use of the z-transform. Also, the methods of Section 4.6 might have been faster and easier
for Example 4.41, although that might be a matter of personal preference.
Similar to work on ODE -IVPs using Laplace transforms, using the z-transform to solve
a LCCHE makes use of the IC(s) right away rather than Section 4.6’s method of first
finding the general solution of the LCCHE. Also, the z-transform makes uses of an inho-
mogeneity right away as opposed to using the method of undetermined coefficients, as in
Section 4.6.
Example 4.42
(Example 4.37 in Section 4.6 again) Solve the Fibonacci LCCHE (4.37) in Section 4.6,
that is, x[n + 2] = x[n + 1] + x[n], with ICs x[0] = 0 and x[1] = 1.
Method: Take the z-transforms of both sides of the difference equation S2x[n] = Sx[n] +
x[n] and denote X(z) ≜Z

x[n]

. Using (4.78) and (4.79), we calculate that the Fibonacci
difference equation implies
z2X(z) −z2x[0] −z x[1] = (zX(z) −z x[0]) + X(z),
that is, using the ICs,
(z2 −z −1)X(z) = z (z x[0] + x[1] −x[0]) = z2 · 0 + z(1 −0) = z · 1.
So,
X(z) = z ·
1
z2 −z −1.
Using the quadratic formula, as in Example 4.37 in Section 4.6 , we factor z2 −z −1 =

z −1+
√
5
2
 
z −1−
√
5
2

.
Use partial fractions in the form
1
z2 −z −1 =
A
z −1+
√
5
2
+
B
z −1−
√
5
2

344
Advanced Engineering Mathematics
where A and B are constants to be determined. Multiply through by

z −1+
√
5
2
 
z −1−
√
5
2

to get
1 = A

z −1 −
√
5
2
 
+ B

z −1 +
√
5
2
 
.
Substitute in z = 1+
√
5
2
and z = 1−
√
5
2
to get, respectively,
1 =

1 +
√
5
2
−1 −
√
5
2
 
A,
hence
A =
1
√
5
,
and
1 =

1 −
√
5
2
−1 +
√
5
2
 
B,
hence
B = −1
√
5
.
So,
X(z) = z ·

1
√
5
·
1
z −1+
√
5
2
−1
√
5
·
1
z −1−
√
5
2
 
.
Using (4.75) in reverse, that is, taking x[n] = Z−1
X(z)

, gives that the solution is
x[n] =
1
√
5
·

1 +
√
5
2
 n
−1
√
5
·

1 −
√
5
2
 n
. ⃝
This conclusion agrees with (4.60) in Section 4.6.
4.7.1 Sinusoidal Signals
Again, from our study of difference equations, we know of other signals whose z-
transforms should be relevant: First, using Euler’s formula, we note that
Z

cos[ωn]

= Z
+1
2

eiωn
+

e−iωn ,
= Z
+1
2

eiωn
+

e−iωn ,
;
hence, using (4.75),
Z

cos[ωn]

=
1
2 z
z −eiω +
1
2 z
z −e−iω = z ·
1
2 (z −e−iω) + 1
2 (z −eiω)
(z −eiω)(z −e−iω)
= z ·
z −eiω + e−iω
2
z2 −(eiω + e−iω)z + 1,
so
Z

cos[ωn]

= z ·
z −(cos ω)
z2 −2(cos ω)z + 1.
(4.80)

Scalar ODEs II
345
Similarly,
Z

sin[ωn]

= 1
2i Z
+ 
eiωn
−

e−iωn ,
;
hence, using (4.75),
Z

sin[ωn]

=
1
2i z
z −eiω −
1
2i z
z −e−iω = z ·
1
2i (z −e−iω) −1
2i (z −eiω)
(z −eiω)(z −e−iω)
= z ·
eiω −e−iω
2i
z2 −(eiω + e−iω)z + 1,
so
Z

sin[ωn]

= z ·
sin ω
z2 −2(cos ω)z + 1.
(4.81)
4.7.2 Steady-State Solution
Analogous to the situation for functions x(t), we have the concepts of transient and steady-
state signal solutions.
Definition 4.10
A transient solution of an LCCHE is a signal x[n] that (a) is either a particular solution
or a solution of the corresponding homogeneous LCCHE and (b) has limn→∞x[n] = 0.
Let xT[n] denote a transient solution. Intuitively, a transient solution is that part of a
solution that becomes insignificant eventually.
The definition does not say that a transient solution must be a solution, by itself, of the
LCCHE. As we will see, often, but not always, a transient solution turns out to be a
solution of the corresponding linear homogeneous LCCHE.
Definition 4.11
A signal f[n] is bounded as n →∞if there is no subsequence nm for which
limm→∞| f[nm]| = ∞.
For example, f[n] ≜enπ cos nπ
2 is not bounded as n →∞because, using the subsequence
nm = 2m, f[2m] = |e2mπ cos mπ| = e2mπ →∞as m →∞.

346
Advanced Engineering Mathematics
Definition 4.12
(a) If a solution of an LCCHE can be written as x[n] = xT[n] + xS[n], where
• xT[n] is a transient solution, and
• xS[n] is bounded as t →∞and does not have
limn→∞xS[n] = 0,
then we say xS[n] is a steady-state solution.
(b) Alternatively, if a solution x[n] of a LCCHE is bounded as n →∞and does
not have limn→∞x[n] = 0, then we say that x[n] = xS[n] is itself a steady-state
solution.
Example 4.43
Find the steady-state solution of x[n + 1] = 1
3 x[n] + 7 cos πn
3 , with IC x[0] = 0.
Method: Take the z-transforms of both sides of the difference equation Sx[n] = 1
3 x[n] +
7 cos πn
3 and denote X(z) ≜Z

x[n]

. Using (4.78), and (4.80) with ω = π
3 , hence cos ω =
1
2 and sin ω =
√
3
2 , we calculate that the difference equation implies
zX(z) −z x[0] = 1
3 X(z) + z ·
7

z −1
2

z2 −z + 1;
hence, using the IC,

z −1
3

X(z) = z ·
7

z −1
2

z2 −z + 1.
So,
X(z) = z ·
7

z −1
2

(z2 −z + 1)

z −1
3
.
Use partial fractions in the form
7

z −1
2

(z2 −z + 1)

z −1
3
 =
A
z −1
3
+
Bz + C
z2 −z + 1
where A, B,
and C are constants to be determined.
We multiply through by
(z2 −z + 1)

z −1
2

to get
(⋆)
7

z −1
2

= A(z2 −z + 1) + (Bz + C)

z −1
3

.
Substitute in z = 1
3 to get
−7
6 = A
1
3
2
−1
3 + 1
 
;
hence,
A = −3
2.

Scalar ODEs II
347
Substitute this value of A into (⋆) to get
7

z −1
2

= −3
2(z2 −z + 1) + (Bz + C)

z −1
3

;
hence,
7

z −1
2

+ 3
2(z2 −z + 1) = (Bz + C)

z −1
3

,
that is,
3
2 z2 + 11
2 z −2 = (Bz + C)

z −1
3

.
So,
(Bz + C) =
3
2 z2 + 11
2 z −2

z −1
3

= 3
2 z + 6.
This gives
X(z) = z ·

−3
2 ·
1
z −1
3
+
3
2 z + 6
z2 −z + 1
 
.
We want to use (4.75), (4.80), and (4.81) in reverse in order to find x[n] = Z−1
X(z)

.
We already know that
Z
+
cos πn
3
,
= z ·

z −1
2

z2 −z + 1,
and (4.81) with ω = π
3 = 1
2 implies
Z
+
sin πn
3
,
= z ·
√
3
2
z2 −z + 1.
So, concerning the
3
2 z + 6
z2 −z + 1 part of X(z), we calculate
Z−1+
z ·
3
2 z + 6
z2 −z + 1
,
= Z−1
⎡
⎣z ·
3
2

z −1
2 + 1
2

+ 6
z2 −z + 1
⎤
⎦= Z−1
⎡
⎣z ·
3
2

z −1
2

+ 27
4
z2 −z + 1
⎤
⎦
= Z−1
⎡
⎣z ·
3
2

z −1
2

z2 −z + 1
⎤
⎦+ Z−1
&
z ·
27
4
z2 −z + 1
'
= 3
2 · cos πn
3 + 27
4 · 2
√
3
· sin πn
3 .
The solution of the LCCHE and the IC is
x[n] = Z−1+
−3
2 ·
1
z −1
3
,
+ 3
2 · cos πn
3 + 9
√
3
2
· sin πn
3
= −3
2 ·
1
3
n
+ 3
2 · cos πn
3 + 9
√
3
2
· sin πn
3 .

348
Advanced Engineering Mathematics
The transient solution is xT[n] = −3
2 ·

1
3
n
. The steady-state solution is
xS[n] = 3
2 cos πn
3 + 9
√
3
2
sin πn
3 . ⃝
In Problem 4.7.5.10, you will express this steady-state solution in the amplitude-phase
form.
4.7.3 Convolution and z-Transforms
The convolution of two signals is defined by
(f ∗g)[n] ≜
n
-
m=0
f[m]g[n −m].
(4.82)
and satisfies the following:
Theorem 4.18
(Convolution)
Z

(f ∗g)[n]

= F(z) · G(z).
(4.83)
4.7.4 Transfer Function
Example 4.44
The signal x[n] is the solution of a digital filter
x[n + 2] −a1x[n + 1] + a2x[n] = y[n + 1] + βy[n],
with ICs x[0] = x[1] = 0 and y[0] = 0. Find the input–output relationship between the
signals x[n] and y[n].
Method: Using (4.78) and (4.79), we have
z2X(z) −z2x[0] −z x[1] −a1 (zX(z) −z x[0]) −a2X(z) = zY(z) −z y[0] + βY(z).
The ICs reduce this to
(z2 −a1z −a2)X(z) = (z + β)Y(z);
hence,
X(z) =
z + β
z2 −a1z −a2
Y(z).
Define the transfer function by
T(z) =
z + β
z2 −a1z + a2
,

Scalar ODEs II
349
so X(z) = T(z) · Y(z). The convolution theorem implies
x[n] =

Z−1
T(z)

∗y

[n]. ⃝
Learn More About It
Complex Variables for Mathematics and Engineering, 5th edn., by John H. Mathews and
Russell W. Howell, Jones and Bartlett Publisher, Inc., c⃝2006, has an extensive chap-
ter on z transforms and applications. Chapter 4 of Lectures on Applications-Oriented
Mathematics, by Bernard Friedman, ed. by Victor Twersky, Holden-Day Inc., c⃝1969
(reprinted by Wiley Classics Library, c⃝1991) has useful material on summation by
parts and on use of the “generating function,” which involves nonnegative powers of
z rather the nonpositive powers of z used in the z transform.
4.7.5 Problems
In problems 1 and 2, solve the difference equation with the IC(s).
1. x[n + 1] = −x[n] + αn, with IC x[0] = 1, assuming the constant α ̸= 1.
2. x[n + 1] = 1
2 x[n] + αn, with IC x[0] = 0, assuming the constant α ̸= 1.
In problems 3–5, find the steady-state solution of the difference equation with the IC(s).
3. x[n + 2] = −1
2 x[n + 1] + 1
2 x[n] + cos πn
3 , with IC x[0] = x[1] = 0.
4. x[n + 1] = 1
2 x[n] + 12 cos πn
3 , with IC x[0] = 1.
5. x[n + 2] = −x[n] + αn, with IC x[0] = x[1] = 0, assuming the constant α satisfies
|α| < 1.
6. Use
table
entry
Z.11
in
Table
4.3
to
explain
why
Z

αn cos ωn

= z ·
z −(α cos ω)
z2 −2z cos ω + 1.
7. Use
table
entry
Z.11
in
Table
4.3
to
explain
why
Z

αn sin ωn

= z ·
a sin ω
z2 −2z(cos ω) + 1.
8. Find the general solution of the Fibonacci LCCHE (4.54) in Section 4.6, that is,
x[n + 2] = x[n + 1] + x[n] in terms of the unspecified initial values x[0] and x[1].
9. Solve x[n + 2] = x[n + 1] + 2x[n] + cos πn
3 , with IC x[0] = x[1] = 0.
10. Express the steady-state solution of Example 4.43 in the amplitude-phase form.
Key Terms
analog-to-digital converter: beginning of Section 4.7
basic solution: Definition 4.8 in Section 4.6
beats phenomenon: after (4.23) in Section 4.2
Casorati determinant: before Definition 4.8 in Section 4.6
complete set of basic solutions: Definition 4.8 in Section 4.6
convolution: Definition 4.4 in Section 4.5, (4.82) in Section 4.7

350
Advanced Engineering Mathematics
TABLE 4.3
Table of Unilateral z-Transforms, Where c, ω Are Positive Constants and ℓ
Is a Nonnegative Integer
Formula
f[n]
X(z) = L[f[n]]
Z.1
x[n] + y[n]
X(z) + Y(z)
Z.2
cx[n]
cX(z)
Z.3
αn
z·
1
z−α
Z.4
x[n + 1]
zX(z) −z x[0]
Z.5
x[n + 2]
z2X(z) −z2x[0] −z x[1]
Z.6
step[n]
1
z−1
Z.7
n step[n]
z
(z−1)2
Z.8
n2 step[n]
z(z+1)
(z−1)3
Z.9
sin ωn
z·
sin ω
z2−2z cos ω+1
Z.10
cos ωn
z·
z−(cos ω)
z2−2z cos ω+1
Z.11
αny[n]
Y
 z
α

Z.12
(f ∗g)[n]
F(z) · G(z)
LZ.13
step[n −ℓ] y[n −ℓ]
z−ℓY(z)
Z.14
δ[n −ℓ] y[n −ℓ]
z−ℓy[ℓ]
delta function: Definition 4.6 in Section 4.5
de-tuning parameter: before Example 4.14 in Section 4.2
digital filter: Example 4.44 in Section 4.7
discrete-time signal: beginning of Section 4.7
forward difference operator: beginning of Section 4.6.1
frequency response: (4.13) in Section 4.2
general solution: Definition 4.1 in Section 4.1, Definitions 4.8 and 4.9 in Section 4.6
method of undetermined coefficients: Section 4.1.2
n-th order difference equation, solution: (4.55) in Section 4.6
n-th order nonhomogeneous linear difference equation: Definition 4.9 in Section 4.6
one-sided z-transform: (4.74) in Section 4.7
particular solution: Definition 4.1 in Section 4.1, Definition 4.9 in Section 4.6
periodic: Definition 4.2 in Section 4.2 and before (4.53) in Section 4.5
polar form: before Example 4.38 in Section 4.6
poles: Definition 4.5 in Section 4.5
practical resonance: before Example 4.14 in Section 4.2
pulse: before (4.50) in Section 4.5
quasiperiodic: Definition 4.2 in Section 4.2
resonance: Section 4.2.1
sawtooth function: Definition 4.7 in Section 4.5
shift operator S: (4.77) in Section 4.7
signal: beginning of Section 4.7
solution in the frequency domain: after Example 4.20 in Section 4.4
square wave: (4.53) in Section 4.5
steady-state solution: before Example 4.12 in Section 4.2, Definition 4.12 in Section 4.7

Scalar ODEs II
351
transfer function: Definition 4.5 in Section 4.5, Example 4.44 in Section 4.7
transient solution: Definition 4.10 in Section 4.7
unilateral z-transform: (4.74) in Section 4.7
unit step function: Definition 4.3 in Section 4.5
variation of constants, variation of parameters: Example 4.16 in Section 4.3
References
Bearman, P.W., Vortex shedding from Oscillating bluff bodies, Ann. Rev. Fluid Mech., 16, 195–222,
1984.
Farlow, J., Hall, J.E., Mc Dill, J.M., and West, B.H., Differential Equations and Linear Algebra, 2nd edn.,
Prentice Hall, Inc., 2007.
Ugural, A.C. and Fenster, S.K., Advanced Strength and Applied Elasticity, Pearson, 2003, Section 8.6.


5
Linear Systems of ODEs
5.1 Systems of ODEs
In a sense, Chapter 5 equals Chapter 2 “plus” Chapter 3, in the sense that Chapter 5
combines use of matrix theory and ordinary differential equation (ODE) methods. When
we have more than one linear ODE, results from matrix theory turn out to be useful.
Example 5.1
For the circuit shown in Figure 5.1, let v(t) be the voltage drop across the capacitor and
I(t) be the loop current. The input V(t) is a given function. Assume, as usual, that L, R,
and C are constants. Write down a system of ODEs in R2 that models this circuit.
Method: The series RLC circuit shown in Figure 5.1 is analogous to the DC series RLC
circuit discussed near the end of Section 3.3. The first ODE models the voltage drop
across the capacitor being v(t) =
1
C q(t), where q(t) is the charge on the capacitor and
˙q(t) = I(t). The second ODE in the system is Kirchhoff’s voltage law, L˙I(t)+RI(t)+v(t) =
V(t), after dividing through by L. The system is
⎧
⎨
⎩
˙v(t) = 1
C I(t)
˙I(t) = 1
L (V(t) −RI(t) −v(t))
⎫
⎬
⎭. ⃝
(5.1)
More generally, consider a system of two ODEs in unknowns x1(t), x2(t):
⎧
⎨
⎩
˙x1(t) = F1

t, x1(t), x2(t)
	
˙x2(t) = F2

t, x1(t), x2(t)
	
⎫
⎬
⎭.
(5.2)
A special case is
⎧
⎨
⎩
˙x1(t) = a11(t)x1 + a12(t)x2 + f1(t)
˙x2(t) = a21(t)x1 + a22(t)x2 + f2(t)
⎫
⎬
⎭,
(5.3)
which is called a linear system.
In (5.3), we write x1 instead of x1(t) even though x1 is a function of t; we call this
“suppressing the dependence on t” from the unknowns x1, x2. We will not suppress
dependence on t in the coefficients aij(t) or the right-hand sides fi(t).
353

5
Linear Systems of ODEs
5.1 Systems of ODEs
In a sense, Chapter 5 equals Chapter 2 “plus” Chapter 3, in the sense that Chapter 5
combines use of matrix theory and ordinary differential equation (ODE) methods. When
we have more than one linear ODE, results from matrix theory turn out to be useful.
Example 5.1
For the circuit shown in Figure 5.1, let v(t) be the voltage drop across the capacitor and
I(t) be the loop current. The input V(t) is a given function. Assume, as usual, that L, R,
and C are constants. Write down a system of ODEs in R2 that models this circuit.
Method: The series RLC circuit shown in Figure 5.1 is analogous to the DC series RLC
circuit discussed near the end of Section 3.3. The first ODE models the voltage drop
across the capacitor being v(t) =
1
C q(t), where q(t) is the charge on the capacitor and
˙q(t) = I(t). The second ODE in the system is Kirchhoff’s voltage law, L˙I(t)+RI(t)+v(t) =
V(t), after dividing through by L. The system is
⎧
⎨
⎩
˙v(t) = 1
C I(t)
˙I(t) = 1
L (V(t) −RI(t) −v(t))
⎫
⎬
⎭. ⃝
(5.1)
More generally, consider a system of two ODEs in unknowns x1(t), x2(t):
⎧
⎨
⎩
˙x1(t) = F1

t, x1(t), x2(t)
	
˙x2(t) = F2

t, x1(t), x2(t)
	
⎫
⎬
⎭.
(5.2)
A special case is
⎧
⎨
⎩
˙x1(t) = a11(t)x1 + a12(t)x2 + f1(t)
˙x2(t) = a21(t)x1 + a22(t)x2 + f2(t)
⎫
⎬
⎭,
(5.3)
which is called a linear system.
In (5.3), we write x1 instead of x1(t) even though x1 is a function of t; we call this
“suppressing the dependence on t” from the unknowns x1, x2. We will not suppress
dependence on t in the coefficients aij(t) or the right-hand sides fi(t).
353

354
Advanced Engineering Mathematics
L
V(t)
I
R
υ(t)
C
FIGURE 5.1
RLC series circuit.
The simplest such system is
⎧
⎨
⎩
˙x1 = a11x1 + a12x2
˙x2 = a21x1 + a22x2
⎫
⎬
⎭,
(5.4)
where a11, a12, a21, a22 are constants.
Chapter 5 will focus on linear constant coefficients homogeneous systems (LCCHS) and
their linear nonhomogeneous analogues; in Chapter 18, we will look at nonlinear systems
of ODEs, including how they relate to linear homogeneous systems of ODEs.
More generally, we can study systems involving n unknowns, x1(t), x2(t), . . . , xn(t).
The system is in Rn if n is the number of unknown functions whose derivatives appear,
assuming no derivatives higher than the first appear. So, (5.1) through (5.4) are all systems
in R2.
We can write system (5.2) compactly in vector form as
˙x(t) = F

t, x(t)
	
,
where we define
x(t) ≜

x1(t)
x2(t)

,
˙x(t) ≜d
dtx(t) ≜

˙x1(t)
˙x2(t)

,
and
F

t, x(t)
	
≜

F1 (t, x1(t), x2(t))
F2 (t, x1(t), x2(t))

.
We can rewrite linear system (5.3) compactly in matrix–vector form as
˙x(t) = A(t)x(t) + f(t),
where we define
A(t) ≜

a11(t)
a12(t)
a21(t)
a22(t)

and
f(t) ≜

 f1(t)
f2(t)

.

Linear Systems of ODEs
355
Here we extend the definition of multiplication of matrix times vector to functions of t:
A(t)x(t) =

a11(t)
a12(t)
a21(t)
a22(t)
 
x1(t)
x2(t)

≜

a11(t)x1(t) + a12(t)x2(t)
a21(t)x1(t) + a22(t)x2(t)

.
In particular, the system of ODEs (5.4) can be written compactly as
˙x = Ax,
where the constant coefficient matrix is A =

a11
a12
a21
a22

.
All of (5.1) through (5.4) can be generalized to systems in Rn, for example, ˙x = Ax can be
short for
⎡
⎢⎢⎢⎢⎣
˙x1
.
.
.
˙xn
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
an1
.
.
.
ann
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
x1
.
.
.
xn
⎤
⎥⎥⎥⎥⎦
.
Definition 5.1
A solution of an ODE system in Rn,
˙x = F(t, x),
(5.5)
is an n-vector of functions x(t) defined on an open interval I for which the derivative also
exists on I and satisfies (5.5), that is, ˙x(t) = F

t, x(t)
	
, for t in I.
Theorem 5.1
(Existence and uniqueness for solution of a linear system) If the matrix of functions A(t)
and the vector of functions f(t) are continuous for t in an open interval I and t0 is inside
I, then the initial value problem (IVP) for the linear system

˙x = A(t)x + f(t)
x(t0)= x0

has exactly one solution on I.
Example 5.2
For the circuit shown in Figure 5.2, let v1(t), v2(t) be the voltage drops across the capaci-
tors whose capacitances are C1, C2 and let I1(t), I2(t) be the loop currents. Write down a
system of ODEs in R3 that models this circuit, assuming L, R, C1, and C2 are, as usual,
constants.
Method: In the first loop, Kirchhoff’s voltage law gives
L˙I1(t) + v1(t) = V(t).
(5.6)

356
Advanced Engineering Mathematics
L
R
C1
C2
I1
V(t)
v1(t)
v2(t)
I2
FIGURE 5.2
RLC two-loop circuit.
The input V(t) is a given function. In the second loop, Kirchhoff’s voltage law gives the
algebraic equation
RI2(t) + v2(t) −v1(t) = 0,
which we can solve for I2 in terms of v1(t), v2(t) to get
I2(t) = 1
R(v1(t) −v2(t)).
(5.7)
In terms of the loop currents, the voltages across the capacitors satisfy
˙v1(t) = 1
C1
(I1 −I2)
and
˙v2(t) = 1
C2
I2.
(5.8)
Together, (5.6) through (5.8) give a linear system in R3, that is, a linear system of three
ODEs in three unknowns, I1(t), v1(t), and v2(t):
d
dt
⎡
⎢⎢⎢⎢⎣
I1(t)
v1(t)
v2(t)
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
0
−1
L
0
1
C1
−
1
C1R
1
C1R
0
1
C2R
−
1
C2R
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
I1(t)
v1(t)
v2(t)
⎤
⎥⎥⎥⎥⎦
+
⎡
⎢⎢⎢⎢⎣
1
LV(t)
0
0
⎤
⎥⎥⎥⎥⎦
. ⃝
(5.9)
Example 5.3
Rewrite as a linear system the ODE that modeled the spring–mass–damper system at
the beginning of Section 3.3.
Method: The spring–mass–damper system is modeled by ODE m¨y + b˙y + ky = 0. If we
define the velocity of the mass by v = ˙y, the physical situation is modeled by the system
of two ODEs:
⎧
⎨
⎩
˙y = v
m˙v = Forces = −bv −ky
⎫
⎬
⎭.
We can rewrite these ODEs in the matrix–vector form of a linear system in R2:
d
dt
⎡
⎣
y
v
⎤
⎦=
⎡
⎣
0
1
−k
m
−b
m
⎤
⎦
⎡
⎣
y
v
⎤
⎦. ⃝
(5.10)

Linear Systems of ODEs
357
Example 5.4
Suppose an object has temperature T and it is in a medium whose temperature is M. In
Section 3.1, we used Newton’s law of cooling,
˙T(t) = −kT(T −M),
where kT is a constant dependent on the object’s material nature, in units of 1
s .
Unlike Section 3.1, now let us assume that the temperature of the medium is affected
by the object. Find a system of ODEs that models the whole situation.
Method: Let us apply Newton’s law of cooling to the medium. We get
˙M(t) = −kM(M −T),
where kM is a constant dependent on the medium’s material nature.
So, the temperature of the medium affects the temperature of the object, which in turn
affects the temperature of the medium: the temperatures of the object and the medium
are intertwined. They satisfy the system of ODEs:
d
dt

 T
M

=

−kT
kT
kM
−kM
 
 T
M

.
(5.11)
We’ll assume that kT, kM are constants, which is reasonable as long as the temperatures
are not changing too much and the materials are not changing their phases. ⃝
5.1.1 Systems of Second-Order Equations
We saw that a second-order scalar ODE can be rewritten as a system of two first-order
scalar ODEs. Newton’s second law of motion relating the acceleration of an object to the
sum of the forces naturally leads to a second-order ODE.
Similarly, if there are several objects, Newton’s law will apply to each of them, giving a
system of second-order scalar ODEs.
Just as for a single second-order scalar ODE, we can rewrite a system of m second-order
scalar ODEs as a system of 2m first-order scalar ODEs. We will see that for certain systems
of second-order scalar ODEs, it is simpler to leave them as first-order ODEs.
Example 5.5
Describe the motion of the two objects, whose masses are m1 and m2, in the phys-
ical system depicted in Figure 5.3. Assume that the system is in equilibrium when
In equilibrium
k1
k1
k2
k2
k3
k3
x >0
x1
x2
ℓ
x1= 0
x2= 0
ℓ+ (x2– x1)
FIGURE 5.3
Two masses and three horizontal springs.

358
Advanced Engineering Mathematics
x1 = x2 = 0. As depicted in the picture, k1, k2, k3 are the spring constants of the three
horizontal springs. Assume there are no damping forces.
Method: Assume x1 > 0 when the first object is to the right of its equilibrium position
and similarly for x2 > 0. The first spring is stretched a distance of x1, if x1 > 0, and con-
versely, the first spring is compressed a distance of −x1, if x1 < 0. The first spring exerts
a force of −k1x1 on the first object, so the first spring acts to bring the first object back to
equilibrium.
The third spring is compressed by a distance of x2 if x2 > 0, and conversely, the third
spring is stretched by a distance of −x2, if x2 < 0. The third spring exerts a force of
−k3x2 on the second object, so the third spring acts to bring the second object back to
equilibrium.
The second, middle spring is compressed by a distance of x1 and compressed by a
distance of −x2. In the picture, x2 > 0, and conversely, the position of the second object
contributes a negative compression, that is, a positive stretch, to the length of the middle
spring. So, the middle spring has (net compression) = x1 + (−x2) = (x1 −x2), that is, the
middle spring has (net stretch) = −(net compression) = (x2−x1). The middle spring exerts
on the first object a force of k2(net stretch), that is, k2(x2 −x1). [For example, the picture
has x1 > x2, so the middle spring pulls the first object to the right.] The middle spring
exerts on the second object a force of k2(x1 −x2). In the picture, x1 > x2, so the middle
spring pushes the second object to the right.
Newton’s second law of motion gives us the ODEs
m1¨x1 = Forces on first object = −k1x1 + k2(x2 −x1)
and
m2¨x2 = Forces on second object = k2(x1 −x2) −k3x3.
Recall that we assumed this system has no damping forces.
We can write this system of second-order ODEs in terms of the vector x =

x1
x2

:
¨x =
⎡
⎢⎢⎢⎣
−k1 + k2
m1
k2
m1
k2
m2
−k2 + k3
m2
⎤
⎥⎥⎥⎦x ≜Ax. ⃝
(5.12)
In Problem 5.1.3.2, you will choose specific values for the physical parameters in
Example 5.5.
5.1.2 Compartment Models
In many biological and chemical systems, there is one or several species or locations of mat-
ter. For example, some matter may transmutate from one isotope into another isotope. In
another example, one type of organism may utilize other organisms to survive or increase
its population.
In Example 5.6 in the following, iodine moves among several locations or categories
in the human body and also leaves the body. Those locations are called compartments. In
Problem 3.1.4.32, we had a one compartment model for the amount of glucose in the blood-
stream. Aside from a basic scientific interest, the study of iodine in the body is relevant to
the prevention of radioactive contamination of the thyroid gland.

Linear Systems of ODEs
359
Example 5.6
(Compartmental model of iodine metabolism) (Riggs, 1952) Iodide compounds contain-
ing iodine are absorbed from food by the digestive system, circulate in the bloodstream,
accumulate in and are used by the thyroid gland and other body tissues including the
organs, and are excreted from the body in urine and, to usually a lesser extent, in feces.
The thyroid gland uses iodine to produce and store thyroid hormone, which is essential
to health. As body tissues use the hormone, it sends iodine, a breakdown product, back
into the bloodstream.
Write down a system of ordinary differential equations modeling the amounts of
iodine in the bloodstream, the thyroid, other body tissues (including other organs), the
urine, and the feces, assuming that the rate of iodine flow out of a compartment to
another is proportional to the amount of iodine in the compartment.
Method: As Riggs (see Riggs, 1952) put it, “· · · these so-called compartments do not
exist within the body as actual physical entities with clearly defined boundaries, but are
merely convenient abstractions.” The amounts of iodine in the five compartments are
defined by
x1(t) = the amount of iodine in the bloodstream
x2(t) = the amount of iodine in the thyroid
x3(t) = the amount of iodine in other body tissues
x4(t) = the amount of iodine excreted in feces
x5(t) = the amount of iodine in urine
We will ignore time delays in the movements of iodine due to nonuniform spatial
distributions. Also, we will not distinguish between the many iodide compounds in
which iodine is found in the body.
The flows of iodine are depicted in Figure 5.4.
The rate of change of x1(t) includes flows into the bloodstream from the digestive
system at a rate f1 and from the other body tissues from breakdown of hormone. We
will ignore flow of iodine into the bloodstream from the thyroid because we assume
that hormone moves very quickly from the bloodstream to the other body tissues.
The rate of change of x1(t) includes flows out of the bloodstream as the thyroid absorbs
iodine, as hormone is absorbed by the other body tissues, and as iodine is excreted in
urine:
˙x1 = a11x1 + a13x3 + f1,
with constant a11 < 0 and constants f1, a13 > 0.
x1
Iodine in
bloodstream
(a11)
f1
a21
a13
a32
a43
a51
x2
Iodine in
thyroid
(a22)
x3
Iodine in
feces
x5
Iodine in
urine
x3
Iodine in
body tissues
(a33)
FIGURE 5.4
Example 5.6: Iodine model.

360
Advanced Engineering Mathematics
The rate of change of x2(t) includes flows into the thyroid from the bloodstream and
flow out in the form of hormone:
˙x2 = a21x1 + a22x2,
with constant a22 < 0 and constant a21 > 0.
The rate of change of x3(t) includes flows into the other body tissues “directly” from
the thyroid and flow out as a breakdown product of hormone:
˙x3 = a32x2 + a33x3,
with constant a33 < 0 and constant a31 > 0.
The rate of change of x4(t) includes flows into the feces from the other body tissues,
specifically the liver. The rate of change of x5(t) includes flows into the urine from the
bloodstream via the kidney(s):
˙x4 = a43x3
˙x5 = a51x1,
with constants a41, a51 > 0.
By the conservation of iodine, we have 0 = a11 + a21 + a51, 0 = a22 + a32, and 0 =
a33 + a13 + a43.
Altogether, the system of ODEs is
˙x =
⎡
⎢⎢⎢⎢⎣
a11
0
a13
0
0
a21
a22
0
0
0
0
a32
a33
0
0
0
0
a43
0
0
a51
0
0
0
0
⎤
⎥⎥⎥⎥⎦
x +
⎡
⎢⎢⎢⎢⎣
f1
0
0
0
0
⎤
⎥⎥⎥⎥⎦
.
We can solve the first three ODEs together by themselves because the amounts x4 and
x5 do not affect x1, x2, or x3. Thus, the system can be reduced to
(⋆)
⎡
⎣
˙x1
˙x2
˙x3
⎤
⎦=
⎡
⎣
a11
0
a13
a21
a22
0
0
−a22
a33
⎤
⎦
⎡
⎣
x1
x2
x3
⎤
⎦+
⎡
⎣
f1
0
0
⎤
⎦.
After solving (⋆), we integrate x1(t) and x3(t) to find x4(t) and x5(t), which modelers can
use as measurable outputs from the body in order to estimate the other parameters in
the system.
According to Killough and Eckerman (1984), appropriate values for the constants are
a11 = −2.773, a13 = 5.199 × 10−2, a21 = 0.832, a22 = −8.664 × 10−3,
a32 = 8.664 × 10−3, a33 = −5.776 × 10−2, a43 = 5.770 × 10−3, a51 = 1.941. ⃝
We’ll explain how to solve all six of Examples 5.1 through 5.6 in the next four sections.
5.1.3 Problems
1. Modify the iodine metabolism model of Example 5.6 to include the assumption
that iodine also flows into the bloodstream from the thyroid in the form of hor-
mone and from there flows into the other body tissues, that is, the flow of iodine
from the thyroid to other body tissues is indirect.

Linear Systems of ODEs
361
2. Write a specific example of the system of two second-order ODEs in (5.12) after
choosing specific values for the physical parameters.
3. Write a general model for a system of three masses and four springs that
generalizes the system of two second-order ODEs in (5.12).
4. Rewrite the system of two second-order ODEs in (5.12) as a system of four first-
order ODEs in a manner similar to what was done in Example 5.3.
5. In each of the two tanks depicted in Figure 5.5, there is a mixture containing
a dye. Write down a system of two first-order ODEs specifying the amount of
dye in tanks #1 and #2. The numbers in the tanks specify the volumes of mix-
ture in the tanks. Each inflow arrow comes with two pieces of information: a
flow rate, in gallons per minute, and a concentration of dye, in pounds per
gallon; if a concentration is not specified, assume that the mixture in the tank
is well-mixed and the concentration in the outflow equals the concentration in
the tank.
6. For the circuit shown in Figure 5.6, let v1(t) be the voltage drop across the first
resistor and v2(t) be the voltage drop across the capacitor, and let I1(t), I2(t) be
the loop currents. Write down a system of ODEs in R3 that models this circuit,
assuming L, R1, R2, and C are, as usual, constants.
7. Suppose two objects have temperatures T1 and T2 and they are in a medium whose
temperature is M. Assuming the two objects are far apart from each other, find a
system of three ODEs that models the whole situation.
4 gal/min
2 lb/gal
1 gal/min
5 gal/min
3 gal/min
2 gal/min
Tank #1
50 gal
Tank #2
70 gal
FIGURE 5.5
Problem 5.1.3.5.
L
R2
R1
C2
I1
V(t)
v1(t)
v2(t)
I2
FIGURE 5.6
Problem 5.1.3.6.

362
Advanced Engineering Mathematics
5.2 Solving Linear Homogenous Systems of ODEs
While most of our attention will be devoted to solving LCCHS
˙x = Ax,
(5.13)
we will also discuss general systems of linear homogeneous ODEs whose coefficients are
not necessarily constant.
What we will learn in Sections 5.2 and 5.3 for systems of linear homogeneous ODEs will
also be useful in Sections 5.4 and 5.5 for solving systems of linear nonhomogeneous ODEs.
Example 5.7
Use eigenvalues and eigenvectors to solve the LCCHS:
˙x =

−4
−2
6
3

x.
(5.14)
Method: Let A =

−4
−2
6
3

. Because A is 2 × 2, x must be a vector in R2.
In Chapter 3, we tried solutions of scalar linear, constant coefficients homogeneous
ordinary differential equations (LCCHODEs) of the form y(t) = cest, where c and s were
constants. Now let’s try solutions of (5.14) in the form
x(t) = eλtv,
where λ is a constant and v is a constant vector in R2. First, we note that d
dt

eλtv

=
λeλtv; you will explain why in Problem 5.2.5.16. So, substituting x(t) into LCCHS (5.14),
we want ˙x = Ax, that is,
λeλtv = d
dt

eλtv

= d
dt

x(t)

= Ax(t) = A(eλtv) = eλtAv.
Multiplying through by e−λt gives us
λv = Av.
So, we want v to be an eigenvector of A corresponding to eigenvalue λ.
In Example 2.4 in Section 2.1, we found eigenvalues and eigenvectors of this matrix A:
λ1 = −1 with v(1) =

 2
−3

and
λ2 = 0 with v(2) =

 1
−2

.
Using the principle of linearity superposition,
x(t) = c1eλ1tv(1) + c2eλ2tv(2) = c1e(−1)t

 2
−3

+ c2e0·t

 1
−2

solves (5.14) for arbitrary constants c1, c2. Theorem 5.2 will explain why
x(t) = c1e−t

 2
−3

+ c2

 1
−2

,
where c1, c2 are arbitrary constants, gives all of the solutions of (5.14). ⃝

Linear Systems of ODEs
363
Example 5.8
Solve the IVP for Example 5.4 in Section 5.1 model of the temperatures of the object and
medium, that is,
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩

 ˙T
˙M

=

−kT
kT
kM
−kM
 
 T
M


 T(0)
M(0)

=

 T0
M0

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(5.15)
and interpret the results physically. Assume kT, kM are positive constants.
Method: Let A =

−kT
kT
kM
−kM

. First, find the eigenvalues:
0 = | A −λI | =

−kT −λ
kT
kM
−kM −λ
 = (−kT −λ)(−kM −λ) −kTkM = λ2 + (kT + kM)λ
= λ(λ + kT + kM).
The eigenvalues are λ1 = −(kT + kM) and λ2 = 0. To find the corresponding eigenvectors,
we do two different row reductions: First,

A −

−

kT + kM
		
I | 0

=

kM
kT
| 0
kM
kT
| 0

∼· · · ∼

1⃝
kT/kM
| 0
0
0
| 0

,
so M is the only free variable and the first eigenvalue’s eigenvectors are
v(1) = c1

−kT
kM

, c1 ̸= 0.
Second,

A −(0)I | 0

=

−kT
kT
| 0
kM
−kM
| 0

∼
· · ·
∼

1⃝
−1
| 0
0
0
| 0

,
so M is the only free variable and the second eigenvalue’s eigenvectors are
v(2) = c1

1
1

, c1 ̸= 0.
The solutions of LCCHS (5.15) are
x(t) = c1e−(kT+kM)t

−kT
kM

+ c2

1
1

,
where c1, c2 are arbitrary constants, which we use to satisfy the initial conditions (ICs).
Using Lemma 1.3 in Section 1.7 and defining c = [c1
c2]T, we have

 T0
M0

=

 T(0)
M(0)

=c1

−kT
kM

+ c2

1
1

=

−kT
1
kM
1

c,
which has unique solution

c1
c2

=

−kT
1
kM
1
−1 
 T0
M0

=
1
−kT −kM

 1
−1
−kM
−kT
 
 T0
M0

=
−1
kT + kM

T0 −M0
−kMT0 −kTM0

.

364
Advanced Engineering Mathematics
After some algebraic manipulations, we see that the solution of the IVP is

 T(t)
M(t)

=
1
kT + kM

(M0 −T0)e−(kT+kM)t

−kT
kM

+ (kMT0 + kTM0)

1
1

.
Because e−(kT+kM)t →0 as t →∞, we have
lim
t→∞

 T(t)
M(t)

=
1
kT + kM

kMT0 + kTM0
kMT0 + kTM0

.
Physically, this means that as t gets larger and larger, the temperatures of the object,
T(t), and the surrounding medium, M(t), both approach the steady-state value of
kMT0 + kTM0
kT + kM
.
This is what we think of as “common sense,” specifically that the temperatures T(t), M(t)
should approach thermal equilibrium, as t →∞. But, our analysis establishes this and
tells us the equilibrium temperature value, which depends on the constants kT and kM
and the initial temperatures. Models whose solutions make quantitative predictions are
very useful in engineering and science. ⃝
The system of differential equations (5.11) in Section 5.1, that is, the ODEs in Example
5.8, has constant solutions T(t) ≡M(t) = T∞for any value of the constant T∞, but we need
to know the initial temperatures in order to find what constant value T∞gives physical
equilibrium.
Analogous to Definition 3.10 in Section 3.4, we have
Definition 5.2
The general solution of a linear homogeneous system of ODEs
˙x = A(t)x
(5.16)
in Rn has the form
xh(t) = c1x1(t) + c2x2(t) + · · · + cnxn(t)
if for every solution x∗(t) of (5.16) there are values of constants c1, c2, . . . , cn giving x∗(t) =
c1x1(t) + c2x2(t) + · · · + cnxn(t). In this case, we call the set of functions {x1(t), . . . , xn(t)}
a complete set of basic solutions. Each of the vector-valued functions x1(t), . . . , xn(t) is
called a basic solution of the linear homogeneous system (5.16).
For an LCCHS (5.13), we can say a lot:
Theorem 5.2
For an LCCHS (5.13), that is, ˙x = Ax in Rn, suppose the n × n constant matrix A has a set
of eigenvectors

v(1), . . . , v(n)
that is a basis for Rn. If the corresponding eigenvalues are
λ1, . . . , λn, then

Linear Systems of ODEs
365

eλ1tv(1), . . . , eλntv(n)
is a complete set of basic solutions of ˙x = Ax.
Why? To be very brief, similar to the explanation of Theorem 3.15 in Section 3.4, this fol-
lows from the existence and uniqueness Theorem 5.1 in Section 5.1 combined with Lemma
1.3 in Section 1.7. The next example will illustrate why Theorem 5.1 in Section 5.1 makes
sense. 2
Complex eigenvalues and eigenvectors will be discussed in Section 5.3.
Example 5.9
Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

−4
−2
6
3

x
x(0) =

5
7

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(5.17)
Method: Using Theorem 5.2 and the result of Example 5.7, the general solution of LCCHS
(5.17) is
x(t) = c1e−t

 2
−3

+ c2

 1
−2

,
where c1, c2 are arbitrary constants. Substitute this into the ICs:

5
7

= x(0) = c1

 2
−3

+ c2

 1
−2

.
By Lemma 1.3 in Section 1.7, this is the same as

5
7

=

 2
1
−3
−2

c, which is solved by
c =

 2
1
−3
−2
−1 
5
7

=

 2
1
−3
−2
 
5
7

=

 17
−29

=

c1
c2

.
The solution of the IVP is
x(t) = 17e−t

 2
−3

−29

 1
−2

=

−29 + 34e−t
58 −51e−t

. ⃝
For a quick check of part of the work, substitute in t = 0 to verify that x(0) =

5
7

.
Example 5.10
Find the general solution of
˙x =
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦x.
(5.18)

366
Advanced Engineering Mathematics
Method: In Example 2.5 in Section 2.1, we found that the matrix
A ≜
⎡
⎣
2
2
4
2
−1
2
4
2
2
⎤
⎦
has eigenvalues λ1 = λ2 = −2, λ3 = 7, with corresponding eigenvectors
v(1) =
⎡
⎣
1
−2
0
⎤
⎦,
v(2) =
⎡
⎣
−1
0
1
⎤
⎦,
v(3) =
⎡
⎣
2
1
2
⎤
⎦,
and that set of three vectors is a basis for R3. By Theorem 5.2, the general solution of
LCCHS (5.18) is
x(t) = c1e−2t
⎡
⎣
1
−2
0
⎤
⎦+ c2e−2t
⎡
⎣
−1
0
1
⎤
⎦+ c3e7t
⎡
⎣
2
1
2
⎤
⎦,
where c1, c2, c3 are arbitrary constants. ⃝
Example 5.11
Solve the IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
˙x =
⎡
⎣
−4
0
3
0
−4
0
1
0
−2
⎤
⎦x
x(0) =
⎡
⎣
1
−2
−1
⎤
⎦
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(5.19)
Method: First, we find the eigenvalues using the characteristic equation, by expanding
along the second row:
0 = | A −λI | =

−4 −λ
0
3
0
−4 −λ
0
1
0
−2 −λ

= (−4 −λ)

−4 −λ
3
1
−2 −λ

= (−4 −λ)(λ2 + 6λ + 5) = (−4 −λ)(λ + 5)(λ + 1).
The eigenvalues are
λ1 = −5, λ2 = −4, λ3 = −1.
To find the corresponding eigenvectors, we do three different but easy row reductions.
The first is

A −(−5)I | 0

=
⎡
⎣
1
0
3
| 0
0
1
0
| 0
1
0
3
| 0
⎤
⎦∼
⎡
⎣
1⃝
0
3
| 0
0
1⃝
0
| 0
0
0
0
| 0
⎤
⎦,
so v3 is the only free variable and the first eigenvalue’s eigenvectors are
v(1) = c1
⎡
⎣
−3
0
1
⎤
⎦, c1 ̸= 0.

Linear Systems of ODEs
367
The second is

A −(−4)I | 0

=
⎡
⎣
0
0
3
| 0
0
0
0
| 0
1
0
2
| 0
⎤
⎦∼
· · ·
∼
⎡
⎣
1⃝
0
0
| 0
0
0
1⃝
| 0
0
0
0
| 0
⎤
⎦,
so v2 is the only free variable and the second eigenvalue’s eigenvectors are
v(2) = c1
⎡
⎣
0
1
0
⎤
⎦, c1 ̸= 0.
The third is

A −(−1)I | 0

=
⎡
⎣
−3
0
3
| 0
0
−3
0
| 0
1
0
−1
| 0
⎤
⎦∼
· · ·
∼
⎡
⎣
1⃝
0
−1
| 0
0
1⃝
0
| 0
0
0
0
| 0
⎤
⎦,
so v3 is the only free variable and the third eigenvalue’s eigenvectors are
v(3) = c1
⎡
⎣
1
0
1
⎤
⎦, c1 ̸= 0.
By Theorems 5.2 and 2.7(c) in Section 2.2, the general solution of LCCHS (5.19) is
x(t) = c1e−5t
⎡
⎣
−3
0
1
⎤
⎦+ c2e−4t
⎡
⎣
0
1
0
⎤
⎦+ c3e−t
⎡
⎣
1
0
1
⎤
⎦,
(5.20)
where c1, c2, c3 are arbitrary constants.
To satisfy the ICs, that is,
⎡
⎣
1
−2
−1
⎤
⎦= c1
⎡
⎣
−3
0
1
⎤
⎦+ c2
⎡
⎣
0
1
0
⎤
⎦+ c3
⎡
⎣
1
0
1
⎤
⎦=
⎡
⎣
−3
0
1
0
1
0
1
0
1
⎤
⎦c,
we solve for c:
c =
⎡
⎣
−3
0
1
0
1
0
1
0
1
⎤
⎦
−1 ⎡
⎣
1
−2
−1
⎤
⎦=
⎡
⎣
−0.25
0
0.25
0
1
0
0.25
0
0.75
⎤
⎦
⎡
⎣
1
−2
−1
⎤
⎦=
⎡
⎣
−0.5
−2
−0.5
⎤
⎦.
The solution of the IVP is
x(t) = −1
2 e−5t
⎡
⎣
−3
0
1
⎤
⎦−2e−4t
⎡
⎣
0
1
0
⎤
⎦−1
2 e−t
⎡
⎣
1
0
1
⎤
⎦=
⎡
⎣
1.5e−5t −0.5e−t
−2e−4t
−0.5e−5t −0.5e−t
⎤
⎦. ⃝
(5.21)
We could have used other choices of eigenvectors and thus had a different looking gen-
eral solution. But the final conclusion would still agree with the final conclusion of (5.21).
You will explore this in Problem 5.2.5.19.

368
Advanced Engineering Mathematics
5.2.1 Fundamental Matrix and etA
Example 5.12
Recall that for Example 5.11 the general solution was (5.20), that is,
x(t) = c1e−5t
⎡
⎣
−3
0
1
⎤
⎦+ c2e−4t
⎡
⎣
0
1
0
⎤
⎦+ c3e−t
⎡
⎣
1
0
1
⎤
⎦,
where c1, c2, c3 are arbitrary constants. If we define three vector-valued functions of t by
x(1)(t) ≜e−5t
⎡
⎣
−3
0
1
⎤
⎦,
x(2)(t) ≜e−4t
⎡
⎣
0
1
0
⎤
⎦,
x(3)(t) ≜e−t
⎡
⎣
1
0
1
⎤
⎦,
Lemma 1.3 in Section 1.7 allows us to rewrite the general solution as
x(t) =

x(1)(t)  x(2)(t)  x(3)(t)

c ≜X(t)c.
(5.22)
This defines the 3 × 3 matrix
X(t) =
⎡
⎣e−5t
⎡
⎣
−3
0
1
⎤
⎦



e−4t
⎡
⎣
0
1
0
⎤
⎦



e−t
⎡
⎣
1
0
1
⎤
⎦
⎤
⎦=
⎡
⎣
−3e−5t
0
e−t
0
e−4t
0
e−5t
0
e−t
⎤
⎦. ⃝
This is an example of our next definition.
Definition 5.3
A fundamental matrix of solutions, or fundamental matrix, for a linear homogeneous
system of ODEs (5.16) in Rn, that is, ˙x = A(t)x, is an n × n matrix X(t) satisfying
• Each of its n columns is a solution of the same system (5.16).
• X(t) is invertible for all t in an open time interval of existence.
Definition 5.4
Given n solutions x(1)(t), . . . , x(n)(t) of the same linear homogeneous system ˙x = A(t)x(t) in
Rn, their Wronskian determinant is defined by
W
 
x(1)(t), . . . , x(n)(t)
!
≜
 x(1)(t) 
· · ·
 x(n)(t)
 .
Theorem 5.3
Suppose Z(t) is a fundamental matrix for ˙x = A(t)x. Then the unique solution of the IVP

˙x = A(t)x
x(t0)= x0

(5.23)

Linear Systems of ODEs
369
is given by
x(t) = Z(t)
 
Z(t0)
!−1
x0.
(5.24)
Why? For all constant vectors
c =
⎡
⎢⎣
c1
...
cn
⎤
⎥⎦,
the vector-valued function
Z(t) c =

x(1)(t) 
· · ·
 x(n)(t)

c = c1x(1)(t) + · · · + cnx(n)(t) ≜x(t),
by Lemma 1.3 in Section 1.7. We assumed that the columns of Z(t), that is, the vector-
valued functions x(1)(t), . . . , x(n)(t), are all solutions of ˙x = A(t)x, so the principle of linear
superposition tells us that x(t) is a solution of ˙x = A(t)x.
Also, to solve the ICs, we want
x0 = x(t0) = Z(t0)c,
and this can be accomplished by choosing c ≜(Z(t0))−1 x0. This leads to the solution of the
IVP being
x(t) = Z(t)c = Z(t) (Z(t0))−1 x0. 2
Theorem 5.4
Suppose an n × n matrix A has a set of n real eigenvectors v(1), . . . , v(n) that is a basis for
Rn, corresponding to real eigenvalues λ1, . . . , λn. Then
Z(t) ≜

eλ1tv(1) 
· · ·
 eλntv(n) 
is a fundamental matrix for LCCHS (5.13), that is, ˙x = Ax.
Theorem 5.5
Suppose Z(t) is an n × n-valued differentiable function of t and is invertible for all t in
an open time interval. Then Z(t) is a fundamental matrix of ˙x = A(t)x if, and only if,
˙Z(t) = A(t)Z(t).

370
Advanced Engineering Mathematics
Why? Suppose Z(t) is any fundamental matrix of ˙x = A(t)x. Denote the columns of Z(t) by
z(1)(t), . . . , z(n)(t). Then
˙Z(t) =

˙z(1)(t) 
· · ·
 ˙z(n)(t)

=

A(t)z(1)(t) 
· · ·
 A(t)z(n)(t)

= A(t)

z(1)(t) 
· · ·
 z(n)(t)

= A(t)Z(t),
using Theorem 1.9 in Section 1.2.
In Problem 5.2.5.23, you will explain why the statement “if ˙Z(t) = A(t)Z(t), then the
columns of Z(t) are all solutions of the same linear homogeneous system” is true. That,
and the assumed invertibility of Z(t), would imply that Z(t) is a fundamental matrix of
˙x = A(t)x. 2
Definition 5.5
If X(t) is a fundamental matrix for LCCHS (5.13) in Rn and X(t) satisfies the matrix-valued
initial condition X(0) = In, then we define
etA ≜X(t).
Theorem 5.6
For LCCHS (5.13), that is, ˙x = Ax,
(a) etA is unique.
(b) If Z(t) is any fundamental matrix of that LCCHS, then etA = Z(t)

Z(0)
	−1.
Why?
(a) Uniqueness of etA follows from uniqueness of solutions of LCCHS (5.13), which
follows from Theorem 5.1 in Section 5.1.
(b) Suppose Z(t) is any fundamental matrix of that LCCHS. Denote X(t) =Z(t)

Z(0)
	−1.
By Theorem 5.5, X(t) is also a fundamental matrix of ˙x = Ax, because

Z(0)
	−1
being a constant matrix implies that
˙X(t) =
 
˙Z(t)
! 
Z(0)
!−1
=
 
AZ(t)
! 
Z(0)
!−1
= A
 
Z(t)

Z(0)
	−1!
= AX(t).
In addition, X(0) = Z(0)

Z(0)
	−1 = In. By the definition of etA, it follows that
etA = X(t) = Z(t)

Z(0)
	−1, as desired. 2

Linear Systems of ODEs
371
One of the nice things about the uniqueness of etA is that different people may come
up with radically different∗-looking fundamental matrices Z(t), but they should still
agree on etA.
For the next result, we need another definition from matrix theory: if B =

bij

is an n×n
matrix, then the trace of B is defined by tr(B) ≜b11 + b22 + · · · + bnn, that is, the sum of the
diagonal elements.
Theorem 5.7
(Abel’s theorem) Suppose x(1)(t), . . . , x(n)(t) are n solutions of the same system of linear
homogeneous system of ODEs ˙x = A(t)x. Then
W
 
x(1)(t), . . . , x(n)(t)
!
= exp
⎛
⎝−
t
t0
tr

A(τ)
	
dτ
⎞
⎠
W
 
x(1)(t0), . . . , x(n)(t0)
!
.
(5.25)
Why? This requires work with determinants that is more sophisticated than we want to
present here. A reference will be given at the end of the chapter. 2
Theorem 5.7 is also known as Liouville’s theorem.
Example 5.13
Find etA for A =

 0
1
−6
−5

.
Method: It’s easy to find that the eigenvalues of A are λ1 = −3, λ2 = −2 and that
v(1) =

 1
−3

,
v(2) =

 1
−2

are corresponding eigenvectors. Theorem 5.4 says that
Z(t) ≜

e−3t

 1
−3
 
 e−2t

 1
−2
 
=

 e−3t
e−2t
−3e−3t
−2e−2t

is a fundamental matrix for ˙x = Ax. Then Theorem 5.6(b) says that
etA = Z(t)

Z(0)
	−1 =

e−3t
e−2t
−3e−3t
−2e−2t
 
 1
1
−3
−2
−1
=

e−3t
e−2t
−3e−3t
−2e−2t
 
−2
−1
3
1

=

−2e−3t + 3e−2t
−e−3t + e−2t
6e−3t −6e−2t
3e−3t −2e−2t

. ⃝
Lemma 5.1
(Law of exponents) etA+uA ≜etA euA, for any real numbers t, u.
∗For example, by using different choices of eigenvectors and a different order of listing the eigenvalues.

372
Advanced Engineering Mathematics
Theorem 5.8
(a) e−tA = (etA)−1, and (b) the unique solution of the IVP
˙x = Ax, x(t0) = x0
is
x(t) = e(t−t0)Ax0.
(5.26)
We will apply this theorem in Example 5.14.
5.2.2 Equivalence of Second-Order LCCHODE and LCCHS in R2
Definition 5.6
A 2 × 2 real, constant matrix is in companion form if it has the form

0
1
∗
∗

,
where the ∗’s can be any numbers.
Given that a second-order LCCHODE
¨y + p˙y + qy = 0
(5.27)
has a solution y(t), let us define
x1(t) ≜y(t), and
x2(t) ≜˙y(t).
Physically, if y(t) is the position, then x2(t) is the velocity, v(t). We calculate that
˙x1(t) = ˙y(t) = x2(t)
and
˙x2(t) = ¨y(t) = −qy(t) −p˙y(t) = −qx1(t) −px2(t).
So,
x(t) ≜

x1(t)
x2(t)


Linear Systems of ODEs
373
satisfies the LCCHS
˙x =

 0
1
−q
−p

x,
(5.28)
which we call an LCCHS in companion form in R2.
On the other hand, in Problem 5.2.5.22, you will explain why y(t) ≜x1(t) satisfies
LCCHODE (5.27) if x(t) satisfies LCCHS (5.28). So, we say that LCCHODE (5.27) and
LCCHS (5.28) in companion form in R2 are equivalent in the sense that there is a natural
correspondence between their solutions.
Example 5.14
For the IVP ˙x =

 0
1
−8
−6

x, x(t0) = x0,
(a) Use eigenvalues and eigenvectors to find etA.
(b) Use the equivalent LCCHODE to find etA.
(c) Solve the IVP.
Method:
(a) First, solve
0 = | A −λI | =

−λ
1
−8
−6 −λ
 = −λ(−6 −λ) + 8 = λ2 + 6λ + 8 = (λ + 2)(λ + 4),
so the eigenvalues are λ1 = −4, λ2 = −2. Corresponding eigenvectors are found by

A −(−4)I | 0

=

 4
1
| 0
−8
−2
| 0

∼

4
1
| 0
0
0
| 0

,
after row operation 2R1 + R2 →R2, so corresponding to eigenvalue λ1 = −4, we
have an eigenvector v(1) =

 1
−4

. Similarly,

A −(−2)I | 0

=

 2
1
| 0
−8
−4
| 0

∼

2
1
| 0
0
0
| 0

,
after row operation 4R1 + R2 →R2, so corresponding to eigenvalue λ1 = −2, we
have an eigenvector v(2) =

 1
−2

. Theorem 5.4 says that
Z(t) ≜

e−4t

 1
−4
 
 e−2t

 1
−2

is a fundamental matrix for ˙x = Ax. Then Theorem 5.6(b) says that
etA = Z(t)

Z(0)
	−1 =

 e−4t
e−2t
−4e−4t
−2e−2t
 
 1
1
−4
−2
−1
=

 e−4t
e−2t
−4e−4t
−2e−2t
 1
2

−2
−1
4
1

=
&
−e−4t + 2e−2t
−1
2 e−4t + 1
2 e−2t
4e−4t −4e−2t
2e−4t −e−2t
'
.

374
Advanced Engineering Mathematics
(b) First, write the equivalent scalar second-order ODE, ¨y+6˙y+8y = 0. Its characteristic
polynomial, s2 + 6s + 8 = (s + 4)(s + 2), has roots s1 = −4, s2 = −2. The scalar ODE
has general solution
y(t) = c1e−4t + c2e−2t,
where c1, c2 are arbitrary constants. Correspondingly, the solutions of the original
system are
x(t) =

y(t)
˙y(t)

=

c1e−4t + c2e−2t
−4c1e−4t −2c2e−2t

= c1e−4t

 1
−4

+ c2e−2t

 1
−2

=

 e−4t
e−2t
−4e−4t
−2e−2t
 
c1
c2

,
so
Z(t) ≜

e−4t

 1
−4
 
 e−2t

 1
−2

is a fundamental matrix for the original 2 × 2 system. To find etA, proceed as in
part (a):
etA = Z(t)

Z(0)
	−1 = · · · =

−e−4t + 2e−2t
−1
2 e−4t + 1
2 e−2t
4e−4t −4e−2t
2e−4t −e−2t

.
(c) Note that t0 and x0 were not specified. Using Theorem 5.8(b), the solution of the
IVP is
x(t) = e(t−t0)Ax0 =
⎡
⎣
−e−4(t−t0) + 2e−2(t−t0)
−1
2 e−4(t−t0) + 1
2 e−2(t−t0)
4e−4(t−t0) −4e−2(t−t0)
2e−4(t−t0) −e−2(t−t0)
⎤
⎦x0. ⃝
The eigenvalues of an LCCHS in companion form equal the roots of the characteristic
equation for the equivalent LCCHODE.
It turns out that the Wronskian for a (possibly time-varying) second-order scalar linear
homogeneous ODE and the Wronskian for the corresponding system of ODEs in R2 are
equal! Here’s why: if y(t) satisfies a second-order scalar ODE ¨y + p(t)˙y + q(t)y = 0, define
x(t) ≜

y(t)
˙y(t)

.
Then ¨y = −p(t)˙y −q(t)y implies that x(t) satisfies the system:
˙x(t) =

0
1
−q(t)
−p(t)

x(t).
The Wronskian for two solutions, y1(t), y2(t), for the second-order scalar ODE ¨y + p(t)˙y +
q(t)y = 0 is
W

y1(t), y2(t)
	
=

y1(t)
y2(t)
˙y1(t)
˙y2(t)
 .

Linear Systems of ODEs
375
The Wronskian for two solutions, x(1)(t), x(2)(t), for the linear homogeneous system in R2,
˙x(t) =

0
1
−q(t)
−p(t)

x(t),
is
W
 
x(1)(t), x(2)(t)
!
=
 x(1)(t)  x(2)(t)
 .
But, for the system, solutions x1(t), x2(t) are of the form
x(1)(t) =

y1(t)
˙y1(t)

,
x(2)(t) =

y2(t)
˙y2(t)

,
so
W
 
x(1)(t), x(2)(t)
!
=
 x(1)(t) 
 x(2)(t)
 =


y1(t)
˙y1(t)
 


y2(t)
˙y2(t)

=

y1(t)
y2(t)
˙y1(t)
˙y2(t)
 = W

y1(t), y2(t)
	
,
so the two types of Wronskian are equal. This is another aspect of the relationship between
the solutions of a linear homogeneous second-order scalar ODE and a linear homogeneous
system of two first-order ODEs.
5.2.3 Maclaurin Series for etA
If A is a constant matrix, we can also define etA using the Maclaurin series for eθ by
replacing θ by tA:
etA ≜I + tA + t2
2! A2 + t3
3! A3 + · · · .
From this, it follows that
AetA = etAA.
(5.29)
It’s even possible to use the Maclaurin series to calculate etA, especially if A is diagonaliz-
able and A = PDP−1 where D is a real diagonal matrix:
etA ≜I + tPDP−1 + t2
2! PD
P−1 
P DP−1 + t3
3!PD
P−1 
P D
P−1 
P DP−1 + · · ·
= P
(
I + tD + t2
2! D2 + t3
3! D3 + · · ·
)
P−1 = PetDP−1.

376
Advanced Engineering Mathematics
Also, if D = diag(d11, . . . , dnn), then etD = diag(ed11t, . . . , ednnt). So, in this special case,
etA = P diag(ed11t, . . . , ednnt) P−1.
5.2.4 Nonconstant Coefﬁcients
One might ask. “What if A is not constant? Can we use etA as a fundamental matrix?”
Unfortunately, “No,” although some numerical methods use it as the first step in an
approximation process.
Recall that in Section 3.5, we saw how to solve the Cauchy–Euler ODE r2y′′(r)+pry′(r)+
qy = 0, where p, q are constants and ′ = d
dr: try solutions in the form y(r) = rn.
Example 5.15
For
r2y′′(r) −4ry′(r) + 6y(r) = 0,
(5.30)
(a) Define x1(r) = y(r), x2(r) = y′(r) and convert (5.30) into a system of the form
x′(r) = A(r)x(r).
(5.31)
(b) Find a fundamental matrix for system (5.31).
(c) Explain why erA(r) is not a fundamental matrix for your system (5.31).
Method: We have x′
1(r) = y′(r) = x2(r), so
x′
2(r) = y′′(r) = r−2 
4ry′(r) −6y(r)
	
= −6
r2 y(r) + 4
r y′(r).
So,
x(r) ≜

x1(r)
x2(r)

satisfies the system
x′(r) =

0
1
−6r−2
4r−1

≜A(r)x(r).
(5.32)
(b) In Example 3.30, in Section 3.5, we saw that the solution of the Cauchy–Euler ODE
r2y′′(r) −4ry′(r) + 6y(r) = 0 is y(r) = c1r2 + c2r3; hence,
x(r) =

x1(r)
x2(r)

=

y(r)
y′(r)

=

 c1r2 + c2r3
2c1r + 3c2r2

= c1

r2
2r

+ c2

 r3
3r2

,
where c1, c2 are arbitrary constants. Using Lemma 1.3 in Secdtion 1.7, we rewrite
this as
x(r) =

r2
r3
2r
3r2

c,
where
c =

c1
c2

.

Linear Systems of ODEs
377
So,
Z(r) =

r2
r3
2r
3r2

is a fundamental matrix for (5.32).
(c) If erA(r) were a fundamental matrix for (5.32), Theorem 5.5 would require that
d
dr

erA(r) 
= A(r)erA(r).
The chain rule and then the product rule imply
d
dr

erA(r) 
=erA(r) d
dr [ rA(r) ] =erA(r) 
A(r) + rA′(r)
	
̸= A(r)erA(r)
because
A′(r) =

0
0
12r−3
−4r−2

̸= O.
So erA(r) is not a fundamental matrix for (5.32), a system with nonconstant
coefficients. ⃝
So, in general, we should not bother mentioning etA(t) unless A(t) is actually constant.
But see Problem 5.2.5.30 for a special circumstance where we can use a matrix exponential
to get a fundamental matrix.
5.2.5 Problems
Use exact values wherever possible, that is, do not use decimal approximations of square
roots.
In problems 1–4, find the general solution of the LCCHS.
1. ˙x =

5
4
4
−1

x
2. ˙x =

−3
√
5
√
5
1

x
3. ˙x =
⎡
⎣
2
1
0
0
3
1
0
0
−1
⎤
⎦x
4. ˙x =
⎡
⎣
−6
5
−5
0
−1
2
0
7
4
⎤
⎦x
In problems 5 and 6, find the general solution of the LCCHS. Determine the time constant,
if all solutions have limt→∞x(t) = 0.
5. ˙x =

−3
√
2
√
2
−2

x

378
Advanced Engineering Mathematics
6. A =
⎡
⎣
−3
0
−1
−1
−4
1
−1
0
−3
⎤
⎦
7. ˙x =

 a
0
b
c

x
Suppose a, b, c are unspecified constants, that is, do not use specific values for
them, but do assume that a ̸= c.
8. Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

1
1
4
1

x
x(0) =

 0
−2

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
9. Find a fundamental matrix for
⎧
⎨
⎩
˙x1
=
2x1
+
x2
˙x2
=
3x2
+
x3
˙x3
=
−
x3
⎫
⎬
⎭.
In problems 10–14, find etA.
10. A =

−1
1
0
−2

11. A =

√
3
−
√
3
−2
√
3
−
√
3

12. A =

1
√
5
√
5
−3

13. A =
⎡
⎣
1
−1
0
0
−1
3
−1
1
0
⎤
⎦
14. Suppose that A is a real, 3 × 3, constant matrix,
[A + 2I | 0] =
⎡
⎣
−2
−2
2
| 0
1
1
−1
| 0
0
0
0
| 0
⎤
⎦
and
[A + 3I | 0] =
⎡
⎣
−1
−2
2
| 0
1
2
−1
| 0
0
0
1
| 0
⎤
⎦.

Linear Systems of ODEs
379
Without finding the matrix A, solve the IVP
⎧
⎪⎪⎨
⎪⎪⎩
˙x = Ax
x(0) =
⎡
⎣
0
0
1
⎤
⎦
⎫
⎪⎪⎬
⎪⎪⎭
.
15. Find a fundamental matrix for
˙x =

−a
b
b
−a

x,
where a, b are unspecified positive constants, that is, do not give specific values
for them.
16. Suppose v =
⎡
⎢⎣
v1
...
vn
⎤
⎥⎦is a constant vector and λ is a constant. Explain why d
dt

eλtv

=
λeλtv. [Hint: First multiply through to get
eλtv =
⎡
⎢⎢⎢⎣
v1eλt
v2eλt
...
vneλt
⎤
⎥⎥⎥⎦.]
17. Let A =
⎡
⎣
■
0
0
⋆
0
0
⋆
⋆
■
⎤
⎦.
(a) Replace the two ■’s by different positive integers and the three ⋆’s by different
negative integers. Write down your A.
(b) For the matrix A you wrote in part (a), solve ˙x = Ax.
18. For the matrix A of Example 4.14 in Section 4.2, find etA:
(a) Using the eigenvectors found in Example 4.14 in Section 4.2.
(b) Using eigenvectors
⎡
⎣
1
−2
0
⎤
⎦,
⎡
⎣
1
0
−1
⎤
⎦,
⎡
⎣
1
1
2
1
⎤
⎦.
19. Suppose that in Example 5.13 you had used instead eigenvectors:
v(1) =
⎡
⎣
−1
2
1
⎤
⎦, v(2) =
⎡
⎣
−1
3
1
⎤
⎦.
(a) Find a fundamental matrix using those eigenvectors.
(b) Use your result from part (a) to find etA. Does it equal what we found in
Example 5.13? If it is, why should it be the same?

380
Advanced Engineering Mathematics
20. Find a fundamental matrix for
˙x =

0
1
−4t−2
−t−1

x.
[Hint: The system is equivalent to a Cauchy–Euler ODE for x1(t), after using the
fact that ˙x1(t) = x2(t) follows from the first ODE in the system.]
21. Find a fundamental matrix for
˙x =

0
1
−2t−2
2t−1

x.
[Hint: The system is equivalent to a Cauchy–Euler ODE for x1(t), after using the
fact that ˙x1(t) = x2(t) follows from the first ODE in the system.]
22. If x(t) satisfies LCCHS (5.28), explain why y(t) ≜x1(t) satisfies LCCHODE (5.27).
23. Suppose Z(t) is an n × n-valued differentiable function of t and is invertible for all
t in an open time interval. If ˙Z(t) = A(t)Z(t), explain why Z(t) is a fundamental
matrix of ˙x = A(t)x, that is, the columns of Z(t) are all solutions of the same linear
homogeneous system.
24. Suppose a system of ODEs (⋆) ˙x = A(t)x has two fundamental matrices X(t) and
Y(t). Explain why there is a constant matrix B such that Y(t) = X(t)B. [Hint: Use
initial conditions X(0) and Y(0) to discover what the matrix B should be.]
25. Suppose a system of ODEs (⋆) ˙x = A(t)x has a fundamental matrix X(t) and B is
an invertible constant matrix and define Y(t) = X(t)B. Must Y(t) be a fundamental
matrix for (⋆)? Why, or why not? If the former, explain; if the latter, give a specific
counterexample, that is, a specific choice of A(t), X(t), and B for which X(t) is a
fundamental matrix but X(t)B isn’t.
26. Must eγ tetA be a fundamental matrix for ˙x = (γ I + A)x?
27. Suppose X(t) is a fundamental matrix for a system of ODEs (⋆) ˙x =A(t)x and
X(0) = I. Suppose also that A(−t) ≡−A(t), that is, A(t), is an odd function.
Explain why X(t) is an even function, that is, satisfies X(−t) ≡X(t). [Hint: Define
Y(t) ≜X(−t) and use uniqueness of solutions of linear systems of ODEs.]
28. Suppose X(t) is a fundamental matrix for a system (⋆) ˙x = A(t)x. Explain why
Y(t) ≜

X(t)T	−1 is a fundamental matrix for the system (⋆⋆) ˙x = −A(t)Tx, by
using the steps in the following:
(a) Explain why d
dt[X(t)T] = ( ˙X(t))T.
(b) Use the product rule for matrices to calculate the time derivatives of both sides
of I = X(t)T 
X(t)T	−1.
(c) Explain why Y(t) satisfies ˙Y = −A(t)TY(t). [By the way, system (⋆⋆) is called
the adjoint system for system (⋆).]
29. For A =

−3
1
1
−3

, calculate the improper integral
∞

0
etATetA dt.

Linear Systems of ODEs
381
30. Suppose A(t) is given and we define B(t) ≜
 t
0 A(s)ds. Suppose A(t)B(t) ≜B(t)A(t).
Explain why eB(t) is a fundamental matrix for the system ˙x = A(t)x. [Hint: First,
find ˙B(t) using the chain rule for matrix exponentials d
dt

eB(t)
= eB(t) d
dt

B(t)

.] [By
the way, if A(t) is constant, then B(t) = tA.]
31. Suppose X(t) is a fundamental matrix for a system of ODEs (⋆) ˙x =A(t)x and
X(0) = I. Suppose also that A(t)T ≡−A(t). Explain why X(t)T = (X(t))−1.
[Hint: Define Y(t) = (X(t))−1 and find the ODE that Y(t) satisfies. How? Begin by
noting that I = X(t) (X(t))−1 = X(t) Y(t) and differentiate both sides with respect
to t using the product rule.]
32. Solve the homogeneous system that corresponds to the model of iodine metabolism
found in Example 5.6 in Section 5.1.
33. Find the generalization to (a) R3 and (b) Rn for the concept of companion form
given in Definition 5.6.
By the way, the MATLAB® command roots finds the roots of an n-th degree
polynomial by rewriting it as the characteristic polynomial of an n × n matrix in
companion form, and then MATLAB exploits its excellent methods for finding the
eigenvalues of that matrix.
34. Suppose AT = −A is a real, n × n matrix. Explain why eitA is a Hermitian matrix.
[Hint: The matrix exponential can also be defined by the infinite series eB = I+B+
1
2!B2+ 1
3!B3+· · · .]
35. If A is a real, symmetric n × n matrix, must etA be real and symmetric? If so, why?
If not, give a specific counterexample.
36. Solve the system of Problem 5.1.3.7 and discuss the long-term behavior of the
solutions.
5.3 Complex or Deficient Eigenvalues
5.3.1 Complex Eigenvalues
Recall that for a second-order LCCHODE ¨y + p˙y + qy = 0, if the characteristic polynomial
has a complex conjugate pair of roots s = α ± iν, where α, ν are real and ν > 0, then
{ eαt cos νt,
eαt sin νt } = { Re(e(α+iν)t),
Im(e(α+iν)t) }
gives a complete set of basic solutions for the ODE. Similar to that result is the
following:
Theorem 5.9
Suppose the characteristic polynomial of a real n × n matrix A has a complex conjugate
pair of roots λ = α ± iν, where α, ν are real and ν > 0, and corresponding eigenvectors

382
Advanced Engineering Mathematics
are v, v. Then LCCHS (5.13) in Section 5.2, that is, ˙x = Ax, has a pair of solutions given by
x(1)(t) ≜Re(e(α+iν)tv),
x(2)(t) ≜Im(e(α+iν)t)v).
In addition, if A is 2 × 2, then {x(1)(t), x(2)(t)} is a complete set of basic solutions of the
LCCHS in R2.
As in Section 2.1, for a complex conjugate pair of eigenvalues, we don’t need the
eigenvector v !
Caution: Usually Re(e(α+iν)tv) ̸= Re(e(α+iν)t)Re(v).
Example 5.16
Find etA for the LCCHS ˙x =

−1
2
−2
−1

x.
Method: First, solve 0 = | A−λI | =

−1 −λ
2
−2
−1 −λ
 = (−1−λ)2+4, so the eigenvalues
are λ = −1 ± i2. Corresponding to eigenvalue λ1 = −1 + i2, eigenvectors are found by

A −(−1 + i2)I | 0

=

−i2
2
| 0
−2
−i2
| 0

∼

1⃝
i
| 0
0
0
| 0

,
after row operations i
2 R1 →R1, 2R1 + R2 →R2. Corresponding to eigenvalue λ1 =
−1 + i2, we have an eigenvector v(1) =

−i
1

. This gives two solutions of the LCCHS: the
first is
x(1)(t) = Re

e(−1+i2)t

−i
1

= Re

e−t(cos 2t + i sin 2t)

−i
1

= Re

e−t

sin 2t −i cos 2t
cos 2t + i sin 2t

= e−t

sin 2t
cos 2t

.
For the second, we don’t have to do all of the algebra steps again:
x(2)(t) = Im

e(−1+i2)t

−i
1

= Im

e−t

sin 2t −i cos 2t
cos 2t + i sin 2t

= e−t

−cos 2t
sin 2t

.
So, a fundamental matrix is given by
Z(t) =

x(1)(t)  x(2)(t)

= e−t

 
sin 2t
cos 2t


−cos 2t
sin 2t
 
= e−t

sin 2t
−cos 2t
cos 2t
sin 2t

.
That gives us
etA = Z(t)

Z(0)
	−1 =

e−t

sin 2t
−cos 2t
cos 2t
sin 2t
 (
0
−1
1
0
−1)
= e−t

sin 2t −cos 2t
cos 2t
sin 2t
 
 0 1
−1 0

= e−t

cos 2t
sin 2t
−sin 2t cos 2t

. ⃝

Linear Systems of ODEs
383
Example 5.17
(Short-cut if A is in companion form) Find etA for the LCCHS
˙x =

0
1
−10
−2

x.
Method: First, write the equivalent scalar second-order ODE, ¨y + 2˙y + 10y = 0. Its char-
acteristic polynomial, s2 + 2s + 10 = (s + 1)2 + 9, has roots s = −1 ± i3. The scalar ODE
has general solution
y(t) = c1e−t cos 3t + c2e−t sin 3t,
where c1, c2 are arbitrary constants. Correspondingly, the solutions of the original
system are, after using the product rule,
x(t) =

y(t)
˙y(t)

=

c1e−t cos 3t + c2e−t sin 3t
c1e−t(−cos 3t −3 sin 3t) + c2e−t(−sin 3t + 3 cos 3t)

= c1e−t

cos 3t
−cos 3t −3 sin 3t

+ c2e−t

sin 3t
−sin 3t + 3 cos 3t

= e−t

cos 3t
sin 3t
−cos 3t −3 sin 3t
3 cos 3t −sin 3t
 
c1
c2

,
≜Z(t)

c1
c2

.
This implicitly defines Z(t), a fundamental matrix for the original 2 × 2 system. So,
etA = Z(t)

Z(0)
	−1 =

e−t

cos 3t
sin 3t
−cos 3t −3 sin 3t
3 cos 3t −sin 3t
 (
 1
0
−1
3
−1)
= e−t

cos 3t
sin 3t
−cos 3t −3 sin 3t
3 cos 3t −sin 3t
 1
3

3
0
1
1

= 1
3 e−t

3 cos 3t + sin 3t
sin 3t
−10 sin 3t
3 cos 3t −sin 3t

. ⃝
Example 5.18
Find the general solution of the LCCHS for the circuit in Example 5.2 in Section 5.1,
assuming V(t) ≡0, L = 1, R = 8
3, C1 = 1
8, and C2 = 3
8.
Method: With these parameter values, the LCCHS is ˙x =
⎡
⎣
0
−1
0
8
−3
3
0
1
−1
⎤
⎦x. First, solve
0 = | A −λI | =

−λ
−1
0
8
−3 −λ
3
0
1
−1 −λ

= −λ

−3 −λ
3
1
−1 −λ
 +

8
3
0
−1 −λ

= · · · = −(λ3 + 4λ2 + 8λ + 8).

384
Advanced Engineering Mathematics
Standard advice says to try λ = ±factors of 4
factors of 1, that is, λ = ± 1, ±2, ±4. We are lucky in
this example, as λ =−2 is a root of the characteristic polynomial. We factor to get
0 = | A −λI | = −(λ + 2)(λ2 + 2λ + 4).
The eigenvalues are λ1 =−2 and the complex conjugate pair λ =−1±i
√
3. Corresponding
to eigenvalue λ1, eigenvectors are found by

A −(−2)I | 0

=
⎡
⎣
2
−1
0
| 0
8
−1
3
| 0
0
1
1
| 0
⎤
⎦∼
⎡
⎣
2
0
1
| 0
0
1
1
| 0
0
0
0
| 0
⎤
⎦,
after row operations −4R1 + R2 →R2, 1
3 R2 →R2, −R2 + R3 →R3, R2 + R1 →R1, so
corresponding to eigenvalue λ1 = −2, we have an eigenvector v(1) =
⎡
⎣
−1
−2
2
⎤
⎦.
Corresponding to eigenvalue λ = −1 + i
√
3, eigenvectors are found by

A −

−1 + i
√
3
	
I | 0

=
⎡
⎣
1 −i
√
3
−1
0
| 0
8
−2 −i
√
3
3
| 0
0
1
−i
√
3
| 0
⎤
⎦∼
⎡
⎢⎣
1
0
3−i
√
3
4
| 0
0
1
−i
√
3
| 0
0
0
0
| 0
⎤
⎥⎦,
after row operations R1 ↔R2,
1
8R1 →R1, −(1 −i
√
3)R1 + R2 →R2, R2 ↔R3,
3 + i
√
3
8
R2 + R3 →R3, 2 + i
√
3
8
R2 + R1 →R1. Corresponding to eigenvalue λ1 =
−1 + i
√
3, we have an eigenvector v(1) =
⎡
⎣
−3 + i
√
3
i4
√
3
4
⎤
⎦.
This gives two solutions of the LCCHS: The first is
x(1)(t) = Re
(
e(−1+i
√
3)t
&
−3 + i
√
3 i4
√
3
4
' )
= e−tRe
⎛
⎜⎜⎝cos(
√
3 t) + i sin(
√
3 t)
⎡
⎢⎢⎣
−3 + i
√
3
i4
√
3
4
⎤
⎥⎥⎦
⎞
⎟⎟⎠
= e−t Re
⎛
⎜⎜⎝
⎡
⎢⎢⎣
−3 cos(
√
3 t) −
√
3 sin(
√
3 t) + i
 √
3 cos(
√
3 t) −3 sin(
√
3 t)
!
−4
√
3 sin(
√
3 t) + i4
√
3 cos(
√
3 t)
4 cos(
√
3 t) + i 4 sin(
√
3 t)
⎤
⎥⎥⎦
⎞
⎟⎟⎠
= e−t
⎡
⎢⎢⎣
−3 cos(
√
3 t) −
√
3 sin(
√
3 t)
−4
√
3 sin(
√
3 t)
4 cos(
√
3 t)
⎤
⎥⎥⎦.

Linear Systems of ODEs
385
For the second, we don’t have to do all of the algebra steps again:
x(2)(t) = Im
⎛
⎜⎜⎝e(−1+i
√
3)t
⎡
⎢⎢⎣
−3 + i
√
3
i4
√
3
4
⎤
⎥⎥⎦
⎞
⎟⎟⎠= e−t
⎡
⎢⎢⎣
√
3 cos(
√
3 t) −3 sin(
√
3 t)
4
√
3 cos(
√
3 t)
4 sin(
√
3 t)
⎤
⎥⎥⎦.
The general solution of the circuit is
⎡
⎣
I1(t)
v1(t)
v2(t)
⎤
⎦
= c1e−2t
⎡
⎣
−1
−2
2
⎤
⎦+ c2e−t
⎡
⎢⎢⎣
−3 cos(
√
3 t)−
√
3 sin(
√
3 t)
−4
√
3 sin(
√
3 t)
4 cos(
√
3 t)
⎤
⎥⎥⎦
+ c3e−t
⎡
⎢⎢⎣
√
3 cos(
√
3 t)−3 sin(
√
3 t)
4
√
3 cos(
√
3 t)
4 sin(
√
3 t)
⎤
⎥⎥⎦,
where c1, c2, c3 are arbitrary constants. Finally, (5.7) in Section 5.1 yields I2(t) = 1
R(v1(t)−
v2(t)). ⃝
5.3.2 Solving Homogeneous Systems of Second-Order Equations
We saw in Example 5.5 in Section 5.1 that a physical system of three horizontal springs
and two masses can be modeled by a system of second-order scalar ODEs.
Just as for a single second-order scalar ODE, we can rewrite a system of m second-order
scalar ODEs as a system of 2m first-order scalar ODEs. As we will see, for certain systems
of second-order scalar ODEs, it is usually simpler to not rewrite them as first-order ODEs.
To solve a system of two second-order ODEs of the special form
¨x = Ax,
where A is a real matrix, it helps to try solutions in the form
x(t) = eσtv.
When we substitute that into the system, we get
σ 2eσtv = ¨x = Ax = eσtAv,
that is,
Ax = σ 2v.
So, we want v to be an eigenvector of A corresponding to eigenvalue λ ≜σ 2. Note that
σ is not necessarily an eigenvalue of A. In the following, we will assume that v is a real
eigenvector of A.

386
Advanced Engineering Mathematics
If A has an eigenvalue λ < 0, then setting σ 2 = λ < 0 would give σ = ±i√−λ ≜±iν,
and thus, the original system would have two solutions:
x(1)(t) = Re
 
eiνtv
!
= cos(νt)v,
x(2)(t) = Im
 
eiνtv
!
= sin(νt)v.
For example, if x is in R2 and A has two distinct negative eigenvalues λ1, λ2 and corre-
sponding eigenvectors v1, v2, denote ν1 = √−λ1, ν2 = √−λ2. Then the system ¨x = Ax has
general solution
x =

c1 cos ν1t + d1 sin ν1t
	
v1 +

c2 cos ν2t + d2 sin ν2t
	
v2,
(5.33)
where c1, c2, d1, d2 are arbitrary constants.
Example 5.19
Find the general solution of the system of second-order ODEs (5.12) in Section 5.1 for the
parameter values m1 = 1, m2 = 2, k1 = 5, k2 = 6, k3 = 8.
Method:
With these parameter values, (5.12) in Section 5.1 is ¨x = Ax, where A =

−11
6
3
−7

. First, find the eigenvalues λ = σ 2 of A by solving
0 = | A −λI | =

−11 −λ
6
3
−7 −λ
 = (−11 −λ)(−7 −λ) −18 = λ2 + 18λ + 59 :
λ = −18 ±
,
182 −4(1)(59)
2
= −18 ±
√
88
2
= −9 ±
√
22 .
Denote λ1 = −9 −
√
22 = σ 2
1 , λ2 = −9 +
√
22 = σ 2
2 . The two frequencies of vibration are
ν1 =
,
−λ1 =
-
9 +
√
22,
ν2 =
,
−λ2 =
-
9 −
√
22 .
Next, find v(1), v(2), eigenvectors of A corresponding to the eigenvalues λ1, λ2 of A,
respectively:

A −(−9 −
√
22)I | 0

=

−2 +
√
22
6
| 0
3
2 +
√
22
| 0

∼

3
2 +
√
22
| 0
0
0
| 0

,
after row operations R1 ↔R2, −
 
−2+
√
22
3
!
R1 + R2 →R2. Corresponding to eigenvalue
λ1 = −9 −
√
22, we have an eigenvector v(1) =

−2 −
√
22
3

.
Second,

A −(−9 +
√
22)I | 0

=

−2 −
√
22
6
| 0
3
2 −
√
22
| 0

∼

3
2 −
√
22
| 0
0
0
| 0

,
after row operations R1 ↔R2, −
 
−2−
√
22
3
!
R1 + R2 →R2. Corresponding to eigenvalue
λ1 = −9 +
√
22, we have an eigenvector v(2) =

−2 +
√
22
3

.

Linear Systems of ODEs
387
Using formula (5.33), we have that the solutions of the two mass and three horizontal
spring systems are given by

x1(t)
x2(t)

=

c1 cos
-
9 +
√
22 t

+ d1 sin
-
9 +
√
22 t
 
−2 −
√
22
3

+

c2 cos
-
9 −
√
22 t

+ d2 sin
-
9 −
√
22 t
 
−2 +
√
22
3

where c1, c2, d1, d2 are arbitrary constants. ⃝
The ratio of the two frequencies of vibration is
,
9 +
√
22
,
9 −
√
22
=
,
9 +
√
22
,
9 −
√
22
,
9 −
√
22
,
9 −
√
22
=
-
(9 +
√
22)(9 −
√
22)
9 −
√
22
=
-
92 −(
√
22)2
9 −
√
22
=
√
59
9 −
√
22
=
√
59
(9 −
√
22)
(9 +
√
22)
(9 +
√
22)
= (9 +
√
22)
√
59
,
hence is not a rational number, so the motion of the positions of the two masses is quasi-
periodic and not periodic, except in the special case when the initial conditions are satisfied
by either c1 = d1 = 0 or c2 = d2 = 0.
From an engineering point of view, the quasiperiodic case is more likely to happen than
the periodic case because it is unusual for the ratio of two randomly chosen real numbers
to be a rational number.
5.3.3 Deﬁcient Eigenvalues
Recall from Example 2.16 in Section 2.2 that
A =

 29
18
−50
−31

has only one distinct eigenvalue, λ = −1, and it is deficient because its algebraic multiplicity
is two, but its geometric multiplicity is one, that is, there is only one linearly independent
eigenvector.
As for complex eigenvalues, it helps to first consider an easier example of a system in
companion form.
Example 5.20
Find the general solution of the LCCHS ˙x =

 0
1
−9
−6

x.
Method: First, write the equivalent scalar second-order ODE, ¨y + 6˙y + 9y = 0 and solve
its characteristic equation, 0 = s2 + 6s + 9 = (s + 3)2: s = −3, −3. The scalar ODE has
general solution
y(t) = c1e−3t + c2te−3t,

388
Advanced Engineering Mathematics
where c1, c2 are arbitrary constants. Correspondingly, the solutions of the original
system are
x(t) =

y(t)
˙y(t)

=
&
c1e−3t + c2te−3t
c1(−3e−3t) + c2(−3t + 1)e−3t
'
.
The general solution of the system of ODEs is
x(t) = c1e−3t

 1
−3

+ c2e−3t

t
−3t + 1

,
where c1, c2 are arbitrary constants, because the Wronskian is

e−3t
te−3t
−3e−3t
(−3t + 1)e−3t
 = e−6t ̸= 0. ⃝
If we study the conclusion of this example, we see that one solution is
x(1)(t) = e−3t

 1
−3

and the second solution is
x(2)(t) = e−3t

t

 1
−3

+

0
1

.
(5.34)
We note that

 1
−3

is an eigenvector of the matrix A =

 0
1
−9
−6

.
Example 5.21
Find the general solution of the LCCHS ˙x =

 29
18
−50
−31

x and the corresponding etA.
Method: Denote A =

 29
18
−50
−31

. In Example 2.16 in Section 2.2, we found that λ = −1
is the only eigenvalue of A, with corresponding eigenvector
v(1) =

−0.6
1

;
hence, Av(1) = (−1)v(1). So,
x(1)(t) = e−t

−0.6
1

gives one solution of the system. Similar to (5.34) in the previous example, let’s try a
second solution of the form
x(2)(t) = e−t  
tv(1) + w
!
.
Substitute it into the LCCHS: by the product rule, we need
−e−t  
tv(1) + w
!
+ e−t  
v(1) + 0
!
= ˙x(2)(t) = Ax(2)(t) = e−tA
 
tv(1) + w
!
.

Linear Systems of ODEs
389
After multiplying through by et, this becomes
−
−tv(1) + v(1) −w = 
tAv(1) + Aw,
where we canceled terms because Av(1) = (−1)v(1). So we need
(A −(−1)I) w = v(1).
(5.35)
Such a vector w is called a generalized eigenvector of A corresponding to the eigenvalue
λ = −1. We can solve for w using row reduction of an augmented matrix:

A −(−1)I | v(1) 
=

 30
18
|
−0.6
−50
−30
|
1

∼

1⃝
0.6
|
−0.02
0
0
|
0

after 5
3R1 + R2 →R2, 1
30R1 →R1. The solutions are
w =

−0.02
0

+ c

−0.6
1

,
where c is an arbitrary constant. For convenience, we can take c = 0, as we shall see later,
so our second solution of the LCCHS is
x(2)(t) = e−t  
tv(1) + w
!
= e−t

t

−0.6
1

+

−0.02
0

.
We check that this gives us a complete set of basic solutions by calculating the
Wronskian:
 x(1)(t)  x(2)(t)
 =

−0.6e−t
e−t(−0.6t −0.02)
e−t
te−t
 = 0.02e−2t ̸= 0.
The general solution is
x(t) = c1e−t

−0.6
1

+ c2e−t

t

−0.6
1

+

−0.02
0

,
where c1, c2 are arbitrary constants.
To find etA, first rewrite the general solution as
x(t) = c1e−t

−0.6
1

+ c2e−t

−0.6t −0.02
t

= e−t

−0.6
−0.6t −0.02
1
t
 
c1
c2

,
so
X(t) ≜e−t

−0.6
−0.6t −0.02
1
t

is a fundamental matrix. We calculate that
etA = X(t) (X(0))−1 = e−t

−0.6
−0.6t −0.02
1
t
 
−0.6
−0.02
1
0
−1
= e−t

−0.6
−0.6t −0.02
1
t
 
0
1
−50
−30

= e−t

30t + 1
18t
−50t
−30t + 1

. ⃝

390
Advanced Engineering Mathematics
The reason we could take c = 0 in finding x(2)(t) is because it succeeded in finding a com-
plete set of basic solutions. The reasons why we would want to take c = 0 are because usually
“simpler is better” and also because if we had kept the c in x(2)(t), then it would include
the redundant term cx(1)(t).
5.3.4 Laplace Transforms and etA
If A is a constant matrix, the unique solution of the IVP ˙x = Ax, x(0) = x0 is
x(t) = etAx0.
On the other hand, if we take the Laplace transform of the LCCHS ˙x −Ax = 0, we get
sL[ x(t) ] −x0 −Ax = 0.
So, etAx0 = L−1[ (sI −A)−1 ] x0. It follows that
Theorem 5.10
L−1[ (sI −A)−1 ] = etA.
Example 5.22
Find the general solution of the LCCHS of Example 5.21.
Method: We have
etA = L−1 [(sI −A)−1 ] = L−1
&
sI −

 29
18
−50
−31
−1'
= L−1
&
s −29
−18
50
s + 31
−1'
= L−1

1
(s −29)(s + 31) + 900

s + 31
18
−50
s −29
 
= L−1
⎡
⎢⎢⎢⎣
⎡
⎢⎢⎢⎣
s + 31
(s2 + 2s + 1)
18
(s2 + 2s + 1)
−
50
(s2 + 2s + 1)
s −29
(s2 + 2s + 1)
⎤
⎥⎥⎥⎦
⎤
⎥⎥⎥⎦= L−1
⎡
⎢⎢⎢⎣
⎡
⎢⎢⎢⎣
(s + 1) + 30
(s + 1)2
18
(s + 1)2
−
50
(s + 1)2
(s + 1) −30
(s + 1)2
⎤
⎥⎥⎥⎦
⎤
⎥⎥⎥⎦
= L−1
⎡
⎢⎢⎢⎣
⎡
⎢⎢⎢⎣
1
(s + 1) +
30
(s + 1)2
18
(s + 1)2
−
50
(s + 1)2
1
(s + 1) −
30
(s + 1)2
⎤
⎥⎥⎥⎦
⎤
⎥⎥⎥⎦
=
⎡
⎣
e−t + 30te−t
18te−t
−50te−t
e−t −30te−t
⎤
⎦.

Linear Systems of ODEs
391
The general solution of the LCCHS is
x(t) =
⎡
⎣
e−t + 30te−t
18te−t
−50te−t
e−t −30te−t
⎤
⎦x0.
This agrees with the second conclusion of Example 5.21. ⃝
5.3.5 Stability
Definition 5.7
LCCHS (5.13) in Section 5.2, that is, ˙x = Ax, is
(a) Asymptotically stable if all its solutions have limt→∞x(t) = 0
(b) Neutrally stable if it is not asymptotically stable, but all its solutions are bounded
on [0, ∞), that is, for each component xj(t) of x(t) = [x1(t)
x2(t)
· · ·
xn(t)]T,
there exists Mj such that for all t ≥0 we have |xj(t)| ≤Mj
(c) Unstable if it is neither asymptotically stable nor neutrally stable, that is, there is
at least one solution x(t) that is not bounded on [0, ∞)
We have,
Theorem 5.11
LCCHS (5.13) in Section 5.2, that is, ˙x = Ax, is
(a) Asymptotically stable if all of A’s eigenvalues λ satisfy Re(λ) < 0
(b) Neutrally stable if all of A’s eigenvalues λ satisfy Re(λ) ≤0 and no deficient
eigenvalue λ has real part equal to zero
(c) Unstable if A has an eigenvalue whose real part is positive or if it has a deficient
eigenvalue whose real part is 0
Why? (a) Suppose λ is a real, negative eigenvalue of A with corresponding eigenvector v.
Then x(t) = eλtv will be a solution of the LCCHS and will have limt→∞x(t) = 0. If λ = α±iν
is a nonreal eigenvalue of A with negative real part α and corresponding eigenvector v,
then solutions
eαtRe
 
eiνtv
!
,
eαtIm
 
eiνtv
!
will have limit 0 as t →∞because α = Re(λ) < 0. The explanation for (b) is similar by
again using the form of solutions in the two cases λ real versus non-real. The explanation
for (c) is similar, although requires some care in the deficient eigenvalue case. 2

392
Advanced Engineering Mathematics
For an ODE, the time constant indicates how long it takes for a solution to decay to 1/e of
its initial value. Suppose an LCCHS is asymptotically stable. The time constant τ for that
system of ODEs can be defined by
τ =
1
rmin
,
where rmin is the slowest decay rate. Because each solution x(t) may include many different
decaying exponential functions, “weighted” by constant vectors, we can’t guarantee that
x(τ) =
1
e x(0). Nevertheless, for physical intuition, it is still useful to think of the time
constant as being about how long it takes for the solution to decay in a standard way.
5.3.6 Problems
In your final conclusions, the symbol “i” should not appear. If you are asked for a
fundamental matrix, do explain why it is invertible.
In problems 1–6, find the general solution of the LCCHS.
1. ˙x =

−2
−5
1
0

x
2. ˙x =

 0
2
−3
−2

x
3. ˙x =

 3
2
−4
−1

x
4. ˙x =

0
1
−9
4
−1

x
5. ˙x =
⎡
⎣
5
0
−10
0
−2
0
4
0
−7
⎤
⎦x
6. ˙x =

−12
−25
4
8

x
In problems 7–13, find a fundamental matrix for the LCCHS.
7. ˙x =

1
−5
1
−3

x
8. ˙x =

−3
2
−5
3

x
9. The system of Problem 5.3.6.6
10. A =

√
3
√
3
−2
√
3
−
√
3

11. ˙x =

 0
1
−1
−2

x
12. ˙x =

−4
−1
9
2

x

Linear Systems of ODEs
393
13. ˙x =

 a
b
−b
a

x, where a, b are unspecified constants, that is, do not give specific
values for them. Do assume that b ̸= 0.
14. ˙x =

−a
b
0
−a

x, where a, b are unspecified constants, that is, do not give specific
values for them. Do assume that b ̸= 0.
15. Find a fundamental matrix and etA for ˙x =

−2
−3
2
−4

x.
In problems 16–18, find etA.
16. A =

1
−4
2
−3

17. A =
⎡
⎣
3
0
−2
0
−1
0
4
0
3
⎤
⎦
18. A =
⎡
⎣
1
0
0
3
1
−2
2
2
1
⎤
⎦
In problems 19 and 20, find etA (a) using eigenvalues and eigenvectors, and (b) using
Laplace transforms.
19. A =

 10
11
−11
−12

20. A =
⎡
⎣
3
3
−1
0
−1
0
4
4
−1
⎤
⎦
21. You may assume that the matrix
A =
⎡
⎣
−1
−1
0
2
−1
1
0
1
−1
⎤
⎦
has eigenvalues −1, −1 ± i. Find etA.
22. For the system
˙x =

−1
4
−1
1

x,
(a) Find a fundamental matrix.
(b) Find the solution that passes through the point (x1(0), x2(0)) = (2, −3).

394
Advanced Engineering Mathematics
23. Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

−1
2
−2
−1

x
x(0) =

π
2

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
24. Solve the IVP
⎧
⎨
⎩
˙x1
=
−2x1
+
x2
˙x2
=
−2x1
−
4x3

x1(0) = 1, x2(0) = −2
⎫
⎬
⎭.
25. Solve the IVP
⎧
⎨
⎩
˙y
=
v
˙v
=
−5y −2v

y(0) = 1, v(0) = 0
⎫
⎬
⎭.
26. Find the exact frequencies of vibration for
¨x =
⎡
⎣
−4
0
0
0
−1
1
0
2
−3
⎤
⎦x.
27. Find the general solution of
¨x =

−3
1
2
−2

x.
28. For the matrix A =
⎡
⎣
0
2
−1
−3
−5
2
−2
−2
0
⎤
⎦,
(a) Find an eigenvector and a generalized eigenvector corresponding to eigen-
value λ = −2.
(b) Use your results for part (a) to help find etA.
In problems 29–35, determine if the system ˙x = Ax is asymptotically stable, neutrally
stable, or unstable. Try to do as little work as is necessary to give a fully explained
conclusion.
29. A =

−1
1
0
−2

30. A =

√
2
√
2
−3
√
2
−
√
2

31. The LCCHS of Problem 5.3.6.6

Linear Systems of ODEs
395
32. A =
⎡
⎣
−1
0
1
0
−1
−2
0
0
0
⎤
⎦
33. A =
⎡
⎣
−3
0
−1
−1
−4
1
−1
0
−3
⎤
⎦
34. A =
⎡
⎣
−5
2
4
2
−8
2
4
2
−5
⎤
⎦
35. Assume A is a constant, real, 5 × 5, matrix and has eigenvalues i, −i, i, −i, −1,
including repetitions. Consider the LCCHS (⋆) ˙x = Ax. For each of (a) through (e),
decide whether it must be true, must be false, or may be true and may be false:
(a) The system is asymptotically stable.
(b) The system is neutrally stable.
(c) The system may be neutrally stable, depending upon more information
concerning A.
(d) (⋆) has solutions that are periodic with period 2π.
(e) (⋆) has solutions of the form tp(t) + q(t) where p(t) is periodic with period 2π.
36. If the matrix A has an eigenvalue λ with Re(λ) = 0 that is deficient, explain why
LCCHS ˙x = Ax is not neutrally stable.
37. (Small project) Analogous to the Cauchy–Euler ODE, consider systems of the form
˙x = t−1Ax, where A is a real, constant matrix. Create a method to find all solutions
of such systems, using solutions of the form x(t) = trv where v is constant. Do
consider at least these three cases of roots: real, complex, and real but deficient.
5.4 Nonhomogeneous Linear Systems
Here we will explain how to solve
˙x = A(t)x(t) + f(t)
(5.36)
using a fundamental matrix X(t) for the corresponding homogeneous linear system ˙x =
A(t)x. Recall that X(t) satisfies
˙X(t) = A(t)X(t),
(5.37)
that is, each column of X(t) is a solution of ˙x = A(t)x. In the special case of a linear constant
coefficients system ˙x = Ax + f(t), we will especially use the fundamental matrix etA.
It turns out that the method developed in the following is a generalization of the method
of variation of parameters that we used in Section 4.3.
We try a solution of (5.36) in the form
x(t) ≜X(t)v(t).
(5.38)

396
Advanced Engineering Mathematics
Using a generalization of the product rule, we have
d
dt [X(t)v(t)] = ˙X(t)v(t) + X(t)˙v(t).
So for x(t) = X(t)v(t), (5.37) implies that
˙x(t) = ˙X(t)v(t) + X(t)˙v(t) = A(t)X(t)v(t) + X(t)˙v(t).
So, for x(t) to solve the original, nonhomogeneous system (5.36), we need
((((((
A(t)X(t)v(t) + X(t)˙v(t) = ˙x(t) = A(t)x(t) + f(t) = ((((((
A(t)X(t)v(t) + f(t),
that is,
X(t)˙v(t) = f(t).
But, one requirement of a fundamental matrix is that it should be invertible at all t, or at
least all t in an interval of existence. The earlier equation is equivalent to
˙v(t) = (X(t))−1 f(t).
(5.39)
Using indefinite integration we have
v(t) =

(X(t))−1 f(t) dt,
or, using definite integration we have
v(t) =
t
t0
(X(t))−1 f(τ) dτ,
where t0 is a constant; either gives a formula for a particular solution, xp(t) = X(t)v(t), for
the original, nonhomogeneous system (5.36).
To get the general solution of (5.36), we add in xh(t) = X(t)c, where c is a vector of
arbitrary constants. The general solution of (5.36) can be written in either of the forms
x(t) = X(t)
 
c +

(X(t))−1 f(t) dt
!
(5.40)
or, using definite integration,
x(t) = X(t)
⎛
⎝c +
t
t0
(X(τ))−1 f(τ) dτ
⎞
⎠,
(5.41)
where t0 is a constant.

Linear Systems of ODEs
397
In the special but often occurring case when A is a constant matrix, we can use X(t) = etA:
Recalling that

esA	−1 = e−sA, we can rewrite these two formulas as
x(t) = etA  
c +

e−tAf(t) dt
!
(5.42)
and, using the law of exponents etAe−τA = e(t+(−τ))A,
x(t) = etAc +
t
t0
e(t−τ)Af(τ) dτ,
(5.43)
respectively. Any one of (5.40) through (5.43) is called a variation of parameters, or
variation of constants, formula for the solutions of the nonhomogeneous system (5.36).
Evaluate (5.43) at t = t0 to get
x(t0) = et0Ac +
t0
t0
e(t0−τ)Af(τ)dτ = et0Ac;
hence,
c =
 
et0A!−1
x(t0) = e−t0Ax(t0).
The solution of an IVP can be written in the form
x(t) = etAe−t0Ax(t0) +
t
t0
e(t−τ)Af(τ) dτ = e(t−t0)Ax(t0) +
t
t0
e(t−τ)Af(τ) dτ.
(5.44)
Example 5.23
Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x1
=
2x1
+
x2
˙x2
=
7x1
−
4x2
−e−t

x1(0)
=
3
x2(0)
=
−2

.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(5.45)
Method: First, we find etA using the eigenvalues and eigenvectors of A =

2
1
7
−4

: The
characteristic polynomial,
P(λ) ≜| A −λI | =

2 −λ
1
7
−4 −λ
 = λ2 + 2λ −15 = (λ + 5)(λ −3)
has roots λ1 = −5, λ2 = 3. We find corresponding eigenvectors:

A −λ1I | 0

=

7
1
| 0
7
1
| 0

yields
v(1) =

 1
−7


398
Advanced Engineering Mathematics
and

A −λ2I | 0

=

−1
1
| 0
7
−7
| 0

yields
v(2)=

1
1

.
So,
Z(t) =

e−5t
e3t
−7e−5t
e3t

.
We use the fundamental matrix to calculate
etA = Z(t)

Z(0)
	−1 =

e−5t
e3t
−7e−5t
e3t
 
 1
1
−7
1
−1
=

e−5t
e3t
−7e−5t
e3t
 1
8

1
−1
7
1

= 1
8

 e−5t + 7e3t
−e−5t + e3t
−7e−5t + 7e3t
7e−5t + e3t

.
Applying (5.44) with t0 = 0, the solution of the IVP is
x(t) = etAx(0) +
t
0
e(t−τ)Af(τ)dτ = 1
8

e−5t + 7e3t
−e−5t + e3t
−7e−5t + 7e3t
7e−5t + e3t
 
 3
−2

+
+
t
0
1
8
⎡
⎣
e−5(t−τ) + 7e3(t−τ)
−e−5(t−τ) + e3(t−τ)
−7e−5(t−τ) + 7e3(t−τ)
7e−5(t−τ) + e3(t−τ)
⎤
⎦

0
−e−τ

dτ
= 1
8

 5e−5t + 19e3t
−35e−5t + 19e3t

+
t
0
1
8
&
e−5te4τ −e3te−4τ
−7e−5te4τ −e3te−4τ
'
dτ.
When integrating with respect to τ, functions of t are treated as if they were constants.
The solution of the IVP is
x(t) = 1
8
⎡
⎣
5e−5t + 19e3t
−35e−5t + 19e3t
⎤
⎦+ 1
8
⎡
⎢⎢⎢⎣
e−5t 
1
4e4τt
0 −e3t 
−1
4e−4τt
0
−7e−5t 
1
4e4τt
0 −e3t 
−1
4e−4τt
0
⎤
⎥⎥⎥⎦
= 1
8
⎡
⎣
5e−5t + 19e3t
−35e−5t + 19e3t
⎤
⎦+ 1
32
⎡
⎣
e−5t 
e4t −1
	
+ e3t 
e−4t −1
	
−7e−5t 
e4t −1
	
+ e3t 
e−4t −1
	
⎤
⎦
= 1
8
&
5e−5t + 19e3t
−35e−5t + 19e3t
'
+ 1
32
&
e−t + e−t
−7e−t + e−t
'
−1
32
&
e−5t + e3t
−7e−5t + e3t
'
.
The solution of the IVP is
x(t) = 1
32
&
19e−5t + 75e3t
−133e−5t + 75e3t
'
+ 1
16
&
e−t
−3e−t
'
. ⃝

Linear Systems of ODEs
399
Example 5.24
Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =
˙x1
=
29x1
+
18x2
+ t
˙x2
=
−50x1
−
31x2

x1(0)
=
3
x2(0)
=
−2

.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(5.46)
Method:
In Example 5.21 in Section 5.3, we found a complete set of basic solutions
{x(1)(t), x(2)(t)}. Using them, a fundamental matrix for the corresponding linear LCCHS
is given by
Z(t) =

x(1)(t)  x(2)(t)

=

−0.6e−t
e−t(−0.6t −0.02)
e−t
te−t

.
So, we calculate
etA = Z(t)

Z(0)
	−1 =

−0.6e−t
e−t(−0.6t −0.02)
e−t
te−t
 
−0.6
−0.02
1
0
−1
=

−0.6e−t
e−t(−0.6t −0.02)
e−t
te−t
 
0
1
−50
−30

=

(30t + 1)e−t
18te−t
−50te−t
(1 −30t)e−t

.
Applying (5.44) with t0 = 0, the solution of the IVP is
x(t) =

(30t + 1)e−t
18te−t
−50te−t
(1 −30t)e−t
 
 3
−2

+
+
t
0

(30(t −τ) + 1)e−(t−τ)
18te−(t−τ)
−50(t −τ)e−(t−τ)
(1 −30(t −τ)) e−(t−τ)
 
τ
0

dτ
=

 (54t + 3)e−t
−(90t + 2)e−t

+ e−t
t
0


(30t + 1)τ −30τ 2	
eτ

−50tτ + 50τ 2	
eτ

dτ.
We calculate on the side that
t
0


(30t + 1)τ −30τ 2	
eτ

−50tτ + 50τ 2	
eτ

=
⎡
⎢⎢⎢⎢⎢⎣
(30t + 1)
t
0
τeτdτ −30
t
0
τ 2eτdτ
−50t
t
0
τeτdτ + 50
t
0
τ 2eτdτ
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎣
(30t + 1) [(τ −1)eτ]t
0 −30

(τ 2 −2τ + 2)eτt
0
−50t [(τ −1)eτ]t
0 + 50

(τ 2 −2τ + 2)eτt
0
⎤
⎥⎦
=
⎡
⎣
(30t + 1)

(t −1)et + 1
	
−30

(t2 −2t + 2)et −2
	
−50t

(t −1)et + 1
	
+ 50

(t2 −2t + 2)et −2
	
⎤
⎦.

400
Advanced Engineering Mathematics
Returning to the full expression for the solution, we have
x(t) =

 (54t + 3)e−t
−(90t + 2)e−t

+ e−t

(31t −61)et + 30t + 61
(−50t + 100)et −50t −100

,
that is,
x(t) =

 31t −61
−50t + 100

+ e−t

84t + 64
−140t −102

. ⃝
Example 5.25
Suppose that the system ˙x =

−t−1
t−1
−2 −3t−1
1 + 3t−1

x has solutions
x(1)(t) =

 t + 1
2t + 1

,
x(2)(t) =

et
(t + 1)et

.
Solve ODE system
˙x =

−t−1
t−1
−2 −3t−1
1 + 3t−1

x +

1
2

(5.47)
on all open intervals not containing t = 0.
Method: We were given two solutions of the corresponding linear homogeneous system,
so we hope that
Z(t) =

x(1)(t)  x(2)(t)

=

 t + 1
et
2t + 1
(t + 1)et

is a fundamental matrix. To affirm this, all we need to do is check its invertibility, a
calculation that will be useful when we need the inverse later:
|Z(t)| =

t + 1
et
2t + 1
(t + 1)et
 = (t + 1)(t + 1)et −(2t + 1)et = t2et,
which is never zero on any open interval not containing t = 0.
In this problem, the matrix A(t) is not constant, so etA(t) is not a fundamental matrix.
We try to use (5.40) to find the solution of the nonhomogeneous system (5.47):
x(t) = Z(t)
 
c +

(Z(t))−1 f(t)dt
!
=

 t + 1
et
2t + 1
(t + 1)et
 
c +

1
t2et

 (t + 1)et
−et
−(2t + 1)
t + 1
 
1
2

dt

=

 t + 1
et
2t + 1
(t + 1)et
 
c +

1
t2et

(t −1)et
1

dt

=

 t + 1
et
2t + 1
(t + 1)et

⎛
⎜⎜⎝c +
⎡
⎢⎢⎣
 (t −1)
t2
dt

t−2e−tdt
⎤
⎥⎥⎦
⎞
⎟⎟⎠.
Unfortunately, the second integral has no “closed form” solution, unless one uses the
Maclaurin (infinite) series for e−t. Instead, we can use (5.41), a definite integral version of

Linear Systems of ODEs
401
the variation of parameters formula: For any t0 ̸= 0, the solution of the nonhomogeneous
system (5.47) is, after using the earlier work,
x(t) = Z(t)
⎛
⎝c +
t
t0
(Z(τ))−1 f(τ)dτ
⎞
⎠
= · · · =

 t + 1
et
2t + 1
(t + 1)et

⎛
⎜⎜⎜⎜⎜⎜⎜⎝
c +
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
t
t0
(τ −1)
τ 2
dτ
t
t0
τ −2e−τdτ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
=

 t + 1
et
2t + 1
(t + 1)et

⎛
⎜⎜⎜⎜⎝
c +
⎡
⎢⎢⎢⎢⎣
ln
 t
t0
 + 1
t −1
t0
t
t0
τ −2e−τdτ
⎤
⎥⎥⎥⎥⎦
⎞
⎟⎟⎟⎟⎠
,
where c is a vector of arbitrary constants. ⃝
5.4.1 Problems
In problems 1–3, a fundamental matrix is given for ˙x = Ax for some real, 2 × 2, constant
matrix A. Without finding A, solve the given nonhomogeneous ODE system:
1. X(t) =

e−2t
e−3t
−2e−2t
−3e−3t

, ˙x = Ax +

1
0

2. X(t) =

cos 3t
sin 3t
−3 sin 3t
3 cos 3t

, ˙x = Ax +

cos t
0

3. X(t) = e−3t

cos t −sin t
cos t + sin t
2 cos t
2 sin t

, ˙x = Ax +

 0
e−3t

4. Suppose X(t) =

2t + t2
3t2 + t3
t2
t3

is a fundamental matrix for ˙x = A(t)x.
Without finding A(t), find all solutions of
˙x = A(t)x +

 0
t3e−t

.
5. Solve
˙x =
⎡
⎣
3
0
−2
0
−1
0
4
0
3
⎤
⎦x +
⎡
⎣
0
e−t
7
⎤
⎦.

402
Advanced Engineering Mathematics
6. Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

−1
−1
1
1

x +

t
0

x(0) =

0
1

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
7. Solve the two-compartment model
⎧
⎨
⎩
˙A1
=
5
−
A1
10
˙A2
=
A1
10
−
A2
6
⎫
⎬
⎭.
8. (a) Find a fundamental matrix for
˙x =

0
1
−3t−2
3t−1

x.
[Hint: The system is equivalent to a Cauchy–Euler ODE for x1(t), after using
the fact that ˙x1(t) = x2(t) follows from the first ODE in the system.]
(b) Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

0
1
−3t−2
3t−1

x +

0
1

x(1) =

−5
1

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
9. (a) Find a fundamental matrix for
˙x =

0
1
−8t−2
5t−1

x.
[Hint: The system is equivalent to a Cauchy–Euler ODE for x1(t), after using
the fact that ˙x1(t) = x2(t) follows from the first ODE in the system.]
(b) Solve the IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

0
1
−8t−2
5t−1

x +

3
0

x(1) =

 0
−1

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
10. Solve
˙x =

 2
1
−3
6

x +

e−5t
4e−t

.

Linear Systems of ODEs
403
11. Explain why there is a particular solution of ODE ¨y + y
=
f(t) given by
y(t) =
 t
0 sin(t −u)f(u)du by using variation of parameters formula (5.43) for the
equivalent system in R2. [Hint: Use a trigonometric identity for the sin(difference
of angles).] [Note: The final conclusion in this problem agrees with the result of
Example 4.33 in Section 4.5.]
12. Explain why there is a particular solution of ODE ¨y + ω2y = f(t) given by y(t) =
1
ω
 t
0 sin(ω(t −τ))f(τ)dτ by using variation of parameters formula (5.43) for the
equivalent system in R2.
13. Explain why (4.39) in Section 4.3, a formula for all solutions of a second-order
ODE ¨y + p(t)˙y + q(t)y = f(t), follows from (5.41).
14. If A is a constant matrix, α is a positive constant, and w is a constant vector, use
Laplace transforms to solve the IVP system

˙x
= Ax + δ(t −α)w
x(0)
= x0

in terms of A, etA, α, w, and x0. [Recall that L[ δ(t −α)] = e−αs.]
15. Use Laplace transforms to solve the IVP system
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x
=

2
−5
1
−2

x +

−cos 2t
sin 3t

x(0)
=

1
0

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
5.5 Nonresonant Nonhomogeneous Systems
Here we use the method of undetermined coefficients to find a particular solution of a non-
homogeneous system of ODEs. If the nonhomogeneous term(s) is simple and the matrix
of coefficients is constant, this can be a quicker and easier method than a variation of
parameters formula requiring integration.
Example 5.26
Solve
˙x =

−1
3
1
1

x +

3e−t
2e−t

.
(5.48)
Method: Because the nonhomogeneous term is of the form
f(t) = e−tw
for a constant vector w, let’s try a particular solution in the form
xp(t) = e−ta

404
Advanced Engineering Mathematics
where a is a constant vector to be determined. We substitute xp(t) into the nonhomoge-
neous system ˙x = Ax + f(t) to get
−e−ta = ˙xp = Axp + f(t) = A
 
e−ta
!
+ e−tw = e−t (Aa + w) ,
that is,
−a = Aa + w,
that is,
−w = (A −(−1)I) a.
The solution is
a = −(A −(−1)I)−1 w,
as long as (A −(−1)I) is invertible.
In this specific example, we have w =

3
2

, and A =

−1
3
1
1

is implicitly given
earlier. So, we have
a = −(A −(−1))−1 w = −

−1
3
1
1

−(−1)I
−1 
3
2

= −

0
3
1
2
−1 
3
2

= 1
3

 2
−3
−1
0
 
3
2

=

 0
−1

.
The general solution of the original, nonhomogeneous problem is
x(t) = xh(t) + xp(t) = Z(t)c + e−t

 0
−1

,
where Z(t) is a fundamental matrix for the corresponding LCCHS and is c is a vector
of arbitrary constants. Unfortunately, our nice method for finding a particular solution
does not help us find Z(t), except we do know that (−1) is not an eigenvalue of A because
we were able to calculate (A −(−1)I)−1!
As usual, we construct Z(t) using eigenvalues and eigenvectors of A:
0 = | A −λI | =

−1 −λ
3
1
1 −λ
 = λ2 −4,
so the eigenvalues are λ = ±2. For λ1 = −2,

A −(−2)I | 0

=

1
3
| 0
1
3
| 0

yields
v(1) =

−3
1

.
For λ2 = 2,

A −2I | 0

=

−3
3
| 0
1
−1
| 0

yields
v(2) =

1
1

.

Linear Systems of ODEs
405
So,
Z(t) =

−3e−2t
e2t
e−2t
e2t

is a fundamental matrix for the corresponding LCCHS. The general solution of problem
(5.48) is
x(t) =

−3e−2t
e2t
e−2t
e2t

c + e−t

 0
−1

,
where c is a vector of arbitrary constants. ⃝
In general, consider a problem of the form
˙x = Ax + eαtw.
We try to find a particular solution of the form
xp(t) = eαta.
Theorem 5.12
Suppose that α is not an eigenvalue of the constant matrix A and w is a constant
vector. Then
xp(t) = −eαt (A −αI)−1 w
(5.49)
is a particular solution of the nonhomogeneous system:
˙x = Ax + eαtw.
(5.50)
Why? As you will explain in Problem 5.5.2.15, by calculations similar to those in
Example 5.27,
xp(t) = eαta
(5.51)
will guarantee that (5.49) is a particular solution of (5.50). 2
We say that “α is not an eigenvalue of A” is a nonresonance assumption. We will explore
this idea further in Example 5.27.
Theorem 5.12 says in the special case of α = 0 that if 0 is not an eigenvalue of A, hence A
is invertible, and A and w are constant, then xp(t) = −A−1w is a particular solution of the
system:
˙x = Ax + eαtw.

406
Advanced Engineering Mathematics
Example 5.27
Here is a model for obsolescence of computer hardware at a company. Assume that their
hardware is sorted into three categories:
(I) The latest models
(II) Not the latest models but very useful
(III) Worth keeping in use but far from the most useful
In general, the categories correspond to the age of the equipment, so we expect that
the rates at which pieces of equipment will fail depend on their categories.
Assume also that with the passage of time, some equipment in category I will move
into category II, some equipment in category II will move into category III, and some
equipment in category III will be disposed of because it becomes obsolete.
Assume that equipment that fails or becomes obsolete will be replaced immediately
by new equipment in category I. (This is probably the most unrealistic assumption in the
model because it may take some time for equipment to be replaced.)
Let xj(t), j = 1, 2, 3 be the fractions of the company’s computer hardware in the three
categories I, II, III, respectively. Note that 0 ≤xj(t) ≤1 for all time.
Corresponding to failure of equipment will be “death” rates δ2 and δ3. Because equip-
ment in category I that fails is immediately replaced by equipment in category I, we
don’t need to know that failure rate.
The earlier assumptions lead to the system of ODEs
⎧
⎨
⎩
˙x1
=
−a11x1
+δ2x2
+(a33 + δ3)x3
˙x2
=
a11x1
−(a22 + δ2)x2
˙x3
=
a22x22
−(a33 + δ3)x3
⎫
⎬
⎭,
where a11, a22, a33, δ2, δ3 > 0. Note that d
dt[x1 + x1 + x1] ≡0, so x1(t) + x2(t) + x3(t) will
be constant in time. Indeed, all of the equipment is in one of the three categories, so
x1(t) + x2(t) + x3(t) ≡1.
Assume the company is just starting up and they estimate the rates as a11 = 0.3, a22 =
0.2, a33 = 0.5, δ2 = 0.1, δ3 = 0.2, assuming time is measured in years. Solve the system
and describe the behavior of the amounts of equipment in the three categories.
Method: Because x1(t) + x2(t) + x3(t) ≡1, we can reduce the size of the system by
substituting x1 = 1 −x2 −x3 into the second and third ODEs of the system. This gives a
system in R2:

˙x2
˙x3

=

−(a11 + a22 + δ2)
−a11
a22
−(a33 + δ3)
 
x2
x3

+

a11
0

.
(5.52)
This system has constant coefficients and a constant forcing function.
The method of undetermined coefficients suggests we try a particular solution of the
form

x2,p
x3,p

=

w2
w3

,
where w = [w2
w3]T is a constant vector. Denoting A =

−(a11 + a22 + δ2)
−a11
a22
−(a33 + δ3)

,
and substituting x = [x2,p
x3,p]T = w into the system (5.52), we get
0 = ˙w = Aw +

a11
0

,

Linear Systems of ODEs
407
whose solution is
w = −A−1

a11
0

=
−1
(a11 + a22 + δ2)(a33 + δ3) + a11a22

−(a33 + δ3)
a11
−a22
−(a11 + a22 + δ2)
 
a11
0

=
−1
(a11 + a22 + δ2)(a33 + δ3) + a11a22

−a11(a33 + δ3)
−a11a22

=

7/16
1/8

after substituting in the specific parameter values given in the narrative of the problem.
The general solution of the nonhomogeneous system of ODEs is

x2
x3

= etAc + w,
(5.53)
where c is a vector of arbitrary constants and w =

7/16
1/8

.
We will suppose that initially there is only hardware in category I, because the com-
pany is just starting up. (This may not be an appropriate assumption in a struggling
economy.) We solve

0
0

=

x2(0)
x3(0)

= c + w
for c to get c = −w. So, the solution of this model is

x2(t)
x3(t)

= −(etA −I)

7/16
1/8

(5.54)
and x1(t) = 1 −x2(t) −x3(t). ⃝
We used MathematicaTM to find approximate eigenvalues −0.65 ± i0.239792 and approx-
imate eigenvector v = [1 (0.166667−i0.799305)]T for the 2×2 matrix A. After using this to
find two real solutions, we found a fundamental matrix and then etA, the details of which
are routine, so we will omit. This gives explicit solution of the model
x2(t) ≈7
16 + e−0.65t

−7(0.834058)
16
cos(0.239792t)
−7(0.208514)
16
sin(0.239792t) + 1
8 sin(0.239792t)

and
x3(t) ≈1
8 + e−0.65t

−7(0.834058)
16
sin(0.239792t)
−1
8 cos(0.239792t) + 0.208514
8
sin(0.239792t)

.
We graphed x1(t) as a dotted curve, x2(t) as a dashed curve, and x3(t) as a solid
curve in Figure 5.7. Notice that even though the solution has oscillatory factors with

408
Advanced Engineering Mathematics
0.8
0.6
0.4
2
4
6
8
10
t
x1(t)
x2(t)
x3(t)
0.2
0.0
FIGURE 5.7
Computer hardware obsolescence model.
period
2π
0.239792, they oscillate so slowly that the relatively rapid decay of e−0.65t makes
the solution appear not to have oscillatory factors.
The model suggests that within about four years the amounts of computer hardware in
the categories I and II will predominate.
5.5.1 Sinusoidal Forcing
Here we consider systems whose nonhomogeneous term(s) is a sinusoidal function times
a constant vector.
Theorem 5.13
Suppose that ±iω is not an eigenvalue of the real, constant matrix A, w is a constant vector,
and g(t) is either cos ωt or sin ωt, where the constant ω is nonzero. Then
˙x = Ax + g(t)w
(5.55)
has a particular solution of the form
xp(t) = (cos ωt)a1 + (sin ωt)a2,
(5.56)
where a1, a2 are constant vectors to be determined.
The next example will illustrate why this theorem is true.
Example 5.28
Find a particular solution of
˙x =

−1
3
1
1

x +

cos 2t
0

.
(5.57)

Linear Systems of ODEs
409
Method: Rather than solve a system of the form
˙x = Ax + (cos 2t)w,
where
A =

−1
3
1
1

, w =

1
0

,
we will solve its complexification:
˙x = Ax + ei2tw.
(5.58)
The relationship between.xp(t), the solution of (5.58), and xp(t), the solution of (5.57), is
xp(t) = Re

.xp(t)
	
,
because cos 2t = Re

ei2t	
.
We try a solution of (5.58) in the form
.xp(t) = ei2t.a,
(5.59)
where.a is constant vector, possibly complex. We substitute (5.59) into (5.58):
i2ei2t.a = ˙.xp(t) = A.xp(t) + ei2tw = A
 
ei2t.a
!
+ ei2tw = ei2t (A.a + w) ,
that is,
−w = (A −i2I).a.
Here we see where the nonresonance condition comes in: if ±i2 is not an eigenvalue of
A, then (A −i2I) is invertible and we can solve for.a. Here, that is,
.a = −(A −i2I)−1 w = −

−1
3
1
1

−i2I
−1 
1
0

= −

−1 −i2
3
1
1 −i2
−1 
1
0

= 1
8

1 −i2
−3
−1
−1 −i2
 
1
0

= 1
8

1 −i2
−1

,
so
.xp(t) = ei2t.a = 1
8 (cos 2t + i sin 2t)

1 −i2
−1

= 1
8

cos 2t + 2 sin 2t −i(2 cos 2t −sin 2t)
−cos 2t −i sin 2t

.
A particular solution is given by
xp(t) = Re

.xp(t)
	
= 1
8

cos 2t + 2 sin 2t
−cos 2t

. ⃝
One of the nice things about the complexification method is that it easily deals with
sinusoidal functions that have phase other than zero.
Example 5.29
Find a particular solution of
˙x =

−1
3
1
1

x +
&
sin
 
2t −π
4
!
0
'
.
(5.60)

410
Advanced Engineering Mathematics
Method: Because sin(2t −π
4 ) = Im

ei
 
2t−π
4
!
, we try
xp(t) = Im

.xp(t)
	
,
where.xp(t) should satisfy
˙x = Ax + e
i

2t−π
4

w.
(5.61)
The A and w are the same as in Example 5.28. The problem that .xp(t) should satisfy is
almost the same as (5.58). So, we try
.xp(t) = ei
 
2t−π
4
!
.a.
(5.62)
When we substitute (5.62) into (5.61), we get the same equation for.a as in Example 5.28:
−w = (A −i2I).a.
So, without repeating all of the steps of Example 5.28, we have
xp(t) = Im

.xp(t)
	
= · · · = Im
1
8
 
cos
 
2t −π
4
!
+ i sin
 
2t −π
4
!! 
1 −i2
−1

= · · · = 1
8Im
⎡
⎢⎣
cos

2t −π
4
	
+ 2 sin
 
2t −π
4
!
−i
 
2 cos
 
2t −π
4
!
−sin
 
2t −π
4
!!
−cos
 
2t −π
4
!
+ i sin
 
2t −π
4
!
⎤
⎥⎦,
that is,
xp(t) = 1
8
⎡
⎢⎣
−2 cos
 
2t −π
4
!
+ sin
 
2t −π
4
!
sin
 
2t −π
4
!
⎤
⎥⎦.
(5.63)
Using trigonometric identities, we have
cos
 
2t −π
4
!
=
1
√
2
(cos 2t + sin 2t)
and
sin
 
2t −π
4
!
=
1
√
2
(sin 2t −cos 2t),
so (5.63), the desired particular solution, simplifies to be
xp(t) = −
1
8
√
2
&
3 cos 2t + sin 2t
cos 2t −sin 2t
'
. ⃝
At this point, we can better understand why a condition such as
α is not an eigenvalue of the constant matrix A
or
±iω is not an eigenvalue of the constant matrix A
is referred to as a “nonresonance” condition. For example, in the conclusion of Theorem
5.13, we have a particular solution
xp(t) = (cos ωt)a1 + (sin ωt)a2,

Linear Systems of ODEs
411
where a1, a2 are constant vectors. If iω were an eigenvalue of the constant matrix A, then
the algebraic system of equations
−w = (A −iωI).a
may or may not have a solution for.a. In the latter case, we should try instead a particular
solution of the form
.xp(t) = eiωt (t.v + .u) ,
which fits our understanding of the word “resonance” that we used in Section 4.2.
5.5.2 Problems
1. Redo Example 5.23 in Section 5.4 using the methods of Section 5.5.
2. Redo Example 5.24 in Section 5.4 using the methods of Section 5.5.
3. Solve ˙x =

−4
3
−2
1

x +

e−3t
0

.
4. Solve ˙x =
⎡
⎣
3
0
−2
0
1
0
4
0
−3
⎤
⎦x +
⎡
⎣
−5
0
e−2t
⎤
⎦.
5. Solve ˙x =

−1
2
−2
−1

x +

cos t
0

.
6. Solve ˙x =

 0
1
−1
−2

x +

 0
cos t

(a) By converting the system into a linear nonhomogeneous second-order scalar
ODE
(b) By a nonresonance method from Section 5.5
(c) By a method of variation of parameters from Section 5.4
7. Solve
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

0
3
1
−2

x −

e−t
0

x(0) =

4
5

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
8. Solve
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
˙x =

 2
1
−3
−2

x +

 −1
cos t

x(0) =

−1
1

⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
9. Solve ˙x =

−4
3
−2
1

x +

e−t
0

.
10. Solve ˙x =
⎡
⎣
−6
5
−5
0
−1
2
0
7
4
⎤
⎦x +
⎡
⎣
0
0
e−3t
⎤
⎦.

412
Advanced Engineering Mathematics
11. Suppose a constant matrix A has an eigenvalue λ and corresponding eigenvector
v. Find a particular solution of ˙x = Ax + eλtv by assuming a solution of the form
xp(t) = eλt(tv + w), analogous to one of the more complicated cases of the method
of undetermined coefficients for scalar ODEs.
12. Suppose that the system
 3I1 + 2˙I1 −2˙I2 = 2
3I2 + 2˙I2 −2˙I1 = −3

models an electrical circuit. Find the general solution of the system of ODEs. [Hint:
First, write the system in the form B˙x = Ax + f.]
13. (Small project) Develop a method of undetermined coefficients, analogous to
those for scalar ODEs and scalar difference equations, for systems of ODEs, that
includes resonant cases.
14. Solve the system that models iodine metabolism found in Example 5.6 in
Section 5.1. For the value of the input of iodine from the digestive system, find a
value for the minimum daily requirement for iodine according to U.S. government
nutritional guidelines. Do cite the source of your value.
15. Explain why Theorem 5.12 is true: suppose that α is not an eigenvalue of the
constant matrix A and w is a constant vector. Substitute into (5.50) a solution in
the form (5.51), that is, xp(t) = eαta. Find a, in order to see that (5.49), that is,
xp(t) = −eαt (A −αI)−1 w, solves (5.50).
5.6 Linear Control Theory: Complete Controllability
There are many types of control problems. Some examples are to drive a process from a
given initial condition to a desired end condition, do so in the shortest time, do so with
least cost of fuel or other expense, model or “realize” the dynamics of a “black box” pro-
cess, and stabilize or decouple a process using feedback control. In this section, we will
study the first problem.
In this section, we will work only with real numbers, vectors, and matrices.
Suppose x(t) in Rn satisfies a single input control system of ODEs, that is,
˙x = Ax + u(t)b,
(5.64)
where A and b are constants. The vector-valued function x(t) is called the state of the
system, and the control process takes place in state space Rn. The scalar-valued function
u(t) is called the control or control function. The system is called “single input” because
there is only one scalar control function.
Given the initial condition x(0) = x0 and the desired end condition x(te) = xe, can we
choose or construct a scalar control function u(t) so that x(t) solves the boundary value
problem consisting of

˙x = Ax + u(t)b
x(0)= x0

(5.65)

Linear Systems of ODEs
413
and
x(te) = xe,
(5.66)
for some time te > 0? If so, we will say that we can drive x(t) from x0 to xe. We can think of
xe as a destination or target that we want to reach. The control that accomplishes the driv-
ing may depend on x0, xe, and te, and there may be more than one scalar control function
that can accomplish the driving.
More generally, we can ask whether the system has the property defined in the
following:
Definition 5.8
System (5.64) is completely controllable if for every x0 and xe there is at least one scalar
control function u(t) and time te for which the boundary value problem (5.65) through
(5.66) has a solution.
Example 5.30
Study whether we can drive x(t) from x0 to 0.
Method: Using variation of parameters formula (5.43) in Section 5.4, the solution of IVP
(5.65) is
x(t) = etA
⎛
⎝x0 +
t
0
e−τAu(τ)b dτ
⎞
⎠.
(5.67)
Substituting t = te and x(te) into (5.67) yields
0 = eteA
⎛
⎝x0 +
te
0
e−tAu(t)b dt
⎞
⎠.
(5.68)
Multiplying on the left by e−teA, solving for x0, and changing the variable of definite
integration from τ to t result in
x0 = −
te
0
e−tAu(t)b dt.
(5.69)
Can we find a final time te and a scalar control function u(t) to satisfy (5.69)? We will see
in Theorem 5.14 that “The answer is yes only if x0 is in V(A, b),” where
V(A, b) ≜Span{ b, Ab, A2b, . . . , An−1b }. ⃝
(5.70)
Note that, as defined in Section 1.7, the span of a set of vectors is a vector subspace.

414
Advanced Engineering Mathematics
Example 5.31
Is there a scalar control function u(t) that can drive x(t) from
⎡
⎣
0
−1
3
⎤
⎦to 0, if x(t) solves
˙x =
⎡
⎣
0
0
0
0
0
1
0
0
0
⎤
⎦x + u(t)
⎡
⎣
0
0
2
⎤
⎦?
Method:
V(A, b) = Span
⎧
⎪⎨
⎪⎩
⎡
⎣
0
0
2
⎤
⎦,
⎡
⎣
0
0
0
0
0
1
0
0
0
⎤
⎦
⎡
⎣
0
0
2
⎤
⎦,
⎡
⎣
0
0
0
0
0
1
0
0
0
⎤
⎦
2 ⎡
⎣
0
0
2
⎤
⎦
⎫
⎪⎬
⎪⎭
= Span
⎧
⎨
⎩
⎡
⎣
0
0
2
⎤
⎦,
⎡
⎣
0
2
0
⎤
⎦,
⎡
⎣
0
0
0
⎤
⎦
⎫
⎬
⎭.
Because
⎡
⎣
0
−1
3
⎤
⎦= 3
2
⎡
⎣
0
0
2
⎤
⎦+
 
−1
2
!
⎡
⎣
0
2
0
⎤
⎦
is in
V(A, b),
yes, there is a scalar control function that drives x(t) from
⎡
⎣
0
−1
3
⎤
⎦to 0. ⃝
We don’t want to be technical about what kind of function the scalar control can be, but
we do need −
 te
0 e−tAu(t)b dt to make sense. For simplicity, let’s assume that
on every finite time interval, u(t) is continuous except at
possibly finitely many values of t where it has finite jumps.
that is, that u(t) is piecewise continuous.
Theorem 5.14
x(t) can be driven from x0 to 0 only if x0 is in V(A, b).
In order to explain why this is true, we will need a few other results. To avoid getting
bogged down in somewhat technical details, we will note those results and explain them
later, that is, we will first keep our eye on explaining why Theorem 5.14 is true.
Our explanation process will seem like patiently peeling layers from an onion, eventu-
ally getting to basic results that explain earlier results. We will use the ▷symbol to indicate
the end of one explanation that is followed by another Lemma or explanation that was
just used.

Linear Systems of ODEs
415
Lemma 5.2
For all te,
 te
0 u(t)e−tA b dt is in V(A, b).
If we can explain why Lemma 5.2 is true, then we will have established Theorem 5.14
because of (5.69). ▷
In order to explain why Lemma 5.2 is true, we will use
Lemma 5.3
For all t, e−tA b is in V(A, b).
Assume for the moment that Lemma 5.3 is true. We will use it to explain why Lemma
5.2 is true: for any fixed te,
 te
0 u(t)e−tA b dt is the limit of Riemann sums
N
/
j=1
u(tj)e−tjA b t.
These sums are linear combinations of terms e−tjAb, each of which is in V(A, b) by Lemma
5.3. The result of Lemma 5.2 will be true as long as we know that a limit of vectors in
V(A, b) will be in V(A, b), and this will follow from Lemma 5.4. ▷
Lemma 5.4
V(A, b) has the property that whenever {SN} is a sequence in V(A, b) and there exists S ≜
limN→∞SN, then it follows that S is in V(A, b).
We will not explain why Lemma 5.4 is true as it involves the subject of analysis some-
times called “real variables.” But we will mention that Lemma 5.4 says that V(A, b) is a
closed vector subspace.
The concept of “closed” is familiar to us from calculus where we talk about [ a, b ] being
a “closed” interval. Another example of a “closed” interval is (−∞, b]: if a sequence of real
numbers {sN} has −∞< sN ≤b for all N and there exists ¯s ≜limN→∞sN, then ¯s will also
be in (−∞, b]. Note that we say a limit exists only if the limit is a finite number.
So, we have reduced the explanations to Lemma 5.4, which we will not explain, and
Lemma 5.3, which we will discuss now: to explain it, we will use two other results.
Theorem 5.15
(Cayley–Hamilton theorem) If A is n×n and has characteristic polynomial P(λ) ≜| A−λI |,
then P(A) = O, a zero matrix.

416
Advanced Engineering Mathematics
To be explicit, the Cayley–Hamilton theorem says that if we use the eigenvalues of A to
factor its characteristic polynomial, that is,
P(λ) ≜| A −λI | = (λ1 −λ)(λ2 −λ) · · · (λn −λ),
then
O = P(A) ≜(λ1I −A)(λ2I −A) · · · (λnI −A).
For example, in Example 5.23 in Section 5.4, we saw that A =

2
1
7
−4

has characteristic
polynomial P(λ) = λ2 + 2λ −15, so the Cayley–Hamilton theorem says that this matrix A
satisfies the matrix equation
O = A2 + 2A −15I.
We will not explain the Cayley–Hamilton theorem in the most general case, but we
can explain it in the special case that A is diagonalizable, that is, there is an invertible
matrix P such that A = PDP−1 for a diagonal matrix D = diag(λ1, λ2, . . . , λn), whose diag-
onal entries are the eigenvalues of A, possibly including repetitions. In this case, we
calculate that
P(A) = (λ1I −A)(λ2I −A) · · · (λnI −A)
=
 
Pλ1IP−1 −PDP−1!  
Pλ2IP−1 −PDP−1!
· · ·
 
PλnIP−1 −PDP−1!
=
 
P(λ1I −D)
P−1!  
P(λ1I −D)
P−1!
· · ·
 
P(λnI −D)P−1!
= Pdiag(0, λ1 −λ2, . . . , λ1 −λn)diag(λ2 −λ1, 0, . . . , λ2 −λn)
· · · diag(λn −λ1, . . . , λn −λn−1, 0)P−1
= Pdiag(0, 0, . . . , 0)P−1 = O. ▷
Corollary 5.1
If A is n × n, then for every integer k ≥0, we can rewrite Ak as a linear combination of
I, A, . . . , An−1.
Why? The Cayley–Hamilton theorem enables us to express An in terms of I, A, . . . , An−1,
and then we can use that to find An+1 = AAn first in terms of A, . . . , An and then in terms
of I, A, . . . , An−1. One can proceed likewise for An+2, An+3, etc.

Linear Systems of ODEs
417
For example, if A2 + 2A −15I = O and A is 2 × 2, then we have
A0 ≜I,
A1 = A,
A2 = −2A + 15I,
A3 = AA2 = A(−2A + 15I) = −2A2 + 15A = −2(−2A + 15I) + 15A = 19A −30I, etc. ▷
This completes the explanation of Theorem 5.14, that is, explains why we can drive x(t)
from x0 to 0 only if x0 is in V(A, b). 2
More generally, we have
Theorem 5.16
System (5.64) is completely controllable if, and only if,
 b  Ab  A2b 
· · ·
 An−1b
 ̸= 0.
(5.71)
Theorem 1.43 in Section 1.7 implies that the controllability condition (5.71) is equivalent to
having V(A, b) = Rn and also equivalent to the n × n matrix

b  Ab 
· · ·
 An−1b

having rank equal to n, by Theorem 1.30 in Section 1.6.
We can explain part of this powerful result by giving a formula for control functions!
Suppose that the controllability condition (5.71) is true. We will construct a scalar control
that drives x(t) from a given x0 to a given xe:
u(t) ≜bTe−tATa,
(5.72)
where a is a constant vector to be chosen later.
First, we can rewrite
u(t) b = b u(t)
if we interpret u(t) on the left-hand side as a scalar-valued function of t and interpret u(t)
on the right-hand side as a 1 × 1 matrix-valued function of t. Indeed, toward the end of
this section, we will consider control systems ˙x = A x+B u(t) with “multivariable control,”
where u(t) will be a vector of control functions and B will be a constant matrix.
Substitute (5.72) into (5.67), the variation of parameters formula applied to our problem,
to get
x(t) = etA
⎛
⎝x0 +
t
0
e−τA b u(τ)dτ
⎞
⎠= etA
⎛
⎝x0 +
t
0
e−τA b bTe−τATa dτ
⎞
⎠.
(5.73)
Define, for any fixed te, the matrix
M ≜
te
0
e−τA b bTe−τATa dτ.
(5.74)

418
Advanced Engineering Mathematics
With this definition, (5.65) and (5.66) can be restated as
xe = eteA(x0 + Ma),
that is,
Ma = e−teAxe −x0.
(5.75)
Lemma 5.5
If the system satisfies the controllability condition (5.71) and te > 0, then M is invertible.
We’ll explain why Lemma 5.5 is true later, but for now, because of the lemma, we can
solve (5.75):
a = M−1  
e−teAxe −x0
!
and thus that we can find a control that drives x(t) from x0 to xe. 2
What may seem a little surprising is that the controllability condition implies x(t) can
be driven from x0 to xe in time te, no matter how small te is! At first, this seems to
be unrealistic. But, intuitively, if te is chosen to be very small, then the matrix M =
 te
0 e−tA b bTe−tATa dt will be very small because it is an integral of a continuous (matrix-
valued) function on a very small time interval. Thus, M−1 will be very “large,” and so
a = M−1 
e−teAxe −x0
	
will be very large. Thus, in this case, the control function given by
(5.72), that is, u(t) = bTe−tATa, would be very large.
So, if you want to drive x(t) from x0 to xe in an unrealistically small amount of time, “all”
that you need is an unrealistically large control function requiring an unrealistically large
amount of resources to produce! In real life, there would be physical and/or economic
limitations on the amount of control we can exert, so we cannot drive x(t) from x0 to xe in
an arbitrarily small amount of time.
Now let’s explain why Lemma 5.5 is true. Suppose that
 b  Ab

· · ·
 An−1b
 ̸= 0
but that M is not invertible. Eventually we will reach a contradiction. There exists an a for
which Ma = 0; hence,
0 = aT0 = aTMa =
te
0
aTe−tA b bTe−tATa dt;
hence,
0 =
te
0
 
bTe−tATa
!T  
bTe−tATa
!
dt =
te
0
w(t)w(t) dt =
te
0
| w(t) |2 dt,
(5.76)

Linear Systems of ODEs
419
where we define the scalar-valued function
w(t) ≜bTe−tATa.
Because |w(t)|2 ≥0 for t in the interval [0, te], (5.76) implies that w(t) = 0 for all t in [0, te],
that is,
0 ≡w(t) = aTe−tAb, 0 ≤t ≤te.
(5.77)
If we take the first through (n −1)-st derivatives of (5.77), we get
0
=
aTe−tA(−A)b
...
0
=
aTe−tA(−A)n−1b.
After that, substitute t = 0 into those equations and into (5.77) to get
0
=
aTb
0
=
aT(−A)b
...
0
=
aT(−A)n−1b.
By Theorem 1.9 in Section 1.2, we have
aT 
b  Ab 
· · ·
 An−1b

=

aTb  aTAb 
· · ·
 aTAn−1b

=

0  0 · · ·
 0

= 0T.
(5.78)
Take the transpose of both sides of (5.78) to get

b  Ab 
· · ·
 An−1b
T
a = 0.
(5.79)
But we assumed the controllability condition, that is, that the determinant
 b  Ab  A2b 
· · ·
 An−1b
 ̸= 0;
hence, (5.79) implies that a = 0, contradicting the assumption that a ̸= 0. So, by
contradiction, we have the desired result that M is invertible. 2
5.6.1 Some Other Control Problems
In establishing the controllability criterion, we used the specific form for control functions
(5.72), that is,
u(t) = bTe−tATa.

420
Advanced Engineering Mathematics
There might be other control functions that also drive x(t) from x0 to xe in a finite amount
of time. The subject of optimal control is concerned with choosing which control does the
driving in such a way that it optimizes a design criterion, such as taking the least amount
of time to arrive at xe or such as minimizing the cost of the driving (e.g., in a specified
amount of time).
System (5.64) has a single control function. One generalization is to have several control
functions, for example, in a multi variable control system
˙x = Ax + Bu,
(5.80)
where B is a constant n × m matrix and u(t) is an Rm-valued function of t. It turns out that
the controllability criterion for (5.80) is a nice generalization of that for the single input
control system (5.64).
Theorem 5.17
System (5.80) is completely controllable, that is, for every xe, x0, there is at least one control
function that drives x(t) from x0 to xe in a finite amount of time, if and only if
n = rank
 
B  AB

· · ·
 An−1B
!
.
The rank of the n × nm matrix (

B  AB 
· · ·
 An−1B

can be at most n, so we can
say that the controllability criterion is that that matrix should have “full rank.”
It turns out that we can use a control function that is a nice generalization of (5.72):
u(t) ≜BTe−tATa,
where a is a constant vector. Note that BTe−tAT is an m × n matrix for all t.
Another basic problem in control theory is that of “observing” the initial state x0 of a
system. The simplest context where we can discuss this is the single input control system
(5.64). Assume that we can measure only some linear combination of the state components,
that is, one can measure
y(t) ≜c1x1(t) + · · · + cnxn(t) = cTx(t),
where c is a constant vector in Rn. Of course, if we could measure each of the state
components x1(t), . . . , xn(t) at all times, then we could just read off x(0) = x0.
Altogether, our system is
⎧
⎨
⎩
˙x
=
Ax + Bu
y(t)
=
cTx(t)
x(0)
=
x0
⎫
⎬
⎭,
(5.81)
where constant matrix A and constant vectors b, c are assumed to be known.∗
∗Yet another type of problem in control theory is to find a way to estimate the model parameters A, b, for
example, by using a “Luenberger observer.”

Linear Systems of ODEs
421
For this problem, there is a precise definition of a concept concerning observing a system:
Definition 5.9
System (5.81) is completely observable if for every x0 there is a finite time te, possibly
dependent on x0, such that if we know both u(t) and y(t) for all t in the interval [0, te], then
we can calculate what x0 was.
In a way, observability is like predicting the past. They say that “hindsight has 20-20
vision,” but even predicting the past can be difficult to do.
It turns out that there is a complete observability criterion akin to the complete
controllability criterion (5.71): the n × n determinant

cT
cTA
...
cTAn−1

should be nonzero. Also, it turns out that this generalizes nicely to systems with multiple
observers, that is, where y(t) ≜CTx(t) can be measured.
Learn More About It
We have barely scratched the surface of the subject of control theory. Here are some
useful references: (1) “Some fundamental control theory I: Controllability, observabil-
ity, and duality,” William J. Terrell, Am. Math. Mon. 106 (1999), 705–719, and (2) “Some
fundamental control theory II: feedback linearization of single input nonlinear sys-
tems,” William J. Terrell, The American Mathematical Monthly 106 (1999), 812–828.
The first article relates controllability to systems in companion form and shows how
controllability and observability are “dual” concepts. Both articles also have useful bib-
liographical references.
Here are two papers that made foundational contributions to the subject of control
theory: (3) “Contributions to the theory of optimal control,” R. E. Kalman, Bol. Soc. Mat.
Mex., Series II 5 (1960), 102–119, and (4) “Observers for multivariable systems,” D. G.
Luenberger, IEEE Trans. Auto. Control AC-11 (1966), 190–197.
5.6.2 Problems
In problems 1 and 2, determine whether the system can be driven from

6
3

to 0.
1. ˙x =

 1
−2
−2
4

x + u(t)

2
1

2. ˙x =

−2
7
1
4

x + u(t)

1
0


422
Advanced Engineering Mathematics
In problems 3–5, determine whether the system is completely controllable.
3. ˙x =

−2
7
1
4

x + u(t)

1
0

4. ˙x =

1
2
3
2

x + u(t)

1
0

5. ˙x =

 1
−2
−2
4

x + u(t)

2
1

6. Suppose A is a 2 × 2 diagonal matrix diag(a11, a22) where a11, a22 are nonzero.
For what vectors b is system (5.64) completely controllable? For what vectors c
is system (5.81) completely observable?
In problems 7 and 8, determine whether the system is completely controllable.
7. ˙x =

0
1
0
0

x +

1
1
0
0

u(t)
8. ˙x =

0
1
0
0

x +

1
1
2
0

u(t)
5.7 Linear Systems of Difference Equations
Here we study systems of first-order difference equations
xk+1 = Akxk
(5.82)
where x is in Rn and Ak is an n × n matrix; usually we will assume that Ak is a constant
matrix A, that is, does not depend of k, so that we have a linear, constant coefficients,
homogeneous system of difference equations (LCCHS)
xk+1 = Axk, k ≥0.
(5.83)
For example, in R2, an LCCHS would have the form
x1,k+1 = a11x1,k + a12x2,k
x2,k+1 = a21x1,k + a22x2,k

.
The solutions of LCCHS (5.83) are easy to find by an inductive process: denoting x0 = c,
we have
x1 = Ax0 = Ac, x2 = Ax1 = A(Ac) = A2c, . . . ,
xk = Axk−1 = A(Ak−1c) = Akc.
(5.84)

Linear Systems of ODEs
423
In the aforementioned, the vector of initial values c is unspecified, hence plays the role
of arbitrary constants. Although (5.84) is a nice formula, it doesn’t tell us much about the
behavior of solutions.
Example 5.32
Use eigenvalues and eigenvectors to write the solution of
x1,k+1
=
−2x1,k
+
x2,k
x2,k+1
=
x1,k
−
2x2,k

.
(5.85)
Method: Denote A =

−2
1
1
−2

. If we can find an eigenvector v of A corresponding to
an eigenvalue λ, then Av = λv implies A2v = A(Av) = A(λv) = λAv = λ2v, etc. We see
inductively that
Akv = λkv.
(5.86)
One can show that this matrix A has eigenvalues and eigenvectors:
λ1 = −3,
v(1) =

−1
1

and
λ2 = −1,
v(2) =

1
1

.
Using these, (5.86), and linear superposition, we know (5.85) has solutions
xk = ¯c1(−3)k

−1
1

+ ¯c2(−1)k

1
1

, k = 1, 2, · · ·
(5.87)
where c1, c2 are arbitrary constants. ⃝
In fact, these are the solutions given by (5.84) by using diagonalization and the
explanation of Theorem 2.10 in Section 2.2:
xk = Akc = (PDP−1)kc = P
 
Dk!
P−1c = P

−3
0
0
−1
k
P−1c
=

v(1)  v(2)  
(−3)k
0
0
(−1)k

P−1c =

(−3)kv(1)  (−1)kv(2) 
P−1c
=

(−3)kv(1)  (−1)kv(2)  
¯c1
¯c2

= ¯c1(−3)kv(1) + ¯c2(−1)kv(2),
where the vector ¯c ≜P−1c.
5.7.1 Color Blindness
A gene is a string of DNA, like a computer file, that can manifest in a characteristic of a
living organism. Chromosomes are like folders containing those gene “files.” Normally, a
human being has a pair of chromosomes, one each inherited from their mother and father.
Normally, a female has two X chromosomes and a male has one X chromosome and one
Y chromosome, so gender is determined by whether or not one has a Y chromosome.

424
Advanced Engineering Mathematics
Some human beings have three, or even more, chromosomes, but such people are rare and
will be ignored in the following.
Consider a sex-linked gene located on an X chromosome, so females have two (possibly
different) copies and males have only one copy. A gene variant is recessive if it manifests
only if all copies are that variant. So, a male will manifest a sex-linked recessive gene
variant if its single copy is that variant, while a female will manifest the variant only if
both of its copies are that variant.
Red–green color blindness (deuteranopia), that is, the inability to see the difference
between the colors red and green, is an example of a sex-linked recessive gene. Other
examples in human beings are hemophilia and Duchenne’s muscular dystrophy.
Suppose that in the kth generation x1,k is the proportion of the gene variant for red–
green color blindness in human females and x2,k is the proportion of that gene variant
in human males. For example, if in the kth generation 1% of females have one copy
of that gene variant and 0.02% of females have two copies of that gene variant, then
x1,k = 0.9898·0+0.01·1+0.0002·2 = 0.0104. This is obviously going to be a simplified model
because human beings, unlike flies in a laboratory experiment, do not live and reproduce
in well-defined generations. A more sophisticated model would break down the human
populations of males and females by age and take into account how their ages affect repro-
duction and future life span. Nevertheless, we can learn something significant from our
simple model.
A male can only inherit the gene variant from one of his mother’s X chromosomes, so
x2,k+1 = x1,k. A female will inherit the average of the proportion of the gene variant in her
mother’s and father’s X chromosomes, so
x2,k+1 = x1,k + x1,k
2
.
So, xk = [x1,k
x2,k]T satisfies the system of linear constant coefficients difference equations
xk+1 =

0.5
0.5
1
0

xk.
(5.88)
It’s easy to calculate that the matrix A ≜

0.5
0.5
1
0

has eigenvalues 1 and −0.5 and
corresponding complete set of corresponding eigenvectors

1
1

,

−0.5
1

,
so we can diagonalize
A =
⎡
⎣
1
−1
2
1
1
⎤
⎦
⎡
⎣
1
0
0
−1
2
⎤
⎦
⎡
⎣
2
3
1
3
−2
3
2
3
⎤
⎦.

Linear Systems of ODEs
425
The solutions are
xk =
⎡
⎣
1
−1
2
1
1
⎤
⎦
⎡
⎣
1
0
0
−1
2
⎤
⎦
k ⎡
⎣
2
3
1
3
−2
3
2
3
⎤
⎦x0 =
⎡
⎣
1
−1
2
1
1
⎤
⎦
⎡
⎣
1
0
0
(−1
2)k
⎤
⎦
⎡
⎣
2
3
1
3
−2
3
2
3
⎤
⎦x0
= 1
3
⎡
⎣
2 + (−1
2)k
1 −(−1
2)k
2 −2(−1
2)k
1 + 2(−1
2)k
⎤
⎦x0 .
So, there exists the steady state
x∞= lim
k→∞xk = 1
3

2
1
2
1

x0 = 2x1,0 + x2,0
3

1
1

.
If p ≜
2x1,0+x2,0
3
is the steady-state proportion of the red–green color blindness gene in
woman, and hence in men, too, 100p% of the men will be red–green color blind. But, a
woman needs two copies of that gene, the probability of which will be about p · p, so about
100p2% of the women will be red–green color blind. Men should be about 1
p as likely to
manifest red–green color blindness as women. In the United States, p ≈0.07, that is, about
7%, of males manifest red–green blindness, and about 0.004 of females manifest it; the
proportions are roughly of the form p and p2, as predicted by the theory. By the way, the
proportion p can vary among different ethnic groups.
For example, about 0.0002, that is, two in 10,000, of men manifest hemophilia A; hence,
about (0.0002)2, or about four in a hundred million, of women would manifest hemophilia
A. But about two in 10,000 women have one copy of the hemophilia A gene and thus
would be “carriers” of this genetic disease.
5.7.2 General Solution and the Casorati Determinant
Analogous to Definition 5.2 in Section 5.2, as well as Definitions 3.2 in Section 3.1, 3.8 in
Section 3.3, and 3.10 in Section 3.4, we have
Definition 5.10
The general solution of a system of linear homogeneous difference equations (5.82) has
the form
x(h)
k
= c1x(1)
k
+ c2x(2)
k
+ · · · + cnx(n)
k
if for every solution x∗
k of (5.82) there are values of constants c1, c2, . . . , cn giving x∗
k =
c1x(1)
k
+ c2x(2)
k
+ · · · + cnx(n)
k . In this case, we call the set of sequences {x(1)
k , x(2)
k , . . . , x(n)
k } a
complete set of basic solutions for linear homogeneous difference equation (5.82). Each
of the n sequences x(1)
k , x(2)
k , . . . , x(n)
k
is called a basic solution of (5.82).

426
Advanced Engineering Mathematics
Theorem 5.18
The system of linear homogeneous difference equations (5.82) in Rn has a general solution,
that is, a complete set of n basic solutions.
Why? The explanation for this is similar to that for Theorem 5.14 in Section 5.6, as well as
those for Theorems 3.9 in Section 3.3 and 3.15 in Section 3.4: Denote the n columns of In by
e(1), . . . , e(n). Each of the n IVPs

x(1)
k+1 = Akx(1)
k , x(1)
0
= e(1)
,

x(2)
k+1 = Akx(2)
k , x(2)
0
= e(2)
, . . . ,

x(n)
k+1 = Akx(n)
k , x(n)
0
= e(n)
has a solution. The rest of the explanation is also similar to that given for Theorem 3.9 in
Section 3.3. 2
If {x(1)
k , x(2)
k , . . . , x(n)
k } is a set of n sequences, the Casorati determinant plays a role analo-
gous to that of the Wronskian determinant for systems of linear homogeneous ODEs. We
define the Casorati by
C(x(1)
k , x(2)
k , . . . , x(n)
k ) ≜
 x(1)
k
 x(2)
k

· · ·
 x(n)
k
 .
Theorem 5.19
(Abel’s theorem) Suppose x(1)
k , x(2)
k , . . . , x(n)
k
are n solutions of the same system of linear
homogeneous difference equations (5.82) in Rn. Then
C
 
x(1)
k , x(2)
k , . . . , x(n)
k
!
=
⎛
⎝
k−1
0
ℓ=0
|Aℓ|
⎞
⎠C
 
x(1)
0 , x(2)
0 , . . . , x(n)
0
!
(5.89)
for any k ≥1.
Why? Analogous to the explanation of Theorems 4.15 in Section 4.6 and 3.12 in Section 3.3,
first we claim that
Ck ≜C
 
x(1)
k , x(2)
k , . . . , x(n)
k
!
satisfies the first-order linear homogeneous difference equation:
Ck+1 = |Ak| Ck.
(5.90)
But this is not difficult to explain, using Theorems 1.9 in Section 1.2 and 1.28(b) in
Section 1.6:
Ck+1 = C
 
x(1)
k+1, x(2)
k+1, . . . , x(n)
k+1
!
= C
 
Akx(1)
k , . . . , Akx(n)
k
!
=
 Akx(1)
k
 · · ·  Akx(n)
k

=
 Ak

x(1)
k
 · · ·  x(n)
k
  = |Ak|
 x(1)
k
 · · ·  x(n)
k
 = |Ak| Ck. 2

Linear Systems of ODEs
427
Theorem 5.20
Suppose x(1)
k , x(2)
k , . . . , x(n)
k
are solutions of the same system of linear homogeneous differ-
ence equations (5.82) in Rn and |Ak| ̸= 0 for k = 0, 1, · · · . Then
C
 
x(1)
k , x(2)
k , . . . , x(n)
k
!
̸= 0 for all k ≥0
if, and only if,

x(1)
k , x(2)
k , . . . , x(n)
k

is a complete set of basic solutions of system (5.82).
Why? Apply Abel’s Theorem 5.19 and the existence and uniqueness conclusions of
Theorem 5.18. 2
5.7.3 Complex Eigenvalues
Suppose a real matrix A has a complex conjugate pair of eigenvalues r = α ± iν, where α, ν
are real and ν > 0, and corresponding complex conjugate pair of eigenvectors v, v. Then,
as in (4.63) in Section 4.6, it helps to use the polar form of the eigenvalues:
α + iν = ρeiω = ρ(cos ω + i sin ω),
where ρ is real and nonnegative and −π < ω ≤π. Note that ω ̸= 0 because ν > 0. Then
we have
Akv =
 
ρk(cos ωk + i sin ωk)
!
v.
It follows that two solutions of LCCHS (5.83) are given by
x(1)
k
= ρk Re

(cos ωk + i sin ωk)v
	
,
x(2)
k
= ρk Im

(cos ωk + i sin ωk)v
	
.
The same as for LCCHS, we don’t need the second eigenvector, v.
Example 5.33
Solve
x1,k+1
= x1,k
−2x2,k
x2,k+1
= 4x1,k
−3x2,k

.
(5.91)
Method:
Denote A =

1
−2
4
−3

. In Example 2.9 in Section 2.1, we found that the
eigenvalues are λ = −1±i2 and that corresponding to λ = −1+i2, there is an eigenvector
v =
⎡
⎣
1
2 + i 1
2
1
⎤
⎦.
We calculate that
α + iν = −1 + i2 = ρ(cos ω + i sin ω),
where
ρ =
-
(−1)2 + 22 =
√
5,
tan ω =
2
−1.

428
Advanced Engineering Mathematics
Because α + iν = −1 + i2 is in the second quadrant, we have ω = π −arctan 2. We
calculate
(cos ωk + i sin ωk)v
= (cos ωk + i sin ωk)
⎡
⎣
1
2 + i 1
2
1
⎤
⎦= 1
2
⎡
⎣
cos ωk −sin ωk + i(cos ωk + sin ωk)
2 cos ωk + i2 sin ωk
⎤
⎦.
By discussion before this example, (5.91) has solutions
x(1)
k
= 1
2 5k/2

cos ωk −sin ωk
2 cos ωk

and
x(2)
k
= 1
2 5k/2

cos ωk + sin ωk
2 sin ωk

.
The general solution of (5.91) is
xk = c1 5k/2

cos ωk −sin ωk
2 cos ωk

+ c2 5k/2

cos ωk + sin ωk
2 sin ωk

;
the factors of 1
2 were absorbed in the arbitrary constants c1, c2. ⃝
5.7.4 Equivalence of Second-Order Scalar Difference Equation and a System in R2
Given a second-order LCCHE
yk+2 = a1yk+1 + a2yk,
(5.92)
we can define
xk =

x1,k
x2,k

≜

 yk
yk+1

.
Then we have
x1,k+1 = yk+1 = x2,k
and
x2,k+1 = yk+2 = a2yk + a1yk+1 = a2x1,k + a1x2,k.
So, xk should satisfy
xk+1 =

x2,k
a2x1,k + a1x2,k

=

 0
1
a2
a1

xk.
The two types of Casorati determinant that we have defined, one for systems of linear
homogeneous difference equations before Theorem 5.19 and the other for scalar linear
homogeneous difference equations before Definition 4.8 in Section 4.6, are equal. For
example, in R2,
C(x(1)
k , x(2)
k ) =

x(1)
1,k
x(2)
1,k
x(1)
2,k
x(2)
2,k

=

y(1)
k
y(2)
k
y(1)
k+1
y(2)
k+1

= C(y(1)
k , y(2)
k ).

Linear Systems of ODEs
429
Example 5.34
Solve
xk+1 =

0
1
−1
4
1

xk.
(5.93)
Method: The matrix A =

0
1
−1
4
1

is in companion form, so it will probably be easier to
solve the equivalent second-order scalar LCCHE:
yk+2 = yk+1 −1
4yk.
The characteristic polynomial for the latter, r2 −r + 1
4, has repeated real root r = 1
2, 1
2, so
the general solution is
yk = c1
 1
2
!k
+ c2 k
 1
2
!k
,
where c1, c2 are arbitrary constants. Correspondingly, the general solution of the original
problem, (5.93), is
xk =
⎡
⎣
yk
yk+1
⎤
⎦=
⎡
⎢⎣
c1 1
2k + c2k 1
2k
c1
1
2k+1 + c2 (k + 1)
1
2k+1
⎤
⎥⎦=
c1
2k+1
⎡
⎣
2
1
⎤
⎦+
c2
2k+1
⎡
⎣
2k
k + 1
⎤
⎦,
where c1, c2 are arbitrary constants. ⃝
5.7.5 Ladder Network Electrical Circuits
One technique electrical engineers use to analyze circuits with a sinusoidal voltage source,
V(t) = cos ωt, is to replace it by its complexification, ˜V(t) = ejωt. Here we are following
the electrical engineering convention of denoting a square root of −1 by j rather than by
i as we do in mathematics. A complex exponential current, ˜I = I0ejωt, is the response of a
circuit element to a complex exponential voltage source, ˜V, and we define their ratio to be
the impedance:
Z ≜
˜V
˜I
.
Here is a table of impedances corresponding to circuit elements, assuming that L, R, and
C are, as usual, constants. Note that the impedances are complex numbers that depend on
the frequency of the source:
Element
Impedance
Resistor
R
Inductor
jωL
Capacitor
(jωC)−1

430
Advanced Engineering Mathematics
L
Z
V~ (t)=e jωt
V~ (t)=e jωt
I~
I~
(a)
(b)
FIGURE 5.8
(a) Inductor in AC circuit and (b) impedance in AC circuit.
I0
+
–
Z0
Y0
Y1
Yk
Yn–2
Yk–1
Yn–1
Z1
…
…
Zk
Zn–1
V1
V0ejωt
V2
Vk
Vn
In
Vk+1
Vn–1
I1
Ik
Ik+1
In–2
In–1
FIGURE 5.9
Ladder network in Example 5.35.
For example, shown in Figure 5.8a is a circuit with only a voltage source and an inductor.
Kirchhoff’s voltage law implies that the voltage across the inductor, L˙˜I = L · (jω)I, equals
the voltage source, so ˜V = jωL˜I; hence, the impedance is
Z =
˜V
˜I
= jωL.
Figure 5.8b shows an abstract picture of the same circuit as Figure 5.8a, if Z = jωL.
The admittance, Y, is defined to be the reciprocal of the impedance, that is, Y = 1/Z.
In the next example and in Figure 5.9, “V0” is the modulus of the voltage source. If all
of the other circuit elements are resistances, then assume the voltage source is constant
and we have a DC circuit. If any of the other circuit elements are capacitors or inductors,
assume the voltage source is V0ejωt and we have an AC circuit.
Example 5.35
Solve the ladder network shown in Figure 5.9. Assume that V0 is given.
Method: Indicated in the picture are currents into or out of nodes in the circuit. This con-
cept of current is a little “old fashioned” but is convenient for ladder network problems.
This is not the loop current concept we used earlier in Examples 1.7 in Section 1.1, 5.1 in
Section 5.1, and 5.2 in Section 5.1.

Linear Systems of ODEs
431
By Kirchhoff’s voltage law and the definition of impedance in each of the loops,
we have
V0 = V1 + Z0I0, V1 = V2 + Z1I1, . . . , Vk = Vk+1 + ZkIk, . . . ,
Vn−1 = Vn + Zn−1In−1,
(5.94)
and by Kirchhoff’s current law and the definition of admittance at each of the nodes,
we have
I0 = I1 + Y0V1, I1 = I2 + Y1V2, . . . , Ik = Ik+1 + YkVk+1, . . . ,
In−2 = In−1 + Yn−2Vn−1.
(5.95)
In addition, we have in the nth loop that
In−1 = In = Yn−1Vn.
(5.96)
For k = 0, . . . , n −1, (5.94) yields
Vk+1 = Vk −ZkIk.
(5.97)
Equations (5.97) and (5.95) together imply, for k = 0, . . . , n −2,
Ik+1 = Ik −YkVk+1 = Ik −Yk (Vk −ZkIk) = −YkVk + (1 + YkZk) Ik.
(5.98)
Define xk ≜

Vk
Ik

. By (5.97) and (5.98), we have
xk+1 = Akxk,
(5.99)
where for k = 0, . . . , n −2,
Ak ≜

 1
−Zk
−Yk
1 + YkZk

,
(5.100)
and (5.96) yields
An−1 ≜

1
−Zn−1
0
1

.
While V0, the “input” voltage source, is assumed to be known, I0 is not. This will make
our problem more difficult than it just being an LCCHS with given initial conditions.
The solution of (5.99) is, for k = 1, . . . , n,
xk = Ak−1Ak−2 · · · A1A0x0,

432
Advanced Engineering Mathematics
that is,

Vk
Ik

= Ak−1Ak−2 · · · A1A0

V0
I0

.
The solution implies that, after noting that In = Yn−1Vn,

V0
I0

= x0 = A−1
0 A−1
1
· · · A−1
n−1xn = A−1
0 A−1
1
· · · A−1
n−1

Vn
Yn−1Vn

,
that is,

V0
I0

= Vn · A−1
0 A−1
1
· · · A−1
n−1

1
Yn−1

.
(5.101)
From the first component of the vector, it follows that
V0 = ηVn,
where η is the (1, 1) entry of the 2 × 1 vector A−1
0 A−1
1
· · · A−1
n−1

1
Yn−1

. Since we assumed
that V0 is given, we can solve for
Vn = V0
η .
When the latter is substituted into (5.101), we can find I0. Noting that Vn is a scalar, we
also see that for k = 1, . . . , n −1,

Vk
Ik

= Ak−1Ak−2 · · · A1A0

V0
I0

= Ak−1Ak−2 · · ·
A1
A0

Vn 
A−1
0 
A−1
1
· · · A−1
n−1

1
Yn−1

= Vn A−1
k A−1
k+1 · · · A−1
n−1

1
Yn−1

= V0
η A−1
k A−1
k+1 · · · A−1
n−1

1
Yn−1

. ⃝
One interesting concept is that of replacing the whole ladder network by an “equivalent
impedance” defined to be
Z ≜V0
In
=
V0
Yn−1Vn
=
V0
Yn−1η−1V0
=
η
Yn−1
.
Example 5.36
For the ladder network shown in Figure 5.10, assume V0 is an unspecified constant:
(a) Find the equivalent impedance.
(b) Find the voltages Vk for k = 0, 1, . . . , 4.
Method:
(a) Indicated in the picture are constant source voltage V0 and impedances Zk = 1 and
admittances Yk = 1 for k = 0, 1, 2, 3. From (5.100), we have matrices
Ak =

 1
−1
−1
2

,

Linear Systems of ODEs
433
I0
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
V4
V3
V2
V1
V0
+
–
1 Ω
1 Ω
I1
I2
I3
I4
FIGURE 5.10
DC ladder network in Example 5.36.
for k = 0, 1, 2 and
A3 =

1
−1
0
1

.
From (5.101) we have

V0
I0

=x0 = V4 A−1
0 A−1
1
· · · A−1
3

1
Y3

=V4

2
1
1
1
3
1
1
0
1

1
1

=· · ·=V4

34
21

.
The equivalent impedance is Z = η/Y3 = 34.
(b) From the last equation in part (a), we have I0 = 21V4 = 21
34V0.
The solution of (5.99) is, for k = 1, 2, 3,
xk = Ak−1Ak−2 · · · A1A0x0 =

 1
−1
−1
2
k 
V0
I0

= V0

 1
−1
−1
2
k 
1
−1
0
1
 
 1
21
34

.
The eigenvalues and eigenvectors enable the diagonalization

 1
−1
−1
2

=

1 −
√
5
1 +
√
5
2
2
 &
3+
√
5
2
0
0
3−
√
5
2
' 
1
4
√
5

−2
1 +
√
5
2
−1 +
√
5

.
This gives, for k = 1, 2, 3,

Vk
Ik

= PDkP−1

V0
I0

=

1 −
√
5
1 +
√
5
2
2

⎡
⎢⎣
 
3+
√
5
2
!k
0
0
 
3−
√
5
2
!k
⎤
⎥⎦
×

1
4
√
5

−2
1 +
√
5
2
−1 +
√
5

1
−1
0
1

 1
21
34

.
= · · · = V0
4
√
5
⎡
⎣
−2(1 −
√
5)λk
1 + 2(1 +
√
5)λk
2
−4λk
1 + 4λk
2
−4λk
1 + 4λk
2
2(1 +
√
5)λk
1 + 2(−1 +
√
5)λk
2
⎤
⎦
⎡
⎣
13
34
21
34
⎤
⎦,

434
Advanced Engineering Mathematics
where
λ1 = 3 +
√
5
2
,
λ2 = 3 −
√
5
2
.
So, for k = 0, 1, 2, 3,
Vk = V0
4
√
5
13
34
 
−2(1 −
√
5)λk
1 + 2(1 +
√
5)λk
2
!
+ 21
34(−4λk
1 + 4λk
2)

.
Using the facts that I4 = I3 and that here V4 = 1 · I4, we have
V4 = V0
4
√
5
13
34
 
−4λ3
1 + 4λ3
2
!
+ 21
34
 
2(1 +
√
5)λ3
1 + 2(−1 +
√
5)λ3
2
!
. ⃝
5.7.6 Stability
Definition 5.11
LCCHS (5.83), that is, xk+1 = xk is
(a) Asymptotically stable if all its solutions have limk→∞xk = 0
(b) Neutrally stable if it is not asymptotically stable but all its solutions are bounded
for k = 0, 1, 2, . . ., that is,
xk =

x1,k
x2,k
· · ·
xn,k
T ,
and for each component xj,k, there exists an Mj such that for all k ≥0 we have
|xj,k| ≤Mj
(c) Unstable if it is neither asymptotically stable nor neutrally stable, that is, there is
at least one solution that is not bounded for k = 0, 1, 2, . . .
Akin to the stability results for LCCHS in Theorem 5.11 in Section 5.3, we have
Theorem 5.21
LCCHS (5.83), that is, xk+1 = Axk, is
(a) Asymptotically stable if all of A’s eigenvalues μ satisfy |μ| < 1
(b) Neutrally stable if all of A’s eigenvalues μ satisfy |μ| ≤1 and no such μ is both
deficient and has modulus equal to 1
(c) Unstable if A has an eigenvalue whose modulus is greater than 1 or is both
deficient and has modulus equal to 1
Why? (a) Suppose μ is a real eigenvalue of A with corresponding eigenvector v. Then
xk = μkv will be a solution of (5.83), so |μ| < 1 implies limk→∞xk = 0. If μ is a nonreal

Linear Systems of ODEs
435
eigenvalue ρeiω of A with corresponding eigenvector v, then solutions
ρkRe
 
eiωkv
!
,
ρkIm
 
eiωkv
!
will have limit 0 as k →∞because ρ = |μ | < 1. The explanation for (b) is similar by again
using the form of solutions in the two cases μ real and μ = ρeiω. The explanation for (c) is
similar but requires more care in the deficient eigenvalue case. 2
Example 5.37
Use z-transforms, as in Section 4.7, to find the general solution of the homogeneous
system of difference equations (LCCHS):
x[n + 1] =

 0
1
−6
−5

x[n], n ≥0.
Method: Denote X(z) ≜Z

x[n]

. Take the z-transforms of both sides of the system of
difference equations and use (4.77) in Section 4.7 to get
z · X(z) −z x[0] = AX(z);
hence,
X(z) = z · (zI −A)−1x[0].
We do some algebraic calculations:
z · (zI −A)−1= z ·

zI −

 0
1
−6
−5
−1
= z ·

z
−1
6
z + 5
−1
= z ·
1
z2 + 5z + 6

z + 5
1
−6
z

= z ·
⎡
⎢⎢⎢⎣
z + 5
z2 + 5z + 6
1
z2 + 5z + 6
−
6
z2 + 5z + 6
z
z2 + 5z + 6
⎤
⎥⎥⎥⎦.
After that, we have to do two partial fractions expansions, from which we can
reassemble z · (zI −A)−1: first,
1
z2 + 5z + 6 =
1
(z + 2)(z + 3) =
A
z + 2 +
B
z + 3 ⇒1 = A(z + 3) + B(z + 2).
Substituting in z = −2 and z = −3 gives, respectively, A = 1 and B = −1.
Second,
z
z2 + 5z + 6 =
z
(z + 2)(z + 3) =
C
z + 2 +
D
z + 3 ⇒z = C(z + 3) + D(z + 2).
Substituting in z = −2 and z = −3 gives, respectively, C = −2 and D = 3.
Using the partial fraction expansions
1
z2 + 5z + 6 =
1
z + 2 −
1
z + 3
and
z
z2 + 5z + 6 = −
2
z + 2 +
3
z + 3,

436
Advanced Engineering Mathematics
we have
z · (zI −A)−1 = z ·
⎡
⎢⎢⎢⎣
−
2
z + 2 +
3
z + 3 + 5
 
1
z + 2 −
1
z + 3
!
1
z + 2 −
1
z + 3
−6
 
1
z + 2 −
1
z + 3
!
−
2
z + 2 +
3
z + 3
⎤
⎥⎥⎥⎦
= z ·
⎡
⎢⎢⎢⎣
3
z + 2 −
2
z + 3
1
z + 2 −
1
z + 3
−
6
z + 2 +
6
z + 3
−
2
z + 2 +
3
z + 3
⎤
⎥⎥⎥⎦.
Using (4.75) in Section 4.7, that is, Z−1
z ·
1
z −α

= αn, we find that the solution of the
system of homogeneous difference equations is
x[n] = Z−1
(zI −A)−1 
x[0] =
⎡
⎣
3(−2)n −2(−3)n
(−2)n −(−3)n
−6(−2)n + 6(−3)n
−2(−2)n + 3(−3)n
⎤
⎦x[0]. ⃝
Learn More About It
Our study of ladder networks was influenced by Circuit Theory, with Computer Appli-
cations, Omar Wing, Holt, Rhinehart and Winston, c⃝1972 and “Difference equations
and their applications,” Louis A. Pipes, Math. Mag. (32), 231–246. The former gives
a thorough study of circuits and often includes the use of the fundamental matrix,
which it calls a “characteristic matrix.” The latter has another interesting network
and also an application to coupled oscillators. Another useful reference is Calcu-
lus of Finite Differences and Difference Equations, by Murray R. Spiegel, Schaum’s Outline
Series, McGraw-Hill Book Company, c⃝1971. Section 5.7.1 was inspired by Appli-
cation 2 in Section 6.3 Linear Algebra, with Applications, 6th ed., by Steven J. Leon,
Prentice-Hall, Inc. c⃝2002.
5.7.7 Problems
In problems 1–6, find the general solution.
1. xk+1 =

3
2
2
−3

xk
2. xk+1 =

−2
7
1
4

xk
3. xk+1 =

1
2
3
2

xk
4. xk+1 =

1
−2
4
−3

xk
5. xk+1 =

0
−1
1
0

xk

Linear Systems of ODEs
437
6. xk+1 =
⎡
⎣
1
−3
6
0
−2
0
2
0
0
⎤
⎦xk. [Hint: −2 is an eigenvalue]
In problems 7–10, find (a) the equivalent impedance and (b) the voltages Vk indicated in
the ladder network shown. Assume V0 is an unspecified constant.
7. The DC network shown in Figure 5.11
8. The DC network shown in Figure 5.12
9. The AC network shown in Figure 5.13
10. The AC network shown in Figure 5.14
In problems 11 and 12, find the equivalent impedance, where V0 is an unspecified constant.
11. The DC network shown in Figure 5.15
12. The AC network shown in Figure 5.16
I0
1 Ω
1 Ω
2 Ω
2 Ω
2 Ω
2 Ω
2 Ω
V4
V5
V3
V2
V1
V0
+
–
1 Ω
1 Ω
1 Ω
I1
I2
I3
I4
I5
I5
FIGURE 5.11
Problem 5.7.7.7: DC ladder network.
I0
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
2 Ω
2 Ω
2 Ω
2 Ω
2 Ω
2 Ω
V0
+
–
V1
V2
V3
V4
I
V5
V6
I1
I2
I3
I4
I5
I6
FIGURE 5.12
Problem 5.7.7.8: DC ladder network.
I0
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
1 Ω
V4
V5
V3
V2
V1
V0e jωt
+
–
1 Ω
1 Ω
1 Ω
I1
I2
I3
I4
I5
I5
FIGURE 5.13
Problem 5.7.7.9: AC ladder network.

438
Advanced Engineering Mathematics
I0
1 Ω
1 Ω
1 F
1 F
1 F
1 F
V4
V2
V1
V0e jωt
+
–
1 Ω
1 Ω
I1
I2
I3
I4
FIGURE 5.14
Problem 5.7.7.10: AC ladder network.
I0
2 Ω
2 Ω
1 Ω
1 Ω
1 Ω
1 Ω
V4
V3
V2
V1
V0
+
–
2 Ω
2 Ω
I1
I2
I3
I4
FIGURE 5.15
Problem 5.7.7.11: DC ladder network.
I0
2 Ω
2 Ω
1 F
1 F
1 F
1 F
V4
V2
V3
V1
V0e jωt
+
–
2 Ω
2 Ω
I1
I2
I3
I4
FIGURE 5.16
Problem 5.7.7.12: AC ladder network.
In problems 13–15, solve the system of difference equations using a method of unde-
termined coefficients analogous to a method for nonresonant nonhomogeneous linear
systems of ODEs.
13. xk+1 =

1
4
1
−1

xk +
 
1
2
!k 
3
1

14. xk+1 =

1
2
2
−1

xk + cos(2k)

3
1

15. xk+1 =
⎡
⎣
1
0
0
2
3
1
−1
−2
5
⎤
⎦xk +
 
1
2
!k
⎡
⎣
1
2
3
⎤
⎦

Linear Systems of ODEs
439
16. (Project) Develop a method of undetermined coefficients, analogous to those for
scalar ODEs and scalar difference equations and possibly to that for systems of
ODEs, which includes resonant cases.
17. Assume A is a real, 5 × 5, constant matrix and has eigenvalues
√
3
2 ± i
2,
√
3
2 ± i
2, 1
2,
including repetitions. Consider the LCCHS (⋆) xk+1 = Axk. For each of (a)
through (d), decide whether it must be true, must be false, or may be true and
may be false:
(a) The system is neutrally stable.
(b) The system is asymptotically stable.
(c) The system may be neutrally stable, depending upon more information
concerning A.
(d) (⋆) has solutions xk that are periodic in k with period 6, that is, xk+6 ≡xk.
18. If the matrix A has an eigenvalue λ with |λ| = 1 that is deficient, explain why
LCCHS xk+1 = Axk is not neutrally stable.
19. Explain why Abel’s Theorem 4.15 in Section 4.6 for the second-order scalar differ-
ence equation yk+2 = a1,kyk+1 +a2,kyk follows from Abel’s Theorem 5.19 for system
of linear homogeneous difference equations (5.82).
20. (Small project) Develop a concept of “fundamental matrix of solutions” for
LCCHS xk+1 = Axk and implement your concept for a specific example of your
choosing.
21. (Small project) Develop a concept analogous to “etA” for LCCHS xk+1 = Axk and
implement your concept for a specific example of your choosing.
In problems 22–27, determine if the system xk+1 = Akxk is asymptotically stable, neu-
trally stable, or unstable. Try to do as little work as is necessary to give a fully explained
conclusion.
22. The system of Problem 5.7.7.1
23. The system of Problem 5.7.7.3
24. The system of Problem 5.7.7.4
25. The system of Problem 5.7.7.5
26. A =

1
1
1
−1

27. A =
⎡
⎢⎣
1
2
1
√
6
0
1
√
6
1
2
0
0
0
1
2
⎤
⎥⎦
5.8 Short Take: Periodic Linear Differential Equations
We will study linear periodic ODEs
¨y + p(t)˙y + q(t)y = 0,
(5.102)
where p(t), q(t) are periodic functions with period T, that is, p(t + T) ≡p(t), q(t + T) ≡q(t).

440
Advanced Engineering Mathematics
First, we will study the general case of systems of first-order ODEs:
˙x = A(t)x,
(5.103)
in Rn, where A(t) is an n×n matrix-valued T-periodic function, that is, satisfying A(t+T) ≡
A(t). Such problems occur in physical problems with “parametric forcing,” for example,
when an object is being shaken. Such systems also occur after “linearization” of nonlinear
systems, as we will see in Chapter 18.
Suppose we have a fundamental matrix Z(t) for system (5.103). Even though the coeffi-
cients in the ODE system are periodic with period T, it may or may not be true that Z(t)
or even one of its columns is periodic with period T. It may even happen that a column
of Z(t), that is, a vector solution x(t), is periodic with period 2T but not period T. This is
called period doubling.
Our work will be easier if we use X(t) = Z(t)

Z(0)
	−1, the principal fundamental matrix
at t = 0, instead of Z(t). Because X(0) = I, the unique solution of the IVP

˙x = A(t)x
x(0)= x0

(5.104)
is x(t) = X(t)x0.
Suppose (5.103) does have a nonzero solution that is T-periodic. Then there is an initial
condition vector x0 ̸= 0 such that
x(t + T) = x(t)
for all t. In particular, it follows that, at t = 0, we must have
x(T) = x(0),
(5.105)
that is,
X(T)x0 = x0.
(5.106)
Pictorially, (5.105) says that x(t) returns to where it started after T units of time.
Equation (5.106) implies that at t = T, the principal fundamental matrix, X(T), should
have μ = 1 as one of its eigenvalues, with corresponding eigenvector being the initial
condition, x0, that produces a T-periodic solution.
Theorem 5.22
System (5.103) has a T-periodic solution if, and only if, X(T) has μ = 1 as one of its
eigenvalues.
Why? We’ve already derived why (5.103) has a T-periodic solution only if X(T) has μ = 1
as one of its eigenvalues.
On the other hand, we should explain why X(T) having μ = 1 as one of its eigenvalues
guarantees that (5.103) has a T-periodic solution: if X(T)x0 = x0, then let x(t) ≜X(t)x0

Linear Systems of ODEs
441
and let z(t) ≜X(t + T)x0. We will see in the following why uniqueness of solutions (see
Theorem 5.1) in Section 5.1 of IVP (5.104) implies that x(t) ≡z(t) ≡x(t + T) so that x(t) is a
T-periodic solution.
First, we know that both x(t) and z(t) are solutions of ODE system (5.103), the latter
because the chain rule gives us
˙z(t) = d
dt[X(t + T)]x0 = ˙X(t + T) · d
dt[t + T]x0 = ˙X(t + T)x0 = A(t + T)X(t + T)x0
= A(t) (X(t + T)x0) = A(t)z(t),
after using Theorem 5.5 in Section 5.2, that is, ˙X(t) = A(t)X(t).
Second, we have
z(0) = X(0 + T)x0 = x0,
so x(t) and z(t) satisfy the same initial condition. By uniqueness of solutions of the IVP
(5.104), x(t) ≡z(t) and thus x(t) is a T-periodic solution. 2
Corollary 5.2
If X(t) has an eigenvalue μ = −1, then (5.103) has a solution that is periodic with period
2T and is not T-periodic.
Why? Note that A(t) being T-periodic implies that A(t) is also 2T-periodic. The rest of the
explanation is similar to that for Theorem 5.22. 2
5.8.1 The Stroboscopic, or “Return,” Map
Lemma 5.6
X(t + T) ≡X(t)X(T).
Why? We postpone the explanation, which is similar to that for Theorem 5.22, to later in
this section so as to not interrupt the flow of ideas.
For all initial conditions x(0) = x0, we have
x(T) = X(T)x(0) = X(T)x0,
so Lemma 5.6 yields
x(2T) = X(T + T)x(0) = X(T)x(T) =

X(T)
	2x0,
x(3T) =

X(T)
	3x0, . . . .
This tells us that for a linear homogeneous periodic system of ODEs, if we look at the
periodic returns of x(t), that is, the sequence x(0), x(T), x(2T), x(3T), . . ., then we are dealing

442
Advanced Engineering Mathematics
with the solution of
xk+1 = X(T)xk,
(5.107)
a system of linear constant coefficient homogeneous difference equations LCCHS!
Because the sequence x0, x1, x2, . . . can be thought of as a sequence of photographs of
the state of a physical system, we refer to X(T) as the stroboscopic map. For example,
think of “Milk Drop Coronet,” 1957, by Harold Edgerton, one photograph in a movie of
the impact of a drop falling into milk. See http://www.vam.ac.uk/vastatic/microsites/
photography/photographer.php?photographerid=ph019&row=4.
As for any other LCCHS, the eigenvalues and eigenvectors of the matrix of coefficients
play important roles in the solution. The matrix X(T) is called a monodromy matrix and
its eigenvalues, μ1, μ2, . . . , μn, are called the characteristic multipliers for system (5.103).
Let’s explain why they are called “multipliers.”
Suppose v(j) is an eigenvector of X(T) corresponding to an eigenvalue, μj. We have
X(T)v(j) = μjv(j).
(5.108)
It follows that
X(T)kv(j) = μk
j v(j).
If the initial condition for a solution of the original ODE system (5.103) is x(0) = v(j), then
we have for k = 1, 2, ...
x(kT) = μk
j x(0),
that is, the system returns to an evermore multiplied version of where it started.
Theorem 5.23
If Z(t) is any fundamental matrix for a linear homogeneous T-periodic system (5.103), then
the eigenvalues of Z(T) are the characteristic multipliers of system (5.103).
5.8.2 Floquet Representation
Theorem 5.24
(Floquet) For system (5.103), there exists a constant matrix C, possibly complex, and a
T-periodic matrix P(t) satisfying
X(t) = P(t)etC
and
P(0) = I.
(5.109)
Before explaining this theorem, first let’s explain Lemma 5.6: to explain why X(t + T) ≡
X(t)X(T), we use a “uniqueness of solutions” explanation similar to what we gave for

Linear Systems of ODEs
443
Theorem 5.22. Let Y(t) ≜X(t)X(T). First, we will explain why Y(t) satisfies the matrix
differential equation ˙Y(t) = A(t)Y(t): we have
˙Y(t) =
 ˙X(t)
	
X(T) = (A(t)X(t)) X(T) = A(t) (X(t)X(T) = A(t)Y(t).
Second, let U(t) ≜X(t + T). We will explain why U(t) satisfies the matrix differential
equation ˙U(t) = A(t)U(t): by the chain rule and periodicity of A(t),
˙U(t) = d
dt[X(t + T)] = ˙X(t + T) · d
dt[t + T] = ˙X(t + T) = A(t + T)X(t + T) = A(t)U(t).
Now,
Y(0) = X(0)X(T) = IX(T) = X(T) and U(0) = X(0 + T) = X(T).
Because Y(t) and U(t) satisfy both the same ODE and the same initial condition, it follows
that Y(t) ≡U(t), that is, X(t)X(T) ≡X(t + T). 2
We now give a partial explanation why the Floquet representation theorem is true in the
sense of explaining how to find C and P(t); we will explain later what other details we’re
omitting. If we could find C, then by periodicity of P(t), C would need to satisfy
X(T) = P(T)eTC = P(0)eTC = IeTC;
hence, we need to choose C so that
eTC = X(T).
(5.110)
Later we will discuss solving (5.110) for C; for now, let’s suppose that we have found
such a C. Then we would use (5.109) to find P(t):
P(t) ≜X(t)
 
etC!−1
= X(t)e−tC.
We need to explain why P(t) is periodic with period T and satisfies P(0) = I. Using Lemmas
5.6 and 5.1 in Section 5.2 and Theorem 5.8 in Section 5.2, we calculate that
P(t + T) = X(t + T)
 
e−(t+T)C!
= X(t)X(T)
 
e−TCe−tC!
= X(t)
 
X(T)e−TC!
e−tC = X(t)Ie−tC = P(t),
as we desired. Also, P(0) = X(0) · eO = I · I = I, as we desired.
As for solving (5.110) for C, we will only look at the special case where X(T) is
diagonalizable and all of its eigenvalues are positive, real numbers. Write
X(T) = QDQ−1,

444
Advanced Engineering Mathematics
where the diagonal matrix D = diag(μ1, . . . , μn) and the eigenvalues of D equal those
of X(T), that is, μ1, . . . , μn are the eigenvalues of X(T). If we try to find C in the form
C = SES−1, where E is diagonal, then we want to solve
QDQ−1 = X(T) = eTC = eTSES−1 = S
 
eTE!
S−1.
But, we can do that. Let S = Q and
E = diag
1
T ln(μ1), . . . , 1
T ln(μn)

. 2
Asides: If an eigenvalue of a monodromy matrix is a negative real number or is complex,
then we need greater care in constructing the matrix E because we don’t yet have a concept
of the natural logarithm of such a number. Also, if a monodromy matrix is not diagonal-
izable, then we would could use a more general result called the “Jordan normal form” to
construct a Floquet representation of solutions.
Example 5.38
Find a Floquet representation for solutions and characteristic multipliers of
1
˙x1 =
 
−1 +
sin t
2+cos t
!
x1
˙x2 =
−x2
+2(sin t)x1
2
.
Method: The system is periodic with period T = 2π. The first equation in the system is
solvable using the method of separation of variables:
ln |x1| = c +
 
−1 +
sin t
2 + cos t

dt = −t −ln |2 + cos t|,
where c is an arbitrary constant, yields
x1(t) = c1e−t (2 + cos t)−1 .
The initial value is
x1(0) = 1
3 c1,
so the general solution of the first ODE in the system can be written as
x1(t) = e−t ·
3
2 + cos t · x1(0).
Substitute that into the second ODE in the system, rearrange terms to put it into the
standard form of a first-order linear ODE, and multiply through by the integrating factor
of et to get
d
dt

etx2

= et(˙x2 + x2)= et · 2 sin t · x1 = ete−t · 2 sin t ·
3
2 + cos t · x1(0)=
 6 sin t
2 + cos t

x1(0).
Indefinite integration with respect to t of both sides yields
etx2(t) = c2 −6 ln(2 + cos t)x1(0),

Linear Systems of ODEs
445
so
x2(t) = e−t 
c2 −6 ln(2 + cos t)x1(0)
!
.
The initial value is
x2(0) = c2 −(6 ln 3)x1(0),
so the solutions of the second ODE can be written in the form
x2(t) = e−t 
6 ln 3 −6 ln(2 + cos t)
!
x1(0) + e−tx2(0).
To find a fundamental matrix, first summarize the general solution by writing it in
matrix times vector form:
x(t) =
⎡
⎢⎢⎢⎣
e−t
3
2 + cos t x1(0)
6 ln
 
3
2 + cos t
!
x1(0) + e−tx2(0)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
e−t
3
2 + cos t
0
e−t 6 ln
 
3
2 + cos t
!
e−t
⎤
⎥⎥⎥⎦x(0).
So, a fundamental matrix is given by
X(t) = e−t
⎡
⎢⎢⎢⎣
3
2 + cos t
0
6 ln
 
3
2 + cos t
!
1
⎤
⎥⎥⎥⎦.
In particular, substitute in t = 2π to see that in this problem X(T) = X(2π) = e−2π I, so
the characteristic multipliers, being the eigenvalues of X(T), are μ = e−2π, e−2π. We can
take Q = I, D = e−2πI in the calculation of the Floquet representation; hence, S = Q = I,
E = diag
 1
2π ln(e−2π), 1
2π ln(e−2π)

= −I,
and C =−I. The Floquet representation here is X(t) = P(t)etC = P(t)

e−tI
	
, so in this
example,
P(t) = etX(t) =
⎡
⎣
3
2+cos t
0
6 ln(
3
2+cos t)
1
⎤
⎦.
To summarize, a Floquet representation is given by
X(t) = P(t)etC =
⎛
⎜⎜⎜⎝
⎡
⎢⎢⎢⎣
3
2 + cos t
0
6 ln
 
3
2 + cos t
!
1
⎤
⎥⎥⎥⎦
⎞
⎟⎟⎟⎠
 
e−tI
!
. ⃝
5.8.3 Stability
The following definitions for periodic systems are the same as for LCCHS given in Defi-
nition 5.7 in Section 5.3 and are similar to the definitions for LCCHS given in Definition
5.10 in Section 5.7.

446
Advanced Engineering Mathematics
Definition 5.12
Linear homogeneous periodic system (5.103) is
(a) Asymptotically stable if all its solutions have limt→∞x(t) = 0
(b) Neutrally stable if it is not asymptotically stable but all its solutions are bounded
on [0, ∞), that is, for j = 1, . . . , n, each component xj(t) of x(t) there exists an Mj
such that for all t ≥0, we have |xj(t)| ≤Mj
(c) Unstable if it is neither asymptotically stable nor neutrally stable, that is, there is
at least one solution that is not bounded on [0, ∞)
Akin to the stability results for LCCHS in Theorem 5.11, we have
Theorem 5.25
Linear homogeneous periodic system (5.103) is
(a) Asymptotically stable if all characteristic multipliers μ satisfy |μ| < 1
(b) Neutrally stable if all characteristic multipliers μ satisfy |μ| ≤1 and no such μ is
both deficient and has modulus 1
(c) Unstable if there is a characteristic multiplier whose modulus is greater than 1 or
is both deficient and has modulus equal to 1
Why? To be very brief, an explanation for this result follows from the Floquet representa-
tion of solutions and Theorem 5.21 for LCCHS. 2
It is very tempting to think that we can determine stability of a linear homogeneous peri-
odic system (5.103) by looking at the eigenvalues of the matrix A(t). Unfortunately, this is
not possible! In the following example, the matrix A(t) has eigenvalues that have nega-
tive real part for all t, yet the system is not even neutrally stable, let alone asymptotically
stable. The results of Theorem 5.11 in Section 5.3 for LCCHS don’t carry over to a linear
homogeneous periodic system (5.103).
Example 5.39
The system (Markus and Yamabe, 1960)
˙x =
⎡
⎣
−1 + 3
2 cos2(t)
1 −3
2 cos t sin t
−1 −3
2 cos t sin t
−1 + 3
2 sin2(t)
⎤
⎦x
has a solution x(t) = et/2

−cos t
sin t

. The system is neither asymptotically stable nor neu-
trally stable, even though the eigenvalues of A(t) are λ(t) = −1 ± i
√
7
4
, which have
negative real part. ⃝

Linear Systems of ODEs
447
For an ODE, the time constant indicates how long it takes for a solution to decay to 1
e of
its initial value. Suppose a linear homogeneous periodic system (5.103) is asymptotically
stable. The time constant τ for that system of ODEs can be defined by
τ =
1
rmin
,
where 0 < rmin = −max{ln(|μ|) : μ is a characteristic multiplier} is the slowest decay rate.
Because each solution x(t) may include many different decaying functions, “weighted” by
time-periodic vectors, we can’t guarantee that x(τ) = 1
e x(0). Nevertheless, for physical
intuition, it is still useful to think of the time constant as being about how long it takes for
the solution to decay in a standard way.
5.8.4 Hill’s Equation
Assume q(t) is T-periodic. The second-order scalar ODE
¨y + (λ + q(t))y = 0
(5.111)
is called Hill’s equation. We will see later why λ is called an “eigenvalue parameter.”
For each value of λ, we can choose two solutions y1(t; λ), y2(t; λ) that satisfy the IVPs
1
¨y1 + (λ + q(t))y1 = 0
y1(0; λ) = 1, ˙y1(0; λ) = 0
2
,
1
¨y2 + (λ + q(t))y2 = 0
y2(0; λ) = 0, ˙y2(0; λ) = 1
2
,
just as we did in explaining why Theorem 5.3 in Section 5.2 was true. [The notation y(t; λ)
indicates that y is a function of t that also depends upon the parameter λ that is in ODE
(5.111)]. We form the matrix
X(t; λ) ≜
&y1(t; λ)
y2(t; λ)
˙y1(t; λ)
˙y2(t; λ)
'
and it is the principal fundamental matrix for ODE system
˙x =
&
0
1
−λ −q(t)
0
'
x.
(5.112)
Example 5.40
Find the principal fundamental matrix for (5.112) in the special case that λ > 0 and
q(t) ≡0.
Method: The second-order ODE is the undamped harmonic oscillator model ¨y + λy =
0. It has solutions y1(t; λ) = cos(
√
λ t and y2(t; λ) =
1
√
λ
sin(
√
λ t), so the principal
fundamental matrix is given by
X(t; λ) ≜
⎡
⎢⎢⎣
cos(
√
λ t)
1
√
λ
sin(
√
λ t)
−
√
λ sin(
√
λ t)
cos(
√
λ t)
⎤
⎥⎥⎦. ⃝

448
Advanced Engineering Mathematics
Returning to the general situation, the monodromy matrix is X(T; λ). The existence of a
T-periodic solution of Hill’s equation is equivalent to
0 = | X(T; λ) −1 · I | =

y1(T; λ) −1
y2(T; λ)
˙y1(T; λ)
˙y2(T; λ) −1

=

y1(T; λ) −1
	 ˙y2(T; λ) −1
	
−y2(T; λ)˙y1(T; λ)
=

y1(T; λ)˙y2(T; λ) −y2(T; λ)˙y1(T; λ)
	
+ 1 −˙y2(T; λ) −y1(T; λ)
= (1) + 1 −˙y2(T; λ) −y1(T; λ) ≜H(λ).
Why? Because Abel’s Theorem 3.12 in Section 3.3 and p(t) ≡0 imply that the Wronskian
determinant satisfies
|X(T; λ)| = y1(T; λ)˙y2(T; λ) −y2(T; λ)˙y1(T; λ) = W(y1, y2)(T)
= exp
⎛
⎝−
T
0
0dτ
⎞
⎠W(y1, y2)(0) = e0 
y1(0; λ)˙y2(0; λ) −y2(0; λ)˙y1(0; λ)
	
= 1 · |X(0; λ)| = 1.
The function H(λ) ≜2 −˙y2(T; λ) −y1(T; λ) is known as the Hill’s discriminant. Using a
modern ODE-IVP numerical “solvers” such as MATLAB’s ode45, which we will discuss
briefly in Chapter 8, we can find approximate numerical solutions, λ, of the “characteristic
equation”:
H(λ) = 0.
We call these values of λ eigenvalues, analogous to finding the eigenvalues of a matrix by
solving the characteristic equation P(λ) = 0.
Hill’s equation originally came up in a model of the three-body celestial dynamics prob-
lem of predicting the orbit of the moon under the gravitational influence of both the earth
and the sun. A special case of Hill’s equation is the Mathieu equation, which models vibra-
tion problems including those for machinery and oil rigs on the ocean. Hill’s equation also
can model vibrations of a crystal lattice.
5.8.5 Periodic Solution of a Nonhomogeneous ODE System
Here we will study the existence of a T-periodic solution of a nonhomogeneous system:
˙x = A(t)x(t) + f(t),
(5.113)

Linear Systems of ODEs
449
where the matrix A(t) and the vector f(t) are both periodic with period T. Using the
variation of parameters formula (5.41) in Section 5.4, the solution of (5.113) is
x(t) = X(t)
⎛
⎝x0 +
t
0
(X(s))−1 f(s)ds
⎞
⎠.
Just as for a linear homogeneous solution, the x(t) is a T-periodic solution of (5.113) if, and
only if,
x(0) = x(T),
that is,
x0 = x(0) = x(T) = X(T)x0 + X(T)
T
0
(X(s))−1 f(s)ds,
that is,
(I −X(T)) x0 = X(T)
T
0
(X(s))−1 f(s)ds.
(5.114)
So, (5.113) has a T-periodic solution x(t) if, and only if, (5.114) has a solution x0.
Theorem 5.26
(Noncritical systems) If the corresponding linear homogeneous system of ODEs (5.103),
that is, ˙x = A(t)x, has no T-periodic solution, then the nonhomogeneous system (5.113) is
guaranteed to have a T-periodic solution.
Why? By Theorem 5.22, if ˙x = A(t)x has no T-periodic solution, then μ = 1 is not a
characteristic multiplier for the corresponding homogeneous system (5.103), that is, μ = 1
is not an eigenvalue of the monodromy matrix X(T) for (5.103), so
0 ̸= | X(T) −I | = (−1)n | I −X(T)| .
It follows that (I −X(T)) is invertible, so we can solve (5.114):
x0 =

I −X(T)
	−1
⎛
⎝X(T)
T
0
(X(s))−1 f(s)ds
⎞
⎠.

450
Advanced Engineering Mathematics
So, in the noncritical case, a T-periodic solution is given by
x(t) = X(t)
⎛
⎝(I −X(T))−1 X(T)
⎛
⎝
T
0
(X(s))−1 f(s)ds
⎞
⎠+
t
0
(X(s))−1 f(s)ds
⎞
⎠. 2
(5.115)
Theorem 5.13 in Section 5.5, the nonresonant case of sinusoidal forcing, is a special case
of Theorem 5.26. If A is a constant matrix, then A is T-periodic for every T. The LCCHS
˙x = Ax is nonresonant for sinusoidal forcing if ±iω are not eigenvalues of A, in which case
the nonhomogeneous system
˙x = Ax + (cos ωt)k,
where A, k are constant, has a T-periodic solution:
x(t) = −Re
 
eiωt(A −iωI)−1k
!
.
Example 5.41
Define the square wave function f(t) as defined in (4.30) in Section 4.5, that is,
f(t) =
 1,
0 < t < T
2
−1,
T
2 < t < T

.
For ODE ˙y + 2y = f(t), (a) find a periodic solution, and (b) find the steady-state solution.
Method:
(a) Using Laplace transforms as in Example 4.34 in Section 4.5 or using the method of
variation of parameters in (5.44) in Section 5.4, we find that the solutions of the ODE
are given by
y(t) = e−2ty0 + e−2t
t
0
e2uf(u)du,
where y(0) = y0. A periodic solution y(t) must have y0 = y(T), that is,
y0 = e−2Ty0 + e−2T
T
0
e2uf(u)du.
Take the term that involves y0 to the left-hand side, factor out y0, and then divide
through by (1 −e−2T) to get
y0 = y⋆
0 =
e−2T
1 −e−2T
T
0
e2uf(u)du.
In Example 4.34 in Section 4.5, implicitly we found that
e−2T
T
0
e2uf(u)du = 1
2
 
−1 −e−2T + 2e−T!
= −1
2
 
1 −e−T!2
.

Linear Systems of ODEs
451
The ODE has exactly one periodic solution:
yP(t) = e−2t
⎛
⎝
e−2T
1 −e−2T ·

−1
2
 
1 −e−T!2
+
t
0
e2uf(u) du
⎞
⎠
= e−2t
⎛
⎝−1
2
e−2T
(1 −e−T)(1 + eT) ·
 
1 −e−T!2
+
t
0
e2uf(u) du
⎞
⎠
= e−2t
⎛
⎝−1
2
e−2T 
1 −e−T	
(1 + eT)
+
t
0
e2uf(u) du
⎞
⎠.
(b) All solutions of the ODE can be written in the form
y(t) = e−2ty0 + e−2t
t
0
e2uf(u) du = e−2t(y0 −y⋆
0 + y⋆
0) + e−2t
t
0
e2uf(u) du
= e−2t (y0 −y⋆
0) + yP(t).
The transient solution is e−2t (y0−y⋆
0) and the steady-state solution is yS(t) = yP(t). ⃝
By the way, the solution graphed in Figure 4.19 is not the periodic solution because it
has y(0) = 0 ̸= y⋆
0. Nevertheless, we can see a steady-state solution hiding in Figure 4.19.
Learn More About It
A standard, useful reference to linear periodic homogeneous ODEs is Hill’s Equation,
Wilhelm Magnus and Stanley Winkler, John Wiley & Sons, c⃝1966 (or Dover Pub-
lications, c⃝1979). Example 5.38 and Problems 5.8.6.9 and 5.8.6.10 were inspired by
an example in NonLinear Ordinary Differential Equations, by R. Grimshaw, Blackwell
Scientific Publications,
c⃝1990. A version of Problem 5.8.6.7 is on p. 82 of A Sec-
ond Course in Elementary Differential Equations, by Paul Waltman, Academic Press, Inc.,
c⃝1986. Problem 5.8.6.11 is on pp. 166–167 in Differential Equations, Classical to Con-
trolled, Dahlard L. Lukes, Academic Press, Inc., c⃝1982. Hill’s equation has its origins
in “On the part of the motion of lunar perigee which is a function of the mean motions
of the sun and moon,” by G. W. Hill, Acta Math. 8 (1886), 1–36.
5.8.6 Problems
1. (a) For the system (⋆) ˙x =

1 + sin t
0
ecos t
0

x, explain why Z(t) =

et−cos t
0
et
1

is a
fundamental matrix.
(b) Find the corresponding Floquet representation for (⋆).
2. For the scalar ODE ˙y = (α + sin t)y, where α is an unspecified constant, (a)
find the Floquet representation, and (b) discuss stability for the three cases α < 0,
α = 0, α > 0.

452
Advanced Engineering Mathematics
3. What does the Floquet theorem say about the scalar linear T-periodic ODE ˙y =
p(t)y? What does the method of integrating factor say about that ODE, as well
as the corresponding scalar linear nonhomogeneous ODE T-periodic ODE ˙y =
p(t)y + f(t)? Also, if
 T
0 p(t) dt < 0, what can you say about the solutions y(t)?
4. In Example 5.39, we gave one solution of a linear periodic homogeneous
system (⋆).
(a) Think of that solution as the first column of a fundamental matrix Z(t). Use
that solution to find an eigenvector of Z(t) and to explain why eπ is an eigen-
value of Z(2π). Then, use Theorem 5.23 to explain why eπ is a characteristic
multiplier for system (⋆).
(b) Explain why |B| = γ1γ2 if B is a 2 × 2 matrix with eigenvalues γ1, γ2. [This fact
is totally independent of the context of studying system (⋆), that is, this is just
a fact about eigenvalues of matrices.]
(c) Use your results from parts (a) and (b), along with the results of Abel’s the-
orem for systems, that is, Theorem 5.7 in Section 5.2, and Theorem 5.23. To
explain why e−2π is a second multiplier for system (⋆). The trace of A(t) is −1
2
for system (⋆).
5. Consider a T-periodic Hill’s equation (5.111), that is, (⋆) ¨y+(λ+q(t))y = 0. Assume
that the corresponding solutions y1(t; λ), y2(t; λ) have
|˙y2(T; λ) + y1(T; λ)| < 2.
Explain why all solutions of (⋆) is neutrally stable.
6. Consider a T-periodic Hill’s equation (5.111), that is, (⋆) ¨y+(λ+q(t))y = 0. Assume
that the corresponding solutions y1(t; λ), y2(t; λ) have
|˙y2(T; λ) + y1(T; λ)| > 2.
Explain why (⋆) has a solution satisfying limt→∞|y(t)| = ∞.
7. Suppose X(t) is the principal fundamental matrix at t = 0 for a T-periodic system
of ODEs (⋆) ˙x = A(t)x. Suppose also that A(−t) ≡−A(t), that is, A(t) is an odd
function. Recall from Problem 5.2.5.27 that it follows that X(−t) ≡X(t). Explain
why all solutions of (⋆) are T-periodic by using the steps in the following:
(a) First, explain why if we can explain why X(T) = I, then we can conclude that
all solutions of (⋆) are T-periodic.
(b) Use the result of Problem 5.2.5.27 to explain why X(−T
2 + T) = X( T
2 −T).
(c) Use Lemma 5.6 to explain why X(−T
2 + T) = X(−T
2)X(T).
(d) Use the results of parts (c) and (b) to explain why X(−T
2)X(T) = X(−T
2).
(e) Use the result of part (e) and the invertibility of X(t) for all t to explain why
X(T) = I.
8. Define a 2π-periodic function
b(t) ≜
h1,
0 < t < π
h2,
π < t < 2π

.

Linear Systems of ODEs
453
For the scalar ODE ¨y + b(t)˙y + y = 0,
(a) Find the characteristic multipliers for the equivalent 2π-periodic system.
(b) Explain why the system is asymptotically stable if, and only if, h1 + h2 > 0.
(c) Interpret physically the result of part (c).
In problems 9 and 10, find the Floquet representation for solutions of the given system.
One can use separation of variables to solve the ODE involving only x1, substitute that
solution into the ODE involving ˙x2, and solve that ODE using an integrating factor. After
that you’ll be able to find a fundamental matrix, find the monodromy matrix [Hint: What’s
the period of the coefficients in the system of ODEs?], and continue from there.
9.
1
˙x1 =
 
−1 −
cos t
2+sin t
!
x1
˙x2 =
2(cos t)x1
−x2
2
10.
1
˙x1 =
 
−1 +
sin t
2+cos t
!
x1
˙x2 =
−(sin t)x1
−x2
2
11. Suppose A0 and  are real, constant n × n matrices, suppose that et is T-periodic,
and suppose that A(t) ≜etA0e−t. Explain why ˙x = A(t)x has Floquet represen-
tation X(t) = P(t)etC where P(t) = et and C = A0 −. [Hint: Use (5.29) in Section
5.2, that is, that B being a constant n × n matrix implies BetB = etBB.]
12. Why can no characteristic multiplier be zero?
13. Find the characteristic multipliers for Meissner’s equation ¨y + (δ + ϵb(t))y = 0,
where
b(t) ≜
 1,
0 < t < π
−1,
π < t < 2π

.
Note: Because the ODE has a coefficient that is not continuous, to solve the ODE,
you must consider two separate ODEs, after substituting in ±1 values for b(t):
(a) ¨y + (δ + ϵ)y = 0, 0 < t < π
(b) ¨y + (δ −ϵ)y = 0, π < t < 2π.
Let y1(t; δ, ϵ) solve (a) with initial data y1(0; δ, ϵ) = 1, ˙y1(0; δ, ϵ) = 0 and then define
A = y1(π; δ, ϵ), B = ˙y1(π; δ, ϵ). Then solve (b) on the interval π < t < 2π with
initial data y1(π; δ, ϵ) = A, ˙y1(π; δ, ϵ) = B. In this way, you will find the first col-
umn of a fundamental matrix. To find the second column, let y2(t; δ, ϵ) solve (a)
with initial data y2(0; δ, ϵ) = 0, ˙y2(0; δ, ϵ) = 1, etc. [This ODE is similar to the
Kronig–Penney model in the subject of quantum mechanics.]
14. Find a condition of the form 0
=
 2π
0
g(s)f(s)ds that guarantees that ODE
¨y + y = f(t) has a 2π-periodic solution. Use formula (5.114) for the equivalent
system in R2.
15. Suppose that A(t) and f(t) are T-periodic and that the system ˙y = −A(t)Ty has a
T-periodic solution y(t). If the system ˙y = A(t)y + f(t) has a T-periodic solution
x(t), explain why 0 =
 T
0 yT(s)f(s)ds must be true.

454
Advanced Engineering Mathematics
In problems 16 and 17, determine if the system ˙x = A(t)x is asymptotically stable, neutrally
stable, or unstable.
16. The system of Problem 5.8.6.9
17. The system of Problem 5.8.6.10
18. Suppose k is a positive constant. Consider the system of ODEs (⋆)
˙x = A(t)x,
where A =

k cos 2kt
k −k sin 2kt
−k + k sin 2kt
−2k + k cos 2kt

is periodic with period π/k.
(a) Verify that x(1)(t) ≜e−kt

sin kt
cos kt

is a solution of (⋆) and thus gives us that one
characteristic multiplier is μ1 = e−π.
(b) Use Abel’s Theorem 5.7 in Section 5.2 to explain why the product of the two
characteristic multipliers is μ1μ2 = e−2π, even though we do not know a
second basic solution of (⋆).
(c) Use parts (a) and (b) to explain why (⋆) is asymptotically stable.
Key Terms
adjoint system: Problem 5.2.5.28
admittance: before Example 5.35 in Section 5.7
asymptotically stable: Definitions 5.7 in Section 5.3, (5.11) in Section 5.7, (5.12) in
Section 5.8
basic solution: Definition 5.2 in Section 5.2, Definition 5.10 in Section 5.7
Casorati determinant: before Theorem 5.19 in Section 5.7
Cayley–Hamilton Theorem: Theorem 5.15 in Section 5.6
characteristic multipliers: after (5.108) in Section 5.8
closed vector subspace: after Lemma 5.4 in Section 5.6
companion form: Definition 5.6 in Section 5.2
compartment models, compartments: after (5.12) in Section 5.1
complete set of basic solutions: Definition 5.2 in Section 5.2, Definition 5.10 in Section 5.7
completely controllable: Definition 5.8 in Section 5.6
completely observable: Definition 5.9 in Section 5.6
complexification: (5.58) in Section 5.5
control, control function: after (5.64) in Section 5.6
drive: after (5.66) in Section 5.6
Floquet representation: Theorem 5.24 in Section 5.8
fundamental matrix (of solutions): Definition 5.3 in Section 5.2
gene: before (5.88) in Section 5.7
general solution: Definition 5.2 in Section 5.2, Definition 5.10 in Section 5.7
generalized eigenvector: after (5.35) in Section 5.3
Hill’s discriminant: before (5.113) in Section 5.8
Hill’s equation: (5.111) in Section 5.8
impedance: before Example 5.35 in Section 5.7
ladder network: Examples 5.35 in Section 5.7, (5.36) in Section 5.7
LCCHS: after (5.4) in Section 5.1
linear system: (5.3) in Section 5.1
monodromy matrix: before (5.108) in Section 5.8

Linear Systems of ODEs
455
multivariable control system: before (5.80) in Section 5.6
neutrally stable: Definitions 5.7 in Section 5.3, (5.11) in Section 5.7, (5.12) in Section 5.8
nonresonance assumption: after (5.51) in Section 5.5
optimal control: before (5.80) in Section 5.6
period doubling: after (5.103) in Section 5.8
piecewise continuous: before (5.70) in Section 5.6
principal fundamental matrix at t = 0: before (5.104) in Section 5.8
recessive (gene): before (5.88) in Section 5.7
sex-linked gene: before (5.88) in Section 5.7
single input control system: (5.64) in Section 5.6
sinusoidal forcing: before Theorem 5.13 in Section 5.5
solution: Definition 5.1 in Section 5.1
state of the system: after (5.64) in Section 5.6
stroboscopic map: before (5.108) in Section 5.8
systems of second order equations: before Example 5.5 in Section 5.1
trace: before Theorem 5.7 in Section 5.2
unstable: Definitions 5.7 in Section 5.3, (5.11) in Section 5.7, (5.12) in Section 5.8
Wronskian: Definition 5.4 in Section 5.2
z−transforms: Example 5.37 in Section 5.7
MATLAB R⃝Commands
ode45: after Example 5.40 in Section 5.8
roots: Problem 5.2.5.33
References
Killough, G.G. and Eckerman, K.F. A conversational eigenanalysis program for solving differential
equation. Midyear topical meeting of the Health Physics Society, Pasco, WA, February 5, 1984,
Technical Report CONF-840202-21 of the Oak Ridge National Lab., TN.
Riggs, D.S. Quantitative aspects of iodine metabolism in man. Pharmacological Reviews 4, 285–369,
1952.


6
Geometry, Calculus, and Other Tools
6.1 Dot Product, Cross Product, Lines, and Planes
In this chapter, we study many geometric and differential calculus results that have many
useful and powerful applications to problems of engineering and science.
6.1.1 Dot Product and Cross Product
We have been using the dot product, which is familiar also from its physical applications.
For example, the work, W, is given by
W = F • u ≜||F|| ||u|| cos θ,
where
F is a constant force
u is the displacement
θ is the angle between the vectors F and u satisfying 0 ≤θ ≤π.
Algebraically, for two vectors r1, r2 in R3,
r1 • r2 =
⎡
⎣
x1
y1
z1
⎤
⎦•
⎡
⎣
x2
y2
z2
⎤
⎦= x1x2 + y1y2 + z1z2.
Denote
ˆı ≜
⎡
⎣
1
0
0
⎤
⎦,
ˆj ≜
⎡
⎣
0
1
0
⎤
⎦,
ˆk ≜
⎡
⎣
0
0
1
⎤
⎦.
(6.1)
The cross product between vectors r1 = x1ˆı + y1 ˆj + z1 ˆk and r2 = x2ˆı + y2 ˆj + z2 ˆk is
defined by
r1 × r2 =

ˆı
ˆj
ˆk
x1
y1
z1
x2
y2
z2

=

y1
z1
y2
z2
 ˆı −

x1
z1
x2
z2
 ˆj +

x1
y1
x2
y2
 ˆk
= (y1z2 −z1y2)ˆı + (z1x2 −x1z2) ˆj + (x1y2 −y1x2)ˆk.
457

458
Advanced Engineering Mathematics
Theorem 6.1
(Properties of the cross product)
(a) x × y is perpendicular to both x and y.
(b) ||x × y|| = || x || ||y|| sin θ, where θ is the angle between the vectors x and y such
that 0 ≤θ ≤π.
(c) x × y = 0 if, and only if, {x, y} is linearly dependent, that is, x and y are parallel
or one of them is the zero vector.
(d) y × x = −x × y.
(e) ˆı × ˆj = ˆk, ˆj × ˆk = ˆı, ˆk × ˆı = ˆj.
(f) ˆı × ˆı = 0, ˆj × ˆj = 0, ˆk × ˆk = 0.
Example 6.1
A force F acting on a lever arm at a position r relative to an axis applies the torque
τ ≜r × F, as depicted in Figure 6.1.
Example 6.2
A charge q moving with a velocity v in a magnetic flux density B experiences the
Lorentz force F = q v × B.
Example 6.3
Find a unit vector normal to both of the vectors r1 = ˆı + 2 ˆj + 3ˆk, and r2 = ˆj −ˆk.
Method: Let u = r1×r2 =

ˆı
ˆj
ˆk
1
2
3
0
1
−1

= −5ˆı + ˆj + ˆk. So, u ≜
1
||u|| u =
1
√
27 (−5ˆı + ˆj + ˆk)
is such a unit vector, as is also −u. ⃝
A  symbol over a vector signifies that it has unit length.
r
τ
F
(x, y, z)
x
y
FIGURE 6.1
Torque.

Geometry, Calculus, and Other Tools
459
6.1.2 Lines
Definition 6.1
(a) Given a point P = (x, y, z), its position vector is
r ≜−→
OP =
⎡
⎢⎣
x
y
z
⎤
⎥⎦= x ˆı + y ˆj + z ˆk.
(b) Given two points P0 = (x0, y0, z0), and P1 = (x1, y1, z1), the vector from P0
to P1 is
−−→
P0P1 = (x1 −x0)ˆı + (y1 −y0) ˆj + (z1 −z0)ˆk.
In particular, when P0 = O = (0, 0, 0), the origin, and P1 = P = (x, y, z), we get that
−→
OP = r is the position vector of point P. Note also that −−→
P0P1 = r1 −r0 is the difference of
the position vectors of P0 and P1.
Remark
Given any two points P0, and P1 on a given line L, let v = −−→
P0P1 = u ˆı + v ˆj + w ˆk and let r0
be the position vector of P0. Then the line L consists of all points P whose position vectors
r are of the form
r = r0 + tv, where −∞< t < ∞,
(6.2)
that is,
⎧
⎪⎨
⎪⎩
x = x0 + tu
y = y0 + tv
z = z0 + tw
⎫
⎪⎬
⎪⎭
, where −∞< t < ∞.
(6.3)
The latter are the parametric equations of the line L.
6.1.3 Planes
Example 6.4
Suppose a force F = 2ˆı + ˆj −3ˆk is applied at a point P whose position vector is r =
x ˆı + y ˆj + z ˆk and is on a lever arm passing through the point P0 = (1, −2, 4). Find the
set of all points P that maximize the ratio of the magnitude of the torque to the distance
from P0 and describe that set geometrically.

460
Advanced Engineering Mathematics
Method: We can write r = r0 + R, where R is a scalar multiple of the direction vector of
the lever arm. Because the torque is τ = r × F, its magnitude is
||τ|| = ||R|| ||F|| sin θ,
where θ is the angle between the vectors F and R with 0 ≤θ ≤π.
The ratio
||τ||
||R|| = ||F|| sin θ
is maximized when θ = π
2 , that is, F is perpendicular to the lever arm. So, the desired
set of points consists of those whose position vectors are r = r0 + R satisfying
0 = R • F = (r −r0) • F = ((x −1)ˆı + (y −(−2)) ˆj + (z −4)ˆk) • (2ˆı + ˆj −3ˆk),
that is,
0 = 2(x −1) + (y + 2) −3(z −4),
(6.4)
that is,
2x + y −3z = −12.
(6.5)
Both (6.4) and (6.5) are equations of a plane that passes through the point P0
=
(1, −2, 4). ⃝
In general, a plane is the set of all points (x, y, z) satisfying a single equation of the form
Ax + By + Cz = D
or
A(x −x0) + B(y −y0) + C(z −z0) = 0
(6.6)
where A, B, C, D are scalar constants. In (6.6) the point P0 = (x0, y0, z0) lies on the plane.
The vector n = A ˆı + B ˆj + C ˆk is normal to the plane described by (6.6), which can be
written as
n • (r −r0) = 0
(6.7)
Example 6.5
Find an equation of the plane that contains the points (x, y, z) = (1, 0, 1), (0, 2, 1), and
(2, 1, 0).
Method 1: Lying in the plane are the three vectors connecting the points:
v1 = −−−−−−−−−−−−→
(0, 2, 1) −(1, 0, 1) = −ˆı + 2 ˆj
v2 = −−−−−−−−−−−−→
(2, 1, 0) −(0, 2, 1) = 2ˆı −ˆj −ˆk
v3 = −−−−−−−−−−−−→
(1, 0, 1) −(2, 1, 0) = −ˆı −ˆj + ˆk.

Geometry, Calculus, and Other Tools
461
No two of these nonzero vectors are parallel, so any pair of them is linearly indepen-
dent and thus spans the plane. If we take the cross product of any two of them, for
example, v1 × v2, we will get a vector normal to the plane because the cross product is
perpendicular to those two. So,
n ≜v1 × v2 = (−ˆı + 2 ˆj) × (2ˆı −ˆj −ˆk) = · · · = −2ˆı −ˆj −3ˆk
is normal to the plane. Using the point P0 = (1, 0, 1), the plane consists of all of the points
that satisfy
0 = n • (r −r0) = (−2ˆı −ˆj −3ˆk) • ((x −1)ˆı −(y −0) ˆj + (z −1)ˆk),
that is,
0 = −2(x −1) −(y −0) −3(z −1). ⃝
(6.8)
Method 2: The plane is described by an equation of the form Ax + By + Cz −D = 0.
Substituting in the three given points, we get a homogeneous system of three linear
equations, respectively, in three unknowns:
⎧
⎨
⎩
A · 1 + B · 0 + C · 1 −D = 0
A · 0 + B · 2 + C · 1 −D = 0
A · 2 + B · 1 + C · 0 −D = 0
⎫
⎬
⎭.
On the augmented matrix
⎡
⎣
1
0
1
−1
| 0
0
2
1
−1
| 0
2
1
0
−1
| 0
⎤
⎦
do the row operations −2R1 + R3 →R3, R2 ↔R3, −2R2 + R3 →R3, 1
5R3 →R3,
2R3 + R2 →R2, −R3 + R1 →R1 to find its RREF
⎡
⎣
1⃝
0
0
−0.4
| 0
0
1⃝
0
−0.2
| 0
0
0
1⃝
−0.6
| 0
⎤
⎦.
There are infinitely many nontrivial solutions with A = 0.4D, B = 0.2D, and C = 0.6C.
For example, for D = 5, A = 2, B = 1, and C = 3, the plane is
2x + y + 3z = 5,
which agrees with (6.8), found by Method 1. ⃝
In any kind of mathematical work, it is very good if we can use two different methods—
and, of course, arrive at the same solution. This not only checks our conclusion but also
enables us to understand the problem from different perspectives. In Example 6.5, Method
1 viewed the problem geometrically, and Method 2 viewed the problem algebraically.
Each person may find one method more satisfying than the other and may even find yet a
third or fourth method most satisfying.
6.1.4 Problems
1. For the vectors A = ˆı −ˆj and B = 2 ˆj + ˆk, find (a) A • B, (b) A × B, and (c) the
angle between A and B.
For problems 2–5, find parametric equations of the line satisfying the following given
conditions:

Geometry, Calculus, and Other Tools
461
No two of these nonzero vectors are parallel, so any pair of them is linearly indepen-
dent and thus spans the plane. If we take the cross product of any two of them, for
example, v1 × v2, we will get a vector normal to the plane because the cross product is
perpendicular to those two. So,
n ≜v1 × v2 = (−ˆı + 2 ˆj) × (2ˆı −ˆj −ˆk) = · · · = −2ˆı −ˆj −3ˆk
is normal to the plane. Using the point P0 = (1, 0, 1), the plane consists of all of the points
that satisfy
0 = n • (r −r0) = (−2ˆı −ˆj −3ˆk) • ((x −1)ˆı −(y −0) ˆj + (z −1)ˆk),
that is,
0 = −2(x −1) −(y −0) −3(z −1). ⃝
(6.8)
Method 2: The plane is described by an equation of the form Ax + By + Cz −D = 0.
Substituting in the three given points, we get a homogeneous system of three linear
equations, respectively, in three unknowns:
⎧
⎨
⎩
A · 1 + B · 0 + C · 1 −D = 0
A · 0 + B · 2 + C · 1 −D = 0
A · 2 + B · 1 + C · 0 −D = 0
⎫
⎬
⎭.
On the augmented matrix
⎡
⎣
1
0
1
−1
| 0
0
2
1
−1
| 0
2
1
0
−1
| 0
⎤
⎦
do the row operations −2R1 + R3 →R3, R2 ↔R3, −2R2 + R3 →R3, 1
5R3 →R3,
2R3 + R2 →R2, −R3 + R1 →R1 to find its RREF
⎡
⎣
1⃝
0
0
−0.4
| 0
0
1⃝
0
−0.2
| 0
0
0
1⃝
−0.6
| 0
⎤
⎦.
There are infinitely many nontrivial solutions with A = 0.4D, B = 0.2D, and C = 0.6C.
For example, for D = 5, A = 2, B = 1, and C = 3, the plane is
2x + y + 3z = 5,
which agrees with (6.8), found by Method 1. ⃝
In any kind of mathematical work, it is very good if we can use two different methods—
and, of course, arrive at the same solution. This not only checks our conclusion but also
enables us to understand the problem from different perspectives. In Example 6.5, Method
1 viewed the problem geometrically, and Method 2 viewed the problem algebraically.
Each person may find one method more satisfying than the other and may even find yet a
third or fourth method most satisfying.
6.1.4 Problems
1. For the vectors A = ˆı −ˆj and B = 2 ˆj + ˆk, find (a) A • B, (b) A × B, and (c) the
angle between A and B.
For problems 2–5, find parametric equations of the line satisfying the following given
conditions:

462
Advanced Engineering Mathematics
2. Passes through points (1, 3, 5) and (0, −1, 4).
3. Passes through points (0, 3, 5) and (2, 1, −1).
4. Passes through the point (2, 4, 1) and is perpendicular to the plane x+2y−z = −1.
5. Passes through the point (2, 4, 1) and is perpendicular to the plane −x+2y+3z = 4.
For problems 6–12, find an equation of the plane satisfying the given conditions.
6. Passes through the points (1, 0, 6), (2, 1, 8), and (3, 2, 4).
7. Passes through the points (0, 1, 1), (1, 0, 2), and (2, 3, 0).
8. Passes through the points (1, 3, 5), (2, −1, 4), and (−1, 1, 0).
9. Passes through the point (2, 4, 1) and is perpendicular to the line whose parametric
equations are r = (2 + 3t)ˆı + (4 −2t) ˆj + (1 + t)ˆk, −∞< t < ∞.
10. Passes through the point (2, 4, 1) and is perpendicular to the line whose parametric
equations are r = 3t ˆı + (5 −2t) ˆj + (3 + t)ˆk, −∞< t < ∞.
11. Passes through the point (2, 4, 1) and is perpendicular to the line that passes
through the points (0, 1, 3), and (−1, 2, 4).
12. Contains both of the lines whose parametric equations are r = (2+3t)ˆı + (4−2t) ˆj +
(1 + t)ˆk, −∞< t < ∞. and r = 2t ˆı + (5 −t) ˆj + (−1 + 2t)ˆk, −∞< t < ∞.
13. Find a unit vector normal to both of the vectors 2ˆı −ˆj + ˆk and 4ˆı + ˆk.
14. Suppose the force F = ˆı −2 ˆj + 4ˆk is applied at a point P whose position vector is
r = x ˆı + y ˆj + z ˆk and is on a lever arm passing through the point P0 = (4, 1, 2).
Find the set of all points P that maximize the ratio of the magnitude of the torque
to the distance from P0, and describe that set geometrically.
15. For what angle(s) θ between the force vector F and the displacement vector u do
we get the maximum magnitude of the work, W?
16. A charge q moves with a constant speed of 10 m/s in a magnetic field B = −2ˆı +
ˆj + 3ˆk, in teslas. What is the smallest charge, in coulombs, that can experience 1 N
of Lorentz force, assuming the direction of travel is chosen to maximize that force?
17. A particle whose mass is m and electric charge is −q moves through a constant
magnetic flux density B0. Assuming there are no other forces on the particle, New-
ton’s second law of motion yields the (vector) differential equation m¨r = −qv×B0,
which along with the relationship between position and velocity yields the system
of ordinary differential equations (ODEs):
˙r = v, m˙v = −qv × B0.
Let the particle have v(0) = v0 ̸= 0 as its initial velocity vector and r(0) = r0 as its
initial velocity vector.
(a) Explain
why
the
particle’s
motion
remains
in
the
plane
r = r(0) +
Span{v(0), a(0)}, where a(0) ≜−q
m v0 × B0 is the particle’s initial acceleration,
as long as a(0) ̸= 0.
(b) Explain why we can express r(t) = r0 + g(t)v(0) + h(t)a(0) for some unknown
functions g(t), h(t).

Geometry, Calculus, and Other Tools
463
(c) Explain why g(t) and h(t) both satisfy the scalar ODE of undamped harmonic
motion, ¨y = −(||a0||2/||v0||2) y.
(d) Solve for g(t), h(t) and thus find the motion of the particle. This is called
cyclotron motion∗.
18. Suppose that A and B are orthogonal vectors. Why is ||A + B||2 = ||A||2 + ||B||2?
[Note that this is the Pythagorean Theorem.]
6.2 Trigonometry, Polar, Cylindrical, and Spherical Coordinates
Any point in the xy-plane can be represented by polar coordinates (r, θ) that satisfy
x = r cos θ,
y = r sin θ,
where r ≜

x2 + y2 ≥0, as shown in Figure 6.2. The Pythagorean theorem implies the
most basic of trigonometric identities:
cos2 θ + sin2 θ ≡1.
(6.9)
The other four basic trigonometric functions are defined by
tan θ ≜y
x = sin θ
cos θ , if x ̸= 0.
sec θ ≜r
x =
1
cos θ , if x ̸= 0.
csc θ ≜r
y =
1
sin θ , if y ̸= 0.
cot θ ≜x
y = cos θ
sin θ , if y ̸= 0.
y
x
r
(x, y)
θ
FIGURE 6.2
Polar coordinates.
∗From Prof. Paul Diament’s EE3401Y Supplementary Class Notes, Columbia University, Spring, 1974.

464
Advanced Engineering Mathematics
Identity (6.9) implies the identities
tan2 θ + 1 ≡sec2 θ,
cot2 θ + 1 ≡csc2 θ.
(6.10)
Recall that an inverse function f −1 for a function f with domain D is defined by the
relationship
y = f(θ), θ in D ⇐⇒θ = f −1(y), y in f(D),
assuming f is one-to-one on D, that is, f(θ1) ̸= f(θ2) when θ1, θ2 are distinct elements of the
domain, D. The set f(D) = {f(θ) : θ in D} is called the range or image of D. Also, if f has
domain D and has inverse function f −1, then we have the cancellation properties
f −1(f(θ)) = θ, for all θ in D,
and
f(f −1(y)) = y, for all y in f(D).
The function f(θ) ≜sin θ has derivative, cos θ, that is positive for −π
2 < θ < π
2 , so f is
strictly decreasing on D. Because of this, sin θ is one-to-one on domain D ≜−π
2 ≤θ ≤π
2 .
The inverse function is sin−1(y), also known as arcsin(y), which is defined for y in f(D) =
{y : −1 ≤y ≤1}.
The function f(θ) ≜tan θ has derivative, sec2 θ, that is positive for −π
2 < θ < π
2 , so f is
strictly increasing on D. Because of this, tan θ is one-to-one on domain D ≜−π
2 < θ < π
2 .
The inverse function is tan−1(z), also known as arctan(z), which is defined for z in f(D) =
{z : −∞< z < ∞}.
The function f(θ) ≜cos θ has derivative, −sin θ, that is negative for 0 < θ < π, so f is
strictly decreasing on D. Because of this, cos θ is one-to-one on domain D ≜0 ≤θ ≤π.
The inverse function is cos−1(x), also known as arccos(x), which is defined for x in f(D) =
{x : −1 ≤x ≤1}.
As we noted in (3.37) in Section 3.3,
θ =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
arctan(y/x),
if x > 0
π + arctan(y/x),
if x < 0
π
2 ,
if x = 0 and y > 0
−π
2 ,
if x = 0 and y < 0
D.N.E.,
if (x, y) = (0, 0)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(6.11)

Geometry, Calculus, and Other Tools
465
y
x
r
θ
1
ψ
(–√3, 1)
–√3
FIGURE 6.3
Example 6.6.
Example 6.6
Find the polar coordinates of the point (x, y) = (−
√
3, 1).
Method:
r =

(−
√
3)2 + (1)2 = 2 and tan θ =
y
x =
1
−
√
3 = −1
√
3. Unfortunately,
θ ̸= tan−1(−1
√
3) because (x, y) is in the second quadrant. It helps to draw the picture
in Figure 6.3: The reference angle ψ = π
6 follows from a 30◦−60◦−90◦triangle having
sides of lengths 1,
√
3, 2, respectively, opposite those angles. Because θ = π −ψ, θ = 5π
6 .
To summarize, the polar coordinates of (−
√
3, 1) are (r, θ) = (2, 5π
6 ). ⃝
6.2.1 Cylindrical Coordinates
Given a point (x, y, z) in R3, we can replace the x and y coordinates by their polar
coordinates r, θ to get the point’s cylindrical coordinates (r, θ, z):
x = r cos θ,
y = r sin θ,
z = z.
Geometrically, to find the cylindrical coordinates of (x, y, z), project (x, y, z) to the point
(x, y, 0) in the z = 0 plane; after that, find the polar coordinates (r, θ) of (x, y). This is shown
in Figure 6.4. The point (x, y, z) has position vector
r = r cos θ ˆı + r sin θ ˆj + z ˆk.
So,
||r|| = ||r cos θ ˆı + r sin θ ˆj + z ˆk|| =

(r cos θ)2 + (r sin θ)2 + z2 =

r2 + z2.
6.2.2 Spherical Coordinates
Given a point (x, y, z) in R3, whose position vector r = x ˆı + y ˆj + z ˆk, define
ρ ≜||r|| =

x2 + y2 + z2.

466
Advanced Engineering Mathematics
(x,y,z)
y
y
x
z
r
r
(x,y,0)
FIGURE 6.4
Cylindrical coordinates.
Define φ to be the angle measured downward from the positive z-axis to the vector
r, where 0 ≤φ ≤π. We have z = ρ cos φ and r = ρ sin φ, as shown in Figure 6.5. Geo-
metrically, (0, 0, z) is the orthogonal projection of (x, y, z) onto the z-axis, and (x, y, 0)
is the orthogonal projection of (x, y, z) onto the z = 0 plane. Using polar coordinates,
x = r cos θ, y = r sin θ. Substituting in r = ρ sin φ, we get the point’s spherical coordinates
(ρ, φ, θ):
x = r cos θ = ρ sin φ cos θ,
y = r sin θ = ρ sin φ sin θ,
z = ρ cos φ.
(6.12)
z
z
x
y
(x,y,z)
r
θ
ρ
f
(x,y,0)
FIGURE 6.5
Spherical coordinates.

464
Advanced Engineering Mathematics
Identity (6.9) implies the identities
tan2 θ + 1 ≡sec2 θ,
cot2 θ + 1 ≡csc2 θ.
(6.10)
Recall that an inverse function f −1 for a function f with domain D is defined by the
relationship
y = f(θ), θ in D ⇐⇒θ = f −1(y), y in f(D),
assuming f is one-to-one on D, that is, f(θ1) ̸= f(θ2) when θ1, θ2 are distinct elements of the
domain, D. The set f(D) = {f(θ) : θ in D} is called the range or image of D. Also, if f has
domain D and has inverse function f −1, then we have the cancellation properties
f −1(f(θ)) = θ, for all θ in D,
and
f(f −1(y)) = y, for all y in f(D).
The function f(θ) ≜sin θ has derivative, cos θ, that is positive for −π
2 < θ < π
2 , so f is
strictly decreasing on D. Because of this, sin θ is one-to-one on domain D ≜−π
2 ≤θ ≤π
2 .
The inverse function is sin−1(y), also known as arcsin(y), which is defined for y in f(D) =
{y : −1 ≤y ≤1}.
The function f(θ) ≜tan θ has derivative, sec2 θ, that is positive for −π
2 < θ < π
2 , so f is
strictly increasing on D. Because of this, tan θ is one-to-one on domain D ≜−π
2 < θ < π
2 .
The inverse function is tan−1(z), also known as arctan(z), which is defined for z in f(D) =
{z : −∞< z < ∞}.
The function f(θ) ≜cos θ has derivative, −sin θ, that is negative for 0 < θ < π, so f is
strictly decreasing on D. Because of this, cos θ is one-to-one on domain D ≜0 ≤θ ≤π.
The inverse function is cos−1(x), also known as arccos(x), which is defined for x in f(D) =
{x : −1 ≤x ≤1}.
As we noted in (3.37) in Section 3.3,
θ =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
arctan(y/x),
if x > 0
π + arctan(y/x),
if x < 0
π
2 ,
if x = 0 and y > 0
−π
2 ,
if x = 0 and y < 0
D.N.E.,
if (x, y) = (0, 0)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(6.11)

464
Advanced Engineering Mathematics
Identity (6.9) implies the identities
tan2 θ + 1 ≡sec2 θ,
cot2 θ + 1 ≡csc2 θ.
(6.10)
Recall that an inverse function f −1 for a function f with domain D is defined by the
relationship
y = f(θ), θ in D ⇐⇒θ = f −1(y), y in f(D),
assuming f is one-to-one on D, that is, f(θ1) ̸= f(θ2) when θ1, θ2 are distinct elements of the
domain, D. The set f(D) = {f(θ) : θ in D} is called the range or image of D. Also, if f has
domain D and has inverse function f −1, then we have the cancellation properties
f −1(f(θ)) = θ, for all θ in D,
and
f(f −1(y)) = y, for all y in f(D).
The function f(θ) ≜sin θ has derivative, cos θ, that is positive for −π
2 < θ < π
2 , so f is
strictly decreasing on D. Because of this, sin θ is one-to-one on domain D ≜−π
2 ≤θ ≤π
2 .
The inverse function is sin−1(y), also known as arcsin(y), which is defined for y in f(D) =
{y : −1 ≤y ≤1}.
The function f(θ) ≜tan θ has derivative, sec2 θ, that is positive for −π
2 < θ < π
2 , so f is
strictly increasing on D. Because of this, tan θ is one-to-one on domain D ≜−π
2 < θ < π
2 .
The inverse function is tan−1(z), also known as arctan(z), which is defined for z in f(D) =
{z : −∞< z < ∞}.
The function f(θ) ≜cos θ has derivative, −sin θ, that is negative for 0 < θ < π, so f is
strictly decreasing on D. Because of this, cos θ is one-to-one on domain D ≜0 ≤θ ≤π.
The inverse function is cos−1(x), also known as arccos(x), which is defined for x in f(D) =
{x : −1 ≤x ≤1}.
As we noted in (3.37) in Section 3.3,
θ =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
arctan(y/x),
if x > 0
π + arctan(y/x),
if x < 0
π
2 ,
if x = 0 and y > 0
−π
2 ,
if x = 0 and y < 0
D.N.E.,
if (x, y) = (0, 0)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(6.11)

Geometry, Calculus, and Other Tools
467
Example 6.7
Find the spherical coordinates of the point (x, y) = (−1, 1, −
√
2).
Method: ρ =

x2 + y2 + z2 =

(−1)2 + 12 + (−
√
2)2 = 2, so
−
√
2 = z = ρ cos φ = 2 cos φ
implies cos φ = −1
√
2. Because 0 ≤φ ≤π, φ = cos−1(−1
√
2) =
3π
4 . It follows that
r = ρ sin φ = 2 sin( 3π
4 ) =
√
2. Our point has x = −1, y = 1, so
−1
= x = r cos θ =
√
2 cos θ and
1
= y = r sin θ =
√
2 sin θ.
It follows that θ = 3π
4 . The spherical coordinates of the point are (ρ, φ, θ) = (2, 3π
4 , 3π
4 ). ⃝
6.2.3 Right-Handed Orthogonal Bases for R3
The standard orthonormal (o.n) basis for R3 is {ˆı, ˆj, ˆk}. It was also denoted {e(1), e(2), e(3)}
in Chapter 1, where we noted that they are the columns of the identity matrix, I3. We will
see in this section that in natural ways, cylindrical and spherical coordinates give different
o.n. bases for R3.
Cylindrical coordinates (r, θ, z) are based on the orthogonal projection P given by
Pr = P
⎡
⎣
x
y
z
⎤
⎦=
⎡
⎣
0
0
z
⎤
⎦= z ˆk.
and its orthogonal complementary projection I −P given by
(I −P)r = (I −P)
⎡
⎣
x
y
z
⎤
⎦=
⎡
⎣
x
y
0
⎤
⎦= x ˆı + y ˆj.
Instead of expressing (I −P)r in terms of ˆı and ˆj, we can find orthogonal basis vectors
more appropriate in cylindrical coordinates. As before, it helps to use polar coordinates in
the z = 0 plane:
(I −P)r = x ˆı + y ˆj = (r cos θ)ˆı + (r sin θ) ˆj = r(cos θ ˆı + sin θ ˆj),
so it makes sense to define
ˆer ≜cos θ ˆı + sin θ ˆj,
which is a unit vector in the direction of the planar vector x ˆı + y ˆj. Note that the vector ˆer
varies as θ varies.
It makes sense to define a vector
ˆez ≜ˆk.

468
Advanced Engineering Mathematics
z
x
y
r



r
ez
er
eθ
(x,y,z)
(x,y,0)
FIGURE 6.6
Right-handed basis {ˆer, ˆeθ, ˆez} in cylindrical coordinates.
Note that
r = r ˆer + z ˆez.
So far, in cylindrical coordinates, we have a set of two o.n. vectors, {ˆer, ˆez}. To get an
o.n. basis for R3, we need a third unit vector, ˆw, that is orthogonal to both ˆez.ˆer. Even
better, we would like {ˆer, ˆw, ˆez} to give a right-handed o.n. basis, that is, that
ˆer × ˆw = ˆez,
ˆw × ˆez = ˆer,
ˆez × ˆr = ˆw.
The latter gives us an easy way to find
ˆw ≜ˆez × ˆr = ˆk × (cos θ ˆı + sin θ ˆj) = −sin θ ˆı + cos θ ˆj,
using Theorem 6.1(d) and (e) in Section 6.1. We denote
ˆeθ ≜−sin θ ˆı + cos θ ˆj.
(6.13)
Figure 6.6 shows the right-handed o.n. basis at an example of a point in R3.
{ˆer, ˆeθ, ˆez} =
⎧
⎨
⎩
⎡
⎣
cos θ
sin θ
0
⎤
⎦,
⎡
⎣
−sin θ
cos θ
0
⎤
⎦,
⎡
⎣
0
0
1
⎤
⎦
⎫
⎬
⎭
(6.14)
at an example of a point in R3.
6.2.4 Orthonormal Basis in Spherical Coordinates
In ways analogous to what we did for cylindrical coordinates, we can also find a right-
handed o.n. basis in spherical coordinates (ρ, φ, θ). One natural vector in that basis is the

Geometry, Calculus, and Other Tools
469
unit vector in the direction of the position vector, that is,
ˆeρ ≜sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk.
(6.15)
Note that
r = ρ ˆeρ.
Analogously to what we did in cylindrical coordinates, we define again
ˆeθ ≜−sin θ ˆı + cos θ ˆj.
(6.16)
A third vector ˆeφ can be chosen so that {ˆeρ, ˆeφ, ˆeθ} is a right-handed o.n. basis for R3: Let
ˆeφ ≜ˆeθ × ˆeρ = · · · = cos φ cos θ ˆı + cos φ sin θ ˆj −sin φ ˆk.
(6.17)
Figure 6.7 shows the right-handed o.n. basis at an example of a point in R3.
{ˆeρ, ˆeφ, ˆeθ} =
⎧
⎨
⎩
⎡
⎣
sin φ cos θ
sin φ sin θ
cos φ
⎤
⎦,
⎡
⎣
cos φ cos θ
cos φ sin θ
−sin φ
⎤
⎦,
⎡
⎣
−sin θ
cos θ
0
⎤
⎦
⎫
⎬
⎭.
(6.18)
We can express the vectors in the o.n. basis in spherical coordinates in terms of the
vectors in the o.n. basis in cylindrical coordinates, specifically
ˆeρ = sin φ ˆer + cos φ ˆez,
ˆeφ = cos φ ˆer −sin φ ˆez,
and ˆeθ is the same in the two bases.
z
x
y
r



r
eρ
ef
eθ
(x,y,z)
(x,y,0)
FIGURE 6.7
Right-handed basis {ˆeρ, ˆeφ, ˆeθ} in spherical coordinates.

470
Advanced Engineering Mathematics
6.2.5 Relationships to the Standard o.n. Basis
Recall from Corollary 2.6 in Section 2.4 that if {q1, q2, q3} is an o.n. basis for R3 and x is in
R3, then
x = ⟨x, q1⟩q1 + ⟨x, q2⟩q2 + ⟨x, q3⟩q3.
Concerning the relationship between the o.n. bases {ˆı, ˆj, ˆk} and {ˆer, ˆeθ, ˆez}, we use
Corollary 2.6 in Section 2.4 to summarize the information of (6.14) in Table 6.1.
Symmetry of the dot product, that is, x • y = y • x, and Corollary 2.6 in Section 2.4,
imply that
ˆı = (ˆı • ˆer)ˆer + (ˆı • ˆeθ)ˆeθ + (ˆı • ˆez)ˆez = cos θ ˆer −sin θ ˆeθ,
ˆj = ( ˆj • ˆer)ˆer + ( ˆj • ˆeθ)ˆeθ + ( ˆj • ˆez)ˆez = sin θ ˆer + cos θ ˆeθ,
and
ˆk = (ˆk • ˆer)ˆer + (ˆk • ˆeθ)ˆeθ + (ˆk • ˆez)ˆez = ˆez.
Similarly, concerning the relationship between the o.n. bases {ˆı, ˆj, ˆk} and {ˆeρ, ˆeφ, ˆeθ}, we
use Corollary 2.6 in Section 2.4 to summarize the information of (6.18) in Table 6.2.
Symmetry of the dot product and Corollary 2.6 in Section 2.4, imply that
ˆı = (ˆı • ˆeρ)ˆeρ + (ˆı • ˆeφ)ˆeφ + (ˆı • ˆeθ)ˆeθ = sin φ cos θ ˆeρ + cos φ cos θ ˆeφ −sin θ ˆeθ,
ˆj = ( ˆj • ˆeρ)ˆeρ + ( ˆj • ˆeφ)ˆeφ + ( ˆj • ˆez)ˆeθ = sin φ sin θ ˆeρ + cos φ sin θ ˆeφ + cos θ ˆeθ,
and
ˆk = (ˆk • ˆeρ)ˆeρ + (ˆk • ˆeφ)ˆeφ + (ˆk • ˆeθ)ˆeθ = cos φ ˆeρ −sin φ ˆeφ.
TABLE 6.1
Entries Are Values of x • y
x \ y
ˆı
ˆj
ˆk
ˆer
cos θ
sin θ
0
ˆeθ
−sin θ
cos θ
0
ˆez
0
0
1
TABLE 6.2
Entries Are Values of x • y
x \ y
ˆı
ˆj
ˆk
ˆeρ
sin φ cos θ
sin φ sin θ
cos φ
ˆeφ
cos φ cos θ
cos φ sin θ
−sin φ
ˆeθ
−sin θ
cos θ
0

Geometry, Calculus, and Other Tools
471
6.2.6 Problems
1. Find the exact polar coordinates of the point (x, y) = (
√
6, −
√
2).
2. Find the exact polar coordinates of the point (x, y) = (−1, −1).
3. What are the exact (r, θ, z) coordinates of the points (a) (x, y, z) = (−
√
3, 1, 5) and
(b) (x, y, z) = (−1,
√
3, 4)?
4. What are the exact (ρ, φ, θ) coordinates of the point (x, y, z) = ( 3
2, −
√
3
2 , −
√
3)?
5. What are the exact (ρ, φ, θ) coordinates of the point (a) (x, y, z) = (−
√
2
2 , −
√
6
2 , −
√
2),
(b) (x, y, z) = (−
√
6
2 ,
√
2
2 , −
√
2)?
6. What are the exact (ρ, φ, θ) coordinates of the point (x, y, z) = (
√
3
2 , −3
2, −
√
3)?
7. Find ˆer, ˆeθ, and ˆez in terms of ˆeρ, ˆeφ, and ˆeθ.
6.3 Curves and Surfaces
In calculus, the first curves we study are graphs, for example, described by y = f(x), and
likewise the first surfaces we study are graphs, for example, described by z = f(x, y). We
also study curves and surfaces that are parametrized.
A curve C is parametrized by t if the curve’s points, P, have position vectors, −→
OP = r =
r(t), where the function r(t) is defined and continuous on an interval of t values, for example,
a ≤t ≤b. If the parametrization gives motion along the curve, then the parameter, t,
measures time. In any case, an arrow on the curve indicates the direction of “travel” as t
increases.
Example 6.8
C : r = r0 + tv, 0 ≤t ≤1,
(6.19)
is a straight line segment from r0 to r0 + v, as shown in Figure 6.8.
z
x
r0
r0+ v
v
y
FIGURE 6.8
Line segment from r0 to r0 + v in Example 6.8.

472
Advanced Engineering Mathematics
In particular, if v = r1 −r0, then the line goes from r0 to r1. The line in Example 6.8 lies in
the plane z = 0 only if both v • ˆk = r0 • ˆk = 0.
Example 6.9
C : r = a cos t ˆı + a sin t ˆj, 0 ≤t ≤2π,
(6.20)
shown in Figure 6.9, is a circle of radius |a| > 0 and center at the origin in the z = 0
plane.
Usually we will take a > 0. The direction of travel on the circle in Example 6.9 is counter-
clockwise in the xy-plane, even if a < 0.
Example 6.10
Parametrize the ellipse shown in Figure 6.10.
Method: Recall that the standard form of an equation for an ellipse, whose center is at
the origin, is
x2
a2 + y2
b2 = 1,
(6.21)
where a, b are positive constants. But having a 1 on the right-hand side and the sum of
squares on the left-hand side remind us of the Pythagorean identity in trigonometry,
cos2 θ + sin2 θ ≡1. Looking at (6.21) suggests rewriting the Pythagorean identity as
a2 cos2 θ
a2
+ b2 sin2 θ
b2
= 1.
This suggests parametrizing the ellipse using x = a cos t, y = b sin t, that is, writing
C : r(t) = a cos t ˆı + b sin t ˆj, 0 ≤t ≤2π. ⃝
(6.22)
x
z
y
FIGURE 6.9
Circle of radius a in the plane z = 0 in Example 6.9.

Geometry, Calculus, and Other Tools
473
y
b
–a
a
x
–b
FIGURE 6.10
Ellipse in the plane z = 0 in Example 6.10.
Example 6.11
In calculus, we learned about curves written in the form r = f(θ). This is really just
shorthand for the parametrization
C : r(θ) = f(θ) · (cos θ ˆı + sin θ ˆj), α ≤θ ≤β.
(6.23)
Example 6.12
Parametrize the circle x2 + y2 = 4x shown in Figure 6.11.
Method: In polar coordinates, the equation of the circle is
r2 = 4r cos θ.
(6.24)
For those points on the circle that have r ̸= 0, we can divide (6.24) by r to get
r = 4 cos θ.
(6.25)
By Example 6.11, let’s try the parametrization
r(θ) = 4 cos θ · (cos θ ˆı + sin θ ˆj).
y
x
2
4
–2
FIGURE 6.11
Example 6.12.

474
Advanced Engineering Mathematics
Are we done? No, not yet. The first issue is, “Does this parametrization give the correct
direction of travel on the curve?” The answer is, “Yes,” as we can see from r(0) = 4ˆı and
r( π
2 ) = 0. Also, we need to find an interval of θ values that will give the whole curve.
While the interval 0 ≤θ ≤2π gives the circle twice, the interval 0 ≤θ ≤π gives the
circle once. To summarize, our curve can be parametrized by
C : r(θ) = 4 cos θ · (cos θ ˆı + sin θ ˆj), 0 ≤θ ≤π. ⃝
(6.26)
Using the identities cos2 θ = 1
2 (1 + cos 2θ) and sin θ cos θ = 1
2 sin 2θ, we can rewrite
(6.26) as C : r(θ) = (2 + 2 cos 2θ) ˆı + 2 sin 2θ ˆj, 0 ≤θ ≤π. This explains why 0 ≤θ ≤π
works so well to give the circle once.
One thing in Example 6.12 that may bother us is that in deriving the equation in polar
coordinates, r = 4 cos θ, we divided by r and said that was o. k. for r ̸= 0, but our circle
passes through the origin. This illustrates the principle that in the process of finding a
parametrization, or, more generally, the solving of any mathematical problem, we may
perform a dubious step as long as the solution we arrive at can be verified to satisfy all of
the requirements of the problem. In other words, a dubious step is allowed if it does not
ultimately produce a spurious solution or an incorrect “solution” and does not lose any
solution.
Example 6.12 naturally leads to the next concepts.
Definition 6.2
A curve C : r = r(t), α ≤t ≤β, is
(a) Closed if r(β) = r(α)
(b) Simple if r(t1) ̸= r(t2) for all t1, t2 with α ≤t1 < t2 ≤β except possibly r(α) =
r(β).
The curves of Example 6.8, parametrized by (6.19); Example 6.9, parametrized by (6.20);
Example 6.10, parametrized by (6.22); and Example 6.11, parametrized by (6.26), are all
simple. The latter three are also closed.
A “figure eight” curve, for example, the one shown in Figure 6.12, is not simple, no
matter how it is parametrized. Figure 6.12 was drawn using the MathematicaTM command:
ParametricPlot[{2Cos[t], Sin[2t]}, {t, 0, 2Pi},PlotStyle →Thick].
6.3.1 Curves and Calculus
Suppose a point P0 is on a simple curve
C : r = r(t), α ≤t ≤β.
Suppose at some t0 in the interval α < t0 < β, the position vector is −−→
OP0 = r(t0). As shown
in Figure 6.13, the tangent vector to the curve C at P0 is given by
T ≜˙r(t0),
(6.27)

Geometry, Calculus, and Other Tools
475
–2
–1
–0.5
0.5
1.0
y
x
1
2
–1.0
FIGURE 6.12
Figure eight.
P0
r(t0)
FIGURE 6.13
Tangent vector to curve at a point.
assuming the derivative exists at t0 and is not the zero vector. If
r(t) = x(t) ˆı + y(t) ˆj + z(t) ˆk,
then
˙r(t) = ˙x(t) ˆı + ˙y(t) ˆj + ˙z(t) ˆk,
assuming all three scalar function derivatives exist.
Linear approximation of the three scalar functions x(t), y(t), and z(t) implies linear
approximation of points on the curve:
r(t0 + △t) ≈r(t0) + (△t)T .
If t measures time, then the tangent vector shows the instantaneous direction (and speed)
of travel along the curve.
If the curve is simple and ˙r(t0) = 0, then there is no tangent vector to the curve at the
point whose position vector is r(t0). If t measures time, then instantaneously the motion
stops at t = t0 if ˙r(t0) = 0.
By definition T ̸= 0, so the unit tangent vector to C at P0 exists and is
T =
1
||T|| T .

476
Advanced Engineering Mathematics
A special case of a parametrized curve is given by the solution of a system of ODEs
˙x = F(x): if the system in R3 has a solution
x(t) =
⎡
⎢⎢⎣
x(t)
y(t)
z(t)
⎤
⎥⎥⎦,
or a system in R2 has a solution
x(t) =
x(t)
y(t)

,
then at a point x(t0) the solution curve has tangent vector F (x(t0)) at that point. The
collection of all tangent vectors at their respective points forms a vector field.
Example 6.13
Consider the ODE system in the plane
˙x ≜
˙x
˙y

=

y
−ω2x

.
(6.28)
Recalling results from Section 3.3 and Section 5.3, you will explain in Problem 6.3.4.30
why the solutions of (6.28) can be written in the form
x(t) =
 A cos(ωt −δ)
−ωA sin(ωt −δ)

,
(6.29)
where A, ω, δ are scalar constants and A, ω are positive. The vector field
F(x) =

y
−ω2x

is shown in Figure 6.14a. We can see that the tangent vectors suggest that the solu-
tions, r(t) = x(t) ˆı + ˙x(t) ˆj, travel clockwise around the origin, perhaps along circles,
ellipses, spirals, or some similar-looking curves. We know the solutions, as shown in
Figure 6.14b, are ellipses because
(x(t))2
A2
+ (˙x(t))2
(ωA)2 =
x(t)
A
2
+
 ˙x(t)
ωA
2
= cos2(ωt −δ) + sin2(ωt −δ) ≡1.
The solutions are shown superimposed on the vector field in Figure 6.14c.
Example 6.14
Suppose a solution of ODE system (6.28) passes through the points (x, ˙x) = (0, 4) and
(−
√
2, 0), for some constant ω. Find the frequency of vibration, ω.
Method: The second data point tells us that for some δ, “time” t0, and positive A, ω,
we get (−
√
2, 0) = (A cos(ωt0 −δ), −ωA sin (ωt0 −δ)). Let θ ≜ωt0 −δ, so this becomes
−
√
2 = A cos θ and 0 = A sin θ. It follows that A =
√
2 and θ = (2k+1)π for any integer k.

Geometry, Calculus, and Other Tools
477
(a)
(b)
(c)
–4
–2
–4
–2
2
4
2
4
y
x
FIGURE 6.14
Solutions of undamped oscillator ODE system are ellipses, for Example 6.13. (a) Oscillator vector field, (b)
oscillator solution curves, and (c) oscillator vector field and solutions.
Now, substitute the value of A into the form of the solution and the first data point
to get (0, 4) = (
√
2 cos(ωt1 −δ), −ω
√
2 sin (ωt1 −δ)), for some δ, “time” t1, and positive
ω. Let ϕ = ωt1 −δ, so this becomes 0 =
√
2 cos ϕ and 4 = −ω
√
2 sin ϕ. It follows that
4 = ω
√
2 and ϕ =

n −1
2

π for some integer n. The frequency of vibration is ω =
4
√
2 =
2
√
2. ⃝
In our work in Example 6.14 to find ω, we didn’t actually need to find θ and ϕ, but
finding their values was part of the method.
6.3.2 Zhukovskii Airfoil
One interesting class of curves, called Zhukovskii airfoils, gives shapes that look like the
cross sections of airplane wings. For example, define
x(ξ, η) ≜ξ · 1 + ξ2 + η2
2(ξ2 + η2) ,
y(ξ, η) ≜η · −1 + ξ2 + η2
2(ξ2 + η2) ,
(6.30)
where
ξ(t) ≜−0.07 + ρ0 cos(t),
η(t) ≜0.05 + ρ0 sin(t),
and
ρ0 =

(1 + 0.07)2 + (0.05)2.
The curve plotted by Mathematica is shown in Figure 6.15a. This curve is simple, closed,
and piecewise smooth, even though it has a cusp at the point (x, y) = (1, 0). A similar-
looking closed, but not simple, curve that is not a Zhukovskii airfoil is shown in
Figure 6.15b.

478
Advanced Engineering Mathematics
(a)
(b)
–1.0
–0.5
0.5
1.0 x
0.10
y
–1.0
–0.5
0.5
1.0 x
0.1
0.2
0.3
–0.1
y
FIGURE 6.15
(a) A Zhukovskii airfoil. (b) Not a Zhukovskii airfoil.
6.3.3 Surfaces
A parametrized surface is given by
S : r = r(u, v), (u, v) in some planar region D.
The independent variables u, v are called the parameters in the parametrization. A special
case is when the surface is a graph of a function, for example, z = f(x, y), which we will
see can be parametrized using x, y as the parameters.
Example 6.15
Find a parametrization of the surface S that is the part of the paraboloid z = 3 −x2 −y2
that lies on or above the xy-plane.
Method: The paraboloid is shown in Figure 6.16. The paraboloid intersects the xy-plane
where z = 0; hence 0 = 3 −x2 −y2, that is, the circle x2 + y2 = 3. To be on the
part of the paraboloid that lies above the z = 0 plane, that is, satisfies z > 0, we need
3
1
0
x
–1
2
1
z
0
–1
0
y
1
FIGURE 6.16
Paraboloidal surface in Example 6.15.

Geometry, Calculus, and Other Tools
479
x2 + y2 = 3 −z < 3, that is, x2 + y2 < 3. So, choose the planar region:
D = {(x, y) : x2 + y2 ≤3}.
This suggests using x, y as the parameters. We have
S : r = x ˆı + y ˆj + (3 −x2 −y2) ˆk, (x, y) in D. ⃝
Example 6.16
Use polar coordinates to find a different parametrization of the surface in Example 6.15.
Method:
Because D is the disk of radius
√
3 and center at the origin in the xy-plane, it
makes sense to use polar coordinates: Let
D = {(r, θ) : 0 ≤r ≤
√
3, 0 ≤θ ≤2π},
x = r cos θ, y = r sin θ. So, another parametrization for S is
S : r = r cos θ ˆı + r sin θ ˆj + (3 −r2) ˆk, (r, θ) in D. ⃝
Here are some famous surfaces expressed in cylindrical coordinates:
Sphere : r2 + z2 = a2, some positive constant a.
Cylinder : r = a, some positive constant a.
Cone : z = a + b r, some constants a, b.
Paraboloid : z = a + b r2, some constants a, b.
Example 6.17
Find the curve(s) of intersection of the sphere x2 + y2 + z2 = 5 and the cone z =
7 −4

x2 + y2.
Method: These two surfaces are r2 + z2 = 5 and z = 7 −4r. They intersect when 5 =
r2 + z2 = r2 + (7 −4r)2 = 17r2 −56r + 49, that is,
0 = 17r2 −56r + 44.
The quadratic formula gives
r± = 56 ±

562 −4 · 17 · 44
2 · 17
= 56 ±
√
144
34
=
 2,
if +
22
17,
if −

.
Because both r± are less than
√
5, neither is a spurious solution. So, there are two circles
of intersection points:
x2 + y2 = 4, z = −1 and
x2 + y2 =
22
17
2
, z = 7 −4 · 22
17 = 31
17. ⃝

480
Advanced Engineering Mathematics
5
0
2
–2
–2
0
2
2
0
0
z
x
5
2
–2
0
y
0
x
z
y
–2
(b)
(a)
FIGURE 6.17
Intersection of sphere and cone.
Two views of the intersection of the sphere and the cone are given in Figure 6.17.
Example 6.18
A sphere of radius a and center at the origin can be described by the equation ρ = a.
Find two different parametrizations of the part of the sphere that lies on or above the
xy-plane.
Method:
(a) Because the problem involves a sphere, it makes sense to try to work in spherical
coordinates, given in (6.12) in Section 6.2:
x = ρ sin φ cos θ,
y = ρ sin φ sin θ,
z = ρ cos φ.
On the sphere, ρ ≡a, so
r = a sin φ cos θ ˆı + a sin φ sin θ ˆj + a cos φ ˆk.
Our parameters are φ, θ. Are we done? Not yet: A parametrization consists of a
formula for the position vector and the description of the domain D in which those
parameters lie.
The part of the Earth that lies on or above the xy-plane consists of the northern
hemisphere and the equator. In (x, y, z) coordinates, we need z ≥0. Because z =
a cos φ, that requires cos φ ≥0, that is, 0 ≤φ ≤π
2 . The sphere has circular symmetry
about the z-axis, so 0 ≤θ ≤2π. To summarize, the northern hemisphere and the
equator can be parametrized by
S : r = a sin φ cos θ ˆı + a sin φ sin θ ˆj + a cos φ ˆk,
for (φ, θ) in D =

(φ, θ) : 0 ≤θ ≤2π, 0 ≤φ ≤π
2

.

Geometry, Calculus, and Other Tools
481
(b) Cylindrical coordinates are also relatively convenient for working with spheres. Our
sphere is r2 + z2 = a2; hence, z = ±

a2 −r2. As in part (a), we require z ≥0. So, the
part of the sphere that lies on or above the xy-plane is the graph of z = +

a2 −r2.
This gives a value of z only for r ≤a. In fact, our part of the sphere lies above the
disk 0 ≤r ≤a in the xy-plane.
Similar to our work in Example 6.16, which also involved a surface that was a graph,
we get a parametrization
S : r = r cos θ ˆı + r sin θ ˆj +

a2 −r2 ˆk,
for (r, θ) in D = {(r, θ) : 0 ≤θ ≤2π, 0 ≤r ≤a}. ⃝
Here are some famous surfaces expressed in spherical coordinates:
Sphere : ρ = a, some positive constant a
Sphere : ρ = 2a cos φ, some positive constant a
The plane z = 0 : φ = π
2
Halfplane : θ = α, some constant α
Double cusped horn : ρ = 2a sin φ, some positive constant a
Example 6.19
Explain why ρ = 2a cos φ, where a is positive constant, gives a sphere. Find its radius
and center.
Method: Multiply both sides of the defining equation by ρ to get x2 + y2 + z2 = ρ2 =
2a(ρ cos φ) = 2a z. Completing the square gives
0 = x2 + y2 + z2 −2a z = x2 + y2 + (z2 −2a z + a2 −a2) = x2 + y2 + (z2 −a)2 −a2,
that is,
x2 + y2 + (z2 −a)2 = a2.
This is an equation of a sphere of radius a and center at (x, y, z) = (0, 0, a). ⃝
The equation of a cusped horn looks like that for a sphere, but having sin φ instead
of cos φ makes a big difference! Multiply both sides of the defining equation by ρ to get
r2 + z2 = ρ2 = 2a(ρ sin φ) = 2a r. Completing the square gives z2 = a2 −(r −a)2, that is,
z = ±

a2 −(r −a)2.
An example is drawn in Figure 6.18.

482
Advanced Engineering Mathematics
2
–2
–1
0
1
2
1
0
–1
–2
y
x
0
z
–2
FIGURE 6.18
Double cusped horn.
Definition 6.3
A surface S : r = r(u, v), (u, v) in some planar region D, is simple if
r(u1, v1) ̸= r(u2, v2)
when (u1, v1) and (u2, v2) are different points in D, except possibly for a curve of points
in S.
For example, the side of a cylinder is a simple surface.
6.3.4 Problems
For problems 1–3, find a parametrization of the curve and sketch it.
1. The line from (1, 2, 4) to (−1, 0, 3).
2. The circle x2 + y2 = −6y.
3. The circle 2x2 + 2y2 = 16y.
In problems 4–10, sketch the curve and put an arrow on it to indicate the direction of
travel.
4. C : r = t2 ˆı + t4 ˆk, 1 ≤t ≤2.
5. C : r = 6 cos 2t ˆı + 2 sin 2t ˆk, 0 ≤t ≤π.

Geometry, Calculus, and Other Tools
483
6. C : r = 2 cos 2t ˆı + sin t ˆk, 0 ≤t ≤π
2 , using technology, for example, Mathematica
or a graphing calculator.
7. r = sin 3θ.
8. r = 1 −cos θ.
9. r = 2 −sin θ.
10. The intersection of the surfaces r = 4 and 2x + z = 9.
For problems 11–13, find a parametrization of the curve and state whether your
parametrization gives a simple and/or closed curve.
11. The ellipse x2
9 + y2
4 = 1.
12. Ellipse x2
9 + y2 = 4.
13. Ellipse x2 + 9y2 = 12x.
14. What, if anything, is the difference(s) between the curves given by (6.20) and the
curve C2 : r = a cos(2π −t)ˆı + a sin(2π −t) ˆj, 0 ≤t ≤2π?
15. What, if anything, is the difference(s) between the curves given by (6.26) and the
curve C2 : r(θ) = 4 cos θ · (cos θ ˆı + sin θ ˆj), 0 ≤θ ≤2π?
16. A fly’s position vector is r(t) = −cos 2t ˆı + sin 2t ˆj + t ˆk.
(a) Sketch the flight path of the fly, and (b) find the unit tangent vector as a
function of time, t. (c) Describe in words the flight path of the fly.
17. Find parametric equations of the tangent line to the planar curve parametrized by
r(t) = sin t ˆı + ( 6
π t −1) ˆj at the point (
√
3
2 , 1).
18. Find parametric equations of the tangent line at the point (2, 1, 5) on the curve
found by intersecting the surface z = x2y −2x + 5 with the plane y = 1.
19. Find the exact points(s) on the planar curve parametrized by r(t) = sin t ˆı + (t +
2 cos t) ˆj at which the tangent line is parallel to the y-axis.
For problems 20–24, describe in words the surface in R3, say what kind of geometrical
object it is, and sketch it.
20. x2 + z2 = 4.
21. ρ = 4 cos φ.
22. φ = π
12.
23. r = a cos θ, where a is an unspecified constant.
24. z = 5 −3r.
For problems 25–29, find two different parametrizations of each surface and describe what
kind of geometrical object it is.
25. x2 + y2 + z2 = −z.
26. x2 + y2 + z2 = 4x.
27. x2 + y2 = z.
28.
√
3 (x2 + y2) = z.

484
Advanced Engineering Mathematics
(a)
(b)
2
y
1
–1
–1
1
2
x
–2
–2
4
6
y
2
–2
–4
5
x
–5
–6
FIGURE 6.19
Figures useful for (a) Problem 6.3.4.31 and (b) Problem 6.3.4.32.
29.

4x2 + 2y2 −2z = 0.
30. Explain why the solutions of ODE system (6.28) can be written in the form (6.29).
31. Suppose that for some positive constant ω, a solution of the ODE system
˙x ≜
˙x
˙y

=

y
−ω2x

passes through the points (2, 0) (0, −
√
3), as shown in Figure 6.19a. Find the exact
value of ω, if possible.
32. [Small project] Suppose that for some positive constants α, ω, a solution of the
damped oscillator ODE system
˙x ≜
˙x
˙y

=

y
−ω2x −2αy

passes through the points (x, ˙x) = (8, 0), (0, −4
√
2), and (2, 0), as shown in
Figure 6.19b. Find the values of α, ω exactly, if possible, or with at least six
significant digits of accuracy. [Caution: This problem is much more subtle and
complicated than Problem 6.3.4.31.]
33. Use cylindrical coordinates to find a parametrization of the surface r = f(θ, z),
where θ and z vary over all possible physical points on the infinite cylinder r = 1.
34. Use spherical coordinates to find a parametrization of the surface ρ = f(φ, θ),
where φ and θ vary over all possible physical points on the unit sphere.
35. Sketch the solid V = {(ρ, φ, θ) : 0 ≤θ ≤2π, 0 ≤φ ≤π
2 , 0 ≤ρ ≤2 cos φ} and describe
it in words. [Hint: To begin, multiply both sides of ρ = 2 cos φ by ρ to get
x2 + y2 + z2 = ρ2 = 2ρ cos φ.]

Geometry, Calculus, and Other Tools
485
6.4 Partial Derivatives
If f = f(x, y), then its partial derivatives are defined by
∂
∂x

f
 
= ∂f
∂x(x, y) ≜lim
△x→0
f(x+△x, y) −f(x, y)
△x
,
(6.31)
assuming the limit exists, and
∂
∂y

f
 
= ∂f
∂y(x, y) ≜lim
△y→0
f(x, y+△y) −f(x, y)
△y
,
(6.32)
assuming the limit exists.
When taking the partial derivative with respect to x, treat y as if it were constant, for
example,
∂
∂x
!
y sin 2x −y2 "
= y ∂
∂x [ sin 2x ] −∂
∂x
!
y2 "
= y 2 cos 2x −0 = 2y cos 2x.
Similarly, when taking the partial derivative with respect to y, treat x as if it were constant,
for example, the chain rule in Calculus I implies
∂
∂y

sin xy
 
= cos xy · ∂
∂y

xy
 
= (cos xy) · x = x cos xy.
Higher order derivatives are defined by
∂2f
∂x2 ≜∂
∂x
 ∂f
∂x

,
∂2f
∂x∂y ≜∂
∂x
 ∂f
∂y

,
∂2f
∂y∂x ≜∂
∂y
 ∂f
∂x

,
∂2f
∂y2 ≜∂
∂y
 ∂f
∂y

,
assuming the limits exist individually.
Theorem 6.2
(Clairaut’s theorem) If
∂2f
∂x∂y(x, y) and
∂2f
∂y∂x(x, y) both exist and are continuous at (x0, y0),
then they are equal, that is,
∂2f
∂x∂y(x0, y0) = ∂2f
∂y∂x(x0, y0).

486
Advanced Engineering Mathematics
6.4.1 Linear Approximation
If f = f(x, y), then the generalization of linear approximation is
f(x0 + △x, y0 + △y) ≈f(x0, y0) + △x ∂f
∂x(x0, y0) + △y ∂f
∂y(x0, y0) ≜f(x0, y0) + L(△x, △y).
(6.33)
Example 6.20
As we will derive in Section 10.4, the speed of sound is approximately
vs =
#
γ P
ϱ ,
where
P is the atmospheric pressure
ϱ is the atmospheric gas mass density
γ is the adiabatic gas parameter in the relationship PVγ = constant.
For example, for γ = 1.4 and “standard” atmospheric conditions at sea level, that is,
temperature = 273.15 K, P = P0 = (1 atmosphere) = 101, 323 N/m2, and mass density ϱ =
ϱ0 = 1.293 kg/m3, the speed of sound is approximately vs,0 = 331.22 m/s.
Predict the approximate effect of a 2% decrease in the atmospheric pressure and a 3%
increase in the mass density.
Method: Consider vs to be a function of (P, ϱ), that is,
vs(P, ϱ) =
#
γ P
ϱ = γ 1/2P1/2ϱ−1/2.
With △P = −0.02P and △ϱ = +0.03ϱ, linear approximation at (P, ϱ) = (P0, ϱ0) gives
vs ≈vs,0 + △P ∂vs
∂P (P0, ϱ0) + △ϱ ∂vs
∂ϱ (P0, ϱ0)
= vs,0 + (−0.02P0)γ 1/2 · 1
2 P−1/2
0
ϱ−1/2
0
+ (0.03ϱ0)γ 1/2P1/2
0
·

−1
2

ϱ−3/2
0
,
so
vs ≈vs,0 −0.02 · 1
2 · γ 1/2 · P1/2
0
ϱ−1/2
0
+ 0.03 ·

−1
2

γ 1/2 · P1/2
0
ϱ−1/2
0
= vs,0 −0.01 · vs,0 −0.015 · vs,0 = 0.975vs,0 ≈322.94 m/s.
So, a 2% decrease in the atmospheric pressure and a 3% increase in the mass density have
the effect of decreasing the speed of sound by about 2.5%, thus adding the individual
effects of about 1% and about 1.5%. ⃝

Geometry, Calculus, and Other Tools
487
TABLE 6.3
Kinematic Viscosity of Steam μ/ϱ = ν(P, T), in 107 m2/s
P\T
400
450
500
550
600
650
700
2.5
29.5
34.6
40.2
46.0
52.1
58.6
65.4
5
14.5
17.0
19.8
22.8
25.9
29.2
32.7
7.5
9.3
11.2
13.1
15.1
17.2
19.4
21.8
10
6.82
8.21
9.67
11.23
12.82
14.52
16.30
12.5
5.26
6.44
7.66
8.91
10.21
11.60
13.01
15
4.21
5.26
6.28
7.36
8.47
9.62
10.85
17.5
3.44
4.40
5.33
6.25
7.23
8.23
9.27
Example 6.21
Table 6.3 gives the kinematic viscosity of steam, ν = ν(P, T), in 107 m2/s, where the
pressure, P, is measured in units of 106 Pa (pascals) and the temperature, T, is measured
in ◦C. Use estimates of partial derivatives to approximate ν(16, 590).
Method: Linear approximation, with △P = 1 and △T = −10, gives
ν(16, 590) ≈ν(15, 600) + (1) ∂ν
∂P(15, 600) + (−10) ∂ν
∂T (15, 600).
Tu use this, we need to estimate the two partial derivatives at (P, T) = (15, 600). The
simplest estimates are
∂ν
∂P(15, 600) ≈ν(17.5, 600) −ν(15, 600)
17.5 −15
= 7.23 −8.47
2.5
≈−0.496
and
∂ν
∂T (15, 600) ≈ν(15, 600) −ν(15, 550)
600 −550
= 8.47 −7.36
50
≈0.0222.
So,
ν(16, 590) ≈8.47 + (1)(−0.496) + (−10)(0.0222) ≈7.752. ⃝
Using the dot product in R2, we can write part of (6.33) as
L(△x, △y) ≜△x ∂f
∂x(x0, y0) + △y ∂f
∂y(x0, y0) =
⎡
⎢⎣
∂f
∂x(x0, y0)
∂f
∂y(x0, y0)
⎤
⎥⎦•
⎡
⎣
△x
△y
⎤
⎦.
This motivates defining the gradient vector:
grad f = ∇f ≜∂f
∂x ˆı + ∂f
∂y ˆj.
Using this definition of grad f,
L(△x, △y) = ∇f(x0, y0) • (△x ˆı+△y ˆj).

488
Advanced Engineering Mathematics
1
1
1
1
0
0
0
–1
–1
–1
–1
y
y
x
x
z
z
3
3
2
2
1
1
0
0
0
(a)
(b)
FIGURE 6.20
Partial derivatives with respect to (a) x and (b) y.
Because of the linear approximation formula (6.33), we define
(Duf)(x0, y0) ≜∇f(x0, y0) • u.
(6.34)
This is called the directional derivative of f in the direction of the unit vector u.
Assuming ∇f(x0, y0) ̸= 0, the direction in which f increases the most is the unit vector
u in the direction of ∇f(x0, y0), because v • u = ||v|| ||u|| cos θ is greatest when u is in the
direction of v.
Here are geometric interpretations of the partial derivatives and the directional deriva-
tive: Consider a surface z = f(x, y). Intersecting the surface with the plane y = y0 gives the
curve z = f(x, y0) as x varies, and ∂f
∂x(x0, y0) is the slope of the tangent line to that curve at
the point (x, y, z) =
$
x0, y0, f(x0, y0)
%
. This tangent line lies in the plane y = y0. The curve
and the tangent line are shown in Figure 6.20a.
Similarly, intersecting the same surface z = f(x, y) with the plane x = x0 gives the curve
z = f(x0, y) as y varies, and ∂f
∂y(x0, y0) is the slope of the tangent line to that curve at the
point (x, y, z) =
$
x0, y0, f(x0, y0)
%
. This tangent line lies in the plane x = x0. The curve and
the tangent line are shown in Figure 6.20b.
Finally, if u = a ˆı + b ˆj is a unit vector, consider the parametric curve
C : r(t) = (x0 + at)ˆı + (y0 + bt) ˆj + f(x0 + at, y0 + bt)ˆk, −δ < t < δ.
It also lies on the surface z = f(x, y). Using the linear approximation in (6.33) we get
f(x0 + at, y0 + bt) ≈f(x0, y0) + at ∂f
∂x(x0, y0) + bt ∂f
∂y(x0, y0).
So, the tangent vector to C at the point (x0, y0, f(x0, y0)) is
dr
dt = a ˆı + b ˆj +

a ∂f
∂x(x0, y0) + b ∂f
∂y(x0, y0)

ˆk = u + (Duf)(x0, y0).

Geometry, Calculus, and Other Tools
489
3
1
0
x
–1
2
z
1
0
–1
0
1
y
FIGURE 6.21
Directional derivative.
In some sense, (Duf)(x0, y0) is the “slope” of the tangent vector to this curve, at the point
(x0, y0, f(x0, y0)), because the individual partial derivatives ∂f
∂x(x0, y0) and ∂f
∂y(x0, y0) are spe-
cial cases of (Duf)(x0, y0) for b = 0 and a = 0, respectively. The geometric interpretation of
the directional derivative is depicted in Figure 6.21.
6.4.2 Multivariable Chain Rules
Recall that if f = f(x) and x = x(t), then the single-variable chain rule says that
d
dt

f(x(t))
 
= df
dx(x(t)) · dx
dt (t) =
 df
dx(x(t))

˙x(t),
where the ˙ signifies the derivative with respect to t. In a straightforward way, this
generalizes to the multivariable chain rule
d
dt
!
f
$
x(t), y(t)
% "
= ∂f
∂x
$
x(t), y(t)
%
· dx
dt (t) + ∂f
∂y
$
x(t), y(t)
%
· dy
dt (t),
that is,
d
dt

f
$
x(t), y(t)
%  
=
 ∂f
∂y
$
x(t), y(t)
%
˙x(t) +
 ∂f
∂y
$
x(t), y(t)
%
˙y(t) .
(6.35)
This follows from the calculation
d
dt

f
$
x(t), y(t)
%  
=
= lim
△t→0
f
$
x(t + △t), y(t + △t)
%
−f
$
x(t), y(t + △t)
%
+ f
$
x(t), y(t + △t)
%
−f
$
x(t), y(t)
%
△t
= lim
△t→0
&
f
$
x(t) + △x, y(t) + △y
%
−f
$
x(t), y(t) + △y
%
△t
+ f
$
x(t), y(t) + △y
%
−f
$
x(t), y(t)
%
△t
'
,

490
Advanced Engineering Mathematics
where △x≜x(t + △t) −x(t), △y ≜y(t + △t) −y(t). So,
d
dt

f
$
x(t), y(t)
%  
= lim
△t→0
f
$
x(t) + △x, y(t) + △y
%
−f
$
x(t), y(t) + △y
%
△x
· △x
△t
+ lim
△t→0
f
$
x(t), y(t) + △y
%
−f
$
x(t), y(t)
%
△y
· △y
△t .
Implicitly we’re assuming that x(t), y(t) are differentiable, so as △t →0, so do △x →0 and
△y →0. So,
d
dt

f
$
x(t), y(t)
%  
= lim
△x→0
△y→0
f
$
x(t) + △x, y(t) + △y
%
−f
$
x(t), y(t) + △y
%
△x
· lim
△t→0
△x
△t
+ lim
△y→0
f
$
x(t), y(t) + △y
%
−f
$
x(t), y(t)
%
△y
· lim
△t→0
△y
△t
=
 ∂f
∂y
$
x(t), y(t)
%
˙x(t) +
 ∂f
∂y
$
x(t), y(t)
%
˙y(t),
that is, (6.35) is true.
The aforementioned calculations are similar to those we did to explain (3.16) in Section
3.2, which is actually a special case of (6.35).
The multivariable chain rule (6.35) can be rewritten as
d
dt

f
$
x(t), y(t)
% 
= ∇f
$
x(t), y(t)
%
• ˙r(t),
(6.36)
where r(t) = x(t)ˆı + y(t) ˆj.
Suppose, instead, that x, y are functions not of a single independent variable but instead
two independent variables, u, v, that is,
x = x(u, v),
y = y(u, v).
We have multivariable chain rules
∂
∂u

f
$
x(u, v), y(u, v)
% 
= ∇f
$
x(u, v), y(u, v)
%
• ∂r
∂u = ∂f
∂x · ∂x
∂u + ∂f
∂y · ∂y
∂u
(6.37)
and
∂
∂v
$
x(u, v), y(u, v)
% 
= ∇f
$
x(u, v), y(u, v)
%
• ∂r
∂v = ∂f
∂x · ∂x
∂v + ∂f
∂y · ∂y
∂v.
(6.38)

Geometry, Calculus, and Other Tools
491
Example 6.22
Suppose f = f(x, y) and x, y are expressed in polar coordinates. Find
∂f
∂r, ∂f
∂θ , and ∂2f
∂θ2 .
Method: With r, θ playing the roles of u, v, we get
∂f
∂r = ∂
∂r

f(x, y)
 
= ∂
∂r

f(r cos θ, r sin θ)
 
= ∂f
∂x · ∂x
∂r + ∂f
∂y · ∂y
∂r = ∂f
∂x · ∂
∂r [ r cos θ ] + ∂f
∂y · ∂
∂r [ r sin θ ]
= cos θ ∂f
∂x + sin θ ∂f
∂y.
Similarly,
∂f
∂θ = ∂
∂θ
!
f(r cos θ, r sin θ)
"
= ∂f
∂x · ∂
∂θ [ r cos θ ] + ∂f
∂y · ∂
∂θ [ r sin θ ] ;
hence,
∂f
∂θ = −r sin θ ∂f
∂x + r cos θ ∂f
∂y.
(6.39)
Finally,
∂2f
∂θ2 = ∂
∂θ
 ∂f
∂θ (r cos θ, r sin θ)

= ∂
∂θ

−r sin θ ∂f
∂x + r cos θ ∂f
∂y

.
Use the same product rule as for functions of a single variable, along with the
multivariable chain rule (6.37), to get
∂2f
∂θ2 = −r cos θ ∂f
∂x −r sin θ ·
&
∂2f
∂x2 · ∂
∂θ [ r cos θ ] + ∂2f
∂y∂x · ∂
∂θ [ r sin θ ]
'
−r sin θ ∂f
∂y + r cos θ ·
&
∂2f
∂x∂y · ∂
∂θ [ r cos θ ] + ∂2f
∂2y · ∂
∂θ [ r sin θ ]
'
= −r cos θ ∂f
∂x + r2 sin2 θ ∂2f
∂x2 −r2 sin θ cos θ ∂2f
∂y∂x
−r sin θ ∂f
∂y −r2 sin θ cos θ ∂2f
∂x∂y + r2 cos2 θ ∂2f
∂2y.
If Clairaut’s theorem, that is, Theorem 6.2, applies, then this simplifies to
∂2f
∂θ2 = −r cos θ ∂f
∂x −r sin θ ∂f
∂y + r2 sin2 θ ∂2f
∂x2 −2r2 sin θ cos θ ∂2f
∂y∂x + r2 cos2 θ ∂2f
∂2y. ⃝

492
Advanced Engineering Mathematics
6.4.3 Gradient Vector in R3
If F = F(x, y, z) then the gradient vector is defined by
grad F = ∇F ≜∂F
∂x ˆı + ∂F
∂y ˆj + ∂F
∂z
ˆk.
Linear approximation is given by
F(x0 + △x, y0 + △y, z0 + △z)
≈F(x0, y0, z0) + △x ∂F
∂x(x0, y0, z0) + △y ∂F
∂y(x0, y0, z0) + △z ∂F
∂z (x0, y0, z0),
that is,
F(x0 + △x, y0 + △y, z0 + △z) ≈F(x0, y0, z0) + L(△x, △y, △z),
where
L(△x, △y, △z) = ∇F(x0, y0, z0) • (△x ˆı + △y ˆj + △z ˆk).
Similarly, the directional derivative is defined by
(DuF)(x0, y0, z0) ≜∇F(x0, y0, z0) • u,
for any unit vector u in R3, and the multivariable chain rules are given by
dF
dt = ∂F
∂x · dx
dt + ∂F
∂y · dy
dt + ∂F
∂z · dz
dt = ∇F|at r(t) • dr
dt
(6.40)
and, for example,
∂F
∂u = ∂F
∂x · ∂x
∂u + ∂F
∂y · ∂y
∂u + ∂F
∂z · ∂z
∂u = ∇F|at r(u,v) • ∂r
∂u .
(6.41)
If x, y, z are functions of three variables u, v, w, then, for example,
∂F
∂w = ∂F
∂x · ∂x
∂w + ∂F
∂y · ∂y
∂w + ∂F
∂z · ∂z
∂w = ∇F|at r(u,v,w) • ∂r
∂w,
(6.42)
where r = x ˆı + y ˆj + z ˆk.

Geometry, Calculus, and Other Tools
493
6.4.4 Scalar Potential Functions
Definition 6.4
Given a vector field F(x, y, z) = Fx(x, y, z)ˆı + Fy(x, y, z) ˆj + Fz(x, y, z)ˆk, it can be of physical
importance to find a scalar potential function f = f(x, y, z) such that
F = ∇f,
if it is possible to do so. If there is such a function f, we say that F is exact.
If there is a potential function, then
Fx ˆı + Fy ˆj + Fz ˆk = ∇f = ∂f
∂x ˆı + ∂f
∂y ˆj + ∂f
∂z
ˆk,
so
Fx = ∂f
∂x,
Fy = ∂f
∂y,
Fz = ∂f
∂z.
If, in addition, all of the first partial derivatives Fx(x, y, z), Fx(x, y, z), and Fx(x, y, z) are
defined and continuous, then Clairaut’s theorem (Theorem 6.2) implies that
∂Fy
∂x ≜∂2f
∂x∂y = ∂2f
∂y∂x ≜∂Fx
∂y .
This and similar calculations give the exactness criterion for a vector field: If F is exact then
∂Fy
∂x −∂Fx
∂y = 0,
∂Fz
∂y −∂Fy
∂z = 0,
and
∂Fx
∂z −∂Fz
∂x = 0.
(6.43)
If F is a planar vector field with no dependence on z and, also, Fz ≡0, then the exactness
criterion reduces to
∂Fy
∂x (x, y) −∂Fx
∂y (x, y) = 0.
(6.44)
If we call M = M(x, y) = Fx(x, y) and N = N(x, y) = Fy(x, y), we see that the exactness
criterion (6.44) is the same as the exactness criterion (3.17) in Section 3.2 for a first-order
ODE! In Section 3.2, we saw how to use anti-partial differentiation to calculate a potential
function φ(x, y) used to solve an exact first-order ODE. The same method works in this
section to find a scalar potential function in R3.

494
Advanced Engineering Mathematics
Example 6.23
If possible, find a potential function for the vector field
F = F(r) = (z + y cos(xy))ˆı + (2 −y + x cos(xy)) ˆj + (x + z)ˆk.
Method: First, for safety, let’s check the exactness criterion—if it fails, we shouldn’t try
to produce a nonexistent potential function. Using the single-variable product and chain
rules, we calculate
∂Fy
∂x −∂Fx
∂y = ∂
∂x

2 −y + x cos(xy)
 
−∂
∂y

z + y cos(xy)
 
= (0 −0 + 1 · cos(xy) + x · (−sin(xy) · y))
−(0 + 1 · cos(xy) + y · (−sin(xy) · x)) = 0;
∂Fz
∂y −∂Fy
∂z = ∂
∂y [ x + z ] −∂
∂z

2 −y + x cos(xy)
 
= (0 −0 + 0) −(0 −0 + 0) = 0;
and
∂Fx
∂z −∂Fz
∂x = ∂
∂z

z + y cos(xy)
 
−∂
∂x [ x + z ] = (1 + 0) −(1 + 0) = 0.
So, there should exist a scalar potential function.
Because we need to have ∂f
∂x = Fx,
f =
 ∂f
∂x ∂x =

Fx ∂x =

(z + y cos(xy))∂x = xz +

y cos(w)∂x,
where we substitute w = xy. Because ∂w
∂x = y, we get
f = xz +

cos(w)∂w = xz + sin(w) + g(y, z) = xz + sin(xy) + g(y, z);
(6.45)
g(y, z) can be an arbitrary function of only y, z, because ∂
∂x

g(y, z)
 
≡0.
Substitute (6.45) into Fy = ∂f
∂y to get
2 −y +
x cos(xy) = Fy = ∂f
∂y = ∂
∂y

xz + sin(xy) + g(y, z)
 
= 0 +
x cos(xy) + ∂g
∂y;
hence, we need
2 −y = ∂g
∂y.
It follows that
g(y, z) =
 ∂g
∂y ∂y =

(2 −y) ∂y = 2y −y2
2 + h(z),
where h(z) can be an arbitrary function of only z. Substitute this into (6.45) to get
f = xz + sin(xy) + 2y −y2
2 + h(z).
(6.46)

Geometry, Calculus, and Other Tools
495
Substitute (6.46) into Fz = ∂f
∂z to get
x + z = Fz = ∂f
∂z = ∂
∂z

xz + sin(xy) + 2y −y2
2 + h(z)

= x + 0 + 0 −0 + dh
dz ;
hence z = h′(z). Because h = h(z) is a function of z alone, its partial and ordinary
derivatives with respect to z are equal.
So, h(z) = z2
2 + c, where c is an arbitrary constant. To summarize,
f = xz + sin(xy) + 2y −y2
2 + z2
2 + c
(6.47)
is a scalar potential function for the given vector field, for any constant c. ⃝
Theorem 6.3
(Existence of a potential function) Suppose a vector field F = Fx ˆı + Fy ˆj + Fz ˆk satisfies the
exactness criterion (6.43) for all (x, y, z) in some open region V in R3. If, in addition, Fx, Fy,
and Fz are all continuous in V, then F is exact in V, that is, there is a function f such that
F(x, y, z) ≡∇f(x, y, z) for all (x, y, z) in V.
6.4.5 Problems
1. If f(x, y) = sin(xy2 + 3), find
∂2f
∂x∂y.
2. If f(x, y) = xey2 + cos(2x −3y + π), find
∂2f
∂x∂y.
3. The elliptic cone z2 = 4x2 + y2 intersects the plane y = −2 in a hyperbola.
Find parametric equations of the tangent line to this hyperbola at the point
(−1, −2, 2
√
2).
4. Table 6.4 gives the rated horsepower capacity, H, of a roller chain as a function
of the sprocket speed, ω, in revolutions per minute, and the ANSI chain number
N. Use the table to estimate the rated horsepower capacity for a sprocket speed of
175 revolutions per minute and ANSI chain number 100, if such a chain existed.
Be clear about which values from the table you used, how you found approximate
partial derivative values, and how you used them.
5. Find the directional derivative of f(x, y) = e−x2+y at the point (1, 2) in the direction
of the vector −ˆı + 2 ˆj.
6. The temperature at a point (x, y, z) is given by T(x, y, z) = 5 −
6
√
x2+4y2+9z2 , in ◦C.
(a) In what unit vector direction is the temperature increasing the most at the point
TABLE 6.4
Horsepower Data
ω\N
80
120
156
150
7.75
25.1
56.3
200
10.0
32.5
72.9
300
14.5
46.8
105

496
Advanced Engineering Mathematics
P = (−1, −2, 2)? (b) What is the maximum value of the directional derivative there,
assuming x, y, z are measured in meters?
7. Define f(x, y, z) =
1
√
x2+y2+z2 . Find (a) ∇f(x, y, z) and (b) the directional derivative
of f in the radial direction.
8. Consider the surface z = 3 −x2 −y2. (a) Find an equation, or the parametric
equations, for the curve obtained by fixing y = 0.8 and letting x vary. (b) Use your
work for part (a) to find the parametric equations of the tangent line to that curve
at the point (x, y, z) = (1, 0.8, 1.36). (c) Use the pictures associated with parts (a)
and (b) to give the pictorial interpretation of the partial derivative with respect to
x of the function f(x, y) = 3 −x2 −y2 at the point (x, y) = (1, 0.8).
9. Consider the surface z = 3 −x2 −y2. (a) Find equations for, or the parametric
equations for, the curve obtained by fixing x = 1 and letting y vary. (b) Use your
work for part (a) to find the parametric equations of the tangent line to that curve
at the point (x, y, z) = (1, 0.8, 1.36). (c) Use the pictures associated with parts (a)
and (b) to give the pictorial interpretation of the partial derivative with respect to
y of the function f(x, y) = 3 −x2 −y2 at the point (x, y) = (1, 0.8).
10. Consider the surface z = 3 −x2 −y2. (a) Find an equation, or the parametric equa-
tions, for the curve obtained by letting x = 1 + at and y = 0.8 + bt as t varies.
(b) Use your work for part (a) to find the parametric equations of the tangent line
to that curve at the point (x, y, z) = (1, 0.8, 1.36). (c) Use the pictures associated
with parts (a) and (b) to give the pictorial interpretation of the directional deriva-
tive of the function f(x, y) = 3 −x2 −y2 at the point (x, y) = (1, 0.8) in the direction
of the vector u = 0.6 ˆı + 0.8 ˆj.
11. If x = −3 + 3t, y = 4 −t2, and z = f(x, y) = x2y, evaluate dz
dt (2).
12. If
x = x(t),
y = y(t),
and
f = f(x, y),
find
d
dt

(f(x(t), y(t))2 
in
terms
of
f, ∂f
∂x , ∂f
∂y , dx
dt , dy
dt and t.
13. If x = ue−v, y = veu, f(x, y) = x
y −x2y and g(u, v) = f(x(u, v), y(u, v)), evaluate
∂g
∂u(−2, 1) and ∂g
∂v(−2, 1).
14. Suppose r(t) is of constant magnitude. [Physically, this corresponds to motion on
a sphere whose center is at the origin.] Why is r(t) • ˙r(t) ≡0? What does this say
about the tangent vector to a point on a circle?
15. If z = f(x, y), x = r cos θ, and y = r sin θ, explain why
∂z
∂x
2
+
∂z
∂y
2
=
∂z
∂r
2
+
1
r · ∂z
∂θ
2
is an identity, at least where everything exists.
16. If z = f(x, y), x = u2 + v2, and y = u2 −v2, explain why
∂z
∂x
2
−
∂z
∂y
2
=
1
4uv
 ∂z
∂u
 ∂z
∂v

is an identity, at least where everything exists.

Geometry, Calculus, and Other Tools
497
17. Use linear approximation to estimate ln
$
(1.02)2 + (2.99)3%
.
18. Determine a scalar potential function for the vector field
F ≜(3x2y + y2z)ˆı + (x3 + 2xyz + z) ˆj + (xy2 + y + z)ˆk.
You may assume that the given vector field is exact.
19. Determine a scalar potential function for the vector field
F≜(yz −2x)ˆı+ (xz + cos z) ˆj+ (xy −y sin z)ˆk.
You may assume that the given vector field is exact.
20. Let F ≜(1 −y −z) ˆı −x ˆj −(x + e−2z)ˆk. Explain why F is exact and find a potential
function for it.
21. Determine a scalar potential function for the vector field
F ≜cos y ˆı + (−x sin y + z) ˆj + (y −1)ˆk.
22. Explain why (6.37) is true, using a derivation similar to the derivation of (6.35).
23. Let ρ = ||r|| and v = ˙r. Fill in the blanks in the following with integers to make the
equation an identity. Explain how you arrived at your conclusions and write the
identity.
d
dt
!
ρ−3r
"
= ρ−5 
r × (v × r) +
ρ2v

.
[Hint: You may use the result about vector triple products that will be mentioned
in Problem 6.8.4.18.]
24. In this problem, you will establish Leibniz’s rule:
d
dt
 t
a
f(t, s) ds

= f(t, t) +
t
a
∂f
∂t (t, s) ds,
assuming a is a constant. How? Begin by using the definition of ordinary
differentiation,
d
dt
 t
a
f(t, s) ds

≜lim
△t→0
1
△t
&t+△t

a
f(t+ △t, s) ds −
t
a
f(t, s) ds
'
.
After that, inside the big parentheses, subtract and add
 t+△t
a
f(t, s) ds.

498
Advanced Engineering Mathematics
6.5 Tangent Plane and Normal Vector
Suppose a point (x0, y0, f(x0, y0)) is on the surface z = f(x, y). At that point, the tangent
plane to the surface consists of all of the points (x, y, z) for which the change
△z ≜z −f(x0, y0)
agrees with the linear approximation of change in f corresponding to simultaneous
changes △x = x −x0 and △y = y −y0. Denote z0 = f(x0, y0). Rewriting (6.33) in Section 6.4
gives
z −z0 = △z = L(△x, △y) =
 ∂f
∂x(x0, y0)

△x +
 ∂f
∂y(x0, y0)

△y
=
 ∂f
∂x(x0, y0)

(x −x0) +
 ∂f
∂y(x0, y0)

(y −y0).
So, the plane is described by the equation:
−
 ∂f
∂x(x0, y0)

(x −x0) −
 ∂f
∂y(x0, y0)

(y −y0) + (z −z0) = 0.
(6.48)
Example 6.24
Find an equation of the tangent plane to the surface z = x2 + 3y2 at the point (2, −1, 7),
and find a normal vector for that plane.
Method: z = f(x, y) ≜x2 + 3y2, so
∂f
∂x(x0, y0) = 2x0 = 2 · 2 = 4,
∂f
∂y(x0, y0) = 6y0 = 6 · (−1) = −6.
An equation of the tangent plane is given by −4(x −2) −(−6)(y −(−1)) + z −7 = 0,
that is,
−4(x −2) + 6(y + 1) + z −7 = 0.
Because a plane A(x −x0) + B(y −y0) + C(z −z0) = 0 has a normal vector n = A ˆı +
B ˆj + C ˆk, in this problem n = −4 ˆı + 6 ˆj + ˆk is a normal vector to the plane. ⃝
Suppose a point P0 = (x0, y0, z0) has position vector r0 on a parametrized surface:
S : r = r(u, v) = x(u, v) ˆı + y(u, v) ˆj + z(u, v) ˆk, (u, v) in D.
Assuming the surface is simple, as we defined at the end of Section 6.3, there is a single
choice of (u0, v0) in D for which r0 = r(u0, v0). We can define two parametrized curves by
C : r = r(u, v0), u in interval Iu,

Geometry, Calculus, and Other Tools
499
where Iu is chosen small enough that (u, v0) is in D for all u in Iu, and
C : r = r(u0, v), v in interval Iv,
where Iv is chosen small enough that (u0, v) is in D for all v in Iv.
At r0 there are correspondingly two tangent vectors Tu, Tv defined by
Tu ≜∂r
∂u(u0, v0),
Tv ≜∂r
∂v(u0, v0).
If Tu ̸= 0, Tv ̸= 0, and {Tu, Tv} is linearly independent, then the vector
n = Tu × Tv = ∂r
∂u × ∂r
∂v,
evaluated at r0, is a nonzero vector normal to the plane:
r0 + span{Tu, Tv}.
(6.49)
If either Tu = 0 or Tv = 0, or more generally {Tu, Tv} is linearly dependent, then the set
given by (6.49) is a point or a line, not a plane.
In the special case that S is a surface z = f(x, y), that is,
S : r = r(x, y) = x ˆı + y ˆj + f(x, y) ˆk, (x, y) in D,
then at a point (x0, y0, f(x0, y0)), we get
Tx ≜∂r
∂x(x0, y0) = ˆı + ∂f
∂x(x0, y0) ˆk,
Ty ≜∂r
∂x(x0, y0) = ˆj + ∂f
∂y(x0, y0) ˆk;
hence
n = Tx ×Ty =

ˆı + ∂f
∂x(x0, y0) ˆk

×

ˆj + ∂f
∂y(x0, y0) ˆk

= −∂f
∂x(x0, y0) ˆı −∂f
∂y(x0, y0) ˆj + ˆk,
which agrees with (6.48). This is very good! One way to understand a general result is by
seeing what it means in a special case and, of course, seeing that in the special case the
general result agrees with a previously known result.
Besides the special case of a surface z = f(x, y) and the general case of a parametrized
surface S : r = r(u, v), there is the case of a level set, that is, S = {(x, y, z) : F(x, y, z) = k},
where k is a constant. On a level set S, at a point whose position vector is r0,
n = ∇F|r0
is a normal vector, as you will establish in Problem 6.5.1.10.

500
Advanced Engineering Mathematics
Example 6.25
Find an equation of the tangent plane to the sphere ρ = a at the point P0 whose position
vector is r0 and find a unit vector normal to that plane.
Method: On the sphere,
r = a sin φ cos θ ˆı + a sin φ sin θ ˆj + a cos φ ˆk = aˆeρ,
so
∂r
∂φ = a cos φ cos θ ˆı + a cos φ sin θ ˆj −a sin φ ˆk = a ˆeφ,
as in (6.17) in Section 6.2, and
∂r
∂θ = −a sin φ sin θ ˆı + a sin φ cos θ ˆj = a sin φ ˆeθ,
as in (6.16) in Section 6.2.
It follows that a normal vector to the tangent plane is given by
n= ∂r
∂φ × ∂r
∂θ =(aˆeφ) ×(a sin φ ˆeθ) = a2 sin φ ˆeφ × ˆeθ = a2 sin φ ˆeρ,
unless φ0 = 0 or φ0 = π, that is, unless the point is on the sphere’s North Pole or South
Pole.
By the way, Problem 6.4.5.14 gives an elegant way to see why the radial vector is a
normal vector at a point on a sphere.
It follows that for 0 ̸= φ0 ̸= π, the unit normal vector is
n =
1
||n|| n = ˆeρ(φ0, θ0).
(6.50)
This agrees with our intuition that at points on a sphere, a radial vector is normal to the
surface. In fact, (6.50) is also correct for φ0 = 0 and φ0 = π, that is, if the point is on the
North or South Pole.
In (6.50) we saw that the radial vector depends upon the position.
An equation of the tangent plane at a point on a sphere is
0 = n • (r −r0) = ˆeρ(φ0, θ0) • (r −a ˆeρ) = ˆeρ(φ0, θ0) • r −aˆeρ(φ0, θ0) • ˆeρ(φ0, θ0).
But,
ˆeρ(φ0, θ0) = 1
a · a ˆeρ(φ0, θ0) = 1
a r0,
so an equation of the tangent plane is
0 = (ˆeρ(φ0, θ0) • r) −a =
1
a r0 • r

−a,
that is, 1
a r0 • r = a, that is,
r0 • r = x0x + y0y + z0z = a2. ⃝
(6.51)
As a quick reality check, is the point whose position vector is r0 on the tangent plane
given by (6.51)? It should be! In fact, r = r0 = x0 ˆı + y0 ˆj + z0 ˆk does satisfy
a2 = r0 • r = x0x + y0y + z0z = x0x0 + y0y0 + z0z0 = x2
0 + y2
0 + z2
0.

Geometry, Calculus, and Other Tools
501
Example 6.26
Find an equation of the tangent plane at the point (−15
√
17,
15
√
17, 2) on a power plant’s
cooling tower that is part of the hyperboloid of one sheet
S : r = r(t, s) = 5 cosh t cos s ˆı + 3 cosh t sin s ˆj + 2 sinh t ˆk, −∞< t < ∞, 0 ≤s ≤2π,
also known as
x2
52 + y2
32 −z2
22 = 1.
Method: We calculate∗
n = ∂r
∂t × ∂r
∂s = (5 sinh t cos s ˆı + 3 sinh t sin s ˆj + 2 cosh t ˆk)
× (−5 cosh t sin s ˆı + 3 cosh t cos s ˆj)
= −6 cosh2 t cos s ˆı −10 cosh2 t sin s ˆj + 15 cosh t sinh t ˆk.
To simplify the algebra, divide through by cosh t, because it is never zero. That gives
a normal vector
n1 = −6 cosh t cos s ˆı −10 cosh t sin s ˆj + 15 sinh t ˆk.
(6.52)
In order to proceed, we must find the parameter values (t0, s0) for which
−15
√
17
ˆı + 15
√
17
ˆj + 2ˆk = r(t0, s0) = 5 cosh t0 cos s0 ˆı + 3 cosh t0 sin s0 ˆj + 2 sinh t0 ˆk,
that is,
−15
√
17
= 5 cosh t0 cos s0,
15
√
17
= 3 cosh t0 sin s0,
and
2 = 2 sinh t0.
The latter implies† t0 = arcsinh(1) = ln(1 +
√
2).
Because of the identity cosh2 t −sinh2 t ≡1 and the fact that sinh t0 = 1,
cosh t0 =

1 + sinh2 t0 =

1 + 12 =
√
2.
Because −15
√
17 = 5 cosh t0 cos s0, we get −15
√
17 = 5
√
2 cos s0; hence,
cos s0 = −
3
√
34
.
Similarly,
15
√
17 = 3 cosh t0 sin s0 implies
sin s0 =
5
√
34
.
∗Note that cosh t ≜1
2 (et + e−t) and sinh t ≜1
2(et −e−t) satisfy cosh2 t −sinh2 t = 1, d
dt [cosh t] = sinh t, and
d
dt [sinh t] = cosh t.
† 1 = sinh t0 = 1
2(et0 −e−t0) ⇐⇒2 = et0 −e−t0 ⇐⇒2et0 = (et0)2 −1 ⇐⇒0 = (et0)2 −2et0 −1 ⇐⇒0 <
et0 = 2+
√
8
2
= 1 +
√
2.

502
Advanced Engineering Mathematics
Because (cos s0, sin s0) is in the second quadrant, it follows that π
2 < s0 < π and thus
s0 = cos−1(−
3
√
34).
To find a vector n that is normal to the surface at the point, we actually only need to
know that cosh t0 =
√
2, sinh t0 = 1, cos s0 = −
3
√
34, sin s0 =
5
√
34, so (6.52) gives
n1 = · · · =
18
√
17
ˆı −50
√
17
ˆj + 15 ˆk.
(6.53)
At this point, we might as well multiply the unit normal vector by
√
17 to get a simpler-
looking normal vector, n = 18 ˆı −50 ˆj + 15
√
17 ˆk.
An equation of the tangent plane at the point P0 is
18

x + 15
√
17

−50

y −15
√
17

+ 15
√
17 (z −2) = 0,
or
18x −50y + 15
√
17 z = 30
√
17. ⃝
We saw that the normal vector to the tangent plane of a surface z = f(x, y) points in the
direction of greatest increase of the function f. This has a natural interpretation in terms of
level curves or, more generally, level sets,
{(x, y) : k = f(x, y)},
or k = f(x, y) for short, where k is a scalar constant. The level sets are also called “contours”
in a contour plot. At a point (x0, y0) on a level set k = f(x, y), the gradient vector
∇f(x0, y0) = ∂f
∂x(x0, y0) ˆı + ∂f
∂y(x0, y0) ˆj
points in the direction of greatest increase of f and hence is normal to the level set at the point
(x0, y0), as shown in Figure 6.22.
A nice example of this is the subject of isobars, that is, “lines” of constant atmospheric
pressure on a map. Not only does the normal to an isobar give the direction of greatest
increase or decrease of atmospheric pressure, the closeness of successive isobars indicates
y
x
(x0, y0)
FIGURE 6.22
Normal to a level curve.

Geometry, Calculus, and Other Tools
503
P
Q
FIGURE 6.23
Isobars.
strength of the winds. For example, we expect stronger winds at point P than at point Q
for the isobars shown in the fictional map in Figure 6.23.
As to the direction of the wind, it would be normal to the isobars if the Earth were
not rotating. But the Earth’s rotation and the resulting “Coriolis acceleration” actually
cause the wind direction to be almost parallel to the isobars, at least at altitudes above
1000 m, where ground effects are insignificant! This is called geostrophic wind and will
be explored in Problem 6.8.4.6.
Similarly, a set of points (x, y, z) that satisfy an equation of the form k = F(x, y, z), for
some scalar constant k, might be a part of a surface. Suppose P0 = (x0, y0, z0) is in that set,
that is, k = F(x0, y0, z0). By linear approximation,
F(x, y, z) ≈F(x0, y0, z0) + (x −x0)∂F
∂x(x0, y0, z0) + (y −y0)∂F
∂y(x0, y0, z0)
+ (z −z0)∂F
∂z (x0, y0, z0),
so the direction of greatest increase of F is
∇F(x0, y0, z0) = ∂F
∂x(x0, y0, z0) ˆı + ∂F
∂y(x0, y0, z0) ˆj + ∂F
∂z (x0, y0, z0) ˆk,
as long as ∇F(x0, y0, z0) ̸= 0.
Example 6.27
(Example 6.26 again) Find a vector normal to the plane tangent to the surface
x2
52 + y2
32 −z2
22 = 1.
at the point (−15
√
17,
15
√
17, 2).

504
Advanced Engineering Mathematics
Method: Define f(x, y, z) ≜
x2
52 + y2
32 −z2
22 , so P0 ≜(−15
√
17,
15
√
17, 2) is on the level set
1 = f(x, y, z). We calculate
∇f = 2x
52 ˆı + 2y
32 ˆj −2z
22 ˆk.
So
n = ∇f(−15
√
17
, 15
√
17
, 2) = 2
25 ·

−15
√
17

ˆı + 2
9 ·
 15
√
17

ˆj −2
4 · 2ˆk
= −
6
5
√
17
ˆı +
10
3
√
17
ˆj −ˆk,
which agrees with (6.53) except for a factor of −1. ⃝
6.5.1 Problems
In problems 1–4, find an equation of the tangent plane to the surface at the given point.
1. 3x2 −y2 + xz = 7 at (1, 2, 8)
2. z = exy −2ex + e2y at (2, 1, 0)
3. z = x2 −3y2 + xy at (1, −1, −3)
4. r = uˆı + v3 ˆj + ue−uv2 ˆk at the point (x, y, z) = (−1, −8, −e4)
5. Find an upward-pointing unit vector that is perpendicular to the plane containing
the vectors ˆı + ˆj −2ˆk and 3ˆı −2 ˆj + ˆk.
6. A helicoid is parametrized by r(u, v) = u cos v ˆı + u sin v ˆj + v ˆk. Find an equation
of the tangent plane to the helicoid at the point (x, y, z) = (0, π, π
2 ).
7. If a surface is given by z = g(x, y) near a point P0 = (x0, y0, z0), define f(x, y, z) ≜
z −g(x, y). (a) Use ∇f to find a vector normal to the surface at P0. (b) For the
particular situation of Example 6.26, does this method produce a normal vector
that agrees with (6.53)?
8. By expressing x2
52 + y2
32 −z2
22 = 1 as a surface in the form z = f(x, y) near the point
P0 = (−15
√
17,
15
√
17, 2), find yet another way to find a vector normal to the tangent
plane to the surface there.
9. Two surfaces φ(x, y, z) = 0 and ψ(x, y, z) = 0 are defined to be orthogonal if at
every point at which they intersect their normal vectors are orthogonal. Are the
surfaces x2 −y2 + z2 = 4 and z =
1
xy2 orthogonal? [Time-saving hint: Do not begin
by trying to find all of their points of intersection.]
10. Suppose a point has position vector r0 on a level set S = {(x, y, z) : F(x, y, z) = k},
where k is a constant. Explain why a nonzero vector n = ∇F|r0 is a normal vector
by considering all curves C : r = r(t) that lie on S and using the multivariable
chain rule (6.40) in Section 6.4.
In problems 11–13, sketch the level curves of the function and find the normal vector to a
point on several of the level curves.
11. f(x, y) = 2x2 + y2

Geometry, Calculus, and Other Tools
505
1
1
2
x
y
k= 10
k = 5
2
FIGURE 6.24
Level curves of f = f(x, y).
12. f(x, y) =
2y
x2+y2 ,
13. f(x, y) = x2+y2
4x
14. Sketched in Figure 6.24 are parts of two level curves for a function f(x, y), for
levels k = 5 and k = 10. (a) Find the unit vector in which f is increasing the
most at the point (x, y) = (2, 1), and (b) use that unit vector to find a point (x, y)
where f(x, y) ≈6. Be accurate and check that your conclusion would make sense
in the picture.
15. Define f(x, y) = x2+2y2. (a) At the point (x, y) = (2, 1), in what unit vector direction
is f increasing the most? (b) For what value of k does the level curve f(x, y) = k
pass through the point (2, 1)? Draw that level curve and the unit normal vector to
it at that point.
6.6 Area, Volume, and Linear Transformations
Recall Theorem 6.1(b) in Section 6.1, that is,
||A × B|| = ||A|| ||B|| sin θ,
(6.54)
where θ is the angle between the vectors A and B with 0 ≤θ ≤π. The parallelogram
determined by A and B has
Area = (base) (height) = ||A|| (||B|| sin θ) = ||A × B||,
as shown in Figure 6.25.
Example 6.28
Find the area of the triangle with vertices (4, 0, 0), (0, 2, 0), (0, 0, 4
3).
Method:
Denote by P, Q, R the three given points. The area of the triangle shown
in Figure 6.26 is one-half of the area of the parallelogram determined by vectors

506
Advanced Engineering Mathematics
B
A
θ
||B|| sin θ
FIGURE 6.25
Parallelogram.
0
0.0
0.0
0.5
1.0
1.5
2.0
0.5
z
x
R
P
Q
1.0
1
2
3
4
y
FIGURE 6.26
Triangle with vertices (4, 0, 0), (0, 2, 0), (0, 0, 4
3 ).
A ≜−→
PQ = −4 ˆı + 2 ˆj and B ≜−→
PR = −4 ˆı + 4
3 ˆk. Because ||A|| =

42 + 22 =
√
20,
||B|| =
(
42 +

4
3
2
=
√
160
3
, and
cos θ =
A • B
||A|| ||B|| =
16
√
20 ·
√
160
3
=
6
5
√
2
implies
sin θ =

1 −cos2 θ =
√
7
5 ,
so the area is
Area = 1
2 base × height = 1
2 ·
√
20 ·
√
160
3
·
√
7
5
= 4
√
14
3
. ⃝

Geometry, Calculus, and Other Tools
507
If A and B happen to lie in the xy-plane, that is,
A = a ˆı + b ˆj and B = c ˆı + d ˆj,
then
A × B = (ad −bc)ˆk.
So, in this special case,
Area = ||A × B|| = ||(ad −bc)ˆk|| = |ad −bc| =
det
a
c
b
d
 ,
and we see that the area of the parallelogram is given by the absolute value of a
determinant.
Theorem 6.4
If A, B are vectors in the xy-plane, then the parallelogram they determine has
Area =
det

A  B
  .
(6.55)
Now look at what happens in R3: first, we will study the special case where A = a ˆı + b ˆj
and B = c ˆı + d ˆj lie in the xy-plane and C is parallel to the z-axis, that is, C = w ˆk for some
scalar w. As shown in Figure 6.27, the parallelepiped determined by the vectors A, B, and
C has
Volume=(Base area) (height)=|ad −bc| |w|=

det
⎡
⎣
a
c
0
b
d
0
0
0
w
⎤
⎦

=
 det

A  B  C
  .
A
B
y
x
z
C
FIGURE 6.27
Volume of parallelepiped #1.

508
Advanced Engineering Mathematics
z
C
φ
B
A
x
y
FIGURE 6.28
Volume of parallelepiped #2.
So, at least in this special case, the volume of a parallelepiped is given by the absolute
value of a determinant.
In fact, C need not be a multiple of ˆk. Assuming again that A and B lie in the xy-plane,
the volume of the parallelepiped determined by the vectors A, B, and C is given by
Volume=(Base area) (height)=(ad −bc) (length of the projection of C on the z−axis),
(6.56)
as shown in Figure 6.28. But, A × B = (ad −bc)ˆk, and
the length of the projection of C on the z−axis = ||C|| | cos φ| = |C • ˆk|,
(6.57)
where φ is the angle from the positive z-axis to the vector C, as in spherical coordinates.
Putting together the information in (6.56) and (6.57), we see that in this special case, the
volume of the parallelepiped is given by
Volume = | (A × B) • C |.
By the way, (A × B) • C is called a scalar triple product.
Denoting
C =
⎡
⎣
u
v
w
⎤
⎦,
we observe that
det
⎡
⎣
a
c
u
b
d
v
0
0
w
⎤
⎦= (ad −bc)w,

Geometry, Calculus, and Other Tools
509
hence, the parallelepiped determined by A, B, and C has volume given by
Volume = (Base area) (height) = |(ad −bc)| |w| =

det
⎡
⎣
a
c
u
b
d
v
0
0
w
⎤
⎦

.
It follows that
Volume =
 det

A  B  C
  .
(6.58)
Finally, to be completely general, drop the assumption that A and B lie in the xy-plane.
Suppose for the moment that we can find a real, orthogonal matrix Q that rotates both A
and B into the xy-plane. Recall that we saw in Section 2.4, specifically Problems 2.4.3.12 and
2.4.3.11, that multiplication by an orthogonal matrix preserves lengths of vectors, that is,
||QA|| = ||A||, and likewise preserves angles between vectors, because (QA)•(QB) = A•B.
After rotation, the vectors QA and QB lie in the xy-plane, so the volume of the
parallelepiped determined by QA, QB, and QC is, by (6.58),
Volume =
 det

QA  QB  QC
  .
But, by Theorem 1.9 in Section 1.2, Q

A  B  C
 
=

QA  QB  QC
 
, so the volume of the
parallelepiped is given by
Volume =
 det(Q

A  B  C
 
)
 =
 det(Q) det

A  B  C
  = |det(Q)|
 det

A  B  C
  .
But all orthogonal matrices have determinant equal to ±1, so (6.58), that is, Volume =
 det

A  B  C
 , holds in the most general case, assuming we can find a real, orthogonal
matrix Q that rotates both A and B into the xy-plane.
Now we will explain why such a real, orthogonal matrix Q can be found. The fact that
we can find it, and how we find it, are interesting for their own sakes! We will consider
two cases: (i) when n ≜A × B ̸= 0 and (ii) if A × B = 0.
In the first case, define n =
1
||n|| n. We will explain why we can find a real, orthogonal
matrix Q such that Qn = ˆk: think of n as the unit normal to a point P0 on the sphere ρ = 1,
specifically n = −−→
OP0. In spherical coordinates,
n = −−→
OP0 = sin φ0 cos θ0 ˆı + sin φ0 sin θ0 ˆj + cos φ0 ˆk.
The matrix
Q0 ≜
⎡
⎢⎢⎢⎢⎣
cos θ0
sin θ0
0
−sin θ0
cos θ0
0
0
0
1
⎤
⎥⎥⎥⎥⎦

510
Advanced Engineering Mathematics
rotates vectors around the z-axis clockwise by an angle θ0. Specifically, we have
Q0 n = Q0
⎡
⎢⎢⎢⎢⎣
sin φ0 cos θ0
sin φ0 sin θ0
cos φ0
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
sin φ0
0
cos φ0
⎤
⎥⎥⎥⎥⎦
.
So, Q0 rotates n to lie in the xz-plane. To rotate Q0n to be ˆk, we can use the matrix
Q1 ≜
⎡
⎢⎢⎢⎢⎣
cos φ0
0
−sin φ0
0
1
0
sin φ0
0
cos φ0
⎤
⎥⎥⎥⎥⎦
.
So, we have arranged to get
Q1(Q0 n) = Q1
⎡
⎢⎢⎢⎢⎣
sin φ0
0
cos φ0
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
0
0
1
⎤
⎥⎥⎥⎥⎦
.
You should check that the matrix
Q ≜Q1Q0 =
⎡
⎢⎢⎢⎢⎣
cos φ0
0
−sin φ0
0
1
0
sin φ0
0
cos φ0
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
cos θ0
sin θ0
0
−sin θ0
cos θ0
0
0
0
1
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
cos φ0 cos θ0
cos φ0 sin θ0
−sin φ0
−sin θ0
cos θ0
0
sin φ0 cos θ0
sin φ0 sin θ0
cos φ0
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
ˆeT
φ
ˆeT
θ
ˆeT
ρ
⎤
⎥⎥⎥⎥⎦
is a real, orthogonal matrix. By construction, it rotates n to ˆk, as we desired. Further, by
Problem 6.6.5.14, ˆk = Qn is orthogonal to both QA and QB. Thus, QA and QB lie in the
xy-plane, as we desired.
In the second case, that is, when n ≜A × B = 0, then A and B are parallel, so it is easy to
find a real, orthogonal matrix that rotates A, and thus also B, into the x-axis and thus into
the xy-plane.

Geometry, Calculus, and Other Tools
511
Thus, we have established the next result.
Theorem 6.5
If A, B, C are vectors in the R3 then the parallelepiped they determine has volume given
by (6.58), that is,
Volume =
 det

A  B  C
  .
6.6.1 Linear Transformations
Suppose A is a real, 2 × 2 matrix
a11
a12
a21
a22

. Take a point (x, y) in the plane and multiply
its position vector,
 x
y

, on the left by the matrix A. This gives the vector
A
 x
y

=
a11
a12
a21
a22
  x
y

=
a11x + a12y
a21x + a22y

,
(6.59)
which is the position vector of a new point, (a11x + a12y, a21x + a22y), in the plane.
If S is a set of points in the plane, then
A(S) ≜{Ax : x in S}
is a set of points∗in the plane. We call A(S) the image of S under the transformation
x →Ax given by (6.59).
A real, 2 × 2 matrix A thus defines a transformation or map of the plane. In fact, (6.59)
defines a linear transformation of the plane because
A(αx1 + βx2) = αA(x1) + β(Ax2)
for all scalars α, β, and all vectors x1, x2 in R2.
Example 6.29
Determine the images of the square S = {(x, y) :
0 ≤x ≤1, 0 ≤y ≤1} under the
transformations defined by the matrices:
(a)
2
0
0
1
2

,
(b)
1
2
0
1

,
(c)
⎡
⎣
cos π
6
−sin π
6
sin π
6
cos π
6
⎤
⎦.
Also, for each of (a), (b), and (c), sketch the mapping, that is, the set S and its image A(S).
∗At this point, we are acting as if there is no difference between a point and its position vector, so that we don’t
have to use an excess of words.

512
Advanced Engineering Mathematics
Method: The unit square S ≜{(x, y) :
0 ≤x ≤1, 0 ≤y ≤1} is the parallelogram
determined by the two vectors:
e(1) =
1
0

,
e(2) =
0
1

.
To see what a matrix A does to S, it helps to see what A does to each of e(1), e(2): because
A(S) = {A(c1e(1) + c2e(2)) : 0 ≤c1 ≤1, 0 ≤c2 ≤1},
(6.60)
we see that A(S) is the parallelogram determined by the vectors Ae(1), Ae(2).
(a) Because
2
0
0
1
2
 1
0

=
2
0

= 2 e(1)
and
2
0
0
1
2
 0
1

=
0
1
2

= 1
2 e(2),
it follows that
2
0
0
1
2

(S) =
)
(x, y) : 0 ≤x ≤2, 0 ≤y ≤1
2
*
,
which is a rectangle. We can think of this rectangle as having been produced by
stretching the unit square in the x direction and compressing it in the y direction.
The mapping is shown in Figure 6.29. The images of the points B, C, D are B′, C′, D′.
(b) Because
1
2
0
1
 1
0

=
1
0

and
1
2
0
1
 0
1

=
2
1

,
1
2
0
1

(S) is the parallelogram determined by the vectors
1
0

,
2
1

. The mapping is
shown in Figure 6.30. The images of the points B, C, D are B′, C′, D′.
(c) Because
⎡
⎣
cos π
6
−sin π
6
sin π
6
cos π
6
⎤
⎦
⎡
⎣
1
0
⎤
⎦=
⎡
⎣
cos π
6
sin π
6
⎤
⎦
and
⎡
⎣
cos π
6
−sin π
6
sin π
6
cos π
6
⎤
⎦
⎡
⎣
0
1
⎤
⎦=
⎡
⎣
−sin π
6
cos π
6
⎤
⎦,
y
y
B
C
D
B΄
C΄
D΄ x
x
1
1
2—
1
2
FIGURE 6.29
Example 6.29(a).

Geometry, Calculus, and Other Tools
513
1
y
y
B
C
D
O
O
B΄
C΄
D΄
x
x
1
1
2
1
3
FIGURE 6.30
Example 6.29(b).
y
y
B
C
D
O
O
B΄
C΄
D΄
x
x
1
1
π
6—
√3
1
2
– —
FIGURE 6.31
Example 6.29(c).
⎡
⎣
cos π
6
−sin π
6
sin π
6
cos π
6
⎤
⎦(S)
is the square obtained by the counterclockwise rotation of the unit square by an
angle of π
6 . The mapping is shown in Figure 6.31. The images of the points B, C, D
are B′, C′, D′. ⃝
Two by two matrices of the form
k
0
0
h

,
where h, k are positive constants, define stretching and/or compressing transformations.
Matrices of the form
1
0
h
1

or
1
k
0
1

define shear transformations. Matrices of the form
cos θ
−sin θ
sin θ
cos θ

define rotation transformations. Matrices of the form
−1
0
0
1

,
1
0
0
−1

,
or
0
1
1
0

define reflection transformations.

514
Advanced Engineering Mathematics
6.6.2 Linear Transformations, Area, and Volume
Because of the formula for area given in (6.55), linear transformations defined by a real,
2 × 2 matrix A have the property that
Area of A(D) = | det(A) | (Area of D)
for any 2D region D. Why? Because of (6.60), A(S) is the parallelogram determined by the
vectors Ae(1), Ae(2), so the area formula for a parallelogram implies
Area of A(S) = | det
!
Ae(1)  Ae(2) "
|=| det

A
!
e(1)  e(2) "
|=| det(AI2) |=| det(A) |.
If a rectangle R is of the form R = {(x0, y0) + (x, y) : 0 ≤x ≤a, 0 ≤y ≤b}, then a similar
calculation explains why
Area of A(R) = | det
!
aAe(1)  bAe(2) "
| = | det

A
!
ae(1)  be(2) "
|
= | det(A) | | det
!
ae(1)  be(2) "
| = | det(A) | ab.
(The translation by (x0, y0) does not affect the area.)
Example 6.30
By what factors do the transformations defined by the given 2 × 2 matrices multiply
area?
(a)
2
0
0
1
2

,
(b)
1
2
0
1

,
(c)
cos θ
−sin θ
sin θ
cos θ

,
(d)
3
1
1
1

.
Method:
(a) |det(A)| = 1, so area is unaffected, that is, is multiplied by a factor of 1.
(b) |det(A)| = 1, so area is unaffected, that is, is multiplied by a factor of 1.
(c) |det(A)| = 1, so area is unaffected, that is, is multiplied by a factor of 1.
(d) |det(A)| = 2, so area is multiplied by a factor of 2. ⃝
Figure 6.32 shows the effect of the linear transformation defined by the matrix in part
(d) of Example 6.30. That transformation shears in both ways and stretches.
Because all orthogonal matrices have determinant equal to ±1, orthogonal matrices
define area preserving linear transformations. One example of a 2× 2 orthogonal matrix is
the rotation matrix in part (c) of Example 6.30.
In R3 we get results similar to those in R2: Any 3 × 3 matrix A defines a linear
transformation of R3 by x →Ax and
Volume of A(V) = |det(A)| (Volume of V)
for any 3D region V.

Geometry, Calculus, and Other Tools
515
y
y
B
C
D
B΄
C΄
D΄
x
x
1
1
2
1
1
2
3
4
O
O
FIGURE 6.32
Example 6.30(d).
Stretching and/or compressing matrices have the form
⎡
⎣
a11
0
0
0
a22
0
0
0
a33
⎤
⎦,
where a11, a22, and a33 are positive constants.
Shear transformations on R3 are upper or lower triangular matrices, just as for shear
transformations on R2:
⎡
⎣
1
a12
a13
0
1
a23
0
0
1
⎤
⎦
or
⎡
⎣
1
0
0
a21
1
0
a31
a32
1
⎤
⎦.
The matrices
⎡
⎣
−1
0
0
0
1
0
0
0
1
⎤
⎦,
⎡
⎣
1
0
0
0
−1
0
0
0
1
⎤
⎦,
⎡
⎣
1
0
0
0
1
0
0
0
−1
⎤
⎦,
⎡
⎣
0
1
0
1
0
0
0
0
1
⎤
⎦,
⎡
⎣
0
0
1
0
1
0
1
0
0
⎤
⎦,
⎡
⎣
1
0
0
0
0
1
0
1
0
⎤
⎦
are reflections, and the matrices of the forms
⎡
⎣
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎤
⎦,
⎡
⎣
cos φ
0
−sin φ
0
1
0
sin φ
0
cos φ
⎤
⎦,
⎡
⎣
1
0
0
0
cos ψ
−sin ψ
0
sin ψ
cos ψ
⎤
⎦
are rotation matrices, as are products of such matrices. An example of such a product,
Q = Q1Q0, was used in the derivation of volume formula (6.58). In that derivation, Q0 was
a rotation by angle −θ0 around the z-axis, and Q1 was a rotation by angle φ0 around the
y-axis.
Similar to what happens for 2 × 2 matrices, multiplication by a real, 3 × 3 matrix A
transforms volume by a factor of |det(A)|. In particular, multiplication by a rotation or
reflection matrix does not change volume, as we would expect.

516
Advanced Engineering Mathematics
In general, a product AnAn−1 · · · A2A1 defines a linear transformation that consists of
first doing the transformation defined by matrix A1, and then after that doing the trans-
formation defined by matrix A2, . . ., and finishing by doing the transformation defined by
matrix An.
If we perform a succession of linear transformations corresponding to a matrix
AnAn−1 · · · A2A1,
then the effect on volume is to multiply it by the factor
| det(AnAn−1 · · · A2A1) | =
n
+
i=1
| det(Ai) |.
6.6.3 Change of Variables, Area, and Volume
Definition 6.5
If x = x(u, v) and y = y(u, v), then the map is a change of variables from the uv-plane to
the xy-plane, as long as the Jacobian matrix
 ∂(x, y)
∂(u, v)

≜
⎡
⎣
∂x
∂u
∂x
∂v
∂y
∂u
∂y
∂v
⎤
⎦
is invertible.
For example, if A is a real, 2 × 2 matrix, then
x
y

≜A
u
v

defines a change of variables as long as A is invertible, that is, as long as |det(A)| ̸= 0. In
fact, this change of variables multiplies area by a factor of |det(A)|: The rectangle
D ≜{(u, v) : u0 ≤u ≤u0+ △u, v0 ≤v ≤v0+ △v}
(6.61)
has Area(D) =△u △v, and its image under the change of variables (Figure 6.33),
A(D) = {A
u
v

:
u
v

in D},
has Area(A(D)) = |det(A)| Area(D).
Similarly, a change of variables
x
y

=
x(u, v)
y(u, v)


Geometry, Calculus, and Other Tools
517
(u,v)
(x,y)
(x(u,v),y(u,v))
FIGURE 6.33
Change of variables in R2.
multiplies the area of D, given in (6.61), by approximately
det
 ∂(x, y)
∂(u, v)
 .
(6.62)
Why? Because the image of D under the change of variables is approximately
⎧
⎪⎪⎨
⎪⎪⎩
⎡
⎢⎢⎣
x0 +

∂x
∂u(u0, v0)

(u −u0) +

∂x
∂v(u0, v0)

(v −v0)
y0+
 ∂y
∂u(u0, v0)

(u −u0)+
 ∂y
∂v(u0, v0)

(v −v0)
⎤
⎥⎥⎦: u0 ≤u ≤u0 + △u, v0 ≤v ≤v0 + △v
⎫
⎪⎪⎬
⎪⎪⎭
,
where x0 ≜x(u0, v0) and y0 ≜y(u0, v0).
As the area of D gets closer and closer to zero, the approximation in (6.62) gets better
and better.
The object in (6.62) is the absolute value of the determinant of the Jacobian matrix, or
“the absolute value of the Jacobian determinant.”
Example 6.31
Polar coordinates can be thought of as a change of variables from the rθ-plane to the
xy-plane:
x
y

≜
r cos θ
r sin θ

.
What effect does this change of variables have on area?
Method: We calculate
det
 ∂(x, y)
∂(r, θ)
 =

det
⎛
⎝
⎡
⎣
∂x
∂r
∂x
∂θ
∂y
∂r
∂y
∂θ
⎤
⎦
⎞
⎠

=
det
 cos θ
−r sin θ
sin θ
r cos θ
  = · · · = |r| = r.
If an “infinitesimal” element of area in the xy-plane is given by dx dy, then the
corresponding element of area in the rθ-plane is
r dr dθ =
 det
 ∂(x, y)
∂(r, θ)
  dr dθ,
as illustrated in Figure 6.34. ⃝

518
Advanced Engineering Mathematics
y
x
dr
dθ
rdθ
FIGURE 6.34
Example 6.30(d): Element of area in polar coordinates.
Similarly a change of variables on R3 is a map:
⎡
⎣
u
v
w
⎤
⎦→
⎡
⎣
x(u, v, w)
y(u, v, w)
z(u, v, w)
⎤
⎦.
(6.63)
Because volume in R3 is also given by a determinant, specifically (6.58), linear approxi-
mation implies that the (possibly nonlinear) transformation (6.63) multiplies volume by
approximately
det
 ∂(x, y, z)
∂(u, v, w)
 ≜

det
⎡
⎢⎢⎢⎢⎣
∂x
∂u
∂x
∂v
∂x
∂w
∂y
∂u
∂y
∂v
∂y
∂w
∂z
∂u
∂z
∂v
∂z
∂w
⎤
⎥⎥⎥⎥⎦

.
Again, this is the absolute value of the determinant of the Jacobian matrix, or “the absolute
value of the Jacobian determinant.”
Example 6.32
Spherical coordinates can be thought of as a change of variables from ρφθ-space to
xyz-space:
⎡
⎣
x
y
z
⎤
⎦≜
⎡
⎣
ρ sin φ cos θ
ρ sin φ sin θ
ρ cos φ
⎤
⎦.
What effect does this change of variables have on volume?

Geometry, Calculus, and Other Tools
519
x
O
0.0
dρ
ρdθ
dθ
z
ρdφ
dφ
0.0
0.0
y
FIGURE 6.35
Example 6.32. Element of volume in spherical coordinates.
Method: We calculate
det
 ∂(x, y, z)
∂(ρ, φ, θ)
 =

det
⎛
⎜⎜⎜⎜⎜⎝
⎡
⎢⎢⎢⎢⎢⎣
∂x
∂ρ
∂x
∂φ
∂x
∂θ
∂y
∂ρ
∂x
∂φ
∂y
∂θ
∂z
∂ρ
∂z
∂φ
∂z
∂θ
⎤
⎥⎥⎥⎥⎥⎦
⎞
⎟⎟⎟⎟⎟⎠

=

det
⎛
⎝
⎡
⎣
sin φ cos θ
ρ cos φ cos θ
−ρ sin φ sin θ
sin φ sin θ
ρ cos φ sin θ
ρ cos φ cos θ
cos φ
−ρ sin φ
0
⎤
⎦
⎞
⎠

= · · · = |ρ2 sin φ| = ρ2 sin φ.
Note that sin φ ≥0 because 0 ≤φ ≤π in spherical coordinates.
If an “infinitesimal” element of volume in xyz space is given by dx dy dz, then the
corresponding element of volume in ρφθ-space is
ρ2 sin φ dρ dφ dθ =
det
 ∂(x, y, z)
∂(ρ, φ, θ)
 dρ dφ dθ.
This is illustrated in Figure 6.35. ⃝
6.6.4 Element of Surface Area
If
S : r = r(u, v) = x(u, v) ˆı + y(u, v) ˆj + z(u, v) ˆk, (u, v) in D,
is a parametrized surface, then linear approximation gives
r(u, v) ≈r0 + (u −u0) ∂r
∂u(u0, v0) + (v −v0) ∂r
∂v(u0, v0),

520
Advanced Engineering Mathematics
where r0 ≜r(u0, v0). So, the image of a rectangle D given by (6.61) is approximately the
parallelogram given by
{r0 + (u −u0) ∂r
∂u(u0, v0) + (v −v0) ∂r
∂v(u0, v0) : u0 ≤u ≤u0 + △u, v0 ≤v ≤v0 + △v}.
This parallelogram has
Area =



△u ∂r
∂u(u0, v0)

×

△v ∂r
∂v(u0, v0)

 =


∂r
∂u(u0, v0)× ∂r
∂v(u0, v0)

 |△u| |△v|,
by the original formula for area given in (6.54). So, an element of surface area is
dS ≜


∂r
∂u(u0, v0) × ∂r
∂v(u0, v0)

 du dv.
(6.64)
A special case of the surface area formula occurs when the surface is “flat,” for example,
when it lies in the xy-plane and is thus given by
S : r = r(u, v) = x(u, v) ˆı + y(u, v) ˆj, (u, v) in D.
(6.65)
In this case,
dS =


∂x
∂u ˆı + ∂y
∂u ˆj

×
∂x
∂v ˆı + ∂y
∂v ˆj

 du dv = · · · =

∂x
∂u
∂y
∂v −∂x
∂v
∂y
∂u
 du dv
=
det
 ∂(x, y)
∂(u, v)
 du dv.
This agrees with (6.62). So, a change of variables multiplies area by the same factor,
 ∂r
∂u × ∂r
∂v
, that appears in the formula for surface area in the special case of a flat surface.
This makes a lot of sense because a change of variables in R2 is really a parametrization of
a flat surface.
Learn More About It
For more about linear transformations, including homogeneous coordinates, transla-
tion, and perspective, see Linear Algebra, 3rd edn., by David C. Lay, Addison-Wesley,
c⃝2003, Section 2.7.
6.6.5 Problems
In problems 1 and 2, find the area of the parallelogram satisfying the given conditions.
1. Parallelogram has vertices (0, 0, 0), (1, 0, −2), (3, 3, −2), (2, 3, 0)
2. Parallelogram is determined by the vectors 3ˆı + ˆj and 2 ˆj + ˆk

Geometry, Calculus, and Other Tools
521
In problems 3 and 4, the given vectors determine a parallelepiped. Find its volume and
sketch it.
3.
⎧
⎨
⎩
⎡
⎣
3
0
0
⎤
⎦,
⎡
⎣
0
4
0
⎤
⎦,
⎡
⎣
0
0
2
⎤
⎦
⎫
⎬
⎭
4.
⎧
⎨
⎩
⎡
⎣
1
1
0
⎤
⎦,
⎡
⎣
0
3
0
⎤
⎦,
⎡
⎣
1
0
2
⎤
⎦
⎫
⎬
⎭
In problems 5 and 6, find the volume of the parallelepiped with the given vertices.
5. (0, 0, 0), (1, 1, 0), (0, 2, 0), (1, 3, 0), (1, 2, 3), (2, 3, 3), (1, 4, 3), (2, 5, 3)
6. (0, 0, 0), (1, 0, 1), (0, 2, 0), (1, 2, 1), (1, 2, 3), (2, 2, 4), (1, 4, 3), (2, 4, 4)
For problems 7–10, sketch the effect of the linear transformation defined by the given
matrix, and describe that effect in words. [Note: For a linear transformation defined by
a product of matrices, the matrix furthest on the right acts first, then the one on its left,
then the one left of that, etc.]
7.
1
0
0
−1

8.
⎡
⎢⎣
1
2
−
√
3
2
√
3
2
1
2
⎤
⎥⎦
9.
⎡
⎣
1
0
0
−1
⎤
⎦
⎡
⎢⎣
1
2
−
√
3
2
√
3
2
1
2
⎤
⎥⎦
10.
⎡
⎢⎣
1
2
−
√
3
2
√
3
2
1
2
⎤
⎥⎦
⎡
⎣
1
0
0
−1
⎤
⎦
11. Find a 2 × 2 matrix that gives the linear transformation sketched in Figure 6.36.
y
y
B
C
D
B΄
C΄
D΄
x
x
1
1
2
1
O
O
1
2
FIGURE 6.36
Problem 6.6.5.11.

522
Advanced Engineering Mathematics
In problems 12 and 13, by what factor is area or volume multiplied as a result of the linear
transformation defined by the given matrix?
12.
30
29
31
30

13.
⎡
⎣
1
2
4
−1
0
5
1
0
3
⎤
⎦
14. Explain why Qn is orthogonal to both QA and QB if Q is an orthogonal matrix
and n ≜A × B.
15. (a) Write down three different examples of 3 × 3 elementary matrices.
(b) Find the effect on volume for each of the matrices you wrote in part (a).
(c) Formulate a conjecture about the effect of elementary matrices on volume.
16. In crystallography, the “primitive cell” of a lattice L is the parallelepiped, three of
whose edges are the vectors a, b, c, which are assumed to be linearly independent.
The a, b, c are called the “translation vectors.” The lattice L is defined to be the
position vectors of an infinite set of points such that L = n1a + n2b + n3c + L, for
all integers n1, n2, n3. In other words, the lattice looks the same under translations
by multiples of the vectors a, b, c.
The “reciprocal (or dual) lattice,” L⋆, is defined to be the set of all vectors k for
which ei2πk•r = 1 for all vectors r in L.
(a) Find the volume, V, of the primitive cell and
(b) Explain why the vectors A, B, C give a primitive cell of the reciprocal lattice,
L⋆, where
A ≜
1
a • (b × c) b × c,
B ≜
1
a • (b × c) c × a,
C ≜
1
a • (b × c) a × b.
[Hint: You may use the result about scalar triple products mentioned in
Problem 6.8.4.15.]
(c) Explain why

a  b  c
 −1 =

A  B  C
 T.
Each of the vectors A, B, C is normal to two of the faces of the primitive cell
parallelepiped.
6.7 Differential Operators and Curvilinear Coordinates
The foundation of calculus in R2 or R3 is the gradient operator. In R2 this is defined by
grad(f) = ∇f ≜∂f
∂x ˆı + ∂f
∂y ˆj,
and in R3 the gradient operator is defined by
grad(f) = ∇f ≜∂f
∂x ˆı + ∂f
∂y ˆj + ∂f
∂z
ˆk.

Geometry, Calculus, and Other Tools
523
Mathematical physics also uses two operators on vector fields: the divergence, defined by
div(F) = ∇• F ≜

ˆı ∂
∂x + ˆj ∂
∂y + ˆk ∂
∂z

•

Fx ˆı + Fy ˆj + Fz ˆk

= ∂Fx
∂x + ∂Fy
∂y + ∂Fz
∂z ,
and the curl operator, defined by
curl(F) = ∇× F ≜

ˆı ∂
∂x + ˆj ∂
∂y + ˆk ∂
∂z
 
Fx ˆı + Fy ˆj + Fz ˆk

,
hence,
curl(F) = ∇× F =
∂Fz
∂y −∂Fy
∂z

ˆı +
∂Fx
∂z −∂Fz
∂x

ˆj +
∂Fy
∂x −∂Fx
∂y

ˆk
= det
⎡
⎢⎢⎢⎢⎣
ˆı
ˆj
ˆk
∂
∂x
∂
∂y
∂
∂z
Fx
Fy
Fz
⎤
⎥⎥⎥⎥⎦
.
There are many physical applications of these operators. Here are a few: Fourier’s law
of heat conduction states that if u is the temperature, then the rate of heat flux is q =
−[κ]∇u. In general, [κ] could be a 3×3 matrix, which allows for heat to flow more intensely
in some directions than in others. We may call [κ] a “tensor.” A special case is when [κ] is
replaced by a scalar κ.
Similarly, if c is the concentration of a substance, then Fick’s law of diffusion states that
the rate of flow of the substance is J = −[A]∇c for some matrix [A] or scalar A. So, heat
flow is analogous to flow of a substance.
Suppose a fluid whose mass density is ϱ = ϱ(x, y, z) flows with velocity vector v =
u ˆı + v ˆj + w ˆk. Then ϱ v is the rate of mass flow. The requirement that
∇• (ϱ v) ≡0
(6.66)
is a physical conservation law that puts a constraint on the velocity vector field. If ϱ is
constant, then (6.66) would require
0 = ϱ(∇• v).
The fluid is called incompressible if ∇• v ≡0 .
Continuing with the situation of fluid flow,
ω ≜∇× v
is called the vorticity of the fluid. We say the fluid is irrotational if
∇× v ≡0.

524
Advanced Engineering Mathematics
The Laplacian operator, denoted by ∇2 or , is defined by
∇2f ≜(∇• ∇)[f] = ∇• (∇f) = ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2
for a scalar function f. If F = Fx ˆı + Fy ˆj + Fz ˆk is a vector field then we define the Laplacian
on it component by component, that is,
∇2[Fx ˆı + Fy ˆj + Fz ˆk] ≜(∇2Fx) ˆı + (∇2Fy) ˆj + (∇2Fz) ˆk.
6.7.1 Properties of the Operators grad, div, and curl
Theorem 6.6
For any scalar functions f and g, vector fields F and G for which the derivatives exist, and
constant scalar α,
(1) ∇(f + g) = ∇f + ∇g, ∇• (F + G) = ∇• F + ∇• G, ∇× (F + G) = ∇× F + ∇× G.
(2) ∇(αf) = α∇f,
∇• (αF) = α∇• F,
∇× (αF) = α∇× F.
(3) ∇(fg) = g∇f + f∇g.
(4) ∇• (gF) = g ∇• F + (∇g) • F.
(5) ∇× (gF) = g ∇× F + (∇g) × F.
(6) ∇• (F × G) = G • (∇× F) −F • (∇× G).
(7) ∇× (F × G) = (G • ∇)F −(F • ∇)G −(∇• F)G + (∇• G)F.
(8) ∇• (∇× F) = 0, if ∇× F is continuously differentiable.
(9) ∇× (∇f) = 0, if ∇f is continuously differentiable.
(10) ∇× (∇× F) = ∇(∇• F) −∇2F.
The next result will turn out to be useful in studying partial differential equations.
Corollary 6.1
(∇f) • (∇g) = ∇• (g∇f) −g∇2f
.
Why? This follows from Theorem 6.6(4) with F ≜∇f. 2
Recall from Definition 6.4 that a vector field F has a potential function f if F = ∇f
continuously on an open region, in which case we say that F is exact.
Corollary 6.2
If F is exact and continuously differentiable, then curl(F) = 0, that is, F is irrotational.

Geometry, Calculus, and Other Tools
525
6.7.2 Curvilinear Coordinates
Suppose there is a change of variables from coordinates (u, v, w) to (x, y, z), that is,
x = x(u, v, w),
y = y(u, v, w),
z = z(u, v, w),
and r = r(u, v, w) = x(u, v, w) ˆı + y(u, v, w) ˆj + z(u, v, w) ˆk is continuously differentiable. We
already know from Section 6.6 that under this change of variables, volume is multiplied by
det
 ∂(x, y, z)
∂(u, v, w)
 .
Let’s find the differential operators grad, div, curl, and Laplacian in curvilinear coordi-
nates.
Definition 6.6
Suppose C1 : r1 = r1(t) and C2 : r2 = r2(τ) are two curves that intersect at a point r0 =
x0 ˆı + y0 ˆj + z0 ˆk. We say the two curves intersect orthogonally at r0 if the corresponding
tangent vectors, that is, dr1
dt and dr2
dτ , are nonzero and orthogonal there.
The two curves intersect at r0 if r1(t0) = r0 = r2(τ0) for some t0, τ0, but nothing is said
about comparing the values of t0 and τ0.
Definition 6.7
(u, v, w) are curvilinear coordinates if at all (u0, v0, w0) the three curves
C1 : r1(u) = r(u, v0, w0),
C2 : r2(v) = r(u0, v, w0),
C3 : r3(w) = r(u0, v0, w)
are pairwise orthogonal there, that is, the set of three tangent vectors
{T1, T2, T3}
is an orthogonal basis for R3, where
T1 ≜∂r
∂u(u0, v0, w0),
T2 ≜∂r
∂v(u0, v0, w0),
T3 ≜∂r
∂w(u0, v0, w0).
Example 6.33
(a) Explain why cylindrical coordinates are curvilinear coordinates except at the ori-
gin, (b) find the gradient operator in cylindrical coordinates, and (c) find the Laplacian
operator in cylindrical coordinates.

526
Advanced Engineering Mathematics
Method:
(a) x = r cos θ, y = r sin θ, z = z express (x, y, z) in terms of cylindrical coordinates
(r, θ, z), that is,
r = r(r, θ, z) = (r cos θ)ˆı + (r sin θ) ˆj + z ˆk.
The three curves are
C1 : r1(r) = r(r, θ0, z0) = r cos θ0 ˆı + r sin θ0 ˆj + z0 ˆk,
C2 : r2(θ) = r(r0, θ, z0) = r0 cos θ ˆı + r0 sin θ ˆj + z0 ˆk,
C3 : r3(z) = r(r0, θ0, z) = r0 cos θ0 ˆı + r0 sin θ0 ˆj + z ˆk,
and the corresponding three tangent vectors are, respectively,
T1 = dr1
dr (r0) = cos θ0 ˆı + sin θ0 ˆj = er0,
T2 = dr2
dθ (θ0) = −r0 sin θ0 ˆı + r0 cos θ0 ˆj = r0eθ0,
and
T3 = dr3
dz (z0) = ˆk = ez.
We already know from Section 6.2 that {ˆer, ˆeθ, ez} is an o.n. basis for R3; hence
{T1, T2, T3} is an orthogonal basis for R3 except at r0 = 0, as we were asked to explain.
(b) By (6.42) in Section 6.4, the multivariable chain rule, if f = f(x, y, z) = f(r cos θ,
r sin θ, z), then
∂f
∂r = ∂f
∂x
∂x
∂r + ∂f
∂y
∂y
∂r + ∂f
∂z
∂z
∂r = cos θ ∂f
∂x + sin θ ∂f
∂y,
∂f
∂θ = ∂f
∂x
∂x
∂θ + ∂f
∂y
∂y
∂θ + ∂f
∂z
∂z
∂θ = −r sin θ ∂f
∂x + r cos θ ∂f
∂y,
∂f
∂z = ∂f
∂x
∂x
∂z + ∂f
∂y
∂y
∂z + ∂f
∂z
∂z
∂z = ∂f
∂z.
These three results can be rewritten as one matrix-vector equation:
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
cos θ
sin θ
0
−r sin θ
r cos θ
0
0
0
1
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
.
It follows that
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
cos θ
sin θ
0
−r sin θ
r cos θ
0
0
0
1
⎤
⎥⎥⎥⎥⎦
−1 ⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
(6.67)

Geometry, Calculus, and Other Tools
527
that is,
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
ˆeT
r
rˆeT
θ
ˆeT
z
⎤
⎥⎥⎥⎥⎦
−1 ⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
(6.68)
Because {ˆer, ˆeθ, ˆez} is an o.n. basis for R3, the matrix
⎡
⎢⎢⎢⎢⎣
ˆeT
r
ˆeT
θ
ˆeT
z
⎤
⎥⎥⎥⎥⎦
−1
=

ˆer  ˆeθ
 ˆez
 
exists; hence,
⎡
⎢⎢⎢⎢⎣
ˆeT
r
rˆeT
θ
ˆeT
z
⎤
⎥⎥⎥⎥⎦
−1
=

ˆer 
1
r ˆeθ
 ˆez

exists.
So,
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=

ˆer 
1
r ˆeθ
 ˆez

⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
= ∂f
∂r ˆer + 1
r
∂f
∂θ ˆeθ + ∂f
∂z ˆez.
(6.69)
Sometimes, we may write (6.69) as
∇= ˆer
∂
∂r + ˆeθ
1
r
∂
∂θ

+ ˆez
∂
∂z.
(6.70)
(c) In rectangular coordinates, the Laplacian operator is
∇2 = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 .
Equation (6.69) can be rewritten as
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=

ˆer 
1
r ˆeθ
 ˆez

⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
cos θ
−1
r sin θ
0
sin θ
1
r cos θ
0
0
0
1
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂r
∂f
∂θ
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
,
so
∂f
∂x = cos θ ∂f
∂r −sin θ
r
∂f
∂θ , ∂f
∂y = sin θ ∂f
∂r + cos θ
r
∂f
∂θ ,
∂f
∂z = ∂f
∂z.
It follows that
∂
∂x = cos θ ∂
∂r −sin θ
r
∂
∂θ
and
∂
∂y = sin θ ∂
∂r + cos θ
r
∂
∂θ .
(6.71)

528
Advanced Engineering Mathematics
So,
∂2f
∂x2 = ∂
∂x
 ∂f
∂x

=

cos θ ∂
∂r −sin θ
r
∂
∂θ
 
cos θ ∂f
∂r −sin θ
r
∂f
∂θ

= cos θ ∂
∂r

cos θ ∂f
∂r −sin θ
r
∂f
∂θ

−sin θ
r
∂
∂θ

cos θ ∂f
∂r −sin θ
r
∂f
∂θ

= cos θ
⎛
⎝cos θ ∂2f
∂r2 +
⎛
⎝sin θ
r2
∂f
∂θ −sin θ
r
∂2f
∂r∂θ
:::::::::
⎞
⎠
⎞
⎠
−sin θ
r
⎛
⎝
⎛
⎝−sin θ ∂f
∂r + cos θ ∂2f
∂θ∂r
::::::::
⎞
⎠+
&
−cos θ
r
∂f
∂θ −sin θ
r
∂2f
∂θ2
'⎞
⎠
= cos2 θ ∂2f
∂r2 −2 sin θ cos θ
r
∂2f
∂r∂θ + 2 sin θ cos θ
r2
∂f
∂θ
+ sin2 θ
r2
∂2f
∂θ2 + sin2 θ
r
∂f
∂r.
after using Clairaut’s theorem (Theorem 6.2) in Section 6.4 to get
∂2f
∂r∂θ =
∂2f
∂θ∂r.
Similarly,
∂2f
∂y2 = ∂
∂y
 ∂f
∂y

=

sin θ ∂
∂r + cos θ
r
∂
∂θ
 
sin θ ∂f
∂r + cos θ
r
∂f
∂θ

= sin θ
⎛
⎝sin θ ∂2f
∂r2 +
⎛
⎝−cos θ
r2
∂f
∂θ + cos θ
r
∂2f
∂r∂θ
:::::::::
⎞
⎠
⎞
⎠
+ cos θ
r
⎛
⎝
⎛
⎝cos θ ∂f
∂r + sin θ ∂2f
∂θ∂r
::::::::
⎞
⎠+
&
−sin θ
r
∂f
∂θ + cos θ
r
∂2f
∂θ2
'⎞
⎠
= sin2 θ ∂2f
∂r2 + 2 sin θ cos θ
r
∂2f
∂r∂θ −2 sin θ cos θ
r2
∂f
∂θ
+ cos2 θ
r2
∂2f
∂θ2 + cos2 θ
r
∂f
∂r.
So,
∇2f = ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2
= cos2 θ ∂2f
∂r2 −
2 sin θ cos θ
r
∂2f
∂r∂θ + 
2 sin θ cos θ
r2
∂f
∂θ + sin2 θ
r2
∂2f
∂θ2
:::::::::
+ sin2 θ
r
∂f
∂r
+ sin2 θ ∂2f
∂r2 + 
2 sin θ cos θ
r
∂2f
∂r∂θ −
2 sin θ cos θ
r2
∂f
∂θ + cos2 θ
r2
∂2f
∂θ2
:::::::::
+ cos2 θ
r
∂f
∂r + ∂2f
∂z2 .

Geometry, Calculus, and Other Tools
529
So, the Laplacian in cylindrical coordinates is
∇2f = ∂2f
∂r2 + 1
r
∂f
∂r + 1
r2
∂2f
∂θ2 + ∂2f
∂z2 ,
(6.72)
which can be written as
∇2f = 1
r
∂
∂r

r ∂f
∂r

+ 1
r2
∂2f
∂θ2 + ∂2f
∂z2 .
(6.73)
Sometimes, we may write (6.72) as
∇2 = ∂2
∂r2 + 1
r
∂
∂r + 1
r2
∂2
∂θ2 + ∂2
∂z2 . ⃝
(6.74)
6.7.3 Differential Operators in Curvilinear Coordinates
Suppose (u, v, w) are general curvilinear coordinates. We will derive the gradient oper-
ator in those coordinates, state the results for the other differential operators, and give
sketches of their derivations. After that, we will state the results in cylindrical and spherical
coordinates, which are commonly used in science and engineering.
Recall that (u, v, w) are curvilinear coordinates for position vectors r(u, v, w) if at every
point r(u0, v0, w0) the three tangent vectors,
Tu = ∂r
∂u(u0, v0, w0),
Tv = ∂r
∂v(u0, v0, w0),
Tw = ∂r
∂w(u0, v0, w0),
are nonzero and give an orthogonal basis for R3. Define the scale factors or “length
factors”
hu = ||Tu|| =


∂r
∂u

 ,
hv = ||Tv|| =


∂r
∂v

 ,
hw = ||Tw|| =


∂r
∂w

 .
Then
ˆeu ≜1
hu
Tu,
ˆev ≜1
hv
Tv,
ˆew ≜1
hw
Tw,
(6.75)
are unit vectors, so
{ˆeu, ˆev, ˆew}
(6.76)
is an o.n. basis for R3.
As we saw in Section 6.6, an element of volume is given by
dV =
det
 ∂(x, y, z)
∂(u, v, w)

du dv dw
 =
det
 ∂r
∂udu 
∂r
∂vdv 
∂r
∂wdw

=
det

huduˆeu  hvdvˆev  hwdwˆew
 
=
det( diag(hudu, hvdv, hwdw))
 ·
det

ˆeu  ˆev  ˆew
  .

530
Advanced Engineering Mathematics
So,
dV = hu hv hw du dv dw | ± 1| = hu hv hw du dv dw,
(6.77)
because the determinant of a real, orthogonal matrix, such as

ˆeu  ˆev  ˆew
 
, is ±1, by
Problem 2.4.4.16.
We see from (6.77) why we refer to hu, hv, hw as length factors.
By (6.42) in Section 6.4, the multivariable chain rule, we have
∂f
∂u = ∂f
∂x
∂x
∂u + ∂f
∂y
∂y
∂u + ∂f
∂z
∂z
∂u
∂f
∂v = ∂f
∂x
∂x
∂v + ∂f
∂y
∂y
∂v + ∂f
∂z
∂z
∂v
∂f
∂w = ∂f
∂x
∂x
∂w + ∂f
∂y
∂y
∂w + ∂f
∂z
∂z
∂w .
These three results can be rewritten as one matrix-vector equation:
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂u
∂f
∂v
∂f
∂w
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
∂x
∂u
∂y
∂u
∂z
∂u
∂x
∂v
∂y
∂v
∂z
∂v
∂x
∂w
∂y
∂w
∂z
∂w
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
.
It follows that
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
∂x
∂u
∂y
∂u
∂z
∂u
∂x
∂v
∂y
∂v
∂z
∂v
∂x
∂w
∂y
∂w
∂z
∂w
⎤
⎥⎥⎥⎥⎥⎦
−1 ⎡
⎢⎢⎢⎢⎢⎣
∂f
∂u
∂f
∂v
∂f
∂w
⎤
⎥⎥⎥⎥⎥⎦
,
(6.78)
that is,
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
huˆeT
u
hvˆeT
v
hwˆeT
w
⎤
⎥⎥⎥⎥⎦
−1 ⎡
⎢⎢⎢⎢⎢⎣
∂f
∂u
∂f
∂v
∂f
∂w
⎤
⎥⎥⎥⎥⎥⎦
.
(6.79)

Geometry, Calculus, and Other Tools
531
Because {ˆeu, ˆev, ˆew} is an o.n. basis for R3,
⎡
⎢⎢⎢⎢⎣
huˆeT
u
hvˆeT
v
hwˆeT
w
⎤
⎥⎥⎥⎥⎦
−1
=
 1
hu
ˆeu 
1
hv
ˆev 
1
hw
ˆew

exists. Similarly to (6.68), and using Lemma 1.2 in Section 1.2, that is, (1.19) in Section 1.2,
we get that
∇f =
⎡
⎢⎢⎢⎢⎢⎣
∂f
∂x
∂f
∂y
∂f
∂z
⎤
⎥⎥⎥⎥⎥⎦
=
 1
hu
ˆeu 
1
hv
ˆev 
1
hw
ˆew

⎡
⎢⎢⎢⎢⎢⎣
∂f
∂u
∂f
∂v
∂f
∂w
⎤
⎥⎥⎥⎥⎥⎦
,
(6.80)
that is,
∇f = 1
hu
∂f
∂u ˆeu + 1
hv
∂f
∂v ˆev + 1
hw
∂f
∂w ˆew
gives the gradient operator in (u, v, w) coordinates. Sometimes, we may write (6.80) as
∇= 1
hu
ˆeu
∂
∂u + 1
hv
ˆev
∂
∂v + 1
hw
ˆew
∂
∂w .
(6.81)
So it was not a coincidence that the vectors ˆer, ˆeθ, ˆez appeared in our results for
cylindrical coordinates in Example 6.33.
It is common to also assume that
{ˆeu, ˆev, ˆew}
is a right-handed coordinate frame, that is,
ˆeu × ˆev = ˆew,
ˆev × ˆew = ˆeu,
ˆew × ˆeu = ˆev.
In fact, this assumption plays a role in deriving the complete results.
Theorem 6.7
Suppose (u, v, w) are curvilinear coordinates that give a right-handed coordinate frame
{ˆeu, ˆev, ˆew} for R3 defined by (6.75). By (6.80), grad is given by
∇f = 1
hu
∂f
∂u ˆeu + 1
hv
∂f
∂v ˆev + 1
hw
∂f
∂w ˆew,

532
Advanced Engineering Mathematics
and the div, curl, and Laplacian operators are given by
∇•
$
Fuˆeu + Fvˆev + Fwˆew
%
=
1
huhvhw
 ∂
∂u [ hvhwFu] + ∂
∂v [ hwhuFv] + ∂
∂w [ huhvFw]

,
(6.82)
∇×
$
Fuˆeu + Fvˆev + Fwˆew
%
=
1
hvhw
 ∂
∂v [ hwFw] −
∂
∂w [ hvFv]

ˆeu
+
1
hwhu
 ∂
∂w [ huFu] −∂
∂u [ hwFw]

ˆev +
1
huhv
 ∂
∂u [ hvFv] −∂
∂v [ huFu]

ˆew,
(6.83)
and
∇2f =
1
huhvhw
 ∂
∂u
 hvhw
hu
∂f
∂u

+ ∂
∂v
 hwhu
hv
∂f
∂v

+ ∂
∂w
 huhv
hw
∂f
∂w

.
(6.84)
Why? We’ve already derived (6.80) by relatively straightforward calculations. But deriving
the other results uses some tricks.
Using (6.80), for the function f(u, v, w) = u, we calculate
∇u = 1
hu
ˆeu
∂u
∂u + 1
hv
ˆev
∂u
∂v + 1
hw
ˆew
∂u
∂w = 1 · 1
hu
ˆeu + 0 · 1
hv
ˆev + 0 · 1
hw
ˆew = 1
hu
ˆeu,
so it follows from Theorem 6.6(9) that
0 = ∇× (∇u) = ∇×
 1
hu
ˆeu

.
Similarly,
∇×
 1
hv
ˆev

= ∇×
 1
hw
ˆew

= 0.
(6.85)
Using (6.85) and Theorem 6.6(6), we get
∇•
 1
hv
ˆev × 1
hw
ˆew

= 0.
But ˆev × ˆew = ˆeu, so (6.85) implies ∇•

1
hvhw
ˆeu

= 0, and similarly, ∇•

1
hwhu
ˆev

= 0,
and ∇×
 1
huhv
ˆew

= 0. The rest of the explanations will be in the homework problems.

Geometry, Calculus, and Other Tools
533
6.7.4 Summary of Operators in Cylindrical Coordinates
Because hr = 1, hθ = r, hz = 1,
∇f = ∂f
∂r ˆer + 1
r
∂f
∂θ ˆeθ + ∂f
∂z ˆez,
∇•
$
Fr ˆer + Fθ ˆeθ + Fz ˆez
%
= 1
r
∂
∂r

rFr
 
+ 1
r
∂
∂θ

Fθ
 
+ ∂
∂z

Fz
 
,
∇×
$
Fr ˆer + Fθ ˆeθ + Fz ˆez
%
=
1
r
∂
∂θ [ Fz ] −∂
∂z [ Fθ ]

ˆer +
 ∂
∂z [ Fr ] −∂
∂r [ Fz ]

ˆeθ +
1
r
∂
∂r [ rFθ ] −1
r
∂
∂θ [ Fr ]

ˆez,
and
∇2f = 1
r
∂
∂r
!
r ∂f
∂r
"
+ 1
r2
∂2f
∂θ2 + ∂2f
∂z2 .
6.7.5 Summary of Operators in Spherical Coordinates
Because hρ = 1, hφ = ρ, hθ = ρ sin φ,
∇f = ∂f
∂ρ ˆeρ + 1
ρ
∂f
∂φ ˆeφ +
1
ρ sin φ
∂f
∂θ ˆeθ,
∇•
$
Fρ ˆeρ + Fφ ˆeφ + Fθ ˆeθ
%
= 1
ρ2
∂
∂ρ
!
ρ2Fρ
"
+
1
ρ sin φ
∂
∂φ
!
sin φ Fφ
"
+
1
ρ sin φ
∂
∂θ

Fθ
 
,
∇×
$
Fρ ˆeρ + Fφ ˆeφ + Fθ ˆeθ
%
=

1
ρ sin φ
∂
∂φ

sin φ Fθ
 
−
1
ρ sin φ
∂
∂θ

Fφ
 
ˆeρ
+

1
ρ sin φ
∂
∂θ

Fρ
 
−1
ρ
∂
∂ρ
!
ρFθ
"
ˆeφ +
 1
ρ
∂
∂ρ
!
ρFφ
"
−1
ρ
∂
∂φ

Fρ
 
ˆeθ,
and
∇2f = 1
ρ2
∂
∂ρ
!
ρ2 ∂f
∂ρ
"
+
1
ρ2 sin φ
∂
∂φ
!
sin φ ∂f
∂φ
"
+
1
ρ2 sin2 φ
∂2f
∂θ2 .
Learn More About It
For complete derivations of the differential operators in curvilinear coordinates and
further observations, see “Orthogonal curvilinear coordinates,” by T. A. S. Jackson,
Math. Gazette, (50), 1966, 28–30. Problem 6.7.6.30 is from The Classical Electromag-
netic Field, by Leonard Eyges, Addison-Wesley, c⃝1972, p.18.

534
Advanced Engineering Mathematics
6.7.6 Problems
1. Evaluate curl

x2y ˆı + x2z ˆj + x3 ˆk

.
In problems 2–5, find the divergence and curl of the vector fields.
2. F ≜xy2z ˆı + 2x2yz ˆk
3. F ≜cos(xy2)ˆı + sin(y + z2) ˆj + ln |x −z| ˆk
4. F ≜
$
3r2 + ln(r) + z
%
ˆer +
$
r3 + 2 −cos θ
%
ˆeθ −
$
z2 −2r2%
ez
5. F ≜ρ−2(z ˆı + x ˆj + y ˆk)
In each of the problems 6 and 7, a “velocity” potential φ = φ(r, θ) is given for 2D irrota-
tional fluid flow past a cylinder (Milne-Thomson, 1968). Find v = ∇φ, the velocity of the
fluid. Assume a and α are positive constants.
6. φ = U

r + a2
r

cos θ
7. φ = U

r + a2
r

cos(θ −α)
By the way, the potential in Problem 6.7.6.6 corresponds to “parallel flow” far from
the cylinder because the velocity vector “at infinity” is given by V∞= Uˆı. Sim-
ilarly, the potential in Problem 6.7.6.7 corresponds has velocity at infinity given
by V∞= U(cos α ˆı + sin α ˆj). This explains why the potential in Problem 6.7.6.7 is
referred to as “at an angle of attack α.”
In problems 8–10, find the divergence and curl of the vector fields in both (a) rectangular
coordinates, and (b) cylindrical coordinates. Table 6.1 in Section 6.2 may be useful. [Note:
To find the curl, for example, in rectangular coordinates of a vector field given in cylin-
drical coordinates, we can find the curl in cylindrical coordinates and convert that into
rectangular coordinates.]
8. F ≜

r −2
r + z

ˆer +

1
2 r2 −2r

ˆeθ + r ez
9. F ≜x(x2 + y2)ˆı + y(x2 + y2 −2z) ˆj + (z2 −3x2 −3y2)ˆk
10. F ≜(2z + x2 −y2)ˆı −(x2 + y2) ˆj + z ˆk
11. For the vector field given as follows, find the divergence and curl in both (a)
rectangular coordinates, and (b) spherical coordinates. Table 6.2 in Section 6.2
may be useful. [Note: To find the curl, for example, in spherical coordinates of
a vector field given in rectangular coordinates, we can find the curl in rectangular
coordinates and convert that into spherical coordinates.]
F ≜x(x2+ y2+ z2)ˆı + (y(x2+ y2+ z2) −2(x2+ y2)) ˆj +
$
z2 −z(x2+ y2+ z2)
%ˆk.
12. Suppose F = ρ−4r, where r = xˆı + y ˆj + zˆk and ρ = ||r||. Find the divergence of F
in as simple a form as possible.
13. If F = ρ−2r, where ρ = ||r||, find ∇• F in as simple a form as possible.

Geometry, Calculus, and Other Tools
535
14. If r = xˆı + y ˆj + zˆk and F ≜||r||r, evaluate curl(F).
15. Suppose that a surface is described in cylindrical coordinates by the equation
z = 2r. Find an equation of the plane tangent to the surface at the point (x, y, z) =
(−
√
3, 1, 4). [Hint: The equation of the tangent plane may be written in rectangular
coordinates.]
16. Explain why the formulas of Theorem 6.6(4) and (5) are correct.
17. Explain why the formula of Theorem 6.6(6) is correct.
18. Explain why the formula of Theorem 6.6(7) is correct.
19. (a) Explain why the formula of Theorem 6.6(9) is correct.
(b) Explain why ∇× (f∇g) = (∇f) × (∇g) is an identity.
20. Determine a scalar potential function for the vector field
F ≜−cos z
r2
ˆer + cos θ
r
ˆeθ −

1 + sin z
r

ez.
21. In the absence of currents, magnetostatics gives magnetic field
H = −H0

1 −2α
 a
ρ
3
cos φ ˆeρ + H0

1 + α
 a
ρ
3
sin φ eφ
outside of a metal sphere ρ = a, where H0 is a constant. Find a potential func-
tion u satisfying H = −∇u. [By the way, α = μ0−μ
2μ0+μ, where μ0 is the magnetic
permeability∗of a vacuum and μ is the magnetic permeability of the metal.]
22. A 2D electric dipole can be idealized by two infinite line charges running parallel
to the xy-plane. A good approximation of the electric field is
E = −pℓ
2πϵ · 1
r2
$
sin θ ˆer −cos θ ˆeθ
%
,
where pℓis the magnitude of the dipole moment. Find a potential function u
satisfying E = −∇u.
In problems 23–26, assume A is an unspecified constant. Determine a scalar potential
function for the given vector field.
23. F ≜(2Ar + 1)(2πθ −θ2)ˆer + 2(Ar + 1)(π −θ) ˆeθ + cos 3z ez
24. F ≜

2r −cos z
r2

ˆer + cos θ
r
ˆeθ −

1 + sin z
r

ez
25. F ≜

z2
2 + A2 cos θ + 2Ar sin θ

ˆer + (−A2 sin θ + Ar cos θ)ˆeθ + rzez
26. F ≜(−A
ρ + 6ρ sin2 φ cos 2θ)ˆeρ + 6ρ sin φ cos φ cos 2θ eφ −6ρ sin φ sin 2θ ˆeθ
27. In particle physics, the “Yukawa potential” is defined by V(r) ≜−k e−μρ
ρ , where k
and μ are scalar constants. Find (a) the corresponding force field F ≜−∇V(r) and
(b) the Laplacian ∇2V(r).
∗From URL http://www.scienceworld.Wolfram.com, the permeability of free space is μ0 ≈1.2566 × 10−6
webers/(amps · m)

536
Advanced Engineering Mathematics
28. Assume A, B are unspecified constants. Determine a scalar potential function for
the vector field
F ≜−B
ρ2 (cos φ + sin θ)ˆeρ−
A
ρ + B
ρ2

sin φ eφ −
A
ρ −B
ρ2
 cos θ
sin φ ˆeθ.
29. Finish the explanation for the formula for the divergence operator in right-handed
curvilinear coordinates, that is, (6.82) in Theorem 6.7: First, rewrite
∇• F ≜∇•
$
Fuˆeu + Fvˆev + Fwˆew
%
= ∇•

hvhwFu ·

1
hvhw
ˆeu

+ ∇•

hwhuFv ·

1
hwhu
ˆev

+ ∇•

huhvFw ·

1
huhv
ˆew

.
For each of these three terms, use the product rule in Theorem 6.6(4) and the
“trick” results we gave, for example, ∇•

1
hvhw ˆeu

= 0.
30. Define a “modified Coulomb’s law” by F(r) ≜K e−ρ/α
ρ

1
ρ + 1
α

ˆeρ, where K and
α are scalar constants. (a) Calculate ∇× F. (b) Decide if F is exact; if it is, find
a potential function for it. [Hint: Look at the answer for Problem 6.7.6.27(a).] (c)
Calculate ∇• F.
31. Finish the explanation for the formula for the Laplacian operator in right-handed
curvilinear coordinates, that is, (6.84) in Theorem 6.7, by using ∇2 = ∇• (∇f) and
the formulas for the gradient and the divergence in (6.80) and (6.82), respectively.
32. Finish the explanation for the formula for the curl operator in right-handed curvi-
linear coordinates, that is, (6.83) in Theorem 6.7: begin similarly to Problem
6.7.6.29 by rewriting
∇× F ≜∇×
$
Fuˆeu + Fvˆev + Fwˆew
%
= ∇×

huFu ·
 1
hu
ˆeu

+ ∇×

hvFv ·
 1
hv
ˆev

+ ∇×

hwFw ·
 1
hw
ˆew

.

33. Suppose x =
⎡
⎣
x
y
z
⎤
⎦, A =
⎡
⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎦, and f(x, y, z) ≜xTAx. For unspecified
A and x, find (a) a formula for ∇f in as simple a form as possible, for exam-
ple, in terms of multiplication of a matrix or sum of matrices times a vector and

Geometry, Calculus, and Other Tools
537
(b) a simpler version of that formula in the special case that A is symmetric. [Hint:
It may help to multiply out to get a more explicit expression for f(x, y, z).]
34. Suppose we make a change of variables from (x, y, z) coordinates to (u, v, w)
coordinates by
⎡
⎣
x
y
z
⎤
⎦= A
⎡
⎣
u
v
w
⎤
⎦,
where A = [aij] is a constant 3 × 3 matrix, and suppose f = f(x, y, z). Use the chain
rule to find a simplified expression for ∇u,v,w f ≜
! ∂f
∂u
∂f
∂v
∂f
∂w
"T
, the gradient
operator in (u, v, w) coordinates, in terms of ∇x,y,z f ≜
! ∂f
∂x
∂f
∂y
∂f
∂z
"T
. Your final
conclusion should be in terms of the matrix A.
6.8 Rotating Coordinate Frames
Here we will discuss motion of a particle moving in a coordinate frame that is rotating
and/or in linear motion relative to a fixed coordinate frame. The results are applicable to
motion on a rotating planet and are applicable to mechanical systems in general.
6.8.1 ODEs Describing Rotation
First, let’s consider the case of a particle rotating around the z-axis with an angular speed ω,
measured in radians per unit of time. The axis of revolution here is ˆk. The angular velocity
is defined to be ω ≜ωˆk = (angular speed) · (axis of revolution).
Denote the initial position vector of the particle by
r0 = −−→
OP0 = x0 ˆı + y0 ˆj + z0 ˆk =
⎡
⎣
x0
y0
z0
⎤
⎦.
The motion of the particle is described by its position vector
r(t) =
⎡
⎣
cos(ωt) x0 −sin(ωt) y0
sin(ωt) x0 + cos(ωt) y0
z0
⎤
⎦=
⎡
⎣
cos(ωt)
−sin(ωt)
0
sin(ωt)
cos(ωt)
0
0
0
1
⎤
⎦
⎡
⎣
x0
y0
z0
⎤
⎦,
as shown in Figure 6.37.
There is another way to write this: denoting the initial position vector of the particle by
r0 = x0 ˆı + y0 ˆj + z0 ˆk = r0 cos θ0 ˆı + r0 sin θ0 ˆj + z0 ˆk,

538
Advanced Engineering Mathematics
z
y
C
P
P
O
r
r
ω
x
FIGURE 6.37
Particle in circular motion around the z-axis.
then the motion of the particle is described by its position vector
r(t) = r0 cos(ωt + θ0) ˆı + r0 sin(ωt + θ0) ˆj + z0 ˆk.
The latter is in a form convenient for finding a tangent vector, specifically the velocity
vector
T(t) = ˙r(t) = −ω r0 sin(ωt + θ0) ˆı + ω r0 cos(ωt + θ0) ˆj = (ωˆk) × r(t) = ω × r(t).
Theorem 6.8
Any rotation of a particle is described by the system of ODEs
˙r(t) = ω × r(t).
(6.86)
Why? Refer to Figure 6.38. First, a particle rotating around an axis of revolution ω describes
a circle lying in a plane that has ω as a normal vector, so the tangent vector, ˙r(t), must be
orthogonal to ω. Also, because the point P is on a circle, the tangent vector ˙r(t) must be
orthogonal to the radial vector that points from C, the center of the circle, to P. But
r ≜−→
OP = −→
CP + (r • ω)ω
(6.87)
is an orthogonal decomposition, where ω is the unit vector in the direction of ω. For
example, if ω = ωˆk then ω = ˆk and (6.87) says
r ≜−→
OP = −→
CP + (r • ˆk)ˆk = (x ˆı + y ˆj) + zˆk.

Geometry, Calculus, and Other Tools
539
x
z
y
C
P
O
r
r
ω
ψ
FIGURE 6.38
Particle in circular motion around an axis along ω.
So, we see that, in general, (6.87) is an orthogonal decomposition of r into its radial
vector, −→
CP, plus the projection of r onto the axis of revolution. Now, the tangent vector,
˙r(t), is orthogonal to both ω and −→
CP, so ˙r(t) is orthogonal to r.
Also, because ˙r(t) is orthogonal to the plane spanned by ω and −→
CP, ˙r(t) is orthogonal to
the plane spanned by ω and r. It follows from the properties of the cross product that
˙r(t) = α(ω × r),
(6.88)
for some scalar α.
But, the linear speed of a particle moving in a circle is (the radius of the circle) ×
(angular speed) = ||−→
CP|| ω, while (6.88) says the linear speed should be
||˙r(t)|| = |α| ||ω × r|| = |α| ||ω|| ||r|| sin ψ,
where ψ is the angle between ω and r. But ||−→
CP|| = ||r|| sin ψ, so the linear speed is
ω · ||−→
CP|| = ||˙r(t)|| = |α| ||ω|| ||−→
CP|| = ω · |α| · ||−→
CP||.
It follows that |α| = 1. We may define “counterclockwise” rotation about ω to be the case
when α = 1. Replacing ω by −ω is equivalent to replacing α = 1 by α = −1. So, rotation is
described by the ODE system (6.86). 2
If ω = ωx ˆı + ωy ˆj + ωz ˆk and, as usual, r = x ˆı + y ˆj + z ˆk, then (6.86) can be rewritten as
d
dt
⎡
⎣
x
y
z
⎤
⎦=
⎡
⎣
0
−ωz
ωy
ωz
0
−ωx
−ωy
ωx
0
⎤
⎦
⎡
⎣
x
y
z
⎤
⎦≜
⎡
⎣
x
y
z
⎤
⎦.

540
Advanced Engineering Mathematics
The solutions of (6.86) can be written as
⎡
⎣
x(t)
y(t)
z(t)
⎤
⎦= et
⎡
⎣
x(0)
y(0)
z(0)
⎤
⎦.
(6.89)
Denoting νx = ωx/ω, νy = ωy/ω, νz = ωz/ω, which are the components of the unit vector ω,
the fundamental matrix is given by
et
=
⎡
⎢⎢⎢⎢⎢⎣
(ν2
y + ν2
z ) cos ωt + ν2
x
νxνy(1 −cos ωt) −νz sin ωt
νzνx(1 −cos ωt) + νy sin ωt
νxνy(1 −cos ωt) + νz sin ωt
(ν2
z + ν2
x) cos ωt + ν2
y
νyνz(1 −cos ωt) −νx sin ωt
νzνx(1 −cos ωt) −νy sin ωt
νyνz(1 −cos ωt) + νx sin ωt
(ν2
x + ν2
y) cos ωt + ν2
z
⎤
⎥⎥⎥⎥⎥⎦
.
(6.90)
As an aside, we have derived that the motion of a rotating particle is described by the
system of ODEs
˙r(t) = ω × r(t),
where ω is the constant angular velocity vector. In that derivation we could have allowed
ω to be time dependent, that is,
˙r(t) = ω(t) × r(t),
but then the solutions would not be given by (6.89).
6.8.2 Velocity and Acceleration
The usual o.n. basis for R3 is {ˆı, ˆj, ˆk}. It is a right-handed basis because of Theorem 6.1(e)
in Section 6.1.
Suppose {eℓ= eℓ(t) : ℓ= 1, 2, 3} is the o.n. set of vectors in R3 that are the rotations of
the initial vectors {ˆı, ˆj, ˆk} with constant angular velocity ω. So,
˙eℓ= ω × eℓ, ℓ= 1, 2, 3.
For example, if ω(t) = ω ˆk for some constant angular speed ω, then
e1(t) =
⎡
⎣
cos(ωt)
sin(ωt)
0
⎤
⎦,
e2(t) =
⎡
⎣
−sin(ωt)
cos(ωt)
0
⎤
⎦,
e3(t) =
⎡
⎣
0
0
1
⎤
⎦.
In general,

e1(t)  e2(t)  e3(t)
 
= et !
ˆı  ˆj  ˆk
"
= etI3 = et.

Geometry, Calculus, and Other Tools
541
We call
{e1(t), e2(t), e3(t)}
a rotating coordinate frame. In Problem 6.8.4.14, you will check that this set of three
vectors is a right-handed basis for R3.
Suppose
r = r(t) ≜x1(t)e1(t) + x2(t)e2(t) + x3(t)e3(t)
expresses the position of a particle relative to the fixed coordinate frame {ˆı, ˆj, ˆk}. Here,
(x1(t), x2(t), x3(t)) are the coordinates of the particle relative to the rotating coordi-
nate frame. By the product rule,
˙r(t) = ˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t) + x1(t)˙e1(t) + x2(t)˙e2(t) + x3(t)˙e3(t).
= ˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t) + x1(t)ω × e1(t) + x2(t)ω × e2(t) + x3(t)ω × e3(t)
= ˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t) + ω × (x1(t)e1(t) + x2(t)e2(t) + x3(t)e3(t)).
It follows that
˙r(t) = ˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t) + ω × r(t).
(6.91)
Define the velocity of the particle relative to the rotating coordinate frame by
vrel(t) ≜˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t).
Then (6.91) can be rewritten as
v(t) ≜˙r(t) = vrel(t) + ω × r(t).
(6.92)
This gives the velocity relative to the fixed coordinate frame {ˆı, ˆj, ˆk}.
Take another derivative with respect to time to get
¨r(t) = ˙v(t) = d
dt[ vrel(t) + ω × r(t) ] = d
dt[ vrel(t) ] + ω × ˙r(t);
(6.93)
recall that we are assuming that the angular velocity vector, ω, is constant. To further
analyze (6.93), we calculate
d
dt[ vrel(t) ] = d
dt[ ˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t) ]
= ¨x1(t)e1(t) + ¨x2(t)e2(t) + ¨x3(t)e3(t) + ˙x1(t)˙e1(t) + ˙x2(t)˙e2(t) + ˙x3(t)˙e3(t)
= ¨x1(t)e1(t) + ¨x2(t)e2(t) + ¨x3(t)e3(t) + ˙x1(t)ω × e1(t) + ˙x2(t)ω × e2(t) + ˙x3(t)ω × e3(t),
that is,
d
dt[ vrel(t) ] = arel(t) + ω ×
$˙x1(t)e1(t) + ˙x2(t)e2(t) + ˙x3(t)e3(t)
%
.
(6.94)

542
Advanced Engineering Mathematics
We define the acceleration of the particle relative to the rotating coordinate frame by
arel(t) ≜¨x1(t)e1(t) + ¨x2(t)e2(t) + ¨x3(t)e3(t).
Substituting the definition of vrel(t) into (6.94) gives
d
dt[ vrel(t) ] = arel(t) + ω × vrel(t).
Substituting this and (6.92) into (6.93) gives
¨r(t) = ˙v(t) = arel(t) + ω × vrel(t) + ω ×
$
vrel(t) + ω × r(t)
%
,
that is,
a(t) ≜¨r(t) = arel(t) + 2ω × vrel(t) + ω × (ω × r(t)).
(6.95)
The term 2ω × vrel(t) is called the Coriolis acceleration.
While ω × ω = 0, if ω × r(t) ̸= 0, then the term ω × (ω × r(t)) ̸= 0 because ω × r(t) is
perpendicular to ω.
Example 6.34
A baseball pitcher standing at latitude 50◦N of the Earth’s equator throws a ball in the
due north direction at a speed of 90.00 mph. Find the initial Coriolis acceleration of
the ball.
Method: Figure 6.39a shows a point at latitude 50◦N of the Earth’s equator. There, a unit
vector pointing due north resolves to be −cos 50◦ˆj +sin 50◦ˆk. The initial velocity vector
of a ball thrown due north resolves to be
vrel(0) = −vrel(0) cos 50◦ˆj + vrel(0) sin 50◦ˆk.
Because 60 miles/h equals 88 ft/s, the initial speed of the ball is
vrel(0) =

132 ft
s

· 0.304800610 m
1 ft
≈40.23368052 m/s,
in KMS units. So, rounding all final conclusions to the four significant digits in the given
initial speed of the ball,
vrel(0) ≈(−25.86 ˆj + 30.82 ˆk) m/s.
The Earth does one complete rotation, that is, 2π radians, in about 86164.1 s, so its
angular rotational speed is ω ≈
2π
86164.1 s ≈7.29212 × 10−5/s and its angular velocity
vector is ω = ω ˆk. The initial Coriolis acceleration is
2ω × vrel(0) ≈2(7.29212 × 10−5/s)ˆk × (−25.86 ˆj + 30.82 ˆk) m/s ≈0.003771 m/s2 ˆı. ⃝

Geometry, Calculus, and Other Tools
543
sin 50° 
50°N
50°N
(a)
(b)
–cos 50° 
k
k
j
j
i
i
FIGURE 6.39
Example 6.34 (a) Ball thrown due north and (b) ball thrown due west.
Example 6.35
Find the Coriolis acceleration of the ball in Example 6.34 if instead it is thrown due west.
Method: Figure 6.39b shows a point at latitude 50◦N of the Earth’s equator and a vector
pointing due west from that point, that is, in the direction of ˆı. Reusing some of the
information from Example 6.9, the initial Coriolis acceleration is
2ω × vrel(0) ≈2(7.29212 × 10−5/s)ˆk × 40.23368052ˆı m/s ≈0.005868 m/s2 ˆj. ⃝
So, on the planet Earth, the Coriolis acceleration is so small that it does not explain why
a “curveball” curves.
Remark
If the angular velocity vector, ω, is not constant, then, instead of (6.95), we get
¨r(t) = arel(t) + 2ω(t) × vrel(t) + ω(t) × (ω(t) × r(t)) + ˙ω(t) × r(t),
(6.96)
where ˙ω(t) is called the angular acceleration vector.
6.8.3 Velocity and Acceleration in a Rotating Frame Whose Origin Is Moving
Suppose there is a third, fixed, rectangular coordinate frame {ˆI, ˆJ, ˆk}. By “fixed” we mean
constant in time, so d ˆI
dt = d ˆJ
dt = d ˆk
dt = 0. Let O be the center of this third, fixed rectangular
frame and let o be the center of both the rotating coordinate frame, {e1, e2, e3}, and the
intermediate frame, {ˆı, ˆj, ˆk}. Suppose o is in motion relative to O, with velocity V(t) and
acceleration, A(t). A particle in the rotating frame at a point P has coordinates (X, Y, Z)

544
Advanced Engineering Mathematics
relative to O, the center of this third, fixed rectangular frame, that is,
R(t) = −→
OP = X(t) ˆI + Y(t) ˆJ + Z(t) ˆK.
So, relative to the third frame, a particle in the rotating coordinate frame has velocity
˙R(t) = V(t) + ˙r(t) = V(t) + vrel(t) + ω × r(t)
and acceleration
¨R(t) = A(t) + ¨r(t) = A(t) + arel(t) + 2ω × vrel(t) + ω × (ω × r(t)).
(6.97)
Remark
Again, if the angular velocity vector, ω, is not constant, then, instead of (6.97), the particle’s
acceleration is
¨R(t) = A(t) + ¨r(t) = A(t) + arel(t) + 2ω × vrel(t) + ω × (ω × r(t)) + ˙ω(t) × r(t).
(6.98)
Learn More About It
Problem 6.8.4.6 is discussed in “The applications of mathematics in meteorology,” by
B. Haurwitz, Am. Math. Mon, (50) (1943), 77–84.
6.8.4 Problems
1. At a latitude of 30◦N of the equator on the surface of the Earth, find ω × (ω × r).
Give a picture showing your coordinate frame and the location of 30◦N.
2. The wind is blowing straight south at 54 km/h at a latitude of 30◦N of the equator
on the surface of the Earth. (a) Find the Coriolis acceleration vector and (b) the
ω×(ω×r) component of the acceleration. Give a picture showing your coordinate
frame, the location of 30◦N, and the wind direction. Assume the radius of the
Earth is about 6400 km.
3. Suppose that in an inertial reference frame, the velocity of a particle is v(t) =
1
1+(ρ(t))3 R, where R is a constant vector and ρ is the magnitude of the position
vector r(t), as usual. Find the acceleration of the particle relative to the inertial
reference frame in terms of R, ρ, r(t), and v(t), the velocity vector relative to the
inertial reference frame.
4. A very small insect is on a compact disk rotating at 60 revolutions per second. The
insect is traveling along a straight line, according to its point of view, at a constant
speed of 0.05 mm/s. What is the Coriolis acceleration vector it experiences? Do
make clear your coordinates.
5. On the planet Jupiter at latitude 60◦N of the equator, find the vector ω×(ω×r) that
is a component of the acceleration. Here are some data about Jupiter: It’s period
of rotation is about 9.925 h∗, and its radius is about 71, 000 km.
∗According to N.A.S.A. at URL http://nssdc.gsfc.nasa.gov/planetary/factsheet/jupiterfact.html

Geometry, Calculus, and Other Tools
545
6. Near the end of Section 6.5, we discussed isobars, for example, shown in
Figure 6.23. The closeness of successive isobars shows the magnitude of the hori-
zontal components of the gradient of atmospheric pressure p. But we will see that
the direction of the geostrophic wind is almost parallel to the isobars because of
the Coriolis acceleration due to the Earth’s rotation. For example, Coriolis accel-
eration effects the “sub tropical jet stream,” which is usually at an altitude of
between 10, 000 and 16, 000 m.
Let v = v(t, r) be the wind velocity. Assuming an altitude above 1000 m so
that ground effects are insignificant, an approximate model for the velocity is the
nonlinear partial differential equation
∂v
∂t + v • ∇v

+ 2ω × v = −1
ϱ0
∇p −g,
where g is the acceleration of gravity and we assume that the air density ϱ0 is
approximately constant. The first two terms are the material derivative of velocity
and the third term is the Coriolis acceleration.
For this problem, it helps to choose the coordinate system so that the outward
normal to the Earth’s surface, at a latitude of ψ north of the equator, is the vector ˆk;
hence, the angular velocity vector is ω = ω sin ψ ˆk and the acceleration of gravity
is g = −g ˆk, that is, is in the vertical direction. Notice that the latitude affects the
equations; in particular, there is a difference between the northern and southern
hemispheres.
Assuming the wind is steady, that is, v(t, r) is approximately constant, find two
equations for the two horizontal components of the wind, u = v • ˆı and v = v • ˆj.
At 39◦45′ 32′′ N, the latitude of Dayton, Ohio, assume the horizontal gradient of
the pressure points due south and has magnitude of 1 millibar per 70 km; note that
1 millibar = 100 Pa in pascals, that is, N/m2. Assume the density of air is about
ϱ0 = 1.293 kg/m3. Find the direction and magnitude of the geostrophic wind.
7. An astronaut stands in the crew compartment at the end of a 60 m long cylinder
rotating about its lateral midpoint at a speed of ω rads/s. What is the maxi-
mum period of rotation that gives the astronaut the sensation of 1 g (gravity) of
acceleration?
8. A baseball pitcher standing on the Earth’s equator throws a ball in the due north
direction at a speed of 90.00 mph. Find the initial Coriolis acceleration of the ball.
9. The velocity at the point (−1, 2, 3), with distances measured in m, from the center
of a rotating coordinate frame with angular velocity ω, is measured to be V =
12 ˆı + 6 ˆj m/s. What is ω, if ωx = 3 rad/s?
In problems 10–13, write down the rotating matrix et for the given ω.
10. ω = ˆı
11. ω = ˆj
12. ω = ˆı + ˆj
13. ω = ˆı + ˆj + ˆk

546
Advanced Engineering Mathematics
14. Suppose a coordinate frame is rotating with constant angular velocity and the
frame is given by {e1(t), e2(t), e3(t)}. Using the explicit representation

e1(t)  e2(t)  e3(t)
 
= et,
explain why the vectors in the rotating frame give a right-handed basis for R3.
15. Explain why scalar triple products satisfy A • (B × C) = C • (A × B).
16. Given any two vector valued functions x(t), z(t), explain why
d
dt[x(t) • z(t)] ≡˙x(t) • z(t) + x(t) • ˙z(t).
17. Suppose a coordinate frame is rotating with a (possibly nonconstant) angu-
lar velocity and the frame is given by {e1(t), e2(t), e3(t)}, where ˙eℓ= ω(t) ×
eℓ, ℓ= 1, 2, 3. Use the results of Problems 6.8.4.15 and 6.8.4.16 to explain why
{e1(t), e2(t), e3(t)} is an o.n. basis for R3. [Hints: Define a scalar function of t by
y(t) ≜ei(t) • ej(t), calculate y(0) and ˙y(t), and use the uniqueness part of the
existence and uniqueness theorem for first-order ODEs.]
18. Define A×(B×C) to be a vector triple product. Explain why vector triple products
satisfy A × (B × C) = (A • C)B −(A • B)C.
19. Given any two vector-valued functions x(t), z(t), explain why
d
dt[x(t) × z(t)] ≡˙x(t) × z(t) + x(t) × ˙z(t).
20. Suppose a coordinate frame is rotating with a (possibly nonconstant) angular
velocity and the frame is given by {e1(t), e2(t), e3(t)}, where ˙eℓ= ω(t) × eℓ, ℓ=
1, 2, 3. Use the results of Problems 6.8.4.17 through 6.8.4.19 to explain why
{e1(t), e2(t), e3(t)} is a right-handed o.n. basis for R3. [Hints: For example, define
a vector function of t by x(t) = e1(t) × e2(t), calculate x(0) and ˙x(t), and use the
uniqueness part of the existence and uniqueness theorem for systems of ODEs.]
Key Terms
angular speed, angular velocity: beginning of Section 6.8
change of variables: Definition 6.5 in Section 6.6
closed: Definition 6.2 in Section 6.3
contour plot: after Example 6.26 in Section 6.5
Coriolis acceleration: after (6.95) in Section 6.8
curl: beginning of Section 6.7
curvilinear coordinates: Definition 6.7 in Section 6.7
cyclotron motion: Problem 6.1.4.17
cylindrical coordinates: after Example 6.6 in Section 6.2
directional derivative: (6.34) and before (6.40), both in Section 6.4
divergence: beginning of Section 6.7
element of surface area: (6.64) in Section 6.6
exactness criterion: (6.43) in Section 6.4

Geometry, Calculus, and Other Tools
547
Fick’s law of diffusion: beginning of Section 6.7
Fourier’s Law of Heat Conduction: beginning of Section 6.7
geostrophic wind: before Example 6.27, Problem 6.8.4.6
gradient vector: (6.34) and before (6.40), both in Section 6.4
image: after (6.10) in Section 6.2, after (6.59) in Section 6.6
incompressible: beginning of Section 6.7
intermediate frame: after (6.96) in Section 6.8
intersect orthogonally: Definition 6.6 in Section 6.7
irrotational: beginning of Section 6.7
isobars: after Example 6.26 in Section 6.5
Jacobian matrix: Definition 6.5 in Section 6.6
Laplacian operator: beginning of Section 6.7
lattice: Problem 6.6.5.16
Leibniz’s rule: Problem 6.4.5.24
level curves, level sets: after Example 6.26 in Section 6.5
linear approximation: (6.33) in Section 6.4
linear transformation: (6.59) in Section 6.6
Lorentz force: Example 6.2 in Section 6.1
map: (6.59) in Section 6.6
mapping: Example 6.29 in Section 6.6
normal: after (6.6) in Section 6.1
one-to-one: after (6.10) in Section 6.2
orthogonal surfaces: Problem 6.5.1.9
parameters: before Example 6.15 in Section 6.3
parametrized: before Example 6.8 in Section 6.3
parametric equations: after (6.3) in Section 6.1
parametrized surface: before Example 6.15 in Section 6.3
position vector: Definition 6.1 in Section 6.1
potential function, exact: Definition 6.4 in Section 6.4
range: after (6.10) in Section 6.2
reciprocal or “dual” lattice: Problem 6.6.5.16
reflection transformation: after Example 6.29 in Section 6.6
right-handed o.n. basis: before (6.13) in Section 6.2
rotating coordinate frame: before (6.91) in Section 6.8
rotation transformation: after Example 6.29 in Section 6.6
scalar triple product: after (6.57) in Section 6.6
scale factors: before (6.75) in Section 6.7
shear transformation: after Example 6.29 in Section 6.6
simple (curve): Definition 6.2 in Section 6.3
simple (surface): Definition 6.3 in Section 6.3
spherical coordinates: Section 6.2.2
standard orthonormal basis for R3: after Example 6.7 in Section 6.2
stretching and/or compressing transformation: after Example 6.29 in Section 6.6
torque: Example 6.1 in Section 6.1
transformation: (6.59) in Section 6.6
unit square: Example 6.29 in Section 6.6
vector field: before Example 6.13 in Section 6.3
vector from P0 to P1: Definition 6.1 in Section 6.1
vector triple product: Problem 6.8.4.18

548
Advanced Engineering Mathematics
vorticity: beginning of Section 6.7
work: before (6.1) in Section 6.1
Zhukovskii airfoils: Section 6.3.2
Mathematica Command
ParametricPlot[{2Cos[t], Sin[2t]}, {t, 0, 2Pi}]: before Figure 6.12
Reference
Milne-Thomson, L.M. Theoretical Hydrodynamics, Section 6.22. Dover Publications, Mineola, NY,
1968.

7
Integral Theorems, Multiple Integrals,
and Applications
7.1 Integrals for a Function of a Single Variable
Suppose f(x) is a function of x on a finite interval [a, b]. Recall that the definite integral of
f on [a, b] can be defined by the limit of Riemann sums, if the limit exists: If a = x0 < x1 <
. . . < xn = b is a partition of [a, b], x∗
k are sampling points satisfying xk−1 ≤x∗
k ≤xk for
k = 1, 2, . . . , n, and △xk = xk −xk−1 for k = 1, 2, . . . , n, then
n

k=1
f(x∗
k) △xk
(7.1)
is a Riemann sum. The △xk’s don’t have to be equal. If the limit of the Riemann sums
exists∗as n →∞and max
1≤k≤n |△xk | →0, we define the definite integral of f from x = a
to x = b:
b
a
f(x)dx = lim
n→∞
 n

k=1
f(x∗
k) △xk

.
(7.2)
If the definite integral of f on [a, b] exists and f(x) ≥0 there, then
 b
a f(x)dx equals the
area under the curve y = f(x) and above the interval a ≤x ≤b on the x-axis, as illustrated
in Figure 7.1.
Theorem 7.1
Suppose f is continuous on the finite interval [a, b]. Then
 b
a f(x)dx exists. Further, for each
n = 1, 2, . . ., we may choose the regular partition a = x0 < x1 < · · · < xn = b, where △
x = b−a
n , xk = a + k △x, and sampling at right endpoints x∗
k = xk, and then
∗For a limit to exist, it must be a finite number. Even for one n, there are infinitely many choices of partitions and
sampling points, so saying the “limit of the Riemann sums exists” says that it is the same number no matter
what partitions and sampling points the Riemann sums use.
549

550
Advanced Engineering Mathematics
y = f(x)
y
x
a
b
FIGURE 7.1
Definite integral calculates an area.
b
a
f(x)dx = lim
n→∞
 n

k=1
f(xk) △x

.
We get the same limit using sampling at left endpoints or sampling at midpoints:
b
a
f(x)dx = lim
n→∞
 n

k=1
f(xk−1) △x

= lim
n→∞
 n

k=1
f
xk−1 + xk
2

△x

.
Theorem 7.2
(Properties of the definite integral) Suppose the definite integrals of f and g on [a, b] exist,
c is a constant in [a, b], and α is a constant. Then
(a)
 b
a (f(x) + g(x))dx =
 b
a f(x)dx +
 b
a g(x)dx.
(b)
 b
a (αf(x))dx = α
 b
a f(x)dx.
(c)
 b
a f(x)dx =
 c
a f(x)dx +
 b
c f(x)dx, as illustrated in Figure 7.2.
y = f(x)
x
y
a
c
b
FIGURE 7.2
Definite integral on a union of intervals.

Integral Theorems, Multiple Integrals, and Applications
551
x
y
2
4
FIGURE 7.3
Definite integral of a piecewise-defined function.
Definition 7.1
If a < b and
 b
a f(x)dx exists,
 a
b f(x)dx ≜−
 b
a f(x)dx.
Example 7.1
Let f(x) be defined “piecewise” by f(x) =
 x,
0 ≤x ≤2
2x,
2 ≤x ≤4

. Evaluate
 4
0 f(x)dx.
Method: We get
 4
0 f(x)dx =
 2
0 f(x)dx +
 4
2 f(x)dx =
 2
0 x dx +
 4
2 2x dx. Interpreted as an
area, we calculate that
 2
0 x dx is the area of a triangle with base of length 2 and height 2,
so
 2
0 x dx = 2 as seen in Figure 7.3. Similarly,
 4
2 x dx is the area of a trapezoid with base 2
and average height 2+8
2 = 5, so
 4
2 x dx = 2 · 5 = 10. Finally,
4
0
f(x)dx =
2
0
x dx +
4
2
2x dx = 2 + 10 = 12. ⃝
Example 7.2
Suppose
 b
a f(x)dx = 5,
 b
a
	
f(x) + g(x)

dx = −4, and
 b
c g(x)dx = 2. Find
 c
a 3g(x)dx.
Method:
c
a
3g(x)dx = 3
c
a
g(x)dx = 3
⎛
⎝
b
a
g(x)dx −
b
c
g(x)dx
⎞
⎠
= 3
⎛
⎝
b
a
	
f(x) + g(x)

dx −
b
a
f(x)dx −
b
c
g(x)dx
⎞
⎠= 3(−4 −5 −2) = −33. ⃝
Definition 7.2
F(x) is an indefinite integral of f(x) on an open interval I if d
dx[F(x)] = f(x) for all x in I, in
which case we denote F(x) =

f(x)dx. Also, we call F(x) an anti-derivative for f(x) on I.

552
Advanced Engineering Mathematics
Theorem 7.3
If F1(x) and F2(x) are anti-derivatives for the same function f(x) on an interval (a, b), then
F1(x) −F2(x) ≡c on [a, b] for some constant c.
Theorem 7.4
(The Fundamental Theorem of Calculus) Suppose f(x) is continuous on a finite interval
[a, b] and c is in [a, b]. Then
(a) G(x) ≜
 x
c f(t)dt satisfies d
dx[G(x)] = f(x), for a < x < b.
(b)
 b
a f(x)dx = F(b) −F(a) ≜

F(x)
b
a, if F(x) is any anti-derivative for f(x) on the
interval (a, b).
There is another way to write Theorem 7.4(b):
Theorem 7.5
[F(x)]b
a =
 b
a

d
dx[F(x)]

dx, assuming d
dx[F(x)] is continuous on (a, b).
Another notation for Theorem 7.4(b) is
b
a
f(x)dx =

f(x)dx
b
a .
Theorem 7.6
(Method of Substitution) Suppose w(x) is continuously differentiable and monotone on a
finite interval [a, b] and f(x) is continuous on the interval [w(a), w(b)]. Then
b
a
f(w(x))dw
dx dx =
w(b)

w(a)
f(w)dw.
Theorem 7.7
(Method of Integration by Parts) Suppose u(x), v(x) are continuously differentiable on a
finite interval [a, b]. Then
b
a
u(x) dv
dx(x) dx = [u(x)v(x)]b
a −
b
a
v(x) du
dx(x) dx.

Integral Theorems, Multiple Integrals, and Applications
553
Definition 7.3
The average value of a function f on the interval [a, b] is
¯f ≜
1
b −a
b
a
f(x) dx =
 b
a f(x) dx
 b
a 1 dx
.
In many problems of science and engineering, the average value of a function is a simple
quantity used to describe a situation. Even though vastly different functions can have the
same average value, nevertheless the average value may be useful information.
7.1.1 Improper Integrals
Definition 7.4
Suppose a is a finite number and f(x) is continuous on the interval [a, ∞).
The
improper integral
 ∞
a
f(x)dx
(a) Is convergent if limb→∞
 b
a f(x)dx = L exists, in which case we say the integral
converges to L
(b) Is divergent if limt→∞
 t
a f(x)dx does not exist
Recall that saying a limit “exists” implicitly says that the limit is a finite value. Figure 7.4
depicts the area under the curve y = x−2 and over the interval [1, t]. For example, in case (a)
of Definition 7.4, we can write
 ∞
a
f(x)dx ≜limt→∞
 t
a f(x)dx .
On the other hand, if limt→∞
 t
a f(x)dx = ∞, that is, the limit is defined but does not exist,
then we write nothing more than “the integral is divergent”; we do not claim that we can
evaluate it.
Similarly,
the convergence or divergence of
 b
−∞f(x)dx is decided by studying
limt→−∞
 b
t f(x)dx.
Both
 ∞
a
f(x)dx and
 b
−∞f(x)dx are called improper integrals on semi-infinite intervals;
 ∞
−∞f(x)dx is called an improper integral on the real line.
Definition 7.5
Suppose a is any finite number and f(x) is continuous on the real line, that is, (−∞, ∞).
The improper integral
 ∞
−∞f(x)dx
(a) Is convergent if both
 ∞
a
f(x)dx = L1 and
 a
−∞f(x)dx = L2 exist, in which case we
say the integral converges to L1+L2 and write
 ∞
−∞f(x)dx =
 a
−∞f(x)dx +
 ∞
a
f(x)dx
(b) Is divergent if either
 ∞
a
f(x)dx or
 a
−∞f(x)dx is divergent

554
Advanced Engineering Mathematics
1.0
0.8
0.6
0.4
0.2
x
y
1
t
10
FIGURE 7.4
 t
1 x−2dx.
Example 7.3
Study the convergence of
 ∞
−∞
2x
x2+1dx.
Method: Choosing a = 0 for convenience,
lim
t→∞
t
0
2x
x2 + 1dx = lim
t→∞

ln(x2 + 1)
t
0 = lim
t→∞

ln(t2 + 1) −0

= ∞,
so
 ∞
−∞
2x
x2+1dx diverges. ⃝
The improper integral in Example 7.3 is divergent even though
lim
t→∞
R
−R
2x
x2 + 1dx = lim
t→∞

ln(x2 + 1)
R
−R = lim
t→∞

ln(R2 + 1) −ln

(−R)2 + 1

= lim
t→∞(0) = 0.
7.1.2 Problems
For the given integral, use the method of substitution and the fundamental theorem of
calculus to find its value.
1.
 π
2
−π cos
	
2t + π
4

dt
2.
 0
−π
2 sin3(2x) cos(2x) dx
3.
 5
0
3x
1+x2 dx
4.
 1
0
e−x
(e−x+1)2 dx

Integral Theorems, Multiple Integrals, and Applications
555
5.
 π
4
0 cos(√x) dx
[Hint: Try the substitution w = √x; after that, you may need to use integration
by parts.]
In problems 6–12, for the given improper integral, determine whether it is convergent or
divergent. If it is convergent, give its value.
6.
 ∞
2
1
x1.1 dx
7.
 ∞
0
x2dx
(1+x3)2
8.
 ∞
0
x dx
(4+x2)3/2
9.
 ∞
4
π x−2 cos(x−1) dx
10.
 ∞
1
dx
√
1+x2
[Hint:

sec θdθ = ln | sec θ + tan θ | may be useful, after making an inverse
trigonometric substitution.]
11.
 ∞
−∞
dx
√
1+x2
12.
 ∞
−∞xe−x dx
13. Suppose p is a real constant. For what values of p does
 ∞
0
1
xp dx (a) converge and
(b) diverge? For those values of p for which the integral converges, state the value
to which it converges.
14. (a) Suppose that
 n+1
n
f(x)dx = 1 for every positive integer n, for a certain function
f(x). Say as much as you can about
 ∞
0
f(x)dx. Also, find a specific example of
such a function f.
(b) Suppose that
 n+1
n
f(x)dx ≥1
2 for every positive integer n, for a certain function
f(x). Say as much as you can about
 ∞
0
f(x)dx. Also, find a specific example of
such a function f.
(c) Suppose that

 n+1
n
f(x)dx
 ≥1
2 for every positive integer n, for a certain func-
tion f(x). Say as much as you can about
 ∞
0
f(x)dx. Also, find a specific example
of such a function f.
15. Explain why f(0) =
 1
0 x f ′′(x) dx for all functions f(x) that are twice continuously
differentiable on [0, 1] and satisfy f(1) = 0 and f ′(1) = 0.
16. Explain why 1 + f(0) =
 1
0 x f ′′(x) dx for all functions f(x) that are twice continu-
ously differentiable on [0, 1] and satisfy f(1) = 0 and f ′(1) = 1.
7.2 Line Integrals
Suppose C : r = r(t), α ≤t ≤β, is a piecewise smooth parametrized∗curve in the xy-plane.
We can find its length by a limiting process on piecewise linear curves that approximate C.
∗The parameter, t, may or may not be the time variable in a physical problem.

556
Advanced Engineering Mathematics
(x(t1), y(t1))
(x(t0), y(t0))
(x(tn), y(tn))
FIGURE 7.5
Piecewise linear approximation of a curve.
Let α = t0 < t1 < . . . < tn = β be a partition and △tk = tk −tk−1. For k = 1, we get
r(t1) −r(t0) = (x(t1) −x(t0)) ˆı + (y(t1) −y(t0)) ˆj ≈˙x(t1) △t1 ˆı + ˙y(t1) △t1 ˆj.
In Figure 7.5, the curve C is in black, and connecting points on the curve are line segments.
The distance between r(t1) and r(t0) is
||r(t1) −r(t0)|| ≈

(˙x(t1)2 + ˙y(t1)2)(△t1)2 =

˙x(t1)2 + ˙y(t1)2 |△t1| = ||˙r(t1)|| |△t1|.
An approximation for L, the total arclength of the curve C, is given by
L ≈
n

k=1
||r(tk) −r(tk−1)|| =
n

k=1
||˙r(tk))|| △tk,
after noting that △tk > 0. Take the limit, as both n →∞and each △tk →0, to get the exact
total arclength:
L =
β
α
||˙r(t)|| dt.
(7.3)
Physically, this makes sense because ||˙r(t)|| is the speed of travel when t is the time.
Definition 7.6
The arclength function is defined by
s(t) ≜
t
α
||˙r(u)|| du.
By the fundamental theorem of calculus, specifically Theorem 7.4(a),
ds
dt = ||˙r(t)||.
(7.4)

Integral Theorems, Multiple Integrals, and Applications
557
3.0
y
2.5
2.0
1.5
x
–1
1
2
3
4
FIGURE 7.6
Example 7.4.
Using this, we have the following:
Definition 7.7
The element of arclength is defined by
ds ≜||˙r(t)|| dt.
Example 7.4
Find the total arclength of the curve C : r(t) = (−1+2t3/2)ˆı +(1+t) ˆj, 0 ≤t ≤2, assuming
distances are measured in cm.
Method: The curve is shown in Figure 7.6. Measuring t in seconds, ˙r(t) = (3t1/2ˆı+ ˆj) cm/s,
so the speed is ||˙r(t)|| =

(3t1/2)2 + (1)2. The total arclength of this curve is
L =
2
0
√
9t + 1 dt =
19

1
√w · dw
9 = 1
9 ·
 w3/2
3/2
19
1 = 2
27

19
√
19 −1

≈6.06067 cm,
by using the method of substitution with w = 9t + 1. ⃝
The curve shown in Figure 7.6 was drawn by the MathematicaTM command:
ParametricPlot[{−1 + 2t∧(3/2), 1 + t}, {t, 0, 2},PlotStyle →Thick].
Example 7.5
A wire in the shape of the curve C of Example 7.4 has a linear mass density of ϱ = f(x, y) ≜
0.05 + 0.01y in g/cm. Find the total mass of the wire.
Method: Approximate C by linear pieces, and assume that in each piece the density
is approximately constant. The line segment in the kth piece, that is, from r(tk−1) to
r(tk), has length approximately ||˙r(tk)|| △tk =

9tk + 1 △tk cm. The mass of that piece is
length × density, which is approximately
(||˙r(tk)|| △tk 
cm) × (f(x(tk), y(tk))g/
cm) =

9tk + 1 △tk × (0.05 + 0.01y(tk)) g
=

9tk + 1 · (0.05 + 0.01(1 + tk)) △tk g.

558
Advanced Engineering Mathematics
Approximate the total mass of the wire by adding up the masses of the pieces:
M ≈
n

k=1

9tk + 1 · (0.05 + 0.01(1 + tk)) △tk g.
After that, take the limit, as both n →∞and max
1≤k≤n |△tk| →0, to get
M =
2
0
√
9t + 1 · (0.05 + 0.01(1 + t))dt = 0.06
2
0
√
9t + 1 dt + 0.01
2
0
t
√
9t + 1 dt
= 0.12
27 (19
√
19 −1) + 0.01
19

1
w −1
9
√
w · dw
9
= 0.12
27 (19
√
19 −1) + 0.01
81
19

1
(w3/2 −w1/2)dw
by again using the method of substitution with w = 9t + 1; hence, t = w−1
9 . So the total
mass of the wire is
M = 0.12
27 (19
√
19 −1) + 0.01
81
2
5w5/2 −2
3w/2)
19
1
= 0.12
27 (19
√
19 −1) + 0.01
81
2
5(192√
19 −1) −2
3(19
√
19 −1)

≈0.434564 g. ⃝
Example 7.5 motivates the definition that the total mass of a curve in the plane
C : r = r(t), α ≤t ≤β, whose linear mass density is ϱ = f(x(t), y(t)), is
M ≜
β
α
f(x(t), y(t))||˙r(t)|| dt.
In general, given a curve C :
r = r(t), α ≤t ≤β, in xyz-space, we calculate its total
arclength to be, as in (7.3),
L =
β
α
||˙r(t)|| dt
and its arclength function to be, as in Definition 7.6,
s(t) ≜
t
α
||˙r(u)|| du.
We calculate, as in (7.4),
ds
dt = ||˙r(t)||
and define, as in Definition 7.7, the element of arclength to be
ds ≜||˙r(t)||dt.

Integral Theorems, Multiple Integrals, and Applications
559
A curve in the xy-plane is a special case where r(t) = x(t)ˆı + y(t) ˆj + 0 · ˆk.
Using these notations and defining
f(r(t)) = f(x(t), y(t), z(t)),
we have
Definition 7.8
The line integral of a function is defined by

C
f(r) ds ≜
β
α
f(r(t)) ||˙r(t)|| dt.
(7.5)
For example, the total mass of a wire in space is the line integral of the density function
ϱ = f(r(t)), that is,
M =

C
ϱ ds ≜
β
α
f(r(t)) ||˙r(t)|| dt.
Note that ||˙r(t)|| =

(˙x(t))2 + (˙y(t))2 + (˙z(t))2.
Example 7.6
Set up the line integral of a function f = f(x, y, z) over one turn of the helix
C : r(t) = (a cos t)ˆı + (a sin t) ˆj + tˆk.
The curve shown in Figure 7.7 was drawn by the Mathematica command:
ParametricPlot3D[{2Cos[t], 2Sin[t], t}, {t, 0, 2π}],PlotStyle →Thick].
Method:
˙r(t) = (−a sin t)ˆı + (a cos t) ˆj + ˆk, and 0 ≤t ≤2π gives one turn of the helix, so
the line integral is

C
f(r) ds =
2π

0
f(a cos t, a sin t, t)

a2 + 1 dt. ⃝
Definition 7.9
The average value of a function f on a curve C is
¯f ≜
1
Length of C

C
f(r) ds =

C f(r) ds

C 1 ds .

560
Advanced Engineering Mathematics
6
4
2
0
–2
–1
0
1
2
–1
0
y
x
z
1
2
–2
FIGURE 7.7
One turn of a helix with a = 2.
Besides the line integrals defined by (7.5) in Definition 7.8, we have the following:
Definition 7.10

C
f(r) dx ≜
β
α
f(r(t))
dx
dt (t)

dt,
(7.6)

C
f(r) dy ≜
β
α
f(r(t))
dy
dt (t)

dt,
(7.7)
and

C
f(r) dz ≜
β
α
f(r(t))
dz
dt (t)

dt.
(7.8)
7.2.1 Line Integrals of Vector-Valued Functions
Suppose F = F(x, y, z) is a vector field, that is, a vector-valued function. We define a
line integral of a vector field by

Integral Theorems, Multiple Integrals, and Applications
561
C
FIGURE 7.8
Curve in a vector field.

C
F(r) • dr ≜
β
α
F(r(t)) • ˙r(t) dt.
(7.9)
For example, if F(r) is a force field, then the total work, W, done by that force on a particle
is defined by
W ≜

C
F(r) • dr.
(7.10)
Taking the dot product of F with ˙r(t), that is, with dr
dt, gives the component of F in the
direction of travel on the curve, that is, in the tangential direction.
Shown in Figure 7.8 is a curve in a vector field.
Another way of writing a line integral of a vector field F = Pˆı + Q ˆj + Rˆk is

C
F(r) • dr =

C
(Pdx + Qdy + Rdz).
A simple, closed, parametrized curve in the xy-plane is said to be positively oriented if,
while standing in the xy-plane with head in the ˆk direction, the inside of the curve is on our
left as we travel on the curve in the direction of its parametrization. While this definition
is admittedly imprecise, it will suffice for our needs.
If C is a simple, closed, piecewise smooth curve, we notate line integrals on C by

C
instead of

C. The little arrow pointing in the counterclockwise direction in the notation

C
indicates that the parametrized curve C is positively oriented.
If C = C1 ∪C2 is a chain, as illustrated in Figure 7.9, then we define

C
. . . ≜

C1
. . . +

C2
. . . .
There is a similar definition for a chain of any finite number of curves.

562
Advanced Engineering Mathematics
1
2
=  1         2
FIGURE 7.9
Chain of two curves.
As a second example, if v = v(x, y, z) is a velocity vector field of fluid particles (from the
“Eulerian” point of view) and C is a simple, closed, positively oriented, piecewise smooth
curve, then the circulation of v around C is defined to be
 ≜

C
v(r) • dr.
(7.11)
Circulation is related to the concept of lift in aerodynamics by
Lift ≜ϱ0 ,
(7.12)
where ϱ0 is the gas (constant) mass density.
Example 7.7
Find the total circulation around the triangle with “nose up” shown in Figure 7.10,
assuming the wind has constant speed U0.
Method: Let ϕ be the constant “pitch” angle, so
v = U0(cos ϕ ˆı + sin ϕ ˆj).
Write C as a chain of three curves as shown in Figure 7.10. Parametrize the curves by
C1 : r1 = (1 −t)ˆı, 0 ≤t ≤1.
C2 : r2 = tˆı + ht ˆj, 0 ≤t ≤1.
C3 : r3 = (1 −t)h ˆj, 0 ≤t ≤1.
v = U0(cos   ˆ+sin    jˆ)
h
1
1
 2
3
j
j
i
FIGURE 7.10
Triangle nose up in airflow.

Integral Theorems, Multiple Integrals, and Applications
563
We calculate
 =

C
v(r) • dr =

C1
v(r1) • dr1 +

C2
v(r2) • dr2 +

C3
v(r3) • dr3
=
1
0
U0(cos ϕˆı + sin ϕ ˆj) • (−ˆı)dt +
1
0
U0(cos ϕˆı + sin ϕ ˆj) • (ˆı + h ˆj)dt
+
1
0
U0(cos ϕˆı + sin ϕ ˆj) • (−h ˆj)dt
=
1
0
U0(−cos ϕ)dt +
1
0
U0(cos ϕ + h sin ϕ)dt +
1
0
U0(−h sin ϕ)dt
= −U0 cos ϕ + U0(cos ϕ + h sin ϕ) + U0(−h sin ϕ) = 0.
So, the lift is zero. ⃝
7.2.2 Fundamental Theorem of Line Integrals
This result is both a generalization of the fundamental theorem of calculus, specifically in
the form of Theorem 7.4, and explained by it!
Theorem 7.8
Suppose C is a piecewise smooth curve in a domain D and F = ∇f in D. Then

C
F(r) • dr = f
	
r(β)

−f
	
r(α)

,
(7.13)
where C is parametrized by t in the interval [α, β], as depicted in Figure 7.11.
Why?

C
F(r) • dr ≜
β
α
F
	
r(t)

• ˙r(t)dt =
β
α
(∇f)
	
r(t)

• ˙r(t)dt.
By (6.42) in Section 6.4, the multivariable chain rule, and Theorem 7.4(b), this gives

C
F(r) • dr =
β
α
 d
dt

f(r(t))

dt =

f(r(t))
β
α = f(r(β)) −f(r(α)). 2
A consequence of Theorem 7.8 is that for an exact force field, the work done does not
depend on the path taken, that is, the work done is path independent.

564
Advanced Engineering Mathematics
r(β)
r(α)
FIGURE 7.11
Fundamental theorem of line integrals.
Example 7.8
Suppose F =
k
||r||3 r, that is, is an inverse square law force field with proportionality con-
stant k, for example, a gravitational or electric force field, due to an object at the origin.
If C is any piecewise smooth curve not passing through the origin, then

C
F(r) • dr = −
k
||r(β)|| +
k
||r(α)||,
where the curve starts at r(α) and ends at r(β).
Why?
k
||r||3 r = ∇

−
k
||r||

. ⃝
The result in Example 7.8 implies that the work done in going from r(β) to r(α) is the
negative of the work done in going from r(α) to r(β). This is why we call a vector field
conservative if it is the gradient of a potential function, such as F = ∇

−k
||r||

.
Corollary 7.1
If C is a simple, closed, piecewise smooth curve in a domain D in which F is exact, then

C F • dr = 0.
Why? By Theorem 7.8, if C is parametrized by t in the interval [α, β] and F = ∇f in D, then

C F • dr = f
	
r(β)

−f
	
r(α)

= 0, because r(β) = r(α). 2
Figure 7.12 depicts an example of a simple, closed, piecewise smooth curve.
FIGURE 7.12
Simple, closed, piecewise smooth curve.

Integral Theorems, Multiple Integrals, and Applications
565
r(β)
r(α+ β – α)
r(α +β – β)
r(α)
–
FIGURE 7.13
Opposite curve.
7.2.3 Path Direction
Given a piecewise smooth parametrized curve C : r = r(t), α ≤t ≤β, we can define the
opposite curve by
−C : r = r(α + β −t), α ≤t ≤β.
(7.14)
The curve −C consists of the same points as C but is traversed in the opposite direction,
as shown in Figure 7.13.
Theorem 7.9
Suppose C is a piecewise smooth curve. Then

−C
f(r)ds =

C
f(r)ds,
(7.15)

−C
f(r)dx = −

C
f(r)dx,
(7.16)
and

−C
F(r) • dr = −

C
F(r) • dr.
(7.17)
Why? For example, the substitution w = α + β −t gives

−C
F(r) • dr ≜
β
α
F(r(α + β −t)) • ˙r(α + β −t)dt =
α
β
F(r(w)) • ˙r(w)(−dw)
= −
β
α
F(r(w)) • ˙r(w)dw = −

C
F(r) • dr. 2
Physically, (7.17) makes sense: The work against a force field along the opposite path
should be the negative of the work done on the original path.
7.2.4 Other Notations
A parametrized curve C : r = r(t), α ≤t ≤β, has tangent vector
T = ˙r(t)

566
Advanced Engineering Mathematics
wherever it is differentiable. Where the tangent vector is not zero, the curve has unit
tangent vector:
T ≜
1
||˙r(t)|| ˙r(t) = 1
ds
dt
˙r(t).
It follows that

C
F(r) • dr =

C
	
F(r) • T

ds,
(7.18)
because
F(r) • dr ≜F(r) • (˙r)dt = F(r) •
ds
dt
T

dt =
	
F(r) • T

 ds
dt

dt ≜
	
F(r) • T

ds.
A third example of a line integral of a vector field is if E is an electrostatic field, that is,
the field due to some stationary electric charges, measured in newtons/coulomb. Because
E is the sum of inverse square law vector fields, it is exact in any domain D that contains
none of the charges. Given two points P1, P2, neither of which is at any of the charges,

C
E • dr
is the same for all piecewise smooth curves C that remain in such a charge-free domain D,
start at P1 and end at P2. So, we can define
ϕ(P1, P2) ≜−
P2

P1
E • dr ≜−

C
E • dr
for any such curve C. Measured in volts, that is, joules/coulomb, ϕ(P1, P2) is called the
electrostatic potential difference between P1 and P2. Moving a charge of one coulomb
through a potential difference of one volt requires one joule of work.
Suppose C is a simple, closed, piecewise smooth curve bounding a planar region D. Let
B be the magnetic flux density and let I be the current through D. Then in a vacuum,
Amp`ere’s circuital law states that

C
B • dr = μ0I.
The magnetic permeability is μ0 = 4π × 10−7 m kg s−2A−2 = 4π
c , where c is the speed of
light in a vacuum, lengths are measured in meters (m), A stands for amp`eres, and I is
measured in amp`eres.

Integral Theorems, Multiple Integrals, and Applications
567
Example 7.9
An infinitely long conducting wire of radius a has circular cross sections. Find the
magnetic flux density B.
Method: Define the circle
C : r = a ˆer = a(cos θ ˆı + sin θ ˆj), 0 ≤θ ≤2π;
hence, dr
dθ = a ˆeθ. Assuming the wire has the z-axis as its longitudinal axis, the magnetic
field is B = B0 ˆeθ, where B0 is a constant. We calculate
μ0I =

C
B • dr =
2π

0
(B0 ˆeθ) • (a ˆeθ)dθ = 2πaB0.
So
B0 = μ0 I
2πa,
and thus that
B =
μ0 I
2πa

ˆeθ. ⃝
Example 7.10
Explain why the work done by a force F on a particle equals the net change in kinetic
energy (KE).
Method: Newton’s second law of motion says in general that
m ¨r = F,
so the work done by the force is
W =

C
F • dr ≜
β
α
F(r) • ˙rdt =
β
α
m¨r • ˙rdt =
β
α
1
2 m d
dt

||˙r||2 
dt =
1
2 m ||˙r||2
β
α
= 1
2 m ||˙r(β)||2 −1
2 m ||˙r(α)||2.
So, the total work done equals the net change of the KE. ⃝
7.2.5 Problems
1. (a) If C is a straight line segment, parametrize it, calculate

C ds, and find that it
gives the length of the line segment.
(b) If C is an arc of a circle of radius a from θ0 to θ1, parametrize it, calculate

C ds,
and find that it gives the length of the arc, that is, a · (θ1 −θ0).
2. A fly’s position vector is r(t) = −cos 2t ˆı + sin 2t ˆj + t ˆk. Find the exact distance
traveled by the fly, that is, the arclength, when it travels from the point (−1, 0, 0)
to the point (−1, 0, 2π).
3. Find the exact arclength of the curve parametrized by r(t) = 1
2 t2 ˆı+
√
6 t ˆj+3 ln(t) ˆk,
1 ≤t ≤e.

568
Advanced Engineering Mathematics
2.5
y
y=1+—x3/2
2
3
2.0
1.5
x
0.5
1.0
1.5
2.0
FIGURE 7.14
Problem 7.2.5.6.
4. Find the exact arclength of the curve y = ln(cos x) over the interval 0 ≤x ≤π
4 .
5. A helical curve C is given by r(t) = cos 2t ˆı + sin 2t ˆj + t ˆk for 0 ≤t ≤π
4 . Evaluate

C x z ds.
6. Set up, with as much specificity as possible, but do not evaluate, a definite integral
that would give the value of

C x ds, where the curve C is shown in Figure 7.14.
Your final conclusion should be in the form

d
with blanks
filled in.
7. A wire bent in the shape of the quarter circle x2 + y2 = a2 in the first quadrant is
made of metal whose density is ϱ = (1 + x2 + y2 −xy) g/m. Find the exact center
of mass of the wire.
8. A wire bent in the shape of the half circle x2 + y2 = 3, y ≥0 is made of a material
whose density is ϱ = (1 + cos2 x)y g/m. Find the total mass of the wire.
9. A wire bent in the shape of the quarter circle x2 + y2 = a2 in the first quadrant is
made of metal of varying composition whose linearly varying density is ϱ = 1 +
0.2 θ g/m. Find the exact center of mass of the wire.
10. Let C be the semicircle that runs from (a, 0) to (−a, 0) and F = ex ˆı −y ˆj. Find the
value of

C F • dr.
11. For the curve shown in Figure 7.15a, evaluate (a)

C xy2 ds, (b)

C xy2 dx, and
(c)

C ∇(xy2) • dr.
12. Find the circulation of the velocity field v = ˆer −r ˆeθ around the circle x2 + y2 = 4.
13. Find the circulation of v = y ˆı −x ˆj + xy ˆk around the half disk x2 + y2 = a2, x ≥0.
14. (a) Explain why the circulation around the circle C : r = a(cos θ ˆı +sin θ ˆj), 0 ≤θ ≤
2π, of a planar vector field F(r, θ) ≜Frˆer + Fθ ˆeθ is  = a
 2π
0
Fθ(a, θ) dθ.
(b) A streaming motion past a circular cylinder r = a may have fluid velocity
v = U

1 −a2
r2

cos θ ˆer + U

κ
r + (−1 −a2
r2 ) sin θ

ˆeθ,

Integral Theorems, Multiple Integrals, and Applications
569
a
3
2
1
1
2
3
x
–2
–1
x
–a
y
y
(a)
(b)
FIGURE 7.15
(a) Problem 7.2.5.11, (b) Problem 7.2.5.18.
where κ is a constant. Explain why the circulation around the cylinder is
2πκ U. [By the way, the velocity vector “at infinity” is given by V∞= Uˆı.]
15. (a) Find a function f(x, y, z) such that
∇f = F ≜y2 ˆı + (2xy + e3z) ˆj + (3ye3z + 1)ˆk.
(b) Find the value of

C F • dr, where C is an unspecified piecewise smooth
parametrized curve from the point (x, y, z) = (1, 0, 1) to the point (0, 1, 1).
16. Find the work done by the force field F = (1 + 4x3y3) ˆı + 3x4y2 ˆj to move a particle
from the point (x, y) = (−1, 2) to the point (1, 0).
17. Find the work done against the gravitational field −mMG
||r||3 r along an unspecified
piecewise smooth parametrized curve from the point (x, y, z) = (−2, 1, 1) to the
point (2, 3, −1).
18. Evaluate

C(y ˆı + (x + 2) ˆj) • dr, where C is shown in Figure 7.15b.
19. Evaluate

C

x
x2+y2 ˆı +
y
x2+y2 ˆj

• dr, where C is shown in Figure 7.16.
20. Evaluate

C

−
y
x2+y2 ˆı +
x
x2+y2 ˆj

• dr, where C is shown in Figure 7.16. [Hint: The
function arctan
	 y
x

is undefined at x = 0, and the function arctan

x
y

is undefined
at y = 0. But there is a trigonometric identity that arctan a+arctan

1
a

≡π
2 . So, we
can define a function φ(x, y) ≜

−arctan
	
x/y

,
y ̸= 0
arctan
	
y/x

−π
2 ,
x ̸= 0

.]
21. The bob of a pendulum, of length ℓ, starts at angular position ϕ = α and travels
to ϕ = 0, as depicted in Figure 7.17. The bob has weight w, so the force of grav-
ity is always F = −w ˆj. What is the work done by gravity during the travel of
the bob, assuming the pendulum arm is of negligible weight? [Hint: What is the
relationship between ϕ and the usual angle θ in polar coordinates?]
22. Use the method of substitution for integration of a function of a single variable
to explain why line integration with respect to arclength does not depend on the
parametrization of the curve. Specifically, if one curve has two piecewise smooth

570
Advanced Engineering Mathematics
1
1
2
3
y
x
2
FIGURE 7.16
Problems 7.2.5.19 and 7.2.5.20.
–ω j
ℓ
j=0
j = α
FIGURE 7.17
Problem 7.2.5.21.
parametrizations C1 : r = r1(t), α1 ≤t ≤β1, C1 : r = r2(t), α2 ≤t ≤β2 for which
there is a piecewise smooth, increasing function w = w(t) so that r2(w(t)) ≡r1(t),
then for all continuous functions f = f(x, y, z),

C1
f(x, y, z) ds =

C2
f(x, y, z) ds.
Note that, in particular, w(α1) = α2 and w(β1) = β2.
7.3 Double Integrals, Green’s Theorem, and Applications
Suppose mass is painted onto a planar region. In this section we will see how the concept
of “double integral” enables us to find the total mass on that region, its moments, and its

Integral Theorems, Multiple Integrals, and Applications
571
d
c
x
y
a
b
FIGURE 7.18
Rectangle.
moments of inertia. We can do this even if the mass density, ϱ, is a function of the x and y
coordinates.
In this chapter, we will also see how to use polar coordinates to more easily work with
a planar region that has circular geometry, for example, a disk, central sector of a disk,
annulus, or sector of an annulus. We will see how many of the concepts and results of
calculus for functions of a single variable generalize to functions of two or three variables.
Suppose the mass density is ϱ = ϱ(x, y) and is given for (x, y) in the rectangle (Figure 7.18)
D ≜{(x, y) : a ≤x ≤b, c ≤y ≤d}.
(7.19)
For example, mass may be painted on with a variable thickness or with a variable mixture
of two materials with different densities. Break up the rectangle into many small sub-
rectangles and assume the mass density is close to constant on each subrectangle. We can
approximate the total mass on the rectangle by
M ≈
m

i=1
n

j=1
ϱ(x∗
i , y∗
j ) △xi △yj,
(7.20)
where (x∗
i , yj) is a sampling point chosen in the ijth subrectangle:
{(x, y) : xi−1 ≤x ≤xi, yj−1 ≤y ≤yj}.
In (7.20), the total mass is approximated by a double Riemann sum.
Take the limit as both m, n →∞and each △xi, △yj →0 to get exact total mass
M =

D
ϱ(x, y)dxdy.
(7.21)
Three other notations for this are
M =

D
ϱ(x, y)dA,

572
Advanced Engineering Mathematics
where dA is an element of area,
M =
d
c
b
a
ϱ(x, y)dxdy,
and
M =
b
a
d
c
ϱ(x, y)dydx.
Notice that changing the order of limits of integration from having x on the inside to
having x on the outside corresponds to the element of area changing from dxdy to dydx.
If ϱ has units of mass per unit area, then in approximation (7.20), M has units of mass
because the terms in the sum have the form
ϱ(x∗
i , y∗
j )
mass

area

△xi(
length) △yj(
length).
As a practical matter, none of these notations tell us how to calculate the value of a
double integral on a rectangle. For that, we have the following
Theorem 7.10
Suppose ϱ(x, y) is continuous on the rectangle D given in (7.19). Then
M =
d
c
b
a
ϱ(x, y)dxdy =
d
c
⎛
⎝
b
a
ϱ(x, y)dx
⎞
⎠dy,
(7.22)
which is called an iterated integral. The latter can be evaluated using a generalization of
the fundamental theorem of calculus: The inner integral,
 b
a ϱ(x, y)dx, is the function of y
given by
b
a
ϱ(x, y)dx = F(b, y) −F(a, y),
where F(x, y) is any function that satisfies ∂F
∂x = ϱ(x, y).
The iterated integration process described in Theorem 7.10 corresponds to first inte-
grating the mass on horizontal line segments from (x, y) = (a, y) to (x, y) = (b, y) and then
“summing up” by integrating from y = c to y = d.
As in Example 6.23 in Section 6.4, a function F(x, y) that satisfies ∂F
∂x = ϱ(x, y) is called an
anti-partial derivative with respect to x. We denote
F(x, y) =

ϱ(x, y)∂x
although other authors write “dx” and not “∂x.”

Integral Theorems, Multiple Integrals, and Applications
573
Example 7.11
If the mass density in μg per cm2 is ϱ(x, y) = 1 + 0.5x + 0.2y, find the total mass on the
rectangle 0 ≤x ≤1 cm, 0 ≤y ≤2 cm depicted in Figure 7.19.
Method: The total mass is
M =

D
ϱdA μg/(
cm2) ×
cm2 =
2
0
1
0
ϱ(x, y)dxdy μg =
2
0
⎛
⎝
1
0
(1 + 0.5x + 0.2y)dx
⎞
⎠dy μg.
For the inner integration, anti-partial differentiation gives
F(x, y) =
1
0
ϱ(x, y)∂x =
1
0
(1 + 0.5x + 0.2y)∂x =

x + 0.25x2 + 0.2yx
1
0
=
	
1 + 0.25 + 0.2y

−(0 + 0 + 0) = 1.25 + 0.2y.
So,
M =
2
0
⎛
⎝
1
0
(1 + 0.5x + 0.2y)dx
⎞
⎠dy μg =
2
0
(1.25 + 0.2y)dy μg =

1.25y + 0.1y22
0 μg
= 2.5 + 0.4 −(0 + 0) μg = 2.9 μg. ⃝
y
x
2.0
1.0
FIGURE 7.19
Variable mass density on a rectangle.

574
Advanced Engineering Mathematics
In Example 7.11, we could have integrated first with respect to y:
M =
1
0
⎛
⎝
2
0
(1 + 0.5x + 0.2y)dy
⎞
⎠dx μg =
1
0

y + 0.5xy + 0.1y22
0 dx μg =
1
0
(2.4 + x) dx μg
=

2.4x + 0.5x21
0 μg = 2.9 μg.
It is not a coincidence that we got the same final conclusion by two different orders of
integration.
Theorem 7.11
(Fubini’s theorem) If f(x, y) is continuous on the rectangle D given in (7.19), then
d
c
⎛
⎝
b
a
f(x, y)dx
⎞
⎠dy =
b
a
⎛
⎝
d
c
f(x, y)dy
⎞
⎠dx.
7.3.1 Double Integral as Volume
Recall from Section 7.1 that a single integral of a function of a single variable can be inter-
preted as the area of a region. Similarly, if f(x, y) ≥0 for all (x, y) in a planar region D,
then the solid in (x, y, z)-space that is bounded on the top by the surface z = f(x, y) and is
bounded on the bottom by the region D in the plane z = 0 has
Volume =

D
f(x, y)dA.
For example, we can recycle Example 7.11 to get the next example.
Example 7.12
Find the total volume of the solid bounded on the top by the plane z = 1+0.5x+0.2y and
on the bottom by the rectangle 0 ≤x ≤1 cm, 0 ≤y ≤2 cm, z = 0. Assume z is measured
in centimeters, too.
Method: The volume is
Volume =
2
0
⎛
⎝
1
0
(1 + 0.5x + 0.2y)dx
⎞
⎠dy cm3 = 2.9 cm3,
using the integration result of Example 7.11. ⃝
Why are the units cm3? Because an approximation of the volume is given by a double
Riemann sum,
M ≈
m

i=1
n

j=1
(volume of thin parallelepiped),
where the parallelepipeds have base area (△x cm)(△y cm) and height z = f(x∗, y∗) cm.

Integral Theorems, Multiple Integrals, and Applications
575
Example 7.13
Use a double Riemann sum and sampling at lower right endpoints of the subrectangles
to approximate the volume in Example 7.12.
Method: The volume is
Volume =

D
(1 + 0.5x + 0.2y)dA cm3.
For example, we may break up the region D into eight subrectangles, each of area
(0.5 cm × 0.5 cm) and sample the integrand, f(x, y) = 1 + 0.5x + 0.2y, at lower right
endpoints, as shown in Figure 7.20. The ×’s mark the points in D where f(x, y) is
sampled, that is, at (x, y) = (0.5, 0), (1, 0), (0.5, 0.5), (1, 0.5), (0.5, 1), (1, 1), (0.5, 1.5), (1, 1.5).
Correspondingly,
Volume ≈
	
f(0.5, 0) + f(1, 0) + f(0.5, 0.5) + f(1, 0.5) + f(0.5, 1) + f(1, 1)
+ f(0.5, 1.5) + f(1, 1.5)

cm × (0.25 cm2)
= (1.25 + 1.5 + 1.35 + 1.6 + 1.45 + 1.7 + 1.55 + 1.8) cm × (0.25 cm2)
= 3.05 cm3. ⃝
The exact volume is 2.9 cm3. Considering how coarsely we sampled the function’s
values, a relative error of 0.15
2.9 ≈5% is pretty small!
In Section 7.4, we will see how to also calculate volumes using triple integrals.
0.5
2.0
y
x
1.5
1.0
0.5
1.0
FIGURE 7.20
Sampling in a double Riemann sum.

576
Advanced Engineering Mathematics
Theorem 7.12
(Integral of a product function) If f(x, y) = g(x)h(y) is a product of continuous functions on
the rectangle D given in (7.19), then
d
c
b
a
g(x)h(y)dxdy =
⎛
⎝
d
c
h(y)dy
⎞
⎠
⎛
⎝
b
a
g(x)dx
⎞
⎠.
Theorem 7.12 makes it faster and easier to calculate integrals in a special case.
Suppose the region is not a rectangle. We can generalize the idea of breaking up a region
into horizontal or vertical line segments, as we did in writing a double integral as an iter-
ated integral in Example 7.11. Indeed, we used this idea to find areas in calculus for a
function of a single variable.
Example 7.14
Express the region shown in Figure 7.21 using inequalities.
Method: For each x in the interval [−1, 2], inside the region is a vertical line segment
between the points (x, y) = (x, −1 + x2) and (x, 1 + x), as shown in Figure 7.21. So, the
region is
D =

(x, y) : −1 ≤x ≤2, −1 + x2 ≤y ≤1 + x

. ⃝
By the way, if we had not been given the picture of the region but instead we had
been told it is the finite region between the curves y = 1 + x and y = −1 + x2, then the first
thing to do would be to find the points of intersection. How? By solving 1+x = y = −1+x2,
that is, 1 + x = −1 + x2, that is, 0 = x2 −x −2 = (x + 1)(x −2), hence x = −1 and x = 2,
so the points of intersection are (x, y) = (−1, 0) and (2, 3).
–1.0
–0.5
1
2
3
y
(x, 1 +x)
(x, –1 +x2)
y =1+x
y=–1+x2
0.5
1.0
1.5
2.0
x
–1
FIGURE 7.21
Example 7.14.

Integral Theorems, Multiple Integrals, and Applications
577
Example 7.15
Find the total mass on the region shown in Figure 7.21, assuming the mass density is
ϱ = 1 + 0.5x + 0.2y.
Method: Using the result of Example 7.13,
M =

D
(1 + 0.5x + 0.2y)dA =
2
−1
⎛
⎝
1+x

−1+x2
(1 + 0.5x + 0.2y)dy
⎞
⎠dx
=
2
−1

y + 0.5xy + 0.1y21+x
−1+x2 dx =
2
−1

(1 + x + 0.5x(1 + x) + 0.1(1 + x)2)
−(−1 + x2 + 0.5x(−1 + x2) + 0.1(−1 + x2)2)

dx
= · · · =
2
−1

2 + 2.2x −0.2x2 −0.5x3 −0.1x4
dx
=

2x + 1.1x2 −0.2
3 x3 −0.125x4 −0.02x5
2
−1
= · · · = 6.165. ⃝
Example 7.16
Find the volume of the solid tetrahedron whose vertices are (0, 0, 0), (12, 0, 0), (0, 4, 0),
and (0, 0, 3).
Method: This tetrahedron, shown in Figure 7.22, is the solid bounded (a) on the bottom
by a triangular region D in the z = 0 plane, that is, the xy-plane, and (b) on the top by
part of a plane above D. First, the region D has vertices (0, 0), (12, 0), and (0, 4) in the
xy-plane and is shown in Figure 7.23.
Using vertical line segments, we can write
D =

(x, y) : 0 ≤x ≤12, 0 ≤y ≤4 −x
3

.
Next, the top is part of the plane that satisfies the equation Ax + By + Cz = E. As in
Section 6.1, we substitute in the three points on the plane to get
⎧
⎨
⎩
12A + 0 + 0 = E
0 + 4B + 0 = E
0 + 0 + 3C = E
⎫
⎬
⎭.
It’s convenient to let E = 12 because that gives integer values A = 1, B = 3, C = 4. So, the
plane is
x + 3y + 4z = 12.
So, part of the top of the solid is on the plane
z = 3 −x
4 −3y
4 ≜f(x, y).

578
Advanced Engineering Mathematics
3
4
3
2
1
0
y
z
x
2
1
00
5
10
FIGURE 7.22
Solid tetrahedron in Example 7.16.
y
x
4
y =4– x
3
12
FIGURE 7.23
D in Example 7.16.
So, the volume of the solid is
Volume =
12

0
⎛
⎜⎝
4−x
3

0

3 −x
4 −3y
4

dy
⎞
⎟⎠dx =
12

0
'
3y −xy
4 −3y2
8
(4−x
3
0
dx
= · · · =
12

0

6 −x + x2
24

dx =
'
6x −x2
2 + x3
72
(12
0
= · · · = 24. ⃝
The tetrahedron in Figure 7.22 was plotted by the Mathematica command:
Graphics3D[Polygon[{{12, 0, 0}, {0, 4, 0}, {0, 0, 3},{0, 0, 0}}], Axes →True],
after which we used the mouse to rotate the view.

Integral Theorems, Multiple Integrals, and Applications
579
A type I region in the xy-plane is one that can be written in the form
D = { (x, y) : a ≤x ≤b, φ1(x) ≤y ≤φ2(x) },
where φ1(x) ≤φ2(x) for a ≤x ≤b. Expressing a region in the type I form is like writing the
region as a union of vertical line segments.
A type II region in the xy-plane is one that can be written in the form
D = { (x, y) : c ≤y ≤d, ψ1(y) ≤x ≤ψ2(y) },
where ψ1(y) ≤ψ2(y) for c ≤y ≤d. Expressing a region in the type II form is like writing
the region as a union of horizontal line segments.
Sometimes we can write a region in both type I and type II forms. This may make our
integration work possible or at least a lot easier.
Example 7.17
Rewrite
5
0
⎛
⎝
10

2y
e−x2dx
⎞
⎠dy
as an integral on a type I region in order to evaluate it.
Method: It helps to draw the type II region D = {(x, y) :
0 ≤y ≤5, 2y ≤x ≤10}
(Figure 7.24) and then express it as a union of vertical line segments (Figure 7.25):
(2y, y)
(10, y)
x
5
y
10
FIGURE 7.24
Horizontal line segment in D.
(x,    )
x
2
5
y
x
(x, 0)
10
FIGURE 7.25
Vertical line segment in D.

580
Advanced Engineering Mathematics
Because
D =

(x, y) : 0 ≤x ≤10, 0 ≤y ≤x
2

,
5
0
⎛
⎝
10

2y
e−x2dx
⎞
⎠dy =
10

0
⎛
⎜⎝
x
2
0
e−x2dy
⎞
⎟⎠dx =
10

0

ye−x2 x
2
0 dx =
10

0
x
2e−x2dx
=

−1
4e−x210
0
= 1
4(1 −e−100). ⃝
Definition 7.11
The average value of a function f on a planar region D is
¯f ≜
1
Area of D

D
f(r) dA =

D f(r) dA

D 1 dA .
7.3.2 Polar Coordinates
Polar coordinates are particularly useful in describing and/or integrating on regions in the
plane that have a boundary curve(s) that is an arc of a circle.
Example 7.18
Describe the region shown in Figure 7.26:
(a) In polar coordinates
(b) As a type II region in rectangular coordinates
Method:
(a) Think of the region as a union of radial line segments, as shown in Figure 7.27:
D = {(r, θ) : 0 ≤θ ≤3π
4 , 0 ≤r ≤a}.
y
(            )
–
,
a
2
√
x
a
a
2
√
a
FIGURE 7.26
Wedge.

Integral Theorems, Multiple Integrals, and Applications
581
(            )
–
,
a
2
√
a
2
√
y
x
a
a
FIGURE 7.27
Wedge as a union of radial line segments.
(            )
–
,
a
2
√
a
2
√
y
x
(√a2– y2, y)
a
(–y, y)
a
FIGURE 7.28
Wedge as union of horizontal line segments.
(b) Think of the region as a union of two domains, each of which consists of horizontal
line segments:
D = D1 ∪D2 =

(x, y) : 0 ≤y ≤a
√
2
, −y ≤x ≤

a2 −y2

∪

(x, y) :
a
√
2
≤y ≤a, −

a2 −y2 ≤x ≤

a2 −y2

. ⃝
Figure 7.28 shows a typical horizontal line segment in the set D1.
We want to do integrals using polar coordinates when that gives a simpler description
of the region D; but, also, sometimes polar coordinates can simplify the integrand
f(x, y) = f(r cos θ, r sin θ).
But, we also need to know the element of area, dA, expressed in polar coordinates:
dA = r dr dθ,
as we derived in Section 6.6.
To summarize,

D in (x,y)
f(x, y)dA =

D in (r,θ)
f(r cos θ, r sin θ) r dr dθ

582
Advanced Engineering Mathematics
Example 7.19
Find the volume of that part of the ball 0 ≤x2 + y2 + z2 ≤a2 that sits above the disk
0 ≤x2 + y2 ≤1, z = 0, assuming a > 1.
Method:
The region D is the disk 0 ≤x2 + y2 ≤1. The top surface of the solid is z =
+

a2 −x2 −y2 =

a2 −r2 in polar coordinates. The volume of the solid is
Volume =

D
z dA =
2π

0
1
0

a2 −r2 rdr dθ =
⎛
⎝
2π

0
dθ
⎞
⎠
⎛
⎝
1
0

a2 −r2 rdr
⎞
⎠.
After doing the first integral factor and making the substitution w = a2 −r2 in the second
factor,
Volume = 2π
a2−1

a2
√w

−dw
2

= 2π

−1
3w3/2
a2−1
a2
= 2π
3 (a3 −(a2 −1)3/2). ⃝
7.3.3 Green’s Theorem
Green’s theorem is the first of three major theorems that relate multiple integrals to
derivatives. Like the fundamental theorem of calculus (Theorem 7.4) and the fundamental
theorem of line integrals (Theorem 7.8), a multiple integral on a region can be evaluated
using the boundary value(s) of anti-derivatives, in some sense.
Specifically, Green’s theorem says that there is an amazing and useful relationship
between the integral around C of a planar vector field and the integral on D of certain
of its derivatives.
Theorem 7.13
(Green’s Theorem) Suppose D is a planar region whose boundary is a simple, closed, piece-
wise smooth, positively oriented curve C. If planar vector field F(r) = P(x, y)ˆı + Q(x, y) ˆj is
continuously differentiable on D and its boundary C, then

C
F • dr =

C
(Pdx + Qdy) =

D
∂Q
∂x −∂P
∂y

dA.
(7.23)
We will not explain this result here, but we will give references at the end of the section.
Also, in the appendix, we will derive Green’s theorem for any rectangle D and any F
that satisfies the smoothness hypothesis. Our emphasis here will be on applying Green’s
theorem.
Example 7.20
(Example 7.7 in Section 7.2 again) Find the total circulation around the triangle with
“nose up” shown in Figure 7.10, assuming the wind has constant speed U0.

Integral Theorems, Multiple Integrals, and Applications
583
Method: The region D is the triangle shown in Figure 7.10. The total circulation is

C
(U0 cos ϕ ˆı + U0 sin ϕ ˆj) • dr
=

D
 ∂
∂x [U0 sin ϕ] −∂
∂y [U0 cos ϕ]

dA =

D
(0 −0)dA = 0.
Corollary 7.2
If F is any constant vector, then

C F • dr = 0 for any simple, closed, piecewise smooth
curve C.
Corollary 7.3
If F is a planar vector field that is exact and continuously differentiable on a planar region
D and C is a simple, closed, piecewise smooth curve strictly inside D, then

C F • dr = 0.
Why? Because F is exact, it has a potential function f = f(r), that is, F = ∇f. It follows that
P(x, y)ˆı + Q(x, y) ˆj = F = ∂f
∂x ˆı + ∂f
∂y ˆj.
By Green’s theorem,

C
F • dr =

C
 ∂f
∂xdx + ∂f
∂ydy

=

D
 ∂
∂x
 ∂f
∂y

−∂
∂y
 ∂f
∂x

dA =

D
0 dA = 0,
by Clairaut’s theorem (Theorem 6.2) in Section 6.4. 2.
Corollary 7.4
If D is a planar region bounded by a simple, closed, positively oriented, piecewise smooth
curve C, then the area of D is given by
Area =

C
xdy =

C
(−y)dx = 1
2

C
(−ydx + xdy).
Why? For example, by Green’s theorem,
1
2

C
(−ydx + xdy) = 1
2

D
 ∂
∂x [x] −∂
∂y

−y

dA = 1
2

D
2dA = Area of D. 2

584
Advanced Engineering Mathematics
7.3.4 Comparison with Single Variable Calculus
Suppose D is a type II planar region,
D = {(x, y) : c ≤y ≤d, ψ1(y) ≤x ≤ψ2(y)},
where ψ1(y) ≤ψ2(y) for c ≤y ≤d. For the sake of specificity, assume ψ1(d) = ψ2(d).
In a single variable calculus course, we found that the area of D is
Area =
d
c
(ψ2(y) −ψ1(y))dy.
(7.24)
As shown in Figure 7.29, we have C = C1 ∪C2 ∪C3, and we can parametrize the curves by
C1 : r1 = ψ1(c + d −t)ˆı + (c + d −t) ˆj, c ≤t ≤d,
C2 : r2 = ψ2(y)ˆı + y ˆj, c ≤t ≤d,
C3 : r3 = (ψ2(c) + t(ψ1(c) −ψ2(c))ˆı + c ˆj, 0 ≤t ≤1.
Note that C3 is a horizontal line segment going from the point (ψ1(c), c) to the point
(ψ2(c), c).
According to Corollary 7.4, we should have the area of D given by
Area =

C1
xdy +

C2
xdy +

C3
xdy
=
d
c
ψ1(c + d −t)(−dt) +
d
c
ψ2(y)dy +
1
0
(ψ2(c) + t(ψ1(c) −ψ2(c))(0 · dt).
y
x
d
c
x=ψ1(y)
x=ψ2(y)
a
b
1
2
3
FIGURE 7.29
Area of type II region.

Integral Theorems, Multiple Integrals, and Applications
585
In the first integral, make the substitution w = c + d −t. So,
Area =
c
d
ψ1(w)(dw) +
d
c
ψ2(y)dy = −
d
c
ψ1(w)(dw) +
d
c
ψ2(y)dy =
d
c
(ψ2(y) −ψ1(y))dy.
This agrees with the abstract formula (7.24) from a single variable calculus course. 2
7.3.5 Green’s Theorem for an Annulus
We can extend Green’s theorem to a planar region that is not bounded by a simple closed
curve.
Example 7.21
Extend Green’s theorem to the annulus a < r < b shown in Figure 7.30.
Method: We can decompose
D ≜{(r, θ) : 0 ≤θ ≤2π, a < r < b} = D+ ∪D−,
as shown in Figure 7.30. In each of D±, we can apply Green’s theorem 7.13: If
F = P(x, y)ˆı + Q(x, y) ˆj is continuously differentiable on D and its boundary curves Ca
and Cb, then

D+
∂Q
∂x −∂P
∂y

dA =

C+
a
F • dr +

C+
1
F • dr +

C+
b
F • dr +

C+
2
F • dr
and

D−
∂Q
∂x −∂P
∂y

dA =

C−
a
F • dr +

C−
2
F • dr +

C−
b
F • dr +

C−
1
F • dr.
But, C−
1 = −C+
1 and C−
2 = −C+
2 , so

D
∂Q
∂x −∂P
∂y

dA =

D+
∂Q
∂x −∂P
∂y

dA +

D−
∂Q
∂x −∂P
∂y

dA
=

C+
a
F • dr +




C+
1
F • dr +

C+
b
F • dr +




C+
2
F • dr +

C−
a
F • dr +




C−
2
F • dr +

C−
b
F • dr +




C−
1
F • dr,
b
=
+
2
+
a
–
b
–
–   a
   a
+
b
+
1
+
2
–
1
–
FIGURE 7.30
Decomposing an annulus.

586
Advanced Engineering Mathematics
half of the terms canceling by Theorem 7.9 in Section 7.2. So,

D
∂Q
∂x −∂P
∂y

dA =

C+
a
F • dr +

C−
a
F • dr +

C+
b
F • dr +

C−
b
F • dr.
The curve Cb is traversed counterclockwise using C+
b ∪C−
b , so

C+
b
F • dr +

C−
b
F • dr =

Cb
F • dr.
The curve −Ca is traversed clockwise using C+
a ∪C−
a , so

C+
a
F • dr +

C−
a
F • dr =

−Ca
F • dr = −

Ca
F • dr.
Putting everything together,

D
∂Q
∂x −∂P
∂y

dA =

Cb
F • dr −

Ca
F • dr. ⃝
7.3.6 Green’s Theorem in Polar Coordinates
If F ≜Frˆer + Fθ ˆeθ is integrated around a curve given in polar coordinates, then we can use
Green’s theorem expressed in polar coordinates. To that end, use results from Section 6.7
to see that
P(x, y)ˆı + Q(x, y) ˆj = F = Frˆer + Fθ ˆeθ = Fr(cos θ ˆı + sin θ ˆj) + Fθ(−sin θ ˆı + cos θ ˆj)
= (Fr cos θ −Fθ sin θ)ˆı + (Fr sin θ + Fθ cos θ) ˆj).
Using (6.71), we calculate
∂Q
∂x =

cos θ ∂
∂r −sin θ
r
∂
∂θ

[Fr sin θ + Fθ cos θ]
= cos θ
∂Fr
∂r · sin θ + ∂Fθ
∂r · cos θ

−sin θ
r
∂Fr
∂θ · sin θ + Fr cos θ + ∂Fθ
∂θ · cos θ −Fθ sin θ

.
Similarly,
−∂P
∂y = −

sin θ ∂
∂r + cos θ
r
∂
∂θ

[Fr cos θ −Fθ sin θ]
= −sin θ
∂Fr
∂r · cos θ −∂Fθ
∂r · sin θ

−cos θ
r
∂Fr
∂θ · cos θ −Fr sin θ −∂Fθ
∂θ · sin θ −Fθ cos θ


Integral Theorems, Multiple Integrals, and Applications
587
Summing up,
∂Q
∂x −∂P
∂y = · · · = cos2 θ ∂Fθ
∂r −sin2 θ
r
∂Fr
∂θ
:::::::::
+ sin2 θ
r
Fθ
+ sin2 θ ∂Fθ
∂r −cos2 θ
r
∂Fr
∂θ
:::::::::
+ cos2 θ
r
Fθ
= · · · = 1
r
∂
∂r [rFθ] −1
r
∂Fr
∂θ .
So, in polar coordinates, the conclusion of Green’s theorem is

C
F • dr =

D
1
r
∂
∂r [rFθ] −1
r
∂Fr
∂θ

rdrdθ =

D
 ∂
∂r [rFθ] −∂Fr
∂θ

dr dθ.
(7.25)
Appendix: Derivation of Green’s Theorem for a Rectangle
Let’s see why Green’s theorem is correct when D is a rectangle, as in (7.19), that is,
D ≜{(x, y) : a ≤x ≤b, c ≤y ≤d}. Let F = P(x, y)ˆı + Q(x, y) ˆj be any continuously differ-
entiable vector field on D and its boundary curve C = C1 ∪· · · ∪C4 shown in Figure 7.31.
Parametrize the curves by
C1 : r1 = (a + t(b −a))ˆı + c ˆj, 0 ≤t ≤1.
C2 : r2 = b ˆı + (c + t(d −c)) ˆj, 0 ≤t ≤1.
C3 : r3 = (b −t(b −a))ˆı + d ˆj, 0 ≤t ≤1.
C4 : r4 = a ˆı + (d −t(d −c)) ˆj, 0 ≤t ≤1.
y
d
c
x
4
1
3
a
b
2
FIGURE 7.31
Green’s theorem on a rectangle.

588
Advanced Engineering Mathematics
We calculate the left-hand side (LHS) of the (desired) conclusion of Green’s theorem:

C
F(r) • dr =

C1
(Pdx + Qdy) +

C2
(Pdx + Qdy) +

C3
(Pdx + Qdy) +

C4
(Pdx + Qdy)
=
1
0
P(a + t(b −a), c)((b −a)dt) +
1
0
Q(b, c + t(d −c))((d −c)dt)
+
1
0
P(b −t(b −a), d)(−(b −a)dt) +
1
0
Q(a, d −t(d −c))(−(d −c)dt).
After making the substitutions w1 = a + t(b −a), w2 = c + t(d −c), w3 = b −t(b −a),
w4 = d −t(d −c), this gives

C
F(r) • dr =
b
a
P(w1, c)dw1 +
d
c
Q(b, w2)dw2 +
a
b
P(w3, d)dw3 +
c
d
Q(a, w4)dw4.
(7.26)
On the other side hand, we calculate the right-hand side (RHS) of the (desired)
conclusion of Green’s theorem:

D
∂Q
∂x −∂P
∂y

dA =
d
c
⎛
⎝
b
a
∂Q
∂x (x, y)dx
⎞
⎠dy −
b
a
⎛
⎝
d
c
∂P
∂y (x, y)dy
⎞
⎠dx
=
d
c

Q(x, y)
b
a

dy −
b
a

P(x, y)
d
c

dx
=
d
c
	
Q(b, y) −Q(a, y)

dy −
b
a
	
P(x, d) −P(x, c)

dx,
that is,

D
∂Q
∂x −∂P
∂y

dA =
b
a
P(x, c)dx +
d
c
Q(b, y)dy −
b
a
P(x, d)dx −
d
c
Q(a, y)dy.
(7.27)
This agrees with (7.26). This explains why Green’s theorem is correct for any rectangle and
any continuously differentiable vector field F. 2
Learn More About It
A good reference is Functions of Several Variables, by Wendell Fleming, Springer-Verlag,
2nd ed., 3rd printing, c⃝1987. Problems 7.3.7.34 and 7.3.7.38 came from Problems 8.7.9
and 8.7.11 on page 361 of that book. The conclusion of Problem 7.3.7.4 is a small part of
an Example on pp. 407–409 in Radiative Heat Transfer, by Michael F. Modest, McGraw-
Hill, Inc., c⃝1993.

Integral Theorems, Multiple Integrals, and Applications
589
TABLE 7.1
f(x, y) Data
x/y
10
12.5
15
17.5
20
0
1
2
3
4
5
1
2
2
3
2
1
2
3
4
3
4
3
3
2
3
2
1
2
4
1
3
4
5
4
7.3.7 Problems
1. Use four subrectangles and sampling at midpoints to approximate the volume
bounded above by the surface z = f(x, y) and below by the rectangle 0 ≤x ≤
4, 10 ≤y ≤20 in the xy-plane, assuming data of values of f(x, y) found in Table 7.1.
Do state what points (x, y) you used to get data about f.
2. Evaluate
 π
6
0
 3
1 xy sin 2x dy dx.
3. Evaluate
 3
1
 4
2 x2exy dy dx.
4. Explain why
π
2
π
2 −
π
0
f(φ, ψ, ) dφ dψ = 2
3(sin  − cos ),
where
f(φ, ψ, ) ≜sin3 φ cos ψ(sin ψ sin  −cos ψ cos )
and  is a constant “scattering” angle. This double integral helps to find the
“phase function” for a diffusely reflecting sphere in the study of radiative heat
transfer. [Hint: Use a cosine addition formula.]
5. Evaluate

D
2
(x+2y)2 dA, where D = {(x, y) : 4 ≤x ≤5, −1 ≤y ≤0}.
6. Evaluate

D cos2(x2 + y2)dA, where D = {(x, y) : 2 ≤x2 + y2 ≤7}.
7. Evaluate

D x dA, where D is shown in Figure 7.32a.
8. Evaluate

D(x −y)dA, where D is shown in Figure 7.32b.
9. Evaluate

D x dA, where D is sketched in Figure 7.33a.
10. Evaluate

D y dA, where D is sketched in Figure 7.33b.
11. Evaluate

D x dA, where D is the finite region in the xy-plane bounded by the
curves x = y2 and x = 2y + 3.
12. Evaluate

D y2exy dA, where D is the triangle in the xy-plane whose vertices are
(0, 0), (0, 2), (4, 2). Draw an appropriate sketch of D showing how you decompose
it into a union of line segments.

Integral Theorems, Multiple Integrals, and Applications
591
(–1, 1)
y
y
x
a
(            )
,
2
6
√
2
2
√
–a
x
(a)
(b)
FIGURE 7.34
(a) Problem 7.3.7.15 and (b) Problem 7.3.7.16.
20. Let D be the half disk 0 ≤x2 + y2 ≤a2, y ≥0. For F = y2 ˆı −2x ˆj, calculate both sides
of the conclusion of Green’s theorem.
21. Evaluate

C
	
cos x sin y dx + sin x cos y dy

,
where C is the positively oriented curve that is the boundary of the triangle in the
xy-plane whose vertices are ( π
2 , π
2 ), (π, π
2 ), ( π
2 , 3π
2 ).
22. Use Green’s theorem to evaluate

C (2y dx + x dy), where C is the positively
oriented curve given in polar coordinates by the cardioid r = 2+sin θ, 0 ≤θ ≤2π.
23. Use Green’s theorem to evaluate

C (y2 dx + 2x2 dy), where C is the positively
oriented boundary of the finite region D that lies between the curves y = x(2 −x)
and y = 0.
24. The area inside the ellipse x2
a2 + y2
b2 = 1 is πab. Use this and Green’s theorem to
evaluate

C F • dr, where C is that ellipse, traversed counterclockwise, and F ≜
(−y + xy2
2 ) ˆı + (x + x2y
2 ) ˆj.
25. Explain why πab is the area inside the ellipse x2
a2 + y2
b2 = 1.
26. Find the circulation of the vector field v = ˆer −r ˆeθ around the circle of radius 2
whose center is at the origin in the xy-plane. [Hint: See Problem 7.2.5.14(a).]
27. Find the area enclosed by the planar curve C : r = 2 cos t ˆı + sin 2t ˆj, 0 ≤t ≤2π.
28. For the special case of a disk D = {(r, θ) : 0 ≤r ≤a, 0 ≤θ ≤2π}, explain why
(7.25) reduces to

C
F • dr =
a
0
aFθ(a, θ) dθ.
[Hints: Use
 1
r
∂
∂r

rFθ

rdr = rFθ,
 
∂Fr
∂θ

dθ = Fr, and 2π-periodicity in θ of the
function Fr = Fr(r, θ).] Note that the conclusion here equals that in Problem
7.2.5.14(a).

590
Advanced Engineering Mathematics
y
y
x
x
3
2
1
1
2
1
2
3
1
2
(a)
(b)
FIGURE 7.32
(a) Problem 7.3.7.7 and (b) Problem 7.3.7.8.
4
3
2
1
x
x
a
y
a
y
x2+ (y–2)2=4
(x –2)2+y2=4
(           )
,
a
2
√
a
2
√
1
2
3
4
(a)
(b)
FIGURE 7.33
(a) Problem 7.3.7.9 and (b) Problem 7.3.7.10.
13. Evaluate
 2
0
 2
x e−y2dy

dx.
14. Find the center of mass of the lamina that is the finite region in the xy-plane
bounded by the curves x = 1, y =
1
x−1, x = 3, and y =
x
x−4, assuming the mass
density is proportional to the distance from the y-axis.
15. Evaluate My for the lamina occupying planar region D shown in Figure 7.34a.
Assume D is a sector of a disk and the mass density is the constant ϱ0.
16. Find the centroid of the thin plate shown in Figure 7.34b.
17. Evaluate

D(x2 −y2)dA, where D is the finite region in the fourth quadrant of the
xy-plane bounded by the circles x2 + y2 = 1 and x2 + y2 = 4.
18. Set up, but do not evaluate, an iterated double integral that gives the volume
enclosed by the ellipsoid 4x2 + 4y2 + z2 = 1. Give an appropriate sketch.
19. Find the volume of the solid bounded below by the disk x2 + y2 = a2 in the z = 0
plane and bounded above by the surface z = 1 −y. Assume that a < 1.

Integral Theorems, Multiple Integrals, and Applications
591
(–1, 1)
y
y
x
a
(            )
,
2
6
√
2
2
√
–a
x
(a)
(b)
FIGURE 7.34
(a) Problem 7.3.7.15 and (b) Problem 7.3.7.16.
20. Let D be the half disk 0 ≤x2 + y2 ≤a2, y ≥0. For F = y2 ˆı −2x ˆj, calculate both sides
of the conclusion of Green’s theorem.
21. Evaluate

C
	
cos x sin y dx + sin x cos y dy

,
where C is the positively oriented curve that is the boundary of the triangle in the
xy-plane whose vertices are ( π
2 , π
2 ), (π, π
2 ), ( π
2 , 3π
2 ).
22. Use Green’s theorem to evaluate

C (2y dx + x dy), where C is the positively
oriented curve given in polar coordinates by the cardioid r = 2+sin θ, 0 ≤θ ≤2π.
23. Use Green’s theorem to evaluate

C (y2 dx + 2x2 dy), where C is the positively
oriented boundary of the finite region D that lies between the curves y = x(2 −x)
and y = 0.
24. The area inside the ellipse x2
a2 + y2
b2 = 1 is πab. Use this and Green’s theorem to
evaluate

C F • dr, where C is that ellipse, traversed counterclockwise, and F ≜
(−y + xy2
2 ) ˆı + (x + x2y
2 ) ˆj.
25. Explain why πab is the area inside the ellipse x2
a2 + y2
b2 = 1.
26. Find the circulation of the vector field v = ˆer −r ˆeθ around the circle of radius 2
whose center is at the origin in the xy-plane. [Hint: See Problem 7.2.5.14(a).]
27. Find the area enclosed by the planar curve C : r = 2 cos t ˆı + sin 2t ˆj, 0 ≤t ≤2π.
28. For the special case of a disk D = {(r, θ) : 0 ≤r ≤a, 0 ≤θ ≤2π}, explain why
(7.25) reduces to

C
F • dr =
a
0
aFθ(a, θ) dθ.
[Hints: Use
 1
r
∂
∂r

rFθ

rdr = rFθ,
 
∂Fr
∂θ

dθ = Fr, and 2π-periodicity in θ of the
function Fr = Fr(r, θ).] Note that the conclusion here equals that in Problem
7.2.5.14(a).

592
Advanced Engineering Mathematics
y
x
c2
c1
3
2
1
y
3
2
1
1
2
3
x
1
2
3
4
(a)
(b)
FIGURE 7.35
(a) Problem 7.3.7.30 and (b) Problem 7.3.7.35.
29. Find the circulation of the vector field v = z ˆı + x ˆj −y ˆk around the triangle whose
vertices are (0, 0, 0), (1, 0, 0), (0, 5, 0).
30. Evaluate

C1(y dx −x dy) +

C2(y dx −x dy), where C1, C2 are shown in Figure 7.35a.
31. D is the twelve square mile region {(x, y) : 0 ≤x ≤4, 0 ≤y ≤3}, where x and y
are measured in miles. Above D is elevated land whose altitude (above sea level)
measurements f(x, y), in tenths of a mile, are given in Table 7.2.
(a) Use exactly six subrectangles and sample at their midpoints to approximate
the volume of land above sea level above D.
(b) Find the average height above sea level of the land over D.
32. A quantity f depends on radius r and angle θ in a disk of radius 4. Data values of
f are given in Table 7.3. Approximate the average value of f on the disk. [Caution:
Be careful as to what function you are integrating in polar coordinates.]
33. Suppose the only things we know about a function f = f(r, θ) are the data f(1, π
8 ) ≈
2, f(1, 3π
8 ) ≈1, f(3, π
8 ) ≈3, f(3, 3π
8 ) ≈5. Approximate the average value of f on the
quarter disk shown in Figure 7.36. [Caution: Be careful as to what function you
are integrating in polar coordinates.]
34. Let C be the positively oriented circle x2 + y2 = 1, D be the disk it encloses, and
planar vector field
F = P ˆı + Q ˆj ≜−
y
x2 + y2 ˆı +
x
x2 + y2 ˆj.
TABLE 7.2
Height f(x, y) Data
x/y
0
0.5
1
1.5
2
2.5
3
0
0
1
2
3
3
2
1
1
1
1
1
2
3
3
2
2
2
2
3
4
4
4
3
3
3
4
4
5
6
5
4
4
2
3
3
4
5
5
4

Integral Theorems, Multiple Integrals, and Applications
593
TABLE 7.3
f(r, θ) Data
r/θ
0
π
2
π
3π
2
0
0
0
0
0
1
3
2
1
2
2
3
1
1
2
3
5
3
3
4
4
6
5
4
5
y
x
4
2
2
4
×
×
×
×
FIGURE 7.36
Problem 7.3.7.33.
Explain why

D

∂Q
∂x −∂P
∂y

dA = 0 and

C F•dr = 2π. Why does this not contradict
Green’s theorem?
The polar moment of inertia for rotation of a plate around the z-axis is
I0 ≜

D
(x2 + y2)ϱ dA.
35. Assume the triangular lamina shown in Figure 7.35b is covered with mass whose
density is ϱ(x, y) = 10 −x + 0.1x2 −y + 0.1y2 g/cm2. Find the center of mass and
the polar moment of inertia of the lamina. You may use technology to evaluate the
integrals.
36. If a plate is actually rotating, then its KE of rotation is
KE ≜1
2 · I0 · ( ˙ϕ)2.

594
Advanced Engineering Mathematics
Use the definition of I0 to rewrite the KE as

D
1
2 · ϱ · (r ˙ϕ)2 dA,
which makes sense as the summing up of the KE of elements of KE 1
2 · (dϱ)(r ˙ϕ)2.
[Note that r ˙ϕ is the linear velocity of a particle traveling around a circle of radius
r at an angular speed of ˙ϕ.]
37. Evaluate
 2
0
 √
4−x2
0

4 −y2 dy dx. First, draw the region D on which the integra-
tion is done. After that, rewrite D as a type II region.
38. Here you will explain why Green’s theorem is true in the special case that D is a
planar region that can be written both as a type I region {(x, y) : a ≤x ≤b, f1(x) ≤
y ≤f2(x)} and as a type II region {(x, y) : c ≤y ≤d, g1(y) ≤x ≤g2(y)}. In either case,
denote by C its positively oriented boundary. Explain why the theorems of calcu-
lus for functions of a single variable and the definition of line integrals guarantee
that for any planar vector field F = P ˆı + Q ˆj,

D
∂Q
∂x dA =

C
Q dy
and

D

−∂P
∂y

dA =

C
P dx.
39. Use Green’s theorem to redo Problem 7.2.5.13.
7.4 Triple Integrals and Applications
Consider a solid box V : a ≤x ≤b, c ≤y ≤d, α ≤z ≤β. If we have mass distributed in
the box with density ϱ(x, y, z) in g/(cm3), then the total mass in the box is
M ≜
β
α
d
c
b
a
ϱ(x, y, z)dxdydz.
(7.28)
The reason for this is similar to why mass on a rectangle is a double integral: Break up V
into small subboxes and assume that in each subbox, the mass density is approximately
constant. This gives
M ≈
m

i=1
n

j=1
ℓ

k=1
ϱ(x∗
i , y∗
j , z∗
k)△xi△yj△zk,
(7.29)

Integral Theorems, Multiple Integrals, and Applications
595
where (x∗
i , y∗
j , z∗
k) is a sampling point chosen in the ijkth subbox:
{(x, y, z) : xi−1 ≤x ≤xi, yj−1 ≤y ≤yj, zk−1 ≤z ≤zk}.
In (7.29), the mass is approximated by a triple Riemann sum.
Take the limit as max{|△xi|, |△yj|, |△zk| : 1 ≤i ≤m, 1 ≤j ≤n, 1 ≤k ≤ℓ} →0 and all
three of m, n, ℓ→∞to get exact total mass given by (7.28), that is,
M =

V
ϱ(x, y, z)dV,
where dV is an element of volume.
There can be many descriptions of more general solids. For example, a solid V is called
type I if it can be written in the form
V = { (x, y, z) : (x, y) in D, φ1(x, y) ≤z ≤φ2(x, y) },
where D is a region in the xy-plane. In this case, we get
M =

V
ϱ(x, y, z)dV =

D
⎛
⎝
φ2(x,y)

φ1(x,y)
ϱ(x, y, z)dz
⎞
⎠dA.
Example 7.22
For the solid tetrahedron of Example 7.16, find the centroid (center of mass, assum-
ing constant density) and the polar moment of inertia (for rotation around the z-axis),
that is,
I0 ≜

V
(x2 + y2)ϱdV.
(7.30)
Method: The density function is assumed to be constant: ϱ = ϱ0. From Example 7.16, we
can see that this tetrahedron can be described as the type I solid:
V =

(x, y, z) : (x, y) in D, 0 ≤z ≤3 −x
4 −3y
4

,
where triangular region D can be described as the type I region:
D =

(x, y) : 0 ≤x ≤12, 0 ≤y ≤4 −x
3

.
So, the total mass is given by
M =

V
ϱ(x, y, z)dV =

D
⎛
⎜⎝
3−x
4 −3y
4

0
ϱ0dz
⎞
⎟⎠dA
= ϱ0
12

0
⎛
⎜⎝
4−x
3

0
⎛
⎜⎝
3−x
4 −3y
4

0
dz
⎞
⎟⎠dy
⎞
⎟⎠dx = ϱ0
12

0
⎛
⎜⎝
4−x
3

0

3 −x
4 −3y
4

dy
⎞
⎟⎠dx
= · · · = ϱ0 · (Volume of V) = 24ϱ0.

596
Advanced Engineering Mathematics
The moments are Myz, Mzx, Mxy. For example, Myz is the moment with respect to the
plane x = 0, so
Myz ≜Mx=0 =

V
xϱ(x, y, z)dV = ϱ0
12

0
⎛
⎜⎝
4−x
3

0
⎛
⎜⎝
3−x
4 −3y
4

0
xdz
⎞
⎟⎠dy
⎞
⎟⎠dx
= ϱ0
12

0
⎛
⎜⎝
4−x
3

0
x

3 −x
4 −3y
4

dy
⎞
⎟⎠dx = ϱ0
12

0
⎛
⎜⎝
4−x
3

0

3x −1
4x2 −3
4xy

dy
⎞
⎟⎠dx
= ϱ0
12

0

3xy −1
4x2y −3
8xy2
4−x
3
0
dx = · · · = ϱ0
12

0

6x −x2 + 1
24x3

dx = · · · = 72ϱ0.
Mzx ≜My=0 = ϱ0
12

0
⎛
⎜⎝
4−x
3

0
⎛
⎜⎝
3−x
4 −3y
4

0
ydz
⎞
⎟⎠dy
⎞
⎟⎠dx = ϱ0
12

0
⎛
⎜⎝
4−x
3

0

3y −1
4xy −3
4y2

dy
⎞
⎟⎠dx
= ϱ0
12

0
3
2 y2 −1
8xy2 −1
4y3
4−x
3
0
dx = · · · = ϱ0
12

0

8 −2x + 1
6x2 −1
216x3

dx = · · · = 24ϱ0.
Mxy ≜Mz=0 = ϱ0
12

0
⎛
⎜⎝
4−x
3

0
⎛
⎜⎝
3−x
4 −3y
4

0
zdz
⎞
⎟⎠dy
⎞
⎟⎠dx
= ϱ0
12

0
⎛
⎜⎝
4−x
3

0
1
2

3 −1
4x −3
4y
2
dy
⎞
⎟⎠dx = · · · = ϱ0
12

0

6 −3
2 x + 1
8x2 −
1
288x3

dx
= · · · = 18ϱ0.
So, this solid tetrahedron’s centroid is at
(¯x, ¯y, ¯z) ≜1
M(Mx=0, My=0, Mz=0) =
1
24ϱ0
(72ϱ0, 24ϱ0, 18ϱ0) =

3, 1, 3
4

.
The polar moment of inertia (for rotation around the z-axis) is
I0 ≜

V
(x2 + y2)ϱdV = ϱ0
12

0
⎛
⎜⎝
4−x
3

0
⎛
⎜⎝
3−x
4 −3y
4

0
(x2 + y2)dz
⎞
⎟⎠dy
⎞
⎟⎠dx
= ϱ0
12

0
⎛
⎜⎝
4−x
3

0

(x2 + y2)

3 −x
4 −3y
4

dy
⎞
⎟⎠dx

Integral Theorems, Multiple Integrals, and Applications
597
= ϱ0
12

0
⎛
⎜⎝
4−x
3

0

3x2 −1
4x3 −3
4x2y + 3y2 −1
4xy2 −3
4y3

dy
⎞
⎟⎠dx
= ϱ0
12

0

16 −16
3 x + 20
3 x2 −28
27x3 +
55
1296x4

dx = · · · = 384ϱ0. ⃝
Mathematica, MATLAB® with the symbolic manipulation toolbox, and many calculators
make short, easy work of such iterated integrals. In these problems, the real mathemati-
cal issue is setting up the integrals rather than the integration drudgery. Indeed, the real
engineering issue is knowing why it is important to calculate the various moments.
7.4.1 Cylindrical Coordinates
If a solid can be conveniently described in cylindrical coordinates, then it is usually easier
to do integrals on that solid in cylindrical coordinates. An element of volume is then
dV = r dr dθ dz.
In the special case of circular symmetry, that is, when there is no dependence on θ
in the integrand or in the limits of integration, it makes sense to write dV = r dr dz dθ or
dV = r dz dr dθ.
Example 7.23
A typical long bone shaft shown in Figure 7.37 has close to circular symmetry and diam-
eter that is smaller in its middle. Also, a long bone has a thin outer shell made of dense
“cortical” bone material and inside there is spongy “trabecular” bone material. Such a
hollow design is strong but lightweight.∗Model this description of the bone’s shaft and
write an integral for its mass.
Method: Say the shaft’s longitudinal axis is part of the z-axis, specifically −H ≤z ≤H.
The circular symmetry and narrowing of the bone, V, can be modeled by having
V = {(r, θ, z) : −H ≤z ≤H, 0 ≤θ ≤2π, 0 ≤r ≤f(z)},
where f decreases for −H ≤z ≤0 and increases for 0 ≤z ≤H.
The thin outer shell of cortical bone has f(z)−ε ≤r ≤f(z), where ε > 0 is the thickness
of the shell. The trabecular bone has 0 ≤r ≤f(z) −ε. So, we can model the mass density
function by
ϱ = ϱ(r, z) =
ϱ0,
0 ≤r ≤f(z) −ε
ϱ1,
f(z) −ε ≤r ≤f(z)

in g/cm3, assuming distances are measured in cm, and ϱ0, ϱ1 are constants.
∗Many bicycle frames are made of similar hollow tubes of narrowing diameter.

598
Advanced Engineering Mathematics
H
z
–H
FIGURE 7.37
Shaft of a long bone.
The total mass of the bone, in g, is

V
ϱ dV =
2π

0
H

−H
f(z)

0
ϱ(r, z) r dr dz dθ = 2π
H

−H
⎛
⎝
f(z)−ε

0
ϱ0 r dr +
f(z)

f(z)−ε
ϱ1 r dr
⎞
⎠dz
= 2π
H

−H

ϱ0
1
2(f(z) −ε)2 + ϱ1
1
2

(f(z))2 −(f(z) −ε)2 
dz
= −πHε2(ϱ1 −ϱ0) + 2πε(ϱ1 −ϱ0)
H

−H
f(z)dz + πϱ0
H

−H
(f(z))2dz. ⃝
Example 7.24
Find the z-coordinate of the center of mass of a homogeneous solid bounded below by
the surface z = 2(x2 + y2) and above by the surface z = 6

1 −
√
x2+y2
2

.
Method: The solid is shown in Figure 7.38. In cylindrical coordinates, the two bounding
surfaces are the paraboloid z = 2r2 and the cone z = 6 −3r. First, let’s find where the two
surfaces intersect:
2r2 = z = 6 −3r ⇐⇒0 = 2r2 + 3r −6.

Integral Theorems, Multiple Integrals, and Applications
599
6
–1.0–0.5 0.0 0.5
y
1.0
4
z
2
0
1.0
0.5
0.0
x
–0.5
–1.0
FIGURE 7.38
Example 7.24.
The latter is solvable by the quadratic formula:
r = −3 ±
√
57
4
.
Because r ≥0 in cylindrical coordinates, the two surfaces intersect only at
r = −3 +
√
57
4
≜r0.
(7.31)
If we were to shine a light down from z = ∞onto the solid, its shadow in the xy-plane,
that is, the z = 0 plane, would be the disk
0 ≤r ≤r0.
We can think of our solid as consisting of “straws” that are vertical, that is, parallel to
the z-axis, running from the bottom surface, z = 2r2, to the top surface, z = 6 −3r. So, in
cylindrical coordinates, our solid is
V = {(r, θ, z) : 0 ≤r ≤r0, 2r2 ≤z ≤6 −3r}.
Because the solid is homogeneous, the mass density is a constant ϱ0. The total mass of
the solid is M = ϱ0Volume, that is,
M = ϱ0
2π

0
r0

0
6−3r

2r2
rdz dr dθ = 2πϱ0
r0

0
r · (6 −3r −2r2)dr = 2πϱ0
r0

0
(6r −3r2 −2r3)dr
= 2πϱ0
'
3r2 −r3 −r4
2
(r0
0
= πϱ0r2
0
2

12 −4r0 −2r2
0

= πϱ0r2
0
2
(12 −4r0 −(6 −3r0)) .

600
Advanced Engineering Mathematics
So
M = πϱ0
2
r2
0(6 −r0).
The moment with respect to the z = 0 plane is
Mxy = Mz=0 = ϱ0
2π

0
r0

0
6−3r

2r2
zrdzdrdθ = πϱ0
r0

0
r ·

(6 −3r)2 −(2r2)2
dr
= πϱ0
r0

0
(36r −36r2 + 9r3 −4r5)dr = πϱ0

18r2
0 −12r3
0 + 9r4
0
4 −2r6
0
3

= πϱ0r2
0

18 −12r0 + 9r2
0
4 −2r4
0
3

= · · · = πϱ0r2
0
57
4 −57r0
8

.
So, the z-coordinate of the center of mass is
¯z = Mxy
M
=
πϱ0r2
0

57
4 −57r0
8

πϱ0
2 r2
0(6 −r0)
=
2

57
4 −57r0
8

6 −r0
= 57(2 −r0)
4(6 −r0) = 57
	
2 −−3+
√
57
4

4
	
6 −−3+
√
57
4

 .
= 57(11 −
√
57)
4(27 −
√
57)
≈2.527735. ⃝
“By eyeball,” this looks reasonable, since the majority of the mass in the solid is below
z = 3, although in the moment Mz = 0, the mass higher up counts more than the mass near
z = 0.
By the way, the surfaces bounding the solid were plotted by Mathematica: After defining
r0 by (7.31), we used the command
ParametricPlot3D[{{rCos[θ], rSin[θ], 2r2},{rCos[θ], rSin[θ], 6 −3r}}, {r, 0, r0}, {θ, 0, 2π}].
Example 7.25
V is the solid between the coaxial cylinders of radii 3 cm and 5 cm whose axis of sym-
metry is the x-axis and run from x = −3 cm to x = 3 cm. If V is filled with positive
electric charge whose density in coulombs per cm3 is proportional to the distance from
the origin, find the total charge in V.
Method: In an alternative version of cylindrical coordinates in which the x-axis is the axis
of revolution, let x = x, y = r cos ϕ, and z = r sin ϕ. Then the two bounding surfaces are
the cylinders r = 3 and r = 5, measured in cm. So,
V = {(x, r, ϕ) : −3 ≤x ≤3, 0 ≤ϕ ≤2π, 3 < r < 5}.
Let σ be the charge density, so
σ = k

r2 + x2
for some positive constant of proportionality k. In coulombs, the total charge in V is
Q =

V
σ dV =
2π

0
3
−3
5
3
k

r2 + x2 r dr dx dϕ.

Integral Theorems, Multiple Integrals, and Applications
601
Using the substitution w = r2 + x2, we get the anti-partial derivative
 
r2 + x2 r ∂r =

w1/2 ∂w
2 = 1
3w3/2 = 1
3(r2 + x2)3/2.
There is no dependence on ϕ in the integrand or in the limits of integration, that is, there
is circular symmetry, so
Q = (2π) · k
3
3
−3

(r2 + x2)3/2r=5
r=3 dx = 2πk
3
3
−3

(25 + x2)3/2 −(9 + x2)3/2
dx.
An integral of the form

(a2 + x2)3/2dx can be done using the inverse trigonometric
substitution x = a tan ψ, but it’s fairly hard work. Instead, we use Mathematica, or a book
of tables of integrals to get

(a2 + x2)3/2dx =

a2 + x2

5a2x
8
+ x3
4

+ 3a4
8
ln
x +

a2 + x2
 .
So, the total charge in V is
Q = 2πk
3
'
25 + x2

125x
8
+ x3
4

+ 1875
8
ln
x +

25 + x2

−

9+x2

45x
8 + x3
4

−243
8
ln
x+

9+x2

(3
−3
= 2πk
3
√
34
375
8
+ 27
4

+ 1875
8
ln
3 +
√
34

−
√
34

−375
8
−27
4

−1875
8
ln
−3 +
√
34

−
√
18
135
8
+ 27
4

−243
8
ln
3 +
√
18


+
√
18

−135
8
−27
4

+ 243
8
ln
−3 +
√
18


= 2πk
3

429
4
√
34 + 1875
8
ln

3 +
√
34
−3 +
√
34

−567
4
√
2 −243
8
ln

3 + 3
√
2
−3 + 3
√
2

= πk
2

143
√
34 −189
√
2 + 625 ln

3 +
√
34
5

−81 ln(1 +
√
2)

≈1336.220 k. ⃝
Definition 7.12
The average value of a function f on a solid V is
¯f ≜
1
Volume of V

V
f(r) dV =
	
V f(r) dV
	
V 1 dV .

602
Advanced Engineering Mathematics
7.4.2 Spherical Coordinates
In spherical coordinates, an element of volume is given by
dV = ρ2 sin φ dρ dφ dϕ,
as was shown in Figure 6.35 in Section 6.6.
Example 7.26
Find the z-coordinate of the centroid of the solid ice cream cone V bounded below by the
surface that is the cone φ = π
6 and from above by the sphere ρ = 2 cos φ.
Method: Since the problem asks for the centroid, the mass density must be a constant ϱ0,
so the total mass is M = ϱ0 · (Volume of V).
One can think of V as consisting of radial straws that run from the origin to the upper
surface ρ = 2 cos φ, as shown in Figure 7.39. In spherical coordinates,
V =

(ρ, φ, θ) : 0 ≤θ ≤2π, 0 ≤φ ≤π
6 , 0 ≤ρ ≤2 cos φ

.
The mass is given by
M =

V
ϱ0dV = ϱ0
2π

0
π
6
0
2 cos φ

0
ρ2 sin φ dρdφdθ = ϱ0
2π

0
π
6
0
'
ρ3
3
(2 cos φ
0
sin φ dφdθ
= (2π) · 8
3 · ϱ0
π
6
0
cos3 φ sin φ dφ = 16πϱ0
3
'
−cos4 φ
4
( π
6
0
= 16πϱ0
12

−cos4 π
6

+ 1

= 4πϱ0
3
⎛
⎝1 −
√
3
2
4⎞
⎠= 4πϱ0
3
· 7
16 = 7πϱ0
12 .
2.0
0.5
0.0
–0.5
x
z
1.5
1.0
0.5
0.0
–0.5
0.0
y
0.5
FIGURE 7.39
Solid V in Example 7.26.

Integral Theorems, Multiple Integrals, and Applications
603
The moment about the plane z = 0 is
Mxy = Mz=0 = ϱ0

V
z dV = ϱ0
2π

0
π
6
0
2 cos φ

0
(ρ cos φ)ρ2 sin φ dρdφdθ
= ϱ0
2π

0
π
6
0
'
ρ4
4
(2 cos φ
0
cos φ sin φ dφdθ = (2π) · 16
4 · ϱ0
π
6
0
cos5 φ sin φ dφ
= 8πϱ0 ·
'
−cos6 φ
6
( π
6
0
= 8πϱ0
6

−cos6 π
6

+ 1

= 4πϱ0
3
⎛
⎝1 −
√
3
2
6⎞
⎠= 4πϱ0
3
· 37
64 = 37πϱ0
48
.
The z-coordinate of the centroid is at
¯z = Mxy
M
=
37πϱ0
48
 ) 7πϱ0
12

= 37
28. ⃝
“By eyeball,” this looks reasonable, since most of the ice cream is above z = 1.
By the way, Mathematica drew the solid V using the command
RegionPlot3D

x2 + y2 ≤z2
3 && x2 + y2 + (z −1)2 ≤1,

x, −
√
3
2 ,
√
3
2

,

y, −
√
3
2 ,
√
3
2

,
{z, 0, 2}, AxesLabel →{′′x′′,′′ y′′,′′ z′′}

followed by rotation with a mouse. This command effectively thinks of V as a union of
vertical straws that run from the cone z = r
√
3 up to the sphere z = 1 +

1 −r2, for (x, y) in
the disk x2 + y2 ≤3
4.
Example 7.27
Evaluate
I ≜

V

x2 + y2 + z2 e−(x2+y2+z2)/(2α)dV,
where
α is a positive constant
V is the solid that lies between the two concentric spheres ρ = 1 and ρ = 3 in the
positive octant x ≥0, y ≥0, z ≥0
Method: The solid is shown in Figure 7.40. Because of the spherical symmetry in the
integrand, we should try using spherical coordinates. To be in the positive octant, we
need (x, y) to be in the positive quadrant of the xy-plane, so 0 ≤θ ≤π
2 . To be in the
northern hemisphere, we need 0 ≤φ ≤π
2 . So,
V =

(ρ, φ, θ) : 0 ≤θ ≤π
2 , 0 ≤φ < π
2 , 1 ≤ρ ≤3

.

604
Advanced Engineering Mathematics
0
3
2
1
0
0
1
2
x
z
y
1
2
3
3
FIGURE 7.40
Solid V in Example 7.27.
So,
I ≜

V

x2 + y2 + z2 e−(x2+y2+z2)/(2α)dV =
π
2
0
π
2
0
3
1
ρe−ρ2/(2α)ρ2 sin φ dρdφdθ
= (2π)
π
2
0
3
1
ρ3e−ρ2/(2α) sin φ dρdφ = 2π
⎛
⎝
π
2
0
sin φ dφ
⎞
⎠
⎛
⎝
3
1
ρ3e−ρ2/(2α)dρ
⎞
⎠.
Using the substitution ρ2 = −2αw, so 2ρdρ = −2αdw, and then integration by parts, this
gives
I = 2π

[−cos φ]
π
2
0
 ⎛
⎝
−9/(2α)

−1/(2α)
(−2αw)ew(−αdw)
⎞
⎠= 2π · 1 · (2α2)

(w −1)ew−9/(2α)
−1/(2α)

= 4πα2
 1
2α + 1

e−1/(2α) −
 9
2α + 1

e−9/(2α)

. ⃝
As a reality check, is the final conclusion for I positive? It should be, because I is the
definite integral of a nontrivial, nonnegative continuous function on a solid V that has
positive (i.e., nonzero) volume.
If we define F(x) ≜(1 + x)e−x, then
I = 4πα2

F
 1
2α

−F
 9
2α

.
So, if we can explain why F(x) is a strictly decreasing function for x in the interval

1
2α , 9
2α

,
then we can conclude I > 0. In fact, F′(x) = −xe−x < 0 at all x > 0, so yes, I passes the
reality check of being positive.

Integral Theorems, Multiple Integrals, and Applications
605
By the way, Mathematica drew the solid V using the command
RegionPlot3D[1 ≤x2 + y2 + z2&& x2 + y2+z2 ≤9, {x, 0, 3}, {y, 0, 3}, {z, 0, 3},
AxesLabel →{′′x′′,′′ y′′,′′ z′′}].
Example 7.28
Find the volume of the solid V bounded by the surfaces z = 4 and φ = π
6 .
Method: We could solve this problem as a triple integral, but it is faster and easier to use
our knowledge of geometrical formulas. Our V is a solid, circular cone whose “base” is
in the plane z = 4 and whose side is a finite part of the cone φ = π
6 . To find the radius of
that base, spherical coordinates have z = ρ cos φ; hence,
ρ =
z
cos φ =
4
cos π
6
=
4
√
3
2
=
8
√
3
.
Cylindrical and spherical coordinates are connected by

x2 + y2 = r = ρ sin φ =
8
√
3
· sin π
6 =
8
√
3
· 1
2 =
4
√
3
.
It follows that the base of the cone has radius R =
4
√
3 and height H = 4, so V has
Volume = πR2H
3
=
π

4
√
3
2
· 4
3
=
π

16
3

4
3
= 64π
9 . ⃝
Learn More About It
A good reference concerning bones is Physics of the Human Body, by Irving P. Herman,
Springer-Verlag, c⃝2007.
7.4.3 Problems
1. Set up an iterated integral for
	
V f(x, y, z) dV, where V is the solid tetrahedron
whose vertices are (0, 0, 0), (1, 0, 0), (0, 2, 0), (0, 0, 3). Write your final conclusion in
the form



d
d
d
and fill in the blanks.
2. Set up an iterated integral for
	
V f(x, y, z) dV, where V is the solid tetrahedron
whose vertices are (0, 0, 0), (2, 0, 0), (0, 3, 0), (0, 0, 1). Write your final conclusion in
the form



d
d
d
and fill in the blanks.
3. Find the y-coordinate of the centroid of the solid tetrahedron whose vertices are
(0, 0, 0), (1, 0, 0), (0, 3, 0), (0, 0, 2).
4. Rewrite the triple integral in the following in the form indicated by filling in the
blanks with specific expressions. Give an appropriate sketch(es) of the solid V and
region(s) D for the iterated triple integral on the RHS, that is, your final conclusion.

606
Advanced Engineering Mathematics
Your final conclusion should be in the form
1
0
4−4z

0
2−y
2 −2z

0
f(x, y, z) dx dy dz =
  
f(x, y, z) dz dx dy
with blanks filled in. The function f(x, y, z) is not specified.
5. Rewrite the triple integral in the following in the form indicated by filling in the
blanks with specific expressions. Give an appropriate sketch(es) of the solid V and
region(s) D for the iterated triple integral on the RHS, that is, your final conclusion.
Your final conclusion should be in the form
4
0
2−z
2

0
1−x
2 −z
4

0
f(x, y, z) dy dx dz =
  
f(x, y, z) dz dy dx
with blanks filled in. The function f(x, y, z) is not specified.
6. For the solid of Example 7.28, use triple integrals to find the volume and the cen-
troid. [Hint: Symmetry will make short work of two of the coordinates of the
centroid.]
7. Find the centroid of the solid that is in the first octant, that is, satisfies x ≥0, y ≥
0, z ≥0, lies above the surface z = 1, and lies below the surface x2 + y2 + z = 4.
8. Find the centroid of the solid that is in the first octant, that is, satisfies x ≥0, y ≥
0, z ≥0, lies above the surface z = 0, and lies below the surface x2 + y2 + z2 = 4.
9. Evaluate
	
V(x2 + y2) dV, where V is the solid bounded by the cylinder x2 + y2 = 4
and the planes z = −1 and y + z = 2.
10. Redo Example 7.26 using cylindrical coordinates.
11. Let V be the finite type I solid bounded above by the plane z = 4 and bounded
below by the positive octant part of the plane x+3y+4z = 12. [The positive octant
consists of all points having x ≥0, y ≥0, and z ≥0.] Set up, with as much
specificity as possible,
	
V f(x, y, z) dV.
12. Let V be the solid bounded below by the circular paraboloid z = x2 + y2 and
bounded above by the circular paraboloid z = 4 −x2 −y2. Set up, with as much
specificity as possible,
	
V f(x, y, z) dV.
13. Let V be the solid bounded below by the circular paraboloid z = 2(x2 + y2) and
bounded above by the circular paraboloid z = 9 −x2 −y2. Find the z-coordinate of
the centroid.
14. Find the volume of the solid that lies above the surface z = 3

x2 + y2 and below
the surface z = 4.
15. Find the polar moment of inertia (for rotation about the z-axis) of the homoge-
neous solid bounded by the surfaces z = 2 −x2 −y2 and z =

x2 + y2.
16. Find the volume of the solid V = {(ρ, φ, θ) : 0 ≤θ ≤2π, 0 ≤φ ≤π
2 , 0 ≤ρ ≤
4 cos φ}.
17. The solid V is the x ≥0 half of the solid between the concentric spheres of radii
2 cm and 3 cm whose centers are at (0, 0, 0). Suppose V is filled with material whose

Integral Theorems, Multiple Integrals, and Applications
607
density, in g/cm3, is proportional to the distance from the yz-plane. Find the total
mass of V in terms of the constant of proportionality.
18. (a) If a thin tube were drilled completely through a large solid ball, then the force
of gravity on an object in that tube would have magnitude |F| = k|z|, for some
constant k, assuming the tube runs along the z-axis, the center of the ball is
at the origin, and the ball is homogeneous, that is, has constant mass density.
Explain this result using a triple integral in cylindrical coordinates.
By the way, a planet is approximately a solid ball.
(b) This would be very difficult to implement if the ball is the Earth because
of its molten metal core. Even Mars has a partially molten core. However,
for the sake of argument, suppose we could do this on Mars. According to
Wikipedia, Mars has a polar radius of about 3396 km and on its surface exerts
a force of gravity about 0.376 that of the force of gravity on the surface of the
Earth. Find the value of the constant k if the tube is drilled from pole to pole
on Mars.
19. In single variable calculus, you studied the method of disks and washers
for finding the volume of a solid of revolution. For example, if f(x) ≥g(x)
for a ≤x ≤b and the finite region in the xy-plane bounded by the curves
y = f(x), y = g(x), x = a, x = b is rotated around the x-axis, then the volume of the
solid generated is π
 b
a
	
f(x)

2 −
	
g(x)

2
dx. Use the alternative version of cylin-
drical coordinates of Example 7.25 and a triple integral to explain why the
“method of washers” is correct. So, you will explain why what we studied in
Section 7.4 agrees with a result from single variable calculus.
20. Use the single variable calculus method of disks to check the volume calculation
that is part of Example 7.24. Can we use the method of disks to do the rest of
Example 7.24, that is, to find the z-coordinate of the centroid? If so, use it; if not,
why not?
21. In single variable calculus, you may have studied the “method of cylindrical
shells” for finding the volume of a solid of revolution. For example, if f(x) ≥g(x)
for a ≤x ≤b and the finite region in the xy-plane bounded by the curves
y = f(x), y = g(x), x = a, x = b is rotated around the y-axis, then the volume of the
solid generated is 2π
 b
a x
	
f(x) −g(x)

dx. Use an alternative version of cylindrical
coordinates and a triple integral to explain why method of cylindrical shells is cor-
rect. So, you will explain why what we studied in Section 7.4 agrees with another
result from single variable calculus.
7.5 Surface Integrals and Applications
Recall from Section 6.3 that the area of a parallelogram whose sides are vectors A and B is
||A×B||. Recall from Section 6.3 that a parametric surface is described by a position vector
that is a function of two parameters, that is,
S : r = r(u, v), (u, v) in D,
(7.32)

608
Advanced Engineering Mathematics
where D is a planar region. Moreover, at any point P whose position vector is r(u0, v0) on
S, the two vectors
∂r
∂u(u0, v0) and ∂r
∂v(u0, v0)
shown in Figure 7.41 span the tangent plane at P. An element of surface area is given by
(6.64) in Section 6.6, that is,
dS ≜


∂r
∂u(u0, v0) du × ∂r
∂v(u0, v0) dv

 =


∂r
∂u(u0, v0) × ∂r
∂v(u0, v0)

 du dv.
(7.33)
Example 7.29
In Example 6.18 in Section 6.3, we considered a sphere of radius a:
S : r = r(φ, θ) = a(sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk), 0 ≤φ ≤π, 0 ≤θ ≤2π,
that is,
S : r = aˆeρ, 0 ≤φ ≤π, 0 ≤θ ≤2π.
Find the element of surface area.
Method: We calculate
∂r
∂φ (φ, θ) = a(cos φ cos θ ˆı + cos φ sin θ ˆı −sin φ ˆk) = aeφ
and
∂r
∂θ (φ, θ) = a(−sin φ sin θ ˆı + sin φ cos θ ˆj) = a sin φ ˆeθ;
hence,
dS =


∂r
∂φ (φ, θ) × ∂r
∂θ (φ, θ)

 dφ dθ =
aeφ × a sin φ ˆeθ
 dφ dθ = a2 ˆeρ
 | sin φ| dφ dθ.
∂r
∂u
—–
∂r
∂υ
—–
FIGURE 7.41
∂r
∂u and ∂r
∂v.

Integral Theorems, Multiple Integrals, and Applications
609
Because sin φ ≥0 for 0 ≤φ ≤π, | sin φ | = sin φ. So, the element of surface area on a
sphere of radius a is
dS = a2 sin φ dφ dθ. ⃝
At φ = π
2 , that is, the equator, we get the largest element of surface area. Note also that
because sin φ ≥0 for 0 ≤φ ≤π, an element of surface area must be positive. (If you
discover a negative element of surface area, write up a paper about it, have it published
by a reputable physics journal, and then claim your Nobel Prize in physics.)
The total surface area of a surface S : r = r(u, v), (u, v) in D, is
Area(S) ≜

S
1 dS ≜

D


∂r
∂u × ∂r
∂v

 du dv.
(7.34)
Let’s check that this formula makes sense in a specific example.
Example 7.30
We will use a geometrical process to visualize the planar surface of an annulus and then
see how its area is naturally calculated using formula (7.34).
As shown in Figure 7.42, let the point P be at r = a cos θ ˆı + a sin θ ˆj, that is, P is a point
on a circle of radius a and center at the origin. On the radial line segment from P, put a
point Q at
r = (a cos θ ˆı + a sin θ ˆj ) + v(cos θ ˆı + sin θ ˆj ),
that is, Q lies on the same radial line as P, but its distance from the origin is (a+v), where
0 ≤v ≤1. Put all such points together to form a surface,
S : r = (a + v) cos θ ˆı + (a + v) sin θ ˆj, 0 ≤θ ≤2π, 0 ≤v ≤1.
Q
P
FIGURE 7.42
Annulus geometry.

610
Advanced Engineering Mathematics
In effect, we have constructed the annulus as the union of radial line segments. Find the
area of the planar surface S.
Method: We have, using the notation of cylindrical coordinates,
S : r = (a + v) ˆer.
So,
dS =


∂r
∂θ × ∂r
∂v

 dθ dv =
(a + v)ˆeθ × ˆer
 dθ dv =

(a + v)(−ˆk)

 dθ dv
= |a + v| dθ dv = (a + v) dθ dv.
The total surface area of S is
Area(S) =

S
1 dS =
1
0
2π

0
(a + v) dθ dv = 2π
1
0
(a + v)dv = 2π
'
av + v2
2
(1
0
= π(2a + 1). ⃝
Let’s check this result: The area of the annulus a ≤r ≤(a + 1) is π
	
(a + 1)2 −a2
=
π(2a + 1), which agrees with the conclusion of Example 7.30.
Example 7.31
In Example 7.29, we found that dS = a2 sin φ dφ dθ is the element of surface area on a
sphere of radius a. Find the total surface area of the sphere.
Method: The sphere has parametrization
S : r = a ˆeρ, 0 ≤φ ≤π, 0 ≤θ ≤2π,
so D = {(φ, θ) : 0 ≤φ ≤π, 0 ≤θ ≤2π}. The sphere has total
Surface area =

S
1 dS =
2π

0
π
0
a2 sin φ dφdθ =
⎛
⎝
2π

0
a2dθ
⎞
⎠
⎛
⎝
π
0
sin φ dφ
⎞
⎠
= 2πa2 [−cos φ dφ]π
0 = 2πa2 · 2 = 4πa2. ⃝
Surfaces z = g(x, y)
Suppose a surface is parametrized by (x, y), specifically
S : r = x ˆı + y ˆj + g(x, y) ˆk, (x, y) in D.
Then
∂r
∂x × ∂r
∂y =

ˆı + ∂g
∂x
ˆk

×

ˆj + ∂g
∂y
ˆk

= −∂g
∂x ˆı −∂g
∂y ˆj + ˆk.
(7.35)

Integral Theorems, Multiple Integrals, and Applications
611
So,
dS =


∂r
∂x × ∂r
∂y

 dx dy =

−∂g
∂x ˆı −∂g
∂y ˆj + ˆk

 dx dy =
*∂g
∂x
2
+
∂g
∂y
2
+ 1 dx dy.
(7.36)
7.5.1 Surface Integral of a Scalar-Valued Function
Over a surface
S : r = r(u, v), (u, v) in D,
the integral of a scalar-valued function f(x, y, z) = f(r) is defined to be

S
f(x, y, z) dS ≜

D
f
	
x(u, v), y(u, v), z(u, v)

 

∂r
∂u × ∂r
∂v

 du dv.
(7.37)
Equation (7.37) is analogous to the definition of line integrals (7.5), that is,

C
f(r) ds ≜
β
α
f(r(t)) ||˙r(t)|| dt.
Example 7.32
Find the centroid of the portion of the plane x+2y+3z = 4 that lies in the positive octant,
that is, satisfies x ≥0, y ≥0, z ≥0.
Method: Figure 7.43 shows our surface, z = 4
3 −x
3 −2y
3 , which can be parametrized as
S : r = x ˆı + y ˆj +
4
3 −x
3 −2y
3

ˆk, (x, y) in D,
where D is the shadow of the surface if a light is shone down on it from z = ∞.
1.0
4
3
2
x
1
0
0.5
z
0.0
0.0
0.5
1.0
1.5
2.0
y
FIGURE 7.43
Surface in Example 7.32.

612
Advanced Engineering Mathematics
y
y=2– x
2
x
2
1
1
2
3
4
FIGURE 7.44
Triangle in Example 7.32.
Formula (7.36) calculates that dS =

(−1
3)2 + (−2
3)2 + 1 dx dy =
√
14
3
dx dy.
Since the problem asks for the centroid, the mass density must be a constant ϱ0, so the
total mass is
M =

S
ϱ0 dS = ϱ0

D
√
14
3
dx dy = ϱ0
√
14
3
(Area of D).
At this point, we must find D : The shadow of S in the xy-plane is the triangle whose
vertices are (0, 0), (4, 0), (0, 2), as shown in Figure 7.44, so
D =

(x, y) : 0 ≤x ≤4, 0 ≤y ≤2 −x
2

has area 1
2 base × height = 4. So, the total mass is
M = ϱ0
√
14
3
· 4 = 4
√
14
3
ϱ0.
By the way, the surface S is itself a triangle with vertices (4, 0, 0), (0, 2, 0), (0, 0, 4
3). In
Example 6.28 in Section 6.6, we found that its Area = 1
2 base × height = · · · = 4
√
14
3
, which
agrees with our calculation in this example.
Now let’s calculate the moments:
Mz=0 ≜

S
zϱ0 dS = ϱ0
4
0
2−x
2

0
z
√
14
3
dy dx.
But, on the surface, z = 1
3(4 −x −2y), so
Mz=0 = ϱ0
√
14
3
4
0
2−x
2

0
4
3 −x
3 −2y
3

dy dx = ϱ0
√
14
9
4
0

4y −xy −y22−x
2
0

dx
= ϱ0
√
14
9
4
0
·

4

2 −x
2

−x

2 −x
2

−

2 −x
2
2
dx = ϱ0
√
14
9
4
0

4 −2x + x2
4

dx
= ϱ0
√
14
9
·
'
4x −x2 + x3
12
(4
0
= 16ϱ0
√
14
27
.

Integral Theorems, Multiple Integrals, and Applications
613
Similarly,
Mx=0 ≜

S
xϱ0 dS = ϱ0
4
0
2−x
2

0
x
√
14
3
dy dx = ϱ0
4
0
√
14
3
x

2 −x
2

dx
= ϱ0
√
14
3
·
'
x2 −x3
6
(4
0
= 16ϱ0
√
14
9
and
My=0 ≜

S
yϱ0 dS = ϱ0
4
0
2−x
2

0
y
√
14
3
dy dx = ϱ0
4
0
√
14
3
· 1
2

2 −x
2
2
dx
= ϱ0
√
14
3
·

−1
3

2 −x
2
34
0
= 8ϱ0
√
14
9
.
The centroid is at
(¯x, ¯y, ¯z) = 1
M(Mx=0, My=0, Mz=0) =
3
4ϱ0
√
14

16ϱ0
√
14
9
, 8ϱ0
√
14
9
, 16ϱ0
√
14
27

=
4
3, 2
3, 4
9

. ⃝
7.5.2 Surface Integral of a Vector Field
Suppose a parametrized surface is given by
S : r = r(u, v), (u, v) in D.
At a point P whose position vector is r(u0, v0), the two vectors
∂r
∂u(u0, v0) and ∂r
∂v(u0, v0)
span the tangent plane at P. It follows that the vector
n ≜∂r
∂u(u0, v0) × ∂r
∂v(u0, v0)
is normal to the tangent plane at P, as long as n ̸= 0. For nonzero n,
n ≜±

1
 ∂r
∂u(u0, v0) × ∂r
∂v(u0, v0)


∂r
∂u(u0, v0) ∂r
∂v(u0, v0)
is a unit vector normal to S at P. In some sense, n gives the direction of travel through S
at P. Note that n is a function of the position vector r, that is, can vary as we move along
the surface, but we usually suppress the dependence on r from the notation for the unit
normal.
Given a vector-valued function F = F(r), that is, a vector field, we can define
I ≜

S
	
F(r) • n

dS ≜

D
	
F(r(u, v)) • n(u, v)

du dv
(7.38)

614
Advanced Engineering Mathematics
to be the surface integral of F on S. One of the two choices for n gives
I =

D
F(r) •

1

 ∂r
∂u × ∂r
∂v

∂r
∂u × ∂r
∂v




∂r
∂u × ∂r
∂v

dudv

=

D
F(r) •
 ∂r
∂u × ∂r
∂v

dudv.
(7.39)
Another name for the surface integral of a vector field is the flux of F across S, whose
physical significance we will discuss further.
Another notation for such integrals is

S
F(r) • dS.
Here we think of dS ≜n dS as a vector-valued element of surface area pointing in the
direction of travel across S.
Example 7.33
Set up an iterated double integral for the flux of a vector field out of a sphere. Be as
specific as possible, for example, get a double integral with specific limits of integration.
Method: The problem is a little vague because it does not say what sphere. It makes sense
to keep it simple: Assume the sphere has center at the origin and has radius a, that is, is
x2 + y2 + z2 = a2. So,
S : r = a ˆeρ, 0 ≤θ ≤2π, 0 ≤φ ≤π.
As we saw in Example 7.29,
± ∂r
∂φ (φ, θ) × ∂r
∂θ (φ, θ) = ± a eφ × a sin φ ˆeθ = ± a2 sin φ ˆeρ.
Choose the + sign to get the outward unit normal:
n ≜ˆeρ.
By (7.39), the flux of a vector field F out of the sphere is

S
F •
 ∂r
∂φ (φ, θ) × ∂r
∂θ (φ, θ)

dS =
2π

0
π
0
F •

a2 sin φ ˆeρ

dφ dθ.
In terms of the components of F, that is,
F = Fρ ˆeρ + Fφ eφ + Fθ ˆeθ,
the total flux out of the sphere is

S
F • dS = a2
2π

0
π
0
Fρ(a, φ, θ) sin φ dφ dθ. ⃝

Integral Theorems, Multiple Integrals, and Applications
615
Example 7.34
Suppose an electric charge of 1 C is at the origin of a vacuum. Find the total flux of the
electric field out of spheres whose center is at the origin.
Method: Denote by ϵ0 the electric permittivity of a vacuum. Then the electric field due to
an electric charge of 1 coulomb is E =
1
4πϵ0 ||r||−3r, so on a sphere Sa = {r : ||r|| = a},
E =
1
4πϵ0
a−3 r =
1
4πϵ0
a−3(a ˆeρ) =
1
4πϵ0
a−2 ˆeρ.
The total flux of E out of Sa is

Sa
E • dS =
2π

0
π
0

1
4πϵ0
a−2 ˆeρ

•

a2 sin φ ˆeρ dφ dθ

=
2π

0
π
0
1
4πϵ0
sin φ dφ dθ
=
1
4πϵ0
⎛
⎝
2π

0
dθ
⎞
⎠
⎛
⎝
π
0
sin φ dφ
⎞
⎠= 2π
1
4πϵ0
[−cos φ]π
0 = 1
ϵ0
. ⃝
By the way,
1
4πϵ0
≈9 × 109 m/F, where F means farads. The exact value satisfies
1
4πϵ0 = 10−7 c2, where the velocity of light in a vacuum is c ≈2.998 × 108 m/s.
The total flux out of such spheres does not depend on their radius! In retrospect, this
makes sense because while the intensity of the electric field behaves like a−2, the surface
area of a sphere behaves like a2; hence, it is not a big shock that the total flux does not
depend on the radius.
Definition 7.13
A surface S : r = r(u, v), (u, v) in D, is piecewise smooth if D can be written as the union
of a finite number of subregions Di, r(u, v) is continuously differentiable on each such
subregion, and r(u, v) is continuous on D.
The last part of the definition that r(u, v) is continuous on D just says that as we move
from (u, v) on one “patch” Di to (u, v) on another patch, the corresponding points r(u, v)
vary continuously on the surface S.
Definition 7.14
A piecewise smooth surface S : r = r(u, v), (u, v) in D, is
(a) Orientable if
n(u, v) ≜∂r
∂u × ∂r
∂v ̸= 0 at all (u, v) in D
(b) Oriented if we can choose normal vector n = n(r(u, v)) that is a continuous and
nonzero function of (u, v) in D, in which case we say that n orients S.

616
Advanced Engineering Mathematics
Example 7.35
If S is a surface of the form z = g(x, y) for (x, y) in D and g(x, y) is continuously
differentiable on D, then S is orientable and
n(x, y) ≜−∂g
∂x ˆı −∂g
∂y ˆj + ˆk
orients S with the upward orientation.
Definition 7.15
A surface is closed if it is the boundary of a finite solid in R3.
If a surface integral is taken on a closed surface, then we may put a “circle” in the nota-
tion

S. This is similar to putting a circle in the notation for a line integral on a closed
curve.
Definition 7.16
A closed oriented surface enclosing a solid V has the positive orientation if the normal
vector n that orients S points out of V everywhere on the surface. To be precise, for all P
on the surface and ε sufficiently small, −→
OP + εn is outside of V and −→
OP −εn is inside V.
Example 7.36
Find the total rate at which fluid mass is leaving the cylinder:
{(x, y, z) : 0 ≤x2 + y2 ≤a2 and −H ≤z ≤H}
if the mass density is constant and its velocity field is v = x ˆı + 2 ˆj + y ˆk.
Method: Define S to be the closed surface that encloses the cylinder positively oriented
by outward unit normal vector n. Let ϱ0 denote the constant mass density, in units of
mass per volume, and let v be the fluid velocity vector. The total rate at which mass is
leaving the cylinder is
I ≜

S
ϱ0 v • n dS,
that is, the flux of the vector field (ϱ0v) out of S. The units for each component of the
vector ϱ0v are (mass/volume) × (length/time), that is, (mass per area per unit of time).
S consists of three parts, each with a corresponding unit normal vector n pointing
out of the cylinder: (1) the top, S+, that is, the surface z = H, 0 ≤x2 + y2 ≤a2, having
n = ˆk; (2) the bottom, S−, that is, the surface z = −H, 0 ≤x2 + y2 ≤a2, having n = −ˆk;
and (3) the side, +
S, that is, the surface −H ≤z ≤H, r = a, having n = ˆer in cylindrical
coordinates. (You will check the conclusions about n in Problem 7.5.3.15.)
So, we know that the total rate of mass flow out of the cylinder is
I = I+ + I−+ +I ≜ϱ0

S+
v • ˆk dS + ϱ0

S−
v • (−ˆk) dS + ϱ0

+
S
v • ˆer dS.

Integral Theorems, Multiple Integrals, and Applications
617
The top and bottom surfaces are flat, so dS = dA = r dr dθ there. On the side, an element
of surface area is given by the product of an element of circular arclength, a dθ, and an
element of vertical length, dz, that is, dS = a dθ dz there.
We calculate
I+ = ϱ0
2π

0
a
0
v(r, θ, H) • ˆkr dr dθ = ϱ0
2π

0
a
0
(x ˆı + 2 ˆj + y ˆk) • ˆkr dr dθ
= ϱ0
2π

0
a
0
yr dr dθ = ϱ0
2π

0
a
0
r sin θ r dr dθ = · · · = 0,
I−= ϱ0
2π

0
a
0
v(r, θ, −H) • (−ˆk)r dr dθ = −ϱ0
2π

0
a
0
(x ˆı + 2 ˆj + y ˆk) • ˆkr dr dθ
= −ϱ0
2π

0
a
0
yr dr dθ = −ϱ0
2π

0
a
0
r sin θ r dr dθ = · · · = 0,
and
+I = ϱ0
H

−H
2π

0
v(a, θ, z) • ˆera dθ dz. = ϱ0
H

−H
2π

0
(x ˆı + 2 ˆj + y ˆk) • ˆer a dθ dz
= aϱ0
H

−H
2π

0
(a cos θ ˆı + 2 ˆj) • (cos θ ˆı + sin θ ˆj)dθ dz = aϱ0
H

−H
2π

0
(a cos2 θ + 2 sin θ) dθ dz
= aϱ0
⎛
⎝
H

−H
dz
⎞
⎠
⎛
⎝
2π

0
(a cos2 θ + 2 sin θ) dθ
⎞
⎠= 2aHϱ0
2π

0
a(1 + cos 2θ)
2
+ 2 sin θ

dθ
= 2aHϱ0
aθ
2 + a sin 2θ
4
−2 cos θ
2π
0
= 2aHϱ0 · πa = 2πa2Hϱ0.
So, the total rate of mass flow out of the cylinder is
I = I+ + I−+ +I = 2πa2Hϱ0
in units of mass per unit of time. ⃝
7.5.3 Problems
1. The side wall of a twisted cylinder, shown in Figure 7.45, is parametrized by
r = (a cos θ −sin z)ˆı + a sin θ ˆj + z ˆk, 0 ≤z ≤2π, 0 ≤θ ≤2π,
where a is an unspecified positive constant. Find dS, the element of area.

618
Advanced Engineering Mathematics
6
–2 –1
0
1
2
y
4
z
2
0
0
–2
2
x
FIGURE 7.45
Twisted cylinder in Problem 7.5.3.1.
2. S is the top, slanted face of the solid V bounded by the cylinder x2 +y2 = 9 and the
planes z = 0 and 2x + z = 8. Find the surface area of S.
3. The solid V is bounded from below by the cone φ = π
6 and from above by the
sphere of radius 3 and center at the origin. The boundary of V is S, which we can
think of as being in two pieces. Find the total surface area of S.
4. Find the surface area of the part of the paraboloid z = x2+y2
2
that lies below the
plane z = 5.
5. Find the centroid of the surface
S : r = u ˆı + (2 −u) ˆj + v ˆk
on the domain 0 ≤u2 + v2 ≤1. Sketch the surface and see whether the location of
the centroid makes sense “by eyeball.”
6. Let  be the tangent plane to the surface z2 = x2 + y −4y2 at the point
(x, y, z) = (3, −1, −2). Let S be the part of  that lies in the positive octant. Find
the total surface area of S.
7. Find the z-coordinate centroid of the surface
S : r = v ˆı + u ˆj + (2 −u −2v)ˆk
on the domain 0 ≤u2 + v2 ≤1.
8. Find the z-coordinate of the centroid of the surface S parametrized by r = 2
3u3/2 ˆı +
v ˆj + (u −v)ˆk on the rectangular domain 0 ≤u ≤1, 0 ≤v ≤2.

Integral Theorems, Multiple Integrals, and Applications
619
9. The surface S is the z ≥0 portion of the sphere with radius a and center at the
origin. Evaluate

S
z dS
and use that to find the z-coordinate of the centroid.
10. Use a computer algebra system to find the centroid of the surface
S : r = u2 ˆı + (2 −u) ˆj + v ˆk
on the half disk domain 0 ≤u2 + v2 ≤1, u ≥0. Use a computer algebra system to
sketch the surface and see whether the location of the centroid makes sense “by
eyeball.”
11. S is the top, slanted face of the solid bounded by the cylinder x2 + y2 = 4 and the
planes z = 0 and 2x + z = 5. (a) Find the total surface area of S, and (b) evaluate

S z dS.
12. Let F ≜(x + y)ˆı + (−x + y) ˆj and S be the paraboloidal surface z = 4(x2 + y2) for
(x, y) in the disk 0 ≤x2 + y2 ≤a2, where a is an unspecified constant. Evaluate

S
F • n dS,
assuming n is a normal vector giving the upward orientation to S. [Hint: Polar
coordinates may be useful in the solution.]
13. Evaluate

S
(y ˆı + z2 ˆk) • dS,
where S is the closed surface enclosing the half ball x2 + y2 + z2 ≤a2, z ≥0. Your
final conclusion should be in terms of a.
14. Evaluate

S

(y −x2)ˆı + 2xy ˆj + z ˆk

• n dS,
where S is the closed surface enclosing the half ball x2 + y2 + z2 ≤a2, z ≥0. Your
final conclusion should be in terms of a.
15. Check the conclusions about n in Example 7.36 using n = ∂r
∂u × ∂r
∂v.
16. (a) Write down, but do not evaluate, an iterated double integral that gives the
surface area of the portion of the surface z =

x2 + 2y2 that lies above the rect-
angle D = {(x, y) : 0 ≤x ≤3, 0 ≤y ≤2}. Write the double integral with as
much specific details as possible.

620
Advanced Engineering Mathematics
u/v
0
0
2
3
4
5
6
7
8
9
10
11
12
j–k
i–k
i–k
i+j
i+k
i–j
i+j
–j
j
j
j
j
j
j
i+k
j+k
i+j
i+j
i–j
i–j
i+k
i–k
i+2j
i+2k
i+k
i+j
–k
–k
–k
k
k
k
k
k
k
k
k
j+k
i+j
i+j
i+j
i+j
i
i
i
i
i
i
k
k
k
k
k
k
k
k
k
i
i
i
i
i
i
i
i
i
k
k
–k
j
j
j
j
j
j
j
j
j
j
1
1
2
3
4
5
j
i+j
i+2j
i+j
k
k
i–k
i
i
i
k
–k
j
6
FIGURE 7.46
Values of r in Problem 7.5.3.17.
0
i–k
i–k
–k
i+j
i
j
–k
j–k
k
k
j–i
–i
j
j
j
j
j
j
j
i
i
i
i
i
–k
i
i
i
i
i
i
i
i
i
i
i
i
i
i+k
i+j
j
j
j
–k
–k
–k
k
k
k
k
k
k
k
k
k
k
j
j
j
j
j
j
j
j
j
j
i+j
i+k
i+j
i+j
i+j
i+2j
i+2k
i+2j
i+j
i+j
i+j
i+j
i+j
i+j
i–j
i–j
i–k
0
1
2
3
4
5
6
1
2
3
4
5
6
7
8
9
10
11
12
k
k
k
k
k
k
k
k
u/v
FIGURE 7.47
Values of ∂r
∂u × ∂r
∂v in Problem 7.5.3.17.
(b) Divide D into six subrectangles of equal area. Sample the integrand in part
(a) at six points in those six subrectangles and use those data to find an
approximation of the surface area in part (a).
17. A surface S is parametrized by r = r(u, v) for (u, v) in the rectangle D = {(u, v) : 0 ≤
u ≤6, 0 ≤v ≤12}. The table in Figure 7.47 gives the values of the vectors ∂r
∂u × ∂r
∂v,
and the table in Figure 7.46 gives the values of the vectors r(u, v), both at some
values of (u, v) in D. Let F ≜x ˆı + y ˆj + z ˆk. Approximate

S F • dS using six
subrectangles of D. State which sampling point method you used, for example,
midpoints or, upper right points.
18. Some single variable calculus courses have a topic called “surfaces of revolution”:
When a curve in the xy-plane is revolved around an axis in the plane, find the
total surface area of the surface so generated. For example, if a curve is given in
the form y = f(x) for a ≤x ≤b and is revolved around the x-axis, then the surface
has area
Area = 2π
b
a
|f(x)|

1 + (f ′(x))2 dx.
Derive this result by considering the surface of revolution to be given in the form
S : r = x ˆı + f(x) cos ϕ ˆj + f(x) sin ϕ ˆk, a ≤x ≤b, 0 ≤ϕ ≤2π,

Integral Theorems, Multiple Integrals, and Applications
621
that is, by using the x-axis as the longitudinal axis and the alternative polar
coordinates y = r cos ϕ, z = r sin ϕ of Example 7.25.
19. (a) Extend the result of Problem 7.5.3.18 to the case of a surface of revolution gen-
erated by rotating a plane curve C : r = x(t)ˆı + y(t) ˆj, α ≤t ≤β, about the
x-axis.
(b) Apply the result of part (a) to find the surface area of the surface generated by
rotating the curve C : r = t2 ˆı + t3
3 ˆj, 2 ≤t ≤3, about the x-axis.
20. Let V be the solid whose base is in the plane z = 0 and whose top is the surface
z = 4−x2−3y2. Let S+ be the top surface that bounds V and let n be the unit vector
normal to S+ that points out of V. Define F = x ˆı +y2 ˆj +z3 ˆk. (a) Find a formula for
n as a function of (x, y, z). (b) Set up, with as much specificity as possible, but do
not evaluate

S F•n dS. (c) Set up, with as much specificity as possible, but do not
evaluate
	
V div(F)dV. “As much specificity as possible” means that, for example,
in part (a), writing your final conclusion in the form


d
d
and
filling in the blanks.
7.6 Integral Theorems: Divergence, Stokes, and Applications
Some of the most powerful tools in mathematical analysis and modeling of electrical and
magnetic phenomena were synthesized by George Green (Grattan-Guiness, 1995). Many
other mathematicians and scientists contributed, too, both before and after him.
Definition 7.17
A solid is nice if it is enclosed by a closed, piecewise smooth, orientable surface.
Theorem 7.14
(Divergence theorem in R3) Suppose F is a continuously differentiable vector field on a
solid V in R3. Suppose V is enclosed by a piecewise smooth, oriented parametrized surface
S with outward-pointing unit normal vectors n =n(r). Then

S
F • n dS =

V
∇• F dV.
(7.40)
This result is also known as Gauss’s theorem.
Caution: Suppose the surface is z = g(x, y). A common error is that when calculating
the surface integral in the LHS of (7.40), we can substitute z = g(x, y) into the formula for
F(x, y, z), but we cannot substitute z = g(x, y) into the formula for F(x, y, z) when calculating
the triple integral in the RHS of (7.40).

622
Advanced Engineering Mathematics
Example 7.37
Use the divergence theorem to redo Example 7.36 in Section 7.5.
Method: The solid V is the solid cylinder {(x, y, z) : −H ≤z ≤H, 0 ≤x2 + y2 ≤a2}; the
closed, positively oriented surface S encloses V; and F = ϱ0(x ˆı + 2 ˆj + y ˆk). The total rate
of mass flow out of the solid is

S
F • n dS =

V
∇• F dV =

V ϱ0 dV = ϱ0 • (Volume of V) = ϱ02πa2H. ⃝
If ∇• F is simple, perhaps as simple as a constant, then the divergence theorem makes
short and easy work of calculating a flux integral. Even if ∇•F is not simple, it might be eas-
ier to work with than F. And, it might be a lot easier to not have to wade through a swamp
of parametrizations of pieces of S and their respective outward unit normal vectors.
Example 7.38
Suppose F is an inverse square law force field due to a mass or electric charge at the
origin. Find the total flux out of F out of any nice solid V, as long as the origin is not in
V or on its boundary S.
Method: We calculate
∇• F = ∇•

k
||r||3 r

= ∂
∂x
 kx
ρ3

+ ∂
∂y
 ky
ρ3

+ ∂
∂z
 kz
ρ3

= k
ρ3 + kx ·

−3ρ−4 ∂ρ
∂x

+ k
ρ3 + kx ·

−3ρ−4 ∂ρ
∂y

+ k
ρ3 + kx ·

−3rho−4 ∂ρ
∂z

= kρ−3 −3kxρ−4 · x
ρ + kρ−3 −3kyρ−4 · y
ρ + kρ−3 −3kzρ−4 · z
ρ
= 3kρ−3 −3k(x2 + y2 + z2)ρ−5 = 3kρ−3 −3kρ2ρ−5 = 0.
By the divergence theorem,

S

k
||r||3 r

• n dS =

V
∇•

k
||r||3 r

dV =

V
0 dV = 0. ⃝
Corollary 7.5
Suppose F is an inverse square law force field due to a mass or electric charge at the origin.
If V is any nice solid that does contain the origin strictly inside, then the total flux of F out
of the boundary of V is 4πk.
Why? Since the origin is strictly inside V, there is a value of a sufficiently small that the
ball Ba ≜{r : ||r|| ≤a} is inside V. Call Sa = {r : ||r|| = a}, the boundary of the ball Ba
and define the set +V to be the set of points that are in V that are not in Ba. It follows that
V = Ba ∪+V

Integral Theorems, Multiple Integrals, and Applications
623
and the boundary of +V consists of two pieces, Sa and S, the boundary of the original set V.
By the divergence theorem,

Sa
F • +n dS +

S
F • +n dS =

+V ∇• F dV =

+V 0 dV,
by Example 7.38. It follows that

S
F • +n dS = −

Sa
F • +n dS.
As far as the solid +V is concerned, Sa is part of the boundary but has outward unit normal
vector +n pointing into the ball Ba. So,
−

Sa
F • +n dS = −(flux of F into Ba) = (flux of F out of Ba).
Denoting by n the unit vector pointing out of Ba,
flux of F out of Ba =

Sa
F • n dS = 4πk,
by the result of Example 7.34 in Section 7.5 (with
1
4πϵ0 replaced by k). In summary, this
gives the desired conclusion that

S
F • +n dS = flux of F out of Ba = 4πk. 2
Figure 7.48 shows half of a solid +V, that is, half of what is left when a ball is removed
from V. In the picture’s example, the outer surface S consists of (x, y, z) satisfying 9 = x2 +
(y2)1.7 + z2, and the inner surface S√
2 consists of (x, y, z) satisfying 2 = x2 + y2 + z2.
Figure 7.48 was produced by the Mathematica command
RegionPlot3D[9 ≥x2 + (y2)1.7 + z2 && x2 + y2+z2 ≥2, {x, −3, 0}, {y, −3, 3}, {z, −3, 3},
AxesLabel →{′′x′′,′′ y′′,′′ z′′}]
followed by rotating using a mouse.
7.6.1 The Divergence Theorem in R2
As you will explain in Problem 7.6.4.17, Green’s theorem implies that

C
F • n ds =

D
∇• F dA
for a nice enough planar vector field F and nice enough planar domain D enclosed by a
positively oriented curve C. Note that n is normal to the curve C in R2.

624
Advanced Engineering Mathematics
2
z
0
–2
–1
0
1
y
ν
Sa
~
–1
–2
–3
x
0
FIGURE 7.48
Half of +V.
7.6.2 Euler’s Conservation Equation
Example 7.39
(Euler’s conservation equation) Establish the principle of conservation of mass for fluid
flow:
∂ϱ
∂t + ∇• (ϱv) = 0.
(7.41)
Method: At first, it seems that what we are being asked to do is not connected to what
we were studying: What does (7.41), an equation involving derivatives, have to do with
integral theorems?
What can motivate our work is the intuition that the divergence theorem relates the
derivatives of a function, specifically the divergence of a vector field, to the behavior of
that function on a boundary.
Because we were told that this has something to do with flow of mass, it makes sense
to recall from Example 7.36 in Section 7.5 that

S
ϱ v • n dS
is the rate of mass flow across S; where ϱ is the mass density, in units of mass per
volume; v is the velocity vector field of the fluid, and S is a constant, that is, time-
independent, closed, orientable surface enclosing a solid V.
But, M =

V ϱ dV is the total mass of the fluid in the solid V. So, the rate of loss of
mass from V equals the rate of mass flow out of V across S; hence,

S
ϱ v • n dS = −dM
dt = −d
dt
'
V
ϱ dV
(
= −

V
∂ϱ
∂t dV,
(7.42)
the latter equality being true because V is time independent.

Integral Theorems, Multiple Integrals, and Applications
625
On the other hand, using the divergence theorem, (7.42) can be rewritten as

V
∂ϱ
∂t dV = −

S
ϱv • n dS = −

V
∇• (ϱv) dV;
hence,

V
∂ϱ
∂t + ∇• (ϱv)

dV = 0.
(7.43)
Because (7.43) is true for all solids V that are enclosed by a constant, closed, orientable
surface S, Euler’s conservation equation (7.41) follows.∗2
Recall from Section 6.7 that the vorticity of a fluid is ω ≜∇× v, where v is the fluid
velocity.
Example 7.40
Explain why the total flux of vorticity across any piecewise smooth, closed orientable
surface is zero, assuming the vorticity is a continuously differentiable function on the
surface and the solid it encloses.
Method: The total flux across such a surface S enclosing a nice solid V is

S
ω • n dS =

V
∇• ω dV =

V
∇• (∇× v) dV =

V
0 dV = 0,
because from Section 6.7, we recall that the divergence of a curl of a vector field is
automatically zero, as long as all the derivatives exist. ⃝
Similar to Corollary 7.4 in Section 7.3, we have
Corollary 7.6
If V is a solid enclosed by a closed, piecewise smooth surface S, then the volume of V is
given by
Volume =

S
x ˆı • dS.
Why? This follows from the divergence theorem. 2
Example 7.41
Use the divergence theorem to express

S
∂u
∂n dS
∗Assuming
 ∂ϱ
∂t + ∇• (ϱv)

is continuous, it cannot be nonzero at a point P because then the integral on a
sufficiently small solid ball about P would be nonzero, contradicting (7.43).

626
Advanced Engineering Mathematics
in spherical coordinates. Also, examine more specifically such integrals in the particular
case of a sphere about the origin.
Method: Recall that ∂u
∂n ≜(∇u) • n, so the divergence theorem says in general that

S
∂u
∂n dS =

S
(∇u) • n dS =

V
∇• (∇u) dV,
where V is the solid enclosed by the positively oriented surface S. But,
∇• (∇u) ≜∇2u
is the Laplacian of u. Section 6.7 gave the formula for the Laplacian in spherical
coordinates, so

S
∂u
∂n dS =

V
∇2u dV
=

V

1
ρ2
∂
∂ρ

ρ2 ∂u
∂ρ

+
1
ρ2 sin2 φ
∂
∂φ

ρ sin φ ∂u
∂φ

+
1
ρ2 sin2 φ
∂2u
∂θ2

dV.
For the specific case of a sphere about the origin, recall that the outward unit normal
vector is n = ˆeρ, the element of volume is ρ2 sin φ dρ dφ dθ, and the element of surface
area is dS = a2 sin φ dφ dθ. So, in the case of the sphere Sa = {r : ||r|| = a} that encloses the
ball Ba = {r : ||r|| ≤a},
2π

0
π
0
∂u
∂ρ (a, φ, θ) a2 sin φ dφ dθ =

Sa
∂u
∂ρ dS =

Sa
(∇u) • ˆeρ dS =

Sa
∂u
∂n dS
=

Ba

∂
∂ρ

ρ2 ∂u
∂ρ

+
1
sin φ
∂
∂φ

sin φ ∂u
∂φ

+
1
sin2 φ
∂2u
∂θ2

sin φ dρ dφ dθ
=
2π

0
π
0
a
0

∂
∂ρ

ρ2 ∂u
∂ρ

+
1
sin φ
∂
∂φ

sin φ ∂u
∂φ

+
1
sin2 φ
∂2u
∂θ2

sin φ dρ dφ dθ.
Using
a
0

∂
∂ρ

ρ2 ∂u
∂ρ

dρ =

ρ2 ∂u
∂ρ
a
0 = a2 ∂u
∂ρ (a, φ, θ) −0, we get
 2π
0
π
0
∂u
∂ρ (a, φ, θ) a2 sin φ dφ dθ
=
2π

0
π
0
∂u
∂ρ (a, φ, θ) a2 sin φ dφ dθ +
2π

0
π
0
a
0

∂
∂φ

sin φ ∂u
∂φ

+
1
sin φ
∂2u
∂θ2

dρ dφ dθ.
It follows that
2π

0
π
0
a
0

∂
∂φ

sin φ ∂u
∂φ

+
1
sin φ
∂2u
∂θ2

dρ dφ dθ = 0.
(7.44)
So, in the case of integration of ∂u
∂n on a sphere Sa, the divergence theorem amounts to
saying that

Sa
∂u
∂n dS =
2π

0
π
0
∂u
∂ρ (a, φ, θ) a2 sin φ dφ dθ =
2π

0
π
0
a
0
∂
∂ρ

ρ2 ∂u
∂ρ

dρ dφ dθ. ⃝

628
Advanced Engineering Mathematics
x
y
z
0
3
–2
–2
–1
0
1
–1
0
1
2
2
1
2
FIGURE 7.49
Positively oriented boundary curve.
We can informally “define” that C has “positive orientation” with respect to S if, as we
walk facing forward in the direction of travel along C, that is, in the direction of T(t), with
our heads “pointing” in the direction of n, then the surface will always be on our left, that
is, our left hand will be pointing toward S.
Here is a precise definition.
Definition 7.18
Suppose S is a piecewise smooth surface oriented by normal vectorn and there is a simple,
closed, piecewise smooth curve C : r = r(t) that is a boundary curve for S. We say C is
positively oriented with respect to S if at every point r(t) the unit tangent vector T has
n × T pointing toward S.
Example 7.42
Similar to work in Examples 6.18 in Section 6.3 and 7.29 in Section 7.5, let S be the
“northern” hemisphere of radius a:
S : r = r(φ, θ) = a(sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk), 0 ≤φ ≤π
2 , 0 ≤θ ≤2π,
that is,
S : r = a ˆeρ, 0 ≤φ ≤π
2 , 0 ≤θ ≤2π.
Study the orientation of the equator, that is, the circle {r : ||r|| = a} with respect to the
surface S.
Method: Let P be any point on the circle, say P = (a cos t0, a sin t0, 0). As the problem was
stated, we were not told which of the two possible unit normal vectors
± ˆeρ ≜± (sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk)
to use to orient S.

Integral Theorems, Multiple Integrals, and Applications
629
If we choose the upward orientation of S, that is,
n = (sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk),
then the counter clockwise orientation of the circle given by
C : r = a(cos t ˆı + sin t ˆj + 0 ˆk), 0 ≤t ≤2π,
will be positive with respect to the orientation of S. Why? Because at P the unit tangent
vector to C is
T = −sin t0 ˆı + cos t0 ˆj + 0 ˆk
and the unit normal vector to S is, noting that φ = π
2 at P,
n = cos t0 ˆı + sin t0 ˆj + 0 ˆk.
Thus,
n × T = (cos t0 ˆı + sin t0 ˆj + 0 ˆk) × (−sin t0 ˆı + cos t0 ˆj + 0 ˆk) = · · · = ˆk
points toward the northern hemisphere rather than away from it.
On the other hand, if we choose the downward orientation of S, that is,
n = −(sin φ cos θ ˆı + sin φ sin θ ˆj + cos φ ˆk),
then the clockwise orientation of the circle given by
C : r = a(cos τ ˆı −sin τ ˆj + 0 ˆk), 0 ≤τ ≤2π,
will be positive with respect to the orientation of S.
Why?
Because at P
=
(a cos t0, a sin t0, 0), that is, at τ = −t0, we calculate
dr
dτ

τ=−t0
= a(−sin τ ˆı −cos τ ˆj)

τ=−t0
= a(−sin(−t0) ˆı −cos(−t0) ˆj)
= a(sin(−t0) ˆı −cos(−t0) ˆj).
So, the unit tangent vector to C is
T = sin t0 ˆı −cos t0 ˆj + 0 ˆk
and the unit normal vector to S is, noting that φ = π
2 at P,
n = −(cos t0 ˆı + sin t0 ˆj + 0 ˆk),
and thus,
n × T = (−cos t0 ˆı −sin t0 ˆj + 0 ˆk) × (sin t0 ˆı −cos t0 ˆj + 0 ˆk) = · · · = ˆk
points toward the northern hemisphere rather than away from it. ⃝
Theorem 7.15
(Stokes’ theorem) Suppose C is a simple, closed, piecewise smooth boundary curve for S,
an oriented, piecewise smooth, parametrized surface with unit normal vectors n. Suppose
C is positively oriented with respect to S. If F is a continuously differentiable vector field
on an open solid containing S, then (7.46) is true, that is,

Integral Theorems, Multiple Integrals, and Applications
631
so n is given by
∂r
∂r × ∂r
∂θ = · · · = r(cos θ ˆı + sin θ ˆj + 1 ˆk).
(7.47)
or its negative. Because (7.47) has the upward orientation, we use it as the normal to S.
So, the work is
W =

S
ˆı • n dS =

S
ˆı •
∂r
∂r × ∂r
∂θ

dr dθ =
π
2
0
2
0
ˆı •

r(cos θ ˆı + sin θ ˆj + 1 ˆk)

dr dθ
=
π
2
0
2
0
r cos θ dr dθ =
⎛
⎝
π
2
0
cos θ dθ
⎞
⎠
⎛
⎝
2
0
r dr
⎞
⎠=

sin θ
 π
2
0
 ⎛
⎝
'
r2
2
(2
0
⎞
⎠= 1 · 2 = 2. ⃝
Example 7.44
Find the total flux of curl(F) across the northern hemisphere x2 + y2 + z2 = a2, z ≥0,
oriented upward, assuming F = y3ˆı −x3 ˆj + x ˆk.
Method: The northern hemisphere has the circle x2 + y2 = a2, that is, the equator, as
boundary curve. As in Example 7.42, if we give the surface the upward orientation,
then the boundary curve is positively oriented with respect to the surface if the curve
has the counter clockwise orientation. So, let
S : r = a ˆeρ, 0 ≤φ ≤π
2 , 0 ≤θ ≤2π,
and
C : R = a cos t ˆı + a sin t ˆj, 0 ≤t ≤2π.
Using Stokes’ theorem, we get that the total flux is given by

S
curl(F) • n dS =

C
F • dR =
2π

0

y3 ˆı −x3 ˆj + x ˆk

• dR
=
2π

0

(a sin t)3 ˆı −(a cos t)3 ˆj + a cos t ˆk

•
	
−a sin t ˆı + a cos t ˆj

dt
=
2π

0

−a4 sin4 t −a4 cos4 t

dt = −a4
2π

0

sin4 t + cos4 t

dt.
(7.48)
Some algebra and trigonometry give
sin4 t + cos4 t = sin4 t + (cos2 t)2 = sin4 t + (1 −sin2 t)2 = sin4 t + 1 −2 sin2 t + sin4 t
= 2 sin4 t + (1 −2 sin2 t) = 2
1 −cos 2t
2
2
+ cos 2t = 1
2(1 −cos 2t)2 + cos 2t
=
1
2 −cos 2t + 1
2 cos2 2t

+ cos 2t
=
1
2 −
cos 2t + 1
4 · (1 + cos 4t)

+
cos 2t
= 3
4 + 1
4 · cos 4t,

632
Advanced Engineering Mathematics
so the total flux is
−a4
2π

0
3
4 + 1
4 · cos 4t

dt =−a4
3t
4 + 1
16 · sin 4t
2π
0
= −3πa4
2
. ⃝
Learn More About It
Problems 7.6.4.15 and 7.6.4.16 are from Physics, Part I, by Robert Resnick and David
Halliday, John Wiley & Sons, c⃝1966, specifically Problem 16.11, page 418.
7.6.4 Problems
1. Let f(x, y, z) = 4x2 −3y2 −2z2. Find the total flux of ∇f out of the sphere x2 + y2 +
z2 = a2.
2. Let V be the solid that lies above the surface z =

x2 + y2 and below the plane
z = 3. Let S be the closed surface that encloses V. Evaluate

S F • dS, where
F(r) = r.
3. Evaluate

S(xy dy dz −y2 dz dx + xy dx dy) where S is the surface that encloses the
half cylinder x2 + y2 ≤9, y ≥0, 0 ≤z ≤2. [Assume that P dy dz + Q dz dx + R dx dy
is another notation for F • dS.]
4. Let V be the solid between the coaxial cylinders x2 + y2 = 4 and x2 + y2 = 9 for
0 ≤z ≤5. Find the total flux out of V for the vector field F = x3 ˆı + z2 ˆk.
5. Suppose φ satisfies Poisson’s equation ∇2φ = ψ(x, y, z) for some unspecified
function ψ. Why is

V
ψ dV =

S
∂φ
∂n dS
an identity for all nice solids V enclosed by a surface S?
6. Suppose φ satisfies Poisson’s equation ∇2φ = ψ(x, y, z) for some unspecified
function ψ. Is

V
∇φ • ∇φ dV =

S
φ · ∂φ
∂n dS −

V
φ ψ dV
an identity for all nice solids V enclosed by a surface S? If so, why? If not, why
not?
7. If F =
k
||r||3 r, that is, is an inverse square law force field with constant k, then we
know ∇• F = 0 for r ̸= 0. Why can’t we use the divergence theorem to conclude
that the total flux out of the sphere Sa = {r : ||r|| = a} is zero?
8. Define a solid ellipsoid by
V = {(x, y, z) : 0 ≤x2
a2 + y2
b2 + z2
c2 ≤1}.

Integral Theorems, Multiple Integrals, and Applications
633
Assume V is made of a homogeneous material. Find the moments of inertia
Ix = 0, Iy = 0, Iz = 0 defined by, for example,
Ix=0 ≜

V
x2ϱdV.
[Hint: Find a simple function F for which ∇•F = x2 and use the divergence theorem
for the parametric surface:
S : r = a sin φ cos θ ˆı + b sin φ sin θ ˆj + c cos φ ˆk,
0 ≤φ ≤π, 0 ≤θ ≤2π.]
9. Use Stokes’ theorem to evaluate

S curl(xy ˆı −xz ˆj) • dS, where the oriented
upward surface S is the part of the paraboloid z = 5−3(x2 +y2) that lies above the
plane z = 2.
10. Evaluate both sides of Stokes’ theorem for the case of a vector field F = 3y ˆı−2x ˆj +
xy ˆk, surface S = {(x, y, z) : x2 +y2 +z2 = 4, z ≥0}, and C being its boundary curve.
11. Use Stokes’ theorem to find the circulation of
F ≜−y3
3 ˆı + x3
3 ˆj + z ˆk
around the circle x2 + y2 = 1 in the z = 0 plane.
12. Redo Example 7.44 by continuing from (7.48) by noting that the curve C is also the
boundary of the flat surface
+
S : +r = x ˆı + y ˆj + 0 ˆk = r(cos θ ˆı + sin θ ˆj), 0 ≤r ≤a, 0 ≤θ ≤2π
and evaluating
Total flux =

S
curl(F) • n dS =

C
F • dR =

+
S
curl(F) • n dS = · · · .
13. If v is an irrotational fluid velocity vector, that is, ∇× v = 0 everywhere, then
explain why the lift around a simple, closed, piecewise smooth curve must be
zero.
14. Calculate both sides of the conclusion of Stokes’ theorem where S is the surface of
the circular paraboloid x2 + y2 = 7 −z that lies above the disk x2 + y2 = 4, z = 0 and
F = xz ˆı + y2 ˆj + x ˆk.
15. Suppose there is a homogeneous spherical shell of matter of total mass M centered
at the origin, as depicted in Figure 7.51a. Explain why the gravitational force on
an object of mass m is (a) zero at point P0 inside the shell and (b) −GMm
||r1||3 r1 at point
P1 outside the shell. Let ri be the position vector of point Pi, i = 1, 2.
16. Suppose there are concentric homogeneous spherical shells of matter of total
masses M1 and M2 centered at the origin, as depicted in Figure 7.51b. Use the

634
Advanced Engineering Mathematics
P0
P1
P1
P0
P2
M2
M1
M
(a)
(b)
FIGURE 7.51
Problems (a) 7.6.4.15 and (b) 7.6.4.16.
results of Problem 7.6.4.15 to find the gravitational force at points (a) P0 inside
both shells, (b) point P1 outside the inner shell but inside the outer shell, and (c)
point P2 outside both shells. Let ri be the position vector of point Pi, i = 0, 1, 2.
17. Explain why Green’s theorem implies that
(⋆)

C
F • n ds =

D
∇• F dA
for a nice enough planar vector field F and nice enough planar domain D enclosed
by a positively oriented curve C, where n is normal to the curve C in R2.
Note that if F = P ˆı + Q ˆj, then (⋆) says that

C
(P ˆı + Q ˆj) • n ds =

D
∂P
∂x + ∂Q
∂y

dA.
[Hint: Traveling along a curve C : r = r(t), a ≤t ≤b, the nonzero tangent
vectors are
˙r(t) = ˙x(t) ˆı + ˙y(t) ˆj.
Explain why n = −˙y(t) ˆı + ˙x(t) ˆj is normal to C. After that, rewrite (P ˆı + Q ˆj) •
(−˙y(t) ˆı + ˙x(t) ˆj)dt = (−Q ˆı + P ˆj) • (dx ˆı + dy ˆj) = −Q dx + P dy.]
18. Suppose a surface S and its boundary curve C satisfy the hypotheses of Stokes’
theorem. Explain why

C
(f ∇g) • dr =

S
(∇f × ∇g) • dS
is an identity.

Integral Theorems, Multiple Integrals, and Applications
635
7.7 Probability Distributions
Here we will develop a foundation for the study of randomness. The most basic concept is
Definition 7.19
A random variable X = X(ω) is a real valued function of an independent variable ω in
some set .∗A random variable is discrete if the values X(ω) can take on are a finite or
countable† set. A random variable is continuous if it is not discrete.
Example 7.45
A die’s six faces show the numbers 1, 2, . . . , 6. When we roll the die, we get a discrete
random variable X(ω). The probability that the die shows the number i is denoted
P({ω : X(ω) = i}) or P(X(ω) = i) or P(X = i) for short. If the die is fair, then 1
6 = P(X = i),
for i = 1, 2, . . . , 6. ⃝
There are deep philosophical issues in understanding what the statement 1
6 = P(X = i)
means. We won’t go into that but will instead use the “frequency interpretation,” namely,
that if we roll the same fair die over and over, without end, then the ratio of the number of
times the die shows i to the number of times the die is thrown will get closer and closer to
1
6, as the number of throws increases without end.
The practical problem with this interpretation is that we usually have better things to do
than to play endlessly with a die.
All probabilities are between 0 and 1, inclusive. It is not possible for something to happen
more than 100% of the time, even though that’s a phrase that is used in some commentaries
about professional athletics.
Example 7.46
Suppose we fill a car’s gasoline tank full and we let X(ω) denote the time t > 0 it takes
for the tank to be 90% emptied during normal use, assuming we don’t refill it until
that time. Then X(ω) is a continuous random variable. While we might have a good
idea about the output of the X(ω) function, there are many random influences on X(ω)
in our human activities, traffic conditions, and weather conditions. We might say that
P(X(ω) ≤1 month) is very close to 1, that is, we are confident that the tank will be 90%
empty in a month or less of normal driving. ⃝
∗It is usually not enlightening to know much about the set  except the technicalities that there should exist a
way to measure the size of some of its subsets and a well-defined concept of which sets are measurable. Those
concepts are beyond the level of the course; the book by Naylor and Sell, mentioned at the end of Chapter 2, is
a useful reference for the study of “measure theory.”
† A set A is countable if it is like the positive integers in that we can enumerate the elements of the set as an
infinite sequence a1, a2, . . . . For example, the set of all rational numbers, Q, is countable, but the set of all real
numbers, R, is not countable.

Integral Theorems, Multiple Integrals, and Applications
637
Example 7.47
For the fair die of Example 7.45, the CDF is
F(x) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if x < 1
1
6,
if 1 ≤x < 2
2
6,
if 2 ≤x < 3
...
5
6,
if 5 ≤x < 6
1,
if x ≥6
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
The CDF in Example 7.47, shown in Figure 7.52, satisfies F′(x) = 0 everywhere except
x = 1, 2, . . . , 6 and that F(x) has a jump of height 1
6 at x = i because
lim
x→i−F(x) = i −1
6
and lim
x→i+ F(x) = i
6, for i = 1, . . . , 6.
Example 7.48
Figure 7.53 shows a hypothetical CDF of the random variable of Example 7.46, that is,
the time it takes for my gas tank to be 90% emptied: This graph says that it is unusual,
1
x
F(x)
5
6
4
6
3
6
2
6
1
6
1
2
3
4
5
6
FIGURE 7.52
CDF for a fair die in Example 7.47.
1.00
0.75
0.50
0.25
x
F(x)
7
14
21
28
FIGURE 7.53
CDF in Example 7.48.

Integral Theorems, Multiple Integrals, and Applications
639
(b) For k = 1, using α = λ, we use integration by parts to calculate
∞

0
x f(x) dx =
∞

0
x αe−λx dx ≜lim
t→∞
t
0
λx e−λx dx = λ lim
t→∞

−x
λe−λx −1
λ2 e−λx t
0
= λ lim
t→∞

−t
λe−λt −1
λ2 e−λt + 1
λ2

= λ

0 + 0 + 1
λ2

= 1
λ.
To get this result, we used L’Hôpital’s rule and the assumption that λ is a positive
constant:
lim
t→∞te−λt = lim
t→∞
t
eλt = lim
t→∞
(t)′
(eλt)′ = lim
t→∞
1
λeλt = 0.
While
 ∞
−∞x0f(x) dx = 1, there is no such restriction for
 ∞
−∞xkf(x) dx for k = 1, 2, . . . .
For k ≥2, we can use a reduction of order formula to find the indefinite integral

xke−λxdx = −
k

j=0
k!
j! xjλ−(k−j+1) e−λx
to conclude, again using L’Hôpital’s rule, that
E[Xk] = k!
λk .
So, in this example, all of the higher moments exist. ⃝
In fact, for some density functions, only a few low-order moments exist.
In Calculus I, if s(t) is the position of a particle on a line, then ˙s(t) = v(t) is called
the instantaneous velocity of the particle. Analogously, f(x) = F′(x) can be called the
instantaneous probability P(X = x), that is, the probability of X(ω) = x happening.
Remarks
If X is a discrete random variable taking on only the values {x1, . . . , xN}, then
(a) -N
n = 1 P(X = xn) = 1 and (b) E[Xk] ≜-N
n = 1 xk
nP(X = xn).
Example 7.50
Find the first two moments of the fair die of Example 7.45.
Method: In this example, N = 6 and xn = n, for n = 1, . . . , 6. (a) The first moment is
E[X] =
6

n=1
nP(X = xn) =
6

n=1
n · 1
6 = 7
2,
and (b) the second moment is
E[X2] =
6

n=1
n2P(X = xn) =
6

n=1
n2 · 1
6 = 91
6 . ⃝

640
Advanced Engineering Mathematics
The first moment, E[X], is called the mean of the random variable and is denoted by
μ. Example 7.50(a) shows that the mean of a random variable might not equal any of its
output values X(ω).
The variance of a random variable is defined by
σ 2 ≜E[(X −μ)2].
Because μ is a real number, it can be considered a constant random variable Y(ω) ≜μ. It’s
easy to see that E[Y] = μ, so we can define E[μ] ≜μ. Using that, you will explain why
σ 2 = E[X2] −(E[X])2
(7.49)
in Problem 7.7.2.3. The standard deviation is σ, that is, the square root of the variance.
By the way (7.49) was used in Section 2.5 when we discussed linear regression.
Example 7.51
Estimate the mean of the random variable whose hypothetical CDF we graphed in
Example 7.48.
Method: We gave the graph of F, the CDF, in Figure 7.53. Based on the graph of F, we can
estimate the density function f = F′ by using difference quotients, that is,
f(x0) = F′(x0) ≈F(x1) −F(x0)
x1 −x0
.
We got the graph of f in Figure 7.54. If the region under the curve y = f(x) were a lamina,
we would guess that its moment Mx = 0, that is, the mean μ, is somewhere between 14
and 21. This says that typically my gasoline tank is 90% empty after about 14–21 days. ⃝
The mode of a continuous random variable is the value of x at which the density function
is maximized. So, the mode is the most frequent outcome of the random variable. While
the mode is, in general, different from the mean, the mode may be used as a very rough
estimate for the median.
Example 7.52
Perhaps the most famous PDF is that for a normal random variable:
f(x) =
1
√
2π σ
exp

−(x −α)2
2σ 2

.
(7.50)
0.10
f(x)
0.05
x
7
14
21
28
FIGURE 7.54
PDF in Example 7.51.

Integral Theorems, Multiple Integrals, and Applications
641
0.14
f(x)
0.12
0.10
0.08
0.06
0.04
0.02
x
–10
–5
5
10
15
FIGURE 7.55
PDF in Example 7.52.
Find the mean of such a random variable.
Another name for a normal random variable is “Gaussian random variable,” and its
distribution is known as the Gaussian distribution.
Method: An example of the graph of such a density function, for α = 1.5 and σ =
√
8, is
shown in Figure 7.55, so we see why this is often called a “bell curve.” You will show in
Problem 7.7.2.8 that this density function satisfies
 ∞
−∞f(x) dx = 1. In this problem, we
need to calculate
μ = lim
b→∞
α+b

α
xf(x) dx + lim
a→∞
α
α−a
xf(x) dx ≜lim
b→∞I1 + lim
a→∞I2,
assuming both limits can be shown to exist. First, substituting z = x −α gives
I1 =
α+b

α
xf(x)dx =
α+b

α
x
1
√
2π σ
exp

−(x −α)2
2σ 2

dx =
b
0
(z + α)
1
√
2π σ
exp

−z2
2σ 2

dz,
so
I1 =
⎛
⎝α
b
0
1
√
2π σ
exp

−z2
2σ 2

dz +
b
0
z
1
√
2π σ
exp

−z2
2σ 2

dz
⎞
⎠.
Using the substitution w = −z2
2σ 2 and dw = −1
σ 2 z dz, we get

z
1
√
2π σ
exp

−z2
2σ 2

dz =

z
1
√
2π σ
ew −σ 2dw
z
= −
σ
√
2π
ew.
So,
lim
b→∞I1 = lim
b→∞
⎛
⎝α
b
0
1
√
2π σ
exp

−z2
2σ 2

dz −
σ
√
2π
exp

−b2
2σ 2

+
σ
√
2π
⎞
⎠
= α
∞

0
1
√
2π σ
exp

−z2
2σ 2

dz +
σ
√
2π
.

642
Advanced Engineering Mathematics
Similarly,
I2 =
0
−a
(z + α)
1
√
2π σ
exp

−z2
2σ 2

dz = · · · = α
0
−∞
1
√
2π σ
exp

−z2
2σ 2

dz −
σ
√
2π
.
So,
μ = lim
b→∞I1 + lim
a→∞I2
= α
∞

0
1
√
2π σ
exp

−z2
2σ 2

dz +


σ
√
2π
+ α
0
−∞
1
√
2π σ
exp

−z2
2σ 2

dz −


σ
√
2π
= α
∞

−∞
1
√
2π σ
exp

−z2
2σ 2

dz = α · 1 = α.
So, μ = E[X] = α is the mean of this random variable. ⃝
Using similar calculations, we can show that for this random variable,
E[(X −μ)2] = σ 2,
that is, that variance of this random variable is σ 2.
Theorem 7.17
If α is a constant and E[X] exists, then (a) E[αX] exists and equals αE[X], and (b) E[ X + α ]
exists and equals E[X] + α.
Definition 7.22
The (Gaussian) error function is defined by
erf(x) ≜
2
√π
x
0
e−u2 du.
Theorem 7.18
The error function is related to F(x), the CDF of a normal random variable X with
mean = μ = 0 and variance = σ 2 = 1
2, in the sense that
erf(x) = P(−x ≤X ≤x) = F(x) −F(−x) =
x
−x
1
√π e−u2 du.

Integral Theorems, Multiple Integrals, and Applications
643
7.7.1 Joint Distribution
Suppose X and Y are random variables, and suppose P(X(ω) ≤x and Y(ω) ≤y), the
probability of the simultaneous event that both X ≤x and Y ≤y, satisfies
P(X(ω) ≤x and Y(ω) ≤y) =

{(u,v): −∞<u≤x,−∞<v≤y}
h(u, v)dudv,
for a function h(x, y). Then we call h(x, y) their joint probability density (function).
Theorem 7.19
The joint density function h(x, y) for random variables satisfies
P(a ≤X(ω) ≤b and c ≤Y(ω) ≤d) =
d
c
b
a
h(x, y)dxdy.
Definition 7.23
Two random variables X and Y are independent if for all intervals I and J,
P({ω : X(ω) is in I and Y(ω) is in J}) = P({ω : X(ω) is in I}) · P({ω : Y(ω) is in J}).
Theorem 7.20
If X and Y are independent random variables with PDF’s f(x) and g(x), respectively, then
their joint distribution function is
h(x, y) = f(x)g(y).
Theorem 7.21
If X and Y are independent random variables with PDF’s f(x) and g(x), respectively, then
the random variable Z ≜X + Y has PDF
k(x) =
∞

−∞
f(u)g(x −u)du.
(7.51)
[This is also called a convolution on the real line; it is more general than the type of
convolution in Definition 4.4 in Section 4.5 we used extensively in work with Laplace
transforms.]

644
Advanced Engineering Mathematics
Why? If the joint PDF of X and Y is h(x, y), then we can extend its definition and the result
of Theorem 7.19 to see that
P(Z ≤z) = P(X + Y ≤z) =

{(x,y): x+y≤z}
h(x, y)dxdy.
Because x + y ≤z is equivalent to y ≤z −x, we can rewrite this as an iterated integral:
P(Z ≤z) =
∞

−∞
z−x

−∞
h(x, y)dydx =
∞

−∞
 z−x

−∞
f(x)g(y)dy

dx.
Make the substitution u = y+x, that is, x = u−y, so dy = du. Note that u = z when y = z−x.
This gives
P(Z ≤z) =
∞

−∞

z
−∞
f(x)g(u −x)du

dx.
But for any fixed z, the infinite strip
{(u, x) : −∞< x < ∞, −∞< u ≤z} = {(x, u) : −∞< u ≤z, −∞< x < ∞},
so F(z), the CDF for Z, is given by
F(z) ≜P(Z ≤z) =
z
−∞
 ∞

−∞
f(x)g(u −x)dx

du.
Let k be the density function for Z. It follows from the definition of PDF in Definition 7.20
that F(z) =
 z
−∞k(u)du, so
k(u) ≜
 ∞

−∞
f(x)g(u −x)dx

(7.52)
is a PDF for Z = X + Y. Reverse the roles of x and u in (7.52) to see that (7.51) is correct,
that is,
k(x) =
∞

−∞
f(u)g(x −u)du. 2
Two random variables X, Y are identically distributed if they have the same CDF, that
is, for every x, P(X ≤x) = P(Y ≤x). Saying two random variables are identically dis-
tributed is not the same as saying the random variables are equal. This is related to what
we said earlier, “We may have no idea how X(ω) functions but we may know something
about what it produces.”

Integral Theorems, Multiple Integrals, and Applications
645
7.7.2 Problems
1. Find the standard deviation of the fair die of Example 7.45.
2. Suppose λ is a positive constant and a random variable X has density function
f(x) = αe−λ|x|, where α is a constant. (a) What value must α be? (b) Which moments
of X exist?
3. Explain why E[(X −μ)2] = E[X2] −μ2, as long as the first and second moments,
E[X] and E[X2], exist. [Hint: Use Theorem 7.17 and E[(X −μ)2] = E[X2 −2μ+μ2].]
4. Suppose X and Y are independent, identically distributed (i. i. d.) random vari-
ables whose density function is the one given in Example 7.49. Find the density
function of X + Y.
5. Suppose X and Y are independent normal random variables, both of which have
mean 0 and standard deviation σ. Find the density function for X + Y.
6. Explain why Theorem 7.18 is correct.
7. Briefly discuss how (7.49) was used in Section 2.5 when discussing linear regres-
sion.
8. Follow these steps to explain why
 ∞
−∞e−x2/2dx =
√
2π. Define f(x, y) = e−(x2+y2)/2.
(a) Let D = {(r, θ) : 0 ≤θ ≤2π, 0 ≤r ≤R} and calculate that

D f(x, y)dA =
2π(1 −e−R2/2).
(b) Let +
D = {(x, y) : −R ≤x ≤R, −R ≤x ≤R}. Explain why |f(x, y)| ≤e−R2/2 for
all (x, y) that are in the square +
D but are not in the disk D.
(c) Explain why (4 −π)R2 is the area of the region N that lies inside +
D but
outside D.
(d) Use parts (b) and (c) together to explain why

N f(x, y)dA
 ≤(4−π)R2e−R2/2.
(e) Use L’Hôpital’s rule to explain why limR→∞

N f(x, y)dA = 0.
(f) Explain why

+
D f(x, y)dA =
 R
−R e−u2/22
.
(g) Use parts (a) and (f) to explain why limR→∞
 R
−R e−u2/2 =
√
2π.
(h) Use part (g) to explain the convergences of
 ∞
0
e−u2/2 =
√
2π
2
and
 ∞
−∞e−u2/2 =
√
2π.
Key Terms
anti-derivative: Definition 7.2 in Section 7.1
anti-partial derivative: before Example 7.11 in Section 7.3
arclength function: Definition 7.6 in Section 7.2
average value: Defnitions 7.3 in Section 7.1, (7.9) in Section 7.2, (7.11) in Section 7.3, (7.12)
in Section 7.4
chain: before (7.11) in Section 7.2
circular symmetry: before Example 7.23 in Section 7.4, Example 7.25 in Section 7.4
circulation: (7.11) in Section 7.2
closed: Definition 7.15 in Section 7.5
conservative: after Example 7.8 in Section 7.2

646
Advanced Engineering Mathematics
continuous random variable: Definition 7.19 in Section 7.7
convergent: Definition 7.4 in Section 7.1
convolution on the real line: (7.51) in Section 7.7
countable: Definition 7.19 in Section 7.7
cumulative distribution function (CDF): Definition 7.20(a) in Section 7.7
definite integral: before Theorem 7.1 in Section 7.1
density function: Definition 7.20(b) in Section 7.7
discrete random variable: Definition 7.19 in Section 7.7
Divergence Theorem: Theorem 7.14 in Section 7.6
divergent: Definition 7.4 in Section 7.1
double Riemann sum: (7.20) in Section 7.3
electrostatic potential difference: before Example 7.9 in Section 7.2
element of arclength: (7.4) in Section 7.2
element of area: after (7.21) in Section 7.3
Euler’s Conservation Equation: Example 7.39 in Section 7.6
expectation: Definition 7.21 in Section 7.7
exponential distribution: Example 7.49 in Section 7.7
flux of F across S: after (7.39) in Section 7.5
Fundamental Theorem of Calculus: Theorem 7.4 in Section 7.1
Fundamental Theorem of Line Integrals: Theorem 7.8 in Section 7.2
Gaussian distribution: Example 7.52 in Section 7.7
(Gaussian) error function: Definition 7.22 in Section 7.7
Gauss Theorem: Theorem 7.14 in Section 7.6
Green’s Theorem: Theorem 7.13 in Section 7.3
higher moments: Definition 7.21 in Section 7.7
improper integral: Definition 7.4 in Section 7.1
identically distributed: after Theorem 7.21 in Section 7.7
indefinite integral: Definition 7.2 in Section 7.1
independent: Definition 7.23 in Section 7.7
iterated integral: (7.22) in Section 7.3
joint probability density function: Section 7.7.1
lift: (7.12) in Section 7.2
line integral: (7.5) in Section 7.2
line integral of a vector field: (7.9) in Section 7.2
magnetic permeability: before Example 7.9 in Section 7.2
mean: before (7.49)
Method of Integration by Parts: Theorem 7.7 in Section 7.1
Method of Substitution: Theorem 7.6 in Section 7.1
mode: after Example 7.51 in Section 7.7
nice solid: Definition 7.17 in Section 7.6
normal random variable: Example 7.52 in Section 7.7
opposite curve: (7.14) in Section 7.2
orientable, oriented, orients: Definition 7.14 in Section 7.5
path independent: before Example 7.8 in Section 7.2
piecewise smooth surface: Definition 7.13 in Section 7.5
polar moment of inertia: Example 7.22 in Section 7.4, Problem 7.3.7.34
positively oriented curve: after (7.10) in Section 7.2
positive orientation: Definition 7.16 in Section 7.5
positively oriented with respect to S: Definition 7.18 in Section 7.6

Integral Theorems, Multiple Integrals, and Applications
647
potential flow: after (7.44) in Section 7.6
probability: Example 7.45 in Section 7.7
probability distribution function (PDF): Definition 7.20(b) in Section 7.7
random variable: Definition 7.19 in Section 7.7
regular partition: Theorem 7.1 in Section 7.1
Riemann sum: before Theorem 7.1 in Section 7.1
semi-infinite: before Definition 7.4 in Section 7.1
standard deviation: after (7.49)
surface integral of a scalar valued function: Section 7.5.1
surface integral of a vector field: (7.38) in Section 7.5
tangency condition: after (7.44) in Section 7.6
triple Riemann sum: (7.29) in Section 7.4
type I or type II: before Example 7.17 in Section 7.3
upward orientation: Example 7.35 in Section 7.5
variance: before (7.49)
work: (7.10) in Section 7.2
Mathematica Commands
ParametricPlot[{−1 + 2t∧(3/2), 1 + t}, {t, 0, 2}]: before Example 7.5 in Section 7.2
ParametricPlot3D[{2Cos[t], 2Sin[t], t}, {t, 0, 2π}]: Example 7.6 in Section 7.2
Graphics3D[Polygon[{{12, 0, 0}, {0, 4, 0}, {0, 0, 3}, {0, 0, 0}}], Axes →True]: after Example
7.16 in Section 7.3
ParametricPlot3D[{{rCos[θ], rSin[θ], 2r2}, {rCos[θ], rSin[θ], 6 −3r}}, {r, 0, r0}, {θ], 0, 2π]}]:
before Example 7.25 in Section 7.4
RegionPlot3D[x2 + y2 ≤z2
3 && x2 + y2 + (z −1)2 ≤1, {x, −
√
3
2 ,
√
3
2 }, {y, −
√
3
2 ,
√
3
2 }, {z, 0, 2},
AxesLabel →{′′x′′, ′′y′′,′′ z′′}]: before Example 7.27 in Section 7.4
RegionPlot3D[1 ≤x2 + y2 + z2& & x2 + y2 + z2 ≤9, {x, 0, 3}, {y, 0, 3}, {z, 0, 3}, AxesLabel →
{′′x′′,′′ y′′,′′ z′′}]: before Example 7.28 in Section 7.4
RegionPlot3D[9 ≥x2 + (y2)1.7 + z2 && x2 + y2 + z2 ≥2, {x, −3, 0}, {y, −3, 3}, {z, −3, 3},
AxesLabel →{′′x′′,′′ y′′, ′′z′′}]: before Example 7.39 in Section 7.6
Reference
Grattan-Guiness, I. Why did George Green write his essay of 1828 on electricity and magnetism?
American Mathematical Monthly 102, 387–396, (1995).


8
Numerical Methods I
8.1 Solving a Scalar Equation
Some engineering problems can be reduced to finding a real number x that solves a single
equation
f(x) = 0.
(8.1)
A characteristic equation 0 = |A −λI| is an example of such an equation. We will study
numerical methods that produce a sequence

xk

= x0, x1, x2, . . .
of approximations for a solution x⋆of (8.1).
8.1.1 Newton–Raphson Method
Given an initial guess x0 for a solution of (8.1), we can construct a next guess, x1,
by “following the tangent line,” as illustrated in Figure 8.1. We hope x1 will be an
improvement over x0 in the sense that 0 ≤| f(x1)| < | f(x0)|.
The Newton–Raphson method, or Newton’s method, for short, says to take the next
guess to be
x1 = x0 −f(x0)
f ′(x0).
(8.2)
Why? The tangent line to the curve y = f(x) at the point (x, y) = (x0, f(x0)) has slope f ′(x0),
so the point–slope formula gives
f ′(x0) = y −f(x0)
x −x0
,
that is,
y = f(x0) + f ′(x0) (x −x0) .
Newton’s method hopes that the point (x1, 0) on the tangent line happens to be closer to a
point (x⋆, f(x⋆)) where f(x⋆) = 0. For (x1, 0) to be on the tangent line, we need
649

650
Advanced Engineering Mathematics
y
y= f(x)
(x0, f (x0))
x
x0
x1
x*
FIGURE 8.1
Newton’s method.
0 = f(x0) + f ′(x0) (x1 −x0) .
(8.3)
Solving (8.3) for x1 gives (8.2).
Continuing from x0 and x1, we can construct the iterates
x2 = x1 −f(x1)
f ′(x1),
x3 = x2 −f(x2)
f ′(x2),
. . . .
In general,
xk+1 = xk −f(xk)
f ′(xk), k = 0, 1, 2, . . . .
(8.4)
Formula (8.2) defines Newton’s Method and requires one initial guess x0.
Example 8.1
Find an approximate solution of x4 + 8x2 + 21x2 + 20x + 5.
Method: Define f(x) = x4 + 8x2 + 21x2 + 20x + 5. With initial guess x0 = −1, Newton’s
method running on a calculator or a computer spreadsheet produces iterates x1, x2, . . .
in Table 8.1. It appears that the iterates converge to x⋆≈−1.3819660113 and f(x⋆) ≈
0.0000000000, so x⋆seems to be a very good approximation of a solution of f(x) = 0. ⃝
TABLE 8.1
Example 8.1 by Newton’s Method
k
xk
f(xk)
△xk ≜xk −xk −1
0
−1.0000000000E + 00
−1.0000000000E + 00
1
−1.5000000000E + 00
3.1250000000E −01
−5.0000000000E −01
2
−1.3750000000E + 00
−1.9287109375E −02
1.2500000000E −01
3
−1.3819542254E + 00
−3.2575519246E −05
−6.9542253521E −03
4
−1.3819660112E + 00
−9.8363983625E −11
−1.1785862403E −05
5
−1.3819660113E + 00
0.0000000000E + 00
−3.5588421099E −11
6
−1.3819660113E + 00
0.0000000000E + 00
0.0000000000E + 00

Numerical Methods I
651
Also, in the table we see that the changes in the iterates,
△xk ≜xk+1 −xk,
(8.5)
display quadratic convergence, meaning that for some constant c and index K,
|△xk+1| ≤c |△xk|2,
(8.6)
for all k ≥K. This shows that the convergence of the iterates is rapid after index K.
A typical software implementation of Newton’s method stops the iteration when the
absolute value of the change in the iterates, that is, |△xk|, is less than a specified value, ε.
The requirement
|△xk| < ε
(8.7)
is called a stopping rule. An example of another stopping rule is |f(xk)| < ε.
In Example 8.1, if we had used ε = 1.E −08 and stopping rule (8.7), then the process
would have stopped at k = 5 and reported −1.3819660112 as an approximate solution. If,
instead, we had used ε = 1.E −04, then the process would have stopped at k = 4 and
reported −1.3819542254 as an approximate solution.
We see from the abstract picture in Figure 8.1 that there may be more than one solution
of (8.1). Which solution Newton’s method converges to can depend on the initial guess,
x0. In Example 8.1, if instead of x0 = −1 we had used x0 = −0.8, then Newton’s method
has iterates xk →x⋆≈−3.6180339887E + 00 as k →∞.
By the way, the word “iterate” is both a noun, as in “xk+1 is the next iterate,” and a verb,
as in “iterate (8.4), that is, Newton’s method.”
Theorem 8.1
If f is twice continuously differentiable at x⋆, f(x⋆) = 0, and f ′(x⋆) ̸= 0, then there is an
open interval I = (x⋆−δ, x⋆+ δ) such that if the initial guess x0 is in I, then the sequence
of Newton’s method iterates {xk} converges quadratically to x⋆as k →∞.
At first, this theorem sounds like a good result, but it is of little practical use because
it requires knowledge of a solution x⋆. If we already know a solution, there would be no
point in finding a sequence of iterates that converges to it. Nevertheless, Theorem 8.1 is a
good inspiration for a practical result.
We know from the intermediate value theorem of calculus that if a function f is contin-
uous on a closed interval [a, b] and f(a) · f(b) < 0 then f has at least one solution in the
interval (a, b), because f(a) · f(b) < 0 implies f changes sign on [a, b]. Here’s a result that
improves on Theorem 8.1.
Theorem 8.2
Suppose f is twice continuously differentiable on an interval (a, b) and continuous on [a, b]
and has f(a) · f(b) < 0. If, in addition, f ′(x) ̸= 0 on (a, b), then in (a, b) there is an x⋆that

652
Advanced Engineering Mathematics
solves (8.1) and a subinterval (c, d) such that whenever the initial guess x0 is in (c, d), then
the iterates of Newton’s method converge quadratically to x⋆as k →∞.
This is a better theorem but it is still vague as to how to find the subinterval in which
to take our initial guess. The problem of narrowing the interval where the initial guess is
taken can be solved partly by using the bisection method: the interval [a, b] can be split
into two equal subintervals, [a, 0.5(a + b)] and [0.5(a + b), b]. Assuming f(a)f(b) < 0, in
at least one of those two intervals f will change sign, so we can use the bisection method
again to split the half-sized interval into quarter-sized intervals and then into eighth-sized
intervals, etc. In this way we can slowly “creep up” to a solution. When we get close
enough to a solution, we can either use a stopping rule or use Newton’s method to rapidly
converge to a solution, x⋆. How close is close enough? Well, that is another theorem, but
we will not state it here because it involves a complicated set of hypotheses to verify.
If, while using the bisection method, an endpoint of a subinterval happens to be a
solution, then we’ve “hit the jackpot” and we can stop bisecting.
What if f ′(x⋆) = 0 or f ′(x⋆) does not exist? Then neither Theorem 8.1 nor 8.2 applies. As a
matter of logic, that does not imply that there is no solution of (8.1) nor does it imply that
Newton’s method does not converge to a solution.
Why, as a matter of logic? Consider the theorem that says, “If f(x) is differentiable at x0
then f is continuous at x0.” If the hypothesis fails, that is, if f is not differentiable at x0, does
that imply that f is not continuous at x0? No. For example, f(x) ≜|x| is not differentiable
at 0, but, in spite of that, f is continuous at 0.
Regardless of the logic, it is good to have examples where f ′(x⋆) is not existing or is zero
and Newton’s method runs into trouble. The behaviors of the iterates in these examples
are models for what you might actually experience in much more complicated situations.
Example 8.2
Use Newton’s method to attempt to find an approximate solution of f(x) = 0, where
f(x) ≜

−√−x,
x < 0
√x,
x ≥0

.
(8.8)
Method: For x ̸= 0,
f ′(x) =
⎧
⎪⎨
⎪⎩
1
2·√−x,
x < 0
1
2·√x,
x > 0
⎫
⎪⎬
⎪⎭
,
so Newton’s method is
xk+1 = · · · = xk −
⎧
⎨
⎩
(−√−xk) · 2 · √−xk,
if xk < 0
√xk · 2 · √xk,
if xk > 0
⎫
⎬
⎭= xk −2xk = −xk,
as long as xk ̸= 0.
No matter what nonzero initial guess we take, the iterates cycle back and forth
between x0 and −x0. For example, with initial guess x0 = 1, the iterates cycle back
and forth between 1 and −1, as shown in Figure 8.2. Newton’s method is unsuccessful
(Table 8.2). ⃝

Numerical Methods I
653
y
x
–1.0
–2
–0.5
0.5
2
1.0
x1=–1
1=x0
y = f (x)
FIGURE 8.2
Example 8.2: Cycling in Newton’s method.
TABLE 8.2
Example 8.2: Cycling in Newton’s Method
k
xk
f(xk)
△xk ≜xk −xk −1
0
1.0000000000E + 00
1.000000000E + 00
1
−1.0000000000E + 00
−1.000000000E + 00
−2.0000000000E + 00
2
1.0000000000E + 00
1.000000000E + 00
2.0000000000E + 00
3
−1.0000000000E + 00
−1.000000000E + 00
−2.0000000000E + 00
So, what method would be successful in Example 8.2? The bisection method is usually
slow but sure, just as the tortoise was in the fable of “the tortoise and the hare.”
Example 8.3
Use the bisection method to attempt to find an approximate solution of f(x) = 0, where
f was given in (8.8).
Method: Because f(−1) = −1 and f(1) = 1 have opposite signs, the intermediate value
theorem guarantees that there is a solution of f(x) = 0 in the interval [−1, 1]. Divide this
interval into two equal subintervals, [−1, 0] and [0, 1], and choose one of them. We hit
the jackpot, because f(0) = 0 gives us a solution. ⃝
8.1.2 Modiﬁed Newton’s Method
If f(x⋆) = f ′(x⋆) = 0, then Newton’s method will have difficulty for iterates xk ≈x⋆. The
order of the zero of f(x) at x⋆is the integer m for which f(x⋆) = f ′(x⋆) = · · · = f (m)(x⋆) = 0,
but f (m+1)(x⋆) ̸= 0, assuming there is such a finite m. If f(x) is m + 1 times continuously
differentiable at x⋆and has a zero of order m at x⋆, then f(x) = (x −x⋆)mg(x) for some
function g(x) that is continuous for x near x⋆and satisfies g(x⋆) ̸= 0.
The modified (or corrected) Newton’s method (Rall, 1966)
xk+1 = xk −m · f(xk)
f ′(xk), k = 0, 1, 2, . . .
will converge to the root x⋆, as long as the initial guess x0 is close enough to x⋆and m is the
order of the zero of f(x) at x⋆.

654
Advanced Engineering Mathematics
8.1.3 Secant Method
Suppose that a function f(x) is not defined by a formula but is instead described by a
physical or software process that produces a unique output y for each input x in some
interval I. For example, one software subroutine might produce the output y by some
process so complicated as to make it too difficult to find a formula for f.
If we have no formula for f(x), then we also have no formula for f ′(x), and thus, we can’t
use Newton’s method. But, instead, we can approximate
f ′(xk) ≈f(xk) −f(xk−1)
xk −xk−1
= △fk
△xk
,
(8.9)
where we define
△fk ≜f(xk) −f(xk−1).
(8.10)
Geometrically, we are approximating the slope of the tangent line by the slope of a secant
line, so we can call the following a secant method:
xk+1 = xk −
f(xk)
△fk/ △xk
, k = 1, 2, 3, . . . ,
that is,
xk+1 = xk −(xk −xk−1) f(xk)
f(xk) −f(xk−1) = xk−1f(xk) −xkf(xk−1)
f(xk) −f(xk−1)
, k = 1, 2, 3, . . . .
(8.11)
This method is also known as the regula falsi, Latin for false position, method. It requires
two initial guesses x0, x1 in order to calculate x2; after that, x1, x2 are used to calculate
x3, etc.
We see that there is danger in this method: the denominators, f(xk) −f(xk−1), will get
close to zero if a sequence of iterates {xk} starts to converge to a value x⋆. Why? If {xk} starts
to converge to x⋆, then both xk and xk−1 will be close to x⋆; if f(x) is continuous at x⋆, then
the denominators, f(xk) −f(xk−1), will be close to f(x⋆) −f(x⋆) = 0.
Of course, the numerators, (xk −xk−1) f(xk), will also be close to zero if {xk} starts
to converge. So, the secant method process involves a delicate situation of xk+1 = xk −
(close to zero)/(close to zero).
What makes the delicate situation even more dangerous is that if two numbers, such as
f(xk) and f(xk−1), are close to each other, then their difference, f(xk) −f(xk−1), may behave
wildly due to “round-off error.” We will not attempt a deep theoretical analysis of how
round-off-error can accumulate, but we note that numerical methods deal in numbers
stored in a computer memory with only a fixed, finite number of digits of precision. For
example, if the computer is forced to round-off
f(xk) = 1.000000024999 ≈1.00000002
and
f(xk−1) = 1.000000005001 ≈1.00000001,

Numerical Methods I
655
then the machine acts as if f(xk) −f(xk−1) = 1.00000002 −1.00000001 = 1 × 10−9. Instead,
if the machine could keep all of the digits of the function values, it would calculate that
f(xk) −f(xk−1) = 1.000000024999 −1.000000005001 = 0.00000001998 ≈2 × 10−9.
There is a big difference between dividing by 1 × 10−9 and dividing by 2 × 10−9! So, the
numerical process may move from xk to xk+1 by almost twice as much as it would if there
were no round-off error. Thus, the process may produce wild swings of the iterates.
One helpful technique is to replace f(xk) by a weighted average of f(xk) and f(xk−1):
xk+1 = xk −(xk −xk−1)

w · f(xk) + (1 −w) · f(xk−1)

f(xk) −f(xk−1)
, k = 1, 2, 3, . . . ,
(8.12)
where the user can adjust the constant w in the interval [0, 1]. Perhaps the reason that (8.12)
can produce better practical results is that the weighted average (w·f(xk)+(1−w)·f(xk−1))
may help to dampen wild swings in the iterates.
In fact, the secant method in (8.11) can give super linear convergence, specifically
|△xk+1| ≤c |△xk|α
(8.13)
with α = 1+
√
5
2
, under the same hypotheses under which Newton’s method gave quadratic
convergence, that is, with α replaced by 2. Linear convergence has α = 1 and quadratic
convergence has α = 2.
In the case of linear convergence, |△xk+2| ≤c |△xk+1| ≤c · c |△xk|, so
|△xk| ≤ck−1 |△x1|.
There is convergence if 0 ≤c < 1.
In the case of convergence with exponent α > 1,
|△xk+2| ≤c |△xk+1|α ≤c · (c |△xk|α)α, . . . ,
so
|△xk| ≤c(1+α+···+αk−1) |△x1|αk.
8.1.4 Fixed Point Problem Iteration
Solving x = g(x) for a real number x is called a fixed-point problem because we want to
find an x that does not move under the action of g(x). Pictorially, solving x = g(x) is the
same as finding a point of intersection of the graphs of y = g(x) and y = x, as illustrated in
Figure 8.3.
The simplest numerical method for solving x = g(x) is to pick x0, an initial guess for the
solution, and iterate
xk+1 = g(xk), for k = 0, 1, 2, . . . .
(8.14)
This is called fixed-point iteration or the method of successive approximations.

656
Advanced Engineering Mathematics
y=cos(x)
y=x
x
y
–3
–3
–2
–2
–1
–1
1
1
2
2
FIGURE 8.3
Fixed-point problem x = cos(x).
TABLE 8.3
Example 8.4 by Successive Approximations
k
xk
g(xk)
△xk + 1 ≜xk + 1 −xk
0
0.0000000000
1.0000000000
1
1.0000000000
0.5403023059
−0.4596976941
2
0.5403023059
0.8575532158
0.3172509100
3
0.8575532158
0.6542897905
−0.2032634253
4
0.6542897905
0.7934803587
0.1391905682
5
0.7934803587
0.7013687736
−0.0921115851
6
0.7013687736
0.7639596829
0.0625909093
7
0.7639596829
0.7221024250
−0.0418572579
8
0.7221024250
0.7504177618
0.0283153367
· · ·
· · ·
· · ·
· · ·
30
0.7390822985
0.7390870427
4.744172930 × 10−6
Example 8.4
Find an approximate solution of x = cos(x) using the method of successive approxima-
tions.
Method: An initial guess x0 = 0 produces the results in Table 8.3. We used an Excel
spreadsheet for this, although we could have used a basic calculator instead. It looks
as if the iterates are converging to about 0.7391 but slowly, for example, with linear
convergence. ⃝
8.1.5 Aitken’s δ2 Method
Aitken’s δ2 method is a convergence acceleration technique that speeds up linear conver-
gence to quadratic convergence, assuming the initial guess is close enough to a simple∗
solution x⋆for the equation x = g(x).
∗Meaning g(x⋆) = x⋆and |g′(x⋆)| ̸= 1.

Numerical Methods I
657
TABLE 8.4
Example 8.5 by Aitken’s δ2 Method
k
xk
f(xk)
△xk + 1 ≜xk + 1 −xk
0
0.000000000E + 00
1.000000000E + 00
−3.149266427E −01
1
6.850733573E −01
7.743726338E −01
−3.571247764E −02
2
7.386601562E −01
7.393713361E −01
−2.862297597E −04
3
7.390851064E −01
7.390851513E −01
−1.732571964E −08
4
7.390851340E −01
7.390851327E −01
8.816138486E −09
5
7.390851415E −01
7.390851276E −01
7.206302000E −09
6
7.390851348E −01
7.390851321E −01
1.746865097E −09
7
7.390851339E −01
7.390851328E −01
2.687278999E −08
8
7.390851596E −01
7.390851154E −01
1.763205759E −08
9
7.390851330E −01
7.390851333E −01
4.258583575E −08
10
7.390851759E −01
7.390851045E −01
2.914004560E −08
11
7.390851336E −01
7.390851330E −01
1.028375171E −07
12
7.390852358E −01
7.390850641E −01
6.925546447E −08
Define
G(x) = x g(g(x)) −(g(x))2
g(g(x)) −2g(x) + x.
(8.15)
An iteration process using Aitken’s δ2 method is
xk+1 = G(xk), for k = 0, 1, 2, . . . .
(8.16)
Example 8.5
(Example 8.4 again) Find an approximate solution of x = cos(x) using Aitken’s δ2
method.
Method: An initial guess x0 = 0 produces the results in Table 8.4. We used an Excel
spreadsheet for this, although we could have used a basic calculator instead. It looks
as if the iterates are rapidly converging to about 0.739085, perhaps with quadratic
convergence. But the error eventually stops improving and instead starts bouncing
around. ⃝
One cycle of Steffensen’s method uses successive approximation for two steps and then
uses Aitken’s δ2 process for a step; after that, a new cycle is done, etc. This usually works
better than just using Aitken’s δ2 method.
8.1.6 Newton’s Method versus Other Methods
A problem posed as a fixed-point problem x = g(x) is equivalent to the problem of finding
a root of f(x) = 0, where f(x) ≜x −g(x). So, we can use Newton’s method instead of the
method of successive approximations or Aitken’s δ2 method.

658
Advanced Engineering Mathematics
Example 8.6
(Example 8.4 again) Find an approximate solution of x = cos(x) using Newton’s method.
Method: Define f(x) = x −cos x, so solving x = cos x is equivalent to solving f(x) = 0.
We used Newton’s method in an Excel spreadsheet and got good results shown in
Table 8.5. ⃝
8.1.7 Trouble
Unfortunately, Newton’s method may not converge to a solution of f(x) = 0 even though
a solution exists.
Perhaps even worse is that Newton’s method, using the usual stopping rule, |△xk| < ε,
may (Donavon et al., 1993) say that it has found a numerical solution when actually f(x) = 0
has no solution or when the numerical solution is far from an actual solution!
In spite of these cautionary notes, Newton’s method is a valuable, elementary approach
to solving equations. In particular, when we can verify the hypotheses of a theorem that
does guarantee convergence, then we can be even more confident that Newton’s method
will be useful.
Learn More About It
Elementary Numerical Analysis: An Algorithmic Approach, 2nd ed., by S. D. Conte and
Carl de Boor, McGraw-Hill, Inc. c⃝1972, has analysis of the convergence of the secant
method and of Müller’s method, which does a good job of searching for complex roots.
Müller’s method effectively interpolates the function f(x) by a quadratic polynomial
and then uses the quadratic formula to approximate a root of f(x).
Analysis of Numerical Methods, by Eugene Isaacson and Herbert Bishop Keller,
Dover Publications
c⃝1994 has a proof that Aitken’s δ2 method gives quadratic
convergence.
8.1.8 Problems
In problems 1–4 use (a) Newton’s method and (b) the bisection method to find an approx-
imate solution of the given equation in the given interval. Show enough steps in a
TABLE 8.5
Example 8.6 by Newton’s Method
k
xk
f(xk)
△xk + 1 ≜xk + 1 −xk
0
0.000000000E + 00
−1.000000000E + 00
−1.000000000E + 00
1
1.000000000E + 00
4.596976941E −01
2.496361322E −01
2
7.503638678E −01
1.892307382E −02
1.125097693E −02
3
7.391128909E −01
4.645589899E −05
2.775752608E −05
4
7.390851334E −01
2.847204694E −10
1.701233407E −10
5
7.390851332E −01
0.000000000E + 00
0.000000000E + 00

Numerical Methods I
659
table(s) to demonstrate how well your methods were working. For the bisection method,
stop the process when |△xk| < 10−7.
1. e−1.5x = cos 2x, 1 ≤x ≤3.
2. e−1.5x = cos 2x, 0.5 ≤x ≤1.
3. ln(x + 3) = cos 2x, −3 < x ≤−2.
4. 0 = −π
2 + x arctan(x2), 1 ≤x ≤2.
In problems 5 and 6 use Newton’s method to find two approximate solutions of the given
equation. Show enough steps in table(s) to demonstrate how well your methods were
working. Describe the convergence.
5. ln(x + 3) = x2 −2.
6. −1 + ln(x + 3) = cos 2x.
In problems 7 and 8 use the secant method to find an approximate solution of the given
equation.
7.
1
1+x2 −e−1.5x = x.
8. 1.5 cos

2
1+x2

= e−1.5x.
9. In Example 9.33 in Section 9.6 we will need to find the solutions of the equation
tanh(θ) = tan(θ). Find numerical approximations of the first four positive solu-
tions for θ, that is, the four positive solutions closest to zero. Try initial guesses of
θ0 equal to 4, 7, 10, 13, respectively. (These guesses were obtained from looking at
Figure 9.30.)
In problems 10 and 11 use (a) the method of successive approximations and (b) an Aitken’s
δ2 method to find an approximate solution of the given equation in the given interval.
Show enough steps in a table(s) to demonstrate how well your methods were working.
Describe the convergence.
10. The equation of Problem 8.1.8.8.
11. The equation of Problem 8.1.8.7.
12. In Tables 8.6 and 8.7 are examples of Newton’s method iteration for solv-
ing the equations f1(x) = 0 and f2(x) = 0, respectively. Comment on the type of
convergence each table shows and speculate on the nature of the zeros of f1, f2 as
the cause for the different types of convergence.
13. In Example 9.32 in Section 9.6 we will need to find the solutions of the equation
0 =
√
2 cos(
√
2λ) sin(
√
3λ) +
√
3 sin(
√
2λ) cos(
√
3λ).
Find numerical approximations of the first four positive solutions for λ, that is, the
four positive solutions closest to zero.
14. In Table 8.8 are examples of Newton’s method and a modified Newton’s method
iteration with m = 3 for solving the equation f(x) = 0. Comment on the type of
convergence each table shows and speculate on the nature of the zero of f as the
cause for the different types of convergence.

660
Advanced Engineering Mathematics
TABLE 8.6
Problem 8.1.8.12 for f1(x) = 0 by Newton’s Method
k
xk
f(xk)
△xk + 1 ≜xk + 1 −xk
0
1.5000000E + 00
1.2500000E −01
−1.6666667E −01
1
1.3333333E + 00
3.7037037E −02
−1.1111111E −01
2
1.2222222E + 00
1.0973937E −02
−7.4074074E −02
3
1.1481481E + 00
3.2515369E −03
−4.9382716E −02
4
1.0987654E + 00
9.6341833E −04
−3.2921811E −02
5
1.0658436E + 00
2.8545728E −04
−2.1947874E −02
6
1.0438957E + 00
8.4579936E −05
−1.4631916E −02
7
1.0292638E + 00
2.5060722E −05
−9.7546106E −03
8
1.0195092E + 00
7.4253990E −06
−6.5030737E −03
9
1.0130061E + 00
2.2001182E −06
−4.3335383E −03
TABLE 8.7
Problem 8.1.8.12 for f2(x) = 0 by Newton’s Method
k
xk
f(xk)
△xk + 1 ≜xk + 1 −xk
0
1.5000000E + 00
1.1275000E −01
−1.5093708E −01
1
1.3490629E + 00
3.0711573E −02
−8.4642976E −02
2
1.2644199E + 00
6.8889308E −03
−3.3243744E −02
3
1.2311762E + 00
8.3882398E −04
−5.3135542E −03
4
1.2258626E + 00
1.9402732E −05
−1.2884470E −04
5
1.2257338E + 00
1.1229867E −08
−7.4658841E −08
6
1.2257337E + 00
3.7747583E −15
−2.5091040E −14
7
1.2257337E + 00
0.0000000E + 00
0.0000000E + 00
TABLE 8.8
Problem 8.1.8.14 for f(x) = 0 by Newton’s Method
Newton’s
Modified
k
xk
f(xk)
△xk + 1
xk
f(xk)
△xk + 1
0
1.50000E + 00
3.4864E + 00
−5.0000E + 00
1.50000E + 00
3.4864E + 00
−1.5000E + 01
1
−3.50000E + 00
−5.1743E + 02
4.5455E −01
−1.35000E + 01
−1.4246E + 09
2.4194E + 00
2
−3.04545E + 00
−1.7988E + 02
4.0541E −01
−1.10806E + 01
−6.6486E + 07
2.3120E + 00
3
−2.64005E + 00
−6.1820E + 01
3.5346E −01
−8.76868E + 00
−3.0146E + 06
2.1642E + 00
4
−2.28659E + 00
−2.0959E + 01
3.0014E −01
−6.60444E + 00
−1.3000E + 05
1.9540E + 00
5
−1.98645E + 00
−6.9973E + 00
2.4745E −01
−4.65041E + 00
−5.0895E + 03
1.6467E + 00
6
−1.73900E + 00
−2.2971E + 00
1.9765E −01
−3.00371E + 00
−1.6218E + 02
1.2013E + 00
7
−1.54135E + 00
−7.4104E −01
1.5287E −01
−1.80238E + 00
−3.1326E + 00
6.3306E −01
8
−1.38849E + 00
−2.3504E −01
1.1465E −01
−1.16932E + 00
−1.5629E −02
1.6027E −01
9
−1.27384E + 00
−7.3401E −02
8.3644E −02
−1.00905E + 00
−2.0302E −06
9.0184E −03
10
−1.19019E + 00
−2.2620E −02
5.9618E −02
−1.00003E + 00
−5.4657E −14
2.7192E −05
11
−1.13058E + 00
−6.8958E −03
4.1710E −02
−1.00000E + 00
−4.0700E −29
2.4647E −10
12
−1.08887E + 00
−2.0849E −03
2.8770E −02
−1.00000E + 00
0.0000E + 00
#DIV/0!

Numerical Methods I
661
8.2 Solving a System of Equations
8.2.1 Newton’s Method in Rn
To find an approximate solution of a system of n equations in n unknowns, that is,
0 = f(x) ≜
⎧
⎪⎪⎨
⎪⎪⎩
f1(x1, . . . , xn)
...
fn(x1, . . . , xn)
⎫
⎪⎪⎬
⎪⎪⎭
,
(8.17)
we can use a generalization of Newton’s method. Recall linear approximation for a
function of several variables, for example, (6.33) in Section 6.4,
f(x0 + △x, y0 + △y) ≈f(x0, y0) + △x ∂f
∂x(x0, y0) + △y ∂f
∂y(x0, y0).
Let xk = [ x1,k
· · ·
xn,k ]T. For each i = 1, . . . , n, linear approximation gives us
fi(x1,k+1, . . . , xn,k+1) ≈fi(x1,k, . . . , xn,k) +
n

j=1
 ∂fi
∂xj
(xk)

(xj,k+1 −xj,k).
This implies that
⎡
⎢⎢⎢⎢⎢⎢⎣
f1(xk+1)
·
·
·
fn(xk+1)
⎤
⎥⎥⎥⎥⎥⎥⎦
≈
⎡
⎢⎢⎢⎢⎢⎢⎣
f1(xk))
·
·
·
fn(xk)
⎤
⎥⎥⎥⎥⎥⎥⎦
+
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
∂f1
∂x1 (xk)
·
·
·
∂f1
∂xn (xk)
·
·
·
·
·
·
·
·
·
∂fn
∂x1 (xk)
·
·
·
∂fn
∂xn (xk)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
x1,k+1 −x1,k)
·
·
·
xn,k+1 −xn,k
⎤
⎥⎥⎥⎥⎥⎥⎦
,
that is,
f(xk+1) ≈f(xk) + ∂f
∂x(xk) · (xk+1 −xk).
Note that ∂f
∂x(xk) is the n × n Jacobian matrix evaluated at xk.
As in Newton’s method for a single equation, we find xk+1 by hoping that
0 = f(xk+1),
that is,
0 ≈f(xk) + ∂f
∂x(xk) · (xk+1 −xk),

662
Advanced Engineering Mathematics
that is,
 ∂f
∂x(xk)

xk+1 =
 ∂f
∂x(xk)

xk −f(xk).
So, the generalization of Newton’s method to a system of n equations in n unknowns is
xk+1 = xk −
 ∂f
∂x(xk)
−1
f(xk) ≜xk + ξk.
(8.18)
We emphasize that in software implementation of Newton’s method for systems, we
do not usually use
 ∂f
∂x(xk)
−1 but instead calculate ξk using the software’s solution of the
system of linear algebraic equations
Aξk = −f(xk),
where
A ≜
 ∂f
∂x(xk)

.
For example, in MATLAB®,
ξk = −A\f(xk).
Example 8.7
Find an approximate solution of the system of equations
 0 = x −y + 1
0 = x3 −y2 + 1

.
(8.19)
Method: Define f(x) = x−y+1, g(x, y) = x3−y2+1, x ≜

x
y

, and f(x) ≜

x −y + 1
x3 −y2 + 1

.
With initial guess (x0, y0) = (−0.6, 0.5), a calculator or a computer software package
produces iterates (x1, y1), (x2, y2), . . . summarized in Table 8.9. It appears that the iterates
converge to (x⋆, y⋆) ≈(−1, 0), with f(x⋆, y⋆) ≈0 and g(x⋆, y⋆) ≈0, so (x⋆, y⋆) seems to be
a very good approximation of a solution of the system of equations 0 = f(x, y) = g(x, y).
In fact, f(−1, 0) = g(−1, 0) = 0 exactly. ⃝
Also, in the table, we see that the norms of the changes in the iterates,
∥(△xk+1, △yk+1)∥≜

(xk+1 −xk)2 + (yk+1 −yk)2,
(8.20)
display quadratic convergence, that is, for some constant c and index K,
∥(△xk+1, △yk+1)∥≤c∥(△xk, △yk)∥2,
for all k ≥K.
The stopping rule is typically ∥(△xk, △yk)∥< ε.

Numerical Methods I
663
TABLE 8.9
Solution of Example 8.7 by Newton’s Method
k
xk
yk
f(xk,yk)
g(xk,yk)
∥(△xk,△yk)∥
0
−6.0000000E −01
5.0000000E −01
−1.0000000E −01
5.3400000E −01
1
−8.5250000E + 00
−7.5250000E + 00
−1.7763568E −15
−6.7518533E + 02
1.1278575E + 01
2
−5.6281647E + 00
−4.6281647E + 00
0.0000000E + 00
−1.9869899E + 02
4.0967438E + 00
3
−3.7228196E + 00
−2.7228196E + 00
0.0000000E + 00
−5.8009740E + 01
2.6945648E + 00
4
−2.4891944E + 00
−1.4891944E + 00
−2.2204460E −16
−1.6640970E + 01
1.7446095E + 00
5
−1.7175880E + 00
−7.1758800E −01
1.1102230E −16
−4.5820035E + 00
1.0912163E + 00
6
−1.2721062E + 00
−2.7210622E −01
0.0000000E + 00
−1.1326331E + 00
6.3000637E −01
7
−1.0623195E + 00
−6.2319540E −02
0.0000000E + 00
−2.0273555E −01
2.9668317E −01
8
−1.0045635E + 00
−4.5635376E −03
1.1102230E −16
−1.3774011E −02
8.1679321E −02
9
−1.0000275E + 00
−2.7496011E −05
0.0000000E + 00
−8.2491056E −05
6.4149316E −03
10
−1.0000000E + 00
−1.0079807E −09
−2.2204460E −16
−3.0239424E −09
3.8883806E −05
11
−1.0000000E + 00
−1.0258136E −16
1.1102230E −16
0.0000000E + 00
1.4255000E −09
12
−1.0000000E + 00
8.4409410E −18
0.0000000E + 00
0.0000000E + 00
1.1102230E −16
13
−1.0000000E + 00
8.4409410E −18
0.0000000E + 00
0.0000000E + 00
0.0000000E + 00
As Newton’s method works on the solution of a system

f(x, y) = 0
g(x, y) = 0

, a change in the
iterate (x, y) can get f(x, y) closer to zero at the expense of g(x, y) going further from zero,
or vice versa. The method works on solving both equations in the long run, but in the short
run, we cannot guarantee that both f(x, y) and g(x, y) are always getting closer to zero.
As in Example 8.1 in Section 8.1, there may be more than one solution of the nonlinear
system of equations (8.20). Which solution Newton’s method converges to can depend
on the initial guess, (x0, y0). For example, if, instead of (x0, y0) = (−0.6, 0.5), we had used
(x0, y0) = (−0.6, 0.6), then Newton’s method has iterates (xk, yk) →(x⋆, y⋆) ≈(2, 3) as k →∞,
as shown in Table 8.10. In fact, f(2, 3) = g(2, 3) = 0 exactly.
Again, the convergence is quadratic, specifically for k ≥3.
TABLE 8.10
Second Newton’s Method Solution of Example 8.7
k
xk
yk
f(xk,yk)
g(xk,yk)
∥(△xk,△yk)∥
0
−6.0000000E −01
6.0000000E −01
−2.0000000E −01
4.2400000E −01
1
4.9333333E + 00
5.9333333E + 00
8.8817842E −16
8.5861926E + 01
7.6851950E + 00
2
3.5291370E + 00
4.5291370E + 00
0.0000000E + 00
2.4441641E + 01
1.9858335E + 00
3
2.6656624E + 00
3.6656624E + 00
0.0000000E + 00
6.5044656E + 00
1.2211375E + 00
4
2.2005907E + 00
3.2005907E + 00
0.0000000E + 00
1.4127987E + 00
6.5771064E −01
5
2.0267424E + 00
3.0267424E + 00
0.0000000E + 00
1.6404943E −01
2.4585864E −01
6
2.0005764E + 00
3.0005764E + 00
0.0000000E + 00
3.4603058E −03
3.7004282E −02
7
2.0000003E + 00
3.0000003E + 00
0.0000000E + 00
1.6602076E −06
8.1481894E −04
8
2.0000000E + 00
3.0000000E + 00
0.0000000E + 00
3.8014036E −13
3.9131449E −07
9
2.0000000E + 00
3.0000000E + 00
0.0000000E + 00
0.0000000E + 00
8.9599943E −14
10
2.0000000E + 00
3.0000000E + 00
0.0000000E + 00
0.0000000E + 00
0.0000000E + 00

664
Advanced Engineering Mathematics
If, instead we had used (x0, y0) = (−0.6, 1.02), then Newton’s method has iterates
(xk, yk) →(x⋆, y⋆) ≈(0, 1) as k →∞. In fact, f(0, 1) = g(0, 1) = 0 exactly. Again, the conver-
gence is quadratic.
8.2.2 Newton–Kantorovich Theorem
We recall the concept of norm from Section 2.3, specifically the three properties that define
the concept in (2.9) in Section 2.3:
(a) ∥x∥≥0, for all x in Rn with equality only if x = 0.
(b) ∥αx∥= |α| ∥x∥, for all x in Rn, and all scalars α.
(c) ∥x + y∥≤∥x∥+ ∥y∥, for all x, y in Rn.
The Euclidean norm defined on Rn by
∥x∥2 =

x2
1 + · · · + x2n
is just one of the norms that can be defined on Rn. All of our previous works used the
Euclidean norm and we will emphasize it in the following text.
Theorem 8.3
Given any norm ∥∥on Rn and any real, n×n matrix A, there is a constant M ≥0 satisfying
∥Ax∥≤M∥x∥,
for all x in Rn.
(8.21)
Definition 8.1
The smallest such number M in the conclusion of Theorem 8.3 is called ∥A∥and is called
the matrix norm induced by the vector norm ∥∥on Rn.
Remarks:
(1) ∥Ax∥≤∥A∥∥x∥, for all x in Rn.
(2) ∥A∥= 0 only if A = O, the zero operator that satisfies Ox = 0 for all x in Rn.
As usual denote the Jacobian matrix by
J(x) =
 ∂f
∂x(x)

and closed balls by
BR(x)≜{x : ∥x −x∥≤R}.

Numerical Methods I
665
Theorem 8.4
(Newton–Kantorovich theorem for system (8.17)) Suppose there exist positive constants R,
K0, K1, η, t and constant vector x such that
(a) f(x) is continuously differentiable.
(b) ∥

J(x)
−1∥≤K0.
(c) ∥J(x) −J(y)∥≤K1∥x −y∥, for all x, y in BR(x).
(d) ∥

J(x)
−1f(x)∥≤η.
(e) h0 ≜K0K1η < 1
4.
(f) tη ≤R, where t is the smaller root of the quadratic equation h0t2 −t + 1 = 0.
Then in Btη(x), there is an x⋆solving (8.17) such that if the initial guess x0 is in Btη(x), then
the iterates of a modified Newton’s method given by
xk+1 = xk −
 ∂f
∂x(x)
−1
f(xk)
converge linearly to x⋆as k →∞.
Note that in this modified Newton’s method, we can calculate the Jacobian matrix J(x)
once and use it over and over again during the iterations. This can speed up the program’s
execution, at the possible cost of a slower convergence exponent α. Note that the word
“modified” here has a different meaning than it did in Section 8.1.
As is usual for equation-solving methods, the closer x is to a solution, the easier it is to
guarantee convergence.
One difficulty with using Theorem 8.4 is that K1 may depend on R, the size of the ball in
which x and y lie, and then K1 influences h0 and thus η in hypothesis (f) in which R again
appears. Finding R can be like a dog chasing its tail.
Theorem 8.5
(Another Newton–Kantorovich theorem for system (8.17) ) Suppose there exist positive
constants R, K0, K1, η and constant vector x such that
(a) f(x) is continuously differentiable.
(b) ∥

J(x)
−1∥≤K0.
(c) ∥J(x) −J(y)∥≤K1∥x −y∥, for all x, y in BR(x).
(d) ∥

J(x)
−1f(x)∥≤η.
(e) K0K1R < 2
3.
(f) η ≤(1 −3
2K0K1R)R.
Then in BR(x), there is an x⋆solving (8.17) such that if the initial guess x0 is in BR(x), then
the iterates of Newton’s method, (8.18), converge quadratically to x⋆as k →∞.

666
Advanced Engineering Mathematics
As is usual for equation solving methods, the closer x is to a solution, the easier it is to
guarantee convergence.
A difficulty with using Theorem 8.5 is that we must find R that simultaneously satisfies
three different hypotheses: (b), (e), and (f). Hypothesis (e) requires that R be relatively
small, but then (f) requires that R be relatively large.
8.2.3 Fixed Point Problem Iteration
Similar to the case of a single scalar equation, solving x = g(x) for x in Rn, that is, a system
of n equations in n unknowns, is a fixed-point problem.
Every problem of the form f(x) = 0 is equivalent to the fixed-point problem x = g(x)
where you may define g(x) = x+f(x) or g(x) = x−f(x). In fact, even the seemingly simple
problem of solving Ax = b can benefit tremendously from rewriting it as a fixed-point
problem, as we will see in Section 8.4.
A very special case of Theorem 8.4 is when f(x) = Cx+z for some constant matrix C and
constant vector z. Then the Jacobian is given identically by J(x) = C; hence, uniformity
in hypothesis (c) allows R to be chosen as large as one needs to make hypothesis (f) be
true. So, in this special case, the hypotheses of Theorem 8.4 simplify considerably: define
K0 ≜∥C−1∥, K1 ≜∥C∥, and η = ∥x + C−1z∥. The hypotheses reduce to requiring only h0 =
∥C∥∥C−1∥η < 1
4; the conclusion holds in the ball Br(x) where r = tη = η
h0 · (1 −

1 −4h0).
But this is all theoretical, and we would wonder why we would need Newton’s method to
solve Cx + z if we had our hands on C−1!
But even this simpler hypothesis, h0 = ∥C∥∥C−1∥η < 1
4, is unnecessarily complicated.
The Banach contraction mapping theorem, a result about fixed points, assures us that as
long as ∥(I−C)∥< 1, then Newton’s method for f(x) = Cx+z will converge, no matter how
far away from being correct is the initial guess x0, assuming there were no round-off error.
Both Theorems 8.4 and 8.5 generalize from Rn to infinite-dimensional, complete normed
vector spaces, that is, Banach spaces. The word “complete” has the same Cauchy conver-
gence meaning as in the definition of Hilbert spaces (Section 2.10). We would also need to
use a generalization of the Jacobian matrix J called the Fréchet derivative, Df.
8.2.4 Secant Method for Systems of Equations
If we are trying to solve the system of equations (8.17) and there is no formula for a function
fi(x), then there is no formula for the partial derivatives ∂fi/∂xj. As in the case of a single
equation in a single unknown, we can approximate the partial derivatives. An analogue of
the secant method (8.12) in Section 8.1 is
xk+1 = xk −

△fi,k
△xj,k
−1 
w · f(xk) + (1 −w) · f(xk−1)

, k = 1, 2, 3, . . . ,
(8.22)
where the approximate Jacobian matrix is given by

△fi,k
△xj,k

≜
fi(xk−1 + (xj,k −xj,k−1)e(j)) −fi(xk−1)
xj,k −xj,k−1
.
(8.23)
Recall that e(1), e(2), . . . , e(n) are the columns of the n × n identity matrix In.

Numerical Methods I
667
In order to use the secant method we have to choose two initial guesses, x0 and x1. We
need them in order to create the first approximate Jacobian matrix in (8.23).
Also, as for the Newton–Raphson method for systems, in software implementation of the
secant method in (8.22), we do not actually compute
 △fi,k
△xj,k
!−1 
w · f(xk) + (1 −w) · f(xk−1)

,
but instead we use
xk+1 = xk + ξk,
where ξk is the software’s solution of the linear algebraic system

△fi,k
△xj,k

ξk = −

w · f(xk) + (1 −w) · f(xk−1)

.
Example 8.8
Use the secant method to find an approximate solution of the system of equations (8.19),
that is,
⎧
⎨
⎩
0 = x −y + 1
0 = x3 −y2 + 1
⎫
⎬
⎭.
Method: Again, define f(x) = x −y + 1, g(x, y) = x3 −y2 + 1,
x ≜
⎡
⎣
x
y
⎤
⎦,
and
f(x) ≜
⎡
⎣
x −y + 1
x3 −y2 + 1
⎤
⎦.
Here, the approximate Jacobian matrix
 
△fi,k/△xj,k
!
1 ≤i ≤2
1 ≤j ≤2
is
⎡
⎢⎢⎢⎢⎣
(xk −yk−1 + 1) −(xk−1 −yk−1 + 1)
xk −xk−1
(xk−1 −yk + 1) −(xk−1 −yk−1 + 1)
yk −yk−1
(x3
k −y2
k−1 + 1) −(x3
k−1 −y2
k−1 + 1)
xk −xk−1
(x3
k−1 −y2
k + 1) −(x3
k−1 −y2
k−1 + 1)
yk −yk−1
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
xk −xk−1
xk −xk−1
−yk + yk−1
yk −yk−1
x3
k −x3
k−1
xk −xk−1
−y2
k + y2
k−1
yk −yk−1
⎤
⎥⎥⎥⎥⎦
=

1
−1
x2
k + xk−1xk + x2
k−1
−yk + yk−1

.
Instead of quadratic convergence, Table 8.11 appears to have super linear conver-
gence. Here, for some constant c,
∥△xk+1∥≤c ∥△xk∥α,
(8.24)
for all k ≥5, where it appears that the convergence exponent is α ≈1.2. ⃝

668
Advanced Engineering Mathematics
TABLE 8.11
Secant Method’s Solution of Example 8.8
k
xk
yk
f(xk,yk)
g(xk,yk)
∥(△xk,△yk)∥
0
−6.0000000E −01
5.0000000E −01
−1.0000000E −01
5.3400000E −01
1
−3.0000000E −01
2.5000000E −01
4.5000000E −01
9.1050000E −01
2
−5.4135115E + 00
−3.4885496E + 00
−9.2496183E −01
−1.6981892E + 02
6.3344102E + 00
3
−3.4026652E + 00
−1.8026881E + 00
−5.9997710E −01
−4.1646186E + 01
2.6240487E + 00
4
−2.8546480E + 00
−1.5871579E + 00
−2.6749008E −01
−2.4781639E + 01
5.8887703E −01
5
−2.4086176E + 00
−1.3081212E + 00
−1.0049634E −01
−1.4684628E + 01
5.2612219E −01
6
−2.0483599E + 00
−1.0148112E + 00
−3.3548794E −02
−8.6243062E + 00
4.6456040E −01
7
−1.7601519E + 00
−7.5007227E −01
−1.0079643E −02
−5.0157962E + 00
3.9134454E −01
8
−1.5325986E + 00
−5.2990567E −01
−2.6929063E −03
−2.8806570E + 00
3.1662889E −01
9
−1.3568335E + 00
−3.5622574E −01
−6.0777948E −04
−1.6248235E + 00
2.4709931E −01
10
−1.2256822E + 00
−2.2558681E −01
−9.5377063E −05
−8.9222788E −01
1.8511402E −01
11
−1.1327300E + 00
−1.3273356E −01
3.5517105E −06
−4.7099831E −01
1.3138431E −01
12
−1.0714720E + 00
−7.1483673E −02
1.1668733E −05
−2.3521577E −01
8.6626155E −02
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
30
−1.0000000E + 00
9.4766651E −11
−4.6629367E −15
2.8428626E −10
5.4491325E −10
31
−1.0000000E + 00
8.8499877E −12
1.9984014E −15
2.6556091E −11
1.2149988E −10
32
−1.0000000E + 00
−4.1666749E −12
1.7763568E −15
−1.2494672E −11
1.8408594E −11
Learn More About It
The version of Newton–Kantorovich Theorem 8.4 is found on pp. 301–302 of Elements
of Functional Analysis, by L. A. Lusternik and V. J. Sobolev, Hindustan Publishing
Co., Halsted Press/John Wiley & Sons, Inc., c⃝1974. One can learn about the Fréchet
derivative in that book, as well as Section 7.2 of Elements of Applicable Functional Anal-
ysis, by Charles W. Groetsch, Marcel Dekker, Inc., c⃝1980. The version of “another”
Newton–Kantorovich Theorem 8.5 is found in Section 1.4 of Numerical Methods for Two-
Point Boundary-Value Problems, by Herbert B. Keller, Blaisdell Publishing Co., c⃝1968
or Dover Publications c⃝1992.
8.2.5 Problems
1. For the system of equations

0 = xy2 −y + 1
0 = x2y + x −0.5

,
(a) Write down Newton’s method explicitly giving xk+1 in terms of xk.
(b) Using part (a) and initial guess x0 =

0.5
1

, show work by hand to find x1.

Numerical Methods I
669
In problems 2–4 use Newton’s method to find two different approximate solutions of the
given system of equations.
2.
 0 = x + 2y + 4
0 = x2 −2x + y2 −24

3.
 0 = x + 2y −9
0 = x2 + y2 −4y −21

4.
 0 = x + 2y −6
0 = x2 + 2x + y2 −2y −23

In problems 5–7 use Newton’s method to find three different approximate solutions of the
given system of equations.
5.
 0 = xy2 −8x −y2 + 8
0 = 2xy −x2y

6.
 0 = 2x + y −π
0 = sin 2x cos y −cos y

7.
 0 = 4x −6y −π
0 = cos x(1 −2 sin y)

8. Use Newton’s method to find an approximate solution of
 0 = ex + y2 −3y −5
0 = x + e−y + 1

.
In problems 9–12 use the secant method to find two approximate solutions, if possible, of
the given system of equations.
9. The system of Problem 8.2.5.1.
10. The system of Problem 8.2.5.2.
11. The system of Problem 8.2.5.4.
12. The system of Problem 8.2.5.5.
8.3 Approximation of Integrals
We use numerical methods for approximating the value of a definite integral because, in
general, we cannot write derive the exact value of the integral.
Suppose f(x) is a function of x on a finite interval [a, b]. Recall that the definite integral
of f on [a, b] can be defined by the limit of Riemann sums, if the limit exists: If a = x0 <
x1 < · · · < xn = b is a partition of [a, b], x∗
k are sampling points satisfying xk−1 ≤x∗
k ≤xk
for k = 1, 2, . . . , n, and each △xk = xk −xk−1 for k = 1, 2, . . . , n, then
n

k=1
f(x∗
k) △xk

670
Advanced Engineering Mathematics
is a Riemann sum. For an equally spaced partition,
h =△x ≜b −a
n
is called the step size, and xk = a + kh for k = 0, 1, . . . , n.
The midpoint approximation, Mn, is given by
b
a
f(x) dx ≈Mn ≜h
n

k=1
f(xk−0.5),
(8.25)
where the midpoints are
xk−0.5 = xk −0.5h = 0.5(xk−1 + xk), for k = 1, . . . , n.
Given a function f, define
fk ≜f(xk), for k = 0, . . . , n.
(8.26)
Another famous approximation you learned about in calculus is the trapezoidal rule,
b
a
f(x) dx ≈Tn ≜h
2
n

k=1

fk−1 + fk

,
(8.27)
as illustrated in Figure 8.4. Also, if n = 2N is even, then the famous Simpson’s rule is
given by
b
a
f(x) dx ≈S2N ≜h
3
N

k=1

f2k−2 + 4f2k−1 + f2k

.
(8.28)
In the latter, it is still true that the step size is
h ≜b −a
n
= b −a
2N .
Any formula that gives an approximation of a definite integral is called a quadrature rule.
In fact there are many quadrature rules. For example, if n = 4N, then the Newton–Cotes
formula of closed type of order four is given by
a=x0
xk–1
xn=b
xk
FIGURE 8.4
Approximation of an integral using areas of trapezoids.

Numerical Methods I
671
b
a
f(x) dx ≈U4N ≜2h
45
N

k=1

7f4k−4 + 32f4k−3 + 12f4k−2 + 32f4k−1 + 7f4k

.
(8.29)
This is also known as Boole’s rule (Weisstein; Boole and Moulton, 1960).
For small n, we may avoid the "
k=1 notation by noting that
Tn = h
2

f0 + 2f1 + 2f2 + · · · + 2fn−1 + fn,

Sn = h
3

f0 + 4f1 + 2f2 + 4f3 + 2f4 + 4f5 + 2f6 + · · · + 4fn−2 + 2fn−1 + fn,

and similarly for Boole’s rule U4N. For n = 8, we have, in particular,
T8 = h
2

f0 + 2f1 + 2f2 + · · · + 2f7 + f8,

S8 = h
3

f0 + 4f1 + 2f2 + 4f3 + 2f4 + 4f5 + 2f6 + 4f7 + f8,

U8 = 2h
45

7f0 + 32f1 + 12f2 + 32f3 + 14f4 + 32f5 + 12f6 + 32f7 + 7f8

.
(8.30)
Example 8.9
Use the midpoint, trapezoidal, Simpson’s, and Boole’s rules with (a) four, and (b) eight
subintervals to approximate the integral
π
−π/2
sin 2x
3 −e−|x| dx.
Method: (a) For n = 4, h = π−(−0.5π)
4
= 3π
8 , and the partition points are xn = a + nh =
−0.5π + 0.375 nπ, and we calculated
f0 = f(−0.5π) =
0
3 −e−π/2 = 0,
f1 = f(−.125π) =
−1/
√
2
3 −e−π/8 ≈−0.3041622875,
f2 = f(.25π) =
1
3 −e−π/4 ≈0.3930722012,
f3 = f(.625π) =
−1/
√
2
3 −e−5π/8 ≈−0.2472718569,
f4 = f(π) =
0
3 −e−π = 0.
The results we calculated for both n = and n = 8 are in Table 8.12. ⃝

672
Advanced Engineering Mathematics
TABLE 8.12
Example 8.9
n
Midpoint Rule
Trapezoidal Rule
Simpson’s Rule
Boole’s Rule
4
−4.1596899e −01
−1.86565776e −01
−5.5747254e −01
−6.7696223e −01
8
−3.6896831e −01
−3.0126738e −01
−3.39501251e −01
−3.2496983e −01
16
−3.4961965e −01
−3.3511785e −01
−3.4640133e −01
−3.4686134e −01
By the way, we could have gotten more accurate results by noting that f(x) = sin 2x
3−e−|x| is
an odd function of x, so
 π/2
−π/2
sin 2x
3−e−|x| dx = 0. Thus,
π
−π/2
sin 2x
3 −e−|x| dx =
π
π/2
sin 2x
3 −e−|x| dx.
8.3.1 Approximation Error for Quadrature Rules
One approach to using numerical methods is to use them as a black box based on their
reputations for success. Another approach to using numerical methods, called numeri-
cal analysis, is to study their accuracy and prove mathematical theorems that guarantee
their success as long as the hypotheses of the theorems are verified. The subject of numer-
ical analysis can be as rigorous as any other mathematical subject and may involve deep
techniques and results of pure mathematics.
We will use numerical methods while following a middle path, as we did in Section 8.1:
We will sometimes mention a mathematical theorem that guarantees a method’s success
under verified hypotheses as a way to better know when a numerical method is appropri-
ate; however, in examples we will usually not rigorously verify the hypotheses, and we
will not prove the mathematical theorems.
Here we will give estimates on the errors. In the results as follows, we will use the
notation
kℓ≜max
a≤x≤b | f (ℓ)(x)|
(8.31)
for a function differentiable ℓtimes on the interval [a, b].
Theorem 8.6
Suppose f is ℓtimes continuously differentiable on an interval (a, b) and continuous on
[a, b]. Then bounds on the absolute value of the error in evaluating a definite integral are
given by
(a) If ℓ≥2, then
##Mn −
 b
a f(x) dx
## ≤(b−a)
24
k2 h2.
(b) If ℓ≥2, then
##Tn −
 b
a f(x) dx
## ≤(b−a)
12
k2 h2.

Numerical Methods I
673
(c) If ℓ≥4, then
##Sn −
 b
a f(x) dx
## ≤(b−a)
180 k4 h4.
(d) If ℓ≥6, then
##Un −
 b
a f(x) dx
## ≤8(b−a)
945
k6 h6.
Because, for example, the second derivative of a first-degree polynomial is zero
everywhere, a consequence of Theorem 8.6 is that
The midpoint and trapezoidal rules are exact for all first-degree polynomials,
Simpson’s rule is exact for all third-degree polynomials, and Boole’s rule is exact
for all fifth-degree polynomials.
Of course, we can easily calculate an integral of a low-degree polynomial using the fun-
damental theorem of calculus. But the theorem and the remark after it give us some
indication of just how good the quadrature rules are.
Example 8.10
Find a value of n so that Simpson’s rule estimates the integral
π
−π
cos 2x
3 −e−x2 dx
with an error that is guaranteed to be less than 10−10. For that value of n, find the
Simpson’s rule approximation of that integral.
Method: Using MathematicaTM, for the integrand f(x) =
cos 2x
3−e−x2 we graphed the function
|f ′′(x)| versus x in the interval [−π, π] in order to estimate k4 ≤26. We confirmed the
value using the Mathematica command FindMaximum[{Abs[f′′′′[x]], −π ≤x ≤π}, {x, 0}].
We want to choose n so that
#####Sn −
π
−π
cos 2x
3 −e−x2 dx
##### ≤(b −a)
180
k4 h4 < 10−10,
that is,
π −(−π)
180
· 26 h4 < 10−10.
This gives h4 < 180
52π · 10−10; hence,
2π
n = b −a
n
= h <≈0.0032398867,
that is,
n >≈
2π
0.0032398867 ≈1939.322545.
Since n must be even in order to use Simpson’s rule, we choose n = 1940 or larger in
order to guarantee that the error would be less than 10−10. We have
π
−π
cos 2x
3 −e−x2 dx ≈S1940 ≈0.113795689531577. ⃝

674
Advanced Engineering Mathematics
8.3.2 Iteration of Quadrature
In practice, we may not be able to apply an error estimate found in Theorem 8.6, either
because the function values f(x) are not generated by a formula but instead by a software
subroutine or because the constant kℓdoes not exist or is difficult to estimate well.
A natural approach is to pick a quadrature rule and use it for an increasing sequence of n,
that is, for a finer and finer partition of the interval [a, b], until the difference between suc-
cessive approximations of the integral becomes less than a chosen error ε. The smallness
of the change in the quadrature values is a stopping rule for the iterative process.
The following is our MATLAB “function” fSimpExtrap that iterates Simpson’s rule for the
function of Example 8.9; the MATLAB “function” f821 was used by fSimpExtrap because
in the command window, we defined fn =′ f821′ and entered values for the interval
endpoints a, b and the error parameter eps. In the command window, we entered
a = −pi/2; b = pi; eps = 10∧(−10); fn =′ f821′; format long e;
fSimpExtrap(a, b, fn, eps)
and got output
a = −1.570796326794897e + 00
b = 3.141592653589793e + 00
n = 8192
h = 5.752427954571154e −04
ans = −3.449284883498875e −01.
With eps = 10−10, the stopping rule was triggered when n = 8192 and we estimated the
definite integral to be −3.449284883498875e −01. Our program used “Richardson extrap-
olation to the limit.” This integrand is not continuously differentiable on its interval of
integration, so the error guarantees of Theorem 8.6 are useless. The MATLAB program
fSimpExtrap and the integrand function evaluation f821 are
function fSimpExtrap = fSimpExtrap(a, b, fn, eps)
a, b
format long e; sumSimp = 0; sumSimpOld = 0; delta = 999; k = 0;
while delta >= eps
k = k + 1;
n = 2 ∗k; h = (b −a)/n;
sum = 0;
for m = 1 : k;
x0 = a + (m −1) ∗2 ∗h; x1 = x0 + h; x2 = x1 + h;
sum = sum + feval(fn, x0) + 4 ∗feval(fn, x1) + feval(fn, x2);
end

Numerical Methods I
675
sumSimp1 = sum ∗h/3;
sumSimpOld = sumSimp;
sumSimp = sumSimp1;
delta = abs(sumSimpOld −sumSimp);
end
n, h
% Uses Richardson extrapolation to the limit
fSimpExtrap = (4 ∗sumSimp −sumSimpOld)/3;
and
function f821 = f821(x)
% Function used in Example 8.9
format long e;
f821 = sin(2 ∗x)/(3 −exp(−abs(x))).
8.3.3 Theory versus Practice
We also ran our MATLAB program fSimpExtrap for the definite integral of Example 8.10
and the stopping rule was triggered at n = 256, giving an approximate integral value of I ≈
0.1137956895441768. We’ve underlined the 10 digits in fSimpExtrap that agree with the con-
clusion of Example 8.10. We first defined a function by f820 = cos(2 ∗x)/(3 −exp(−x$2)).
The integral’s approximate value S1940, in Example 8.10, is guaranteed by mathematical
theory to be correct to within 10−10 of the correct value. It took significantly more time to
compute S1940 than to calculate fSimpExtrap with n = 256.0
The theory was overly cautious in requiring n = 1940, while iteration gave a good result
with only n = 256. The difference between theory and practice is a “two-edged sword”:
Following the demands of theory may give software too slow to be useful, while ignor-
ing the demands of theory may lead to software that sometimes produces misleading
conclusions.
One problem with numerical quadrature is numerical drift in which reducing the step
size h eventually becomes counter productive due to increased round-off error.
Learn More About It
The Handbook of Mathematical Functions, edited by Milton Abramowitz and Irene
Stegun, Dover Publications
c⃝1964, contains many quadrature rules, including
Newton–Cotes formulas of both closed, and open-type and Gaussian quadrature. (By
the way, in that book, Boole’s rule is mistakenly referred to as Bode’s rule.) Elementary
Numerical Analysis: An Algorithmic Approach, 2nd ed., by S. D. Conte and Carl de Boor,
McGraw-Hill, Inc. c⃝1972, discusses “Richardson extrapolation to the limit.”

676
Advanced Engineering Mathematics
8.3.4 Problems
In problems 1–3 use Simpson’s rule with n = 8 to approximate the length of the
parametrized curve.
1. C : r = cos 2t ˆı + sin t ˆj + t ˆk, 0 ≤t ≤2π
2. The Zhukovskii airfoil given in (6.30), that is,
x(ξ, η) ≜ξ · 1 + ξ2 + η2
2(ξ2 + η2) , y(ξ, η) ≜η · −1 + ξ2 + η2
2(ξ2 + η2) ,
where
ξ(t) ≜−0.07 + ρ0 cos(t),
η(t) ≜0.05 + ρ0 sin(t),
and ρ0 =

(1 + 0.07)2 + (0.05)2.
3. As in (8.30), write down the trapezoidal, Simpson’s, and Boole’s rules for n = 12.
4. Use the trapezoidal rule T4 to approximate
 π
0

2 −sin2 x dx, and give an upper
bound on the error in that approximation.
5. Let I =
 π/3
−π/4 tan θ dθ.
(a) Approximate I using trapezoidal rule T4.
(b) Give an upper bound on the error in that approximation.
(c) Use

tan θ dθ = −ln | cos θ| to find the exact value of I.
(d) Show verification that your results in parts (a) and (c) agree with your result
in part (b).
6. Suppose the maximum value of | f ′′(x)| on the interval [1, 2] is 10 and the max-
imum value of | f ′′′′(x)| on the interval [1, 2] is 60. Find the smallest values,
respectively, of n for which (a) the trapezoidal rule and (b) Simpson’s rule
approximates
 2
1 f(x)dx to within a guaranteed accuracy of 10−3.
7. Use (a) Simpson’s rule and (b) Boole’ rule and as much of the vehicle speed data
as possible in Table 8.13 to estimate the change in position x(t) of the vehicle over
the time interval 0 ≤t ≤4 s.
8. Suppose that a CAT scan of a 12-cm-long human kidney shows cross sections 1 cm
apart. The cross-sectional areas, in cm2, are 0, 4, 5, 6, 7, 7, 8, 7, 6, 6, 5, 4, 0. Use
the trapezoidal rule, the Simpson’s rule, and the boole’s rule to approximate the
volume of the kidney.
TABLE 8.13
Vehicle Speed for Problem 8.3.4.7
Time (10−1 s)
0
5
10
15
20
25
30
35
40
Speed (m/s)
1
4
8
13
16
20
24
27
30

Numerical Methods I
677
8.4 Numerical Solution of Ax = b
In Section 1.1 we learned a theoretical method for finding all of the solutions of Ax = b, a
system of m linear algebraic equations in n unknowns. This method usually works well for
small systems, that is, where m and n are “small.”
But how “small” is “small?” And how “well” is “well,” and how often is “usually?”
It turns out that even for small systems, there can be serious inaccuracies in the numer-
ical values of the solution produced by the Gauss–Jordan or the Gaussian elimination
algorithm, even if m = n.
In addition, the use of numerical solutions requires careful consideration of two main
issues, “convergence” and “stability.” We will not establish the technical definitions of
these two concepts but instead discuss them informally. A numerical method is conver-
gent if a limiting process results in what is a correct solution. A numerical method is stable
if a small change in the parameters of the problem leads to a small change in the solution.
In order to address the concept of stability, we must have a way of measuring both the
input parameters and the solution.
From Sections 2.3 and 8.2, recall the concept of norm, and recall the concept of matrix
norm induced by the vector norm ∥∥on Rn. Recall that ∥A∥, the norm on a matrix ∥A∥,
satisfies
∥Ax∥≤∥A∥∥x∥, for all x in Rn.
Using the concept of induced matrix norm, we can define a concept that is commonly
used to study square matrices and numerical methods that use them.
Definition 8.2
Suppose ∥∥is a norm on Rn and A is a real, n×n matrix. We define the standard condition
number of A by
κ(A) ≜∥A∥∥A−1∥.
(8.32)
By the way, we saw κ(C) implicitly in the context of fixed-point iteration in Section 8.2.
Let’s understand how the standard condition number measures the stability of solving
the linear algebraic system Ax = b. Numerical computer software packages store numbers
with a finite number of digits of precision, so there is always the danger of round-off
error. In particular, when solving a system Ax = b, the vector b and the matrix A stored
in computer memory may differ from their measured values. In addition, their measured
values may differ from their true values because all measurement processes are subject
to error.
Assume that A is invertible and b ̸= 0. Denote by x the exact solution of the true system
Ax = b. Because A is linear, x ̸= 0. The standard condition number takes into account
the error in the right-hand side of the system: We will assume that the system solved by
computer software is

678
Advanced Engineering Mathematics
AX = b + δ,
(8.33)
where δ is the error in the vector b. Let’s define ξ ≜X −x, the difference between the
solution of (8.33) and the exact solution of the true system.
The absolute error in approximating b by (b + δ) is ∥δ∥and the corresponding relative
error is ∥(b + δ) −b∥/∥b∥, that is,
∥δ∥
∥b∥.
Similarly, the relative error in the solution is ∥X −x∥/∥x∥, that is,
∥ξ∥
∥x∥.
Using Ax = b and (8.33), we get
b + δ = AX = A(x + ξ) = Ax + Aξ = b + Aξ;
(8.34)
hence,
δ = Aξ.
So, the relative error in the solution is
∥ξ∥
∥x∥= ∥ξ∥
∥δ∥· ∥δ∥
∥b∥· ∥b∥
∥x∥.
(8.35)
By remark (1) following Definition 8.1 in Section 8.2, ∥b∥= ∥Ax∥≤∥A∥∥x∥. Similarly,
because A−1δ = ξ, we have ∥ξ∥= ∥A−1δ∥≤∥A−1∥∥δ∥. So, (8.35) implies
∥ξ∥
∥x∥≤∥A−1∥∥δ∥
∥δ∥
· ∥δ∥
∥b∥· ∥A∥∥x∥
∥x∥
= ∥A−1∥∥A∥∥δ∥
∥b∥= κ(A) ∥δ∥
∥b∥.
So, the relative error in the solution is the relative error in the right-hand side of the system
times a magnification factor of κ(A).
In Problem 8.4.3.11 you will use the first remark after Definition 8.1 in Section 8.2 to
explain why the matrix norm has the property that
∥AB∥≤∥A∥∥B∥,
(8.36)
as long as the multiplication AB makes sense.

Numerical Methods I
679
Using (8.36), it follows that
1 = ∥I∥= ∥A−1A∥≤∥A−1∥∥A∥= κ(A).
When a matrix has the smallest possible standard condition number, that is, κ(A) = 1, we
say the matrix is perfectly conditioned. In Problem 8.4.3.9 you will explain why orthog-
onal matrices are perfectly conditioned. In fact, if Q =
%
q1  · · ·
 qn
&
is an orthogonal
matrix, then Corollary 2.8 in Section 2.4 implies that the system Qx = b has unique solution
x = ⟨x, q1⟩e1 + · · · + ⟨x, qn⟩en.
It can be difficult to calculate ∥A∥using Definition 8.1 in Section 8.2. Instead, we can
recall from Section 2.10 that the Frobenius norm on matrices, defined by
∥A∥F ≜
'
(
(
)
n

j=1
n

k=1
|ajk|2,
satisfies ∥A∥≤∥A∥F, by Theorem 2.47 in Section 2.10. So, if ∥A−1∥F∥A∥F is not much bigger
than one, then we know that the square matrix A is well conditioned.
8.4.1 Partial Pivoting in the Gauss–Jordan Algorithm
Recall that in the Gaussian elimination method for solving a linear system Ax = b, we sys-
tematically row reduce the augmented matrix [A|b] to a row echelon form. In the process
of doing so, we create leading entries in the pivot positions in all of the nonzero rows of a
row echelon form. The Gauss–Jordan elimination algorithm goes further and row reduces
the augmented matrix to RREF. Both the Gauss–Jordan and Gaussian elimination algo-
rithms are called direct methods for finding the solution x = [ x1
. . .
xn ]T because they
involve reading x1, . . . , xn directly, or almost directly in the case of Gaussian elimination,
from a row echelon form of the augmented matrix. Unfortunately, these methods do not
read the solution directly from A or b.
The Gauss–Jordan and Gaussian elimination algorithms can fail to numerically con-
verge because of round-off error accumulation. It was also discovered that, in practice,
the technique of partial pivoting usually makes the algorithm convergent, that is, allows the
algorithm to give accurate results for the solution of a system Ax = b.
First, let’s see where trouble lies. For clarity we will use the equals sign to denote true,
that is, exact values, and the approximation sign to denote numbers rounded off to four
digits of accuracy.
Example 8.11
Use the Gauss–Jordan algorithm to solve the system of equations

1.000 × 10−5x + 1.000y = 1.000
1.000x + 1.000y = 2.000

.
(8.37)
Assume that numbers can only be stored with four digits of accuracy. Compare your
result with the true solution.

680
Advanced Engineering Mathematics
Method: The Gauss–Jordan algorithm begins with

1.000 × 10−5
1.000
|1.000
1.000
1.000
|2.000

∼
105R1 →R1
−R1 + R2 →R2

1.000
1.000 × 105
|
1.000 × 105
0.000
−1.000 × 105
|
−1.000 × 105

.
The (2, 2) entry in the latter matrix is −1.000 × 105 because 1.000 −100,000 ≈−100,000,
rounded off to four digits of precision. The (2, 3) entry in the latter matrix is −1.000×10−5
because 2.000 −100,000 ≈−100,000, rounded off to four digits of precision.
Continuing further with the Gauss–Jordan algorithm gives
∼
−10−5R2 →R2

1.000
1.000 × 105
|
1.000 × 105
0.000
1.000
|
1.000

∼
−105R2 + R1 →R1

1.000
0.000
|
0.000
0.000
1.000
|
1.000

.
So, in four-digit arithmetic, the solution to the system seems to be x ≈0.000, y ≈1.000.
To find the true solution, we can use the inverse of a 2 × 2 matrix to get

x
y

=

1.000 × 10−5
1.000
1.000
1.000
−1 
1.000
2.000

=
1
1.000 × 10−5 −1.000

1.000
−1.000
−1.000
1.000 × 10−5
 
1.000
2.000

=
1
0.99999

1
0.99998

=

1.0000100001 . . .
0.9999899998 . . .

.
So, the Gauss–Jordan algorithm gave a seriously inaccurate solution. ⃝
The matrix A =

1.000 × 10−5
1.000
1.000
1.000

in Example 8.11 is well conditioned, because we
can use the Frobenius norm to conclude that
κ(A) = ∥A−1∥∥A∥≤∥A−1∥F∥A∥F = 3 + 10−10

1 −10−5 .
So, merely having a well-conditioned A does not guarantee that a practical implementa-
tion of the Gauss–Jordan algorithm will accurately approximate the solution of the system
Ax = b.
Partial pivoting says that for the augmented matrix of Example 8.11, it would have
been better to first exchange the first row with the second row because the division of

Numerical Methods I
681
the original first row by 1.000 × 10−5 led to rounding off 2.000 −10,000 ≈−10,000 in
four-digit accuracy.
To be precise, partial pivoting says to do first a row exchange so that in the first column
the pivot position entry is as large, in magnitude, as possible. Continuing after that, partial
pivoting is done to have large magnitude numbers in pivot positions in all pivot columns.
Example 8.12
Use the Gauss–Jordan algorithm with partial pivoting to solve the system of equations
of Example 8.11, that is, system (8.37). Again, assume that numbers can only be stored
with four digits of accuracy. Compare your result with the true solution.
Method: The Gauss–Jordan algorithm with partial pivoting starts with

1.000 × 10−5
1.000
|1.000
1.000
1.000
|2.000

∼
R1 ↔R2

1.000
1.000
|2.000
1.000 × 10−5
1.000
|1.000

∼
−10−5R1 + R2 →R2

1.000
1.000
|2.000
0.000
1.000
|1.000

,
because the (2, 2) entry in the latter matrix is −1.000 × 10−5 ≈1.000, rounded off to four
digits of precision.
Continuing further with the Gauss–Jordan algorithm gives
∼
−R2 + R1 →R1

1.000
0.000
|1.000
0.000
1.000
|1.000

,
so, in four-digit arithmetic, the solution to the system seems to be x ≈1.000, y ≈1.000.
As in Example 8.11, the true solution is x = 1.0000100001 . . ., y = 0.99998999989 . . .,
so the Gauss–Jordan algorithm with partial pivoting algorithm gave a very accurate
solution. ⃝
There is a carnival game called “whack a mole” in which the player uses a mallet to
knock down toy rubber moles that pop up from many holes in the playing surface. Each
mole is analogous to a difficulty that “pops up” when trying to solve a problem. Knocking
down one annoying issue may just lead to another annoying issue popping up.
Partial pivoting is an attempt to suppress all of the problems that can come up during
the implementation of row reduction on a computer that keeps a finite number of digits of
accuracy. Unfortunately, even partial pivoting is not enough to guarantee accurate results,
as the next example shows.
Example 8.13
Again, assume that numbers can only be stored with four digits of accuracy. Using the
Gauss–Jordan algorithm with partial pivoting to solve the system of equations
 1.000x
+
1.000y = 2.000
10.00x
+
1.000 × 106y = 1.000 × 106

(8.38)

682
Advanced Engineering Mathematics
gives a solution,

x
y

=

0.000
1.000

, that is seriously inaccurate compared to the exact
solution,

x
y

=

1.0000100001 . . .
0.99998999989 . . .

.
This system is a disguised version of that in Example 8.11. Because the (2, 1) entry is
the largest in magnitude in its column, the Gauss–Jordan algorithm with partial pivoting
starts with
1.000
1.000
|2.000
10.00
1.000 × 106
|1.000 × 106

∼
R1 ↔R2
0.1000R1 →R1

1.000
1.000 × 105
|1.000 × 105
1.000
1.000
|2.000

.
In Problem 8.4.3.12 you will check the rest of the details, including the serious inaccuracy
of the solution. ⃝
Example 8.13 illustrates that choosing the pivot for the first column by exchanging to
the row whose entry in the first column is of largest magnitude ignores the magnitude of
the other entries in that row. The technique of implicit partial pivoting first “normalizes”
each nonzero row by also storing the row divided by its entry of largest magnitude, so
each normalized row’s entry of largest magnitude is 1. Then, for example, to create the
pivot in the (1, 1) position, exchange with the first row the row whose normalized row’s
entry in the first column is the largest in magnitude.
For the augmented matrix of Example 8.13, the technique of implicit partial pivoting
sees right through the phony “10.00” entry in the second row and does not do the row
interchange R1 ↔R2. In Problem 8.4.3.13 you will check for yourself that the Gauss–
Jordan algorithm with implicit partial pivoting gives solution

x
y

=

1.000
1.000

, which is
quite accurate.
8.4.2 Iterative Methods for Solving Ax = b
Recall from Section 8.2 that iterative methods can be used to solve systems of equations,
even nonlinear equations. Newton’s method or the secant method may solve f(x) = 0,
while other methods may solve a fixed-point problem x = g(x).
The simplest application of this to Ax = b, a system of n linear algebraic equations in n
unknowns, is to rewrite it as 0 = f(x) ≜Ax −b. Unfortunately, as you will see in Problem
8.4.3.10, the Jacobian matrix of this f(x) is A, so Newton’s method reduces to
xk+1 = xk −
 ∂f
∂x(xk)
−1
f (xk) = xk −A−1 (Axk −b) = · · · = A−1b.
Theoretically, this gives the exact solution x1 = A−1b no matter what initial guess x0 is cho-
sen. But, if we could evaluate A−1 accurately why would we even bother with Newton’s,
method?
So, some great mathematicians found other iterative methods for approximating the
solution of Ax = b. These methods and their later modifications helped enable accurate
numerical solution of large systems of linear algebraic equations, which in turn helped

Numerical Methods I
683
enable accurate approximate numerical solving of partial differential equations. Not only
that, but these iterative methods often make very efficient use of computer memory. And,
these methods often can exploit sparseness, that is, presence of vastly many zeros, of
matrices that are commonly used in applications to partial differential equations.
These iterative methods use the fact that finding the inverse of a diagonal or triangu-
lar matrix can be done accurately and quickly despite the issue of round-off error. First,
rewrite
A = L + D + U,
where
D = diag(a11, . . . , ann) is the matrix of diagonal entries of A
L is the lower triangular matrix of entries taken from A −D
U is the upper triangular entries taken from A −D
Historically, the first such method was Jacobi iteration, which first rewrites Ax = b as
Dx = −(L + U)x + b and then rewrites it as the fixed-point problem
x = D−1 (−(L + U)x + b) ,
assuming D is invertible, that is, all of the diagonal entries of A are nonzero. The method
of successive approximations in Section 8.2 would say to iterate
xk+1 = D−1 (−(L + U)xk + b) .
(8.39)
This method converges as long as ∥D−1(L + U)∥< 1 or as long as the spectral radius∗
ρ

D−1(L + U)

< 1. It makes sense to use as the initial guess x0 = D−1b that requires only
n divisions: x0 =
%
a−1
11 b1
· · ·
a−1
nn bn
&T.
The Jacobi method is called a relaxation method because we can think of the iterates as
xk+1 = D−1b + (correction involving xk). It’s as if we think of the k + 1-iterate as computing
the solution of Dx = b, a “relaxingly” easier version of (D+L+U)x = b, but then correcting
using the previously computed xk.
In many practical situations, Jacobi iteration converges but does so slowly.
Gauss–Seidel iteration is an old but good relaxation method that rewrites (D+L+U)x =
Ax = b as (D + L)x = −Ux + b and iterates
xk+1 = (L + D)−1 (−Uxk + b) .
(8.40)
This method converges as long as ∥(D + L)−1U∥< 1 or as long as the spectral radius
ρ

(D + L)−1U

< 1. It is known that in the special case when A is symmetric, that is,
AT = A, when it does converge the Gauss–Seidel iteration converges twice as fast as Jacobi
iteration. On the other hand, when A is not symmetric, it is possible for Jacobi iteration to
converge even though Gauss–Seidel does not!
The final methods we will mention are the over-relaxation methods of successive over-
relaxation (SOR) and symmetric successive over-relaxation (SSOR).
∗ρ(A), the spectral radius of a square matrix A, is the maximum of the absolute values of its eigenvalues.

684
Advanced Engineering Mathematics
SOR is a modification of Gauss–Seidel iteration to include a relaxation parameter ω in
the iteration:
xk+1 = (ωL + D)−1 (ω(−Uxk + b) + (1 −ω)Dxk) ,
that is,
xk+1 = (ωL + D)−1 (ωb + ((1 −ω)D −ωU) xk) .
(8.41)
Note that when ω = 1, SOR is the Gauss–Seidel iteration method.
Sophisticated software packages estimate what value of the parameter ω is optimal for a
given system, that is, best controls error, by converging quickly to a good approximation
of the solution.
SSOR is another modification of Gauss–Seidel iteration to break up each iteration cycle
into two half cycles as well as to include a relaxation parameter ω: assuming A is symmetric,
that is, U = LT,
⎧
⎨
⎩
xk+0.5 = M−1
ω (Nωxk + ωb)
xk+1 =

MT
ω
−1 
NT
ωxk+0.5 + ωb

⎫
⎬
⎭,
(8.42)
where Mω ≜D + ωU, Nω = (1 −ω)D −ωL.
NOTES: It can be difficult to find the exact value of ∥A∥, and the Frobenius norm, ∥A∥F,
only gives an upper bound on ∥A∥.
There is a good repair for this situation:
for two other choices of norm on
x = [ x1
· · ·
xn ]T in Rn (or Cn), there is an exact formula for ∥A∥.
The ∥∥∞norm on Rn is given by
∥x∥∞= max
j=1,...,n |xj|.
The induced matrix norm is defined to be the smallest number M for which it is true that
∥Ax∥∞≤M∥x∥∞, for all unit vectors x.
[By unit vector x we mean “satisfying” ∥x∥∞= 1.]
It follows that for all x in Rn,
∥Ax∥∞≤∥A∥∞∥x∥∞.
Theorem 8.7
If A = [aij] 1 ≤i ≤n
1 ≤j ≤n
, then ∥A∥∞= maxi
"n
j=1 |aij|, that is, the “maximum row sum of absolute
values.”

Numerical Methods I
685
The ∥∥1 norm on Rn is given by
∥x∥1 =
n

j=1
|xj|.
The induced matrix norm is defined to be the smallest number M for which it is true that
∥Ax∥1 ≤M∥x∥1, for all unit vectors x.
[By unit vector x we mean “satisfying” ∥x∥1 = 1.]
It follows that for all x in Rn,
∥Ax∥1 ≤∥A∥1∥x∥1.
Theorem 8.8
If A = [aij] 1 ≤i ≤n
1 ≤j ≤n
, then ∥A∥1 = maxj
"n
i=1 |aij|, that is, the “maximum column sum of
absolute values.”
Definition 8.3
An n × n matrix A is
(a) Row diagonally dominant if for each i = 1, . . . , n,
|aii| >

j
j ̸= i
|aij|
(b) Column diagonally dominant if for each i = 1, . . . , n,
|aii| >

i
i ̸= j
|aij|
Theorem 8.9
If A is either row or column diagonally dominant, then Jacobi iteration converges.

686
Advanced Engineering Mathematics
Learn More About It
Matrix Computations and Mathematical Software, by John R. Rice, McGraw-Hill, Inc. c⃝
1981, is among many good resources for learning about matrix computations and soft-
ware issues. Our Example 8.11 is basically the same as his Example 5.2. Rice’s book,
on pp. 140–141, includes a discussion of three false but commonly held beliefs about
matrix iterations.
Also good are Matrix Computations, 3rd edition, by Gene H. Golub and Charles F.
Van Loan, the Johns Hopkins University Press c⃝1996, and Numerical Recipes: The Art
of Scientific Computing, 3rd edition, by William H. Press et al., Cambridge University
Press c⃝2007.
8.4.3 Problems
In problems 1 and 2 find an upper bound on the condition number of the matrix by using
the Frobenius norm.
1.
⎡
⎢⎣
1
−2
3
2
0
1
3
1
0
⎤
⎥⎦
2.
⎡
⎢⎢⎢⎣
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
⎤
⎥⎥⎥⎦
[Aside: This is a 3 × 3 example of a Hilbert matrix. In general, Hilbert matrices
give useful examples for testing algorithms for matrices.]
In problems 3 and 4 use the Gauss–Jordan algorithm (a) without partial pivoting, and
(b) with partial pivoting, to solve the system of equations. Assume that numbers can only
be stored with four digits of accuracy. Compare your result with the true solution.
3.
 5.000 × 10−5x + 1.000y = 1.000
1.000x + 1.000y = 2.000

4.
 8.000 × 10−5x + 1.000y = 1.000
1.000x + 1.000y = 2.000

In problems 5 and 6 use the Gauss–Jordan algorithm with (a) partial pivoting, and
(b) implicit partial pivoting to solve the system of equations. Assume that numbers can
only be stored with four digits of accuracy. Compare your result with the true solution.
5.
 1.000x +
1.000y = 2.000
5.000x + 1.000 × 105y = 1.000 × 105


Numerical Methods I
687
6.

1.000x +
1.000y = 2.000
10.00x + 1.000 × 105y = 1.000 × 105

In problems 7 and 8 implement an iterative method of your choice, such as Jacobi,
Gauss–Seidel, SOR, or SSOR, to find an approximate solution, if possible, of the system
of equations. Compare your result with the true solution.
7. The system of Example 8.13.
8. The system of Problem 8.4.3.4.
9. Why are all orthogonal matrices perfectly conditioned?
10. If f(x) ≜Ax −b, calculate that the Jacobian matrix of f(x) is A.
11. Use the first remark after Definition 8.1 in Section 8.2 to explain why the matrix
norm has the property (8.36), that is, ∥AB∥≤∥A∥∥B∥, as long as the multiplication
AB makes sense: First, use that remark to note that ∥Ax∥≤∥A∥∥x∥for all x. So,
for all x,
∥ABx∥= ∥A(Bx)∥≤∥A∥∥Bx∥.
Continue by using that remark again.
12. Finish Example 8.13 by checking the rest of the details, including the serious
inaccuracy of the solution.
13. Check for yourself the remarks in the second paragraph following Example 8.13
that the Gauss–Jordan algorithm with implicit partial pivoting gives solution

x
y

=

1.000
1.000

and that the latter is quite accurate.
8.5 Linear Algebraic Eigenvalue Problems
We will see that there are many methods for approximating the eigenvalues and eigen-
vectors of a matrix. Which method is best depends on the size of the matrix, whether the
matrix is real and symmetric, whether we need all of the eigenvalues, and whether we
need all of the eigenvectors.
8.5.1 Elementary Method
The simplest approach to the numerical eigenvalue/eigenvector problem is to look on
eigenvalues as the roots of the characteristic polynomial P(λ) ≜|A −λIn| and thus try to
solve the equation
P(λ) = 0
by one of the methods discussed in Section 8.1. For example, the Müller root-finding
method works well for small matrices, for example, of size 20 or so. One nice thing about

688
Advanced Engineering Mathematics
this approach is that it works just as well for the generalized eigenvalue problem Ax = λBx,
where x ̸= 0, by using characteristic polynomial
P(λ) ≜|A −λB|.
8.5.2 Power Methods
Suppose that we wanted to find the largest frequency of vibration of ¨x = Ax, a system of
second-order linear constant coefficients ordinary differential equations. Here, A is a real,
n × n matrix whose eigenvalues are all negative.
Suppose A is any n × n matrix whose eigenvalues in magnitude are |λ1| ≥|λ2| ≥· · · ≥
|λn|. We say A has dominant eigenvalue λ1 if, in addition, |λ1| > |λ2|. If A is diagonaliz-
able, then in theory there exists a matrix P such that A = P diag(λ1, . . . , λn)P−1, and thus
for any x, we have
Akx = P diag(λk
1, . . . , λk
n)P−1x.
Intuitively, the dominant eigenvalue of A plays a greater and greater role in Akx as k →∞.
Suppose A has a real eigenvalue λ and corresponding real eigenvector x. Then ∥Ax∥=
∥λx∥= |λ| ∥x∥, so |λ| measures how much A magnifies eigenvector x. Also, if q is a unit
vector, that is, ∥q∥= 1, then qT(Aq) = qT(λq) = λ∥q∥2 = λ. Recall that if q is a unit vector
and A is a real, symmetric matrix, then qTAq is the Rayleigh quotient.
The basic power method for finding λ1 is to iterate the process
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
$xk = Axk−1
Choose index ik such that |$xik,k| = max1≤j≤n |$xj,k|
Mk ≜$xik,k
xk = M−1
k $xk
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
(8.43)
The first line in the process says that essentially we take powers of the matrix multiplying
an initial guess x0. The other lines say that we normalize the iterate xk so that its entry of
maximum magnitude is 1. When there is a choice of more than one ik in the second line,
the process can choose ik to change as little as possible as k increases.
If the iteration process converges, that is, if xk →x∞as k →∞, then the process can
probably be chosen so that there is a K and an index i such that ik = i and xi,k = 1 for all
k ≥K. It follows that ∥xk∥≥1 for all k ≥K, so ∥x∞∥≥1; hence, x∞̸= 0.
For k ≥K, let
μk+1 ≜$xi,k+1/xi,k = $xi,k+1/1 = $xi,k+1 = Mk+1.
(8.44)
We claim that Mk →μ as k →∞where μ is an eigenvalue of A with corresponding
eigenvector x∞. Why? (8.44) substituted into the third line of (8.43) implies that
Mkxk+1 = $xk+1 = Axk →Ax∞, as k →∞.
(8.45)

Numerical Methods I
689
But, in the product on the left-hand side, Mkxk+1, we know xk+1 →x∞. It follows that
Mk →μ for some μ, and thus, (8.45) implies that
μx∞= Ax∞.
Because x∞̸= 0, it is an eigenvector of A corresponding to eigenvalue μ.
In fact, using a basis for Rn consisting of x∞and n −1 other vectors, we can justify the
following result.
Theorem 8.10
If A has dominant eigenvalue λ1 and the initial guess x is not in E⊥
λ=λ1, then the power
method converges to the dominant eigenvalue and a corresponding eigenvector. The
method has linear convergence in the sense that
∥xk −x∞∥≤α ·
####
λ2
λ1
####
k
for some constant α.
In practice, the assumption that x is not in “E⊥
λ=λ1” is not needed. Why? Because after
a few iterations of the power method, the iterates will not be in E⊥
λ=λ1 because of round-
off error. This is one situation where round-off error actually comes to the rescue of a
numerical method rather than ruins it!
Example 8.14
Use the power method to find the dominant eigenvalue and a corresponding eigenvector
of A =

−5
6
−3
4

.
Method: With initial guess x0 =

1
0

, we calculate
$x1 =

−5.000000000
−3.000000000

M1 = −5.000000000
x2 =

1
0.6000000000

,
$x2 =

−1.400000000
−0.600000000

M2 = −1.400000000
x2 =

1
0.4285714286

,
$x3 =

−2.428571429
−1.285714286

M3 = −2.428571429
x3 =

1
0.5294117647

,

690
Advanced Engineering Mathematics
$x4 =

−1.823529412
−0.8823529412

M4 = −1.823529412
x4 =

1
0.4838709677

,
$x5 =

−2.096774194
−1.064516129

M5 = −2.096774194
x4 =

1
0.5076923077

.
It appears that slowly Mk →−2, the dominant eigenvalue, and xk →

1
0.5

, a
corresponding eigenvector. ⃝
In the “read more about it” in this section, we refer the reader to the book by Biswa
Nath Datta to find other useful methods. In addition, you can learn that using a shift of
A, namely, replacing A by A −σIn, can speed up convergence of the power method by
replacing |λ2/λ1| by a significantly smaller number |(λ2 −σ)/(λ1 −σ)|. Note that A has
eigenvalue λ and corresponding eigenvector x if, and only if, A −σI has eigenvalue λ −σ
and the same corresponding eigenvector x. Also, it is possible to use the Rayleigh quotient
to help pick a good initial guess for the power method. Further, iteration using powers
of (A −σI)−1 rather than A −σI gives us the inverse power method, which also can be
useful.
8.5.3 Deﬂation and Similarity
Recall that if P is any invertible matrix whose columns are eigenvectors of a matrix A,
then P−1AP = D is a diagonal matrix whose diagonal entries are the eigenvalues of A.
Recall also that if two matrices A and B are “similar,” that is, if there is an invertible matrix
P such that P−1AP = B, then A and B’s eigenvalues and their algebraic and geometric
multiplicities are the same. Further, the eigenvectors of A are of the form Py where y is
any eigenvector of B, as you will explain in Problem 8.5.7.10. The process A →P−1AP is
called a similarity transformation.
Further, having obtained the dominant eigenvalue of A, it is possible to use deflation of
polynomials, or deflation, for short. This is a fancy name for using a root λ1 to factor the
polynomial P(λ) = (λ0 −λ)g(λ). Further, it involves using a similarity transformation
A →
⎡
⎢⎢⎣
λ1
|
0T
−−
|
−−
0
|
A22
⎤
⎥⎥⎦,
reducing the size of the problem from n to n −1. If A is a real, symmetric matrix, then this
method works well.
8.5.4 Using Similarity Transformations
Some successful iterative methods for approximating the eigenvalues and eigenvectors of
A define a sequence
Ak ≜P−1
k
· · · P−1
1 A0P1 · · · Pk,
(8.46)

Numerical Methods I
691
so Ak is similar to A for k = 0, 1, . . . . If, for some k, Ak is nearly a diagonal or triangular
matrix, then the diagonal entries of Ak are approximations of the eigenvalues of A. Some
of the methods use a preliminary algorithm to do one similarity transformation A →A0
to enable (8.46), the second algorithm, to be computationally efficient. In other words, the
preliminary algorithm prepares the matrix for the second algorithm.
These methods are like a patient detective who systematically uncovers clues that give
a more and more accurate and fuller picture of the case. A detective hopes to know the
truth beyond a reasonable doubt. Numerical methods used in research and development
need mathematical assurances that they have controlled their errors. Historically, the soft-
ware routines in LINPACK and then EISPACK were breakthroughs in providing such robust
capabilities to work with matrices.
8.5.5 Background
Recall from Problem 2.4.4.17 that if q is a unit vector in Rn, then the Householder matrix
Q ≜I −2qqT is both symmetric, that is, QT = Q, and is orthogonal.
Corollary 8.1
If u is any nonzero vector and βu ≜
2
∥u∥2 , then Qu ≜I −βuuuT is a Householder matrix.
Why? You will derive this in Problem 8.5.7.9. 2
Recall that e(1) ≜
% 1
0
· · ·
0 &T. Suppose A =
% a1
· · ·
an
&
is a given n × n matrix,
but we want to work with a matrix B that replaces a1, the first column of A, by a constant
times e(1). It turns out we can use a Householder matrix to accomplish this.
Define the extended sign function Sign(x) by
Sign(x) ≜

−1,
x < 0
+1,
x ≥0

.
Lemma 8.1
Suppose x =
% x1
· · ·
xn
&T is a real, nonzero vector. Define
u = x + (Sign(x1)∥x∥)e(1) and Q = Qu,
where ∥∥is the usual, Euclidean norm on Rn. Then
Qx = −(x1∥x∥)e(1).
(8.47)
Why? You will explain this in Problem 8.5.7.8. 2

692
Advanced Engineering Mathematics
Definition 8.4
A matrix A = [aij] 1 ≤i ≤n
1 ≤j ≤n
is in upper Hessenberg form if ai,j = 0 for i > j + 1. Another
name for Hessenberg matrix is almost triangular matrix.
For example, A =
⎡
⎢⎢⎢⎢⎢⎢⎣
⋆
⋆
⋆
⋆
⋆
■
⋆
⋆
⋆
⋆
0
■
⋆
⋆
⋆
0
0
■
⋆
⋆
0
0
0
■
⋆
⎤
⎥⎥⎥⎥⎥⎥⎦
is in unreduced upper Hessenberg form if ■
stands for any nonzero real number and ⋆stands for any real number.
Similarly, a matrix is in lower Hessenberg form if ai,j = 0 for i+1 < j. A matrix that is in
both upper and lower Hessenberg form is tridiagonal, meaning that aij = 0 for |i −j| > 1.
A partitioned matrix [Ars] with blocks Ars = O for r < s is called upper quasi-
triangular. For example, the partitioned matrix A in Example 1.32 in Section 1.5 is
upper quasi-triangular, and the matrix A in both Problems 1.5.3.24 and 1.5.3.25 is lower
quasi-triangular. All Hessenberg matrices are also quasi-triangular.
Lemma 8.2
Every real, square matrix A can be reduced to an upper Hessenberg matrix by a sequence
of at most (n −1) similarity transformations. Further, if A is symmetric then that upper
Hessenberg matrix is tridiagonal.
Why? For example, given an n×n matrix A, we can find an n×n Householder matrix Q so
that QAQ has its first column become
% a11
⋆
0
· · ·
0 &T. How? Write A in partitioned
form as
⎡
⎢⎢⎢⎢⎢⎣
a11

yT
−−

−−

x

A(n−1)
⎤
⎥⎥⎥⎥⎥⎦
.
By using an (n −1) × (n −1) Householder matrix (n−1)P, we can change the (n −1)-vector
x ≜
% a21
· · ·
an1
&T to
% −a21∥x∥
0
· · ·
0 &T. Let Q have the partitioned form
⎡
⎢⎢⎢⎢⎢⎣
1

0T
−−

−−

0

(n−1)P
⎤
⎥⎥⎥⎥⎥⎦
,

Numerical Methods I
693
where 0 is the zero vector in Rn−1. In fact, Q = I −βuvvT, where v =
%
0
uT &T. The
multiplication of A on the left by Q has the desired effect on the first column and the
multiplication of QA on the right by Q does not change the first column. Householder
matrices are both symmetric and orthogonal, so A →QAQ is a similarity transformation.
If A is symmetric, then multiplication of (QA) on the right by Q also has the desirable
effect of changing the first row to A to be
% −a21∥x∥
0
· · ·
0 &
.
Whether or not A is symmetric, the similarity transformation A →QAQ reduces A to
the form
QAQ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a11

yT
−−

−−
⋆

zT
−−

−−

0

A(n−2)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
If x = 0 (to within the precision of the machine), then we can take Q = I and not bother to
multiply to get QAQ = A.
In a similar way we can use an (n−2)×(n−2) Householder matrix (n−2)P as part of an n×n
Householder matrix Q2 so that the first two columns of Q2(QAQ)Q2 are as they should be
in an upper Hessenberg form. Continuing in this way completes the explanation. 2
8.5.6 QR Algorithm
Having found a similarity transformation that transforms the matrix to one in upper
Hessenberg form, the next step in finding the eigenvalues and eigenvectors is to use the
“QR algorithm.” This method relies on the ability to find the QR factorization, as in Section
2.7, of each iterate in the process.
Suppose Ak is an n × n matrix iterate. Theoretically, there exists at least one QR
factorization of Ak, namely,
Ak = QkRk.
Because A is real and square, Qk is a real, orthogonal n × n matrix and Rk is an upper
triangular matrix. If Ak is invertible, then so is Rk. Because Qk is orthogonal, the reverse
multiplication RkQk is orthogonally similar to Ak, by Problem 2.7.7.28. Remarkably, the QR
algorithm process, specifically
Ak+1 = RkQk, where Ak = QkRk is a QR −factorization,
produces a sequence of similarity transformations, that is, (8.46), that converge to a
matrix very close to a triangular matrix. As mentioned before, we can read approximate
eigenvalues and eigenvectors of that triangular matrix.
The reason for the preliminary transformation to upper Hessenberg form is purely for
computational efficiency, as discussed in Matrix Computations, by Gene H. Golub and
Charles F. Van Loan, mentioned immediately as follows.

694
Advanced Engineering Mathematics
Learn More About It
Numerical Linear Algebra and Applications, 2nd ed., by Biswa Nath Datta, Society for
Industrial and Applied Mathematics,
c⃝2010, has much useful information about
algorithms for finding eigenvalues and eigenvectors. Also, as in Section 8.4, Matrix
Computations, 3rd edition, by Gene H. Golub and Charles F. Van Loan, the Johns
Hopkins University Press c⃝1996, and Numerical Recipes: The Art of Scientific Comput-
ing, 3rd edition, by William H. Press et al., Cambridge University Press c⃝2007, are
excellent resources.
8.5.7 Problems
In problems 1–3, reduce the given matrix to upper Hessenberg form. You may use tech-
nology, for example, Mathematica, to help with the arithmetic and algebra, but be sure to
show all of the steps involved.
1.
⎡
⎢⎢⎢⎢⎣
1
1
1
1
1
2
0
1
1
0
−1
1
0
−1
2
1
⎤
⎥⎥⎥⎥⎦
2.
⎡
⎢⎢⎢⎢⎣
1
1
1
1
1
0
−1
1
1
2
0
1
0
−1
2
1
⎤
⎥⎥⎥⎥⎦
3.
⎡
⎢⎢⎢⎢⎣
1
1
1
−1
1
1
0
1
1
−2
0
1
0
−1
−2
1
⎤
⎥⎥⎥⎥⎦
In problems 4 and 5 apply the QR algorithm for two iterations to the given matrix A0
to find A1 and A2. You may use technology, for example, the Mathematica command
QRDecomposition[A], but be sure to show all of the steps involved.
4.
⎡
⎢⎣
1
1
1
1
−2
0
0
1
−1
⎤
⎥⎦
5.
⎡
⎢⎣
1
0
−1
−1
2
0
0
−1
1
⎤
⎥⎦
In problems 6 and 7 apply the power method to find the dominant eigenvalue of the given
matrix.

Numerical Methods I
695
6.

2
−1
5
−4

7.

3
1
2
2

8. Derive the result of Lemma 8.1.
9. Derive the result of Corollary 8.1, that is, that Qu ≜I −βuuuT is a Householder
matrix if u is any nonzero vector and βu ≜
2
∥u∥2 .
10. Suppose P−1AP = B. Explain why the eigenvectors of A are of the form Py where
y is any eigenvector of B.
8.6 Approximations of Derivatives
Given a function f(x) and a real number a, in Section 8.3, we defined
xi = a + ih,
xi−0.5 = a + (i −0.5)h,
fi = f(xi),
and fi−0.5 = f(xi−0.5).
We would prefer to use k for the index, but we use i because that is the usual notation in
this topic. Also, standard notation in this topic uses k to mean something else, as we will
see in (8.63) in Section 8.7, (8.68) in Section 8.7, and other places.
In addition, we can define
f ′
i ≜f ′(xi).
Linear approximation says that
f ′(xi) ≈f(xi+1) −f(xi)
xi+1 −xi
= fi+1 −fi
h
.
This motivates defining the forward difference operator  by
fi ≜fi+1 −fi.
(8.48)
So, we can approximate
f ′
i ≈h−1fi.
(8.49)
By definition,
2fi ≜
%
fi
&
= 
%
fi+1 −fi
&
= fi+1 −fi =

fi+2 −fi+1

−

fi+1 −fi

;
hence,
2fi = fi+2 −2fi+1 + fi.

696
Advanced Engineering Mathematics
It is also useful to define the central difference operator δ by
δfi ≜fi+0.5 −fi−0.5.
(8.50)
Similar to the calculation of 2, we can show that
δ2fi = fi+1 −2fi + fi−1.
Because
f ′′(xi) ≈f ′(xi) −f ′(xi−1)
xi+1 −xi
≈h−1fi −h−1fi−1
h
= (fi+1 −fi) −(fi −fi−1)
h2
,
we see that
f ′′
i ≈h−2δ2fi.
(8.51)
We know that second derivatives play an important role in science and engineering, so
(8.51) explains why central difference operators are very popular.
How good are the approximations (8.49) and (8.51)? The answers come from Taylor
series: because xi+1 −xi = h,
fi+1 = f(xi+1) = f(xi) + hf ′(xi) + h2
2! f ′′(xi) + h3
3! f ′′′(xi) + · · · .
Theorem 8.11
(Taylor’s theorem with remainder) If f(x) is (n + 1) times continuously differentiable on
an open interval I and x and y are in I, then
f(y) = f(x) + (y −x)f ′(x) + (y −x)2
2!
f ′′(x) + · · · + (y −x)n
n!
f (n)(x) + (y −x)n+1
(n + 1)!
f (n+1)(ξ)
(8.52)
for some ξ in I.
We will also use the big O notation.
Definition 8.5
A quantity ε is O(hp) if there exists
lim
h→0
ε
hp ,
in which case we say ε is of order p as h →0.

Numerical Methods I
697
Define an interval by
Ii,η ≜(xi −η, xi + η).
Theorem 8.12
(a) If f(x) is twice continuously differentiable on an interval of the form Ii,η for some
η > h, then
f ′(xi) = h−1fi + f ′′(ξ)
2
h,
(8.53)
for some ξ in the interval Ii,h, and
### f ′(xi) −h−1fi
### = O(h1).
(8.54)
(b) If f(x) is four times continuously differentiable on an interval of the form Ii,η for
some η > h, then
f ′′(xi) = h−2δ2fi + f ′′′′(ξ+) + f ′′′′(ξ−)
24
h2,
(8.55)
for some ξ± in the interval Ii,h, and
### f ′′(xi) −h−2δ2fi
### = O(h2).
(8.56)
Why? (a) By Taylor’s theorem, there exists an ξ in Ii,h such that
f(xi+1) = f(xi) + hf ′(xi) + f ′′(ξ)
2
h2,
that is,
fi+1 −fi = hf ′(xi) + f ′′(ξ)
2
h2.
Rearrange terms and divide through by h to get
f ′(xi) = h−1 
fi+1 −fi

+ f ′′(ξ)
2
h,
that is,
f ′(xi) = h−1fi + f ′′(ξ)
2
h,

Numerical Methods I
699
In effect, symmetry of h versus −h enabled the cancellations that gave us the higher-
order estimate, O(h2), instead of O(h).
8.6.1 Problems
1. Derive the formula for δδfi.
In problems 2 and 3 you are given a formula for approximating a derivative. Find the order
of the error.
2. f ′(xi) ≈(3h)−1 
2fi −fi−1 −fi−2

.
3. f ′′(xi) ≈(3h2)−1 
fi+1 −3fi−1 + 2fi−2

.
In problems 4–7 you are given a formula for approximating a derivative. Find the value of
the parameter α that makes it a valid approximation and find the order of the error of the
approximation for that value of α.
4. f ′(xi) ≈(αh)−1 
fi+1 −3fi + 2fi−1

.
5. f ′(xi) ≈(αh)−1 
fi+1 −4fi + 3fi−1

.
6. f ′(xi) ≈(αh)−1 
4fi+1 −3fi −fi−2

.
7. f ′′(xi) ≈(αh2)−1 
2fi+1 −3fi + fi−2

.
8. Explain why the central difference approximation of f ′ given by
f ′(xi) ≈fi+1 −fi−1
2h
,
has error whose order is two.
9. We want to approximate (⋆)
f ′(xi) ≈afi−2 + bfi + cfi+1. Choose the constants a,
b, c so as to make the approximation (⋆) have the highest order. State what that
order is.
8.7 Approximate Solutions of ODE-IVPs
Recall from Sections 3.1 and 3.2 that a scalar ODE-IVP has the form (3.19) in Section 3.2,
that is,
⎧
⎪⎨
⎪⎩
dy
dt = f(t, y)
y(t0) = y0
⎫
⎪⎬
⎪⎭
.
(8.61)
We will see that finding a numerical approximation of the solution of an ODE-IVP relies
on approximating the derivative, ˙y(t), so knowledge from Section 8.6 will be useful.
We use numerical methods for approximating the solution of an ODE because, in gen-
eral, we cannot derive the exact solution of the ODE. Nevertheless, our examples will be
for ODEs where we can write down the exact solution so we can see clearly how well the
numerical methods are working.

700
Advanced Engineering Mathematics
In this section we will present some of the elementary concepts and methods in the topic
of numerical solutions of ODEs, which plays a great role in applied science and engi-
neering. For that reason, there is a vast and ever-changing “state-of-the-art” literature on
this topic. The truth is that those working on computationally intensive problems should
employ the services of numerical analysts just as people working on statistically intensive
problems should employ statistical consultants.
Similar to the notation in Sections 8.3 and 8.6, define
ti = t0 + ih,
ti+0.5 = t0 + (i + 0.5)h,
yi = y(ti),
and yi+0.5 = y(ti+0.5),
and, in addition,
fi ≜f(ti, yi).
(8.62)
The simplest way of approximating a solution of (8.61) is Euler’s method (EM), which
uses the tangent line at the point (ti, yi) on the graph of y(t) to approximate y(ti+1):
 y0 = y(t0),
and
yi+1 ≜yi + k1,
where k1 ≜hfi = hf(ti, yi)

.
(8.63)
So,
y(t1) ≈y1 = y0 + hf0,
y(t2) ≈y2 = y1 + hf1 = y0 + hf0 + hf1,
y(t3) ≈y3 = y2 + hf2 = y0 + hf0 + hf1 + hf2, etc.
Note that (8.63) is a first-order nonlinear difference equation, as in Section 4.6.
After we obtain the approximate solution values at t1, t2, . . . , we can plot those points
and connect them with straight line segments. This gives a piecewise linear approximate
solution graph, as illustrated in Figure 8.5.
(t0, y0)
(t1, y1)
(t3, y3)
(t2, y2)
t0
t1
t2
t3
FIGURE 8.5
Numerical solution of an ODE-IVP.

Numerical Methods I
701
A step size h < 0 is allowed by numerical methods for ODEs. Recall that Section 3.2’s
existence and uniqueness theory for (8.61) gives solutions that exist both for t less than
the initial time, t0, and for t greater than the initial time. With the exception of Problems
8.7.7.1 and 8.7.7.12, our examples and problems will be concerned with h > 0, although
they could just as easily have h < 0.
To understand the performance of numerical methods for approximating the solution of
an ODE-IVP, it helps to have one more notation and another background result from the
first-year calculus. Define
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
˙fi ≜d
dt
 
f(t, y(t))
!###
t=ti
¨fi ≜d2
dt2
 
f(t, y(t))
!###
t=ti
...
...
f (n−1)
i
≜dn−1
dtn−1
 
f(t, y(t))
!###
t=ti
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.64)
For example, if ˙y = f(t, y) = 2ty2, then, using implicit differentiation, we find that
˙fi =

2y2 + 4ty˙y
###
t=ti
=

2y2 + 4ty · 2ty2###
t=ti
= 2y2
i (1 + 4tiyi).
Taylor’s Theorem 8.11 in Section 8.6, specifically (8.52) in Section 8.6, for the function
˙y(t) ≜f(t, y(t)) implies
y(ti+1) = y(ti) + hfi + h2
2!
˙fi + h3
3!
¨fi + · · · + hn
n! f (n−1)
i
+ O(hn+1).
(8.65)
Recall that (3.16) in Section 3.2, or (6.36) in Section 6.4, the multivariable chain rule, implies
d
dt
%
f(t, y(t))
&
= ∂f
∂t (t, y(t)) + ∂f
∂y(t, y(t)) · dy
dt (t).
Substituting in ˙y = f(t, y) gives
d
dt
%
f(t, y(t))
&
= ∂f
∂t (t, y(t)) + ∂f
∂y(t, y(t)) · f(t, y(t)).
(8.66)
So, the first definition in (8.64) can be evaluated as
˙fi = ∂f
∂t (ti, yi) + ∂f
∂y(ti, yi) · fi.
(8.67)
Suppose we have a numerical solution y0, y1, y2, . . . that approximates the solution of
(8.61). We define the local error to be
E =
##y(ti+1) −yi+1
## ,

702
Advanced Engineering Mathematics
that is, the difference between the exact solution’s value at t = ti+1 and the numerical
solution’s value at t = ti+1, under the assumption that yi = y(ti), that is, that the numerical
solution was exactly correct before the step of going from (ti, yi) to (ti+1, yi+1).
If we had not assumed that the numerical solution was exactly correct in previous steps,
for example, if yi ̸= y(ti), then
##y(ti+1) −yi+1
## is the global error.
The local error tells us how good any one step of the numerical solution process is, while
the global error tells us how error propagates and compounds itself. In this section we
will get results about the local error; getting results for global error is usually a lot more
difficult. But, here is a “rule of thumb”: if the local error is O(hp), then the global error is
usually O(hp−1).
Theorem 8.13
For Euler’s Method (EM), the local error estimate is O(h2).
Why? For n = 1, Taylor’s theorem in the form of (8.65) gives
y(ti+1) = y(ti) + hfi + O(h2).
Recall that when evaluating local error, we assume yi = y(ti), and recall that (8.63) has
k1 ≜hfi. The local error is
##y(ti+1) −yi+1
## =
###

y(ti) + hfi + O(h2)

−

yi + k1
###
=
###

y(ti) +
hfi + O(h2) −

yi +
hfi
### = O(h2). 2
Example 8.16
The improved tangent method (ITM), also known as a midpoint method or as a Runge–
Kutta method of order two, is
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
y0 = y(t0)
yi+0.5 ≜yi + k1, where k1 ≜h
2f(ti, yi)
yi+1 ≜yi + k2, where k2 ≜hf(ti+0.5, yi+0.5).
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(8.68)
Find the local error estimate.
Method: Recall that when evaluating local error, we assume yi = y(ti). Equation (8.68)
gives
yi+1 = yi + hf(ti+0.5, yi+0.5) = yi + hf

ti + h
2, yi + k1

.
(8.69)
Taylor series for a function of two variables, found as follows in Theorem 13.7 in Section
13.2, implies that
f

ti + h
2, yi + k1

= f(ti, yi) + h
2
∂f
∂t (ti, yi) + k1
∂f
∂y(ti, yi) + O
*h
2
2
+ h
2k1 + k2
1
+
= f(ti, yi) + h
2
∂f
∂t (ti, yi) + k1
∂f
∂y(ti, yi) + O

h2
.

Numerical Methods I
703
Combining this with (8.69) gives
yi+1 = yi + hf

ti + h
2, yi + k1

= yi + h

f(ti, yi) + h
2
∂f
∂t (ti, yi) + k1
∂f
∂y(ti, yi) + O

h2
.
But,
k1 = h
2 f(ti, yi) = h
2 fi,
so
yi+1 = yi + hfi + h2
2
∂f
∂t (ti, yi)0 + ∂f
∂y(ti, yi) · fi

+ O

h3
.
On the other hand, Taylor’s theorem in the form of (8.65), along with (8.67), implies
y(ti+1) = yi + hfi + h2
2
˙fi + O(h3) = yi + hfi + h2
2
∂f
∂t (ti, yi) + ∂f
∂y(ti, yi) · fi

+ O(h3).
So, the local error is
##y(ti+1) −yi+1
## =
#####
yi +

hfi +

h2
2
∂f
∂t (ti, yi) + ∂f
∂y(ti, yi) · fi

+ O(h3)
−
yi −

hfi +


h2
2
∂f
∂t (ti, yi) + ∂f
∂y(ti, yi)fi
##### ,
that is, the local error is O(h3). ⃝
To calculate even higher orders, we would need further applications of the multivariable
chain rule, for example,
d2
dt2
 
f(t, y(t))
!
= d
dt
∂f
∂t (t, y(t)) + ∂f
∂y(t, y(t)) · f(t, y(t))

= ∂2f
∂t2 (t, y(t)) + 2 ∂2f
∂t∂y(t, y(t)) · f(t, y(t)) + ∂f
∂y(t, y(t)) · d
dt
%
f(t, y(t))
&
+ ∂2f
∂y2 (t, y(t)) ·

f(t, y(t))
2.
So, akin to (8.67),
¨fi = ∂2f
∂t2 (ti, yi) + 2 ∂2f
∂t∂y(ti, yi) · fi + ∂f
∂y(ti, yi)
∂f
∂t (ti, yi) + ∂f
∂y(ti, yi) · fi

+ ∂2f
∂y2 (ti, yi) · (fi)2,
that is,
¨fi = ∂2f
∂t2 (ti, yi) + 2 ∂2f
∂t∂y(ti, yi) · fi + ∂f
∂y(ti, yi) · ˙fi + ∂2f
∂y2 (ti, yi) · (fi)2.
(8.70)

704
Advanced Engineering Mathematics
8.7.1 Runge–Kutta Methods
Runge–Kutta methods were invented separately by Runge and Kutta. While their imple-
mentation by hand is a little more intricate than EM, they are much more accurate, and
their implementations in software programming are not difficult.
Very popular is the Runge–Kutta method of order four given by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
y0 = y(t0)
yi+1 ≜yi + 1
6(k1 + 2k2 + 2k3 + k4), where
k1 ≜hf(ti, yi), k2 ≜hf(ti+0.5, yi + 1
2k1),
k3 ≜hf(ti+0.5, yi + 1
2k2), and
k4 ≜hf(ti+1, yi + k3)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.71)
A simpler example of a Runge–Kutta method of order two is the ITM discussed in
Example 8.16.
Example 8.17
Consider the ODE-IVP ˙y = −y + e−2t, y(2.0) = 1.
(a) Use EM with step size, h1 = 0.25, to estimate y(5.0).
(b) Use the ITM given in (8.68) with step size, h2 = 0.5, to estimate y(5.0).
(c) Find the exact solution of the ODE-IVP.
(d) Compare the function value for the exact solution of the ODE-IVP with your
approximate results from parts (a) and (b).
Method: Results for parts (a) and (b) are in Table 8.14. Those results were produced using
a spreadsheet.
(c) The exact solution of the ODE-IVP is y(t) = −e−2t+(e2+e−2)e−t, which can be found
using the integrating factor method in Section 3.2.
(d) The exact solution value is y(5) = −e−10 + (e2 + e−2)e−5 ≈0.05065355. The ITM with
step size h2 = 0.5 gave y(5) ≈y6 ≈0.06033006 and EM with step size h1 = 0.25
gave y(5) ≈y12 ≈0.03260821, so the ITM with twice as large a step size produced a
significantly more accurate conclusion than EM. The honorific “improved” is truly
deserved here, unlike its often inappropriate use in consumer advertising. ⃝
We see in Table 8.14 that, as is typical for numerical methods for approximating the
solution of an ODE-IVP, the more time steps we take, the higher the relative global error
becomes. This is because, for example, an error after four time steps causes two kinds of
error in the fifth step to approximate y5: (i) The global error-prone value of y4 is substituted
into the numerical method, and (ii) even if y4 had been correct, one more local error could
be made in going to the next step. Error can accumulate and propagate.
Numerical approximation of solutions of ODE-IVPs often involves a tension between
(a) the need to reduce local error by reducing the step size, (b) the danger of increased
round-off error because of the step size being too small, and (c) the danger of taking too

Numerical Methods I
705
TABLE 8.14
Example 8.17: EM and ITM
i
ti
≈y(ti) by EM
Exact y(ti)
i
ti
≈y(ti) by ITM
0
2.00
1.00000000
1.00000000
0
2.00
1.00000000
1
2.25
0.75457891
0.78195602
2
2.50
0.56871143
0.61090171
1
2.50
0.62826504
3
2.75
0.42821806
0.47693148
4
3.00
0.32218524
0.37213864
2
3.00
0.39386679
5
3.25
0.24225862
0.29024888
6
3.50
0.18206982
0.22630505
3
3.50
0.24660862
7
3.75
0.13678034
0.17640364
8
4.00
0.10272352
0.13747857
4
4.00
0.15429295
9
4.25
0.07712651
0.10712621
10
4.50
0.05789575
0.08346503
5
4.50
0.09649289
11
4.75
0.04345266
0.06502389
12
5.00
0.03260821
0.05065355
6
5.00
0.06033006
many time steps so that the program runs too slowly to produce useful results. For exam-
ple, it took many decades of improved hardware and software before machines could
accurately produce a forecast of the weather an hour from now in less than an hour of
computations! As another example, if we need one hundred years of computing time to
produce a design for a more aerodynamically efficient automobile, then it would put the
company out of business.
8.7.2 Multistep Methods
Some useful methods of approximating the solution of an ODE are called multistep
methods because they calculate yi+n using yi, yi+1, . . . , yi+n−1. Such a multistep method
is an n-th order difference equation. The order of a multistep method is the order of the
corresponding difference equation.
One family of very popular multistep methods is called Adams methods. For example,
the Adams–Bashforth method of order two is
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
y0 = y(t0)
y1 calculated by some other method
yi+1 ≜yi + h
2 ·

3f(ti, yi) −f(ti−1, yi−1)

, i = 1, 2, . . .
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(8.72)
One drawback of any multistep method is that it begins by using some other method,
for example, a Runge–Kutta method, to approximate the values between y0 and yn, for
example, y1 for the second-order method in (8.72). When “filling in” those values, we must
take care that the order of the other method’s global error is no worse than the order of
the local error of the multistep method or take many small steps in between, for example,
y0.1, y0.2, . . . , y1.0, where y(x0 + βh) ≈yβ, to achieve the same accuracy as the multistep
method.

706
Advanced Engineering Mathematics
8.7.3 Predictor–Corrector Methods
These “implicit” methods use a formula of the form
yi+n = yi+n−1 + g(yi+n, yi+n−1, . . . , yi)
that must be solved for yi+n by some numerical, iterative method, for example, Newton’s
method. For example, an Adams–Moulton “corrector” implicit formula of order two is
given by
yi+3 = yi+2 + h
24

9fi+3 + 19fi+2 −5fi+1 + fi

,
where fj ≜f(tj, yj). The iteration may begin with an initial guess for yi+3 calculated by a
predictor formula, for example, an Adams–Bashforth formula of order four given by
yi+3 = yi+2 + h
24

55fi+2 −59fi+1 + 37fi −9fi−1

.
As mentioned earlier, the Adams–Bashforth formula requires filling in of intermediate
values, by some other method, before it can proceed.
One advantage of corrector methods is that (Conte and de Boor, 1972) they can provide
an estimate of the local error; the software package can use that estimate to decrease the
step size temporarily to increase the accuracy. For this reason, Adams and other predictor–
corrector methods are often used if we suspect the system is “stiff,” a concept that we will
briefly discuss in Section 8.7.6.
8.7.4 Systems of ODEs
All of the methods for scalar ODEs have natural generalizations to systems of aforemen-
tioned ODEs. For example, EM for a system is
 x0 = x(t0)
xi+1 ≜xi + k1,
where k1 ≜h fi = h f(ti, xi)

.
8.7.5 Taylor’s Formula Method
Perhaps one of the oldest higher-order generalization of EM is to use more terms in the
Taylor series expansion (8.52) in Section 8.6. Here we will write that as
y(ti+1) = y(ti) + h˙y(ti) + h2
2! ¨y(ti) + · · · + hn
n! y(n)(ti) +
hn+1
(n + 1)! y(n+1)(ξ).
(8.73)
Let yi be the approximation for y(ti), ˙yi be the approximation for ˙y(ti), etc. The order of
Taylor’s method (TM) (8.73) is n, the local error is O(hn+1), and the global error is O(hn).
Consider the solution of an ODE ˙y = f(t, y). As in Section 8.6, in principle we need the
chain and product rules to calculate the formulas for ¨y, ...y, etc., that appear in the Taylor
series.

Numerical Methods I
707
TABLE 8.15
Example 8.18: TM and ITM
i
ti
≈y(ti) by TM
Exact y(ti)
i
ti
≈y(ti) by ITM
0
1.00
−1.00000000
−1.00000000
0
1.00
−1.00000000
1
1.25
−0.68750000
−0.64000000
1
1.25
−0.68359375
2
1.50
−0.48948288
−0.44444444
2
1.50
−0.48492460
3
1.75
−0.36078143
−0.32653061
3
1.75
−0.35703326
4
2.00
−0.27470751
−0.25000000
4
2.00
−0.27194580
Example 8.18
Consider the ODE-IVP ˙y = 2ty2, y(1.0) = −1.
(a) Use TM of order two with step size, h1 = 0.25, to estimate y(2.0).
(b) Use the ITM given in (8.68) with step size, h2 = 0.25, to estimate y(2.0).
(c) Find the exact solution of the ODE-IVP.
(d) Compare the function value for the exact solution of the ODE-IVP with your
approximate results from parts (a) and (b).
Method: (a) ˙y = f(t, y) ≜2ty2, so
¨y = d
dt
%
2ty2&
= 2 · 1 · y2 + 2t · (2y ˙y) = 2y2 + 2t · 2y · (2ty2) = 2y2(1 + 4t2y).
So, TM of order two for this example is
yi+1 = yi + h˙yi + h2
2 ¨yi = yi + h · 2tiy2
i + h2
2 · 2y2
i (1 + 4t2
i yi).
The results of TM and (b) the ITM, both with step size h = 0.25, are shown in Table 8.15.
In particular, y(2.0) ≈y4 ≈−0.20439427 for TM and y(2.0) ≈y4 ≈−0.27194580 for ITM
versus the exact solution value y(2.0) = −0.25.
(c) The method of separation of variables in Section 3.2 or integrating factor in Section
3.1 produces the exact solution y(t) = −t−2; hence, the exact value is y(2.0) = −0.25.
(d) With h = 0.25, we see that TM is not very accurate, but the ITM is fairly accurate.
One method may be more accurate than another for a particular example, even though
they both have order two. ⃝
By the way, TM of order two with h = 0.125 gives y(2.0) ≈y8 ≈−0.25381128, with
h = 0.0625 gives y(2.0) ≈y16 ≈−0.25079251, and with h = 0.03125 gives y(2.0) ≈y32 ≈
−0.25018258, with very similar results for the ITM. Decreasing h seems to lead to better
and better results fairly quickly.
In the realm of the “old becoming new again,” the book Numerical Recipes: The Art
of Scientific Computing, mentioned in the “read more about it” as follows, discusses a
“multivalue (or Nordsieck) method” that also uses Taylor series terms.
8.7.6 Numerical Instability and Stiffness
Astute users of numerical methods notice that sometimes if the step size, h, is too large,
then some of the numerical solutions of an ODE-IVP behave qualitatively in significantly

708
Advanced Engineering Mathematics
wrong ways. For example, an asymptotically stable system of ODEs could have numerical,
approximate solutions that don’t go to zero or even blow up as t →∞.
Throughout this section, we will assume that the LCCCHS ˙x = Ax is asymptotically
stable. According to Theorem 5.11 in Section 5.3, that is, all eigenvalues λ of the square
matrix A have negative real part.
For example, for EM for ˙x = Ax, what is the basic cause of numerical instability? The
corresponding linear homogeneous system of difference equations is xi+1 −xi = hAxi,
that is,
xi+1 = (I + hA)xi.
It may be unstable, according to Theorem 5.21 in Section 5.7, because the matrix I + hA
may have an eigenvalue μ whose magnitude is greater than one.
Let λM be the eigenvalue of A whose real part is the most negative. By Theorem 2.8 in
Section 2.2, the eigenvalues of I + hA are μ = 1 + hλ. Suppose h > 0. If
h · Re(λM) < −2,
then |μ| = |1 + hλM| > 1, and thus, I + hA would have an eigenvalue μ whose magnitude
is greater than one, causing the system of difference equations to be unstable.
Also, no matter what the sign of h, if λ0 is the eigenvalue A whose imaginary part is
the greatest in magnitude, then the system of difference equations would be unstable if
|h · Im(λ0)| > 1.
Now let us define the concept of “stiffness.” Let λm be the eigenvalue with the least
negative real part. We say the system of ODEs is stiff if the ratio λM/λm is “large.”
The trouble with stiff systems is that the demand for numerical stability implies we must
take h to be smaller than 2

Re(λM)
−1. But, because Re(λm) is small, the system will change
very slowly and require many of those small time steps, which would tend to increase
round-off error. The most rapidly decaying part of the solution, due to λM, is imposing its
need for numerical stability on the rest of the system.
One way to repair the situation is to have an adaptive step size. Here we are getting
into the territory of the methods really used by software packages. For example, the
MATLAB command ode45 is a Runge–Kutta–Fehlberg method. It combines results from
one of the fourth-order Runge–Kutta methods and a fifth-order Runge–Kutta method to
estimate the local error. If it suspects the error might be large, it decreases the time step
size. For truly informative discussions of this and also the Bulirsch–Stoer method, please
see the references as follows.
Learn More About It
Further useful developments and discussion are found in An Introduction to Numerical
Methods and Analysis, by James F. Epperson, John Wiley & Sons, Inc. c⃝2002, and
Numerical Recipes: The Art of Scientific Computing, 3rd edition, by William H. Press et al.,
Cambridge University Press c⃝2007.

Numerical Methods I
709
8.7.7 Problems
1. For the ODE-IVP ˙y = t2 −y2, y(0) = 1
2.
(a) Use EM with step size h = 0.5 to get an approximate solution on the
interval [0, 1].
(b) Use EM with step size h = −0.5 to get an approximate solution on the
interval [−1, 0].
(c) Use your results from parts (a) and (b) to sketch a piecewise linear approxi-
mate solution on the interval −1 ≤t ≤1.
2. Consider the ODE-IVP t˙y +(t −1)y = t2, y(1) = −2.
(a) Use EM with step size h = 0.4 to get an approximate solution on the interval
[1, 5].
(b) Find the exact solution of the ODE-IVP.
(c) On the same set of axes, graph the exact solution and the piecewise lin-
ear approximation using your results from part (a). Discuss how good an
approximation EM gave.
In each of problems 3 and 4, you are given a desired function value for the solution of the
given ODE-IVP.
(a) Use EM, by hand with the given step size, h1, to estimate the desired function
value.
(b) Use the Runge–Kutta method of order four given in (8.71), by hand with the
given step size, h4, to estimate the desired function value.
(c) Find the exact solution of the ODE-IVP.
(d) Compare the function value for the exact solution of the ODE-IVP with your
approximate results from parts (a) and (b).
3. ˙y = y −t, y(1.0) = 3, desired function value y(1.6), and (a) h1 = 0.2, (b) h4 = 0.6.
4. ˙y = y −t, y(1) = 3, desired function value y(2.0), and (a) h1 = 0.2, (b) h4 = 0.5.
In each of problems 5–6, you are given a desired function value for the solution of the
given ODE-IVP.
(a) Use EM, by hand with the given step size, h1, to estimate the desired function
value.
(b) Use the Runge–Kutta method of order two given in (8.68), by hand with the
given step size, h2, to estimate the desired function value.
(c) Find the exact solution of the ODE-IVP.
(d) Compare the function value for the exact solution of the ODE-IVP with your
approximate results from parts (a) and (b).
5. ˙y = −2y + e−t, y(1.0) = 3, desired function value y(1.6), and (a) h1 = 0.2,
(b) h2 = 0.6.
6. ˙y = −2y + e−t, y(1.0) = 3, desired function value y(2.0), and (a) h1 = 0.2,
(b) h2 = 0.5.
In each of problems 7–9, you are given an ODE and a local error order, O(hn+1). Derive
TM of order n for this ODE. Do not substitute in a specific value of h.

710
Advanced Engineering Mathematics
7. ˙y = y2 −t, O(h3).
8. ˙y = t + t2y, O(h4).
9. ˙y = yecos(t−y), O(h3).
In each of problems 10 and 11, you are given a desired function value for the solution of
the given ODE-IVP.
(a) Derive TM of order two for this ODE. Do not substitute in a specific value of h.
(b) With step size h, now use TM to estimate the desired function value.
(c) Find the exact solution of the ODE-IVP.
(d) Compare the function value for the exact solution of the ODE-IVP with your
approximate result from part (b).
10. ˙y + y cos t = t, y(0) = 2, desired function value y( 1
3), and (i) h = 1
3, (ii) h = 1
6
11. ˙y = −2y + cos 3t, y(0) = 1, desired function value y(2.0), and h = 0.25.
12. Let α be an unspecified constant. Use EM with h = ±0.1 to approximate (a) y(1.2)
and (b) y(0.8), where y(t) solves the ODE-IVP ˙y = αy −t, y(1) = 0. Your final
conclusions should be in terms of the unspecified constant α. Work by hand and
show all work.
13. If we use the approximation ˙y(ti) ≈(2h)−1(yi+1 −yi−1) and substitute that into
the ODE ˙y + y = 0 then, we get the second-order difference equation yi+1 + 2hyi
−yi−1 = 0. Show that, no matter how small h > 0 is chosen, the difference equation
has a solution that does not behave, as t →∞, like the solutions of the original
ODE. This is an example of numerical instability.
14. If we use the usual central difference approximation of the second derivative in
the ODE ¨y + 4y = 0, we get the second-order difference equation yi+1 −2yi +
yi−1 + 4h2yi = 0. Show that for all h satisfying 0 < h < 1, the difference equation
has solutions that do behave like the solutions of the original ODE.
15. If we use EM for the undamped harmonic oscillator system
˙x =

0
1
−1
0

x,
explain why the solutions of the system of difference equations are spirals, not the
ellipses that are the solutions of the system of differential equations in R2.
16. If we use EM with h = 1/n for the especially simple ODE-IVP ˙y = f(t), y(0) = y0,
what would yn approximate? You may assume that f(t) is a continuous function.
[Hints: Note that y1 = y0 + hf(t0), y2 = y0 + hf(t0) + hf(t1). Also, think of how the
definite integral was defined.]
17. The modified EM is
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
y0 = y(t0)
yi+1 ≜yi + 1
2(k1 + k2), where
k1 ≜hf(ti, yi), k2 ≜hf(ti+1, yi + k1)
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.

Numerical Methods I
711
With h = 1/n, use the modified EM for the especially simple ODE-IVP ˙y = f(t),
y(0) = y0. What would yn approximate? You may assume that f(t) is a continuous
function. [Hint: Note that y1 = y0 + h
2 f(t0) + h
2 f(t1), etc.]
The modified EM is also known as a Runge–Kutta method of order two.
18. Find the local error estimate for the improved EM, also known as another Runge–
Kutta method of order two given in (8.68).
19. Explain why the local error is O(h4) for Runge–Kutta method of order three,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
y0 = y(t0)
yi+1 ≜yi + 1
6(k1 + 4k2 + k3), where
k1 ≜hf(ti, yi),
k2 ≜hf(ti+0.5, yi + 1
2k1), and
k3 ≜hf(ti+1, yi −k1 + 2k2)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.74)
20. Explain why the local error is O(h4) for a second Runge–Kutta method of order
three,
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
y0 = y(t0)
yi+1 ≜yi + 1
4(k1 + 3k2), where
k1 ≜hf(ti, yi), and
k2 ≜hf(ti + 1
3h, yi + 1
3k1)
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.75)
21. (Small project) Explain why the local error is O(h5) for the Runge–Kutta method
of order four given in (8.71).
22. (Small project) Explain why the local error is O(h5) for Gill’s method,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
y0 = y(t0)
yi+1 ≜yi + 1
6

k1 + 2

1 −

1
2

k2 + 2

1 +

1
2

k3 + k4

, where
k1 ≜hf(ti, yi),
k2≜hf(ti+0.5, yi + 1
2k1),
k3 ≜hf

ti+0.5, yi +

−1
2 +

1
2

k1 +

1 −

1
2

k2

, and
k4≜hf

ti+1, yi −

1
2 k2 +

1 +

1
2

k3

⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.76)

712
Advanced Engineering Mathematics
8.8 Approximate Solutions of Two Point BVPs
Let ′ = d
dx. Here is an example of a two-point boundary value problem:
⎧
⎪⎨
⎪⎩
y′′ + 1
xy′ + y = 0
y(1) = 0, y(3) = −1
⎫
⎪⎬
⎪⎭
.
(8.77)
Because the ODE is to be satisfied over the interval (1, 3), let the step size be
h = Length of the interval
N
= 2
N.
Denote xi = 1+ih and yi = y(xi), for i = 0, 1, . . . , N. The simplest approach to approximating
a solution of (8.77) is to replace the second derivative in the ODE by a central difference
approximation and replace the first derivative by a forward difference approximation:
y′′(xi) ≈h−2δ2yi = h−2 
yi+1 −2yi + yi−1

and y′(xi) ≈h−1yi = h−1 
yi+1 −yi

.
[In homework problems you will use the central difference approximation of the first
derivative, which has higher-order accuracy than the forward difference approximation.]
The approximation of the ODE becomes the replacement equations for this finite
difference method: For i = 1, . . . , N −1,
h−2 
yi+1 −2yi + yi−1

+ 1
xi
· h−1 
yi+1 −yi

+ yi = 0,
that is, after multiplying through by h2, yi+1 −2yi + yi−1 + h
xi

yi+1 −yi

+ h2yi = 0, that is,
yi−1 +

−2 −h
xi
+ h2

yi +

1 + h
xi

yi+1 = 0.
(8.78)
Our finite difference method does not have replacement equations for the ODE at the
endpoints x = x0 and x = xN because we do not have boundary conditions involving
x−1 or xN+1.
The boundary conditions become
y0 = 0
and
yN = −1.

Numerical Methods I
713
For i = 1, substitute the left-hand boundary condition and x1 = 1 + h in the replacement
equation, (8.78), to get
0 +

−2 −
h
1 + h + h2

yi +

1 +
h
1 + h

yi+1 = 0.
For i = 2, . . . , N −2, we keep the replacement equation, (8.78), as is.
For i = N −1, substitute the right-hand boundary condition and xN−1 = 3 −h in the
replacement equation, (8.78), to get
yN−2 +

−2 −
h
3 −h + h2

yN−1 +

1 +
h
3 −h

· (−1) = 0.
Define
y ≜
⎡
⎢⎢⎣
y1
...
yN−1
⎤
⎥⎥⎦.
After a little further algebraic manipulations, the replacement equations along with the
boundary conditions are
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−2
1
0
·
·
·
0
1
−2
1
·
0
1
−2
·
·
·
·
·
·
·
·
−2
1
0
·
1
−2
1
0
·
·
·
0
1
−2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
y
+
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−−h
1+h
h
1+h
0
·
·
·
0
0
−h
1+2h
h
1+2h
·
·
0
−h
1+3h
·
·
·
·
·
·
·
·
−h
3−3h
h
3−3h
0
0
−h
3−2h
h
3−2h
0
·
·
·
0
−h
3−h
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
y + h2 y =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
·
·
·
0
1 +
h
3−h
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.79)
System (8.79) can be written compactly as Ay = b. Notice that part of A is the famous
tri-diagonal matrix we saw in Problem 4.6.4.15.

714
Advanced Engineering Mathematics
Example 8.19
Find the approximate solution of ODE-BVP (8.77) when N = 5.
Method: For N = 5, h = 0.4, so (8.79) is
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−2 −0.4
1.4 + 0.16
1 + 0.4
1.4
0
0
1
−2 −0.4
1.8 + 0.16
1 + 0.4
1.8
0
0
1
−2 −0.4
2.2 + 0.16
1 + 0.4
2.2
0
0
1
−2 −0.4
2.6 + 0.16
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
y =
⎡
⎢⎢⎢⎢⎢⎣
0
0
0
3
2.6
⎤
⎥⎥⎥⎥⎥⎦
,
whose solution is
⎡
⎢⎢⎢⎢⎢⎣
y1
y2
y3
y4
⎤
⎥⎥⎥⎥⎥⎦
≈
⎡
⎢⎢⎢⎢⎢⎣
−0.5875491867
−0.9714146554
−1.158319393
−1.159650930
⎤
⎥⎥⎥⎥⎥⎦
. ⃝
Figure 8.6 shows the dashed graph of the approximate solution, including y0 = 0 and
y5 = −1: The dashed line segments “connect the dots” between the approximate solution
graph’s points (1, 0), (1.4, −0.5875491867), . . . , (3, −1). The exact solution is the solid curve.
By the way, it is possible to find the exact solution of ODE-BVP (8.77) using “Bessel func-
tions.” The ODE in (8.77) is known as Bessel’s equation of order zero and its solutions,
J0(x) and Y0(x), are known with as much accuracy as the trigonometric functions sin and
cos are known. The general solution of ODE y′′ + 1
xy′ + y = 0 is y(x) = c1J0(x) + c2Y0(x),
where c1, c2 are arbitrary constants. Substituting this into the boundary conditions gives
 J0(1)c1 + Y0(1)c2 =
0
J0(3)c1 + Y0(3)c2 = −1

,
1.5
–0.2
–0.4
–0.6
–0.8
–1.0
–1.2
y(t)
2.0
2.5
3.0 t
FIGURE 8.6
Numerical solution of an ODE-BVP.

Numerical Methods I
715
a system of two algebraic equations in unknowns c1, c2. Using tabulated values of J0(1),
Y0(1), J0(3), Y0(3), we arrive at the solution of (8.77):
y(x) ≈0.2662994102 J0(x) −2.469811754 Y0(x).
By the way, we chose N = 5 so that the matrix A would be large enough to demonstrate
the details of the implementation of the finite difference replacement equations (8.79).
The approximate solution using N = 5 differs from the exact solution by at most 17%,
which is not very bad. Ideally, we should get a numerical solution for first N = 5 and then
N = 10 and then compare at the common abscissas x = 1.4, 1.8, 2.2, 2.6, then N = 20, etc.,
and apply a stopping rule if the numerical solutions appear to change little if h is halved.
8.8.1 An ODE-BVP Eigenvalue Problem
As we will see in Chapter 9 and especially in Chapter 11, the ODE-BVP eigenvalue
problem

X′′ + λX = 0
X(0) = 0, X(π) = 0

(8.80)
plays an important role in solving partial differential equations. By an eigenvalue of ODE-
BVP (8.80), we mean a value of λ for which (8.80) has, on the interval [0, π], a solution X(x)
that is not identically zero, that is, a nontrivial solution. That solution X(x) is called a
corresponding eigenfunction. As we will see in Section 9.3, the eigenvalues of (8.80) are
λ = 12, 22, 32, . . . , that is, λn = n2 for n = 1, 2, 3, . . . , and the corresponding eigenfunctions
are Xn(x) = sin nx or nonzero multiples thereof.
Example 8.20
Find approximate eigenvalues and corresponding eigenfunctions of ODE-BVP (8.80)
using finite differences with N = 5.
Method: Let h = π
5 . For i = 0, 1, 2, . . . , 5, define xi = ih and Xi = X(xi). The boundary
conditions in (8.80) are replaced by X0 = X5 = 0, and the ODE is replaced by
h−2 
Xi+1 −2Xi + Xi−1

+ λXi = 0, i = 1, . . . , 4.
Define X = [ X1
X2
X3
X4 ]T. After using the replacement boundary conditions,
the system of ODE replacement equations can be written as the discretized eigenvalue
problem
AX + h2λ X = 0,
(8.81)
where the famous tridiagonal matrix is
A =
⎡
⎢⎢⎢⎢⎢⎣
−2
1
0
0
1
−2
1
0
0
1
−2
1
0
0
1
−2
⎤
⎥⎥⎥⎥⎥⎦
.
(8.82)

716
Advanced Engineering Mathematics
1.0
λ1≈0.9675312092750799, X1(x) ≈c1 sin x
λ2≈3.500560800333524, X2(x) ≈c2 sin 2x
λ3≈6.631557563900255, X3(x)≈c3 sin 3x
λ4≈9.164587154958701, X4(x)≈c4 sin 4x
0.8
0.6
0.4
0.2
–1.0
–0.5
0.5
1.0
0.5
1.0
2.0
3.0
x
1.5
2.5
0.5
1.0
2.0
3.0
x
1.5
2.5
–0.5
0.5
1.0
0.5
1.0
2.0
3.0
x
1.5
2.5
–0.5
0.5
1.0
0.5
1.0
2.0
3.0
x
1.5
2.5
–1.0
–1.0
FIGURE 8.7
Numerical eigenvalues and eigenfunctions.
So, finding an approximate eigenvalue of the ODE-BVP (8.80) is equivalent to find-
ing an eigenvalue of the linear algebra problem (8.81). We did that using the MATLAB
command [V, D] = eig(A). The results for the four approximate eigenvalues are
λ1 ≈0.9675312092750799,
X1(x) ≈c1 sin x,
λ2 ≈3.5005608003335249,
X2(x) ≈c2 sin 2x,
λ3 ≈6.631557563900255,
X3(x) ≈c3 sin 3x,
λ4 ≈9.164587154958701,
X4(x) ≈c4 sin 4x,
and the corresponding numerical eigenfunctions (dashed) curves) are compared pic-
torially to the exact eigenfunctions (solid curves) in Figure 8.7. While the numerical
eigenfunctions seem to be shrunken versions of the exact eigenfunctions, this is an arti-
fact of the coefficients ci. We used Mathematica to draw those pictures, and of course, we
could have used it to find the approximate eigenvalues, too. ⃝
8.8.2 Using IVP Numerical Methods to Solve BVPs
The finite difference method appears to work pretty well, especially if p(x), q(x), the coef-
ficients in the second-order ODE, y′′ + p(x)y′ + q(x) = f(x), and f(x), the right-hand side,
are not too wildly varying functions. Even if the ODE is nonlinear, say
y′′(x) = f

x, y(x), y′(x)

,

Numerical Methods I
717
we can still get finite difference replacement equations, for example,
h−2 
yi+1 −2yi + yi−1

= f

xi, yi, (2h)−1 
yi+1 −yi−1

,
using central difference approximations, but in general they will be a system of nonlinear
equations. As we saw in Section 8.2, solving a system of nonlinear equations can be slow.
Yet another general approach is to exploit modern software packages capabilities for
solving IVPs. For example, suppose we want to find an approximate numerical solution of

y′′ = f(x, y, y′)
y(a) = α, y(b) = β

.
Shooting methods numerically solve the IVP

y′′ = f(x, y, y′)
y(a) = α, y′(a) = z

to get “solution” y(x; z) and then use a numerical method to solve the scalar nonlinear
equation
y(b; z) = β
for z = z⋆. The numerical solution of the original ODE-BVP is then y(x; z⋆).
8.8.3 Finding Periodic Solutions of Linear Problems
Suppose a system of ODEs in Rn given by
˙x = A(t; λ)x + f(t)
(8.83)
is periodic with period T, that is, A(t + T; λ) ≡A(t; λ) and f(t + T) ≡f(t). We denote the
solutions of (8.83) by
x = x(t; λ, x0)
to indicate that the solution depends on both the parameter λ or vector of parameters
λ and the initial data x0. We studied such systems in Section 5.8, albeit often without
parameters λ.
If we want to find a value of λ for which (8.83) has a periodic solution, we can approach
the problem by using an IVP numerical method to find numerical approximations of the n
basic solutions
x(1)(t; λ) = x(t; λ, e(1)),
· · ·
, x(n)(t; λ) = x(t; λ, e(n))
of the corresponding homogeneous system ˙x = A(t; λ)x. After that, we assemble them into
a numerical approximation of the principal fundamental matrix
X(t; λ) =
 
x(1)(t; λ)  · · ·
 x(n)(t; λ)
!

718
Advanced Engineering Mathematics
and use the method of variation of constants to find the solutions of the original ODE
system (8.83). According to Theorem 5.26 in Section 5.8 (noncritical systems), if det

I −
X(T)

̸= 0, then (8.83) has exactly one periodic solution given by (5.115) in Section 5.8,
that is,
x(t) = X(t)
⎛
⎝
I −X(T)
−1X(T)
⎛
⎝
T
0
(X(s))−1 f(s) ds
⎞
⎠+
t
0
(X(s))−1 f(s) ds
⎞
⎠.
In the latter’s notation, we have suppressed the dependence on λ.
In the case when f(t) ≡0, that is, the system is homogeneous, it has a periodic solution
if, and only if, the Hill’s discriminant has
H(λ) ≜det

I −X(T; λ)

= 0.
This and related problems come up in studying electronic behavior of crystals. This is a
single equation to be solved for λ, so we can use the secant method to get an approximate
value(s) of λ for which the homogeneous system ˙x = A(t; λ)x has a periodic solution. In
effect we’re using a shooting method for a linear problem. Shooting methods are discussed
in the references found in the “Learn More About It” at the end of Section 8.2.
We (Turyn 1987) used this method to find an approximate double eigenvalue for the
ODE ¨y + π2
λ0 + λ1 cos(2πt) + 2 cos(4πt)

y = 0, that is, a value for the parameter vector
λ = [λ0
λ1]T such that the ODE has two linearly independent eigenfunctions that are
periodic with period T = 1.
8.8.4 Problems
In problems 1–5 we are given a two-point boundary value problem, where ′ =
d
dx, and a
step size, h.
(a) Find a system of replacement linear algebraic equations. For the second deriva-
tive, use the usual central difference approximation, and for a first derivative in
the ODE, use the central difference approximation y′(xi) ≈1
2h

yi+1 −yi−1

.
(b) Solve that system and graph the piecewise linear approximate solution.
(c) If possible, find the exact solution, graph it, and compare it to your approximate
solution in part (b).
1. ¨y + 2
√
2 cos

2πt
3

y = 0, y(0) = 0, y(1) = −1, and h = 0.25.
2. y′′ + xy′ −2y = sin(πx), y(0) = 0, y(1) = 3, and h = 0.25.
3. y′′ + y = x, y(0) = −1, y(2) = 5, and h = 0.5.
4. y′′ +2y = 0, y′(0) = 0, y(1) = 5, and h = 0.25. [For the boundary condition at x = 0,
use the central difference approximation y′(0) ≈1
2h

y(h) −y(−h)

.]
5. y′′ +2y = 0, y′(0) = 3, y(1) = 5, and h = 0.25. [For the boundary condition at x = 0,
use the central difference approximation y′(0) ≈1
2h

y(h) −y(−h)

.]
6. For the ODE-BVP (⋆) ¨y + ˙y = t, y(0) = −1, y(1) = 1, and h = 0.25, use the central
difference approximation of ¨y.

Numerical Methods I
719
(a) For the first derivative term, use the central difference approximation ˙y(ti) ≈
1
2h

yi+1 −yi−1

, find the system of replacement linear algebraic equations,
solve them, and graph the piecewise linear approximate solution.
(b) Find and graph the exact solution of (⋆) and compare it to the approximate
solution from part (a).
(c) For the first derivative term, use the forward difference approximation ˙y(ti) ≈
1
h

yi+1 −yi

, find the system of replacement linear algebraic equations for
(⋆), solve them, graph the piecewise linear approximate solution, and com-
pare it to the approximate solution from part (a) and the exact solution from
part (b).
7. For the two-point boundary value problem, y′′ = −x, y(0)−y′(0) = 1, and y(1) = 0,
where ′ = d
dx,
(a) Find a system of replacement linear algebraic equations using step size, h =
0.25. For the second derivative, use the usual central difference approxima-
tion. For the first derivative in the boundary condition, use the backward
difference approximation, y′(0) ≈h−1(y(0) −y(−h)) ≈h−1(y0 −y−1). Because
this equation involves y−1, also include the replacement equation for the ODE
at x0, that is, h−2(y1 −2y0 + y−1) = −x0 = 0.
(b) Solve that system and graph the piecewise linear approximate solution.
(c) If possible, find the exact solution, graph it, and compare it to your approxi-
mate solution in part (b).
8. Find three approximate eigenvalues and corresponding eigenfunctions of ODE-
BVP r2R′′ + rR′ + λR = 0, R(1) = R(3) = 0, where ′ =
d
dr, using finite
differences with N = 4. Compare the approximate eigenvalues and corresponding
eigenfunctions to the exact eigenvalues and eigenfunctions.
9. Find three approximate eigenvalues and corresponding eigenfunctions of ODE-
BVP R′′ +
2
r+1R′ + λ
1
(r+1)4 R = 0, R(0) = R(1) = 0, where ′ =
d
dr, using finite
differences with N = 4.
10. If we use finite differences with h =
b−a
N
to study an eigenvalue problem y′′ +
p(x)y′ + (q(x) + λ)y = 0, y(a) = y(b) = 0, how many approximate eigenvalues
should we expect to find?
8.9 Splines
Splines are functions that are used to (1) approximate curves and surfaces and (2) approxi-
mate solutions of differential equations. What is particularly nice about the approximation
is that it has a certain degree of smoothness. For example, “cubic” splines will be twice
continuously differentiable, and this makes them good for approximating a solution of
a second-order ODE-BVP. So, splines are an important part of the modern toolbox of
mathematical modeling of physical problems.
One overall characteristic of spline methods is that they can express an approximation
of a function using a linear combination of basis functions. As such, the coefficients in the
linear combination are a highly “compressed” representation of the function.

720
Advanced Engineering Mathematics
The simplest spline is the piecewise linear approximation, or “connecting the dots” on
the graph of a function. This is also effectively what we used to approximate a definite
integral using the trapezoidal rule illustrated in Figure 8.4.
Throughout this section we will assume that a = x0 < x1 < · · · < xN = b is a partition of
the interval [a, b].
Definition 8.6
(a) A function g(x) is piecewise polynomial on an interval [a, b] if there is a partition of
[a, b] and, on each subinterval [xj−1, xj], g(x) is a polynomial.
(b) Suppose k ≥2. A spline of order k on an interval [a, b] is a function g(x) that is
(i) k −2 times continuously differentiable on [a, b], (ii) piecewise polynomial, and (iii) has
each polynomial piece of degree less than or equal to k −1.
Note that a spline of order 2 is only required to be continuous on [a, b] and piecewise linear.
Suppose we have data points (xj, yj) = (xj, f(xj)) on the graph of y = f(x). Define
LN(x) ≜
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
y0 + m1(x −x0),
x0 ≤x ≤x1
y1 + m2(x −x1),
x1 ≤x ≤x2
y2 + m3(x −x2),
x2 ≤x ≤x3
...
yN−1 + mN(x −xN−1),
xN−1 ≤x ≤xN
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
(8.84)
where mj =
yj−yj−1
h
, for i = 1, . . . , N. The piecewise linear function LN(x) is illustrated in
Figure 8.8.
a = x0
xj–1
xj
xj +1
xN= b
FIGURE 8.8
Approximation of a function using uniform tent–spline functions.

Numerical Methods I
721
1.0
y(t)
0.8
0.6
0.4
0.2
t
–3
–2
–1
1
2
3
FIGURE 8.9
Tent–spline function.
In this very brief introduction to the subject, we will emphasize the uniform case where
the abscissas are equally spaced, that is, there is a step size h with xj −xj−1 = h for j =
1, . . . , N.
We will express the piecewise linear approximation of a function as a linear combination
of basis functions. This will give a simple introduction to more complicated, powerful
spline methods involving higher-order polynomials. Define a tent function φ(t) by
φ(x) ≜
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
(x + 1),
−1 ≤x ≤0
(1 −x),
0 ≤x ≤1
0,
|x| ≥1
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
b−1(x + 1),
−1 ≤x ≤0
b0(x),
0 ≤x ≤1
0,
|x| ≥1
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(8.85)
where b−1(x) ≜x, b0(x) ≜1 −x. Its graph is shown in Figure 8.9, and has the interesting
property that
b−1(x) + b0(x) ≡1, for 0 < x ≤1.
After the horizontal translation x →(x −xj), followed by the dilation x →x
h, we get the
uniform tent basis functions
Tj(x) ≜
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
 x−xj
h
+ 1

,
xj−1 ≤x ≤xj

1 −
x−xj
h

,
xj ≤x ≤xj+1
0,
|x −xj| ≥h
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
= φ
x −xj
h

,
(8.86)
for j = 0, . . . , N + 1, where we define x−1 = a −h and xN+1 = b + h.

722
Advanced Engineering Mathematics
Because (i) Tj(xj) = 1 and (ii) Ti(xj) = 0 for i ̸= j, we can approximate f(x) on the interval
[a, b] using
f(x) ≈fN(x) ≜
N

j=0
yjTj(x).
(8.87)
In Problem 8.9.6.6 you will explain why (8.87) is actually the same as the piecewise linear
approximation in (8.84).
Definition 8.7
A function g(x) has support interval I, or support for short, if (a) g(x) = 0 for all x not in I
and (b) I is the smallest such closed interval satisfying (a).
The function Tj(x) has support [xj−1, xj+1]. So, when evaluating fN(x) at an x in the inter-
val [a, b], at most two of the functions Tj(x) will be nonzero in the sum. For example, if
xj−1 ≤x ≤xj, then
fN(x) = yj−1Tj−1(x) + yjTj(x).
The function φ(x) is continuous on the real line; in particular, φ(0−) = φ(0+). So, each of
the basis functions Tj(x) is continuous on the real line, and thus, fN(x) is continuous.
8.9.1 Cubic B-Splines
Define a function by
ψ(x) ≜1
6
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(x + 2)3,
−2 ≤x ≤−1
−3(x + 1)3 + 3(x + 1)2 + 3(x + 1) + 1,
−1 ≤x ≤0
−3(1 −x)3 + 3(1 −x)2 + 3(1 −x) + 1,
0 ≤x ≤1
(2 −x)3,
1 ≤x ≤2
0,
|x| ≥2
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
b−2(x + 2),
−2 ≤x ≤−1
b−1(x + 1),
−1 ≤x ≤0
b0(x),
0 ≤x ≤1
b1(x −1),
1 ≤x ≤2
0,
|x| ≥2
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
(8.88)

Numerical Methods I
723
y(t)
0.6
0.5
0.4
0.3
0.2
0.1
–3
–2
–1
1
2
3
FIGURE 8.10
Cubic B-spline.
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
b−2(x) ≜1
6 x3,
b−1(x) ≜1
6

−3x3 + 3x2 + 3x + 1

,
b0(x) ≜1
6

3x3 −6x2 + 4

,
b1(x) ≜1
6

−x3 + 3x2 −3x + 1

= 1
6(1 −x)3.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(8.89)
The graph of ψ(x) is shown in Figure 8.10. It is called a cubic B-spline or a blending
function and has the interesting property that
ψ(x −2) + ψ(x −1) + ψ(x) + ψ(x + 1) = b−2(x) + b−1(x) + b0(x) + b1(x) ≡1, for 0 < x ≤1.
After the horizontal translation x →(x −xj), followed by the dilation x →x
h, we get the
cubic spline uniform basis functions
Cj(x) ≜
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
b−2
 x−xj
h
+ 2

,
xj−2 ≤x ≤xj−1
b−1
 x−xj
h
+ 1

,
xj−1 ≤x ≤xj
b0
 x−xj
h

,
xj ≤x ≤xj+1
b1
 x−xj
h
−1

,
xj+1 ≤x ≤xj+2
0,
|x −xj| ≥2h
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
= ψ
x −xj
h

,
(8.90)
for j = −1, 0, . . . , N + 1, where we define x−1 = a −h and xN+1 = b + h.

724
Advanced Engineering Mathematics
In Problem 8.9.6.8 you will check that the function ψ(x) is twice continuously differen-
tiable on the real line. In particular, you will calculate that
ψ(k+) = ψ(k−), ψ′(k+) = ψ′(k−), ψ′′(k+) = ψ′′(k−), for k = −2, −1, −, 0, 1, 2.
(8.91)
It follows that each of the basis functions Cj(x) is twice continuously differentiable on the
real line, as is required of splines of order four.
We will see how to approximate a given function f(x) using a linear combination of the
cubic B-splines:
f(x) ≈fN(x) ≜
N+1

j=−1
zjCj(x).
(8.92)
We include the functions C−1(x) because it is twice continuously differentiable at x =
x0 and contributes to the sum information about f(x) and its first two derivatives there.
Similarly we include CN+1(x) in order to use as much information as possible about f(x)
and its first two derivatives at x = xN.
To solve for the zj, it helps to calculate that
ψ(−2) = 0, ψ(−1) = 1
6, ψ(0) = 2
3, ψ(1) = 1
6, ψ(2) = 0,
ψ′(−2) = 0, ψ′(−1) = 1
2, ψ′(0) = 0, ψ′(1) = −1
2, ψ′(2) = 0,
ψ′′(−2) = 0, ψ′′(−1) = 1, ψ′′(0) = −2, ψ′′(1) = 1, ψ′′(2) = 0.
By the chain rule, it follows that
Cj(xj−2) = 0, Cj(xj−1) = 1
6, Cj(xj) = 2
3, Cj(xj+1) = 1
6, Cj(xj+2) = 0,
Cj′(xj−2) = 0, Cj′(xj−1) = 1
2h, Cj′(xj) = 0, Cj′(xj+1) = −1
2h, Cj′(xj+2) = 0,
Cj′′(xj−2) = 0, Cj′′(xj−1) = 1
h2 , Cj′′(xj) = −2
h2 , Cj′′(xj+1) = 1
h2 , Cj′′(xj+2) = 0.
(8.93)
In particular, at x = xj, at most three of the terms in (8.92) are not zero:
fN(xj) ≜
N+1

j=−1
zjCj(xj) = 1
6 zj−1 + 2
3 zj + 1
6 zj+1.
(8.94)
As an aside, note that this can be written in terms of the functions b−2(x), . . . , b1(x):
fN(xj) = b−2(0) zj−2 + b−1(0) zj−1 + b0(0) zj + b1(0) zj+1 + b1(1) zj+2.
Similarly to (8.93) we find
f ′
N(xj) ≜
N+1

i=−1
ziCi′(xj) = zj−1C′
j−1(xj) + zj+1C′
j+1(xj) = 1
2h · (−zj−1 + zj+1),
(8.95)

Numerical Methods I
725
and
f ′′
N(xj) ≜
N+1

i=−1
ziCi′(xj) = zj−1C′′
j−1(xj) + zjC′′
j (xj) + zj+1C′′
j+1(xj) = 1
h2 · (zj−1 −2zj + zj+1).
(8.96)
It follows that
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
f ′′
N(x0)
f ′′
N(x1)
f ′′
N(x2)
...
f ′′
N(xN−1)
f ′′
N(xN)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= 1
h2
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
−2
1
0
·
·
·
0
0
1
−2
1
·
·
0
1
−2
·
·
·
·
·
·
·
·
·
·
·
·
1
−2
1
0
0
·
·
·
0
1
−2
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
z−1
z0
z1
...
zN−1
zN
zN+1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
and
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
f ′
N(x0)
f ′
N(x1)
f ′
N(x2)
...
f ′
N(xN−1)
f ′
N(xN)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= 1
2h
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
0
1
0
·
·
·
0
0
−1
0
1
·
·
0
−1
0
·
·
·
·
·
·
·
·
·
·
·
·
−1
0
1
0
0
·
·
·
0
−1
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
z−1
z0
z1
...
zN−1
zN
zN+1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Example 8.21
Find the approximate solution of ODE-BVP (8.77) in Section 8.8, that is,
⎧
⎪⎨
⎪⎩
y′′ + 1
xy′ + y = 0
y(1) = 0, y(3) = −1
⎫
⎪⎬
⎪⎭
,
using cubic B-splines for N = 3.
Method: For N = 3, h = 3−1
3 , and our approximate solution is
yN(x) ≜
4

j=−1
zjCj(x),
where we will solve for the constants z−1, z0, . . . , z4. Using (8.94), the boundary condi-
tions require
0 = y(1) = yN(x0) = 1
6

z−1 + 4z0 + z1

(8.97)

726
Advanced Engineering Mathematics
and
−1 = y(3) = yN(x3) = 1
6 (z2 + 4z3 + z4) .
(8.98)
Substituting h = 2
3 and N = 3 into (8.94)–(8.96), the replacement equations for the
ODE are, for j = 0, 1, 2, 3,
9
4

zj−1 −2zj + zj+1

+ 1
xj
· 3
4

−zj−1 + zj+1

+ 1
6

zj−1 + 4zj + zj+1

= 0.
(8.99)
We include replacement equations for the ODE at the endpoints x = x0 and x = xN
because, unlike the finite difference methods we used in Example 8.19 in Section 8.8, the
spline functions are twice continuously differentiable at the endpoints.
Altogether, (8.97)–(8.99) give us the system
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
6
4
6
1
6
0
0
0
0
0
0
1
6
4
6
1
6
9
4 −9
4 + 1
6
−18
4 + 4
6
9
4 + 9
4 + 1
6
0
0
0
0
9
4 −9
4 · 3
5 + 1
6
−18
4 + 4
6
9
4 + 9
4 · 3
5 + 1
6
0
0
0
0
9
4 −9
4 · 3
7 + 1
6
−18
4 + 4
6
9
4 + 9
4 · 3
7 + 1
6
0
0
0
0
9
4 −9
4 · 3
9 + 1
6
−18
4 + 4
6
9
4 + 9
4 · 3
9 + 1
6
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
×
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
z−1
z0
z1
z2
z3
z4
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
−1
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.100)
The solution of (8.100) is
z−1 ≈1.71275633, z0 ≈−0.155705051, z1 ≈−1.08993554, z2 ≈−1.35065140
z3 ≈−1.05687409, z4 ≈−0.421852245. ⃝
While we could use a calculator to solve (8.100), it is convenient to use Mathematica or
MATLAB to enter the correct 36 entries in the matrix of coefficients.
The solid curve in Figure 8.11 is the exact solution, as discussed in Example 8.19 in
Section 8.8. The thick dashed curve is the solution using cubic B-splines. The fact that
the latter solution is twice continuously differentiable is apparent as well as the excellent
agreement with the exact solution.
In Example 8.21 we substituted the approximate solution into the ODE at a finite number
of points xj. This is called a collocation method.

Numerical Methods I
727
–0.2
1.5
2.0
2.5
3.0
t
y(t)
–0.4
–0.6
–0.8
–1.0
–1.2
FIGURE 8.11
Solution of Example 8.21 using cubic B-splines.
8.9.2 Nonuniform Splines
The partition a = x0 < x1 < · · · < xN = b need not be equally spaced. Define hj = xj −xj−1
for i = 1, . . . , N and the usual data points (xj, yj) = (xj, f(xj)) on the graph of y = f(x). We
get the tent basis functions
Tj(x) ≜T(x −jh) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
*
x −xj−1
xj −xj−1
+
,
xj−1 ≤x ≤xj
*
1 −
x −xj
xj+1 −xj
+
,
xj ≤x ≤xj+1
0,
|x −xj| ≥h
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(8.101)
The uniform tent basis splines in (8.86) are a special case of (8.101), as you will verify in
Problem 8.9.6.10.
We can still approximate f(x) by a piecewise linear function
f(x) ≈fN(x) ≜
N

j=0
yjTj(x),
(8.102)
as illustrated in Figure 8.12. As in the uniform case, when evaluating fN(x) at an x in the
interval [a, b], at most two of the functions Tj(x) will be nonzero in the sum. For example,
if xj−1 ≤x ≤xj, then
fN(x) = yj−1Tj−1(x) + yjTj(x).
In Problem 8.9.6.9, you will explain why (8.102) is actually the same as the piecewise
linear approximation in (8.84), even when the partition is not equally spaced.
Nonuniform tent–spline functions are illustrated in Figure 8.13.

728
Advanced Engineering Mathematics
y= f(x)
fN(x)=Σ yjTj(x)
j=0
N
a = x0
xj–1
xj–2
xj
xj+1
xN= b
FIGURE 8.12
Approximation of a function using nonuniform tent–spline functions.
1
y=Tj–1(x)
y=Tj(x)
0
xj–2
xj–1
xj
xj + 1
FIGURE 8.13
Nonuniform tent–spline functions Tj−1(x) and Tj(x).
Unfortunately, in the nonuniform case we cannot write Tj(x) as a dilation of a translate
of a simpler function, as we did in (8.86).
Indeed, the definition of higher-order splines, for example, quadratic and cubic
B-splines, is much more complicated in the nonuniform case. One standard method is to
use the Cox–de Boor recursive definition (de Boor 2001, p. 90). The spline basis functions
Nj,k(x) of order k > 1 satisfy
Nj,k(x) ≜ωj,k(x)Nj,k−1(x) +

1 −ωj+1,k(x)

Nj+1,k−1(x), for j = 0, . . . , N −2,
(8.103)
where
ωj,k(x) ≜
x −xj
xj+k−1 −xj
,
(8.104)
and
Nj,1(x) ≜
 1,
xj ≤x ≤xj+1
0,
all other x

.

Numerical Methods I
729
In Problem 8.9.6.7 you will explain why the recursive definition (8.103) implies that for
k = 2,
Nj,2(x) ≜
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
x −xj
xj+1 −xj
,
xj ≤x ≤xj+1
xj+2 −x
xj+2 −xj+1
,
xj+1 ≤x ≤xj+2
0,
all other x
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
= Tj+1(x)
in terms of the tent basis function we defined earlier in (8.101).
8.9.3 Spline Approximation of a Curve in Rn
Suppose that in Rn we have a parametrized curve
C : r = r(t), 0 ≤t ≤m.
Note that if the parameter interval is not [0, m], we can change the parametrization to make
it so.
The points P0 ≜r(0), P0 ≜r(1), . . . , Pm ≜r(m) lie on the curve C. We will find a paramet-
ric curve that interpolates P0 and Pm, the endpoints of the curve, but not necessarily any
of the points in between.
Define “fantom” points P−1 = 2P0 −P1 and Pm+1 = 2Pm −Pm−1.
The four points P−1, P0, P1, P2 determine a B-spline curve parametrized by
R(0)(t) ≜b1(t −0)P−1 + b0(t −0)P0 + b−1(t −0)P1 + b−2(t −0)P2, 0 ≤t ≤1,
in terms of the bi(t) functions defined in (8.89). Because of the way the fantom point P−1
was chosen, the spline curve R(0)(t) agrees with the original curve at t = 0, that is, R(0)(0) =
P0. In addition, the spline curve is (a) tangent to the line segment from P0 to P1, that is,
˙R(0)(0) = P1 −P0, at P0, and (b) tangent to the line segment from Pm−1 to Pm, that is,
˙R(0)(1) = Pm −Pm−1, at P0.
Next, the four points P0, . . . , P3 determine a spline curve parametrized by
R(1)(t) ≜b1(t −1)P0 + b0(t −1)P1 + b−1(t −1)P2 + b−2(t −1)P3, 1 ≤t ≤2.
It turns out that the “composite” curve
R(t) =
 R(0)(t),
0 ≤t ≤1
R(1)(t),
1 ≤t ≤2

is a vector-valued spline of order four on the interval [0, 2], including being twice
continuously differentiable at the join point 1
6

P1 + 4P2 + P3

at t = 1.

730
Advanced Engineering Mathematics
Continuing in this way, the four points Pi−1, . . . , Pi+2 determine a spline curve
parametrized by
R(i)(t) ≜b1(t −i)Pi−1 + b0(t −i)Pi + b−1(t −i)Pi+1 + b−2(t −i)Pi+2, i ≤t ≤i + 1.
The composite curve parametrized by
R(t) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
R(0)(t),
0 ≤t ≤1
...
...
R(i)(t),
i ≤t ≤i + 1
...
...
R(m−1)(t),
m −1 ≤t ≤m
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
is a vector-valued spline of order four on the interval [m, 0]. It also interpolates the
endpoints using R(0)(0) = P0 = r(0) and R(m−1)(m) = Pm = r(m).
Example 8.22
For “control points” (0, 1), (0.9, 0.9), (1.2, 0.1), (0, 0), (1.2, −0.1), (0.9, −0.9), and (0, −1),
find a composite curve that interpolates the first and last points in the list.
Method: Let P0, P1, . . . , P6 denote the control points. Construct the fantom points P−1 =
2P0 −P1 = (−0.9, 1.1) and P7 = 2P6 −P5 = (−0.9, −1.1) and then construct the com-
posite curve’s pieces R(0)(t), . . . , R(5)(t). The “control polygon” consists of the dashed
line segments connecting the solid dots at P−1, P0, . . . , P7. The results are shown in
Figure 8.14. ⃝
1.0
x
y
0.5
0.5
–0.5
–0.5
–1.0
–1.0
1.0
FIGURE 8.14
Composite B-spline curve.

Numerical Methods I
731
8.9.4 Surface Splines
A parametric surface may be approximated using splines. The generalization of a control
polygon is then a “control polyhedron.” Given a rectangular array of control points
P00
P10
P20
·
·
·
Pm0
P01
P11
P21
·
·
·
Pm1
·
·
·
·
·
·
·
·
·
P0n
P1n
P2n
·
·
·
Pmn,
we can parametrize the (i, j)th bicubic B-spline surface S(i,j) by
R(i,j)(u, v) =
3

k=0
3

ℓ=0
b1−k(u −i)b1−ℓ(v −j)Pi+k−1,j+ℓ−1, i ≤u ≤i + 1, j ≤v ≤j + 1,
where 1 ≤i ≤m −1, 1 ≤j ≤n −2. The four cubic polynomials b1, . . . , b−2 were defined
in (8.89).
The composite surface is the union
m−2
0
i=1
n−2
0
j=1
S(i,j).
Similar to the work for composite cubic B-spline curves, we can introduce fantom ver-
tices, bordering the rectangular array on all sides, so as to interpolate the boundary of the
composite surface.
8.9.5 Triangular Surface Patches
The method of “finite elements” often uses an approximation of a piece of a surface by
a triangular region. To understand how this relates to splines, it helps to have more
background information.
Let points P1, P2, P3 be the three vertices of a triangle T in the xy–plane. Suppose the
area of the triangle is A. If P is a point inside T , define
Ai ≜area of triangle whose vertices are
P
and the
Pj
for
j ̸= i.
and areal coordinates
ai ≜Ai
A .
For example, a1 is the area of the subtriangle, whose vertices are P, P2, P3, divided by
the area of the whole triangle. We have a1 + a2 + a3 = 1 and ai ≥0 for i = 1, 2, 3. Areal

732
Advanced Engineering Mathematics
P
P2
P1
P3
FIGURE 8.15
Triangle T and areal coordinates.
coordinates are the n = 3 version of barycentric coordinates. Note that for every point P
in T , there is a unique triple of values (a1, a2, a3) for which
P = a1P1 + a2P2 + a3P3.
In looking at Figure 8.15, keep in mind that the triangle T , including the point P, lies
in the xy-plane, despite the optical illusion that suggests a tetrahedron rather than the
decomposition of T into three subtriangles.
Denote a = (a1, a2, a3).
Denote a multi-index by i = (i1, i2, i3) where i1, i2, i3 are non negative integers and i1 +
i2 + i3 = 1. By convention, 0! ≜1, and we define
i! ≜i1! i2! i3!
and
3
i

≜3!
i! =
3!
i1! i2! i3!
and cubic basis polynomials by
pi(a) ≜
3
i

ai1
1 ai2
2 ai3
3 .
There is a specific way to choose control points Qi, all of which are on the boundary of
the triangle except for Q111, which is at the centroid of the triangle. Additionally, Q300 ≜
P1, Q030 ≜P2, and Q003 ≜P3.
If z = f(x, y) is a surface whose domain is T , then the surface is interpolated by
z = g(a) ≜

i
f(Qi)pi(a).
Learn More About It
Handbook on Splines for the User, by Eugene V. Shikin and Alexander I. Plis, CRC Press,
Inc. c⃝1995, gave the results we quoted for a composite B-spline curves and surfaces.
Approximation Techniques for Engineers, by Louis Komzsik, CRC Press, Inc. c⃝2007, gave
the results we quoted for triangular surface patches.

Numerical Methods I
733
8.9.6 Problems
In problems 1–5 we are given a two-point boundary value problem, where ′ =
d
dx, and a
step size, h.
(a) Use uniform cubic B-splines to find a system of replacement linear algebraic
equations.
(b) Solve that system and graph the approximate solution.
(c) If possible, find the exact solution, graph it, and compare it to your approximate
solution in part (b).
1. ¨y + 2
√
2 cos

2πt
3

y = 0, y(0) = 0, y(1) = −1, and h = 0.5.
2. y′′ + xy′ −2y = sin(πx), y(0) = 0, y(1) = 3, and h = 0.5.
3. y′′ + y = x, y(0) = −1, y(2) = 3, and h = 0.5.
4. y′′ + 2y = 0, y′(0) = 0, y(1) = 5, and h = 0.25.
5. y′′ = −x, y(0) −y′(0) = 1, y(1) = 0, and h = 0.25.
6. Explain why (8.87) is actually the same as the piecewise linear approximation in
(8.84).
7. Explain why the recursive definition (8.103) implies that for k = 2, Nj,2(x) =
Tj+1(x) in terms of the tent basis function we defined earlier in (8.101).
8. Check that the function ψ(x) given in (8.88) is twice continuously differentiable
on the real line by evaluating the left- and right-hand limits of ψ(x), ψ′(x), and
ψ′′(x) at x = −2, −1, 0, 1, and 2. What is the support of ψ(x)?
9. Explain why (8.102) is actually the same as the piecewise linear approximation
in (8.84) even when the partition is not equally spaced.
10. Explain why the uniform tent basis splines in (8.86) are a special case of (8.101).
In problems 11 and 12 construct a composite B-spline curve for the given control points.
11. (−0.7, 0.95), (−0.5, 1), (0, 1), (1.3, 1), (0.6, 0.8), (0.1, 0.33), (0, −0.6).
12. (0.9, 0.5), (0.5, 0.9), (0, 0.5), (0.5, 0.3), (0.9, 0.7), (0.85, 0.3), (0.8, 0).
Key Terms
absolute error: after (8.33) in Section 8.4
Adams, Adams-Bashforth method: (8.72) in Section 8.7
Adams-Moulton: after (8.72) in Section 8.7
adaptive step size: end of Section 8.7
Aitken’s δ2 method: (8.16) in Section 8.1
almost triangular matrix: Definition 8.4 in Section 8.5
areal coordinates: end of Section 8.9
B-spline curve: before Example 8.22 in Section 8.9
Banach Contraction Mapping Theorem: after Theorem 8.5 in Section 8.2
Banach spaces: after Theorem 8.5 in Section 8.2

734
Advanced Engineering Mathematics
barycentric coordinates: end of Section 8.9
basis polynomials: end of Section 8.9
Bessel’s equation of order zero: after Example 8.19 in Section 8.8
big O: Definition 8.5 in Section 8.6
bisection method: before Example 8.2 in Section 8.1
blending function: (8.89) in Section 8.9
Boole’s rule: (8.50) in Section 8.6
central difference operator: (8.27) in Section 8.3
collocation method: after Example 8.21 in Section 8.9
column diagonally dominant: Definition 8.3(b) in Section 8.2
convergent: beginning of Section 8.4
convergence acceleration: before (8.16) in Section 8.1
Cox-de Boor recursive definition: (8.103) in Section 8.9
cubic B-spline: (8.89) in Section 8.9
cubic spline uniform basis functions: (8.90) in Section 8.9
deflation of polynomials: before (8.46) in Section 8.5
dilation: (8.86) in Section 8.9
direct methods: before Example 8.11 in Section 8.4
discretized eigenvalue problem: (8.81) in Section 8.8
dominant eigenvalue: before (8.43) in Section 8.5
double eigenvalue: end of Section 8.8
eigenfunction: after (8.80) in Section 8.8
eigenvalue of ODE-BVP: after (8.80) in Section 8.8
Euler’s Method: (8.63) in Section 8.7
extended sign function: before Lemma 8.1 in Section 8.5
false position: (8.11) in Section 8.1
finite difference method: (8.78) in Section 8.8
fixed point problem iteration: (8.14) in Section 8.1; after Theorem 8.5 in Section 8.2
fixed point problem: before (8.14) in Section 8.1
forward difference operator: (8.48) in Section 8.6
Fréchet derivative: after Theorem 8.5 in Section 8.2
Gauss–Seidel iteration: (8.40) in Section 8.4
Gill’s Method: Problem 8.7.7.23
global error: before Theorem 8.13 in Section 8.7
Hilbert matrix: Problem 8.4.3.2
horizontal translation: (8.86) in Section 8.9
Householder matrix: before Corollary 8.1 in Section 8.5
implicit partial pivoting: after Example 8.13 in Section 8.4
improved tangent method: Example 8.16 in Section 8.7
interpolates: before Example 8.22 in Section 8.9
inverse power method: after Example 8.14 in Section 8.5
iterates: before Example 8.1 in Section 8.1; after (8.19) in Section 8.2
Jacobi iteration: (8.39) in Section 8.4
join point: before Example 8.22 in Section 8.9
linear convergence: after (8.13) in Section 8.1
local error: before Theorem 8.13 in Section 8.7
lower Hessenberg form: after Definition 8.4 in Section 8.5
matrix norm induced: Definition 8.2 in Section 8.4
method of successive approximations: (8.14) in Section 8.1

Numerical Methods I
735
midpoint approximation: (8.25) in Section 8.3
midpoint method: Example 8.16 in Section 8.7
modified Euler’s method: Problem 8.7.7.17
modified (or corrected) Newton’s Method: after Example 8.3 in Section 8.1
multi-index: end of Section 8.9
multistep method: before (8.72) in Section 8.7
Newton-Cotes formula of closed type: (8.29) in Section 8.3
Newton’s method, Newton-Raphson method: (8.2) in Section 8.1
Newton’s method in Rn: (8.18) in Section 8.2
nontrivial solution: after (8.80) in Section 8.8
norm: Theorem 8.3 in Section 8.2
Numerical Analysis: before (8.31) in Section 8.3
numerical drift: end of Section 8.3
numerical methods: Section 8.1 beginning
order of a multistep method: before (8.72) in Section 8.7
order of a spline: Definition 8.6(b) in Section 8.9
order of the zero: after Example 8.3 in Section 8.1
over-relaxation methods: before (8.41) in Section 8.4
partial pivoting: before Example 8.12 in Section 8.4
perfectly conditioned: after (8.36) in Section 8.4
piecewise polynomial: Definition 8.6(a) in Section 8.9
power method: (8.43) in Section 8.5
predictor-corrector method: after (8.72) in Section 8.7
QR Algorithm: end of Section 8.5
quadratic convergence: (8.6) in Section 8.1; after (8.20) in Section 8.2
quadrature rule: after (8.28) in Section 8.3
quasi-triangular matrix: after Definition 8.4 in Section 8.5
regula falsi: (8.11) in Section 8.1
relative error: after (8.33) in Section 8.4
relaxation method: before (8.40) in Section 8.4
relaxation parameter: before (8.41) in Section 8.4
replacement equations: (8.78) in Section 8.8
round-off error: before (8.12) in Section 8.1
row diagonally dominant: Definition 8.3(a) in Section 8.4
Runge–Kutta method of order two: Example 8.16 in Section 8.7; Problem 8.7.7.17; Problem
8.7.7.18
Runge–Kutta method of order three: Problem 8.7.7.19; Problem 8.7.7.20
Runge–Kutta method of order four: (8.71) in Section 8.7, Problem 8.7.7.21
secant method: (8.11) in Section 8.1, (8.22) in Section 8.2
shift of A: after Example 8.14 in Section 8.5
shooting methods: before (8.83) in Section 8.8
similarity transformation: before (8.46) in Section 8.5
Simpson’s Rule: (8.28) in Section 8.3
sparseness: before (8.39) in Section 8.4
spectral radius: after (8.39) in Section 8.4
spline: Definition 8.6(b) in Section 8.9
stable: beginning of Section 8.4
standard condition number: (8.32) in Section 8.4
step size: before (8.25) in Section 8.3

736
Advanced Engineering Mathematics
Steffensen’s method: after Example 8.5 in Section 8.1
stiff: end of Section 8.7
stopping rule: (8.7) in Section 8.1
super-linear convergence: (8.13) in Section 8.1
support, support interval: Definition 8.7 in Section 8.9
surface splines: near end of Section 8.9
Taylor’s formula method: (8.73) in Section 8.7
tent basis functions: (8.101) in Section 8.9
tent function: (8.85) in Section 8.9
trapezoidal rule: (8.27) in Section 8.3
triangular surface patches: end of Section 8.9
tri-diagonal matrix: after Definition 8.4 in Section 8.5
uniform spline: (8.84) in Section 8.9
uniform tent basis functions: (8.86) in Section 8.9
upper Hessenberg form: Definition 8.4 in Section 8.5
MATLAB R⃝Commands
A\f(xk): before Example 8.7 in Section 8.2
fSimpExtrap: after Example 8.10 in Section 8.3
MATLAB “function” f821: after Example 8.10 in Section 8.3
ode45: end of Section 8.7
[V, D] = eig(A): after (8.81) in Section 8.8
Mathematica Command
FindMaximum[{Abs[f′′′′[x]], −π ≤x ≤π}, {x, 0}]: Example 8.10 in Section 8.3
References
Boole, G. and Moulton, J.F. A Treatise on the Calculus of Finite Differences, 2nd edn. Dover, New York,
1960, p. 47.
de Boor, C. A Practical Guide to Splines. Springer-Verlag, New York, 2001.
Conte, S.D. and de Boor, C. Elementary Numerical Analysis: An Algorithmie Approach, 2nd edn.
McGraw-Hill, Inc., New York, 1972.
Donovan, G.C., Miller, A.R., and Moreland, T. J. Pathological functions for Newton’s method.
American Mathematical Monthly 100 (1993), 53–58.
Rall, L.B. Convergence of the Newton process to multiple solutions. Numerische Mathemalik 9, 23–27,
1966.
Turyn, L. Pertubation of periodic boundary conditions. II: Bifurcation and symmetry. Journal of
Mathematical Analysis and Applications 124, 305–326, 1987.
Weisstein, E.W. Boole’s rule. From MathWorld—A Wolfram web resource. http://mathworld.
wolfram.com/BoolesRule.html

9
Fourier Series
9.1 Orthogonality and Fourier Coefficients
9.1.1 Introduction
A Fourier series is a way of decomposing a function into a possibly infinite sum of
“harmonic components”. We call those pieces “harmonic” because they are sinusoidal
functions whose frequencies are multiples of a common base frequency. Those func-
tions are also solutions of harmonic oscillator problems. We call them components
because decomposing a function in this way is like decomposing a vector into orthogonal
components, as we did in Section 2.4, specifically Corollary 2.7 in Section 2.4.
Suppose f(x) is a function defined only on the interval [ −L, L ]. Here, L is an unspecified
positive number. The Fourier series “expansion,” that is, decomposition into components,
is given by
f(x) .= fs(x) = a0
2 +
∞

n=1

an cos
nπx
L

+ bn sin
nπx
L

,
(9.1)
where the real constants a0; a1, a2, a3, ...; b1, b2, b3, ... are called the Fourier coefficients.
The notation f(x) .= fs(x) means that f is “represented” by fs, the Fourier series for f. We
will see that fs(x) may be unequal to f(x) at some value(s) of x; even worse, the Fourier
series fs(x) may fail to converge at some value(s) of x. So, in principle, fs(x) and f(x) may
be different functions.
Note that even though the original function f(x) is defined only for −L ≤x ≤L, its
Fourier series fs(x) may be defined for all x and is periodic with period 2L, that is, fs(x + k ·
2L) = fs(x) for all integers k.
One way to think of the relationship of fs to f is that it is like the relationship of your
computer game’s “avatar” to you. In some circumstances your avatar may behave just
like you, but perhaps not always. Another way to think of the relationship is between a
movie’s “stunt double” and the actress she replaces: From certain viewpoints they may
look and behave exactly alike. In fact, the actress may do the stunt herself so she is being
her own stunt double.
The term a0
2 can be thought of as a0
2 cos
0 · πx
L

, an “honorary” cosine term.
Another way to display (9.1) is to display the infinite series more explicitly using the
“+ · · · ” notation:
737

738
Advanced Engineering Mathematics
f(x) .= fs(x) = a0
2 + a1 cos
πx
L

+ b1 sin
πx
L

+ a2 cos
2πx
L

+ b2 sin
2πx
L

+ · · · .
(9.2)
In Calculus we worked with infinite series of constants and power series. We defined
“partial sums” and studied their convergence. Similarly, here we define the partial sums
by s0(x) ≜a0
2 and
sN(x) ≜a0
2 +
N

n=1

an cos
nπx
L

+ bn sin
nπx
L

,
(9.3)
for N
=
1, 2, . . . . In Theorem 9.1 we will give the main result which guarantees
convergence of the sequence of partial sums.
If f(x) is defined for x in a finite interval [ a, b ], except possibly at finitely many such x,
we will say f(x) is defined* on [ a, b ]. We put an * next to the word “defined” to indicate
that exceptional points are allowed. For example, an f(x) that is defined* on [ 0, L ] may
actually be defined only on the interval (0, L).
Let us see two examples of Fourier series.
Example 9.1
Define f(x) = (x −1)2 on the interval −2 < x < 2. The Fourier series of this function is
fs(x) ≜7
3 + 8
π
∞

n=1
(−1)n
 2
n2π cos
nπx
2

+ 1
n sin
nπx
2

.
(9.4)
We will postpone to Example 9.3 the explanation for how we found this Fourier series.
For now, use graphs to illustrate the convergence of the sequence of partial sums,
sN(x) = 7
3 + 8
π
N

n=1
(−1)n
 2
n2π cos
nπx
2

+ 1
n sin
nπx
2

.
(9.5)
Method: Figure 9.1 depicts the convergence of the sequence of partial sums. The values
of N in the graphs of the partial sums are N = 4, 16, 128, 8192. Notice that on the interval
−2 < x < 2 the graphs of the partial sums get closer and closer to the graph of f(x) =
(x −1)2, as N gets larger and larger. But there are problems at x = ±2, where the partial
sums seem to converge to vertical line segments.
Notice also that the Fourier series, and its partial sums, are periodic with period 4, so
there is no relationship between the Fourier series and (x −1)2 for |x| > 2. ⃝
By the way, these graphs were plotted by MathematicaTM.
Example 9.2
The function f(x) = x, −π < x < π has Fourier series
x .= fs(x) = −2
∞

n=1
(−1)n
n
sin nx;

Fourier Series
739
8
s128(x)
6
4
2
x
8
s8192(x)
6
4
2
x
s4(x)
6
4
2
N =4
–2
2
4
6
x
8
s16(x)
6
4
2
N =16
–2
2
4
6
x
N = 128
–2
2
4
6
N =8192
–2
2
4
6
FIGURE 9.1
Convergence of sequence of partial sums for Example 9.1.
f(x)
f(x)
π
–π
–π
π
x
–2π
2π
–π
π
s
FIGURE 9.2
Fourier series for f(x) = x, −π < x < π.
we will explain why this is true in Example 9.4. Shown in Figure 9.2 are graphs of the
original function, f(x), which is defined only for −π < x < π, and its Fourier series
expansion, fs(x), which is defined for all real numbers x. At x = π all of the functions
sin nx are zero, so fs(π) should be zero.
9.1.2 Convergence of Fourier Series
Denote
f(ξ+) ≜lim
x→ξ+ f(x)
and
f(ξ−) ≜lim
x→ξ−f(x).

740
Advanced Engineering Mathematics
Definition 9.1
f has a finite jump discontinuity at ξ if both of f(ξ±) exist as finite numbers but f(ξ+) ̸=
f(ξ−).
Definition 9.2
Suppose [ a, b ] is a finite interval, that is, −∞< a < b < ∞. A function f(x) is nice on the
interval [ a, b ] if all of the following conditions are met:
(a) For all x in the interval [ a, b ] the function values f(x) and f ′(x) are defined* and
f(x) and f ′(x) are continuous, except at possibly a finite number of values of x,
and
(b) f(a+), f(b−), f ′(a+), and f ′(b−) exist as finite numbers, and
(c) At all points ξ of discontinuity of f and f ′, the discontinuity is a finite jump.
Another name for nice is piecewise smooth.
Theorem 9.1
If a function f(x) is nice on the interval [ −L, L ], then
(a) At all x in (−L, L), the Fourier series of f converges to
fs(x) = f(x−) + f(x+)
2
,
that is, the average of f(x−) and f(x+), and
(b) At x = ±L, the Fourier series of f converges to the average of f(−L+) and f(L−),
that is,
fs(±L) = f(−L+) + f(L−)
2
.
If a nice function f(x) is defined and continuous at x0 then f(x−
0 ) = f(x+
0 ) = f(x0), hence
fs(x0) = f(x0) at such a point of continuity. That is, the Fourier series agrees with the
original function where the latter is continuous.
Also, if f(x−
0 ) = f(x+
0 ) but f(x0) is not defined, then we might as well define an
“improved” version of f, say fImp, by fImp(x0) = f(x−
0 ) = f(x+
0 ). Then fImp(x) agrees with
the Fourier series fs at x0.

Fourier Series
741
9.1.3 Orthogonality and Calculating Fourier Coefﬁcients
Recall from Corollary 2.7 in Section 2.4 that if v1, . . . , vk is an orthogonal set of nonzero
vectors in Rm and y is a vector in Span{v1, . . . , vk}, then we can resolve a vector into
components using the formula
y =
k

i=1
1
||vi||2 ⟨y, vi⟩vi.
There is a similar recipe for Fourier series.
First, let’s define an inner product for functions f(x) and g(x) defined for −L < x < L:

f, g

≜
L
−L
f(x)g(x) dx.
Using the trigonometric identities
cos α cos β = 1
2
	
cos(α −β) + cos(α + β)

,
sin α sin β = 1
2
	
cos(α −β) −cos(α + β)

,
and
sin α cos β = 1
2
	
sin(α −β) + sin(α + β)

,
one can explain why for positive integers m and n we have the orthogonality relations

cos
mπx
L

, cos
nπx
L

=
L,
if m = n
0,
if m ̸= n

,

sin
mπx
L

, sin
nπx
L

=
L,
if m = n
0,
if m ̸= n

,
and

sin
mπx
L

, cos
nπx
L

= 0, for all m, n.
For example, for m ̸= n,

cos
mπx
L

, cos
nπx
L

=
L
−L
cos
mπx
L

cos
nπx
L

dx

742
Advanced Engineering Mathematics
= 1
2
L
−L

cos
(m −n)πx
L

+ cos
(m + n)πx
L

dx = 1
2

sin
	 (m−n)πx
L

−(m −n)
+ sin
	 (m+n)πx
L

−(m + n)
L
−L
= 1
2

sin
	
(m −n)π

−(m −n)
+ sin
	
(m + n)π

−(m + n)
−sin
	
−(m −n)π

−(m −n)
−sin
	
−(m + n)π

−(m + n)

= 0.
But if m = n,

cos
mπx
L

, cos
nπx
L

=
L
−L
cos2 mπx
L

dx = 1
2
L
−L

1+cos

2
	mπx
L


dx = · · · = L.
Also, we can explain easily why
⟨1, 1⟩= 2L,

1, cos
mπx
L

= 0, for all m,
and

1, sin
mπx
L

= 0, for all m.
Using these facts, we can find formulas for the Fourier coefficients a0; a1, a1, a3, . . .;
b1, b1, b3, . . . . In this work we will ignore issues concerning convergence of an infinite
series of orthogonal functions, such as interchange of the operations of taking an infinite
sum and taking an inner product. In addition, we will assume that if f(x) .= fs(x) then
⟨f(x), h(x)⟩= ⟨fs(x), h(x)⟩for any piecewise smooth function h(x).
Suppose f(x) is a nice function and has Fourier expansion given by (9.1), that is,
f(x) .= a0
2 +
∞

m=1
am cos
mπx
L

+
∞

m=1
bm sin
mπx
L

.
Take the inner product of both sides of this equation with cos
	 nπx
L

to get

f(x), cos
nπx
L

=

a0
2 +
∞

m=1
am cos
mπx
L

+
∞

m=1
bm sin
mπx
L

, cos
nπx
L

= a0
2 ·

1, cos
nπx
L

+
∞

m=1
am ·

cos
mπx
L

, cos
nπx
L

+
∞

m=1
bm ·

sin
mπx
L

, cos
nπx
L

= a0
2 · 0 +
∞

m=1
am ·
L,
if m = n
0,
if m ̸= n

+
∞

m=1
bm · 0 = anL.
So, an = 1
L

f(x), cos
	 nπx
L


, that is,
an = 1
L
L
−L
f(x) cos
nπx
L

dx.
(9.6)

Fourier Series
743
Similarly,

f(x), sin
nπx
L

= a0
2 ·

1, sin
nπx
L

+
∞

m=1
am ·

cos
mπx
L

, sin
nπx
L

+
∞

m=1
bm ·

sin
mπx
L

, sin
nπx
L

= a0
2 · 0 +
∞

m=1
am · 0 +
∞

m=1
bm
L,
if m = n
0,
if m ̸= n

.
So,
bn = 1
L
L
−L
f(x) sin
nπx
L

dx.
(9.7)
Also,

f(x), 1

= a0
2 ⟨1, 1⟩+
∞

m=1
am·

cos
mπx
L

, 1

+
∞

m=1
bm·

sin
mπx
L

, 1

= a0
2 · 2L +
∞

m=1
am · 0 +
∞

m=1
bm · 0 = L · a0,
so
a0 = 1
L
L
−L
f(x) dx.
(9.8)
An interesting and useful aside about the last formula is that a0
2 , the first term in the Fourier
series, has a very natural physical interpretation:
a0
2 = 1
2L
L
−L
f(x) dx
which is the average value of the function f on the interval [ −L, L ]. For example, if f(x) is
the equilibrium temperature in a rod extending over the interval −L ≤x ≤L, then a0
2 is the
average temperature in the rod.
Formulas (9.6) through (9.8) give the Fourier coefficients for the general Fourier series
(9.1) for a nice function f(x) defined on the interval −L < x < L. The Fourier series is unique:
Given f, (9.6) through (9.8) specify the values for the coefficients a0 and an, bn, n = 1, 2, . . .
In the next problem and many other problems it is useful to note that sin nπ = 0 and
cos(±nπ) = (−1)n for n = 1, 2, . . . .

744
Advanced Engineering Mathematics
Example 9.3
(Example 9.1 again) Find the Fourier series for the function f(x) = (x −1)2 defined only
on the interval −2 < x < 2.
Method: Let L = 2. Using integration by parts and (9.6) through (9.8), we calculate
a0 = 1
2
2
−2
f(x)dx = 1
2
2
−2
(x −1)2 dx = 1
2
1
3(x −1)3
2
−2
= · · · = 14
3 ,
an = 1
2
2
−2
f(x) cos
nπx
2

dx = 1
2
2
−2
(x −1)2 cos
nπx
2

dx
= 1
2

(x −1)2 sin
	 nπx
2

nπ
2
+ 2(x −1) cos
	 nπx
2

	 nπ
2

2
−2 sin
	 nπx
2

	 nπ
2

3
2
−2
= 1
2

0 + 2(−1)n
	 nπ
2

2 −0

−

0 + 2(−3)(−1)n
	 nπ
2

2
−0

=
16
n2π2 (−1)n,
and
bn = 1
2
2
−2
f(x) sin
nπx
2

dx = 1
2
2
−2
(x −1)2 sin
nπx
2

dx
= 1
2

(x −1)2 cos
	 nπx
2

−nπ
2
+ 2(x −1) sin
	 nπx
2

	 nπ
2

2
+ 2 cos
	 nπx
2

	 nπ
2

3
2
−2
= 1
2

(−1)n
−nπ
2
+ 0 +




2(−1)n
	 nπ
2

3

−

9(−1)n
−nπ
2
+ 0 +




2(−1)n
	 nπ
2

3

= 8
nπ (−1)n,
The Fourier series of this function is
(x −1)2 .= fs(x) = 7
3 + 8
π
∞

n=1
(−1)n
 2
n2π cos
nπx
2

+ 1
n sin
nπx
2

. ⃝
9.1.4 Even and Odd Functions and Their Fourier Series Coefﬁcients
A function F is even if F(−x) ≡F(x) whenever both x and −x are in the domain of F.
For example, an even power of x is an even function, cos αx is an even function for any
constant α, and |x| is an even function of x. In fact, if g is any function, then g(|x|) is an
even function of x.
Thinking graphically, a function is even if its graph y = f(x) is symmetric about the
y-axis, that is, (x, y) is on the graph if and only if (−x, y) is on the graph, as illustrated in
Figure 9.3.
A function F is odd if F(−x) ≡F(x) whenever both x and −x are in the domain of F. For
example, an odd power of x is an odd function, sin αx is an odd function for any constant α,
and x|x| is an odd function of x. In fact, if g is any function, then xg(|x|) is an odd function
of x.
Thinking graphically, a function is odd if its graph y = f(x) is symmetric about the
origin, that is, (x, y) is on the graph if and only if (−x, −y) is on the graph, as illustrated in
Figure 9.4.

Fourier Series
745
F(x)
x
FIGURE 9.3
Even function picture.
F(x)
x
FIGURE 9.4
Odd function picture.
Theorem 9.2
(a) If F is an even function, then for any value of L,
 L
−L F(x) dx = 2
 L
0 F(x) dx.
(b) If F is an odd function, then for any value of L,
 L
−L F(x) dx = 0.
Why?
(a) To see this, write
 L
−L F(x) dx =
 0
−L F(x) dx +
 L
0 F(x) dx; for the former, make the
substitution z = −x to see that
 0
−L F(x) dx =
 L
0 F(x) dx.
(b) To see this, write
 L
−L F(x) dx =
 0
−L F(x) dx +
 L
0 F(x) dx; for the former, make the
substitution z = −x to see that
 0
−L F(x) dx = −
 L
0 F(x) dx. 2
Using these observations, we note that
Theorem 9.3
(a) If f is an odd function, then its Fourier coefficients a0, a1, . . . , an,. . . are zero, that
is, the Fourier series of f on [ −L, L ] consists only of sine function terms, that is,

746
Advanced Engineering Mathematics
f(x) .= fs(x) =
∞

n=1
bn sin
nπx
L

, where bn = 2
L
L
0
f(x) sin
nπx
L

dx.
(b) If f is an even function, then its Fourier coefficients b1, . . . , bn, . . . are zero, that is,
the Fourier series of f on [ −L, L ] consists only of the constant term and the cosine
function terms, that is,
f(x) .= fs(x) = a0
2 +
∞

n=1
an cos
nπx
L

,
where
a0 = 2
L
L
0
f(x) dx
and
an = 2
L
L
0
f(x) cos
nπx
L

dx.
Why? (a) If f is odd, then F(x) ≜f(x) cos
	 nπx
L

is odd for all n, so Theorem 9.2(b) implies
that an = 0 for all n. The formula for bn follows from Theorem 9.2(a).
For (b) you will use similar reasoning to explain this in Problem 9.1.8.23. 2
By Theorem 9.3, to find the Fourier series of an odd or even function we have fewer
integrations to do and we need only integrate over the interval [ 0, L ].
This suggests that we should replace a given function by, say, an even function, if we
could. For example, sometimes we expect the solution of a physical problem to have the
symmetry property of being an even function. This might require some caution; if we were
wrong to assume the solution is even, we might find no solution or an incorrect “solution”
or miss finding some of the correct solutions.
Example 9.4
Show that the function f(x) = x, −π < x < π has Fourier series
x .= fs(x) =
∞

n=1
(−1)n+1 2
n sin nx.
Method: With L = π, Theorem 9.3 says that f being an odd function implies its Fourier
series consists only of sine terms, specifically
x .=
∞

n=1
bn sin nx
and the coefficients are, after using integration by parts,
bn = 2
π
π
0
x sin nx dx = 2
π
x cos nx
−n
+ sin nx
n2
π
0
= 2
π
−π cos nπ + 0
n
+ 0 −0
n2

= −2(−1)n
n
.
The result follows. ⃝

Fourier Series
747
Example 9.5
For the square wave function with period 2L, that is, given by
f(x) =
−1,
−L < x < 0
1,
0 < x < L

,
(a) Find the Fourier series expansion, and
(b) Sketch the graphs of the original function and its Fourier series.
Method: (a) With L = π, Theorem 9.3 says that f being an odd function implies its Fourier
series consists only of sine terms, specifically
f(x) .= fs(x) =
∞

n=1
bn sin
nπx
L

.
We calculate
bn = 2
L
L
0
f(x) sin
nπx
L

dx = 2
L
L
0
1 · sin
nπx
L

dx = 2
L

cos
	 nπx
L

−nπ
L
L
0
= 2
nπ
	
1 −(−1)n
.
The Fourier series of the square wave is given by
f(x) .= fs(x) = 2
π
∞

n=1
1 −(−1)n
n
sin
nπx
L

.
(9.9)
Note that (1 −(1)n) is zero for even n and (1 −(1)n) equals two for odd n. Odd integers
can be expressed in the form n = 2k −1, so we can rewrite the Fourier series (9.9) as
f(x) .= fs(x) = 4
π
∞

k=1
1
2k −1 sin
(2k −1)πx
L

.
(b) Shown in Figure 9.5 are graphs of the original function, f(x), which is defined only
for −L < x < 0 and 0 < x < L, and its Fourier series expansion, fs(x), which is defined
for all real numbers x. The Fourier series graph, that is, for fs(x), has filled circles at
x = nL, for all integers n. These are values of x where the original function, f(x), had
jump discontinuities. As we see from Theorem 9.1, this “filling in” of the function is a
result of using a Fourier series expansion. ⃝
Square waves are commonly used in engineering, so their Fourier series is a useful tool
in our toolbox of mathematical results.
f(x)
fs(x)
x
–L
–L
–2L
2L
L
1
1
L
x
FIGURE 9.5
Example 9.1: Fourier series for square wave.

748
Advanced Engineering Mathematics
9.1.5 Finding the Fourier Series in a Special Case
Example 9.6
Find the Fourier series for the function f(x) = 2 sin 2x −cos 5x defined for −π < x < π.
Method: This can be done very easily: Because L = π, the Fourier series given by (9.1)
should be in the form
2 sin 2x −cos 5x = f(x) .= fs(x) = a0
2 +
∞

n=1
an cos nx +
∞

n=1
bn sin nx.
So, this function f(x) was already given as a Fourier series! There is no need to calculate
the integrals used in formulas (9.6) through (9.8). But if we did, by uniqueness of the
Fourier series of f(x), we would find that a0 = 0, a5 = −1, b2 = 2, and all other an’s and
bn’s are zero. The Fourier series of f(x) = 2 sin 2x −cos 5x, −π < x < π, is
fs(x) = 2 sin 2x −cos 5x. ⃝
If we wanted to do Example 9.6 by calculating integrals, the work would be tedious
because we would need to do separate calculations for n = 2 and n = 5.
Note that although the original function is defined only for −π < x < π, its Fourier
series is defined for all x and periodic with period 2π.
9.1.6 Periodic Extension of a Function Given for 0 < x < 2L
Suppose f(x) is given for 0 < x < 2L and we are told that f is periodic with period 2L, that
is, f(x −2kL) = f(x) at all x in the domain of f and all integers k. This implicitly gives the
values of f(x) for all x with −∞< x < ∞, with the possible exception of a discrete set of
points. For example, f(−L
2) = f

−L
2 + 2L

= f

3L
2

, by periodicity.
Now suppose that we are given a function f(x) defined* only for x in the interval [ 0, 2L ].
We can extend f periodically to be defined for all x, or at least all x that are an integer
multiple (possibly negative) of 2L away from the places where f is defined* on [ 0, 2L ]. The
picture “says a thousand words.” In symbols, the periodic extension of f is a function f
defined by
f(x) ≜f(x −k2L),
where k is chosen so that 0 ≤x −k2L ≤2L; note that k may depend on x. For example,
if L = π, thenf

31π
6

≜f

7π
6

, by choosing k = 2, andf

−31π
6

≜f

5π
6

, by choosing
k = −3.
Because the original function f(x) is given on the interval [ 0, 2L ] and its periodic exten-
sion is given on the whole real line, we can use values off(x) for −L < x < L and thus,
we can calculate the Fourier series of f(x) on the interval −L < x < L. Because of the
periodicity off(x) we see, as in Figure 9.6, that the values of the Fourier series off(x) for
−L < x < 0 give us the values of the Fourier series of f(x) for −L < x < 0.

Fourier Series
749
0.5
–0.5
2
x
f
0.5
–0.5
2
–2
4
6
x
f~
f (x)
FIGURE 9.6
Periodic extension of a function.
Example 9.7
For the function defined by f(x) =
sin 2x,
0 < x < π
0,
π < x < 2π

, (a) sketch the graph of the
periodic extension of f on the interval [−π, π], (b) sketch the graph of the Fourier series
of f on an interval of length two periods, and (c) find the Fourier series, that is, evaluate
the Fourier coefficients.
Method:
(a) Letf be the periodic extension of f whose period is 2π. Then
f(x) =

0,
−π < x < 0
sin 2x,
0 < x < π

and L = π. The sketch of the graph of the original function and the periodic
extension, graphed only on the interval [−π, π], is in Figure 9.7.
2
1
4
3
6
5
x
0.5
–0.5
1.0
f (x)
2
1
3
–2
–1
–3
x
0.5
–0.5
1.0
f(x)
~
–1.0
–1.0
FIGURE 9.7
Graphs for Example 9.7(a).

750
Advanced Engineering Mathematics
–6
–4
–2
2
4
6
x
0.5
–0.5
1.0
fs(x)
–1.0
FIGURE 9.8
Graph for Example 9.7(b).
(b) The sketch of the graph of the Fourier series of f on an interval of length two periods
is in Figure 9.8.
(c) We calculate
a0 = 1
π
π
−π
f(x) dx = 1
π
⎛
⎝
0
−π
0 · dx +
π
0
sin 2x dx
⎞
⎠= 1
π
cos 2x
−2
π
0
= 0.
For n ̸= 2,
an = 1
π
π
−π
f(x) cos nx dx = 1
π
π
0
sin 2x cos nx dx
= 1
π
π
0
1
2

sin
	
(2 −n)x

+ sin
	
(2 + n)x


dx = 1
2π

cos
	
(2 −n)x

−(2 −n)
+ cos
	
(2 + n)x

−(2 + n)
π
0
= 1
2π
 (−1)n
−(2 −n) +
(−1)n
−(2 + n)

−

1
−(2 −n) +
1
−(2 + n)

=
4
2π(4 −n2)
	
1 −(−1)n
,
and similarly
bn = 1
π
π
−π
f(x) sin nx dx = 1
π
π
0
sin nx sin 2x dx = 1
π
π
0
1
2

cos
	
(n −2)x

−cos
	
(n + 2)x


dx
= 1
2π

sin
	
(n −2)x

n −2
−sin
	
(n + 2)x

n + 2
π
0
= 0.
For n = 2, we evaluate
a2 = 1
π
π
−π
f(x) cos 2x dx = 1
π
π
0
cos 2x sin 2x dx = 1
π
π
0
1
2 sin 4x dx = 1
2π
cos 4x
−4
π
0
= 0,

Fourier Series
751
and
b2 = 1
π
π
−π
f(x) sin 2x dx = 1
π
π
0
sin 2x sin 2x dx = 1
π
π
0
1 −cos 4x
2
dx
= 1
2π

x −sin 4x
4
π
0
= 1
2.
The Fourier series of the original function f is the same as the Fourier series of the
periodic extensionf, that is,
f(x) .= fs(x) = 1
2 sin 2x −2
π
∞

n̸=2
1 −(−1)n
n2 −4
cos nx. ⃝
(9.10)
Again, (1−(1)n) is zero for even n. Odd integers can be expressed in the form n = 2k−1,
so we can rewrite the Fourier series (9.10) as
f(x) .= fs(x) = 1
2 sin 2x −4
π
∞

k=1
1
(2k −1)2 −4 cos
	
(2k −1)x

.
Remark
In the above example, we used the values off(x) = f(x −2L) for −L < x < 0 in order to
find the Fourier coefficients forf and thus for f. Alternatively, we could use the result of
Problem 9.1.8.22 to calculate the Fourier coefficients using the values of f(x) for 0 < x < 2L:
a0 = 1
L
2L

0
f(x) dx,
an = 1
L
2L

0
f(x) cos
nπx
L

dx,
bn = 1
L
2L

0
f(x) sin
nπx
L

dx.
(9.11)
9.1.7 Other Kinds of Fourier Series
In Section 9.4 we will learn about the “complex Fourier series” defined by
f(x) .= fC(x) =
∞

n=−∞
cn einπx/L
with complex coefficients cn, for a nice function f(x) defined* on the interval [ −L, L ].
This will turn out to be completely equivalent to the “real” form of Fourier series defined
in (9.1).
In Section 9.5 we will learn about the “discrete Fourier transform” (DFT) defined by
Fℓ≜
1
√
N
N−1

k=0
fk e−i2πkℓ/N, for ℓ= 0, 1, . . . , N −1,
with complex coefficients fk, for a function f(t) defined at t = 0, t1, . . . , tN−1. This is a very
popular tool for analysis of discrete time systems, including for signal analysis, and also

752
Advanced Engineering Mathematics
a useful tool for data analysis and manipulation. The DFT can be motivated by complex
Fourier series.
In Section 9.4 we will learn about the “Fourier transform” defined by
F[ f(t) ] = F(ω) ≜
1
√
2π
∞

−∞
f(t) e−iωt dt
defined for a nice function f(x) defined on the interval (−∞, ∞). We will see that it can be
motivated by taking complex Fourier series on (−L, L) and letting L →∞. It will turn
out that Fourier transforms can help us to solve partial differential equations. Fourier
transforms are also important in signal processing.
Learn More About It
Linear Operator Theory in Engineering and Science, (vol. 40 of Applied Mathematical Sci-
ences), by Arch W. Naylor and George R. Sell, Springer-Verlag, c⃝1982, has a useful
exposition of “Integration and Measure Theory” in its Appendix D.
9.1.8 Problems
1. Find the Fourier series of the function f(x) whose graph is sketched in Figure 9.9.
2. What is the Fourier representation of f(x) = 2 + sin x, defined for −π < x < π?
3. Evaluate the appropriate integrals for the Fourier coefficients to explain why the
representation x .=
∞
n=1
2(−1)n+1L
nπ
sin
	 nπx
L

is true on the interval [ −L, L ].
For problems 4–6, a function is given over one period. Sketch the graphs of (a) the given
function, (b) the periodic extension of the function over an interval of length two periods,
and (c) the Fourier series over an interval of length two periods. For (c), use open and
closed circles at appropriate places on the graph.
4. f(x) =
 0,
−π < x < 0
x
π ,
0 < x < π

5. f(x) =
 0,
−π < x < −π
2
cos x,
−π
2 < x < π

1
0.5
f (x)
0
–0.5
–1
x
–3
–2
–1
0
1
2
3
FIGURE 9.9
Graph of function in Problem 9.1.8.1.

Fourier Series
753
–1.5
–1.0
–0.5
0.2
0.4
0.6
0.8
1.0
f(x)
0.5
1.0
1.5
–π
π
f(x)
x
π
x
(a)
(b)
FIGURE 9.10
Graph of functions in Problems (a) 9.1.8.7 and (b) 9.1.8.8.
6. f(t) =
−1,
−π < t < −π
2
sin t,
−π
2 < t < π

For problems 7 and 8, a function is sketched over one period. Sketch the graphs of (a)
the periodic extension of the function over an interval of length two periods and (b) the
Fourier series over an interval of length two periods. For (a) and (b), use open and closed
circles at appropriate places on the graph.
7. The partly sinusoidal function whose graph is sketched over one period in
Figure 9.10a
8. The function whose graph is sketched over one period in Figure 9.10b
For problems 9–14, find the Fourier series representation of the given functions that
are defined* over one period. Some of these problems can be done without evaluating
integrals, for example, by using trigonometric function identities.
9. f(x) = |x|, −L < x < L
10. f(x) = cos2 x, −π < x < π
11. f(x) = 1 + sin(x −π
3 ) cos2 x, −π
2 < x < π
2
12. f(x) = | sin(x)|, −π < x < π
13. f(x) = x cos x, −π < x < π
14. The function whose graph is sketched over one period in Figure 9.11
15. The “sigmoidal” function f is sketched over one period in Figure 9.12a.
(a) Find the Fourier series of f.
(b) Sketch the graph of the Fourier series of f over an interval of length exactly
two periods.
(c) Find the function g that consists of the first five nonzero terms of the Fourier
series of f.
(d) Sketch the graphs of f and g over an interval of length exactly one period.

754
Advanced Engineering Mathematics
–1
1
2
–3
–2
–1
1
2
3
x
f(x)
FIGURE 9.11
Graph of function in Problem 9.1.8.14.
f (x)
f (x)
π
2
π
2
–
π
2
–
π
π
π
π
–π
–π
–π
2
x
x
(a)
(b)
FIGURE 9.12
Graphs of functions in (a) Problems 9.1.8.15 and (b) 9.1.8.16.
For each of problems 16–21, the function given is defined* over one period. (a) Find the
Fourier series representation, and (b) sketch the graph of the Fourier series over an interval
of length exactly two periods.
16. The function f(x) sketched over one period, −π ≤x ≤π, in Figure 9.12b
17. f(x) = e−x, −L < x < L. You may find useful the facts in (3.9) in Section 3.1, (3.10)
in Section 3.1, that is, that

eαx cos β) dx =
eαx
α2 + β2 (β sin βx + α cos βx) ,

eαx sin βx dx =
eαx
α2 + β2 (−β cos βx + α sin βx) .
18. (i) f(x) = |π −x|, −π < x < π,
(ii) f(x) = π −|x|, −π < x < π
19. f(x) =
⎧
⎪⎨
⎪⎩
x,
−π < x < 0
x
2,
0 < x < π
⎫
⎪⎬
⎪⎭
20. The function f(x) sketched over one period in Figure 9.13
21. The function f(x) sketched over one period in Figure 9.14

Fourier Series
755
f (x)
1
x
1
2
1
– 2
FIGURE 9.13
Graph of function in Problem 9.1.8.20.
–2
–1
–1
1
1
2
f(x)
x
–2
FIGURE 9.14
Graph of function in Problem 9.1.8.21.
22. Show that if F(x) is periodic with period 2L and [ a, b ] is any interval of length 2L,
that is, b −a = 2L, then
 b
a F(x) dx =
 L
−L F(x) dx. [This result is useful because one
might find it more convenient to integrate on [ a, b ] rather than on [ −L, L ]].
23. Suppose f(x) is even. Explain why Theorem 9.3(b) is true, that is, the Fourier series
of f on [ −L, L ] consists only of the constant term and the cosine function terms.
24. Suppose f(x) is periodic with period 2π and satisfies f(x −π) = −f(x) for all x.
Explain why all the even-indexed Fourier coefficients are zero.
9.2 Fourier Cosine and Sine Series
A “Fourier cosine series” or a “Fourier sine series” is a way to analyze a function f(x) that
is given only on the interval [ 0, L ], that is, only the right half of the interval [ −L, L ]. For
this reason a “Fourier sine series” or a “Fourier cosine series” is called a half-range expan-
sion. Such half-range expansions are very useful in the process of solving some partial
differential equations by a method called separation of variables.

756
Advanced Engineering Mathematics
We will see that both the ideas and calculations of Fourier cosine and Fourier sine series
are based on the same for Fourier series.
9.2.1 Fourier Cosine Series
If a function f(x) is defined* only on the interval [ 0, L ], we may decompose f(x) into its
Fourier cosine series, a possibly infinite sum:
f(x) .= a0
2 +
∞

n=1
an cos
nπx
L

,
(9.12)
where the constants a0; a1, a2, a3, . . . are called the Fourier cosine coefficients.
This corresponds to a graphical process of
• First extending the function f evenly to give a function ˘f defined* on the interval
[ −L, L ],
• Next, finding the Fourier series of the even function ˘f defined* on the interval
[ −L, L ],
• Next, noting that there are no sine terms in the Fourier series of ˘f, and
• Finally, seeing that the Fourier series of ˘f gives the Fourier cosine series for the
original function f.
As for a Fourier series, an averaging formula gives the values of the Fourier cosine series,
fcos(x), at jump points and endpoints of the graph of the function ˘f(x).
In general, the Fourier cosine series of a given function f defined* on the interval [ 0, L ]
is the Fourier series of the even function ˘f defined* on the interval [ −L, L ]. So,
f(x) .= fcos(x) = a0
2 +
∞

n=1
an cos
nπx
L

,
where
a0 = 1
L
L
−L
˘f(x) dx = 2
L
L
0
f(x) dx,
and
an = 1
L
L
−L
˘f(x) cos
nπx
L

dx = 2
L
L
0
f(x) cos
nπx
L

dx, n = 1, 2, . . . .
Even though the existence of the Fourier cosine series of f is accomplished using its
even extension, ˘f, when we calculate the Fourier cosine coefficients we only need to use
integrals on the interval [ 0, L ]. So, we need only use values of the original function f, not
the artificially created function ˘f.

Fourier Series
757
3
2
1
1
fcos(x)
2
3
x
f(x)
3
2
1
1
–1
–2
–3
2
3
x
3
2
1
2
–2
–4
–6
4
6
x
f(x)
˘
(a)
(b)
(c)
FIGURE 9.15
Graphs of (a) f(x), (b) f(x), (c) fcos in Example 9.8.
Example 9.8
(Graphically) Suppose f(x) = x for 0 < x < L. Show successively the graphs of (a) f(x)
for 0 < x < L, (b) ˘f(x) for −L < x < L, and (c) fcos(x), the Fourier cosine series of f on the
interval −L ≤x ≤3L, that is, graphed for two periods of the Fourier cosine series.
Method: Figure 9.15 gives the complete graphical solution. ⃝
Example 9.9
Find the Fourier cosine series of f(x) = x defined for 0 < x < π.
Method: First, note that L = π in this example. The Fourier cosine series of f defined on
the interval (0, π) is given in the form
f(x) .= fcos(x) = a0
2 +
∞

n=1
an cos nx,
where
a0 = 2
π
π
0
f(x) dx, and an = 2
π
π
0
f(x) cos nx dx.
So, we calculate
a0 = 2
π
π
0
x dx = 2
π
1
2 x2
π
0
= π,
an = 2
π
π
0
x cos nx dx = 2
π
%x sin nx
n
+ cos nx
n2
&π
0 = · · · = 2
	
(−1)n −1

n2π
.
The Fourier cosine series expansion of f is
f(x) .= fcos(x)= π
2 +
∞

n=1
2
	
(−1)n −1

n2π
cos nx = π
2 −4
π
∞

k=1
1
(2k −1)2 cos
	
(2k −1)x

. ⃝
Recall the concept of a nice function from Definition 9.2 in Section 9.1.
The convergence of the Fourier cosine series of f follows from results about convergence
of the Fourier series of ˘f, the even extension of f defined* on [ −L, L ].

758
Advanced Engineering Mathematics
Theorem 9.4
If a function f is nice on the interval [ 0, L ], then at all x in [ 0, L ] the Fourier cosine series
of f converges to the average of ˘f(x−) and ˘f(x+), hence
fcos(x) = f(x−) + f(x+)
2
, 0 < x < L,
fcos(0) = f(0+),
and
fcos(L) = f(L−).
Note that the function fcos(x) is even and periodic with period 2L.
If a nice function f has f(x) defined and continuous at some x value in the interval 0 <
x < L, then ˘f(x−) = ˘f(x+) = f(x), hence fcos(x) = f(x) at such a point of continuity. The
Fourier cosine series agrees with the original function wherever the latter is continuous.
Note also that if f is not defined or not continuous at some x value in the interval 0 < x <
L, but, nevertheless, f(x−) = f(x+), then we might as well define an “improved” version
of f, say fImp, by fImp(x) = f(x−) = f(x+), and the Fourier cosine series of f agrees with fImp
at that x.
9.2.2 Fourier Sine Series
If a function f(x) is defined* only on the interval [ 0, L ], we may decompose f(x) into its
Fourier sine series, a possibly infinite sum of sine functions:
f(x) .=
∞

n=1
bn sin
nπx
L

,
(9.13)
where the constants b1, b2, b3, . . . are called the Fourier sine coefficients.
This corresponds to a graphical process of
• First extending the function f oddly to give a function ˇf defined* on the interval
[ −L, L ],
• Next, finding the Fourier series of the odd function ˇf defined* on the interval
[ −L, L ],
• Next, noting that there are no constant and no cosine terms in the Fourier series
of ˇf, and
• Finally, seeing that the Fourier series of ˇf gives the Fourier sine series for the
original function f.
As usual, an averaging formula gives the values of the Fourier sine series, fsin(x), at jump
points and endpoints of the graph of the original ˇf(x).
In general, the Fourier sine series of a given function f defined* on the interval [ 0, L ] is
the Fourier series of the odd function ˇf defined* on the interval [ −L, L ]. So,
f(x) .= fsin(x) =
∞

n=1
bn sin
nπx
L

,

Fourier Series
759
where
bn = 1
L
L
−L
ˇf(x) sin
nπx
L

dx = 2
L
L
0
f(x) sin
nπx
L

dx, n = 1, 2, . . . .
Even though the existence of the Fourier sine series of f is accomplished using its odd
extension, ˇf, when we calculate the Fourier sine coefficients we only need to use inte-
grals on the interval [ 0, L ]. So, we need only use values of the original function f, not the
artificially created function ˇf.
Example 9.10
(Graphically) Suppose f(x) = x for 0 < x < L. Show successively the graphs of (a) f(x)
for 0 < x < L, (b) ˇf(x) for −L < x < L, and (c) fsin(x), the Fourier sine series of f on the
interval −L ≤x ≤3L, that is, graphed for two periods of the Fourier sine series.
Method: Figure 9.16 gives the complete graphical solution. ⃝
Example 9.11
Find the Fourier sine series of f(x) = x defined for 0 < x < π.
Method: First, note that L = π in this example. The Fourier sine series of f defined* on
the interval (0, π) is the same as the Fourier series of ˇf, the odd extension of f, defined*
on the interval (−π, π). Because ˇf(x) is odd on (−π, π), its Fourier series is given in
the form
ˇf(x) .=
∞

n=1
bn sin nx,
where oddness of ˇf guarantees that
bn = 2
π
π
0
f(x) sin nx dx.
(a)
(b)
(c)
2
1
1
fsin(x)
2
3
x
f(x)
2
1
1
–1
–2
2
3
x
2
1
2
–2
–4
–6
4
6
x
f(x)
˘
FIGURE 9.16
Graphs of (a) f(x), (b) f(x), (c) fsin in Example 9.10.

760
Advanced Engineering Mathematics
So, we calculate
bn = 2
π
π
0
x sin nx dx = 2
π
x cos nx
−n
+ sin nx
n2
π
0
= · · · = 2(−1)n+1
n
.
The Fourier sine series expansion of f is
f(x) .= fsin(x) = −2 ·
∞

n=1
(−1)n
n
sin nx. ⃝
As to the convergence of the Fourier sine series of f, results about this follow from results
about convergence of the Fourier series of ˇf, the odd extension of f defined* on [ −L, L ].
Theorem 9.5
If a function f is nice on the interval [ 0, L ], then at all x in [ 0, L ] the Fourier sine series of f
converges to the average of ˇf(x−) and ˇf(x+), hence
fsin(x) = f(x−) + f(x+)
2
, 0 < x < L,
and
fsin(0) = fsin(L) = 0.
Note that the function fsin(x) is odd and periodic with period 2L.
If a nice function f(x) is defined* and continuous at x0 in the interval (0, L), then ˇf(x−
0 ) =
ˇf(x+
0 ) = f(x0), hence fsin(x0) = f(x0) at such a point of continuity. The Fourier sine series
agrees with the original function where the latter is continuous, except possibly at the
endpoints x = 0, x = L.
Note also that if f is not defined or not continuous at some x value in the interval 0 < x <
L, but, nevertheless, f(x−) = f(x+), then we might as well define an “improved” version
of f, say fImp, by fImp(x) = f(x−) = f(x+), and then the Fourier sine series of f agrees with
fImp at that x.
By the way, it’s interesting to see the graph of the Fourier series of the function defined
by f(x) = −sin
	 x
2

on the interval 0 < x < π. Note that fsin(x) = −sin
	 x
2

only for the
intervals . . . , −5π < x < −3π, −π < x < π, 3π < x < 5π, . . ., which make up about half
of the real line. See Figure 9.17.
Remarks
1. If a function f(x) is given only on the interval [ 0, L ], we may decompose f(x) into
either a Fourier cosine series or into a Fourier sine series. In an applied context,
usually we are interested in one or the other but not both at the same time, unless
we have some reason to contrast the two series.
In particular, given a function f defined* on [ 0, L ], it makes no sense to add
the Fourier cosine series and the Fourier sine series. In fact, on the interval [ 0, L ],
fcos(x) + fsin(x) .= 2f(x), not f(x).
2. Suppose a function is a linear combination of sine functions whose least period is
2L, for example, f(x) = 2 sin x −sin 3x, whose least period is 2π. [Note that f is also

Fourier Series
761
–5
5
–0.5
0.5
1.0
fsin(x)
x
–1.0
FIGURE 9.17
Graphs of fsin.
periodic with period 4π, 6π, etc.] As we saw in Example 9.6 in Section 9.1, there
are several ways to calculate the Fourier series coefficients of ˇf, the odd extension
of f: We could calculate the usual integrals, we could use orthogonality, or we
could note that
f(x) = 2 sin x −sin 3x .= fsin(x) =
∞

n=1
bn sin nx
explains why f was already given in the form of its Fourier sine series on [ 0, π ],
with b1 = 2, b3 = −1, and all other bn = 0. In short,
f(x) .= fsin(x) = 2 sin x −sin 3x.
Example 9.12
For the function f(x) = cos
	 x
2

, defined for 0 < x < 2π, find the Fourier cosine series
and Fourier sine series and graph them on an interval of length exactly two periods.
Method:
(a) For a function on the interval 0 < x < 2π, the Fourier cosine series has the form
f(x) .= fcos(x) = a0
2 +
∞

n=1
an cos
nx
2

.
So, f(x) = cos
	 x
2

is its own Fourier cosine series, with a1 = 1; an = 0, for n =
0, 2, 3, . . . . In short,
f(x) .= fcos(x) = cos
x
2

.
(b) For a function on the interval 0 < x < 2π, the Fourier sine series has the form
f(x) .= fsin(x) =
∞

n=1
bn sin
nx
2

.
We calculate
bn = 2
2π
2π

0
cos
x
2

sin
nx
2

dx = 2
2π · 1
2
2π

0

sin
(n + 1)x
2

+ sin
(n −1)x
2

dx.

762
Advanced Engineering Mathematics
1.0
0.5
–5
5
10
x
fcos(x)
–10
–0.5
1.0
0.5
–5
5
10
x
fsin(x)
–10
–0.5
–1.0
–1.0
FIGURE 9.18
Example 9.12.
For n ̸= 1,
bn = 1
2π
⎡
⎣
cos

(n+1)x
2

−n+1
2
+
cos

(n−1)x
2

−n−1
2
⎤
⎦
2π
0
= · · · = 4n
	
(−1)n −1

2π(n2 −1)
.
For n = 1, we have
b1 = 2
2π
2π

0
cos
x
2

sin
x
2

dx = 2
2π · 1
2
2π

0
sin x dx = 1
2π
+
−cos x
,2π
0
= 0.
So, noting that the only nonzero terms involve even n, that is, n that can be written
in the form n = 2k, we have
f(x) .= fsin(x) = 8
π
∞

k=1
k
4k2 −1 sin kx.
(c) (d) The graphs of the Fourier cosine series and the Fourier sine series, each on an
interval of length exactly two periods, are given in Figure 9.18. ⃝
9.2.3 Fourier Analysis and Oscillations
To solve an ordinary differential equation (ODE) of the form L[ y ] = f(t), we can decom-
pose f(t) in a Fourier series, Fourier cosine series, or Fourier sine series. This will naturally
decompose the solution y(t) in a series. This method is useful for handling an arbitrary
function f(t) and is natural in the sense that in the study of oscillations it is a common
technique to Fourier analyze either a forcing function or the solution, or both.
This method can be thought of as an alternative to finding a solution in the form of a
convolution. A convolution may be elegant in its simple form but may still require a lot of
further study, as in Example 4.34 in Section 4.5, in order to say much about the behavior
of the solution.
Example 9.13
Use Fourier analysis to solve the IVP of Example 4.34 in Section 4.5 in the particular case
of T = 2π, that is,

Fourier Series
763
⎧
⎨
⎩
˙y + 2y = f(t)
y(0) = 0
⎫
⎬
⎭,
where the square wave f(t) is periodic with period 2π, that is, is given over one period by
f(t) =
 1,
0 < t < π
−1,
π < t < 2π

,
and compare the solution to that found in Chapter 4.
Method: Recall from Example 9.5 in Section 9.1 that, with L = π,
f(t) .= fs(t) = 4
π
∞

k=1
1
2k −1 sin
	
(2k −1)t

.
A convergence theorem, specifically Theorem 9.1 in Section 9.1, says that f(t) = fs(t)
except at t being an integer multiple of π.
Our ODE can be rewritten as
˙y + 2y = 4
π
∞

k=1
1
2k −1 sin
	
(2k −1)t

.
Look for a particular solution in the form
yp(t) = 4
π
∞

k=1
1
2k −1 yk,p(t),
where yk,p(t) satisfies
˙yk,p + 2yk,p = sin
	
(2k −1)t

, k = 1, 2, . . . .
(9.14)
The method of undetermined coefficients for (9.14) gives solution form
yk,p(t) = A cos
	
(2k −1)t

+ B sin
	
(2k −1)t

,
where A, B are constants to be determined. Substitute this into ODE (9.14) and sort terms
to get
0 cos
	
(2k −1)t

+ 1 sin
	
(2k −1)t

= ˙yk,p + 2yk,p
=
	
2A + (2k −1)B

cos
	
(2k −1)t

+
	
−(2k −1)A + 2B

sin
	
(2k −1)t

.
So, we need
A
B

=

2
2k −1
−(2k −1)
2
−1 0
1

=
1
4 + (2k −1)2

2
−(2k −1)
2k −1
2
 0
1

=
1
4 + (2k −1)2
−(2k −1)
2

.
The solutions, for k = 1, 2, . . ., are
yk,p(t) =
1
4 + (2k −1)2
	
−(2k −1) cos
	
(2k −1)t

+ 2 sin
	
(2k −1)t


.

764
Advanced Engineering Mathematics
Thus
yp(t) = 4
π
∞

k=1
1
2k −1 yk,p(t)
= 4
π
∞

k=1
1
(2k −1)(4 + (2k −1)2)
	
−(2k −1) cos
	
(2k −1)t

+ 2 sin
	
(2k −1)t


.
The general solution of the ODE is
y(t) = yh(t) + yp(t) = c1e−2t + yp(t).
The initial condition yields
0 = y(0) = c1 + yp(0) = c1 + 4
π
∞

k=1
1
(2k −1)(4 + (2k −1)2)
	
−(2k −1)

,
hence
c1 = 4
π
∞

k=1
1
(2k −1)(4 + (2k −1)2) ·
	
2k −1

.
By adding two infinite series together, the solution of the ODE is
y(t) = 4
π
∞

k=1
1
(2k −1)(4 + (2k −1)2)
×

(2k −1)e−2t −(2k −1) cos
	
(2k −1)t

+ 2 sin
	
(2k −1)t


.
Figure 9.19(a) and (b) show graphs of the approximation of the solution y(t) by a
finite sum
yN(t) ≜4
π
N

k=1
1
(2k −1)(4 +(2k −1)2)
×

(2k −1)e−2t −(2k −1) cos
	
(2k −1)t

+ 2 sin
	
(2k −1)t


.
0.4
y
y
0.2
–0.2
10
5
5
15
–0.4
0.4
0.2
–0.2
10
15
–0.4
(a)
(b)
FIGURE 9.19
Fourier analysis and oscillations in Example 9.13. (a) Approximate solution (dashed) y2(t) and exact solution and
(b) approximate solution (dashed) y8(t) and exact solution.

Fourier Series
765
In each picture the approximation is drawn as a dashed curve and the exact solution (see
Example 4.34) in Section 4.5 is drawn as a solid curve. In Figure 9.19a the approximation
uses N = 2 and in Figure 9.19b the approximation uses N = 8. The latter appears to be
correct to within 1%. Even with N = 2 the approximation is pretty good.
As we discussed in Example 5.41 in Section 5.8, the exact solution is not periodic
with period 2π but there is a periodic steady-state solution. The approximate solution
yN(t) is also not periodic with period 2π but the corresponding approximate steady-state
solution
yN,S(t) ≜4
π
N

k=1
1
(2k −1)(4 + (2k −1)2)
	
−(2k −1) cos
	
(2k −1)t

+ 2 sin
	
(2k −1)t


.
is close to the exact steady-state solution. ⃝
9.2.4 Problems
For problems 1–3, for the given functions that are defined* only on the interval given
or shown, (a) find the Fourier sine series representation, (b) find the Fourier cosine series
representation, and (c) sketch the graphs of those two series, each over an interval of length
exactly two periods.
1. The function f(x) = 1
2 −cos 2x defined for 0 < x < π.
2. The function whose graph is sketched in Figure 9.20.
3. The function whose graph is sketched in Figure 9.21.
For problems 4–6, for the given functions that are defined* only on the interval given or
shown, find the Fourier sine series representation and sketch the graph of the series over
an interval of length exactly two periods.
4. The function f(x) = cos 3x defined for 0 < x < π
2
5. The function defined* on [ 0, L ] by
f(x) =
 2x
0 < x < L
2
L −x
L
2 < x < L

.
f (x)
2
1
1
x
FIGURE 9.20
Function given in Problem 9.2.4.2.

766
Advanced Engineering Mathematics
f (x)
1
2
1
2
3
x
–1
FIGURE 9.21
Graph of function in Problem 9.2.4.3.
f (x)
1
x
1
2
FIGURE 9.22
Graph of function in Problems 9.2.4.6 and 9.2.4.9.
6. The function whose graph is sketched in Figure 9.22.
For problems 7–9, for the given functions that are defined* only on the interval given or
shown, find the Fourier cosine series representation and sketch the graph of the series over
an interval of length exactly two periods.
7. The function f(x) = sin x defined for 0 < x < π.
8. The function defined by
f(x) =
 2x
0 < x < L
2
L −x
L
2 < x < L

.
9. The function whose graph is sketched in Figure 9.22.
10. Suppose f(x) is “even about x = L
2,” that is, f(x + L
2) = f(x −L
2) for 0 < x < L, and
let -∞
n=1 bn sin
	 nπx
L

be its Fourier sine series. Explain why bn = 0 for all even n.

Fourier Series
767
In problems 11–13, use Fourier analysis to solve the IVP where f(t) is as in Example 9.13.
Give graphs of the exact solution and an approximate solution. [Note: Mathematica can
plot the graph of a function defined in terms of a definite integral from u = 0 to u = t, so
for the purpose only of plotting the graph of the solution it is not necessary to analyze the
convolution as in Example 4.34 in Section 4.5.]
11. ˙y + 3y = f(t), y(0) = 0.
12. ˙y + 3y = f(t), y(0) = 1.
13. ¨y + 2y = f(t), y(0) = 0, ˙y(0) = 1.
14. Suppose f(x) is an even function. (a) If g(x) is an even function, explain why f(g(x))
is even. (b) if g(x) is an odd function, explain why f(g(x)) is even.
9.3 Generalized Fourier Series
Given a function f defined* on the interval [ 0, L ] we saw that we can expand f in a Fourier
cosine series or a Fourier sine series. In this section we will both learn about other series
expansions and see how the Fourier sine series and Fourier cosine series expansions are
special cases of a general result. As a particularly useful application of the more gen-
eral result we will learn about some types of expansions that are used in solving Chapter
11 problems involving heat conduction in a one dimensional rod and problems in polar,
cylindrical, and spherical coordinates.
Expanding a function as a Fourier sine series
f(x) .=
∞

n=1
bn sin
nπx
L

, 0 < x < L,
(9.15)
or as a Fourier cosine series
f(x) .= a0
2 +
∞

n=1
an cos
nπx
L

, 0 < x < L,
(9.16)
are unified by both being half-range expansions based on Fourier series.
As we will see, both (9.15) and (9.16) are special cases of a method called “gener-
alized Fourier series expansions.” We unify all of these special cases using eigenvalue
problems for an ODE on an interval and boundary conditions (BCs) at the endpoints
of that interval. We began our study of eigenvalue problems in Section 2.1 when we
discussed eigenvalues of a matrix, and we will see how eigenvalue problems for an ODE-
BVP (boundary value problem) have some concepts that are analogous to those of the
eigenvalue problem for a matrix.
We will start with the special case that reiterates the Fourier sine series expansion.
Consider ODE-BVP
X′′(x) + λX(x) = 0, 0 < x < L
X(0) = X(L) = 0

.
(9.17)

768
Advanced Engineering Mathematics
By an eigenfunction of (9.17) we mean a function X = X(x) that (i) satisfies ODE X′′(x) +
λX(x) = 0 on the interval 0 < x < L, (ii) satisfies the boundary conditions X(0) = 0 and
X(L) = 0, and (iii) is not the identically zero function on 0 < x < L. Condition (iii) is
analogous to the requirement that a matrix’s eigenvector not be the zero vector. By an
eigenvalue of (9.17) we mean a value of λ for which (9.17) has an eigenfunction. This is
analogous to saying an eigenvalue of a matrix A is a value of λ for which the homogeneous
system of linear algebraic equations Ax −λx = 0 has an eigenvector, that is, a nonzero
solution vector x.
It will turn out that the terms sin
	 nπx
L

are eigenfunctions for (9.17), so Fourier sine series
are related to eigenvalue problem (9.17).
Example 9.14
Explain why ODE-BVP (9.17) has eigenvalues λn =
	 nπ
L

2 and corresponding eigenfunc-
tions Xn(x) = sin
	 nπx
L

, for n = 1, 2, . . .
Method: To find the eigenvalues, we must consider three separate cases: λ = 0, λ > 0,
and λ < 0.
Case 1: If λ = 0, then the differential equation X′′(x) + λX(x) = 0 is X′′(x) = 0, whose
solutions are X = c1 + c2x, for arbitrary constants c1, c2. Applying the first BC gives
0 = X(0) = c1 +c2 ·0 = c1, hence X = c2x. Applying the second BC gives 0 = X(L) = c2L,
so c2 = 0. So, X = c1 + c2x = 0 + 0 · x ≡0. When λ = 0, the ODE-BVP has only the trivial
solution for the function X(x). So, λ = 0 is not an eigenvalue for this problem.
Case 2: If λ > 0, it will turn out to be convenient to rewrite λ = ω2, where ω ≜
√
λ. Then
the differential equation X′′(x) + λX(x) = 0 is X′′(x) + ω2X(x) = 0, the undamped har-
monic oscillator differential equation of Section 3.3 whose solutions are X = c1 cos ωx +
c2 sin ωx, for constants c1, c2. Applying the first BC gives 0 = X(0) = c11 + c20 = c1. So,
X = c2 sin ωx. Applying the second BC gives 0 = X(L) = c2 sin(ωL), so either c2 = 0
or sin(ωL) = 0. We cannot allow c2 = 0 because then X = c2 sin ωx = 0 · sin ωx ≡0.
So, we have the trivial solution for the function X(x) unless ω satisfies the “characteristic
equation”
sin(ωL) = 0.
(9.18)
But, trigonometry tells us that there are infinitely many values of ω that make this true:
ω = nπ
L , any integer n.
Now, n = 0 is an integer but gives ω = 0, hence λ = 02, which is not allowed in Case
2. Additionally, while any integer n < 0, say n =−m, does give ω = −mπ
L
that satisfies
sin(ωL) = 0, it turns out that n < 0 gives no eigenfunction for X(x) beyond the ones we
get for n > 0. Why? Because, if n =−m then X(x) = sin ωx = sin
	 −mπx
L

= −sin
	 mπx
L

,
which essentially duplicates the eigenfunction sin
	 mπx
L

, and
	
−mπ
L

2 =
	 mπ
L

2.
The case λ > 0 gives eigenvalues λn =
	 nπ
L

2 , n = 1, 2, . . ., and corresponding
eigenfunctions Xn(x) = sin
	 nπx
L

, n = 1, 2, . . .
Case 3: If λ < 0, it will turn out to be convenient to rewrite λ = −ω2, where ω ≜√−λ.
Then the differential equation X′′(x) + λX(x) = 0 is X′′(x) −ω2X(x) = 0, whose solutions
are X = c1 cosh(ωx) + c2 sinh(ωx), for arbitrary constants c1, c2. Applying the first BC
gives 0 = X(0) = c11 + c20 = c1. So, X = c2 sinh(ωx). Applying the second BC gives
0 = X(L) = c2 sinh(ωL), so either c2 = 0 or sinh(ωL) = 0. We cannot allow c2 = 0
because then X = c2 sinh(ωx) = 0 sinh(ωx) ≡0. So, we have the trivial solution for the
function X(x) unless we insist that sinh(ωL) = 0. But, there is no value of ω that make

Fourier Series
769
1
0.5
–0.5
3
6
πω
3
9
12
15
18
ω
sin (
)
–1
FIGURE 9.23
Finding eigenvalues.
this true, because sinh(θ) > 0 for all θ > 0. So, Case 3 gives no further eigenvalue or
eigenfunction. ⃝
Shown in Figure 9.23 is a graph of y = sin(ωL) versus ω in a specific example, that of
L = π
3 . For this value of L that there are infinitely many roots ω of the characteristic equa-
tion, sin(ωL) = 0, specifically ω = 3, 6, 9, . . .
Recall the concept of a nice function from Definition 9.2 in Section 9.1. A nice function
automatically satisfies the square integrability condition
L
0
|f(x)|2dx < ∞.
(9.19)
Remark
Each nice function f(x) can be represented as Fourier sine series
f(x) .=
∞

n=1
bn sin
nπx
L

, 0 < x < L,
corresponding to BCs X(0) = X(L) = 0, where
bn =
L
0
f(x) sin
	 nπx
L

dx
L
0
| sin
	 nπx
L

|2dx
= 2
L
L
0
f(x) sin
nπx
L

dx.

770
Advanced Engineering Mathematics
The formula for the coefficients bn followed from the orthogonality relation
L
0
sin
nπx
L

sin
n′πx
L

dx = 0, for n ̸= n′.
9.3.1 Other Boundary Conditions
Now, instead of the boundary conditions X(0) = 0, X(L) = 0, there are three other simple
pairs of boundary conditions:
1. X′(0) = 0, X′(L) = 0
2. X(0) = 0, X′(L) = 0
3. X′(0) = 0, X(L) = 0.
It turns out that for each pair of boundary conditions, ODE X′′ + λX = 0, 0 < x < L gives
an infinite number of eigenvalues and corresponding eigenfunctions.
The eigenvalue problem X′′ + λX = 0, X′(0) = X′(L) = 0 gives eigenfunctions 1 and
cos
	 nπx
L

, n = 1, 2, . . ., and when we expand a nice function in those eigenfunctions we get
the familiar Fourier cosine series expansion
f(x) .= a0
2 +
∞

n=1
an cos
nπx
L

, 0 < x < L.
The results are summarized in Table 9.1, at the end of this section. In Problems 9.3.3.2
and 9.3.3.3 you will use some of the same kind of analysis as we did above.
The eigenvalue problem X′′ + λX = 0, X(0) = X′(L) = 0 gives eigenfunctions
sin

(n −1
2)πx
L

,
and when we expand a nice function in those eigenfunctions we get a general-
ized Fourier series expansion
f(x) .=
∞

n=1
cn sin

(n −1
2)πx
L

, 0 < x < L,
where
cn =
L
0
f(x) sin

(n−1
2 )πx
L

dx
L
0
.. sin

(n−1
2 )πx
L
 ..2 dx
= 2
L
L
0
f(x) sin

(n −1
2)πx
L

dx.

Fourier Series
771
TABLE 9.1
Eigenvalues and Eigenfunctions for X′′ + λX = 0
Boundary Conditions\Values of λ
λ>0
λ=0
λ<0
X(0) = X(L) = 0
λn =

nπ
L
2
,
None
None
Xn(x) = sin

nπx
L

, n = 1, 2, . . .
X′(0) = X′(L) = 0
λn =

nπ
L
2
,
λ0 = 0,
None
Xn(x) = cos

nπx
L

, n = 1, 2, . . .
X0(x)≡1
X(0) = X′(L) = 0
λn =

(n−1
2 )π
L
2
,
None
None
Xn(x) = sin

(n−1
2 )πx
L

, n = 1, 2, . . .
X′(0) = X(L) = 0
λn =

(n−1
2 )π
L
2
,
None
None
Xn(x) = cos

(n−1
2 )πx
L

, n = 1, 2, . . .
The eigenvalue problem X′′ + λX = 0, X′(0) =X(L) = 0 gives eigenfunctions
cos

(n −1
2)πx
L

,
and when we expand a nice function in those eigenfunctions we get a generalized Fourier
series expansion
f(x) .=
∞

n=1
dn cos

(n −1
2)πx
L

, 0 < x < L,
where
dn =
L
0
f(x) cos

(n−1
2 )πx
L

dx
L
0
.. cos

(n−1
2 )πx
L
 ..2 dx
= 2
L
L
0
f(x) cos

(n −1
2)πx
L

dx.
The results are summarized in Table 9.1.

772
Advanced Engineering Mathematics
So far, we’ve mentioned ODE X′′ + λX = 0 along with four choices of BCs, which give
Fourier sine series, Fourier cosine series, and two generalized Fourier series, respectively.
In Chapter 11 we will use each of these four types of expansions repeatedly to help solve
partial differential equations models.
In fact, this can be generalized even more, in anticipation of even more partial differ-
ential equation models. We’ll start with generalizing the boundary conditions: consider
ODE-BVP
X′′(x) + λX(x) = 0, 0 < x < L,
(9.20)
⎧
⎨
⎩
ϵ0X(0) −ϵ1X′(0) = 0,
γ0X(L) + γ1X′(L) = 0
⎫
⎬
⎭,
(9.21)
where ϵ0, ϵ1, γ0, γ1 are given real constants (scalars).
As we will discuss in Section 10.2, boundary conditions (9.21) include many interesting
physical models for problems of heat conduction in a thin rod. Let X(x) be the equilibrium
temperature distribution. Here are three examples:
1. If ϵ0 = 1 and ϵ1 = 0, the BC at x = 0 is X(0) = 0, which specifies the temperature
to be zero there.
2. If ϵ0 = 0 and ϵ1 = 1, the BC at x = 0 is X′(0) = 0, which specifies the heat flux to
be zero there, that is, specifies that the left end is insulated.
3. If ϵ0 = h and ϵ1 = 1, the BC at x = 0 is hX(0) = X′(0), which specifies that heat is
flowing out of the rod at a rate proportional to the amount that the temperature
exceeds 0◦C.
Theorem 9.6
For eigenvalue problem (9.20) and (9.21), there are infinitely many eigenvalues
λ1 < λ2 < λ3 < · · · →∞
(9.22)
and for each eigenvalue λn the only eigenfunctions are multiples of
Xn(x) = cos
/
λn x −δn

.
(9.23)
The positive eigenvalues are found by solving a characteristic equation
0 = (ϵ0γ0 −λ ϵ1γ1) sin
√
λ L

+ (ϵ0γ1 + ϵ1γ0)
√
λ cos
√
λ L

(9.24)
and the eigenfunctions satisfy the orthogonality relation
0 =
L
0
cos
/
λn x −δn

cos
/
λn′ x −δn′

dx, for n ̸= n′.
(9.25)

Fourier Series
773
Further, we have a generalized Fourier expansion for nice functions
f(x) .=
∞

n=1
An cos
/
λn x −δn

, 0 < x < L,
where
An =
L
0
f(x) cos
	√λn x −δn

dx
L
0
| cos
	√λn x −δn

|2 dx
.
Finally, at most one eigenvalue can be negative, and, if ϵ0, ϵ1, γ0, γ1 ≥0 and not both of ϵ0
and γ0 are zero, then all eigenvalues are positive.
As we’ll see in Section 10.2, the case where ϵ0, ϵ1, γ0, γ1 ≥0 is physically correct for heat
flow because heat tends to flow from a warm place to a cool place.
Example 9.15
Illustrate the results of Theorem 9.6, when ϵ0 = 0, ϵ1 = 1, γ0 = 0, γ1 = 1, that is, the BCs
which specify that both ends are insulated.
Method: In Problem 9.3.3.1, you will explain why for these BCs there is an eigenvalue
λ1 = 0 and there is no negative eigenvalue. The characteristic equation satisfied by all
positive eigenvalues is
0 = −λ sin
√
λ L

.
According to Theorem 9.6, the positive eigenvalues are
π
L
2
<
2π
L
2
< · · · →∞.
To find the corresponding eigenfunctions, substitute λ =
	 nπ
L

2 into the ODE-BVP: The
ODE becomes X′′(x) +
	 nπ
L

2 X(x), whose solutions are X(x) = c1 cos
	 nπx
L

+ c2 sin
	 nπx
L

,
which must also satisfy the BCs. Since X′(x) = nπ
L

−c1 sin
	 nπx
L

+c2 cos
	 nπx
L

 
, the BCs
imply 0 = X′(0) = nπ
L c2 and 0 = X′(L) = nπ
L (−c1 sin nπ + c2 cos nπ) = nπ
L (−1)nc2, either
of which yields c2 = 0. That leaves X = X(x) = c1
	
cos
	 nπx
L


, that is, X(x) is a multiple of
a single eigenfunction Xn(x) ≜cos

(n−1)πx
L

, for n = 2, 3, . . . .
Putting everything together, we summarize the results for this ODE-BVP: the eigen-
values are
0 <
π
L
2
<
2π
L
2
< · · · →∞
and the corresponding eigenfunctions are 1, cos
	 πx
L

, cos

2πx
L

, . . . . ⃝

774
Advanced Engineering Mathematics
Example 9.16
Illustrate the results of Theorem 9.6 for the ODE-BVP eigenvalue problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
X′′(x) + λX(x) = 0, 0 < x < L
hX(0) −X′(0) = 0,
X′(L) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(9.26)
where h > 0 is a given heat transfer coefficient.
Method: First, λ = 0 is not an eigenvalue. Why? If λ = 0, the solutions of ODE X′′+0·X =
0 are X = c1 +c2x. When this is substituted into the two BCs we get hc1 −c2 = 0 and c1 +
Lc2 = 0, which yields c1 = c2 = 0. Why? Because the determinant
....
h
−1
1
L
.... = hL + 1 > 0
follows from h > 0 and L > 0. Also, in Problem 9.3.3.4 you will explain why there is no
eigenfunction when λ < 0.
When λ > 0, the solutions of the ODE are X = c1 cos(
√
λ x) + c2 sin(
√
λ x). The BCs
require
hc1 −c2
√
λ = 0
and
√
λ

−c1 sin
√
λ L

+ c2 cos
√
λ L

= 0.
Because h > 0, the first equation gives c1 =
√
λ
h c2. Substituting that into the second
equation gives
0 =
√
λ

−
√
λ
h sin
√
λ L

+ cos
√
λ L

c2.
Because an eigenfunction must be not identically zero, we need c2 ̸= 0 in order to have
an eigenvalue. So, λ must satisfy the characteristic equation
0 = −
√
λ
h sin
√
λ L

+ cos
√
λ L

,
that is,
√
λ
h sin
√
λ L

= cos
√
λ L

.
Dividing through by cos(
√
λ L) and multiplying through by
h
√
λ gives
tan
√
λ L

=
h
√
λ
.
(9.27)
It is convenient to substitute θ =
√
λ L into (9.27) to get the equivalent equation
tan(θ) = hL
θ .
The graphical method of analysis says to graph the functions y = tan(θ) versus θ and
y = hL
θ versus θ on the same axes, so there are infinitely many points of intersection
θn, and in fact θn ≈(n −1)π, as n →∞. The eigenvalues of the original problem are
λn ≜

θn
L
2
.

Fourier Series
775
6
4
2
0
2
θ
4
6
8
10
–2
–4
–6
Tan (θ)
FIGURE 9.24
Graphical method for finding eigenvalues.
As a more specific example, consider the case where h = 1
4 and L = 2, as graphi-
cally analyzed in Figure 9.24. The picture suggests that the minimum eigenvalue, λ1, is
approximately 0.7. We proceed to get a better approximation by using Newton’s itera-
tive method for finding a root, discussed in Section 8.1. If we define f(θ) ≜θ tan θ −0.5
and take as our initial guess θ0 = 0.7, then Newton’s Method says to take
θn+1 ≜θn −f(θn)
f ′(θn) = θn −
θn tan θn −0.5
tan θn + θn sec2 θn
.
A calculator or spreadsheet gives successive iterates θ1 ≈0.656053888613, θ2 ≈
0.653281389255, θ3 ≈0.653271187232, θ4 ≈0.653271187094, θ5 ≈0.653271187094, so L = 2
gives the minimum eigenvalue λ1 =
 θ5
2
2
≈0.106690810972. We will revisit this in
Example 9.34 in Section 9.7.
In Problem 9.3.3.5, you will establish why all positive eigenvalues for (9.20) and (9.31)
satisfy the characteristic equation (9.24). ⃝
Table 9.1 gives results about the eigenvalues and eigenfunctions for ODE X′′(x) +
λX(x) = 0 for the four most common sets of separated homogeneous boundary condi-
tions. Each of the four groups of entries corresponds to a different set of physical boundary
conditions.
9.3.2 Periodic Boundary Conditions and the Full Fourier Series
Consider ODE-BVP eigenvalue problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
X′′(x) + λX(x) = 0, −π < x < π
X(−π) −X(π) = 0,
X′(−π) −X′(π) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(9.28)

776
Advanced Engineering Mathematics
The boundary conditions here are unlike the ones we have studied because each of
these two periodic boundary conditions mix the behavior of the function X(x) at two
points, ±π.
If λ = 0, the ODE in (9.28) becomes X′′(x) = 0, whose solutions are X(x) = c1 + c2x.
The first boundary condition requires c1 −c2π −(c1 + c2π) = X(−π) −X(π) = 0, hence
−2c2π = 0, hence c2 = 0. We see that X(x) ≡c1 satisfies both boundary conditions, so
X0(x) ≡1 is an eigenfunction corresponding to eigenvalue λ = 0.
If λ > 0, the ODE in (9.28) has solutions X(x) = c1 cos(
√
λ x) + c2 sin(
√
λ x). The first
boundary condition requires
0 = X(−π) −X(π) = c1 cos

−
√
λ π

+ c2 sin

−
√
λ π

−

c1 cos
√
λ π

+ c2 sin
√
λ π

,
hence −2c2 sin(
√
λ π) = 0, after using the facts that cosine is an even function and sine is
an odd function. Since X′(x) = −
√
λ c1 sin(
√
λ x) +
√
λ c2 cos(
√
λ x), the second boundary
condition requires
0 = X′(−π) −X′(π)
=
√
λ

−c1 sin

−
√
λ π

+ c2 cos

−
√
λ π

−

−c1 sin
√
λ π

+ c2 cos
√
λ π

,
hence 2c1 sin(
√
λ π) = 0. So, λ is an eigenvalue if, and only if, sin(
√
λ π) = 0. For n =
1, 2, . . ., λ = λn = n2 are eigenvalues, with corresponding eigenfunctions X(x) = c1 cos nx+
c2 sin nx, for arbitrary constants c1, c2.
Please check for yourself that λ < 0 does not provide any eigenvalues.
We may write “{cos nx, sin nx} are the eigenfunctions,” meaning that both cos nx and
sin nx are eigenfunctions corresponding to the same eigenvalue λn = n2.
So, from ODE-BVP (9.28) we get the full Fourier series expansion of a function on the
interval [−π, π].
9.3.3 Problems
Near the beginning of this section, we studied ODE-BVP (9.17), that is,
X′′(x) + λX(x) = 0, 0 < x < L
X(0) = X(L) = 0

.
Using the three cases λ = 0, λ > 0, and λ < 0, we explained why (9.18) has infinitely many
solutions for the eigenvalues λn =
	 nπ
L

2 , n = 1, 2, . . . and corresponding eigenfunctions
Xn(x) = sin
	 nπx
L

, n = 1, 2, . . ., and for no other values of λ is there a nontrivial solution
for X(x), that is, not identically zero function.
In problems 1–3 you will use a similar analysis of three cases to establish the other lines
of Table 9.1.
1. Establish the second group of entries of Table 9.1.
2. Establish the third group of entries of Table 9.1.
3. Establish the fourth group of entries of Table 9.1.

Fourier Series
777
4. Explain why ODE-BVP eigenvalue problem (9.26) has no eigenvalue λ < 0 by sub-
stituting into the two BCs the solutions of the ODE, in the form X = c1 cosh(ωx) +
c2 sinh(ωx), where ω ≜√−λ.
5. Use analysis similar to that of Example 9.16 to explain why all of the positive
eigenvalues for (9.20) and (9.21) satisfy the characteristic equation (9.24). Hint:
Substitute the general solution of the ODE into the two BCs (9.21) and write the
equations as a homogeneous system A
c1
c2

=
0
0

, where A is 2 × 2 matrix.
6. Find all eigenvalues λn and corresponding eigenfunctions Xn(x) for ODE-BVP
⎧
⎨
⎩
0 = X′′(x) + 2X′(x) + (λ + 1)X(x), 0 < x < L,
X(0) = 0, X(L) = 0
⎫
⎬
⎭.
7. Find all nonnegative eigenvalues λn and corresponding eigenfunctions Xn(x) for
ODE-BVP
⎧
⎨
⎩
0 = X′′(x) + 2X′(x) + (λ + 1)X(x), 0 < x < L,
X′(0) = X′(L) = 0.
⎫
⎬
⎭.
8. The function f(x) = sin x is defined only for 0 < x <
π
2 . Find the generalized
Fourier series for f(x) expanded in the functions sin
	
(2n −1)x

, n = 1, 2, . . .
9. The function f(x) = sin x is defined only for 0 < x < π. Find the generalized
Fourier series for f(x) expanded in the functions cos
	
(n −1
2)x

, n = 1, 2, . . .
10. Assume that h > 0 is a constant. Use a graphical analysis to explain the existence of
infinitely many eigenvalues for the problem X′′(x) + λX(x) = 0, X′(0) = 0, X(π) +
hX′(π) = 0.
11. Assume that μ > 0. For the ODE-BVP
(⋆)
⎧
⎨
⎩
X′′ + 2μX′ + 2μ2X(x) = 0, 0 < x < L
X(0) = 0, X′(L) = 0
⎫
⎬
⎭,
find an equation in μ whose solutions give eigenvalues for (⋆). Solve the equation,
that is, find the eigenvalues.
9.4 Complex Fourier Series and Fourier Transform
Recall that a function defined* and square integrable on the interval [ −L, L ] has Fourier
series representation (9.1) in Section 9.1, that is,
f(x) .= fs(x) = a0
2 +
∞

n=1
an cos
nπx
L

+
∞

n=1
bn sin
nπx
L

,

778
Advanced Engineering Mathematics
where
a0 = 1
L
L
−L
f(x)dx,
an = 1
L
L
−L
f(x) cos
nπx
L

dx,
bn = 1
L
L
−L
f(x) sin
nπx
L

dx.
Let us use what we know about complex functions:
cos θ = 1
2

eiθ + e−iθ
and
sin θ = 1
2i

eiθ −e−iθ
= i
2

−eiθ + e−iθ
,
the latter by using the fact that 1
i = −i. We can rewrite the Fourier series representation as
f(x) .= fs(x) = a0
2 + 1
2
∞

n=1
an
	
einπx/L + e−inπx/L
+ 1
2
∞

n=1
ibn
	
−einπx/L + e−inπx/L
= a0
2 + 1
2
∞

n=1
	
an −ibn

einπx/L + 1
2
∞

n=1
	
an + ibn

e−inπx/L.
Define complex numbers
c0 = 1
2 a0,
cn = 1
2
	
an −ibn

,
c−n = 1
2
	
an + ibn

.
We have the complex Fourier series representation
f(x) .= fC(x) =
∞

n=−∞
cn einπx/L.
(9.29)
Define an inner product on complex valued functions by

f(x), g(x)

≜
L
−L
f(x)g(x) dx,
(9.30)
where z denotes the complex conjugate of z. We have

einπx/L, eimπx/L
=
L
−L
einπx/L eimπx/L dx =
L
−L
einπx/L e−imπx/L dx =
L
−L
ei(n−m)πx/L dx.
For n ̸= m,

einπx/L, eimπx/L
=

ei(n−m)πx/L
(n −m)π/L
L
−L
=
L
(n −m)π
	
ei(n−m)π−e−i(n−m)π
=
L
(n −m)π
	
(−1)n−m −(−1)−(n−m)
= 0,

Fourier Series
779
because
(−1)−(n−m) = (−1)n−m
follows
from
	
(−1)n−m
2
=
1
for
all
integers
n, m.
On the other hand, if n = m, then

einπx/L, eimπx/L
=
L
−L
ei(n−n)πx/L dx =
L
−L
1 dx = 2L.
To summarize, {einπx/L}∞
n=1 is an orthogonal set of functions. Formally, that is, without
rigorous proof,

f(x), eimπx/L
=

∞

n=−∞
cn einπx/L, eimπx/L

=
∞

n=−∞
cn
2L,
n = m
0,
n ̸= m

= cm · 2L.
It follows that the complex Fourier series coefficients in (9.29) are given by
cn = 1
2L
L
−L
f(x) e−inπx/L dx = 1
2L
2L

0
f(ξ) e−inπξ/L dξ.
(9.31)
The latter follows from the result of Problem 9.1.8.22, which says that the integral is the
same over all intervals of length 2L. So,
f(x) .= fC(x) = 1
2L
∞

n=−∞
⎛
⎝
2L

0
f(ξ) e−inπξ/L dξ
⎞
⎠einπx/L
(9.32)
Example 9.17
Find the complex Fourier series of the function defined only by
f(x) =
sin 2x,
−π < x < 0
0,
0 < x < π

.
Method: Let L = π.
cn = 1
2π
π
−π
f(x) e−inx dx = 1
2π
0
−π
sin 2x e−inx dx = 1
2π
0
−π
1
2i
	
ei2x −e−i2x
e−inx dx,
that is,
cn =
1
4πi
0
−π
	
e−i(n−2)x −e−i(n+2)x
dx.
(9.33)
We have to consider separately the cases n = 2, n = −2, and |n| ̸= 2. For n = 2,
c2 =
1
4πi
0
−π
	
1 −e−i4x
dx =
1
4πi

x −
1
−i4e−i4x
0
−π
= · · · =
1
4πi · π = −i
4.

780
Advanced Engineering Mathematics
For n = −2,
c−2 =
1
4πi
0
−π
	
ei4x −1

dx = −c2 = i
4.
For |n| ̸= 2, (9.33) implies
cn =
1
4πi

e−i(n−2)x
−i(n −2) −e−i(n+2)x
−i(n + 2)
0
−π
.
Using i2 = −1,
cn = 1
4π

1
(n −2)
	
1 −(−1)−(n−2)
−
1
(n + 2)
	
1 −(−1)−(n+2)

= 1
4π
	
1 −(−1)n
·

1
n −2 −
1
n + 2

=
4
4π(n2 −4)
	
1 −(−1)n
.
For n = even,
	
1 −(−1)n
= 0; for n = odd = 2k + 1,
	
1 −(−1)n
= 2. So, the complex
Fourier series of f(x) is
f(x) .= fC(x) = i
4 ·

e−i2x −ei2x
+ 2
π
∞

k=−∞
1
(2k + 1)2 −4 ei(2k+1)x. ⃝
Example 9.18
Find the complex Fourier series of the function defined only by
f(x) = 3 −2 cos 2x + sin 5x, −π < x < π.
Method: Let L = π. Instead of calculating complex Fourier coefficients we can directly
write
f(x) = 3 −2 · 1
2

ei2x + e−i2x
+ i
2

−ei5x + e−i5x
,
that is,
f(x) = i
2 e−i5x −e−i2x + 3 −ei2x −i
2 ei5x. ⃝
Note that Example 9.18’s method couldn’t have been used in Example 9.17 because its
function f(x) is not simply sin 2x on the whole interval −π < x < π.
9.4.1 The Fourier Transform
Definition 9.3
A function f(x) is absolutely integrable on R if the improper integral
 ∞
−∞| f(x)| dx
converges.

Fourier Series
781
Definition 9.4
If f(x) is absolutely integrable on R then its Fourier transform is defined by the improper
integral
F(ω) ≜
1
√
2π
∞

−∞
f(x) e−iωx dx.
(9.34)
Note that F(ω) is a function of the parameter ω that appears in the integrand.
Theorem 9.7
(Fourier Inversion Theorem) If f(x) is absolutely integrable on R and is piecewise smooth
on every finite interval, then
f(x) .=
1
√
2π
∞

−∞
F(ω) eiωx dω,
(9.35)
in the sense that at every x0
f(x+
0 ) + f(x−
0 )
2
=
1
√
2π
∞

−∞
F(ω) eiωx0 dω.
(9.36)
If, in addition, f(x) is continuous at x0 then
f(x0) =
1
√
2π
∞

−∞
F(ω) eiωx0 dω.
Note that the conclusion of Theorem 9.7 at a point where f(x) has a finite jump
discontinuity is reminiscent of the result for convergence of Fourier series.
The Fourier transform and its inversion theorem are very useful for solving some physi-
cal problems, as we will see in Section 17.2. A reference for rigorous proofs for the existence
of the Fourier transform and its inversion theorem will be given in the “Learn More About
It” found at the end of this section.
In the real world we deal with finite, albeit indefinitely large intervals, so letting L →∞
for intervals [ −L, L ] motivates the Fourier transform on (−∞, ∞).
For fixed positive L, define
ωn = nπ
L
and
△ω ≜ωn+1 −ωn = π
L .

782
Advanced Engineering Mathematics
The complex Fourier series on [ −L, L ] given in (9.29) can be rewritten as
f(x) .= fC(x) =
∞

n=−∞
cn eiωnx
(9.37)
where
cn = 1
2L
L
−L
f(x) e−iωnx dx.
We will explain why (9.37) is a Riemann sum for the integral in (9.35): Rewrite (9.37) as
f(x) .= fC(x)=
∞

n=−∞
L
π · cn eiωnx△ω =
1
√
2π
∞

n=−∞
⎛
⎝√
2π · L
π · 1
2L
L
−L
f(x) e−iωnxdx
⎞
⎠eiωnx△ω,
that is,
f(x) .= fC(x) =
1
√
2π
∞

n=−∞
⎛
⎝
1
√
2π
L
−L
f(x) e−iωnx dx
⎞
⎠eiωnx △ω.
So,
f(x) .= fC(x) =
1
√
2π
∞

n=−∞
FL(ωn)eiωnx △ω,
(9.38)
where
FL(ω) ≜
1
√
2π
L
−L
f(x) e−iωx dx.
By definition of principal value,
lim
L→∞FL(ω) = P.V.
1
√
2π
∞

−∞
f(x) e−iωx dω = P.V. F(ω),
where F(ω), the Fourier transform, was defined in (9.34).
In a non-rigorous way, take the limit of (9.38), as L →∞, to get
f(x) .= fC(x) = P.V.
1
√
2π
∞

n=−∞
F(ωn)eiωnx △ω .
(9.39)

Fourier Series
783
Also in a non-rigorous way, take the limit of (9.39), as △ω →0+, to get
f(x) .=
1
√
2π
∞

−∞
F(ω)eiωx dω.
The above calculations motivate why we can think of the Fourier transform as a
continuous version of complex Fourier series.
Just as the symbol L stands for the Laplace transform operator and L−1 for the inverse
Laplace transform operator, we denote the Fourier transform operator and its inverse
operator by
F[ f(t)] = F(ω) ≜
1
√
2π
∞

−∞
f(t) e−iωtdt
(9.40)
and
f(t) = F−1[ F(ω)] ≜
1
√
2π
∞

−∞
F(ω) eiωtdt.
(9.41)
We caution that there are many slightly different definitions of the Fourier transform
and corresponding inverse transform in current use. For example, some authors dispense
with the factor
1
√
2π in the Fourier transform but then have a factor of
1
2π in the inverse
Fourier transform. Other authors replace e−iωt by e−i2πωt and eiωt by ei2πωt, in which case
they do not need the factor of
1
√
2π in either the Fourier or inverse Fourier transform.
Example 9.19
Let a be a positive constant. Find the Fourier transform of the finite pulse
f(t) =
⎧
⎨
⎩
1,
−a < t < a
0,
|t| > a
⎫
⎬
⎭.
Method: F[ f(t) ](ω) =
1
√
2π
 −a
−∞0 e−iωt dt+
 a
−a 1 e−iωtdt +
 ∞
a
0 e−iωtdt

, so for ω ̸= 0,
F[ f(t) ](ω) =
1
√
2π

e−iωt
−iω
a
−a
=
1
√
2π
· 2
ω · −1
2i

e−iaω −eiaω
=
0
2
π · sin(aω)
ω
.
For ω = 0,
F[ f(t) ](0) =
1
√
2π
a
−a
1 dt =
1
√
2π
%
t
&a
−a =
0
2
π · a.
So,
F[ f(t) ](ω) = a ·
0
2
π ·
⎧
⎪⎨
⎪⎩
sin(aω)
aω
,
ω ̸= 0
1,
ω = 0
⎫
⎪⎬
⎪⎭
. ⃝

784
Advanced Engineering Mathematics
Note that in Example 9.19, the function f(t) has a jump discontinuity but its Fourier
transform, the function F(ω), is continuous everywhere. This is why people say that the
operation of taking Fourier transform is “smoothing.”
Example 9.20
(Frequency shift) (a) For all absolutely integrable functions f(t) and constants a
explain why
(a) F[ eiat f(t) ](ω) = F(ω −a)
and
(b) F−1[ F(ω −a) ] = eiat f(t),
(9.42)
where F(ω) = F[ f(t) ](ω) is assumed to exist.
Method: (a) Because there exists F[ f(t) ](ω) =
1
√
2π
 ∞
−∞f(t) e−iωtdt, we calculate that
F[ eiat f(t) ](ω) =
1
√
2π
∞

−∞
f(t) eiat e−iωt dt =
1
√
2π
∞

−∞
f(t) e−i(ω−a)t dt = F(ω −a).
(b) Follows from taking the inverse transform of the result in part (a). ⃝
Example 9.21
Find the Fourier transform of an example of a finite wave train given by
f(x) =
⎧
⎨
⎩
sin 2x,
0 ≤x ≤3π
0,
all other x
⎫
⎬
⎭.
Method: Using a well-known indefinite integration formula for an exponential function
times a sine function, we calculate that, for ω ̸= ±2,
F[ f(x)](ω) ≜
1
√
2π
3π

0
sin 2x e−iωxdx =
1
√
2π

e−iωx
(−iω)2 + 22
	
−iω sin 2x −2 cos 2x

3π
0
=
1
√
2π
·
1
4 −ω2 ·

−2e−i3ωπ + 2

=
0
2
π · 1 −e−i3ωπ
4 −ω2
.
For ω = ±2,
F[ f(x)](±2) ≜
1
√
2π
3π

0
i
2

−ei2x + e−i2x
e∓i2x dx = · · · =
0
2
π ·

∓3πi
4

.
So
F[ f(x) ](ω) =
0
2
π ·
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1 −e−i3ωπ
4 −ω2
,
|ω| ̸= 2
∓3πi
4 ,
ω = ±2
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
. ⃝

Fourier Series
785
By the way, using L’Hôpital’s Rule we can check that the Fourier transform in Example
9.21 is continuous at ω = ±2.
Theorem 9.8
Suppose a is a positive constant and F(ω) = F[ f(x) ](ω) exists. Then
F[ f(ax) ](ω) = 1
a F
ω
a

and
F−1 %
F
ω
a
 &
(x) = a F−1[ F(ω) ](x).
(9.43)
Why? A non-rigorous explanation, that is, one that does not consider issues of conver-
gence, uses the change of variables y = ax to get
F[f(ax)](ω) ≜
1
√
2π
∞

−∞
f(ax)e−iωx dx = · · · =
1
√
2π
∞

−∞
f(y)e−iω(y/a)(a−1dy) = a−1F
ω
a

. 2
The next result doubles the usefulness of tables of Fourier transforms because we can
reverse the roles of Fourier transform and inverse Fourier transform. This was not possible
for the Laplace transform.
Theorem 9.9
(a) F−1[ F(ω) ](x) = F[ F(ω) ](−x)
and
(b) F[ f(x) ](ω) = F−1[ f(x) ](−ω),
(9.44)
where F[ f(x) ](ω) and F[ F(ω) ](x) are assumed to exist.
Why? The only difference in form between the definitions of F and F−1 is e−iωx
versus eiωx. 2
9.4.2 Convolution
Define convolution on the real line by
(f ∗g)(x) =
1
√
2π
∞

−∞
g(x −ξ) f(ξ) dξ.
Note that, in general, rigorous calculation of the improper integral in a convolution
involves limiting processes.
For any constant a, define a function for −∞< t < ∞by
Step(t −a) ≜
⎧
⎨
⎩
1,
t > a
0,
t < a
⎫
⎬
⎭.
(9.45)

786
Advanced Engineering Mathematics
Note that the function step(t −a), as opposed to Step(t −a), is defined only for 0 ≤t < ∞.
We used step(t −a) in the context of Laplace transforms in Chapter 4.
Example 9.22
Find Step(t −a) ∗Step(t −b), assuming a and b are constants with a < b.
Method: Step(t −a) ∗Step(t −b) =
1
√
2π
 ∞
−∞Step
	
(t −ξ) −a) Step(ξ −b) dξ
=
1
√
2π
∞

−∞
⎧
⎨
⎩
1,
t −a −ξ > 0
0,
t −a −ξ < 0
⎫
⎬
⎭
⎧
⎨
⎩
1,
ξ −b > 0
0,
ξ −b < 0
⎫
⎬
⎭dξ
=
1
√
2π
∞

−∞
⎧
⎨
⎩
1,
ξ < t −a
0,
ξ > t −a
⎫
⎬
⎭
⎧
⎨
⎩
1,
ξ > b
0,
ξ < b
⎫
⎬
⎭dξ =
1
√
2π
∞

−∞
⎧
⎨
⎩
1,
b < ξ < t −a
0,
ξ > t −a or ξ < b
⎫
⎬
⎭.
We analyze this by considering two cases.
Case (1): If b < t −a, that is, t > a + b, then
Step(t −a) ∗Step(t −b) =
1
√
2π
t−a

b
1 dξ =
1
√
2π
·
	
(t −a) −b

.
Case (2): If b > t −a, that is, t < a + b, then Step(t −a) ∗Step(t −b) =
1
√
2π
 t−a
b
0 dξ = 0.
So,
Step(t −a) ∗Step(t −b) =
1
√
2π
	
t −(a + b)

Step
	
t −(a + b)

. ⃝
(9.46)
By the way, the function on the right-hand side of (9.46) is called a ramp function. A
graph of such a function is shown in Figure 9.25.
Theorem 9.10
(Convolution)
F[ (f ∗g)(x) ] = F[ f(x) ] · F[ g(x) ]
and
(f ∗g)(x) = F−1[ F(ω) G(ω) ],
(9.47)
assuming F(ω) ≜F[ f(x) ](ω) and G(ω) ≜F[ g(x) ](ω) exist.
f(t)
t
a + b
FIGURE 9.25
Example 9.22: Ramp function.

Fourier Series
787
TABLE 9.2
Fourier Transforms: a > 0, β > 0, k > 0, τ > 0, c, t0 Are Nonzero Constants
F(ω) = F[ f(x) ]
F.1
f ′(x)
iωF(ω)
F.2
eiat f(t)
F(ω −a)
F.3
f(t) =
⎧
⎪⎨
⎪⎩
1,
−a < t < a
0,
|t| > a
⎫
⎪⎬
⎪⎭
a ·
1
2
π ·
⎧
⎪⎪⎨
⎪⎪⎩
sin(aω)
aω
,
ω ̸= 0
1,
ω = 0
⎫
⎪⎪⎬
⎪⎪⎭
F.4
sinc(t) ≜sin t
t
, t ̸= 0
1
π
2 ·
⎧
⎪⎨
⎪⎩
1,
−1 < ω < 1
1
2,
ω = ±1
0,
|ω| > 1
⎫
⎪⎬
⎪⎭
F.5
1
a2+t2
1
π
2 · e−|aω|
|a|
F.6
e−|ax|
|a|
1
2
π ·
1
a2+ω2
F.7
e−x2/2
e−ω2/2
F.8
f(ax)
1
a F
	 ω
a

F.9
e−βx2
1
√
2β · e−ω2/(4β)
F.10
f(t −t0)
e−iωt0 F(ω)
F.11
e−at Step(t), where a > 0
1
√
2π
1
a+iω
F.12
(f ∗g)(t)
F(ω)G(ω)
F.13
f(x) cos kx
1
2
	
F(ω −k) + F(ω + k)

F.14
f(x) sin kx
1
i2
	
F(ω −k) −F(ω + k)

F.15
f(x) =
⎧
⎪⎨
⎪⎩
sin kx,
0 ≤x ≤τ
0,
all other x
⎫
⎪⎬
⎪⎭
1
2
π ·
1
k2−ω2
	
e−iωτ 	
−iω sin kτ −k cos kτ

+ k

You will give a non-rigorous explanation for Theorem 9.10, that is, Table 9.2 entry F.12,
in Problem 17.1.2.8.
Note that the function Step(t−a) is not absolutely integrable on the real line so its Fourier
transform is not defined. So we could not use Theorem 9.10 to do Example 9.22. However,
the mathematical subject of the “theory of distributions” (Friedman 1991) can make rigor-
ous use of Fourier transforms of functions that are not absolutely integrable as well as the
“delta” function.

788
Advanced Engineering Mathematics
Theorem 9.11
(Time delay)
F[ f(t −t0) ](ω) = e−iωt0F[ f(t) ](ω),
(9.48)
where F[ f(t) ](ω) is assumed to exist.
Learn More About It
A mathematically rigorous proof of the Fourier inversion theorem, Theorem 9.7, is
found in Complex Variables, 2nd edn., by Stephen D. Fisher, Dover Publications c⃝1999.
9.4.3 Problems
In problems 1–4 find the complex Fourier series of the given function defined* only on the
given interval.
1. f(x) =
⎧
⎨
⎩
cos 3x,
−π < x < 0
0,
0 < x < π
⎫
⎬
⎭
2. f(t) =
⎧
⎨
⎩
−1,
0 ≤t ≤4
2,
4 ≤t ≤6
⎫
⎬
⎭
3. f(x) = 1 −i3 sin 2x + 1
2 cos 5x, −π < x < π.
4. f(t) = eiπt/2, −1 ≤t ≤1.
In problems 5–8 find the Fourier transform of the given function.
5. f(t) = Step(t −1) −Step(t −3), where Step(t −a) was defined in (9.45).
6. f(t) ≜
⎧
⎨
⎩
t,
0 < t < 1
2 −t,
1 < t < 2
⎫
⎬
⎭
7. f(t) ≜
⎧
⎨
⎩
e−αt sin(ω0t),
t ≥0
0,
−∞< t < 0
⎫
⎬
⎭, where α is a positive constant.
8. f(t) ≜
⎧
⎨
⎩
1
ν
	
1 + cos(2πνt)

,
|t| < 1
2ν
0,
all other t
⎫
⎬
⎭, where ν is a positive constant.
9. Suppose a is a positive constant. Use the definition of Fourier transform to
explain table entry F.11 in Table 17.1 in Section 17.1, that is, why for a > 0,
F[ e−at Step(t) ] =
1
√
2π
1
a + iω.

Fourier Series
789
10. Find the Fourier transform of tf(t) in terms of the Fourier transform of the
unspecified function f(t).
11. Find the inverse Fourier transform of 1−e−i5πω
4−ω2
.
In problems 12–14 use the method of Theorem 9.8 and/or the results of previous examples
to find the Fourier or inverse Fourier transform of the given function.
12. F
+
f(t) ](ω), where
f(t) ≜
⎧
⎨
⎩
sin x,
0 ≤x ≤6π
0,
all other x
⎫
⎬
⎭
13. F−1+ sin bω
ω
,
, where b is a nonzero constant
14. F
+
e−x2 ,
.
15. Recall that the finite pulse
p(t) ≜
⎧
⎨
⎩
1,
−1 < t < 1
0,
all other t
⎫
⎬
⎭
has F[ p(t) ] =
1
2
π · sin ω
ω . Use Theorem 9.11 and a geometric series to help find the
Fourier transform of
g(t) ≜
∞

n=1
e−np(t −2n).
16. Use the results of Example 9.20 and Theorem 9.9 to explain why Theorem 9.10 is
correct.
17. Use Table 9.2’s entry F.2 to explain why Table 9.2’s entries (a) F.13 and (b) F.14 are
true.
18. Explain why the Fourier transform of the function f(t) ≜
⎧
⎨
⎩
1,
−b < t < 0
−1,
0 < t < b
0,
|t| > b
⎫
⎬
⎭is
F[ f(t) ](ω) =
0
2
π ·
⎧
⎪⎨
⎪⎩
cos(ωb) −1
iωb
,
ω ̸= 0
0,
ω = 0
⎫
⎪⎬
⎪⎭
.
19. Use the result of Problem 9.4.3.18 to find the Fourier transform of g(t)
≜
cos(bt) −1
bt
.

790
Advanced Engineering Mathematics
9.5 Discrete Fourier and Fast Fourier Transforms
Suppose f(t) is periodic with period N, where N is a positive, even integer. Define the
finite Fourier transform of f(t) to be
F(N)[ f(t) ](ω) ≜
1
√
N
N
0
f(t)e−i2πt ω/N dt.
(9.49)
Let L = N
2 . We recall that a complex Fourier series on interval
%
−N
2 , N
2
&
, or on [0, N], is
given by
f(t) .=
∞

n=−∞
cnei2πt n/N,
where the coefficients are
cn = 1
N
N
0
f(t)e−i2πt n/N dt =
1
√
N
F(N)[ f(t) ](n), n = 0, ±1, ±2, . . . .
So, the finite Fourier transform is related to a complex Fourier series, with inversion
theorem resulting in
f(t) .=
1
√
N
∞

n=−∞
F(N)[ f(t) ](n)ei2πt n/N.
If we replace the integral in (9.49) by a Riemann sum later we will get a “discrete Fourier
transform” in (9.50) below. For any positive integer N, partition the interval 0 ≤t ≤N into
N equal subintervals of length △t = 1:
0 = t0 < t1 < · · · < tN−1 < tN = N.
Given a function f(t) defined on the interval [0, N], denote
fk = f(tk), k = 0, 1, . . . , N −1,
and form the vector
f ≜
⎡
⎢⎢⎢⎣
f0
f1
...
fN−1
⎤
⎥⎥⎥⎦,
which lies in CN, or in RN if f(t) is real-valued.

Fourier Series
791
We may also consider vectors to be sequences, that is, we may write
f =
4
fk
5N−1
k=0 .
Another name for a finite or infinite sequence of function values is a time series.
We define F = DFT[ f ] to be the vector F ≜
⎡
⎢⎢⎢⎣
F0
F1
...
FN−1
⎤
⎥⎥⎥⎦, whose components are given by
Fℓ≜
1
√
N
N−1

k=0
fk e−i2πkℓ/N, for ℓ= 0, 1, . . . , N −1,
(9.50)
which is called the analysis equation. This defines the Discrete Fourier Transform (DFT).
Note that Fℓis a Riemann sum approximation for F(N)[ f(t) ](ℓ), with sampling at left
end points.
Note that
e−i2πkℓ/N =

e−i2π/Nkℓ
= ω−kℓ,
where ω ≜ei2π/N is an “N-th root of unity,” that is, satisfies ωN = 1. So,
F = DFT[ f ] =
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
f0
+f1+
· · ·
+fN−1
f0
+ω−1f1+
· · ·
+ω−(N−1)fN−1
f0
+ω−2f1+
· · ·
+ω−2(N−1)fN−1
.
.
.
f0
+ω−(N−1)f1+
· · ·
+ω−(N−1)(N−1)fN−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
f
=
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
.
.
.
1
1
ω−1
.
.
.
ω−(N−1)
1
ω−2
ω−2(N−1)
.
.
.
.
.
.
.
.
.
1
ω−(N−1)
.
.
.
ω−(N−1)(N−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
f.
Implicitly this defines a symmetric matrix
F = [Fℓk]
0 ≤ℓ≤N −1
0 ≤k ≤N −1
≜
1
√
N
[ωkℓ]
0 ≤ℓ≤N −1
0 ≤k ≤N −1
=

792
Advanced Engineering Mathematics
=
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
.
.
.
1
1
ω1
ω2
ωN−1
1
ω2
ω4
ω2(N−1)
.
.
.
.
.
.
.
.
.
1
ωN−1
ω2(N−1)
.
.
.
ω(N−1)(N−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Because ω−1 = e−i2π/N is the complex conjugate of ω, the discrete Fourier transform is
given by
F = DFT[ f ] = F∗f,
where F∗≜
	
FT
, that is, the complex conjugate of the transpose of F is called the Hermitian
conjugate of the matrix F. (In Section 2.10 we called this the adjoint operator.)
The discrete Fourier transform defines a map f →F on Cn. In the subject of discrete
systems theory, the map (9.50), that is, f →F∗f, is an example of a digital filter.
An alternative way of writing F takes advantage of the fact that ωN = 1, hence ω2(N−1) =
ωN−2, ω3(N−1) = ωN−3, etc., so
F =
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
.
.
.
1
1
ω1
ω2
ωN−1
1
ω2
ω4
ωN−2
.
.
.
.
.
.
.
.
.
1
ωN−1
ωN−2
.
.
.
ω
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(9.51)
F has many interesting properties:
Theorem 9.12
FT = F,
	
F∗
T = F∗,
and
F = F∗.
Example 9.23
Explain why F is a unitary matrix, that is,
FF∗= F∗F = I.
(9.52)
The latter is the generalization to complex matrices of the definition of “orthogonal
matrix.”
Method: We want to explain why the dot product of the ℓ-th row of F with the k-column
of F∗is 1 if k = ℓand 0 if k ̸= ℓ. That dot product is

Fourier Series
793
1
√
N
⎡
⎢⎢⎢⎢⎢⎣
1
ωℓ
ω2ℓ
...
ωℓ(N−1)
⎤
⎥⎥⎥⎥⎥⎦
•
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎣
1
ω−k
ω−2k
...
ω−k(N−1)
⎤
⎥⎥⎥⎥⎥⎥⎦
= 1
N
N−1

n=0
ωℓn ω−kn = 1
N
N−1

n=0
ω(ℓ−k)n = 1
N
N−1

n=0

ωℓ−kn
.
(9.53)
If ℓ= k, then (9.53) reduces to 1
N · N = 1. If ℓ̸= k, then ω(ℓ−k) ̸= 1 so (9.53) is a geometric
series whose sum is
1
N ·
1 −

ωℓ−kN
1 −ωℓ−k
= 1
N ·
1 −1
1 −ωℓ−k = 0,
because ωℓ−k is also an N-th root of unity. ⃝
From (9.52), that is, the conclusion of Example 9.23, we conclude that
F−1 = F∗.
This in turn implies that
F F = f,
(9.54)
which expresses the inverse discrete transform F →f in terms of the matrix F.
While (9.50) is called the analysis equation, (9.54), that is,
fk =
1
√
N
N−1

ℓ=0
Fℓei2πkℓ/N, for k = 0, 1, . . . , N −1,
(9.55)
is called the synthesis equation.
Example 9.24
When N = 2,
F =
1
√
2
1
1
1
eiπ

=
1
√
2
1
1
1
−1

is real and an orthogonal matrix. A related matrix is
H2 ≜
√
2 F =
1
1
1
−1

,
which satisfies H2HT
2 = HT
2 H2 = 2I = N ·I and has all of its entries being ±1. H2 is called
the Hadamard matrix of order two. The question of which higher order Hadamard
matrices exist is an open subject and has great implications for coding theory.

794
Advanced Engineering Mathematics
Example 9.25
For general N find the discrete Fourier transform of cos

4πt
N

.
Method: The problem as stated has some ambiguity because the function f(t) ≜cos

4πt
N

is periodic with period being any positive integer multiple of N/2, including 2N/2, 3N/2,
etc. The simplest choice is to make the N in the definition of f(t)’s denominator be the
same N as in the definition of the discrete Fourier transform.
It helps to use Euler’s formula to rewrite
f(t) = 1
2

ei4πt/N + e−i4πt/N
and thus
fk = f(k) = 1
2

ei4πk/N + e−i4πk/N
= 1
2

ω2k + ω−2k
,
where ω ≜ei2π/N.
Because 1 = ωN = ωm ωN−m, for any positive integer m, we have a “circularity” or
“aliasing” property:
ω−m = ωN−m .
(9.56)
It follows that
fk = 1
2

ω2k + ωN−2k
.
So,
f =
⎡
⎢⎢⎢⎣
f0
f1
...
fN−1
⎤
⎥⎥⎥⎦= 1
2
⎡
⎢⎢⎢⎣
1
ω2
...
ω2(N−1)
⎤
⎥⎥⎥⎦+ 1
2
⎡
⎢⎢⎢⎣
1
ωN−2
...
ω(N−2)(N−1)
⎤
⎥⎥⎥⎦=
√
N
2

F(2) + F(N−2)
,
(9.57)
where we denote the columns of the matrix F by F =
+
F(0)
F(1)
· · ·
F(N−1),
Denote the columns of the N × N identity matrix by I =
+
d(0)
d(1)
· · ·
d(N−1),
.
The usual notations for the columns of the identity matrix are e(1), e(2), . . . , e(N), so d(k) =
e(k+1).
By orthonormality of the columns of F, the discrete Fourier transform of f(t) is
F = DFT[ f ] = F∗
√
N
2
F(2) +
√
N
2
F(N−2)

=
√
N
2

d(2) + d(N−2)
=
√
N
2
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1
0
...
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
that is, F2 = FN−2 =
√
N
2
and Fℓ= 0 for all other indices ℓ. ⃝
The fact that a sinusoidal of frequency 2, in units of 2π
N , has DFT with nonzero second
and (N −2)th components is an example of aliasing.

Fourier Series
795
Other commonly used “wrap around” or circularity notations define F−ℓ≜FN−ℓ,
F(−ℓ) ≜F(N−ℓ), and d(−ℓ) ≜d(N−ℓ), for ℓ= 0, 1, . . . , N. Using the latter notation, we see that
F = F∗
√
N
2
F(2) +
√
N
2
F(−2)

=
√
N
2

d(2) + d(−2)
when f is the discrete vector of values of f(t) = 1
2
	
e2(i2πt/N) + e−2(i2πt/N)
.
As a last comment about Example 9.25, finding the DFT of a sinusoidal whose frequency
is a multiple of the basic frequency is very similar to the problem of finding the Fourier
series of a sinusoidal whose frequency is a multiple of the basic frequency. In that special
case we were able to find the Fourier coefficients without calculating integrals. In Example
9.25 we were able to find the DFT components without calculating the summations in the
definition in the analysis equation, (9.50).
We caution that there are many slightly different definitions of the DFT in current use.
For example, instead of defining it as in the analysis equation (9.50), some authors leave
out the factor of
1
√
N in Fℓ≜
1
√
N
-N−1
k=0 . . . but then have a factor of 1
N instead of
1
√
N in
the synthesis equation, fk = · · · . Yet other authors consider sequences as being
4
5N
k=1
rather than
4
5N−1
k=0 , so they define the DFT by Fℓ≜
1
√
N
-N
k=1 fk e−i2π(k−1)(ℓ−1)/N, for ℓ=
1, . . . , N; again, they may also leave out the factor of
1
√
N.
Of course, electrical engineers may replace i ≜
√
−1 by j ≜
√
−1. This is one reason why
we do not use j as an index in our definitions in this section.
Example 9.26
Let f(t) be the square wave of period N given by
f(t) =
⎧
⎨
⎩
1,
0 ≤t < N
2
0,
N
2 ≤t < N
⎫
⎬
⎭.
(9.58)
If N is an even positive integer, find the analysis and synthesis of f(t).
Method: Unfortunately, there is no advantage in using the orthonormality of the columns
of the N × N matrix F as we did in Example 9.25. From (9.58),
fk = f(k) =
⎧
⎨
⎩
1,
k = 0, 1, . . . , N
2 −1
0,
k = N
2 , . . . , N
⎫
⎬
⎭,
so the analysis equation gives
Fℓ=
1
√
N
N
2 −1

k=0
1 · e−i2πkℓ/N, for ℓ= 0, 1, . . . , N −1.
For ℓ= 0, e−i2πℓ/N = 1, so e−i2πkℓ/N =
	
e−i2πℓ/N
k = 1 for k = 0, . . . , N
2 −1. We
calculate
F0 =
1
√
N
N
2 −1

k=0
1 · 1 =
1
√
N
· N
2 =
√
N
2 .

796
Advanced Engineering Mathematics
For ℓ= 1, . . . , N −1, e−i2πℓ/N ̸= 1, but we can still use the fact that e−i2πkℓ/N =
	
e−i2πℓ/N
k for k = 0, . . . , N
2 −1. We calculate
Fℓ=
1
√
N
N
2 −1

k=0
1 ·

e−i2πℓ/Nk
=
1
√
N
· 1 −
	
e−i2πℓ/N
N/2
1 −e−i2πℓ/N
=
1
√
N
·
1 −e−iπℓ
1 −e−i2πℓ/N
=
1
√
N
·
1 −(−1)ℓ
1 −e−i2πℓ/N =
2
√
N(1 −e−i2πℓ/N)
·
1,
ℓ= 1, 3, . . . , N −1
0,
ℓ= 2, 4, . . . , N −2

. ⃝
Example 9.27
A square wave voltage v(t) is periodic with period 8 ms and is given by
v(t) =
⎧
⎨
⎩
1,
0 ≤t < 0.004
0,
0.004 ≤t < 0.008
⎫
⎬
⎭volts.
(9.59)
Find V=DFT[v] =
+
V0
V1 · · · V7
,T and its power spectrum
+
|V0|2
|V1|2 · · · |V7|2,T.
Give graphs of (a) Re(DFT[ v ]) = {Re(Vℓ)}7
ℓ=0, (b) Im(DFT[ v ]) = {Im(Vℓ)}7
ℓ=0, and
(c) the power spectrum.
Method: Measure time, t, in milliseconds, so v(t) is periodic with period N = 8. Example
9.26 with N = 8 gives
V = DFT[ v ]
=
√
2
1
√
2(1 −ω−1)
0
1
√
2(1 −ω−3)
0
1
√
2(1 −ω−5)
0
1
√
2(1 −ω−7)
T
,
where ω−1 = e−i2π/8 = e−iπ/4 =
1
√
2 (1 −i). To be more explicit,
DFT[v] ≈
[1.414
.3536−i0.8536
0
.3536−i0.1464
0
.3536+i0.1464
0
.3536+i0.8536]T
and its real and imaginary parts are graphed in Figure 9.26a and b.
The power spectrum is the vector

2
1
2(2 −
√
2)
0
1
2(2 +
√
2)
0
1
2(2 +
√
2)
0
1
2(2 −
√
2)
T
≈[2
0.8536
0
0.1464
0
0.1464
0
0.8536]T
and is graphed in Figure 9.27. ⃝
Mathematica and MATLAB®
Mathematica’s command
W = Fourier[v, FourierParameters →{0, −1}]

Fourier Series
797
1.4
Re(Vℓ)
1.2
1.0
0.8
0.6
0.4
0.2
ℓ
Im(Vℓ)
0.5
–0.5
1
2
3
4
5
6
7
ℓ
1
(a)
(b)
2
3
4
5
6
7
FIGURE 9.26
Example 9.27 (a) and (b).
2.0
|Vℓ|2
1.5
1.0
0.5
ℓ
1
2
3
4
5
6
7
FIGURE 9.27
Example 9.27 (c) Power spectrum.
implements our definition of the DFT except that its DFT vector is indexed by 1, 2, . . . , 8,
that is, is
W = [ first number second number . . . eighth number ]
instead of our vector
V = DFT[ v ] = [ V0 . . . V7 ].
Basically, we used the Mathematica command
ListPlot[Abs[V2], PlotStyle →{Blue, PointSize[Large]},
Filling →Axis, FillingStyle →Red,
AxesLabel →{Text[Style["ℓ]", Italic, 18]],Text[Style["|Vℓ|2", Italic, 18]]}]
to get the plot in Figure 9.26, although we did make some small drawing changes for
clarity.

798
Advanced Engineering Mathematics
Example 9.28
In MATLAB, with N = 4, input the vector
a = [1, −0.6 −0.8i, −0.28 + 0.96i, 0.936 −0.352i]
The MATLAB command fft(a) produced
[ 1.0560 −0.1920i,
0.8320 + 0.5760i,
0.3840 + 2.1120i,
1.7280 −2.4960i ],
which is
√
N = 2 times the DFT as we have defined it. MATLAB leaves out the factor
1
√
N that is in our definition of DFT. ⃝
9.5.1 Convolution and Auto-Correlation
Given two sequences x =
4
xk
5N−1
k=0 and y =
4
yk
5N−1
k=0 we can form their discrete convolu-
tion, defined by
x ∗y =
4
(x ∗y)k
5N−1
k=0 ≜
1
√
N
N−1

m=0
xmyk−m.
(9.60)
We use the wrap-around convention that y−n ≜yN−n if n = 0, . . . , N.
Theorem 9.13
(a) x ∗y = y ∗x (b) If X = DFT[ x ] and Y = DFT[ y ] then the DFT of x ∗y is given by
DFT[ x ∗y ] =
4
XℓYℓ
5N−1
ℓ=0 .
Why? You will explain (a) in Problem 9.5.3.8. For (b), let z ≜x∗y and, as usual, ω ≜ei2π/N.
By (9.50) we calculate that for ℓ= 0, 1, . . . , N −1, the ℓth component of DFT[ z ] is
Zℓ≜
1
√
N
N−1

k=0
zk ω−kℓ=
1
√
N
N−1

k=0

1
√
N
N−1

m=0
xmyk−m

ω−kℓ.
Exchanging the order of finite summations and writing ω−kℓ= ω−mℓω−(k−m)ℓgives that
Zℓ=
1
√
N
N−1

m=0
xmω−mℓ·

1
√
N
N−1

k=0
yk−mω−(k−m)ℓ

=
1
√
N
N−1

m=0
xmω−mℓ· Yℓ
= Yℓ·
1
√
N
N−1

m=0
xmω−mℓ= YℓXℓ. 2

Fourier Series
799
Above, we used the change of index K = k −m and wrap-around to see that
1
√
N
N−1

k=0
yk−mω−(k−m)ℓ=
1
√
N
N−1

K=0
yKω−Kℓ= Yℓ.
The auto-correlation of a sequence x =
4
xk
5N−1
k=0 is defined to be the sequence z =
4
zk
5N−1
k=0
where
zk ≜
N−1

m=0
xmxm−k.
(9.61)
This is the discrete convolution of x with the complex conjugate of a reversed order
version of x.
Theorem 9.14
The DFT of the auto-correlation is the power spectrum.
Why? In terms of the sequence x, the DFT of its auto-correlation, z is given by
Zℓ=
1
√
N
N−1

k=0
zkω−kℓ=
1
√
N
N−1

k=0

1
√
N
N−1

m=0
xmxm−k ω−kℓ

=
1
√
N
N−1

m=0
xm ω−mℓ·

1
√
N
N−1

k=0
xm−k ω−(k−m)ℓ

=
1
√
N
N−1

m=0
xm ω−mℓ·

1
√
N
N−1

K=0
xK ωKℓ

,
where we defined K ≜m −k. But
Xℓ=
1
√
N
N−1

K=0
xK ω−Kℓ=
1
√
N
N−1

K=0
xK ωKℓ,
so the DFT of its auto-correlation, z, is given by
Zℓ=
1
√
N
N−1

m=0
xm ω−mℓ·

Xℓ

= Xℓ·
1
√
N
N−1

m=0
xm ω−mℓ= Xℓ· Xℓ= |Xℓ|2. 2
9.5.2 Fast Fourier Transform
Mathematicians found tricks for calculating the discrete Fourier transform. There are many
clever algorithms that improve the speed and memory storage requirements of the DFT.

800
Advanced Engineering Mathematics
Collectively these algorithms are called the Fast Fourier transform (FFT). The improve-
ments were so dramatic as to make feasible the solution of problems that otherwise would
have required so much computer processing time as to be effectively insoluble.
Nevertheless, it should be emphasized that the FFT produces the same end result as the
DFT would theoretically produce if it used sufficiently high precision arithmetic and were
allowed to run for a sufficiently long time. Of course, DFT would be useless if it takes
years to produce results that our technology needs to know in milliseconds.
Let N be a positive, even integer and denote ω = ei2π/N. We choose N to be even because
the fast part of the phrase FFT can come from breaking down calculations into even versus
odd parts.
Let a sequence x =
4
xk
5N−1
k=0 have DFT
X = DFTN[x ] =
4
Xℓ
5N−1
ℓ=0 , where Xℓ=
1
√
N
N−1

k=0
xk ω−kℓ,
which is a DFT of size N. Define the even part of x to be
y =
4
ym
5 N
2 −1
m=0 , where ym = x2m,
and define the odd part of x to be
z =
4
zm
5 N
2 −1
m=0 , where zm = x2m+1.
So, x consists of the interleaving of y and z:
x =
+
y0
z0
y1
z1
. . .
y N
2 −1
z N
2 −1
,T.
Correspondingly, each of the even and odd parts has a DFT of size N
2 : Because
e−i2π/(N/2) = ω−2.
we see that Y = DFTN/2[ y ] is given by
Yn ≜
1
/
N/2
N
2 −1

m=0
ym ω−2mn
and Z = DFTN/2[ z ] is given by
Zn ≜
1
/
N/2
N
2 −1

m=0
zm ω−2mn.

Fourier Series
801
The DFTN[ x ] = {Xℓ}N−1
ℓ=0 of the original sequence can be constructed from the DFTN/2 of
the even and odd parts: For ℓ= 1, . . . , N
2 −1,
Xℓ=
1
√
N
N−1

k=0
xk ω−kℓ=
1
√
2
⎛
⎜⎝
1
/
N/2
N
2 −1

m=0
x2m ω−2mℓ+
1
/
N/2
N
2 −1

m=0
x2m+1 ω−(2m+1)ℓ
⎞
⎟⎠
=
1
√
2
⎛
⎜⎝
1
/
N/2
N
2 −1

m=0
ym ω−2mℓ+ ω−ℓ·
1
/
N/2
N
2 −1

m=0
zm ω−2mℓ
⎞
⎟⎠=
1
√
2

Yℓ+ ω−ℓ· Zℓ

.
For ℓ= N
2 , . . . , N −1,
Xℓ=
1
√
2
·
1
/
N/2
⎛
⎜⎝
N
2 −1

m=0
x2m ω
−2m

ℓ−N
2

· ω−2m· N
2 + ω−ℓ·
N
2 −1

m=0
x2m+1 ω
−2m

ℓ−N
2

· ω−2m· N
2
⎞
⎟⎠.
But, ω−2m· N
2 = ω−mN =
	
ωN
−m = 1 for all integers m, so for ℓ= N
2 , . . . , N −1,
Xℓ=
1
√
2
·
⎛
⎜⎝
1
/
N/2
N
2 −1

m=0
x2m ω
−2m

ℓ−N
2

+ ω−ℓ·
1
/
N/2
N
2 −1

m=0
x2m+1 ω
−2m

ℓ−N
2

⎞
⎟⎠.
Using the fact that ω
N
2 = eiπ = −1, so ω−ℓ= −ω−ℓ−N
2 , we have that for ℓ= N
2 , . . . , N −1,
Xℓ=
1
√
2

Yℓ−N
2 −ω
−

ℓ−N
2

· Zℓ−N
2

.
Further, if N
2 is even, then we can break down y and z into their even and odd parts, etc.
This is an example of a “divide and conquer” strategy.
In summary,
Zℓ=
1
√
2
⎧
⎪⎨
⎪⎩
Yℓ+ ω−ℓ· Zℓ,
ℓ= 0, . . . , N
2 −1
Yℓ−N
2 −ω
−

ℓ−N
2

· Zℓ−N
2 ,
ℓ= N
2 , . . . , N −1
⎫
⎪⎬
⎪⎭
.
(9.62)

802
Advanced Engineering Mathematics
So, why is this fast? Most of the computer processing time done by the DFT or FFT is in
multiplications, and it turns out that when N is “large,” the FFT defined above requires
significantly fewer multiplications than does DFT.
First, we note that the DFT requires N2 multiplications.∗For each of ℓ= 0, 1, . . . , N −1,
we need N multiplications to calculate Zℓby the DFT defined in (9.50), so the DFT requires
N2 multiplications.
On the other hand, when N is a power of two we will use an inductive argument† to see
why calculating the FFT requires only 2N logN multiplications and is thus much faster than
the DFT when N is “large.”
Assume that it takes only 2 · N
2 · log N
2 multiplications to calculate each of Y = DFTN/2[ y ]
and Z = DFTN/2[ z ]. Using (9.62), calculating X = DFTN/2[ x ] requires 2 ·

2 · N
2 · log N
2

multiplications to get the Ym’s and the Zm’s and also N
2 multiplications to calculate the
products ω−ℓ·Zℓ, for ℓ= 0, . . . , N
2 −1. The latter are used to calculate Xℓfor ℓ= 0, . . . , N
2 −1
but can be stored to calculate Xℓfor ℓ= N
2 , . . . , N −1.
So, the number of multiplications needed to calculate the FFT is
2 ·

2 · N
2 logN
2

+ N
2 = 2N · (log N −log 2) + N
2 = 2Nlog N + (0.5 −2log2)N < 2Nlog N.
By inductive reasoning, when N is a power of two we have an upper bound on the number
of multiplications needed, although we do need to verify the beginning step that when
N = 2, the number of multiplications needed is two: when x = [x0
x1]T, its DFT is
[X0
X1]T where X0 =
1
√
2 (x0 −x1) and X1 =
1
√
2 (x0 + x1). So, when N = 2 the number of
multiplications needed is two, which is less than 2N · log N = 4log 2.
If N is not a power of two then some algorithms “pad” the vector x with zeros to create a
vector whose length is a power of two. At first it might appear paradoxical that increasing
the size of the problem of calculating the DFT makes it faster to do, but we saw how
efficient it is to calculate the FFT when the size is a power of two.
One particularly useful application of the FFT is evaluating a discrete convolution x ∗y
by an indirect method: By Theorem 9.13(a),
x ∗y = DFT−1+
{XℓYℓ}N−1
ℓ=0
,
.
The numbers of multiplications needed to calculate this are (a) 2Nlog N to get X =
DFT [ x ], (b) 2Nlog N to get y = DFT [ y ], (c) N to get {XℓYℓ}N−1
ℓ=0 , and (d) 2Nlog N to get
DFT−1+
{XℓYℓ}N−1
ℓ=0
,
, for a total of N + 6N · log N rather than the N2 to directly calculate the
discrete convolution x ∗y.
∗We neglect in the count the (N −1) multiplications needed to create the multipliers ω−kℓ. First, as we saw in
(9.51), we only need the powers ω−1, . . . , ω−(N−1), not ω−1, . . . , ω−(N−1)(N−1). Second of all, we can calculate
ω−1, . . . , ω−(N−1) by storing the successive products ω−2 = ω−1 · ω−1, ω−3 = ω−1 · ω−2, ..., ω−(N−1) =
ω−1 · ω−(N−2). Finally, we can calculate and store “offline” ω−1, . . . , ω−(N−1) just once at the beginning of
solving a problem.
† We are following the argument of (Cochran et al. 1967).

Fourier Series
803
Learn More About It
Perhaps the most cited mathematics paper is “An algorithm for the machine calcula-
tion of complex Fourier series,” J. W. Cooley and J. W. Tukey, Math. Comput. 19 (1965),
297–301. In addition to the paper by Cochran, et al. cited earlier, two of the many
good references to the details of FFT algorithms are Computational Frameworks for the
Fast Fourier Transform, by Charles Van Loan, S. I. A. M. Frontiers Appl. Mat, Volume
10, c⃝1992, and A First Course in Fourier Analysis, by David W. Kammler, Cambridge
University Press, c⃝2007. The former often gives “pseudo-code” for FFT algorithms.
Many other interesting properties of the matrix F are found in Phillip J. Davis, Circulant
Matrices, John Wiley & Sons, Inc., c⃝1979.
9.5.3 Problems
In problems 1–3 assume f(t) is periodic with period N. Find its DFT.
1. f(t) = sin

4πt
N

.
2. f(t) = cos

6πt
N

.
3. f(t) = at, for 0 ≤t ≤N and is extended to be periodic with period N; assume the
constant a satisfies a = e−i2πk0/N and k0 is a positive integer with 1 < k0 < N.
In problems 4–7 a graph or table of values of f(t) is given for 0 ≤t ≤4 ms. For both (a)
N = 4 (b) N = 8, (i) write down the vector of function values f ≜[f0
. . .
fN−1]T and
(ii) F = DFTN[ f(t) ]. You may use technology but document what technology you used by
stating what command you used and in what software package.
4. f(t) graphed in Figure 9.28.
5. f(t) graphed in Figure 9.29.
6. f(t) whose data is in Table 9.3.
7. f(t) whose data is in Table 9.4.
8. Explain why Theorem 9.13(a) is true.
9. Use the FFT algorithm by hand for the sequence
a = [1, −0.6 −0.8i, −0.28 + 0.96i, 0.936 −0.352i]
of Example 9.28.
10. Abstraction of Examples 9.27 and 9.28 suggest that if f =
4
fk
5N−1
k=0
is a real
sequence then

804
Advanced Engineering Mathematics
2.0
1.5
1.0
0.5
–0.5
–1
1
2
3
4
t
f(t)
–1.0
FIGURE 9.28
Problem 9.5.3.4.
2.0
1.5
1.0
0.5
–0.5
–1
1
2
3
4
t
f(t)
–1.0
FIGURE 9.29
Problem 9.5.3.5.
TABLE 9.3
Problem 9.5.3.6
t
0
0.5
1.0
1.5
2.0
2.5
3.0
3.6
4.0
f(t)
8
7
7
6
6
6
7
8
9

Fourier Series
805
TABLE 9.4
Problem 9.5.3.7
t
0
0.5
1.0
1.5
2.0
2.5
3.0
3.6
4.0
f(t)
2
3
2
2
0
−1
−2
−1
0
(a) Its DFTN[f ] = F =
4
Fℓ
5N−1
ℓ=0 symmetric in the sense that FN−ℓ= Fℓ, and
(b) Its power spectrum is symmetric, in the sense that | FN−ℓ| = | Fℓ| = | Fℓ|,
for ℓ= 0, . . . , N. [We used the wrap around convention that F−ℓ≜FN−ℓif
ℓ= 0, . . . , N.]
11. Establish the converse of Problem 9.5.3.10(a), that is, that if FN−ℓ= Fℓ, for ℓ=
0, . . . , N, where DFTN[f ] = F =
4
Fℓ
5N−1
ℓ=0 , then f is a real sequence. [We used the
“wrap around” convention that F−ℓ≜FN−ℓif ℓ= 0, . . . , N.]
9.6 Sturm–Liouville Problems
The ODE in (9.20) in Section 9.3 can be generalized in many ways that are useful for
engineering problems. We start with
Definition 9.5
(a) A Sturm-Liouville problem is given by an ODE in the form
	
p(x)X′(x)

′ + (λs(x) + q(x))X(x) = 0, a < x < b,
(9.63)
along with the familiar BCs
⎧
⎨
⎩
ϵ0X(a) −ϵ1X′(a) = 0,
γ0X(b) + γ1X′(b) = 0
⎫
⎬
⎭,
(9.64)
where ϵ0, ϵ1, γ0, γ1 are given constants (scalars). We will assume always that either
ϵ0 ̸= 0 or ϵ1 ̸= 0, and either γ0 ̸= 0 or γ1 ̸= 0.
(b) A Sturm-Liouville problem is regular if all of the functions p, p′, q, s are defined,
continuous, and real-valued on the interval [ a, b ], p(x) > 0 on [ a, b ], and s(x) ≥0
and is not identically zero on [ a, b ].
The results for a regular Sturm–Liouville problem are pretty much the same as for the
ODE-BVP eigenvalue problem with a much simpler ODE. It is unusual in any subject to
have such a successful generalization.

806
Advanced Engineering Mathematics
Theorem 9.15
For a regular Sturm-Liouville eigenvalue problem (9.63) and (9.64), there are infinitely
many eigenvalues
λ1 < λ2 < λ3 < · · · →∞,
and for each eigenvalue λn the only eigenfunctions are the nonzero multiples of a single
eigenfunction
Xn(x).
The eigenvalues are found by solving a characteristic equation and the eigenfunctions
satisfy an orthogonality relation
0 =
b
a
s(x)Xn(x)Xm(x)dx, for n ̸= m.
(9.65)
Further, for nice functions we have a generalized Fourier expansion
f(x) .=
∞

n=1
AnXn(x), a < x < b,
where
An =
b
a
s(x)Xn(x)f(x)dx
b
a
s(x)|Xn(x)|2dx
.
(9.66)
Finally, at most finitely many eigenvalues can be negative, and, if ϵ0, ϵ1 > 0, γ0, γ1 ≥0 and
q(x) ≤0 on the interval [ a, b ], then all eigenvalues are positive.
Except for Example 9.29, we will not give most of the explanations for Theorem 9.15.
Example 9.29
Explain the orthogonality relation (9.65).
The orthogonality relation also helps explain the formula for the coefficients An in
(9.66).
Method:
Suppose λn and λm are two distinct eigenvalues, with corresponding eigen-
functions Xn(x) and Xm(x). From the ODE, with λ replaced by λn and X(x) replaced by
Xn(x), we get
	
p(x)X′
n(x)

′ + (λns(x) + q(x))Xn(x) = 0, a < x < b,
(9.67)

Fourier Series
807
and similarly we get
	
p(x)X′
m(x)

′ + (λms(x) + q(x))Xm(x) = 0, a < x < b.
Multiply the first ODE, that is, (9.67), by Xm(x), and subtract from that Xn(x) times the
second ODE to get
0 = Xm(x)
	
p(x)X′
n(x)

′ + λnXm(x)s(x)Xn(x) −Xn(x)
	
p(x)X′
m(x)

′ −λmXn(x)s(x)Xm(x).
Integrate from a to b to get
0 =
b
a
0 dx =
b
a

Xm(x)
	
p(x)X′
n(x)

′ −Xn(x)
	
p(x)X′
m(x)

′
dx
+ (λn −λm)
b
a
s(x)Xn(x)Xm(x) dx.
(9.68)
For the first term, integration by parts gives
b
a
Xm(x)
	
p(x)X′
n(x)

′ dx =
%
Xm(x)
	
p(x)X′
n(x)

&b
a −
b
a
X′
m(x)
	
p(x)X′
n(x)

dx
= Xm(b)p(b)X′
n(b) −Xm(a)p(a)X′
n(a) −
b
a
p(x)X′
m(x)X′
n(x) dx,
and similarly
−
b
a
Xn(x)
	
p(x)X′
m(x)

′ dx = −Xn(b)p(b)X′
m(b) + Xn(a)p(a)X′
m(a) +
b
a
p(x)X′
n(x)X′
m(x) dx.
So, in (9.68), after canceling two terms of
 b
a p(x)X′
n(x)X′
m(x)dx and combining the terms
evaluated at x = b and x = a, we have
0 = p(b)
	
Xm(b)X′
n(b) −Xn(b)X′
m(b)

−p(a)
	
Xm(a)X′
n(a) −Xn(a)X′
m(a)

+ (λn −λm)
b
a
s(x)Xn(x)Xm(x) dx.
(9.69)
We claim that each of the two terms involving evaluations at x = b or x = a cancel.
Consider two cases: If in the boundary condition at x = b we have γ1 = 0, then to satisfy
the BC there we get 0 = γ0Xn(b) + 0 · X′
n(b) = γ0Xn(b), hence Xn(b) = 0 and likewise
Xm(b) = 0, so p(b)
	
Xm(b)X′
n(b) −Xn(b)X′
m(b)

= p(b)(0 · X′
n(b) −X′
n(b) · 0 = 0. On the
other hand, if γ1 ̸= 0, then the BC that Xn satisfies at x = b yields X′
n(b) = −γ0
γ1
Xn(b),
and similarly X′
m(b) = −γ0
γ1
Xm(b), so
Xm(b)X′
n(b) −Xn(b)X′
m(b) = 
Xm(b)

−γ0
γ1
Xn(b)

−
Xn(b)

−γ0
γ1
Xm(b)

= 0.
Similarly,
Xm(a)X′
n(a) −Xn(a)X′
m(a) = 0.

808
Advanced Engineering Mathematics
So our claim is correct. So, (9.69) reduces to
0 = (λn −λm)
b
a
s(x)Xn(x)Xm(x) dx.
Because λn ̸= λm, we can divide through by (λn −λm) to get the orthogonality relation
(9.65). ⃝
9.6.1 Other Sturm–Liouville Problems
There are other kinds of Sturm–Liouville problems. For example, if the continuous func-
tion s(x) is not strictly positive on the interval [ a, b ] then the Sturm–Liouville problem is
not regular. Also, the problem is not regular if one of the functions p, p′, s, q fails to be
continuous on the interval [ a, b ].
Suppose the Sturm–Liouville problem satisfies all of the requirements to be regular
except that the nonnegative function s(x) fails to be positive on [ a, b ] and/or q(x) fails
to satisfy |q(a+)| < ∞or |q(b−)| < ∞. We will refer to such a situation as a singu-
lar Sturm Liouville problem. We will see in Chapter 11 that such singular ODE-BVPs
naturally occur when solving partial differential equation models in certain physical
geometries, for example, problems in a disk or cylinder, and we will understand why
we call such ODE-BVPs “singular.”
In Sections 11.5 and 11.6 we will often use results concerning the eigenvalue problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
d
dr

r dR
dr

+ (λr −n2
r )R(r) = 0, 0 < r < a,
| R(0+) | < ∞,
γ0R(a) + γ1 dR
dr (a) = 0
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
(9.70)
where n is a nonnegative integer. The ODE here is Bessel’s equation of order n. The
singular Sturm-Liouville problem (9.70) has infinitely many eigenvalues λnm satisfying
0 < λn0 < λn1 < λn2 < · · · →∞.
The corresponding eigenfunctions are Rm(r) = Jn(λnmr) and satisfy the orthogonality
relation
0 =
a
0
rRm(r)Rm′(r) dr, for m ̸= m′.
(9.71)
Notice that even though this is a singular Sturm–Liouville problem, the orthogonality
relation is the same as found in (9.65): the functions p, s, q here are p(r) = r, s(r) = r,
q(r) = −n2
r .
Notice also that we can rewrite the orthogonality relation as 0 =
 a
0 Rm(r)Rm′(r)r dr, for
m ̸= m′, and we recall that rdr is part of the element of area in polar coordinates, dA =
rdrdθ, that we saw in Sections 6.6 and 7.3. As we will see in Section 11.5, it is not an accident
that the r dr of polar coordinates shows up in the study of Bessel functions, because Bessel
function originated in the solution of partial differential equations in polar coordinates.

Fourier Series
809
Example 9.30
Show the positivity of all eigenvalues of
⎧
⎨
⎩
r
	
rR′
′ + λr2R = 0, 0 < r < a
| R(0+) | < ∞, R(a) = 0
⎫
⎬
⎭,
which is an ODE-BVP for Bessel’s equation of order zero that we will use in
Section 11.5.
Method: Multiply through the ODE by 1
r R(r) and integrate on the interval 0 < r < a
to get
0 =
a
0
R(r)
	
rR′(r)

′ dr + λ
a
0
r
	
R(r)

2 dr.
Integrate by parts to get
0 =
%
R(r)
	
rR′(r)

&a
0 −
a
0
R′(r)
	
rR′(r)

dr + λ
a
0
r
	
R(r)

2 dr,
hence
λ
a
0
r
	
R(r)

2 dr = −aR(a)R′(a) + lim
r→0+ rR(r)R′(r) +
a
0
R′(r)
	
rR′(r)

dr,
and the BCs imply
λ
a
0
r
	
R(r)

2 dr =
a
0
r
	
R′(r)

2 dr.
Note that an eigenfunction R(r) cannot have R′(r) ≡0 on (0, a] because of the boundary
condition R(a) = 0.
Because both
 a
0 r
	
R(r)

2dr > 0 and
 a
0 r
	
R′(r)

2dr ≥0 for a nontrivial differentiable
eigenfunction R(r), it follows that λ ≥0. As to whether λ = 0 can be an eigenvalue,
if λ = 0 then R(r) satisfies the Cauchy-Euler ODE r2R′′ + rR′ = 0. As we discussed in
Section 3.5, substituting in R = rm gives characteristic equation m(m −1) + m = 0, hence
the roots are m = 0, m = 0 and the ODE has general solution R(r) = c1 +c2 ln(r). Because
of the BC | R(0+) | < ∞, we must have c2 = 0, so R(r) = c1. The only way for R(r) to
satisfy the BC R(a) = 0 is for c1 = 0, hence R(r) ≡0 and there is no eigenfunction. So, all
of the eigenvalues have λ ≥0 and λ ̸= 0, so all eigenvalues are positive. ⃝
Example 9.31
In Section 11.5 we will use the solutions of the eigenvalue problem
⎧
⎨
⎩
	
(1 −x2)′(x)

′ + λ(x) = 0, −1 < x < 1
|(−1+)| < ∞, |(1−)| < ∞,
⎫
⎬
⎭.
(9.72)
The ODE here is Legendre’s equation, as in Appendix C. There are infinitely many
eigenvalues λn = n(n + 1), and the corresponding eigenfunctions are the Legendre
polynomials Pn(x). They satisfy the orthogonality relation
0 =
1
−1
Pn(x)Pn′(x) dx, for n ̸= n′.

810
Advanced Engineering Mathematics
The orthogonality relation is the same as for the results for regular Sturm-Liouville
problems, even though (9.72) is singular.
As we will see in Section 11.6, the eigenvalue problem (9.72) and hence the Legen-
dre polynomials are used in solving Laplace’s equation in spherical coordinates. When
angle φ lies in the interval (0, π) and we substitute x = cos φ, we have correspondingly
−1 < x < 1. The orthogonality relation involves
0 =
1
−1
Pn(x)Pn′(x) dx=
0
π
Pn(cos φ)Pn′(cos φ)
	
−sin φ dφ

=
π
0
Pn(cos φ)Pn′(cos φ) sin φ dφ.
Again, it is no accident that sin φ dφ appears in the orthogonality relation, because in
spherical coordinates the element of volume is dV = ρ2 sin φ dρ dφ dθ. ⃝
9.6.2 A Composite Media Problem
Example 9.32
Suppose X(x) satisfies ODE
	
κ(x)X′(x)

′ + λX(x) = 0, 0 < x < 1 and 1 < x < 2,
(9.73)
where the function κ(x) is defined piecewise by
κ(x) =
⎧
⎨
⎩
1
2,
0 < x < 1
1
3,
1 < x < 2
⎫
⎬
⎭.
Suppose X(x) satisfies boundary conditions
X′(0) = X′(2) = 0,
(9.74)
and interface conditions
lim
x→1−X(x) = lim
x→1+ X(x)
and
lim
x→1−
	
κ(x)X′(x)

= lim
x→1+
	
κ(x)X′(x)

.
(9.75)
Find all of the eigenvalues and corresponding eigenfunctions.
As we will see in Section 10.2 and Example 11.5 in Section 11.1, the jump in the coef-
ficient κ in the ODE, as well as these new, interface conditions at x = 1, correspond
physically to joining two types of materials at x = 1. For example, the two materials
could be two metals and κ could be a coefficient of thermal conductivity. Physically, the
two interface conditions require the continuity of both the temperature and the heat flux
at x = 1.
Method: We will find a characteristic equation for this problem: to do this, we will solve
the ODE separately on the two subintervals [0, 1] and [1, 2] and then match the two
solutions using the interface boundary conditions. Define X(1) = X(x) for 0 < x < 1 and
X(2) = X(x) for 1 < x < 2.
First consider the case of λ = 0: On subinterval [0, 1], κ(x) = 1
2, so in the ODE there is
1
2 X′′(x) + 0 · X(x) = 0, that is, X′′(x) = 0, so the solutions are X(1)(x) = c1 + c2x. The BC
X′(0) = 0 yields c2 = 0, so X(1)(x) = c1, 0 < x < 1.
On [1, 2], κ(x) = 1
3, so in the ODE there is 1
3X′′(x) + 0 · X(x) = 0. The solutions are
X(2)(x) = d1 + d2x. The BC X′(2) = 0 yields that d2 = 0, so X(2)(x) = d1, 1 < x < 2.
Matching the solutions X(1)(x) and X(2)(x) requires c1 = X(1)(1−) = X(2)(1+) = d1,
hence c1 = d1, as well as 0 = limx→1−κ(x)(X(1))′(x) = limx→1+ κ(x)(X(2))′(x) = 0, which

Fourier Series
811
effectively imposes no further requirement on c1 or d1. So, λ = 0 is an eigenvalue, with
corresponding eigenfunctions
X(x) =
c1,
0 < x < 1
c1,
1 < x < 2

,
and we can define X(1) = limx→1−c1 = limx→1+ c1 = c1, so X(x) = c1 · 1. So, an
eigenfunction X0(x) ≡1 corresponds to eigenvalue λ0 = 0.
For λ > 0, on [0, 1], κ(x) =
1
2, so in the ODE there is 1
2 X′′(x) + λX(x) = 0, so the
solutions are X(1)(x) = c1 cos(
√
2λ x) + c2 sin(
√
2λ x). The BC X′(0) = 0 yields c2 = 0, so
X(1)(x) = c1 cos(
√
2λ x),
(X(1))′(x) = −c1
√
2λ sin(
√
2λ x), 0 < x < 1.
On [1, 2], κ(x) = 1
3, so the ODE there is 1
3X′′(x)+λX(x) = 0. We could take the solutions
there, X(2)(x), to be a linear combination of cos(
√
3λ x) and sin(
√
3λ x), but instead we
can use another set of basic solutions that will help us solve the BC X′(2) = 0. Let
X(2)(x) = d1 cos
	√
3λ (2 −x)

+ d2 sin
	√
3λ (2 −x)

.
The chain rule yields
(X(2))′(x) = d1
√
3λ sin
	√
3λ (2 −x)

−d2
√
3λ cos
	√
3λ (2 −x)

.
The BC X′(2) = 0 yields
0 = d1
√
3λ sin
	√
3λ (2 −2)

−d2
√
3λ cos
	√
3λ (2 −2)

=
√
3λd1 · 0 + d2 · 1,
hence d2 = 0. So∗
X(2)(x) = d1 cos
	√
3λ (2 −x)

,
	
X(2)
′(x) = d1
√
3λ sin
	√
3λ (2 −x)

, 1 < x < 2.
Matching the solutions X(1)(x) and X(2)(x) requires, by the interface conditions (9.75),
c1 cos(
√
2λ) = X(1)(1−) = X(2)(1+) = d1 cos(
√
3λ),
that is,
cos(
√
2λ)c1 −cos(
√
3λ)d1 = 0,
(9.76)
as well as
−1
2
√
2λ c1 sin(
√
2λ)= lim
x→1−κ(x)(X(1))′(x) = lim
x→1+ κ(x)(X(2))′(x)= 1
3
√
3λ d1 sin(
√
3λ),
that is,
−1
2
√
2λ sin(
√
2λ)c1 −1
3
√
3λ sin(
√
3λ)d1 = 0.
(9.77)
Equations (9.76) and (9.77) form a homogeneous system in two unknowns, c1, d1. After
dividing the last equation through by
1
λ
6 (recall that we assumed λ > 0 in this case), we
can write the system as
A
⎡
⎣
c1
d1
⎤
⎦≜
⎡
⎣
cos(
√
2λ)
−cos(
√
3λ)
−
√
3 sin(
√
2λ)
−
√
2 sin(
√
3λ)
⎤
⎦
⎡
⎣
c1
d1
⎤
⎦=
⎡
⎣
0
0
⎤
⎦.
∗In Section 11.3 we will call “clairvoyance” the method of using
4
cos
	√
3λ (2 −x)

, sin
	√
3λ (2 −x)

5
instead of
4
cos(
√
3λ x), sin(
√
3λ x)
5
. If we use the latter we would arrive at the same solution for X(2)(x) but it would take
more work, involving trigonometric identities, to satisfy the boundary conditions.

812
Advanced Engineering Mathematics
There is a nontrivial solution for the eigenfunction
X(x) =
⎧
⎨
⎩
X(1)(x),
0 < x < 1
X(2)(x),
1 < x < 2
⎫
⎬
⎭
if and only if the system of two equations has a nontrivial solution for c1, d1, that is, if
and only if
|A| =
......
cos(
√
2λ)
−cos(
√
3λ)
−
√
3 sin(
√
2λ)
−
√
2 sin(
√
3λ)
......
= 0.
This reduces to the characteristic equation
0 = −
√
2 cos(
√
2λ) sin(
√
3λ) −
√
3 sin(
√
2λ) cos(
√
3λ).
(9.78)
We can use a graphical method in Problem 9.6.4.9 to see why there are infinitely many
eigenvalues and we can use Newton’s Method to get good approximations to a finite
number of them. (See Problem 8.1.8.13 for finding some eigenvalues.)
As to the corresponding eigenfunctions, we can use the “adjugate matrix method,” of
Theorem 2.2 in Section 2.1, as in Example 2.5 in Section 2.1, to find a nontrivial solution
for the vector
c1
d1

: We calculate that
adj(A) =
⎡
⎣
−
√
2 sin(
√
3λ)
cos(
√
3λ)
√
3 sin(
√
2λ)
cos(
√
2λ)
⎤
⎦.
Any nonzero column of adj(A) is a useful solution for
c1
d1

.
Denote adj(A) = [ v1
v2 ]. The first column is nonzero, because∗
||v1||2 =

−
√
2 sin(
√
3λ)
2
+
√
3 sin(
√
2λ)
2
̸= 0.
By the way, using the fact that λ being an eigenvalue yields
−
√
2 cos(
√
2λ) sin(
√
3λ) =
√
3 sin(
√
2λ) cos(
√
3λ),
we calculate that
cos(
√
2λ)v1 =
√
3 sin(
√
2λ)v2,
confirming that† rank(adj(A)) = 1, as predicted by Theorem 1.33 in Section 1.6.
So, we can take
⎡
⎣
c1
d1
⎤
⎦= v1 =
⎡
⎣
−
√
2 sin(
√
3λ)
√
3 sin(
√
2λ)
⎤
⎦. Correspondingly, our eigen-
functions are
Xn(x) =
⎧
⎨
⎩
−
√
2 sin(
√
3λ) cos(
√
2λ x),
0 < x < 1
√
3 sin(
√
2λ) cos(
√
3λ (2 −x)

,
1 < x < 2
⎫
⎬
⎭,
where λ is any of the eigenvalues λn, n = 1, 2, . . . .
We leave it to the reader to explain why there are no eigenvalues with λ < 0. ⃝
∗This is by a somewhat pure mathematical explanation: sin(
√
2λ) = 0 and sin(
√
3λ) = 0 can both be true only
if both
√
2λ = nπ and
√
3λ = mπ for some integers m, n, which implies that n
m =
√
2λ
√
3λ =
1
2
3 , hence
1
2
3 is a
rational number. We’ll spare you how that leads to a contradiction.
† Because we cannot have both sin(
√
2λ) = 0 and cos(
√
2λ) = 0.

Fourier Series
813
9.6.3 Fourth-Order ODE-BVP
Example 9.33
Follow the steps below to find all eigenvalues and eigenfunctions of the fourth order
ODE-BVP
⎧
⎨
⎩
X(iv)(x) + λX = 0, 0 < x < L
X(0) = X′′(0) = X(L) = X′(L) = 0
⎫
⎬
⎭.
(9.79)
The boundary conditions correspond to a beam clamped at x = 0 and fixed at x = L,
and the eigenvalue parameter λ = −EI, where E is the Young’s modulus and I is the
moment of inertia.
(a) First, explain why λ = 0 is not an eigenvalue.
(b) Second, explain why λ > 0 cannot be an eigenvalue.
(c) Third, find all eigenvalues and eigenfunctions for the case λ < 0.
Method:
(a) For λ = 0 the ODE in (9.79) becomes X′′′′(x) = 0, so X(x) = c0 + c1x + 1
2 c2x2 + 1
6 c3x3,
where the ci’s are arbitrary constants. Substitute X(x) and X′′(x) = c2 + c3x into the
first two BCs to get
0 =
X(0) = c0
0 =
X′′(0) = c2

.
Substitute these into X(x) to reduce it to X(x) = c1x + 1
6 c3x3. Substitute X(x) and
X′(x) = c1 + 1
2 c3x2 into the last two BCs to get the system of equations
⎧
⎨
⎩
0 =
X(L) = c1L
+ 1
6 c3L3
0 =
X′(L) = c1
+ 1
2 c3L2
⎫
⎬
⎭,
that is,
⎡
⎣
L
1
6 L3
1
1
2 L2
⎤
⎦
⎡
⎣
c1
c3
⎤
⎦=
⎡
⎣
0
0
⎤
⎦.
Because
......
L
1
6 L3
1
1
2 L2
......
= 1
3 L3 ̸= 0,
it follows that c1 = c3 = 0. This yields X(x) ≡0, so λ = 0 is not an eigenvalue of this
fourth order ODE-BVP.
(b) Multiply through ODE −λX = X′′′′ by X(x) and integrate from x = 0 to x = L to get
−λ
L
0
(X(x))2 dx =
L
0
X(x)X′′′′(x) dx .

814
Advanced Engineering Mathematics
Similar to work in Example 9.30, use integration by parts and the BCs X(0) = X(L) =
0 to get
−λ
L
0
(X(x))2 dx =
+
X(x)X′′′(x)
,L
0 −
L
0
X′(x)X′′′(x) dx
= 

X(L)X′′′(L) −
X(0)X′′′(0) −
L
0
X′(x)X′′′(x) dx = −
L
0
X′(x)X′′′(x) dx.
Use integration by parts again to get
−λ
L
0
(X(x))2 dx = −
+
X′(x)X′′(x)
,L
0 +
L
0
X′′(x)X′′(x) dx,
= −X′(L)X′′(L) + X′(0)X′′(0) +
L
0
	
X′′(x)

2 dx.
The BCs X′′(0) = X′(L) = 0 imply
−λ
L
0
(X(x))2 dx =
L
0
	
X′′(x)

2 dx ≥0.
(9.80)
Because∗ L
0 (X(x))2 dx > 0, (9.80) yields λ ≤0. So, λ > 0 cannot be an eigenvalue.
(c) Define ω = (−λ)1/4 for convenience. The ODE X′′′′ −ω4X = 0 has characteristic
polynomial s4 −ω4, which has roots s = ±ω, ±iω. The solutions of the ODE are
X(x) = c1 cosh(ωx) + c2 sinh(ωx) + c3 cos(ωx) + c4 sin(ωx).
Substitute that and X′′(x) = ω2	
c1 cosh(ωx) + c2 sinh(ωx) −c3 cos(ωx) −c4 sin(ωx)

into the first two BCs to get
0 =
X(0) = c1 + c3
0 =
X′′(0) = c1 −c3

,
which implies c1 = c3 = 0. Substitute these into X(x) to reduce it to X(x)
=
c2 sinh(ωx) + c4 sin(ωx). Substitute X(x) and X′(x) = ω
	
c2 cosh(ωx) + c4 cos(ωx)

into
the last two BCs to get the system of equations
0 =
X(L)
= c2 sinh(ωL)
+
c4 sin(ωL)
0 =
X′(L)
= ω
	
c2 cosh(ωL)
+
c4 cos(ωL)


,
that is,
sinh(ωL)
sin(ωL)
cosh(ωL)
cos(ωL)
 c2
c4

=
0
0

.
(9.81)
There is a nontrivial solution for X(x) if, and only if,
0 =
....
sinh(ωL)
sin(ωL)
cosh(ωL)
cos(ωL)
.... = sinh(ωL) cos(ωL) −cosh(ωL) sin(ωL) ,
∗If not, X(x) ≡0 on [ 0, L ].

Fourier Series
815
2
1
–1
2
tan h (θ)
tan θ
4
6
8
10
12
θ
–2
FIGURE 9.30
Graphical method for finding eigenvalues.
that is, if, and only if, sinh(ωL) cos(ωL) = cosh(ωL) sin(ωL), which is true only if
sinh(ωL)
cosh(ωL) = sin(ωL)
cos(ωL), that is,
tanh(θ) = tan(θ),
(9.82)
where θ = ωL and tanh(θ) ≜sinh(θ)
cosh(θ).
Figure 9.30’s “graphical solution” of (9.82) shows that there are infinitely many
eigenvalues
λn = −
θn
L
4
, n = 1, 2, . . . .
Note that θn ≈
π
4 + (n −1)π, as n →∞. For the corresponding eigenfunctions,
again we can use the “adjugate matrix method” of Theorem 2.2 in Section 2.1, as in
Example 9.32, to find a solution of (9.81):
c2
c4

=

cos(θn)
−cosh(θn)

,
The corresponding eigenfunctions are
Xn(x) = cos(θn) sinh
θnx
L

−cosh(θn) sin
θnx
L

, n = 1, 2, . . . . ⃝
Eigenfunctions, also known as mode shapes, for ODE X′′′′(x) + λX = 0 with various
combinations of boundary conditions, are given on page 6.97 of Merritt et al. (1996).
9.6.4 Problems
1. Explain the very last result mentioned in Theorem 9.15, that is, explain why
ϵ0, ϵ1 > 0, γ0, γ1 ≥0 and q(x) ≤0 on the interval [ a, b ] together imply that all
eigenvalues for problems (9.63) and (9.64) are positive.
For the ODE-BVP in problems 2–4, (a) find all the eigenvalues and (b) the orthogonality
relation. Note that the ODE is a Cauchy-Euler equation.

816
Advanced Engineering Mathematics
2.
⎧
⎨
⎩
(r2R′)′ + λR(r) = 0, a < r < b
R′(a) = R′(b) = 0
⎫
⎬
⎭
3.
⎧
⎨
⎩
(rR′)′ + λ 1
r R(r) = 0, a < r < b
R′(a) = R′(b) = 0
⎫
⎬
⎭
4.
⎧
⎨
⎩
r2R′′ + rR + λR = 0, 1 < r < 3
R(1) = R(3) = 0
⎫
⎬
⎭
For the ODE-BVP in problems 5–7, (a) find the characteristic equation satisfied by all eigen-
values, and, if possible, their values, and (b) the orthogonality relation satisfied by the
eigenfunctions.
5.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
X′′ + λX = 0, π
4 < x < 3π
4
X
	 π
4

−3X′ 	 π
4

= 0
X

3π
4

+ X′ 
3π
4

= 0
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
6.
⎧
⎨
⎩
X′′ + 2βX′ + λX(x) = 0, 0 < x < L,
X(0) = X(L) = 0
⎫
⎬
⎭, where λ > β2.
7.
⎧
⎨
⎩
d4X
dx4 + λX = 0, 0 < x < L
X(0) = X′(0) = X(L) = X′(L) = 0
⎫
⎬
⎭
The boundary conditions correspond to a beam “fixed” at both x = 0 and x = L.
8. Suppose ODE-BVP
⎧
⎨
⎩
ψ′′ + q(x)ψ(x) = 0, a < x < b
ψ(a) = ψ(b) = 0
⎫
⎬
⎭
has a solution ψ(x) and nonhomogeneous ODE-BVP
⎧
⎨
⎩
y′′ + q(x)y(x) = f(x), a < x < b
y(a) = y(b) = 0
⎫
⎬
⎭
has a solution y(x). Show that
b
a
ψ(x)y(x) = 0.
[This is an example of a “Fredholm Alternative” result, similar to Problem 1.4.1.7.]

Fourier Series
817
9. For Example 9.32, we derived the characteristic equation (9.78), that is,
0 = −
√
2 cos(
√
2λ) sin(
√
3λ) −
√
3 sin(
√
2λ) cos(
√
3λ).
Use a graphical method to analyze this. For example, you could move the second
term to the other side and then divide through by
√
3 sin(
√
3λ) cos(
√
2λ) to get
0 =
0
2
3 + tan(
√
2λ)
tan(
√
3λ)
.
[To use Newton’s Method, it’s probably better to let θ
≜
√
λ and define a
function by
f(θ)=−
√
2 cos(
√
2 θ) sin(
√
3 θ) −
√
3 sin(
√
2 θ) cos(
√
3 θ).]
10. Assume that λ > 0. For the ODE-BVP
(⋆)
⎧
⎨
⎩
X′′ + λX(x) = 0, 0 < x < L
X(0)−hX′(0) = 0, X(L)+hX′(L) = 0
⎫
⎬
⎭,
(a) Find an equation in λ whose solutions give eigenvalues for (⋆). Do not solve
the equation, that is, do not find the eigenvalues.
(b) For the eigenvalues of (⋆) use the adjugate matrix method of Theorem 2.2
in Section 2.1, as in Examples 9.32 and 9.33, to find a formula for the
corresponding eigenfunctions.
11. Theorem 9.15 guarantees that the problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
X′′ + λX(x) = 0, a < x < b,
ϵ0X(a) −ϵ1X′(a) = 0,
γ0X(b) + γ1X′(b) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
has infinitely many eigenvalues λn and corresponding eigenfunctions Xn(x). For
those n for which ω2
n = λn > 0, Xn(x) satisfies the undamped oscillator ODE, so
Xn(x) = c1,n cos(ωnx) + c2,n sin(ωnx). Substitute this into the BCs and use the adju-
gate matrix method of Theorem 2.2 in Section 2.1 to find c2,n in terms of c1,n and
thus find a way to express Xn(x) in terms of ωn, cos(ωnx), sin(ωnx), ϵ0, ϵ1, γ0, γ1, a, b.

818
Advanced Engineering Mathematics
12. Define the function κ(x) piecewise by κ(x)
=
⎧
⎨
⎩
1,
0 < x < 1
1
4,
1 < x < 2
⎫
⎬
⎭.
Find all
eigenvalues and corresponding eigenfunctions for problem (⋆) below.
(⋆)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
	
κ(x)X′(x)

′ + λX(x) = 0,
0 < x < 1 and 1 < x < 2,
X′(0) = X′(2) = 0,
X(1−) = X(1+),
limx→1−κ(x)X′(x)
= limx→1+ κ(x)X′(x)
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
13. [Small project suggested by Professor Allen Hunt] Consider Example 9.32 with
instead
κ(x) =
⎧
⎨
⎩
κ1,
0 < x < 1
κ2,
1 < x < 2
⎫
⎬
⎭.
If √κ2/κ1 is a rational number, that is, the ratio of integers, find all the eigenvalues
and eigenfunctions. For a smaller project, make a specific choice of κ1, κ2 for which
√κ2/κ1 is rational.
9.7 Rayleigh Quotient
Consider a Sturm–Liouville eigenvalue problem with special boundary conditions
⎧
⎨
⎩
	
p(x)X′(x)

′ + (λs(x) + q(x))X(x) = 0, a < x < b,
X(a) = X(b) = 0
⎫
⎬
⎭.
Multiply the ODE by X(x) and then integrate from a to b to get
0 =
b
a
X(x) ·
	
p(x)X′(x)

′ dx +
b
a
X(x) · (λs(x) + q(x))X(x) dx.
Similar to how we established the orthogonality relation (9.65) in Section 9.6, use
integration by parts on the first integral to get
0 =
+
X(x) · p(x)X′(x)
,b
a −
b
a
X′(x) · p(x)X′(x) dx +
b
a
X(x) · (λs(x) + q(x))X(x) dx.

Fourier Series
819
Because X(a) = X(b) = 0, the terms in the brackets are zero. Put the first integral on the
other side of the equation to get
b
a
p(x)
	
X′(x)

2 dx = λ
b
a
s(x)
	
X(x)

2 dx +
b
a
q(x)
	
X(x)

2 dx,
and then move the second integral on the right-hand side to the left-side of the equation
to get
b
a
	
−q(x)
	
X(x)

2 + p(x)
	
X′(x)

2
dx = λ
b
a
s(x)
	
X(x)

2 dx.
If we assume that s(x) satisfies one of the assumptions of a regular Sturm-Liouville
problem, namely, that
s(x) ≥0 and is not identically zero on [a, b],
it is reasonable to expect that X(x) being an eigenfunction implies that there is no division
by zero problem in rewriting the last identity involving integrals as
λ =
b
a
	
p(x)
	
X′(x)

2 −q(x)
	
X(x)

2
dx
b
a
s(x)
	
X(x)

2dx
.
This explains why the eigenvalue λ can be expressed in terms of definite integrals
involving the eigenfunction.
This looks more interesting than it really is, because it seems to say that in order to
evaluate λ we should find the eigenfunction X(x) first in order to evaluate the integrals.
But, to find X(x) we would seem to need to solve the ODE which involves the unknown
value λ! That is circular logic. It is therefore pretty amazing that the quotient can be used
to find an eigenvalue(s).
Definition 9.6
For a regular Sturm–Liouville (SL) problem (9.63) in Section 9.6, with its general BCs (9.64)
in Section 9.6, the Rayleigh quotient is given by
RSL[ X(x) ] ≜
−
+
p(x)X(x)X′(x)
,b
a +
b
a
	
p(x)
	
X′(x)

2−q(x)
	
X(x)

2
dx
b
a
s(x)
	
X(x)

2dx
.
(9.83)

820
Advanced Engineering Mathematics
Remarks:
(a) As we will see in Section 11.1, the minimum eigenvalue λ1 is used in calculating
the “time constant” which indicates how quickly the temperature distribution
of an object settles down to be close to the equilibrium temperature distri-
bution. In fact, the Rayleigh Quotient estimate can be very useful in making
approximations, or “back of the envelope” calculations, for physical problems.
(b) The Rayleigh Quotient for a Sturm–Liouville problem is “homogeneous” in the
sense that if X is a nice, nontrivial function on the interval (a, b) and β is a nonzero
scalar, then
RSL[ βX(x) ] = RSL[ X(x) ].
(9.84)
We’ll see the application of this remark in Example 9.35.
Theorem 9.16
For a regular Sturm-Liouville problem (9.63) and (9.64) in Section 9.6, the minimum
eigenvalue, λ1, is given by
λ1 = min{RSL[X(x)] : nice, differentiable, nontrivial X(x)}.
(9.85)
In the above, “X(x) ̸≡0” means “X(x) is not identically zero on the interval (a, b).”
Theorem 9.16 is quite a deep result because its explanation can use the advanced mathe-
matical subject of “Functional Analysis.” In fact, the origins of that subject were exactly in
the symbolic calculations we have just done, and expressions such as
−
+
p(x)X(x)X′(x)
,b
a +
b
a
	
p(x)
	
X′(x)

2 −q(x)
	
X(x)

2
dx
and
b
a
s(x)
	
X(x)

2 dx
are called “quadratic functionals.” While the Theorem is true, I am sweeping under the
carpet all sorts of abstract mathematical details about what such a minimization entails,
including why there is such a minimum value. Also, in Section 10.6 and Chapter 14 we
will see examples of quadratic functionals and their interpretation as energy, either kinetic
and potential.
Example 9.34
Use Theorem 9.16 and a single trial function X(x) to get an estimate for the minimum
eigenvalue for the ODE-BVP eigenvalue problem

Fourier Series
821
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
X′′(x) + λX(x) = 0, 0 < x < L,
hX(0) −X′(0) = 0,
X′(L) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(9.86)
where h > 0 is a given heat transfer coefficient.
Method: According to Definition 9.6, the Rayleigh Quotient for this problem is
RSL[X(x)] =
−
	
X(L)X′(L) −X(0)X′(0)

+
L
0
	
X′(x)

2 dx
L
0
	
X(x)

2 dx
=
h
	
X(0)

2 +
L
0
	
X′(x)

2 dx
L
0
	
X(x)

2 dx
.
For a specific function X(x), the single number RSL[ X(x) ] will be an estimate for the
minimum eigenvalue λ1.
If X(x) were an eigenfunction of (9.86) then the Rayleigh Quotient might exactly equal
the minimum eigenvalue. So, it makes sense to choose X(x) that satisfies the boundary
conditions hX(0) −X′(0) = X′(L) = 0.
A quadratic function
X(x) = α + βx + 1
2 ξx2
will satisfy the BCs if we correctly choose values for the parameters α, β, ξ. Because
X′(x) = β + ξx, the two BCs require hX(0) −X′(0) = h · α −β = 0 and X(L) = β + ξL = 0.
This yields α =
β
h and ξ
= −β
L . So, for all β, the function X(x) =
β
h + βx −
β
2Lx2 = β

1
h + x −1
2Lx2
satisfies the BCs. But by (9.84), RSL
%
β

1
h + x −1
2Lx2 &
=
RSL
% 
1
h + x −1
2Lx2 &
, so it will be enough to get an estimate using the single trial
function
X(x) ≜1
h + x −1
2Lx2.
We calculate that X(0) = 1
h and X′(x) = 1 −1
Lx, so
RSL
1
h +x−1
2Lx2

=
1
h +
L
0

1 −1
Lx
2
dx
L
0

1
h + x −1
2Lx2
2
dx
= . . .=
1
h + 1
3L
L
h2 + 2L2
3h + 2L3
15
=
15h + 5h2L
15L + 10hL2 + 2h2L3 .
So, we get an estimate for the minimum eigenvalue of problem (9.86):
λ1 ≈
15h + 5h2L
15L + 10hL2 + 2h2L3 .
For the example where L = 2 and h = 1
4, we get
λ1 ≈35
328 ≈0.1067073171.
This compares quite well with λ1 ≈0.10669081097, which we got in Example 9.16 in
Section 9.3 using Newton’s Method to find an approximate solution of the characteristic
equation for (9.86). Also, because the function X(x) = 4 + x −1
4 x2 satisfies the boundary

822
Advanced Engineering Mathematics
conditions and gives a good estimate for λ1, we can use X(x) as an approximation for an
eigenfunction. ⃝
Example 9.35
Use Theorem 9.16 and a family of functions to get an estimate for the minimum
eigenvalue for (9.86), the same ODE-BVP eigenvalue problem as in Example 9.34.
Method: Again, the Rayleigh Quotient for this problem is
RSL[ X(x) ] =
h
	
X(0)

2 +
L
0
	
X′(x)

2 dx
L
0
	
X(x)

2 dx
.
We can substitute into the Rayleigh Quotient a family of trial functions and get an esti-
mate for the minimum eigenvalue by minimizing RSL[ X(x) ] for that family. Let’s try
X(x) = α + βx, that is, all “linear” functions, as the trial functions. By (9.84), we don’t
need two adjustable parameters α, β in our family of trial functions, since
RSL[ α + βx) ] = RSL

α

1 + β
α x
 
= RSL
%
1 + β
α x
&
for all α ̸= 0. Denoting μ = β
α , we have
RSL[1 + μx] =
h +
L
0
μ2dx
L
0
(1 + μx)2 dx
=
h + μ2L
L + μL2 + 1
3μ2L3 =
h + μ2L
L(1 + μL + 1
3μ2L2)
≜F(μ).
The minimum value of F(μ) will give us an estimate for the minimum eigenvalue λ1.
For the example with h = 1
4 and L = 2 and F(μ) =
0.25 + 2μ2
2(1 + 2μ + 4
3μ2)
,
the
Mathematica
command∗
NMinimize
[F[μ], μ]
has
output
{0.11064, {μ−>
0.129787}}.
So, using a family of all linear functions as trial functions, we estimate the minimum
eigenvalue of λ ≈0.11064 for the ODE-BVP eigenvalue problem (9.86). We have no
interest in the value of μ that gives an estimate for the minimum of the Rayleigh Quo-
tient. (If we had chosen the family of trial functions to be admissible, that is, to satisfy
the BCs, then the value of μ and the corresponding trial function X(x) = 1 + μx would
be of interest as an approximation for an eigenfunction.) This compares pretty well with
the approximate minimum eigenvalue λ ≈0.10669084 that we found in Example 9.16 in
Section 9.3. ⃝
9.7.1 Problems
1. For RSL[ X(x) ] given in (9.83), explain why (9.84) is true for all constants β ̸= 0
and nice, differentiable nontrivial functions X(x).
∗Or, we could have used the analytical, exact methods of minimization found in Calculus I.

Fourier Series
823
2. Use a single “trial” function X(x) to get an estimate for the minimum eigenvalue
for ODE-BVP eigenvalue problem
⎧
⎨
⎩
X′′(x) + λX(x) = 0, 0 < x < L
X′(0) = 0,
hX(L) + X′(L) = 0
⎫
⎬
⎭,
where h > 0 is a given heat transfer coefficient. Write down what the Rayleigh
quotient is here.
3. Use a family of “trial” functions to get an estimate for the minimum eigenvalue
for ODE-BVP eigenvalue problem
⎧
⎨
⎩
X′′(x) + λX(x) = 0, 0 < x < L
X′(0) = 0,
hX(L) + X′(L) = 0
⎫
⎬
⎭,
where h > 0 is a given heat transfer coefficient. Write down what the Rayleigh
quotient is here.
4. Find a nontrivial second degree polynomial function X(x) that satisfies the bound-
ary conditions X(0) = X(L) = 0. Use X(x) as a single “trial” function to get an
estimate for the minimum eigenvalue for ODE-BVP eigenvalue problem
X′′(x) + λxX(x) = 0, 0 < x < L
X(0) = X(L) = 0

.
Write down what the Rayleigh quotient is here.
5. Find a function(s) X(x) of the form cos ωx that satisfies the boundary conditions
X′(0) = X′(L) = 0. Use X(x) as a “trial” function to get an estimate for the
minimum eigenvalue for the ODE-BVP eigenvalue problem
X′′(x) + λxX(x) = 0, 0 < x < L
X′(0) = X′(L) = 0

.
Write down what the Rayleigh quotient is here.
6. Use a single “trial” function X(x) to get an estimate for the minimum eigenvalue
for the ODE-BVP eigenvalue problem
	
(1 + x)X′(x)

′ + λX(x) = 0, 0 < x < L
X′(0) = X(L) = 0

.
Write down what the Rayleigh quotient is here.
7. A light cantilever beam is clamped at the end x = L with vertical yawing axis and
loaded at the free end x = 0 with load P acting on the longitudinal, that is, vertical,
axis. As in Example 2.4 of (Temple and Bickley 1956), assume the beam’s vertical
displacement X(x) satisfies (ODE) ACX′′+P2x2X = 0 for 0 < x < L, where A is the

824
Advanced Engineering Mathematics
torsional rigidity and C is the flexural rigidity of the beam section. The geometrical
assumptions at the ends yields the boundary conditions X′(0) = X(L) = 0.
This problem will demonstrate, by example, a method for improving accu-
racy by using higher order boundary conditions to refine a trial function in the
Rayleigh Quotient method.
(a) Write down the corresponding Rayleigh Quotient RSL. The eigenvalue param-
eter is λ = P2, so the minimum load is P1 = √λ1. For P < P1, the beam is
undeflected, that is, has X(x) ≡0.
(b) Because X(x) satisfies the ODE, take the limit, as x →0+, to explain why ODE
yields the boundary condition X′′(0) = 0.
(c) Because X(x) satisfies the ODE, take the limit, as x →L−, to explain why ODE
yields the boundary condition X′′(L) = 0.
(d) Take the derivative with respect to x of both sides of the ODE to conclude that
ACX′′′ + P2(2xX + x2X′) = 0. After that, take the limit, as x →0+, to explain
why ODE yields the boundary condition X′′′′(0) = 0.
(e) Find a polynomial q(x) of degree 5 that satisfies all five boundary conditions
q′(0) = q′′(0) = q′′′(0) = q(L) = q′′(L) = 0. [Note that q is not required to satisfy
the ODE itself.] Use q(x) as a trial function in the Rayleigh Quotient to get an
estimate on the load in terms of the other physical parameters, specifically
P1 ≈
0
858
53
√
AC
L2
≈4.023
√
AC
L2 .
By the way, from a table of zeros of the Bessel function of order −1
4 we get
P1 ≈4.0126
√
AC
L2 .
9.8 Parseval’s Theorems and Applications
As we will see in Chapter 10, the square of a function, integrated on an interval, can mea-
sure the total amount of some physical quantity. For example, if the mass density of a
string is 100 gm/m and the vertical position of points on the string are given by y = u(x, t)
for 0 < x < L, then the total kinetic energy of the string in joules is
(0.1) ·
L
0
....
∂u
∂t (x, t)
....
2
dx.
Using orthogonality we can express that in terms of Fourier sine series coefficients.
In Section 2.4, we saw Parseval identities for vectors in Rn, specifically (2.19)(b) and
(2.19)(c) in Section 2.4.

Fourier Series
825
First, let’s derive the first Parseval’s Identity for a Fourier series: Suppose f(x) is given
on the interval [ −L, L ]. Then
f(x) .= fs(x) = a0
2 +
∞

n=1
an cos
nπx
L

+
∞

n=1
bn sin
nπx
L

.
Let us now integrate the square of f on the interval [ −L, L ]. We will not worry about the
pure mathematical technicalities that are needed to justify the convergence of the infinite
sums and why we can move the -∞
n=1 operation in or out of the integral that defines the
inner product. So, in a non-rigorous way we have
L
−L
| f(x)|2dx = ⟨f(x), f(x)⟩
=

a0
2 +
∞

m=1
am cos
mπx
L

+
∞

m=1
bm sin
mπx
L

, a0
2 +
∞

n=1
an cos
nπx
L

+
∞

n=1
bn sin
nπx
L

=
a0
2
2
⟨1, 1⟩+ a0
∞

n=1
an

1, cos
nπx
L

+ a0
∞

n=1
bn

1, sin
nπx
L

+ 2
∞

n=1
∞

m=1
ambn

cos
mπx
L

, sin
nπx
L

+
∞

n=1
∞

m=1
aman

cos
mπx
L

, cos
nπx
L

+
∞

n=1
∞

m=1
bmbn

sin
mπx
L

, sin
nπx
L

=
a0
2
2
2L + a0 · 0 + a0 · 0 + 2
∞

n=1
∞

m=1
ambn · 0 +
∞

n=1
ananL +
∞

n=1
bnbnL
= 2L
a0
2
2
+ 1
2
∞

n=1

a2
n + b2
n

.
Theorem 9.17
Assuming both sides exist,
1
2L
L
−L
|f(x)|2 dx =
a0
2
2
+ 1
2
∞

n=1

a2
n + b2
n

,
where the Fourier series of f on the interval [ −L, L ] is given by
f(x) .= fs(x) = a0
2 +
∞

n=1
an cos
nπx
L

+
∞

n=1
bn sin
nπx
L

.

826
Advanced Engineering Mathematics
In a similar way we can use orthogonality to establish Parseval’s Theorems for a Fourier
sine series or a Fourier cosine series:
Theorem 9.18
Assuming both sides exist,
1
L
L
0
|f(x)|2dx = 1
2
∞

n=1
b2
n,
where the Fourier sine series of f on the interval [ 0, L ] is given by
f(x) .= fsin(x) =
∞

n=1
bn sin
nπx
L

,
where bn = 2
L
 L
0 f(x) sin
nπx
L

dx.
Theorem 9.19
Assuming both sides exist,
1
L
L
0
|f(x)|2dx =
a0
2
2
+ 1
2
∞

n=1
a2
n,
where the Fourier cosine series of f on the interval [ 0, L ] is given by
f(x) .= fcos(x) = a0
2 +
∞

n=1
an cos
nπx
L

,
where a0 = 2
L
 L
0 f(x) dx and an = 2
L
 L
0 f(x) cos
nπx
L

dx.
Example 9.36
Suppose that the current through a resistor of R  is
I(t) =
N

n=1
bn sin
nπt
L

.
Find the total energy used by that resistor over one period of time.
Method: For n = 1, 2, . . . , N, the function sin
	 nπt
L

is periodic with period 2π
nπ
L
= 2L
n . For
n = 1, . . . , N, each sin
	 nπt
L

is periodic with period 2L, so I(t) is periodic with period 2L.

Fourier Series
827
To expand I(t) in a Fourier series with period 2L, we see that it is already given to us as
I(t) = 0
2 +
∞

n=1
0 · cos
nπt
L

+
N

n=1
bn sin
nπt
L

+
∞

n=N+1
0 · sin
nπt
L

.
Across a resistor of R , the voltage drop is V = RI, so the power used by the resistor is
P = IV = RI2. The total energy used by the resistor over one period is
 2L
0
R(I(t))2 dt, by
Problem 9.1.8.22. Applying Parseval’s Theorem 9.17 and problem 9.1.8.22, we get that
E =
2L

0
R
	
I(t)

2dt = RL
N

n=1
b2
n
is the total energy used by the resistor over one period of time. ⃝
We can also use the various forms of Parseval’s Theorem to establish exact values for
interesting infinite series.
Example 9.37
Use Parseval’s Theorem for a Fourier cosine series for f(x) = x to establish the identity
π4
96 =
∞

k=1
1
(2k −1)4 = 1
14 + 1
34 + 1
54 + · · · ·
Method:
From Example 9.9 in Section 9.2, the Fourier cosine series of f(x) for
0 < x < π is
x .= fcos(x) = a0
2 +
∞

n=1
an cos
nπx
L

= π
2 +
∞

k=1
4
π(2k −1)2 cos
	
(2k −1)x

.
Applying Theorem 9.19 gives us
π2
3 = 1
π
π
0
x2dx = 1
π
π
0
|f(x)|2dx =
a0
2
2
+ 1
2
∞

n=1
a2
n =
π
2
2
+ 1
2
∞

k=1

4
π(2k −1)2
2
= π2
4 + 8
π2
∞

k=1
1
(2k −1)4 ,
hence, as desired,
∞

k=1
1
(2k −1)4 = π2
8

π2
3 −π2
4

= π4
96 . ⃝
9.8.1 Best Approximation by a Partial Sum of a Fourier Series
A f(x) defined and continuous on the interval (−L, L) is square integrable if
L
−L
|f(x)|2 dx < ∞.

828
Advanced Engineering Mathematics
The set of all such functions can be made larger by considering sequences
4
fN(x)
5∞
N=1 of
such functions and saying fN →f if the sequence of real numbers
L
−L
|f(x) −fN(x)|2dx →0, as N →∞.
We can consider the set L2[−L, L] to be the set of all such f, although to be honest, elements
of L2[−L, L] are not exactly functions but are “represented” by functions; in addition, such
functions have to be “measurable” and the concept of integration is actually “Lebesgue
integration.” We’re “sweeping under the rug” a tremendous amount of beautiful and use-
ful mathematics, but for our purpose of applying mathematics we will think of L2[−L, L]
as a Hilbert space of “functions” when given the real inner product
⟨f, g⟩≜
L
−L
f(x)g(x) dx
or the complex inner product
⟨f, g⟩≜
L
−L
f(x)g(x) dx.
Given a function in L2[−L, L], it can be expanded in a Fourier series
f(x) .= fs(x) = a0
2 +
∞

n=1

an cos
nπx
L

+ bn sin
nπx
L

in the sense that the sequence
4
sN(x)
5∞
N=1 of partial sums of the Fourier series has
L
−L
|f(x) −sN(x)|2dx →0,
as N →∞.
Here,
sN(x) = a0
2 +
N

n=1

an cos
nπx
L

+ bn sin
nπx
L

,
where the formulas for a0; a1, a2, . . . , b1, b2, . . . were given in (9.6) through (9.8) in
Section 9.1.

Fourier Series
829
Theorem 9.20
If f is in L2[−L, L] then the function gN in
VN ≜Span

1, cos
πx
L

, sin
πx
L

, . . . , cos
Nπx
L

, sin
Nπx
L

that is closest to f, that is, which minimizes
||f −gN||2 ≜
L
−L
|f(x) −gN(x)|2dx,
is gN = sN. In other words, the best approximation by a linear combination of the first
2N + 1 Fourier terms is a partial sum of the Fourier series of f.
Why? It is easy to normalize to get an o.n. set
 1
2L · 1, 1
L cos
πx
L

, 1
L sin
πx
L

, . . . , 1
L cos
Nπx
L

, 1
L sin
Nπx
L

.
After that, use Theorem 2.54(c) in Section 2.10. ⃝
Theorem 9.21
(Bessel’s Inequality) If f is in L2[−L, L] then sN, a partial sum of its Fourier series, satisfies
L ·

a2
0
2 +
N

n=1
(|an|2 + |bn|2)

= || sN ||2 ≤|| f||2|| ≜
L
−L
| f|2 dx.
9.8.2 Complex Fourier Series
Recall that we defined an inner product on complex valued functions by (9.30) in Section
9.4, that is,

f(x), g(x)

≜
L
−L
f(x)g(x) dx,
where z denotes the complex conjugate of z.
We note first that
L
−L
| f(x)|2 dx =
L
−L
f(x) f(x) dx =

f(x), f(x)


830
Advanced Engineering Mathematics
=

fC(x), fC(x)

=

∞

n=−∞
cn einπx/L ,
∞

m=−∞
cm eimπx/L

=
∞

n=−∞
∞

m=−∞
cncm ·
L
−L
ei(n−m)πx/L dx =
∞

n=−∞
∞

m=−∞
cncm ·
2L,
n = m
0,
n ̸= m

.
The above calculations are not mathematically rigorous proof because we do not study the
convergence of the infinite series and how that interacts with taking integrals.
We have the Parseval identity
L
−L
| f(x)|2 dx = 2L
∞

n=−∞
|cn|2.
(9.87)
It follows that the root mean square (RMS) value of f is
fRMS ≜
8
9
9
9
: 1
2L
L
−L
| f(x)|2 dx =
8
9
9
:
∞

n=−∞
|cn|2.
(9.88)
9.8.3 Fourier Transforms
Theorem 9.22
(Parseval’s Theorem) If F(ω) = F[ f(t) ](ω) and G(ω) = F[ g(t) ](ω) exist then
(a)
∞

−∞
f(t) g(t) dt =
∞

−∞
F(ω) G(ω) dω
(9.89)
and, in particular with g = f,
(b)
∞

−∞
| f(t)|2 dt =
∞

−∞
|F(ω)|2 dω
(9.90)
Learn More About It
A derivation of Theorem 9.22 is in Section 1.3 of Fourier Transforms, by Ian N. Sneddon,
Dover Publications., Inc., c⃝1995.

Fourier Series
831
9.8.4 Problems
1. Explain why Theorem 9.18 is true, using work similar to what justified Theorem
9.17, or by applying Theorem 9.17 to the special case of a function that is odd on
the interval −L < x < L.
2. Explain why Theorem 9.19 is true, using work similar to what justified Theorem
9.17, or by applying Theorem 9.17 to the special case of a function that is odd on
the interval −L < x < L.
3. Find the power used by a resistor of R  by the periodic current
I(t) = 1
2 +
∞

n=1

e−n cos
nπt
L

+ e−2n sin
nπt
L

amps
over one period of time. Simplify your conclusion using, for example, e−2n =
(e−2)n, and the sum of a geometric series,
∞

n=1
rn =
1
1 −r, if |r| < 1.
4. For f(x) = x,
(a) use Parseval’s Theorem for a Fourier sine series to establish the identity
π2
6 =
∞

n=1
1
n2 = 1
12 + 1
22 + 1
32 + · · · ·
(b) If you use a different interval than the one you used in part (a), do you get the
same identity? Why, or why not?
5. Establish an identity for an infinite series by using one of Parseval’s Theorems
applied to an example of a Fourier series, Fourier cosine series, or Fourier sine
series found in an earlier section, either in the narrative or the problems. (Your
example should not duplicate Example 9.37 or Problem 9.8.4.4.)
6. Establish a Parseval identity for a generalized Fourier series.
7. If you have read Section 2.10, “Hilbert Space,” use Paresval’s Identity and Def-
inition 2.39 to explain why the sequence of functions { 1
√π sin(nx)}∞
n=1 converges
weakly to the zero function in L2(−π, π), even though || 1
√π sin(nx)|| = 1 for all
n = 1, 2, . . . .
Key Terms
admissible: Example 9.35 in Section 9.7
aliasing: after (9.57) in Section 9.5
analysis equation: before (9.55) in Section 9.5
auto-correlation: (9.61) in Section 9.5
average value: after (9.8) in Section 9.1

832
Advanced Engineering Mathematics
Bessel’s Inequality: Theorem 9.21 in Section 9.8
best approximation: Theorem 9.20 in Section 9.8
characteristic equation: before (9.19) in Section 9.3
clamped (boundary conditions): Example 9.33 in Section 9.6
convolution on the real line: before Theorem 9.10 in Section 9.4, Problem 17.1.2.8
digital filter: before (9.51) in Section 9.5
discrete convolution: (9.60) in Section 9.5
Discrete Fourier Transform (DFT): (9.50) in Section 9.5
eigenfunction, eigenvalue: after (9.17) in Section 9.3
even: before Theorem 9.2 in Section 9.1
extend periodically, periodic extension: before Example 9.7 in Section 9.1
Fast Fourier transform (FFT): after (9.61) in Section 9.5
finite Fourier transform: (9.49) in Section 9.5
finite jump discontinuity: Definition 9.1 in Section 9.1
finite pulse: Example 9.19 in Section 9.4
finite wave train: Example 9.21 in Section 9.4
fixed (boundary conditions): Example 9.33 in Section 9.6
Fourier coefficients: (9.1) in Section 9.1
Fourier cosine series, Fourier cosine coefficients: (9.12) in Section 9.2
Fourier series: (9.1) in Section 9.1
Fourier sine series, Fourier sine coefficients: (9.13) in Section 9.2
generalized Fourier series expansion: before (9.20) in Section 9.3
graphical method: after (9.27) in Section 9.3
Hadamard matrix: Example 9.24 in Section 9.5
half-range expansion: before (9.12) in Section 9.2
Hermitian conjugate: before (9.51) in Section 9.5
interface conditions: (9.75) in Section 9.6
match: Example 9.32 in Section 9.6
nice function: Definition 9.2 in Section 9.1
odd: before Theorem 9.2 in Section 9.1
Parseval identity: Theorems 9.17 through 9.19 in Section 9.8 and Theorem 9.22 in Section
9.8, and (9.87) in Section 9.8
partial sums: (9.3) in Section 9.1
piecewise smooth: after Definition 9.2 in Section 9.1
periodic boundary conditions: (9.28) in Section 9.3
power spectrum: Example 9.27 in Section 9.5
Rayleigh quotient: Definition 9.6 in Section 9.7
regular: Definition 9.5 in Section 9.6
root mean square (RMS): (9.88) in Section 9.8
singular Sturm Liouville problem: before (9.70) in Section 9.6
square integrable: before Theorem 9.20 in Section 9.8
square integrability condition: (9.19) in Section 9.3
square wave: Example 9.5 in Section 9.1
Sturm-Liouville problem: Definition 9.5 in Section 9.6
synthesis equation: (9.55) in Section 9.5
time series: before (9.50) in Section 9.5
trial function: Example 9.34 in Section 9.7
unitary matrix: Example 9.23 in Section 9.5

Fourier Series
833
Mathematica Command
NMinimize[F[μ], μ]: Example 9.35 in Section 9.7
References
Cochran, W.T, Cooley, J.W et al. What is the fast Fourier transform? IEEE Transactions on Audio
Electroacousting AU-15, 45–55, 1967.
Davis, P.J. Circulant Matrices. John Wiley & Sons, Inc., New York, 1979.
Friedman, B. Lectures on Applications-Oriented Mathematics, ed. by V. Twersky, Wiley Classics Library,
1991.
Merritt, F.S. et al. Standard Book for Civil Engineers. McGraw-Hill, New York, 1996.
Temple, G. and Bickley, W.G. Rayleigh’s Principle and Its Applications to Engineering, Dover Publica-
tions Inc., Mineola, NY, 1956.


10
Partial Differential Equations Models
10.1 Integral and Partial Differential Equations
In this chapter, we will discuss a variety of physical problems that can be modeled by
integral equations, typically over planar regions or spatial regions. We will see that
Chapter 7’s divergence theorem, Stokes’ theorem, and change of variable formulas enable
us to turn integral equations into partial differential equations (PDEs).
In Chapters 11, 16, and 17, we will learn methods for getting solution formulas for PDEs,
and in Chapters 12 and 14, we will learn methods for numerical approximation of solutions
of PDEs.
Our first physical applications are to flow of mass or heat energy.
Recall from Example 7.36 in Section 7.5 that

S
ϱ v • n dS
is the rate of mass flow out of S. Here ϱ is the mass density, in units of mass per volume,
v is the velocity vector field of the fluid; and S is a constant, that is, time-independent,
closed, piecewise smooth, oriented surface enclosing a solid V having outward unit normal
vector n. We may refer briefly to such a situation as S encloses a “control” volume V.
In a physical problem in which a substance can flow through the boundary of a control
volume, the divergence theorem can relate that flow, which is an integral over the surface
bounding the control volume, to an integral over the control volume.
Define M =

V ϱ dV to be the total mass of the fluid in the solid V. In Example 7.39 in
Section 7.6, we saw that

S
ϱ v • n dS = −dM
dt = −

V
∂ϱ
∂t dV.
The integral equation

V
∂ϱ
∂t dV +

S
ϱ v • n dS = 0
(10.1)
expresses the physical law of conservation of mass.
In Example 7.39 in Section 7.6, we used the divergence theorem, Theorem 7.14 in
Section 7.6, along with reasoning using continuity and the arbitrariness of V, to see why
(10.1) implies the differential equation (7.41) in Section 7.6, that is,
835

836
Advanced Engineering Mathematics
∂ϱ
∂t + ∇• (ϱv) = 0,
(10.2)
which likewise expresses the physical law of conservation of mass.
In this and later sections, we will see other physical situations where a physical law
expressed as an integral equation can also be expressed as a PDE. In Chapter 11, we will
learn a technique for solving many PDEs.
Example 10.1
Find an integral equation that expresses conservation of heat energy in a fixed control
volume, and use that to establish a PDE that expresses the same law.
Method:
Let e = e(x, y, z, t) be the density of heat energy in a substance, in units of
energy/volume, and let S enclose a control volume V. Then, analogously to total mass,
the total heat energy in V is
E ≜

V
e dV.
The heat flux vector q = q(x, y, z, t) measures the time rate of heat flow, in units of
energy/(time·area). Figure 10.1 depicts heat flowing in or out of parts of a solid V. Suppose
S has outward unit normal vector n. Conservation of heat energy says that
dE
dt = −

S
q(r, t) • n dS +

V
Q(r, t) dV,
where Q = Q(r, t) is the rate of heat production, in units of energy/(time · volume). It
follows that

V
∂e
∂t dV = −

S
q(r, t) • n dS +

V
Q(r, t) dV.
(10.3)
FIGURE 10.1
Heat flux vectors.

Partial Differential Equations Models
837
The divergence theorem enables us to rewrite this as the integral equation

V
∂e
∂t dV = −

V
∇• q(r, t) dV +

V
Q(r, t) dV.
(10.4)
As in Example 7.39 in Section 7.6, reasoning using continuity and the arbitrariness of
V explains why (10.4) implies the PDE
∂e
∂t = −∇• q + Q.
(10.5)
We have omitted the dependence on (r, t) only so as to write the PDE in a compact
form. ⃝
Example 10.2
Derive the one space dimensional version of the heat balance equation (10.5) in a thin,
homogeneous rod 0 ≤x ≤L, 0 ≤y2 + z2 ≤R2, where R, L are constants.
Method: For an arbitrary control volume V, we derived (10.3), that is,

V
∂e
∂t dV = −

S
q(r, t) • n dS +

V
Q(r, t) dV.
It is natural to rewrite an arbitrary cylindrical slice V = {(x, y, z) : a < x < b, 0 ≤y2 +
z2 ≤R2} in an alternative cylindrical coordinates as V = {(x, r, ϑ) : a < x < b, 0 ≤
r < R, −π < ϑ ≤π}. The surface S that bounds V consists of three parts, similarly to
Example 7.36 in Section 7.5: (1) S−, the disk x = a, 0 ≤y2 + z2 ≤R2, on which n = −ˆı;
(2) S+, the disk x = b, 0 ≤y2 + z2 ≤R2, on which n = ˆı and (3) ˜S, the lateral surface
a < x < b, r = R, −π < ϑ ≤π, on which n = ˆer = cos ϑ ˆı + sin ϑ ˆj. In these cylindrical
coordinates, q = qx ˆı + qr ˆer + qϑ ˆeϑ.
So,
−

S
q(r, t) • n dS = −

S−
q(r, t) • n dS −

S+
q(r, t) • n dS −

˜S
q(r, t) • n dS
=−
π
−π
R
0
q(a, r, ϑ, t) • (−ˆı)rdrdϑ −
π
−π
R
0
q(b, r, ϑ, t) • (ˆı)r dr dϑ −
b
a
π
−π
q(x, R, ϑ, t) • ˆer dϑdx
So (10.3) is
b
a
π
−π
R
0
∂e
∂t (x, r, ϑ, t)r drdϑdx =
π
−π
R
0
q(a, r, ϑ, t) • ˆı r dr dϑ +
π
−π
R
0
q(b, r, ϑ, t) • (−ˆı)r dr dϑ
−
b
a
π
−π
q(x, R, ϑ, t) • ˆer dϑdx +
b
a
π
−π
R
0
Q(x, r, ϑ, t)r drdϑdx
=
π
−π
R
0
qx(a, r, ϑ, t)r dr dϑ −
π
−π
R
0
qx(b, r, ϑ, t)r dr dϑ −
b
a
π
−π
qr(x, R, ϑ, t)dϑ dx
+
b
a
π
−π
R
0
Q(x, r, ϑ, t)r dr dϑ dx.

838
Advanced Engineering Mathematics
If we assume that e, q, and Q do not depend on r or ϑ and we assume that qr ≡0, that is,
there is no heat flux out of the rod at any point on the lateral surface of the rod, then we
can integrate to get
πR2
b
a
∂e
∂t (x, t)dx = −πR2 
qx(b, t) −qx(a, t)

−0 + πR2
b
a
Q(x, t) dx.
The first two terms can be rewritten using
qx(b, t) −qx(a, t) =
b
a
∂qx
∂x (x, t) dx.
After dividing through by the constant πR2 and moving terms to the left-hand side,
we get
b
a
∂e
∂t (x, t) + ∂qx
∂x (x, t) −Q(x, t)

dx = 0.
This being true for all a, b satisfying 0 < a < b < L implies the PDE
∂e
∂t (x, t) = −∂qx
∂x (x, t) + Q(x, t),
the one-space-dimensional version of the heat balance equation. ⃝
Effectively, we used a “control interval” a < x < b instead of a control volume V.
10.1.1 Maxwell’s Equations of Electromagnetism
The electromotive force E produced by an electric field E along a simple, closed, piecewise
smooth curve C is defined by
E =

C
E • dr.
(10.6)
In a physical problem in which a vector field is integrated over a closed, simple curve
that is the boundary curve of a surface, Stokes’ theorem can relate that line integral to an
integral over the surface it bounds.
Michael Faraday observed that if an electric current is changing in time, it can induce
a current in another conductor. Define c to be the speed of electromagnetic waves in a
medium; for example, the speed of light in a vacuum is∗c = 2.99792458×108 m/s. Assume
that, as in Stokes’ theorem, C is a simple, closed, piecewise smooth boundary curve for
S, an oriented, piecewise smooth, parametrized surface, and assume that C and S are
fixed, that is, independent of time. The Faraday–Maxwell’s law of Induction states that E
is related to the magnetic flux density B by
∗From the National Institute of Standards and Technology, http://www.nist.gov/pml/div823/museum-
timeline.cfm, by William B. Penzes, paraphrased, “On October 20, 1983, the meter was redefined to be the
distance traveled by light in vacuum during a time interval of 1/299,792,458 of a second... The second is
determined to an uncertainty, U = 1 part in 1014 by the Cesium clock.”

Partial Differential Equations Models
839
E = −1
c
d
dt
⎛
⎝

S
B • dS
⎞
⎠.
(10.7)
Combining (10.6) and (10.7) yields
−1
c
d
dt
⎛
⎝

S
B • dS
⎞
⎠=

C
E • dr.
(10.8)
By Stokes’ theorem

C
E • dr =

S
(∇× E) • dS.
(10.9)
Because S is fixed,
d
dt
⎛
⎝

S
B • dS
⎞
⎠=

S
∂B
∂t • dS
Putting the latter together with (10.8) and (10.9) gives one of Maxwell’s integral equations
of electromagnetism:
−1
c

S
∂B
∂t • dS =

S
(∇× E) • dS.
(10.10)
To express the PDE version of the physical law in (10.10), rewrite it as

S
1
c
∂B
∂t + ∇× E

• dS = 0
and then reason that S is an arbitrary-oriented, piecewise smooth, parametrized surface;
hence,
1
c
∂B
∂t + ∇× E = 0,
that is,
1
c
∂B
∂t = −∇× E.
(10.11)
The units of ||E|| are V/m, and the units of ||B|| are Wb/m2, so the units of the two sides
of (10.11) are
1
s · Wb
m2 · m2 and V
m · m,

840
Advanced Engineering Mathematics
respectively. The equality in (10.11) thus implies that
1 Wb = 1 V
1 s .
The magnetomotive force M produced by a magnetic field H along a simple, closed,
piecewise smooth curve C is defined by
M =

C
H • dr.
(10.12)
Note that H = μ B, where μ is the magnetic permeability of the medium, in units of
henrys/m. Conceivably, μ could be a 3 × 3 matrix if the medium is anisotropic.
Ampère—Maxwell’s Law states that M is related to the electric field density D and the
electric current density J by
M =

S
J • dS + d
dt
⎛
⎝

S
D • dS
⎞
⎠,
(10.13)
where, as in Stokes’ theorem, C is a simple, closed, piecewise smooth boundary curve for
S, an oriented, piecewise smooth, parametrized surface.
Note that D = ϵ E, where ϵ is the electric permittivity∗of the medium, in units of
farads/m. Conceivably, ϵ could be a 3 × 3 matrix if the medium is anisotropic.
Combining (10.12) and (10.13), Ampère—Maxwell’s law can be restated as

S
J • dS + d
dt
⎛
⎝

S
D • dS
⎞
⎠=

C
H • dr.
(10.14)
In Problem 10.1.3.4, you will explain how to use (10.14) to get
∂D
∂t = −J + ∇× H.
(10.15)
Equations (10.11) and (10.15) are two of the four Maxwell’s equations of electromag-
netism, a crowning achievement of nineteenth century physics.
10.1.2 Continuum Mechanics
Here we will discuss the deformation of a “body” of matter. We will derive an integral
equation of continuum mechanics by using both the divergence theorem and the change
of variables formula for integration over a spatial region.
A reference state consists of particles at “material points” X in some initial, control
volume V0 = V(0) in R3. The Lagrangian description says to follow the motion of each
particle overtime. This motion is described by a function x = φ(X, t) satisfying φ(X, 0) = X.
∗From http://www.scienceworld.Wolfram.com, the permittivity of a vacuum is ϵ0 ≈8.8542 × 10−12 F/m.

Partial Differential Equations Models
841
FIGURE 10.2
Deformation of a ball.
This notation means that the particle that begins at X is located at position φ(X, t) at time t.
In this way, the material points in a control volume V0 move around to become the region
V(t) ≜{φ(X, t) : X in V0}
at time t. To understand the notation, it helps to have Figure 10.2’s picture illustrating the
concept involved.
We expect that the function φ(X, t) should depend smoothly on both X and t. For any
fixed time t > 0, the map
X →φ(X, t)
is a transformation, possibly not linear. But, we expect that the map should be invertible,
that is, for any point x in V(t), there should be exactly one initial point X such that
x = φ(X, t).
Also we expect that the inverse map
x →X,
defines an inverse function
X = φ−1(x, t)
that depends smoothly on both x and t.
We have already seen an example of such a situation in Section 6.8 where we “rigidly
rotated” the points on or inside the planet Earth around an axis ω. By “rigidly,” we mean
that the distance between points is unchanged by the rotation. A particle that begins in the

842
Advanced Engineering Mathematics
planet at location X at time t = 0 will be at position φ(X, t) = et X at time t. In this simple
example, we can explicitly find the inverse map: it is given by
φ−1(x, t) = e−t x
because x = φ(X, t) = et X implies that
e−t x = e−t(etX) = (e−t et)X = IX = X.
Define ϱ0 = ϱ0(X) to be the mass density at point X, at time t = 0, and define ϱ(x, t) to be
the mass density at point x, at time t ≥0. Note that ϱ(X, 0) = ϱ0(X).
Conservation of mass tells us that for t ≥0,

V(t)
ϱ(x, t) dx =

V0
ϱ0(X) dX.
(10.16)
Here, dx is short for dx1dx2dx3, and dX is short for dX1dX2dX3.
Define the Jacobian determinant by
J(X, t) = det
∂φ(X, t)
∂X

.
We assume J(X, t) ̸= 0 at all (X, t).
By the change of variables formula for the transformation X →φ(X, t),

V(t)
ϱ(x, t) dx =

V0
ϱ(φ(X, t), t) J(X, t) dX,
(10.17)
Why? Recall from Section 6.6 that the Jacobian measures how an element of volume, dX,
is transformed by a change of variables. The assumption that J(X, t) ̸= 0 at all (X, t) both
guarantees that the element of volume dx = |J(X, t)| dX never vanishes and guarantees the
smoothness and invertibility of the transformation X →φ(X, t). Further,
J(X, 0) = det
∂φ
∂X(X, 0)

> 0
because for small △t, φ(X, 0) ≈X + △t ∂φ
∂t (X, 0) implies J(X, 0) = det(I) = 1 > 0 and conti-
nuity in t implies J(X, t) > 0 at all (X, t); hence, |J(X, t)| ≡J(X, t).
For example, for the rigid rotation φ(X, t) = et X, the Jacobian determinant is identically
1 or −1 because ∂φ(X, t)
∂X
= et is a real, orthogonal matrix for all t.
Combining (10.16) and (10.17) gives

V0
ϱ0(X) dX =

V0
ϱ(φ(X, t), t) J(X, t) dX.

Partial Differential Equations Models
843
This being true for all control volumes V0 implies a PDE expression of conservation of
mass that is given by
ϱ0(X) = ϱ(φ(X, t), t) J(X, t).
(10.18)
Suppose further that a nice surface S is part of the boundary of a solid V and n is the
outward unit normal at a point r on S. At a point r on S, a contact force acting on the
solid at a point on the bounding surface S will exert an element of force proportional to
the surface area, that is,
dF = σ dS,
for some vector σ known as the stress, or traction, vector.
We expect stress to depend on the direction of the unit normal n. The simplest such
dependence is by a linear transformation, that is,
σ = [τ]n,
where the 3 × 3 matrix [τ] is called the stress tensor.
Recalling from Chapter 7 the notation dS = n dS, we see that
dF = [τ] dS.
(10.19)
The displacement, or deformation, of the solid is defined by
U(X, t) ≜x −X = φ(X, t) −X.
Note that U(X, 0) ≡0. Define
u = u(x, t) = U(φ−1(x, t), t).
The total linear momentum in the volume V(t) is

V(t)
ϱ(x, t) ∂u(x, t)
∂t
dx.
Using the change of variables X →φ(X, t) and conservation of mass,

V(t)
ϱ(x, t) ∂u(x, t)
∂t
dx =

V0
ϱ(φ(X, t) ∂U(X, t)
∂t
J(X, t) dX =

V0
ϱ0(X) ∂U(X, t)
∂t
dX.
(10.20)
Newton’s second law of motion states that
d
dt[total linear momentum in the volume V(t)]
= sum of the forces acting on points in V(t)
and the forces in contact with the boundary of V(t).

844
Advanced Engineering Mathematics
The body forces, f(x, t), are the forces such as gravity that act on points inside V(t). So,
using (10.20),
d
dt
⎡
⎣

V0
ϱ0(X) ∂U(X, t)
∂t
dX
⎤
⎦=

V(t)
f(x, t)dx +

∂V(t)
dF,
(10.21)
where ∂V(t) is the nice surface that encloses V(t). We want the integral on the left-hand
side to be over V0 rather than over V(t) because it will make it easier later to take the
derivative with respect to t.
The contact forces are given by (10.19), so the divergence theorem implies

∂V(t)
dF =

∂V(t)
[τ]n dS =

V(t)
∇• [τ] dx,
(10.22)
where we write [τ] in terms of its rows
[τ] ≜
⎡
⎢⎢⎢⎢⎣
τ T
1
τ T
2
τ T
3
⎤
⎥⎥⎥⎥⎦
and then define
∇• [τ] ≜
⎡
⎢⎢⎢⎢⎣
∇• τ T
1
∇• τ T
2
∇• τ T
3
⎤
⎥⎥⎥⎥⎦
.
Combining (10.21) and (10.22), we get
d
dt
⎡
⎣

V0
ϱ0(X) ∂U(X, t)
∂t
dX
⎤
⎦=

V(t)
f(x, t)dx +

V(t)
∇• [τ] dx.
(10.23)
The left-hand side of (10.23) can be rewritten as
d
dt
⎡
⎣

V0
ϱ0(X) ∂U(X, t)
∂t
dX
⎤
⎦=

V0
∂
∂t

ϱ0(X) ∂U(X, t)
∂t

dX =

V0
ϱ0(X) ∂2U(X, t)
∂t2
dX
=

V0
ϱ(φ(X, t), t) J(X, t) ∂2U(X, t)
∂t2
dX =

V(t)
ϱ(x, t) ∂2u(x, t)
∂t2
dx.

Partial Differential Equations Models
845
So, (10.23) is equivalent to

V(t)
ϱ(x, t) ∂2u(x, t)
∂t2
dx =

V(t)
f(x, t) dx +

V(t)
∇• [τ] dx.
This being true for all V(t), the PDE version of Newton’s second law of motion is the
Cauchy equation
ϱ(x, t) ∂2u(x, t)
∂t2
= f(x, t) + ∇• [τ].
(10.24)
By the way, conservation of angular momentum implies that the stress tensor is a
symmetric matrix, that is, [τ]T = [τ].
Learn More About It
Our derivation of the PDEs of continuum mechanics follows the exposition in Applied
Solid Mechanics, Peter Howell, Gregory Kozyreff, and Jim Ockendon, Cambridge
University Press, 2009. There is much more in that book, including the derivation of
the symmetry of the stress tensor.
10.1.3 Problems
In problems 1–3, as in Example 10.2, use integral equation (10.3) in R3 to
1. Derive the two-space-dimensional version of the heat balance equation (10.5) in a
short, rectangular slab {(x, y, z) : 0 ≤z ≤H, 0 ≤x ≤a, 0 ≤y ≤b}, assuming the
temperature and boundary conditions (BCs) in the usual rectangular coordinates
do not depend on z.
2. Derive the one-space-dimensional version of the heat balance equation (10.5) in
a short, cylindrical slab {(x, y, z) : 0 ≤z ≤H, 0 ≤x2 + y2 ≤R2}, assuming the
temperature and BCs in the usual cylindrical coordinates do not depend on z or
θ. [Hint: In Example 10.2, we used an arbitrary cylindrical slice over a < x < b
because we assumed no dependence on r or ϑ. In this problem, the analogous
arbitrary solid is a cylindrical shell {(r, θ, z) : a < r < b, 0 ≤θ ≤2π, 0 < z < H}.]
3. Derive the one-space-dimensional version of the heat balance equation (10.5) in a
thin, homogeneous rod assuming that the lateral side of the rod is not insulated
but instead is losing heat according to Newton’s law of cooling.
4. Use the Ampere–Maxwell’s law, (10.14), and Stokes’ theorem to derive (10.15).

846
Advanced Engineering Mathematics
10.2 Heat Equations
Recall from Section 10.1, specifically (10.5), that
∂e
∂t = −∇• q + Q,
where
e = e(r, t) is the density of heat energy in a substance, in units of energy/volume
the heat flux vector q = q(r, t) measures the time rate of heat flow, in units of
energy/(time · area)
Q = Q(r, t) is the rate of heat production, in units of energy/time
Before we can get a solvable mathematical problem, we need further concepts. In par-
ticular, we will use Fourier’s law of heat conduction, which relates the heat flux vector
to the gradient of the temperature distribution. Such a relationship is an example of a
“constitutive law.”
Also in this section, we will see a special case when the physical situation involving
heat energy is in equilibrium, that is, the temperature does not depend on time. We will
have what is known as the “steady-state problem” whose solution is the “steady state
temperature distribution.”
Let u = u(r, t) be the temperature, in units of ◦C and let c = c(r, u) be the specific heat,
in units of energy/(mass ·◦C). As usual, ϱ = ϱ(r, u) denotes the mass density, in units of
mass/volume. If the specific heat does not depend∗on the temperature, then
e = cϱu,
(10.25)
that is, the temperature is proportional to the amount of heat energy, and (10.4) in
Section 10.1 becomes
∂
∂t[cϱu] = −∇• q + Q.
(10.26)
We need one more assumption in order to get PDEs we can solve in Chapter 11: We need
a constitutive relation between the heat flux, q, and temperature, u. The simplest form of
such a relation is Fourier’s law of heat conduction
q = −κ∇u,
(10.27)
where the thermal conductivity κ = κ(r, u) either is a positive scalar or is a positive definite
matrix all of whose entries are nonnegative. These sign conventions on κ guarantee that
(10.27) implies that heat flows from hot spatial points to cool points. Note that if heat
flows preferentially in some directions then κ would be a matrix other than a multiple of
the identity matrix.
∗If the specific heat does depend on the temperature, then e(r, u) =
u
0
ϱ(r, ξ) c(r, ξ) dξ.

Partial Differential Equations Models
847
Substituting (10.27) into (10.26) gives
∂
∂t[cϱu] = ∇• (κ∇u) + Q.
(10.28)
A commonly used special case is when there is heat conduction in a homogeneous mate-
rial; the latter means that c, ϱ, and κ are constants and κ is a scalar. In this case, (10.28)
becomes
∂u
∂t = α ∇2u + Q
cϱ,
(10.29)
where
α = κ
cϱ
is called the thermal diffusivity. If, further, there is no heat source or sink, then Q ≡0.
We call
∂u
∂t = α ∇2u
(10.30)
the heat equation∗.
Recall that for a first-order ODE dy
dt = f(t, y), there is an existence and uniqueness theo-
rem that states that if an initial condition (IC) y(t0) = y0 is given, then there will be exactly
one solution of the IVP (3.19) in Section 3.2, that is,
⎧
⎪⎨
⎪⎩
dy
dt = f(t, y)
y(t0) = y0
⎫
⎪⎬
⎪⎭
,
as long as f(t, y) satisfies a technical hypothesis such as its dependence on y being
differentiable or satisfying a Lipschitz condition.
(The latter will be discussed in
Chapter 18.)
Similarly, in order to predict the temperature distribution u = u(r, t) later in time, we
must know the initial temperature distribution, that is, the function
u(r, t0),
at least for r in some spatial region in which we want to solve the PDEs (10.29) or (10.30).
In addition, in order to solve the PDE in a spatial region V, we need to know something
about the temperature’s or the heat flux’s behavior on V’s boundary surface S. This makes
sense because certainly those behaviors can affect the temperature inside V. It turns out
that, mathematically, it suffices to know either the temperature or the heat flux. We can
specify the temperature on some part(s) of S and the heat flux on another part(s).
∗Similarly, if c is the concentration of a substance, it may satisfy the heat equation, called also the diffusion
equation in this context.

848
Advanced Engineering Mathematics
Here are some commonly used BCs than can be specified at a point r on the surface S:
Dirichlet BC :
Specified temperature, u(r, t)
Neumann BC :
Specified heat flux in normal direction, that is, q(r, t) • n
Robin BC :
Specified linear combination of u(r, t) and q(r, t) • n.
(10.31)
The Neumann BC specifies the heat flux in the normal direction because that is the com-
ponent of flux that passes through the boundary; the tangential component of the flux just
moves along, rather than through, the boundary, as we can see in (10.3) in Section 10.1.
Fourier’s law of heat conduction, q =−κ∇u, says that the component of flux in the
normal direction is
n • q = −n • (κ∇u) = −κ ∂u
∂n,
assuming κ is a scalar. So, a Neumann BC in that case can be stated as
∂u
∂n = g(r, t)
(10.32)
for some given function g.
The Robin BC comes from combining Newton’s law of cooling, which states that the rate
of heat loss is proportional to the difference between a hot object and a cool medium, with
a Neumann BC. Suppose that a medium’s temperature, u0(t), is given outside of V but
near a point r on the boundary of V. Then
n • q = k(r) (u(r, t) −u0(t))
(10.33)
expresses the proportionality, where k is a constant. Similarly to what was said for a
Neumann BC, we can rewrite (10.33) as
u(r, t) + h(r) ∂u
∂n = g(r, t).
(10.34)
On different parts of the boundary, different BCs can be given. For example, if V is
the box
V = { (x, y, z) : 0 ≤x ≤a,
0 ≤y ≤b,
0 ≤z ≤c },
then on the six boundary faces of V, we might specify the BCs, for 0 < t < ∞,
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
u(0, y, z, t) = u(a, y, z, t) = 0,
for 0 < y < b, 0 < z < c
∂u
∂y(x, 0, z, t) = u(x, b, z, t) = 0,
for 0 < x < a, 0 < z < c
u(x, y, 0, t) −h∂u
∂z (x, y, 0, t) = 1, ∂u
∂z (x, y, c, t) = 0,
for 0 < x < a, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
(10.35)

Partial Differential Equations Models
849
10.2.1 Steady-State Temperature
A steady-state temperature, also known as an equilibrium temperature, is one that is
not changing in time. For example, if u satisfies the heat equation (10.30) in a solid V and
has ∂u
∂t ≡0, then
0 = ∇2u in V,
(10.36)
after substituting ∂u
∂t ≡0 and dividing through by the positive scalar α.
Equation (10.36) is called Laplace’s equation or the potential equation. For both
physical and mathematical reasons, closely related to (10.36) is Poisson’s equation,
−Q(r)
cϱ
= α ∇2u in V.
This equation is satisfied by a steady-state temperature that satisfies (10.29) if Q = Q(r)
does not depend on time, t. It makes sense that we can’t expect to have a steady-state
temperature if there is a time varying source of heat.
In the context of heat equations, the concept of “steady state” does not allow for oscil-
lation in time. This is different from the “steady-state solutions” we had in the study of
ODEs. We are using the commonly used terminologies that, unfortunately, are not consis-
tent with each other. Referring to “equilibrium temperature” rather than to “steady-state
temperature” may help us avoid confusion.
10.2.2 Lower Dimensional Problems
In practical problems, it is often true that we can ignore one or more spatial variables and
thus consider heat flow in one or two spatial dimensions instead of three.
Example 10.3
Suppose V is a thin cylindrical rod
{(x, r, ϑ) : 0 ≤r < R, 0 ≤ϑ ≤2π, 0 ≤x ≤L}
whose longitudinal axis is the x-axis. If the material properties c, ϱ, κ do not vary with r
or ϑ and the longitudinal side
{(x, r, ϑ) : r = R, 0 ≤ϑ ≤2π, 0 ≤x ≤L}
is kept perfectly insulated, then we may look for a solution of the heat equation (10.30),
that is, ∂u
∂t = α ∇2u, satisfying ∂u
∂r ≡0 and ∂u
∂ϑ ≡0. Using the result and method of
Example 10.2 in Section 10.1, (10.30) becomes
∂u
∂t = α ∂2u
∂x2 , 0 < x < L, 0 < t < ∞.
(10.37)
Along with PDE (10.37), we would need to specify BCs on the rest of the boundary,
namely, the disks 0 ≤r < R and −π < ϑ ≤π at x = 0 and x = L, again with no dependence
on r or ϑ.

850
Advanced Engineering Mathematics
For example, if the ends x = 0 and x = L are perfectly insulated, u = u(x, t) needs to
satisfy the BCs
∂u
∂x (0, t) = ∂u
∂x (L, t) = 0, 0 < t < ∞.
(10.38)
Along with the perfect insulation on the longitudinal side, (10.38) guarantees that no
heat is flowing into or out of the rod, because at the left of the rod, the heat flux out of
the rod is
q(0, t) • (−ˆı) = κ∇u(0, t) = κ ∂u
∂x (0, t) = 0,
and, similarly, at the right end the heat flux out of the rod is zero.
If the BCs on either of the disks 0 ≤r < R and −π < ϑ ≤π at x = 0 and x = L involve
either r or ϑ dependence by the temperature, for example, a Dirichlet BC specifying a
nonconstant temperature dependence
u|x=0 = f(r, ϑ),
then we should not expect the solution to be independent of r and ϑ, even if inside the thin
rod, the material properties c, ϱ, κ do not depend on r or ϑ.
Example 10.4
Suppose a thin, longitudinally insulated homogeneous rod has its left end kept at 20◦C
and heat is leaving the right end at a rate of 100 W/m2. Assume the mass density of
the material in the rod is ϱ kg/m3, the specific heat is c J/(kg ·◦C), and the thermal
conductivity is κ W/(m ·◦C). Also, assume that heat is being generated inside the rod at
a constant rate of Q W/m3. Find the equilibrium temperature distribution.
Method: Because the longitudinal side is insulated, the material is homogeneous, and the
source term and the BCs do not depend on r, ϑ, or t, we need to look for an equilibrium
temperature u = u(x) that solves
0 = α d2u
dx2 + Q
cϱ
and u(0) = 20, as well as a BC at x = L. The latter has
q(L) • ˆı = 100 W/m2.
As is often the case in physical problems, it helps to keep track of the units.
At x = L, Fourier’s law of heat conduction gives
q(L) = −

κ
W
m ·◦C

·
du
dx (L)
◦C
m

ˆı;
hence,
100 W/m2 = −

κ
W
m ·◦C

·
du
dx (L)
◦C
m

.
So, the appropriate BC at the right end is du
dx (L) = −100
κ , in units of ◦C/m. Note that κ
is constant because we assumed that the rod’s material is homogeneous.

Partial Differential Equations Models
851
Because α = κ
cϱ , the ODE to be satisfied by u(x) can be rewritten as
u′′ = −Q
κ .
The right-hand side is a constant, so indefinite integration with respect to x of both sides
gives
u′ = −Q x
κ
+ c1,
where c1 is an arbitrary constant. Using the BC at x = L, we get
−100
κ
= u′(L) = −Q L
κ
+ c1;
hence, c1 = (Q L −100)/κ, and so
u′ = 1
κ (−Q x + Q L −100).
A second indefinite integration with respect to x of both sides gives
u = 1
κ

−Q
2 x2 + (Q L −100)x

+ c2,
where c2 is an arbitrary constant. The BC at the left end requires
20 = u(0) = 0 + c2.
So, the equilibrium temperature distribution is
u(x) = 20 + 1
κ

−Q
2 x2 + (Q L −100)x

. ⃝
It turns out that u′(0) > 0; hence, there is a positive heat flux out of the left end of the rod,
not just the right end. The BC at x = L required positive heat flux out of the right end of the
rod. Positive heat flux out of the left end of the rod is a result of the combined effects of
the ODE and BCs.
The units of both α d2u
dx2 and Q
cϱ are ◦C/s. Also, note that the radius of the thin rod is not
needed to solve this problem.
For a thin solid rod of copper, the physical parameters include κ ≈390 W/(m ·◦C). Sup-
posing also that we have more information, specifically that Q = 60 W/m3 and L = 2 m,
then the equilibrium temperature distribution is
u(x) ≈20 + 0.002564

−30x2 + 20x

, 0 < x < 2,
which is graphed in Figure 10.3. Note that u′(2) ≈−0.2564 at the right end.
Example 10.5
The region between concentric spheres of radii 3 and 6 cm is filled with a homogeneous
material whose thermal conductivity is constant κ = 50 W/(m ·◦C) and is generating
heat at a rate of 960 w/m3. Suppose the inside of the shell, that is, the ball of radius 3 cm,

852
Advanced Engineering Mathematics
20.00
u
19.95
19.90
19.85
x
u΄(2) ≈–0.2564
0.5
1.0
1.5
2.0
FIGURE 10.3
Temperature distribution.
consists of a perfect thermal insulator and the outer boundary of the shell is kept at a
constant temperature of 10◦C. Find the steady-state temperature distribution between
the spheres.
Method: Let u = u(ρ) be the steady-state temperature; it does not depend on φ, θ because
the material is homogeneous and the BCs ∂u
∂n(0.03) = 0 and u(0.06) = 10, do not depend
on φ, θ, or t. On the inner sphere, the outward unit normal vector is n = −ˆeρ, so ∂u
∂n =
−∂u
∂ρ there.
The steady-state temperature satisfies the PDE 0 = κ ∇2u + Q. Using the Laplacian in
spherical coordinates given at the end of Section 6.7, the ODE BVP we need to solve is
0 = 50 1
ρ2
d
dρ

ρ2 du
dρ

+ 960,
du
dρ (0.03) = 0,
u(0.06) = 10.
(10.39)
Rewrite the ODE in (10.39) as
d
dρ

ρ2 du
dρ

= −19.2 ρ2,
and then integrate once with respect to ρ to get
ρ2 du
dρ = −6.4ρ3 + c,
(10.40)
where c is an arbitrary constant. At this point, we may substitute in the inner BC to get
0 = 0.032 du
dρ (0.03) = −6.4(0.033) + c;
hence, c = 1.728 × 10−4. Substitute this into (10.40) to get ρ2 du
dρ = −6.4ρ3 + 1.728 × 10−4.
Divide through by ρ2 to get
du
dρ = −6.4ρ + 1.728 × 10−4ρ−2,
and then integrate with respect to ρ to get
u(ρ) = −3.2ρ2 −1.728 × 10−4ρ−1 + c2,
where c2 is an arbitrary constant. We substitute in the outer BC to get
10 = u(0.06) = −3.2(0.062) −1.728 × 10−4(0.06−1) + c2.

Partial Differential Equations Models
853
10.005
u
10.004
10.003
10.001
10.002
ρ
0.040
0.050
0.060
FIGURE 10.4
Temperature distribution between concentric spheres.
This gives
c2 = 10 + 1.152 × 10−2 + 2.88 × 10−3 = 10.0144.
The equilibrium temperature distribution is
u(ρ) = 10.0144 −3.2ρ2 −1.728 × 10−4ρ−1
and is graphed in Figure 10.4. ⃝
Example 10.6
Set up, but do not solve, a PDE and BCs for the steady-state temperature distribution in a
thin homogeneous plate 0 ≤x ≤a, 0 ≤y ≤b. Assume that there is no heat source or sink
inside the plate, and the top and bottom flat surfaces of the plate are insulated. Assume
that in the xy-plane, the “vertical” sides x = 0 and x = a are insulated, the bottom
horizontal side is kept at 0◦C, and the top horizontal side has temperature distribution
u = ax −x2.
Method: Let u = u(x, y) be the steady-state temperature; it does not depend on z because
the material is homogeneous, the plate is thin, and the top and bottom surfaces are
insulated. The steady-state temperature satisfies Laplace’s equation; because of the lack
of z dependence, this becomes 0 = ∂2u
∂x2 + ∂2u
∂y2 . For two of the BCs, we are given that the
sides x = 0, 0 < y < b, and x = a, 0 < y < b, are insulated. There, n = ±ˆı; hence, those
BCs are ∂u
∂x (0, y) = ∂u
∂x (a, y) = 0. On the other two sides, the temperature is specified.
Putting everything together, the steady-state temperature should satisfy the problem
consisting of PDE, BC on each of the vertical sides of a rectangle, and BC on each of the
horizontal sides of a rectangle:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 ,
0 < x < a, 0 < y < b
∂u
∂x (0, y) = ∂u
∂x (a, y) = 0,
0 < y < b
u(x, 0) = 0, u(x, b) = ax −x2,
0 < x < a.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
. ⃝
Figure 10.5 summarizes the conclusions of Example 10.6.
We will learn how to use Fourier series to solve this problem in Section 11.3.

854
Advanced Engineering Mathematics
y
b
x
u= ax – x2
∂u
∂u =0
∂2u
∂y2 =0
+
∂2u
∂x2
∂u
∂x =0
a
u=0
FIGURE 10.5
Boundary value problem.
10.2.3 Composite Rod
Suppose a thin homogeneous rod for 0 ≤x ≤L, as in Example 10.3, consists of two different
materials joined at the point x = 1, where 1 < L. Then each of the material properties c, ϱ, κ
may be two different constants depending on whether x < 1 or x > 1. Generally we will
assume that the temperature and the heat flux vector should be continuous at x = 1, that is,
u(1−, t) = u(1+, t)
and
q(1−, t) = q(1+, t).
Because of the thin rod assumption, we assume that q(x, t) = qxˆı. Further, by using
Fourier’s law, we have
q(x, t) = −κ(x)∂u
∂x(x, t)ˆı.
So, continuity of heat flux requires
lim
x→1−

κ(x)∂u
∂x(x, t)

= lim
x→1+

κ(x)∂u
∂x(x, t)

,
that is,
κ(1−) ∂u
∂x(1−, t) = κ(1+) ∂u
∂x(1+, t).
Yet another interesting situation is the case of contact resistance when two pieces of the
same material are not perfectly joined at x = 1. In this case, the BCs usually assumed are
continuity of heat flux and the dependence of the heat flux on the temperature jump at the
interface:
∂u
∂x(1−, t) = ∂u
∂x(1+, t)
and
u(1+, t) −u(1−, t) = h∂u
∂x(1, t).

Partial Differential Equations Models
855
10.2.4 Problems
1. Solve the ODE BVP
⎧
⎪⎪⎨
⎪⎪⎩
1
r
d
dr

r du
dr

= −1, a < r < b
u(a) = T0, u′(b) = 0
⎫
⎪⎪⎬
⎪⎪⎭
,
where 0 < a < b and T0 are constants.
2. Find the equilibrium temperature for
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂u
∂t = ∂2u
∂x2 + x, 0 < x < L, 0 < t < ∞
∂u
∂x(0, t)−2u(0, t)=0, u(L, t)=T1, 0<t<∞
u(x, 0) = f(x), 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
in terms of the unspecified constants T1 and L.
3. Find the equilibrium temperature for the problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂u
∂t = ∂2u
∂x2 −η(u(x, t)−¯T), 0<x<L,0<t<∞
u(0, t)= T0, u(L, t)= T1, 0< t <∞
u(x, 0) = f(x), 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
in terms of the unspecified positive constants η, ¯T, T0, T1, and L.
4. A thin copper rod is 0.3 m long. Assume the lateral surface of the rod is perfectly
insulated. Assuming there is no internal heat source or sink, what is the equilib-
rium temperature distribution in the rod if the left end is kept at temperature T0
and the right end is kept at temperature 2T0?
5. Consider the problem of finding the equilibrium temperature distribution in a
thin, homogeneous rod with internal heat source proportional to the temperature
modeled by ∂u
∂t = ∂2u
∂x2 + 4u, 0 < x < L. Assume the lateral surface of the rod is
perfectly insulated and the rod is insulated at the right end and is kept at a con-
stant nonzero temperature at the left end. For which value(s), if any, of L, is there
(a) exactly one solution? (b) infinitely many solutions, and (c) no solution?
6. Suppose the equilibrium temperature distribution u = u(x) in a thin, homoge-
neous rod of length 1 satisfies

0 = u′′ + π2u
u′(0) = A, u′(1) = B

,

856
Advanced Engineering Mathematics
where A, B are unspecified constants. Find a condition(s) on A and B that
guarantees that there is an equilibrium temperature distribution, and then find it.
7. A thin copper rod has length L. Assume the lateral surface of the rod is perfectly
insulated. What is the equilibrium temperature distribution in the rod if the left
end is kept insulated, the right end is kept at temperature T1, and there is an
internal heat source Q(x) = cos
 πx
L

? Assume that the thermal conductivity, κ, is
constant.
8. Suppose a thin rod made of a homogeneous material has constant cross section
and length L, and suppose there is a heat source Q(x) = g′′(x) for some function g.
[Do not specify g or try to solve for it.] Assume the lateral surface of the rod is per-
fectly insulated. Suppose the rod is in thermal equilibrium and the temperatures
at the left and right ends are constants T0, T1, respectively. (a) Find the equilibrium
temperature distribution in the rod, in terms of T0, T1, L, and g(x), and (b) find the
rate of heat flow out of the rod at the left end, in terms of T0, T1, L, and g′(x).
9. Describe the physical situation modeled by the problem
∂u
∂t = 1
ρ2
∂
∂ρ

ρ2 ∂u
∂ρ

+
1
ρ2 sin φ
∂
∂φ

sin φ ∂u
∂φ

−Q0,
π
2 ≤φ ≤π, 0 ≤ρ < a, 0 < t < ∞
∂u
∂ρ (a, φ, t) = 0, π
2 ≤φ ≤π, 0 < t < ∞
1
ρ
∂u
∂φ

ρ, π
2 , t

= −

u

ρ, π
2 , t

−20

,
0 ≤ρ < a, 0 < t < ∞
u(ρ, φ, 0) = 40, π
2 ≤φ ≤π, 0 ≤ρ < a.
In particular, in what physical region is this taking place?
10. Write down mathematical equations to model the physical situation of a double-
walled cylindrical rod of length L whose cross section is shown in Figure 10.6. The
inner, unshaded region contains a solid metal in which heat is being conducted.
The outer, shaded region contains a different metal whose inner wall dissipates
a
b
FIGURE 10.6
Problem 10.2.4.10.

Partial Differential Equations Models
857
heat from the inner solid and whose outer wall dissipates heat to the surrounding
medium, both according to Newton’s law of cooling. The z = 0 end of the rod is
kept at a constant temperature T0, and the end z = L is insulated. Assume that the
material properties of the two metals are different.
11. Regarding remarks after Example 10.4, recall that the component of flux in the
normal direction is n • q =−κ ∂u
∂n, if κ is a scalar. Suppose that the object is a thin,
homogeneous rod. Note that at the left end of the rod, n =−ˆı; at the right end
of the rod, n = ˆı. So, the normal derivative at the left end of the rod is ∂u
∂n =
−∂u
∂x. Explain why (a) heat flux out of the rod at x = 0 is positive if, and only if,
∂u
∂x

at x=0 > 0, and (b) heat flux out of the right end of the rod at x = L is positive
if, and only if, ∂u
∂x

at x=L < 0.
12. Suppose the temperature is in equilibrium in a thin annular plate a < r < b
made of a homogeneous material in which there is no heat generation or absorp-
tion inside. Suppose the annulus outer circle is kept at temperature 0◦C. and the
inner circle is kept at temperature T0 ≜u(a). If the temperature distribution in
the plate does not depend on θ, find a formula for the total heat flow out of the
plate through the inner circle, in terms of the unspecified constants a, b, T0 and the
unspecified constant thermal conductivity κ.
13. Suppose the temperature is in equilibrium in a thin annular plate a < r < b made of
a homogeneous material in which there is no heat generation or absorption inside.
Suppose the annulus inner circle is kept at temperature 0◦C, and β is the total rate
of heat flow out of the plate through the outer circle. If the temperature distribu-
tion in the plate does not depend on θ, find a formula for the constant temperature
T ≜u(b), in terms of the unspecified constants a, b, β and the unspecified constant
thermal conductivity κ.
14. Assume a homogeneous ball has radius a in m, the spherical surface is kept at
20◦C, and inside the ball heat is generated at a rate of 1000 W/m3. Find (a) the
steady-state temperature distribution, and (b) the rate at which heat is flowing out
of the ball. Your conclusions should be in terms of the unspecified parameter a.
15. Three pieces of wire, with thermal conductivities κ1, κ2, κ3, respectively, are joined
in perfect thermal contact at the point (x, y) = (0, 0), as shown in Figure 10.7.
y
v
H
x
u
w
–L
L
FIGURE 10.7
Three wires in perfect contact.

858
Advanced Engineering Mathematics
y
x
L
–L
–2L
FIGURE 10.8
Line and circle in perfect contact.
Let u(x, t), −L < x < 0, v(y, t), 0 < y < H, and w(x, t), 0 < x < L be the tempera-
ture distributions in the three pieces of wire. Write down all of the BCs needed to
model perfect thermal contact at (0, 0).
16. Two pieces of wire, with thermal conductivities κ1, κ2, respectively, are joined
in perfect thermal contact at the point (x, y) = (−L, 0), as shown in Figure 10.8.
The first piece of wire is a line segment, and the second piece is a circle. Let
u(x, t), −2L < x < 2L, and v(θ, t), −π < θ < π be the temperature distributions in
the two pieces of wire. Write down all of the BCs needed to model perfect thermal
contact at (−L, 0).
17. Suppose the sectors,
D1 =

(r, θ) : 0 < r < a, 0 < θ < π
4
 
,
D2 =

(r, θ) : 0 < r < a, π
4 < θ < π
2
 
,
with thermal conductivities κ1, κ2, respectively, are joined in perfect thermal con-
tact. Write down all of the BCs needed to model perfect thermal contact, assuming
the temperature in Di is ui = ui(r, θ, t), for i = 1, 2.
In problems 18–20, the temperature of a thin, homogeneous rod of constant cross-sectional
area A satisfies the problem
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 + 1
cϱ Q(x), 0 < x < L, 0 < t
∂u
∂x(0, t) = ∂u
∂x(L, t) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,
where α is a constant. Define the total energy in the rod by E = cϱ A
L
0
u(x, t) dx. Then
E = E(t), that is, E may be a function of t.
18. If Q(x) ≡0, explain why E(t) is constant. [Hint: What ODE does E(t) satisfy?]

Partial Differential Equations Models
859
19. If Q(x) ≡Q0 is constant and E(t) = E(0) + βt for some constant β, find Q0 in
terms of the other constants mentioned in the problem. [Hint: What ODE does
E(t) satisfy?]
20. If ¯Q ≜1
L
 L
0 Q(x) dx is the average value of Q and E(t) = E(0)+βt for some constant
β, find ¯Q in terms of the other constants mentioned in the problem. [Hint: What
ODE does E(t) satisfy?]
21. Suppose that U(x, t) solves the problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂U
∂t = ∂2U
∂x2 −U, 0 < x < π, t > 0,
U(0, t) = U(π, t) = 0, t > 0,
U(x, 0) = exf(x), 0 < x < π,
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
and T(x, t) ≜e−xU(x, t).
(a) Explain why T(x, t) will solve the PDE BVP–IVP
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 +2∂T
∂x , 0<x<π, t>0
T(0, t) = T(π, t) = 0, t > 0,
T(x, 0) = f(x), 0 < x < π.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
(10.41)
(b) Use this to solve (10.41) for the initial condition T(x, 0) = sin
πx
L

, 0 < x < π.
22. Suppose u = u(x, t) satisfies the PDE-BVP
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < L, 0 < t
∂u
∂x(0, t) = u(L, t) = 0, 0 < t
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
Define G(t) ≜
 L
0
1
2((u(x, t))2 dx. Explain why ˙G(t) ≤0 for 0 < t.
23. Suppose the temperature is in equilibrium in a spherical shell a < ρ < b made of
a homogeneous material. Suppose the shell’s inner sphere is kept at temperature
20◦C. and β is the total rate of heat flow out of the shell through the outer sphere
ρ = b. If the temperature distribution in the shell is spherically symmetric, that
is, does not depend on angles φ or θ, find a formula for the constant temperature
T ≜u(b), in terms of the unspecified constants a, b, β and the unspecified constant
thermal conductivity κ.

860
Advanced Engineering Mathematics
10.3 Potential Equations
As we saw, a steady-state temperature may satisfy Laplace’s equation
∇2u = 0
(10.42)
or Poisson’s equation
∇2u = −Q
κ .
(10.43)
In fact, many static or steady physical problems are solvable using Laplace’s or Poisson’s
equation.
For example, suppose that the fluid is flowing in a region V and the fluid velocity is
v = u ˆı + v ˆj + w ˆk.
Potential flow assumes that the vector field v is exact, that is, that
v = ∇
(10.44)
for some scalar potential function . If the fluid is incompressible, then
∇• v = 0.
(10.45)
In that case, substitute (10.44) into (10.45) to get
0 = ∇• v = ∇• (∇) = ∇2,
so  would satisfy Laplace’s equation. This example of fluid flow is one of many reasons
why Laplace’s equation is referred to as a potential equation.
By the way, if we had assumed that v is irrotational, that is, that the vorticity
ω ≜∇× v
is the zero vector and that v is continuously differentiable in V, then Theorem 6.3 in Section
6.4 would guarantee that v is exact and thus that there is a potential function .
In addition, in this section, we will learn about some physically significant mathematical
properties of solutions of Laplace’s and Poisson’s equation.
10.3.1 Magnetostatics
Another source of potential equation models is magnetostatics. If the magnetic flux den-
sity, B, does not depend on time, t, then a consequence of Faraday–Maxwell’s law,
specifically (10.11) in Section 10.1, implies that
∇× E = 0.
(10.46)

Partial Differential Equations Models
861
Analogously to the case of irrotational fluid flow, if E is continuously differentiable in a
region V, then (10.46) would imply that there exists a “scalar” potential function , that is,
a function satisfying
∇ = −E.
(10.47)
In addition, another of Maxwell’s laws relates the electric field to the electric charge
density, ϱ :
∇• E = 4πϱ.
(10.48)
If we substitute (10.47) into (10.48), then we see that  satisfies Poisson’s equation because
−4πϱ = ∇• (−E) = ∇• (∇),
which yields
∇2 = −4πϱ.
10.3.2 Boundary Conditions
As for the steady-state heat equation, it makes sense that what happens on the boundary
of a region can affect what happens inside. This is true whether the region is in R3, or
R2, or even R1 if symmetry, or homogeneity and thinness, allows us to look for a solution
in a lower number of spatial dimensions.∗As for heat problems, for Laplace’s or Pois-
son’s equation, everywhere on the boundary, we should specify a BC. Again, we may
specify different types of BCs on different parts of the boundary, as in Example 10.5 in
Section 10.2.
10.3.3 Properties of Solutions
Example 10.7
Suppose u = u(r) satisfies Laplace’s equation at all r in a region V whose boundary,
denoted by ∂V, is a simple, closed, piecewise smooth, positively oriented surface S.
Explain why

S
∂u
∂n dS = 0.
(10.49)
Method: By the divergence theorem,

S
∂u
∂n dS =

S
(∇u) • n dS =

V
∇• (∇u) dV =

V
∇2u dV =

V
0 dV = 0. ⃝
∗Historically, such lower dimensional problems were often amenable to special mathematical techniques. Even
today, computational methods that would take too much processing time in three space dimensions may give
conclusions for lower dimensional versions of a physical problem.

862
Advanced Engineering Mathematics
Example 10.8
Suppose u = u(r) satisfies the Neumann problem for Laplace’s equation, that is,
⎧
⎨
⎩
∇2u(r) = 0,
r in V
∂u
∂n = g(r),
r in S
⎫
⎬
⎭,
where the nice surface S encloses V. Then g must satisfy the solvability condition

S
g(r)dS = 0,
(10.50)
that is, the average value of g on S must be zero.
The results of both Examples 10.7 and 10.8 make sense in the case where u, the solution of
Laplace’s equation, is the steady-state temperature distribution. Laplace’s equation mod-
els a steady-state temperature when (a) the material is homogeneous and thus the thermal
conductivity κ is constant in V and (b) there is no heat sink or source in V. In this case,
κ

S
∂u
∂n dS
measures the total heat flux out of V through S. To have a steady-state temperature
distribution and no heat sink or source, the total heat flux out of V must be zero.
The total heat flux can be zero if heat flux out of one part of S is balanced by heat flux
into another part of S. So, the solvability condition does not imply that g is identically zero.
Example 10.9
Suppose u = u(r) satisfies the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∇2u(r) = 0,
r in the half disk D = {(r, θ) : 0 ≤r < a, 0 < θ < π}
∂u
∂θ (r, 0) = g1(r),
0 < r < a
∂u
∂r (a, θ) = g2(θ),
0 < θ < π
∂u
∂θ (r, π) = g3(r),
0 < r < a
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
Find the solvability condition in terms of the functions g1, g2, g3.
Method: The positively oriented boundary of D consists of three curves, as depicted in
Figure 10.9. Parametrize the curves by
C1 : r1 = rˆer

θ=0, 0 ≤r ≤a .
C2 : r2 = aˆer, 0 ≤θ ≤π.
C3 : r3 = (a −t)ˆer

θ=π, 0 ≤t ≤a.

Partial Differential Equations Models
863
–a
a
x
y
C2
C3
C1
∂u
∂r (a, θ) = g2(θ)
∂u
∂θ (r, π)=g3(r)
∂u
∂θ (r, 0) = g1(r)
FIGURE 10.9
Example 10.9.
Similar to Example 10.8’s solvability condition (10.49), a solution of a Neumann problem
for Laplace’s equation in two space dimensions must satisfy the solvability condition
0 =

C
∂u
∂n ds.
We calculate
0 =

C
∂u
∂n ds =

C1
∂u
∂n(r, 0) ds +

C2
∂u
∂n(a, θ) ds +

C3
∂u
∂n(a −t, π) ds.
In polar coordinates, that is, cylindrical coordinates without z, ∇u = ∂u
∂r ˆer + 1
r
∂u
∂θ ˆeθ.
On C1, the outward unit normal vector is n = −ˆeθ, so ∂u
∂n = −1
r
∂u
∂θ . On C1, ds =

dr
dr

 dr =

 d
dr[rˆer]

 dr =

ˆer

 dr = dr.
On C2, n = ˆer, so ∂u
∂n = ∂u
∂r and ds =

 dr
dθ

 dr =

 d
dθ
!
aˆer
" 
 dθ =

aˆeθ

dθ = a dθ.
On C3, n = ˆeθ, so ∂u
∂n = 1
r
∂u
∂θ and ds =

dr
dt

dt =

 d
dt
!
(a −t)ˆer
"
 dt = || −ˆer

dt = dt.
The solvability condition is
0 = −
a
0
1
r
∂u
∂θ (r, 0) dr + a
π
0
∂u
∂r (a, θ) dθ +
a
0
1
a −t
∂u
∂θ (a −t, π) dt.
In the latter term, make the change of variables r = a −t to get
 a
0
1
a −t
∂u
∂θ (a −t, π) dt =
 0
a
1
r
∂u
∂θ (r, π)(−dr) =
 a
0
1
r
∂u
∂θ (r, π) dr.
The solvability condition is
0 = −
a
0
1
r g1(r) dr + a
π
0
g2(θ) dθ +
a
0
1
r g3(r) dr,

864
Advanced Engineering Mathematics
that is, the solvability condition is
a
0
1
r g1(r) dr −
a
0
1
r g3(r) dr = a
π
0
g2(θ) dθ. ⃝
Theorem 10.1
(Minimum and maximum principles) Suppose u satisfies the Dirichlet problem for
Laplace’s equation in an open region V in R3 enclosed by a nice surface ∂V, that is,
⎧
⎨
⎩
∇2u(r) = 0,
r in V
u = g(r),
r in ∂V
⎫
⎬
⎭.
Suppose that g is continuous on ∂V, and denote by gmin and gmax the minimum and
maximum values of g on ∂V. Then
(a) gmin ≤u(r) ≤gmax for all r inside V.
(b) If g(r0) > gmin for at least one r0 in ∂V, then u(r) > gmin for all r in V.
(c) If g(r0) < gmax for at least one r0 in ∂V, then u(r) < gmax for all r in V.
Results in R2 are analogous, with V replaced by D and nice surface ∂V replaced by nice
curve C.
Theorem 10.2
(Poisson’s formula for a disk) Suppose u satisfies the Dirichlet problem for Laplace’s
equation in the disk D = {(r, θ) : 0 ≤r < a, −π < θ ≤π}, that is,
⎧
⎨
⎩
∇2u(r) = 0,
r in disk D
u(a, θ) = g(θ),
−π < θ ≤π
⎫
⎬
⎭.
Suppose that g(θ) is continuous and periodic with period 2π. Then
u(r, θ) = 1
2π
π
−π
a2 −r2
a2 + r2 −2a r cos(θ −ϑ) g(ϑ) dϑ = 1
2π
π
−π
||R||2 −||r||2
||R −r||2
g(ϑ) dϑ,
where R = R(ϑ) = a

cos ϑ ˆı + sin ϑ ˆj

, −π ≤ϑ ≤π, parametrizes the circle ||R|| = a that
bounds the disk and let r = r(r, ϑ) = r

cos θ ˆı + sin θ ˆj

be any point inside the disk, that
is, has ||r|| < a.
So, Poisson’s formula gives a formula for the values of u inside the disk in terms of the
boundary values of u on the circle.

Partial Differential Equations Models
865
In particular, at the center of the disk, that is, at r = 0,
u|origin = 1
2π
π
−π
g(ϑ) dϑ,
that is, u(0) equals the average of the boundary value of u.
This result generalizes to R3.
Theorem 10.3
Suppose u satisfies the Dirichlet problem for Laplace’s equation in a ball V =
{r:||r −r0|| < a}, that is,
⎧
⎨
⎩
∇2u(r) = 0,
r in ball V
u(r) = g(r),
on ||r −r0|| = a
⎫
⎬
⎭.
Suppose that g(r) is continuous on the sphere {r : ||r −r0|| = a}. Then
u(r0) =
1
4πa2

{r: ||r−r0||=a}
g(r) dS,
that is, the value of u at the center of the ball is the average of the boundary values of u on
the sphere.
It is also true∗that if u satisfies the Laplace’s equation in the ball {r : ||r −r0|| < a} and is
continuous on the closed ball {r : ||r −r0|| ≤a}, then
u(r0) =
1
πa3

{r: ||r−r0||≤a}
u(r) dV.
10.3.4 Problems
1. Suppose u = u(x) solves the 1D Laplace’s equation u′′(x) = 0 for 0 < x < L.
Explain why u′(L) −u′(0) = 0. [The latter is the 1D version of the solvability
condition for the Neumann problem.]
2. Suppose u = u(x) solves a 1D Poisson’s equation u′′(x) = −Q(x) for 0 < x < L. Find
an equation satisfied by u′(0), u′(L), and
L
0
Q(x) dx.
3. A coaxial cable has inner radius 1.3 cm and outer radius 2.5 cm. Suppose the elec-
tric potential is a constant V0 on the surface r = 1.3, is a constant V1 on the surface
∗Nice explanations for this and for Theorem 10.3 are in Section 2.2.2 of Evans (1988).

866
Advanced Engineering Mathematics
r = 2.5, and satisfies Laplace’s equation and is circularly symmetric between those
two surfaces. Find the potential between the two surfaces.
4. For the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∇2u = 0, 0 < x < L, 0 < y < H
∂u
∂x(0, y) = 0, ∂u
∂x(L, y) = g(y), 0 < y < H
∂u
∂y(x, 0) = f(x), ∂u
∂y(x, H) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
,
find the solvability condition in terms of the unspecified functions f, g. Express
your final conclusion in terms of
 L
0 f(x) dx and
 H
0 g(y) dy.
5. For what value of the constant k does the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∇2u = 0, 0 < x < L, 0 < y < H
∂u
∂x(0, y) = y, ∂u
∂x(L, y) = 0, 0 < y < H
∂u
∂y(x, 0) = 0, ∂u
∂y(x, H) = k, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
have a solution? Express your final conclusion for k in terms of L and H.
6. Suppose u satisfies the Neumann problem for Laplace’s equation in the half ball
{(ρ, φ, θ) : 0 ≤ρ < a, 0 ≤φ < π
2 , −π < θ ≤π}. Write down the solvability condition
in as much detail as possible, including use of specific formulas for the outward
unit normal vector and the gradient in spherical coordinates.
7. Suppose ∇2u = 0 in the region D that lies between the rectangle and the circle r = a
drawn in Figure 10.10 and suppose the boundary of the rectangle is insulated
except on the upper horizontal line segment {(x, y) : −L < x < L, y = H}. Find the
∂u
∂y (x, H)=g(x)
2H
x
a
y
= f(θ)
∂u
∂r
2L
FIGURE 10.10
Problem 10.3.4.7.

Partial Differential Equations Models
867
solvability condition in terms of the unspecified functions f, g. Express your final
conclusion in terms of
 π
−π f(θ) dθ and
 L
−L g(x) dx.
8. Suppose the temperature is in equilibrium in a thin annular plate a < r < b made
of a homogeneous material with no internal heat source or sink. If the temperature
u = u(r, θ) satisfies
b ∂u
∂r (b, θ) ≡a ∂u
∂r (a, θ),
does it satisfy the solvability condition?
9. For
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t = 1
r
∂
∂r

r ∂u
∂r

−4, 0 ≤r < a
| u(0+, t) | < ∞, ∂u
∂r (a, t) = 5
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,
for what value of “a” is there an equilibrium temperature distribution? (b) for all
such value(s) of a, find the equilibrium solution(s) and confirm that the solution
satisfies the solvability condition.
10. For what value(s) of “a” is there an equilibrium temperature distribution for
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t = 1
r
∂
∂r

r ∂u
∂r

−4 u
r2 , 0 ≤r < a
| u(0+, t) | < ∞, ∂u
∂r (a, t) = 5
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
?
For all such value(s) of a, find the equilibrium solution and confirm that the
solution satisfies the solvability condition.
11. For the Poisson’s equation in Problem 10.2.4.7, (a) find the solvability condition,
and (b) use your result from part (a) to find the rate of heat flow out of the right
end of the homogeneous rod.
12. Suppose u = u(r) satisfies Poisson’s equation κ∇2u =−Q in a solid V for some
unspecified function Q(r). Explain why

S
∂u
∂n dS +

V
Q
κ dV = 0,
where κ is constant and the positively oriented, piecewise smooth, parametrized
surface S encloses V.
13. Suppose u = u(r) satisfies the Neumann problem for an anisotropic variant of
Laplace’s equation, that is,
⎧
⎨
⎩
∇• (κ(r)∇u) = 0,
r in V
n • (κ(r)∇u) = g(r),
r in S
⎫
⎬
⎭,

868
Advanced Engineering Mathematics
where the solid V is enclosed by a positively oriented, piecewise smooth,
parametrized surface S. Find the solvability condition to be satisfied by u.
14. Suppose u = u(x, y, z) satisfies Poisson’s equation κ∇2u =−Q(x, y, z) in a solid V for
some unspecified function Q, where κ is constant. Is

V
∇u •∇u dV=

S
u ∂u
∂n dS+

V
uQ
κ dV,
where the positively oriented, piecewise smooth, parametrized surface S encloses
V? If so, why? If not, why not? [Hints: Multiply the PDE through by u and then
use Corollary 6.1 in Section 6.7.]
15. Suppose u = u(r, θ) satisfies Laplace’s equation in the exterior of a disk, specifically
for a < r < ∞. Define a change of variables by p = 1
r and let U(p, θ) = u

1
p, θ

.
Explain why U satisfies the PDE
∂2U
∂p2 + 1
p
∂U
∂p + 1
p2
∂2U
∂θ2 = 0.
10.4 Wave Equations
The simplest wave equation is, in one space dimension,
1
c2
∂2y
∂t2 = ∂2y
∂x2 ,
(10.51)
where c is called the wave speed. The formula for the wave speed in a string is c =
#
T0/ϱ0,
where T0 is the tension in the string and ϱ0 is the linear mass density. As we will see in
Section 11.2, (10.51) has “product solutions” whose dependence on time, t, is given by
c1 cos
 nπct
L

+ c2 sin
 nπct
L

. These are periodic functions having frequencies of vibration
ωn = nπc
L , that is, frequencies fn = ω
2π = nc
2L Hz.
10.4.1 Guitar String
Example 10.10
Suppose a guitar string has mass density of 4.466 × 10−3 kg/m and length 640.0 mm
and is under tension equal to the weight of a mass of 9.03 kg at sea level. Find the basic
frequency of vibration of the string.
Method: The n = 1 mode has the basic frequency, that is, the basic frequency is f1 =
c
2L Hz. The tension is found by multiplying 9.03 kg by 9.80665 m/s2, the acceleration due
to gravity at sea level, so T0 ≈(9.03) · (9.80665) N ≈88.554 N. The wave speed is
c ≈
#
T0/ϱ0 =
$
88.554 N/(4.466 × 10−3 kg/m) ≈140.81 m/s. The basic frequency is
f1 ≈140.81 m/s
2(0.640 m) ≈110 Hz. ⃝

Partial Differential Equations Models
869
This frequency of 110 Hz gives a “pitch” of A2, in “scientific notation,” that is, A in
“Helmholtz notation,” and is a minor tenth below “middle C.′′ The fifth lowest of the six
guitar strings is tuned to this pitch.
Guitar players also sometimes finger a string so as to excite a higher harmonic, that is,
not the basic frequency, because that way, they can play very high notes that also have a
“mystical” or ethereal “tone color.”
By the way, the true response of a guitar string is nonlinear, so even if only a few modes
are excited initially, many other modes can be involved, although beyond a certain fre-
quency, the intensity of the modes will be insignificant and human beings won’t hear
them, anyway. The relative magnitudes of the amplitudes of the higher harmonics are
responsible for the tone colors of various musical instruments, for example, why bagpipes
don’t sound like trumpets.
The PDE modeling vibrations of a string generalize in two space dimensions to
1
c2
∂2u
∂t2 = ∂2u
∂x2 + ∂2u
∂y2 .
(10.52)
This can be used to model vibrations of a membrane.
If the material is not homogeneous, then we will see an even more general linear PDE;
for example, in one space dimension,
ϱ(x) ∂2u
∂t2 = ∂
∂x

T(x) ∂
∂x

.
(10.53)
Even more general is the nonlinear system of PDEs
ϱ(x) ∂2r
∂t2 = ∂
∂x

n
 ∂r
∂x

.
In addition, there are models for vibrations of plates and other solids and models for
electromagnetic waves in transmission lines and waveguides.
10.4.2 Vibrating String
Suppose a string, when undisturbed and unstretched, occupies the interval 0 ≤X ≤L on
the X-axis. This is called the reference configuration or reference state, as we referred to
it in Section 10.1’s discussion of continuum mechanics. At a later time, the string forms a
curve
r = r(X, t).
In effect, a material particle whose initial position vector was X ˆı will be on the string at
a point whose position vector is r(X, t). So, for each t ≥0, we have a map of material
particles
X →r(X, t).

870
Advanced Engineering Mathematics
Y
X
x
y
(L, 0, 0)
r (X, t)
0≤X ≤L
Z
FIGURE 10.11
String.
For example, in Figure 10.11, we see that the material particle at the origin is mapped to
the origin and the material particle at X = L is mapped to the point (x, y, z) = (L, 0, 0). So,
r(0, t) = 0
and
r(L ˆı, t) = L ˆı.
(10.54)
For each fixed t, the map X →r(X, t), 0 ≤X ≤L, gives a parametric curve.
We say that the string is tied at the ends if BC (10.54) are satisfied. Figure 10.11 depicts
the string at some time after it has moved, possibly by an out-of-the-xy-plane motion.
The local elongation of the string is
%%%%
∂r
∂X(X, t)
%%%% ,
which we will assume is positive at all X in 0 ≤X ≤L and all t > 0. (Technically, we
should talk about ∂r
∂X(0+, t) rather than about ∂r
∂X(0, t), but we will avoid being fussy.)
Let T = T(X, t) be the contact force or tension force exerted on the material segment
[0, X) by the material segment (X, L]. One definition of a string is that it is a zero thickness
idealization of material that has the property that T(X, t) is tangent to the string at r(X, t),
that is,
T(X, t) = β · ∂r
∂X(X, t)
for some scalar function β.∗A string is said to be elastic if β is a function of
 ∂r
∂X(X, t)
. In
this case, the tension force is
T(X, t) = β
 ∂r
∂X(X, t)

 ∂r
∂X(X, t).
(10.55)
Consider any interval I ≜(a, b) with 0 < a < b < L. Denote by P the piece of the string
that came from the material in I, that is,
P = {r(X, t) : a ≤X ≤b} = r(I, t).
∗The dependence of β on r(X, t) may be quite complicated. For example, a viscoelastic string would have β
depending upon the “history” of ∂r
∂X , that is, values of ∂r
∂X (X, t −s) for s > 0.

Partial Differential Equations Models
871
Newton’s second law of motion says that
d
dt
!
linear momentum in P
"
= sum of forces on P.
On the left end of P, the material from [0, a) exerts a contact force
−T(a, t)
on P; similarly, the material from (b, 1] exerts a contact force
T(b, t)
on P. In addition, there may be “body” forces f(X, t) acting on each point r(X, t) in P. Body
forces may include gravity or frictional forces.
The total linear momentum in P is
b
a
ϱ(X) ∂r
∂t(X, t) dX,
where ϱ(X) is the linear mass density, and the total body force acting on P is
b
a
f(X, t) dX.
So, Newton’s second law of motion says that
b
a
ϱ(X) ∂2r
∂t2 (X, t) dX = d
dt
⎡
⎣
b
a
ϱ(X)∂r
∂t(X, t) dX
⎤
⎦= −T(a, t) + T(b, t) +
b
a
f(X, t) dX. (10.56)
By the fundamental theorem of calculus,
−T(a, t) + T(b, t) =
b
a
∂T
∂X(X, t) dX,
so, we can rewrite (10.56) as
b
a
&
ϱ(X) ∂2r
∂t2 (X, t) −∂T
∂X(X, t) −f(X, t)
'
dX.
This being true for all a, b with 0 < a < b < L, if we assume that r(X, t) is smooth enough,
then this implies that r satisfies the PDE
ϱ(X) ∂2r
∂t2 (X, t) = ∂T
∂X(X, t) + f(X, t), 0 < X < L, 0 < t < ∞.
(10.57)

872
Advanced Engineering Mathematics
For an elastic string, the constitutive law (10.55) holds. Substitute that into (10.57) to get
the PDE
ϱ(X) ∂2r
∂t2 = ∂
∂X

β

 ∂r
∂X(X, t)


 ∂r
∂X

+ f, 0 < X < L, 0 < t < ∞.
(10.58)
A special case is when
β

 ∂r
∂X(X, t)



≡T0 and ϱ(X) ≡ϱ0,
that is, the tension coefficient is constant and the mass density is constant. If we assume
also that the motions of the string have small amplitude and come close to remaining in the
xy-plane, we can approximate r(X, t) ≈Xˆı + y(X, t) ˆj ≈xˆı + y(x, t) ˆj, so the PDE becomes
(10.51), that is,
1
c2
∂2y
∂t2 = ∂2y
∂x2 ,
where c =
#
T0/ϱ0.
Along with a PDE such as (10.51), it makes sense to specify BC at material points X = 0
and X = L. Also, analogous to our study of vibrations of a spring in Section 3.3, we should
specify initial conditions, usually giving as data the functions of x given by
y(x, 0), 0 < x < L
and
∂y
∂t (x, 0), 0 < x < L.
10.4.3 Speed of Sound
Gases are compressible, so the density ϱ = ϱ(x, t) is not necessarily constant. Let u = u(x, t)
be the gas velocity in one space dimension, P the pressure, and f be the sum of the body
forces. The basic model for vibrations of the gas the
Continuity equation :
∂ϱ
∂t + ∂
∂x
!
ϱu
"
= 0,
(10.59)
that is, the 1D version of Euler’s conservation of mass PDE, (7.41) in Section 7.6 or (10.2) in
Section 10.1, and the
Momentum equation :
ϱ ∂u
∂t + ϱ u ∂u
∂x + ∂P
∂x = ϱ f,
(10.60)
which follows from Newton’s second law of motion and neglecting viscosity. Sometimes
(10.60) is referred to as “Euler’s equation of motion for an ’ideal’ fluid.” The first two terms
of (10.60) combined can be rewritten in terms of the “material derivative” as ϱ Du
dt .

Partial Differential Equations Models
873
By “still air,” we mean the constant physical state ϱ ≡ϱ0, P = P0, and u ≡0. This is the
analogue of the reference state we mentioned in the study of continuum mechanics.
We will consider “small disturbances,” that is,
⎧
⎨
⎩
ϱ = ϱ0 + ε ϱ1(x, t)
u = ε u1(x, t)
⎫
⎬
⎭,
(10.61)
where we expect ε to be very small, which we denote by |ε| ≪1. We will also assume that
f ≡0, that is, that the body forces are so small that we can neglect them.
Noting that ϱ0 is constant, substitute (10.61) into (10.59) to get
0 = ∂
∂t
(
ϱ0 + ε ϱ1
)
+ ∂
∂x
(
(ϱ0 + ε ϱ1)ε u1
)
= ∂ϱ0
∂t + ε ∂ϱ1
∂t + ε ϱ0
∂u1
∂x + ε2 ∂
∂x
(
ϱ1u1
)
,
that is,
0 = 0 + ε ∂ϱ1
∂t + ε ϱ0
∂u1
∂x + ε2 ∂
∂x
(
ϱ1u1
)
.
(10.62)
Because |ε| ≪1, terms involving ε2 should be significantly smaller than terms involving ε0
or ε1.
Neglecting the ε2 term in (10.62) and then dividing by ε gives
∂ϱ1
∂t + ϱ0
∂u1
∂x = 0.
(10.63)
Similarly, substitute (10.61) into (10.60) and recall that we assumed f ≡0; after that,
neglect terms involving ε2 or ε3. This process gives
0 =

ϱ0 + ε ϱ1
∂

ε u1

∂t
+

ϱ0 + ε ϱ1

ε u1
 ∂

ε u1

∂x
+ ∂P
∂x
= ε ϱ0
∂u1
∂t + ε2 ϱ1
∂u1
∂t + ε2ϱ0 u1
∂u1
∂x + ε3ϱ1 u1
∂u1
∂x + ∂P
∂x ,
hence,
ε ϱ0
∂u1
∂t + ∂P
∂x = 0.
(10.64)
To have a system of PDEs that is solvable, we will need a constitutive relationship. For
a gas, let’s assume that the pressure is a function of the mass density:
P = P(ϱ).

874
Advanced Engineering Mathematics
Then the chain rule and Taylor series together give
∂P
∂x = ∂
∂x
(
P (ϱ(x, t))
)
= dP
dϱ · ∂ϱ
∂x = dP
dϱ (ϱ(x, t)) · ∂ϱ
∂x (x, t) = dP
dϱ (ϱ0 + ε ϱ1) · ∂
∂x
(
ϱ0 + ε ϱ1
)
.
=
&
dP
dϱ (ϱ0) + ε ϱ1
d2P
dϱ2 (ϱ0) + · · ·
'
·

0 + ε ∂ϱ1
∂x

,
where “· · · ” is short for terms involving ε2. This gives
∂P
∂x = ε dP
dϱ (ϱ0) · ∂ϱ1
∂x + · · · .
Substitute this into (10.64), neglect terms involving ε2, and then divide by ε to get
ϱ0
∂u1
∂t + dP
dϱ (ϱ0) · ∂ϱ1
∂x = 0.
(10.65)
The system of equations (10.63) and (10.65), that is,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂ϱ1
∂t = −ϱ0
∂u1
∂x
ϱ0
∂u1
∂t = −P′(ϱ0) · ∂ϱ1
∂x
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,
(10.66)
is called the “linearization” of (10.59) and (10.60).
The linearized system can be reduced to a single PDE, specifically the wave equation:
Partially differentiate (10.63) with respect to t to get
∂2ϱ1
∂t2 + ϱ0
∂2u1
∂x∂t = 0
(10.67)
and take the partial derivative of (10.65) with respect to x to get
ϱ0
∂2u1
∂t∂x + P′(ϱ0) · ∂2ϱ1
∂x2 = 0.
(10.68)
Solve (10.68) for ϱ0
∂2u1
∂t∂x and substitute into (10.67) to get
∂2ϱ1
∂t2 = P′(ϱ0) · ∂2ϱ1
∂x2 .
This is the wave equation in one space dimension and wave speed c =
#
P′(ϱ0)
, which
is the speed of sound.

Partial Differential Equations Models
875
The analysis we used, including the use of a small parameter ε, is an example of a
“perturbation” method.
To find the value of the speed of sound, it is necessary to use a definite expression for
the constitutive law P = P(ϱ). For an “ideal” gas, in adiabatic conditions, that is, when
there is no or negligible heat exchange with surroundings, P Vγ = k for some constants
γ , k. It follows that
P = kV−γ .
Further, there is a relationship between volume, V, and mass density, ϱ: Let M be the total
mass of a gas in a fixed volume V. Then M = ϱ V, hence V−1 = ϱ/M. It follows that
P = kV−γ = k

V−1γ
= k
 ϱ
M
γ
;
hence,
P = kM−γ ϱγ ,
and thus,
P′(ϱ0) = k · γ M−γ ϱγ −1
0
.
(10.69)
But, in still air, that is, when ϱ = ϱ0, P = P0, and V = V0 ≜M/ϱ0,
k = P0 Vγ = P0
M
ϱ0
γ
= P0 ϱ−γ
0
Mγ .
Substitute this into (10.69) to get
P′(ϱ0) =

P0 ϱ−γ
0 Mγ  
γ M−γ ϱγ −1
0

;
hence,
P′(ϱ0) = γ P0
ϱ0
.
(10.70)
So, according to this model, the speed of sound is
c =
*
γ P0
ϱ0
.
Assuming that “standard conditions” have temperature 273.15 K, that is, 0◦C, atmo-
spheric pressure at sea level being P0 = (1 atm) ≈101, 323 N/m2, and density of air being

876
Advanced Engineering Mathematics
ϱ0 ≈1.293 kg/m3. Assuming the adiabatic gas constant is γ ≈1.4,
c =
#
P′(ϱ0) =
*
γ P0
ϱ0
≈
*
(1.4)(101, 323 N/m2)
1.293 kg/m3
≈331.22 m/s ≈1086.7 ft/s.
This is quite close to the experimental value of 1088 ft/s for the speed of sound.
10.4.4 Linear Elasticity
In Section 10.1, we derived the continuum mechanics PDE implied by Newton’s second
law of motion (10.24) in Section 10.1, that is,
ϱ(x, t) ∂2u(x, t)
∂t2
= f(x, t) + ∇• [τ],
(10.71)
where
the displacement vector is u ≜x(X, t) −X,
material particles X are in a region V0 = V(0), f is the sum of the body forces,
[τ] is the stress tensor due to contact forces.
The 3 × 3 matrix is symmetric because of conservation of angular momentum. Also, as
the system evolves, the material particles may move, so the material occupies the region
V(t) ≜{x(X, t) : X in V0}. Writing the displacement vector as
u = u1(X, t)ˆı + u2(X, t) ˆj + u3(X, t)ˆk,
we define the 3 × 3 linearized strain tensor [ε] in terms of its entries:
εij ≜1
2
 ∂ui
∂Xj
+ ∂uj
∂Xi

.
The exact strain tensor has other terms in its ij-th entry, namely,
+3
k=1
∂uk
∂Xi
∂uk
∂Xj

, which
we neglect based on the assumption that displacements are small.
Note that
[ε] = 1
2
(∂u
∂X
)
+
(∂u
∂X
)T
is a symmetric matrix.
Example 10.11
In Sections 1.2 and 6.6, we called
1
0
h
1

the matrix of a linear shear transformation on
R2. Assuming h is a scalar constant, find the linear strain tensor for the corresponding

Partial Differential Equations Models
877
mapping of material particles given by
x(X) ≜
⎡
⎣
1
0
0
h
1
0
0
0
1
⎤
⎦X.
Method: The displacement is
u(X) ≜x(X) −X =
⎡
⎣
0
0
0
h
0
0
0
0
0
⎤
⎦X =
⎡
⎣
0
hX1
0
⎤
⎦,
so the strain tensor is
[ε] = h
2
⎡
⎣
0
1
0
1
0
0
0
0
0
⎤
⎦. ⃝
Definition 10.1
The principal strains are the eigenvalues of the symmetric matrix [ε].
By Theorem 2.23 in Section 2.6, the principal strains are real numbers because [ε] is real
and symmetric. In Example 10.11, the principal strains are 0, ± h
2.
Definition 10.2
Plane strain is when [ε] has one of the forms
⎡
⎢⎢⎢⎢⎣
ε11
ε12

0

ε21
ε22

0
−−
−−

−−
0
0

0
⎤
⎥⎥⎥⎥⎦
,
⎡
⎢⎢⎢⎢⎣
ε11
0
ε13
0
0
0
ε31
0
ε33
⎤
⎥⎥⎥⎥⎦
,
⎡
⎢⎢⎢⎢⎣
0

0
0
−−

−−
−−
0

ε22
ε23

0

ε32
ε33
⎤
⎥⎥⎥⎥⎦
,
(10.72)
or, more generally,
O
⎡
⎢⎢⎢⎢⎣
ε11
ε12

0

ε21
ε22

0
−−
−−

−−
0
0

0
⎤
⎥⎥⎥⎥⎦
OT,
(10.73)
for some 3 × 3 orthogonal matrix O.
In the latter case of (10.73), {Oˆı, O ˆj} is an orthonormal basis for the plane in which there
is strain and Oˆk is normal to that plane.
In any case, we have plane strain if, and only if, at least one of the principal strains
is zero.

878
Advanced Engineering Mathematics
By itself, the PDE (10.71) cannot be solved, even if the body forces are zero. To have a
solvable problem, we need to have a constitutive law that gives the stress tensor in terms
of the displacement vector. To that end, we say that we have linear elasticity if Hooke’s
law holds in the sense that the stress tensor is a linear transformation of the linearized
strain tensor, that is,
[τ] = C[ε],
where C is a linear transformation from the space of all 3 × 3 symmetric matrices to the
space of 3×3 symmetric matrices. In terms of indices, for each 1 ≤i ≤3 and each 1 ≤j ≤3,
τij =
3
+
k=1
3
+
ℓ=1
cijklεkℓ.
Fortunately, the 81 possible entries in C can be reduced to 36 due to the fact that both
[τ] and [ε] must be symmetric matrices. If we further assume that the material is isotropic,
that is, it is homogeneous in the sense that it “bends in the same way in all directions,”
then it turns out that C is specified by exactly two numbers, λ, μ, and
τij = λ · (ε11 + ε22 + ε33)δij + 2μ εij,
(10.74)
where δij is the Kronecker delta, that is,
δij ≜
1
i = j
0
i ̸= j

.
We can rewrite (10.74) more compactly as
[τ] = λ · (ε11 + ε22 + ε33)I + 2μ[ε].
Lamé constants are the constants λ and μ. Another name for μ is the shear modulus.
We will substitute the isotropic constitutive law (10.74) into the PDE (10.71). Further, for
small amplitude motions we can make the approximations x(X, t) ≈X and ϱ(x, t) ≈ϱ0(X)
to get the “Navier” or “Lamé” equation,
ϱ0(X) ∂2u(X, t)
∂t2
= f(X, t) + (λ + μ)∇
!
∇• u
"
+ μ ∇2u.
Alternatively, using the identity
∇2u = ∇
!
∇• u
"
−∇× (∇× u),
we can rewrite the Navier–Lamé equation as
ϱ0(X) ∂2u(X, t)
∂t2
= f(X, t) + (λ + 2μ)∇
!
∇• u
"
−μ ∇× (∇× u).

Partial Differential Equations Models
879
10.4.5 Linear Elastostatics
For a steady state, that is, for the elastostatic problem, ∂u(X, t)
∂t
≡0 implies
0 = f(X, t) + (λ + 2μ)∇[∇• u] −μ ∇× (∇× u).
Uniaxial stretching, say along the x-axis, is when displacements are given by
u = α
⎡
⎣
x
−ν y
−ν z
⎤
⎦,
for some ν. In Problem 10.4.6.6, you will explain why for these displacements, the strain
tensor is the diagonal matrix
[ε] =
⎡
⎣
α
0
0
0
−αν
0
0
0
−αν
⎤
⎦.
(10.75)
Assuming isotropy, Hooke’s law says that the stress tensor is
[τ] =
⎡
⎣
α (λ(1 −2ν) + 2μ)
0
0
0
α (λ(1 −2ν) −2μν)
0
0
0
α (λ(1 −2ν) −2μν)
⎤
⎦,
(10.76)
as you will explain in Problem 10.4.6.7. If ν is chosen to equal Poisson’s ratio, that is,
ν =
λ
2(λ + μ),
then, as you will explain in Problem 10.4.6.8,
[τ] =
⎡
⎣
α · 2μ (1 + ν)
0
0
0
0
0
0
0
0
⎤
⎦.
(10.77)
In this case, τ11 = Eα, where the Young’s modulus is defined by
E ≜μ · (3λ + 2μ)
λ + μ
,
in terms of which the Lamé constants are given by
μ =
E
2(1 + ν); hence , λ =
ν E
(1 + ν)(1 −2ν),
(10.78)
as you will explain in Problem 10.4.6.9.
Finally, we briefly mention that the theory of finite thickness beams, rods, and plates
takes into account “transverse shear” forces and that each piece of such a 3D object

880
Advanced Engineering Mathematics
exerts a bending moment(s) on the rest of the object. Thus, the equation of motion takes
into account a spatial derivative(s) of the shear force, and the latter may be modeled as
being proportional to a spatial derivative(s) of the bending moment. Further, the bending
moment may be modeled as being approximately proportional to a second derivative(s)
of the deformation in the longitudinal direction(s). This leads to linear PDE models, for
example, of the forms
ϱ ¨w = −EIw(iv) + Tw′′ −ϱg, 0 < x < L.
ϱ ¨w = −E[I]w(iv) + Tw′′ + ϱf, 0 < x < L.
ϱ ¨w = −D∇4w + T∇2w −ϱg, (x, y) in D.
for beams, rods, and plates, respectively. The symbol I stands for a 2 × 2 inertia matrix,
and w = [v(x, t)
w(x, t)]T are the displacements in the directions transverse to the rod.
In the third PDE, we assumed that the stress tensor is isotropic. There are even more
sophisticated, nonlinear, approximate models such as von Kármán’s plate theory.
Learn More About It
Our derivation of PDEs for vibrating strings is based on part of “The equations
for large vibrations of strings,” by Stuart S. Antman, Am. Math. Monthly 87 (1980)
pp. 359–370. As in Section 10.1, our discussion of the PDEs of elasticity follows
Chapter 1 of Applied Solid Mechanics, Peter Howell, Gregory Kozyreff, and Jim
Ockendon, Cambridge University Press, 2009. In addition, the latter book’s Chapter 4
is a good reference for the linear theory of finite thickness beams, rods, and plates.
Example 10.11 is based on Continuum Mechanics, by Segel, p. 156, Example 3.
Problem 10.4.6.10 is a version of Exercise 2.2.2 on page 40 of Partial Differential
Equations: An Introduction, by Walter A. Strauss, John Wiley & Sons, Inc., 1992.
10.4.6 Problems
1. In deriving the wave equation for a vibrating string, we assumed that the mass
density, ϱ, and the tension force, T0, were constants. Instead, let the mass density
and the tension force be functions of x and derive a PDE for the string.
2. In the transmission line equation,
∂2i
∂x2 = LC∂2i
∂t2 + (LG + RC)∂i
∂t + RGi,
suppose there is no current leakage, that is, G = 0, and let i(x, t) = e−ptu(x, t).
Choose the constant p so that the transmission line equation reduces to a PDE
of the form ∂2u
∂x2 = c2 ∂2u
∂t2 −ru and state the values of the constants c, r in terms
of R, L, C.
3. Suppose a 13 7
8 in.(≈352 mm) long mandolin string is under 21.3 lb of tension and
is tuned to a pitch of 440 Hz, that is, A4 in the scientific notation for pitches. What

Partial Differential Equations Models
881
is the mass density of the string, in kg/m? Assume that at sea level, an object
whose mass is 1 kg weighs about 9.80665 N, that is, about 2.2 lb.
4. Find a real, orthogonal matrix O such that
⎡
⎢⎢⎢⎢⎣
0

0
0
−−
−−−
0

d
c

0

b
a
⎤
⎥⎥⎥⎥⎦
= O
⎡
⎢⎢⎢⎢⎣
a
b

0

c
d

0
−−−−
−−
0
0

0
⎤
⎥⎥⎥⎥⎦
OT.
[Hint: What matrix multiplication has the effect of swapping the variables x
and z?]
5. Find a real, orthogonal matrix O such that
⎡
⎢⎢⎢⎢⎣
a
0
b
0
0
0
c
0
d
⎤
⎥⎥⎥⎥⎦
= O
⎡
⎢⎢⎢⎢⎣
a
b

0

c
d

0
−−−−
−−
0
0

0
⎤
⎥⎥⎥⎥⎦
OT.
[Hint: What matrix multiplication has the effect of swapping the variables y
and z?]
6. Explain why in the strain tensor is the diagonal matrix (10.75) in the example of
isotropic, uniaxial stretching.
7. Explain why Hooke’s law for isotropic, uniaxial stretching gives (10.76).
8. Explain why the stress tensor [τ] is given by (10.77) for the uniaxial stretching
example, assuming ν is chosen to equal Poisson’s ratio.
9. Explain why the Lamé constants for isotropic, uniaxial stretching are given
by (10.78).
10. Suppose that u(x, t) satisfies the 1D wave equation with wave speed √c = 1, that
is, ∂2u
∂t2 = ∂2u
∂x2 . Define the energy density function e(x, t) and the momentum den-
sity function by
e(x, t)= 1
2
&∂u
∂x
2
+
∂u
∂t
2'
, p(x, t)= ∂u
∂x
∂u
∂t ,
respectively. Explain why both e(x, t) and p(x, t) satisfy that very same 1D wave
equation.
11. The principle of virtual work for the string equation (10.57) can be obtained from
it by taking the dot product of both sides of that PDE with an unspecified “test
function” η = η(X, t), integrating both sides with respect to X over the interval
[ 0, L ], using integration by parts (with respect to X) on the term involving η(X, t)•
∂T
∂X(X, t), and then integrating both sides of the equation with respect to t over the
interval [0, ∞). Finally, use integration by parts (with respect to t) on the term

882
Advanced Engineering Mathematics
involving η(X, t) • ϱ(X)∂2r
∂t2 (X, t). Use this process to explain why we have the
principle of virtual work equation
∞

0
L
0
 ∂η
∂X •T−f• η+ϱ(X)∂η
∂t • ∂r
∂t

dXdt = 0.
We also say (10.57) has the latter integral equation as its “weak form,” whose
meaning is related to the weak convergence idea discussed in Section 2.10.
10.5 D’Alembert Wave Solutions
Solutions of the wave equation (10.51) in Section 10.4 in one space dimension, that is,
∂2y
∂t2 = c2 ∂2y
∂x2 ,
(10.79)
along with initial conditions defined on the real line, that is,
⎧
⎨
⎩
y(x, 0) = (x)
∂y
∂t (x, 0) = (x)
⎫
⎬
⎭, −∞< x < ∞,
(10.80)
can be expressed as a sum of d’Alembert wave solutions:
y(x, t) = f(x + ct) + g(x −ct),
(10.81)
assuming that (x) and (x) are piecewise continuous on the real line. In the process of
justifying (10.81), we will be specific about the relationship between the initial conditions,
(10.80), and the solution (10.81).
First, we should understand the physical behavior of a solution such as y(x, t) = f(x+ct):
As t increases, it gives a wave traveling to the left with speed c > 0. Equivalently, as t
decreases, it gives a wave traveling to the right. It depends on whether you are watching
a movie run forward or backward! An example is depicted in Figure 10.12.
Example 10.12
(a) Find the wave speed from the graphs in Figure 10.12, assuming time t is measured
in seconds and spatial position x is measured in meters.
(b) If the graphs in Figure 10.12 correspond to a solution of a vibrating string whose
linear mass density is 4 g/m, what is the magnitude of the tension force in the string?

Partial Differential Equations Models
883
0.2
0.4
0.6
0.8
x
y(x, 0)
0.2
0.4
0.6
0.8
x
y(x, 0.1)
0.2
0.4
0.6
0.8
x
y(x, 0.2)
–4
–3
–2
–1
1
2
–4
–3
–2
–1
1
2
–4
–3
–2
–1
1
2
FIGURE 10.12
Wave moving left.
Method:
(a) Define y(x, 0) = f(x). It appears that the solution is a wave moving to the left and
y(x, 0.1) ≈f(x + 1). We want f(x + c · (0.1)) = y(x, 0.1) ≈f(x + 1), so c · (0.1) = 1. The
wave speed is approximately c = 10 m/s.
(b) Recall that for a vibrating string, the wave speed is given by c =
#
T0/ϱ0. So, using
the result of part (a), we have 10 m/s =
#
T0/(0.004 kg/m); hence, the tension force
in the string has magnitude
T0 = (100 m2/s2) · (0.004 kg/m) = 0.4 kg · m/s = 0.4 N. ⃝
10.5.1 Zero Initial Velocity
Example 10.13
Use (10.81) to solve the IVP consisting of the wave equation PDE (10.79) and a special
case of the ICs: y(x, 0) = (x), ∂y
∂t (x, 0) ≡0, −∞< x < ∞.
Method: In (10.81), f and g are functions of a single variable, for example, f(x + ct) =
f(ξ)|ξ=x+ct. Before substituting (10.81) into the ICs, it helps to calculate that, using the
chain rule,
∂y
∂t (x, t) = ∂
∂t
!
f(x + ct) + g(x −ct)
"
= f ′(x + ct) ∂
∂t [x + ct] + g′(x + ct) ∂
∂t [x −ct] ;
hence,
∂y
∂t (x, t) = ∂
∂t
!
f(x + ct) + g(x −ct)
"
= c f ′(x + ct) −c g′(x −ct).
(10.82)
So, the ICs require that f and g satisfy the system of equations
⎧
⎨
⎩
(x) = y(x, 0) = f(x) + g(x)
0 ≡∂y
∂t (x, 0) = c

f ′(x) −g′(x)

⎫
⎬
⎭, −∞< x < ∞.
(10.83)
The second equation in (10.83) implies that f ′(x) ≡g′(x). The fundamental theorem of
calculus for functions of a single variable implies that
f(x) ≡g(x) + c1,
for some constant c1. Substituting this into the first equation in (10.83) implies that
(x) ≡f(x) + g(x) = g(x) + c1 + g(x);

884
Advanced Engineering Mathematics
hence,
g(x) = 1
2 (x) −1
2 c1 and f(x) = 1
2 (x) + 1
2 c1.
So, the solution to the IVP is
y(x, t) = f(x + ct) + g(x −ct) = 1
2 (x + ct) + 

1
2 c1 + 1
2 (x −ct) −

1
2 c1,
that is,
y(x, t) = 1
2 ((x + ct) + (x −ct)) . ⃝
(10.84)
Example 10.14
If y(x, 0) ≜
⎧
⎨
⎩
1,
1 ≤x < 2
0,
all other x
⎫
⎬
⎭, c = 10 m/s, and y(x, t) is the solution of the IVP of
Example 10.13, (a) sketch the solution y(x, t) at times t = 0, 0.05, 0.1, 0.2 s, and (b) find as
explicit a formula for the solution y(x, t) as possible.
Method: (a) (10.84) gives us a “graphical method” of sketching the solution y(x, t) at time
t > 0: First, we can rewrite the IC as
y(x, 0) = 1
2 (x) + 1
2 (x),
where
(x) =
⎧
⎨
⎩
1,
1 ≤x < 2
0,
all other x
⎫
⎬
⎭
and sketch this decomposition in Figure 10.13. Because the wave speed is c = 10 m/s, it
is convenient to measure x in meters and t in seconds.
(i) At time t = 0.05, we have y(x, 0.05) = 1
2 (x+0.5)+ 1
2 (x−0.5). Figure 10.14 shows
on the left the graph of y(x, 0.05) versus x for −1 < x < 4. It is constructed as the
sum of the graph of 1
2 (x), shifted left by 0.5 m, and the graph of 1
2 (x), shifted
right by 0.5 m.
(ii) At time t = 0.1, we have y(x, 0.1) = 1
2 (x + 1) + 1
2 (x −1). Figure 10.15 shows on
the left the graph of y(x, 0.1) as the sum of the graph of 1
2 (x), shifted left by 1 m,
and the graph of 1
2 (x), shifted right by 1 m.
(iii) At time t = 0.2, we have y(x, 0.2) = 1
2 (x + 2) + 1
2 (x −2). Figure 10.16 shows on
the left the graph of y(x, 0.2) as the sum of the graph of 1
2 (x), shifted left by 2 m,
and the graph of 1
2 (x), shifted right by 2 m.
1.0
0.8
0.6
0.4
y (x, 0)
0.2
x
1.0
0.8
0.6
0.4
=
+
—Φ(x)
1
2
0.2
x
1.0
0.8
0.6
0.4
—Φ(x)
1
2
0.2
x
–1
0
1
2
3
4
–1
0
1
2
3
4
–1
0
1
2
3
4
FIGURE 10.13
Decomposing the initial condition.

Partial Differential Equations Models
885
1.0
0.8
0.6
0.4
y(x, 0.05)
0.2
x
1.0
0.8
0.6
0.4
=
+
—Φ(x+0.5)
1
2
0.2
x
1.0
0.8
0.6
0.4
—Φ(x–0.5)
1
2
0.2
x
–1
0
1
2
3
4
–1
0
1
2
3
4
–1
0
1
2
3
4
FIGURE 10.14
y(x, 0.05).
1.0
0.8
0.6
0.4
y(x, 0.1)
0.2
x
1.0
0.8
0.6
0.4
=
+
—Φ(x+1)
1
2
0.2
x
1.0
0.8
0.6
0.4
—Φ(x –1)
1
2
0.2
x
–1
0
1
2
3
4
–1
0
1
2
3
4
–1
0
1
2
3
4
FIGURE 10.15
y(x, 0.1).
1.0
0.8
0.6
0.4
y(x, 0.2)
0.2
x
1.0
0.8
0.6
0.4
=
+
—Φ(x+2)
1
2
—Φ(x –2)
1
2
0.2
x
1.0
0.8
0.6
0.4
0.2
x
–1
0
1
2
3
4
–1
0
1
2
3
4
–1
0
1
2
3
4
FIGURE 10.16
y(x, 0.2).
(b) To write as explicit a formula for y(x, t) as possible, we recall that
(x) is nonzero only for 1 ≤x < 2.
By replacing x by ξ ≜x + ct, we see that
(x + ct) = (ξ) is nonzero only for 1 ≤ξ = x + ct < 2,
that is,
(x + ct) is nonzero only for 1 −ct ≤x < 2 −ct.
It helps to create a new notation for a “moving interval”: Define
I−
t ≜[1 −ct, 2 −ct),
that is, x is in I−
t if, and only if, 1 −ct ≤x < 2 −ct. Similarly, we define
I+
t ≜[1 + ct, 2 + ct).

886
Advanced Engineering Mathematics
For t=0.02
1
2
x
     =[1–10t, 2–10t)
    =[1 + 10t, 2 + 10t)
FIGURE 10.17
I−
t and I+
t overlap.
In terms of these notations, we have that
(x + ct) is nonzero only for x in I−
t
and
(x −ct) is nonzero only for x in I+
t .
For t ≥0, I−
t , and I+
t
overlap if, and only if, 1 + ct < 2 −ct, that is, 2ct < 1, that is,
0 ≤t < 1
2c. In this particular example, c = 10, so
I−
t and I+
t overlap ⇐⇒0 ≤t < 0.05.
Figure 10.17 illustrates this with the example of t = 0.02.
For 0 ≤t < 0.05,
y(x, t) = 1
2 (x + ct) + 1
2 (x −ct)
=
⎧
⎨
⎩
0.5,
1 −10t ≤x < 2 −10t
0,
x < 1 −10t and x ≥2 −10t
⎫
⎬
⎭+
⎧
⎨
⎩
0.5,
1 + 10t ≤x < 2 + 10t
0,
x < 1 + 10t and x ≥2 + 10t
⎫
⎬
⎭
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < 1 −10t
0.5,
1 −10t ≤x < 1 + 10t
0.5,
1 + 10t ≤x < 2 −10t
0,
2 −10t ≤x < 2 + 10t
0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
+
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < 1 −10t
0,
1 −10t ≤x < 1 + 10t
0.5,
1 + 10t ≤x < 2 −10t
0.5,
2 −10t ≤x < 2 + 10t
0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0 + 0,
x < 1 −10t
0.5 + 0,
1 −10t ≤x < 1 + 10t
0.5 + 0.5,
1 + 10t ≤x < 2 −10t
0 + 0.5,
2 −10t ≤x < 2 + 10t
0 + 0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < 1 −10t
0.5,
1 −10t ≤x < 1 + 10t
1,
1 + 10t ≤x < 2 −10t
0.5,
2 −10t ≤x < 2 + 10t
0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.

Partial Differential Equations Models
887
1.0
0.8
0.6
0.4
y(x, 0.02)
0.2
–1
0
1
2
3
4
FIGURE 10.18
y(x, 0.02).
For example, for t = 0.02, the solution is given by
y(x, 0.02) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < 0.8
0.5,
0.8 ≤x < 1.2
1,
1.2 ≤x < 1.8
0.5,
1.8 ≤x < 2.2
0,
x ≥2.2
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
which is graphed in Figure 10.18.
For t ≥0.05, the intervals I−
t and I+
t do not overlap, so
y(x, t) = 1
2 (x + ct) + 1
2 (x −ct)
=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
x < 1 −10t
0.5,
1 −10t ≤x < 2 −10t
0,
x ≥2 −10t
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
+
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
x < 1 + 10t
0.5,
1 + 10t ≤x < 2 + 10t
0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < 1 −10t
0.5,
1 −10t ≤x < 2 −10t
0,
2 −10t ≤x < 1 + 10t
0.5,
1 + 10t ≤x < 2 + 10t
0,
x ≥2 + 10t
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
The jumps in the initial condition y(x, 0) = (x) remain in the solution at time t ̸= 0.
This means that our concept of solution should include the possibility that the PDE is
not satisfied at some isolated x in the interval −∞< x < ∞.
Also, unlike what happens with Fourier series, the d’Alembert solution does not use an
average at a jump.

888
Advanced Engineering Mathematics
10.5.2 Writing the Solution Using Step Functions
For the purpose of using a calculator or computer, it can be useful to use step functions
to express the solution of Example 10.14, that is, y(x, t) = 1
2 ((x + 10t) + (x −10t)) in
(10.84). To do that, we write the “pulse” (x) in terms of step functions:
(x) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
x < 1
1,
1 ≤x < 2
0,
2 ≤x
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
x < 1
1,
1 ≤x < 2
1 −1,
2 ≤x
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
= step(x −1) −step(x −2),
where we define the function step(x) on the whole real line by
step(x) =
⎧
⎨
⎩
0,
x < 0
1,
x ≥0
⎫
⎬
⎭.
Using this notation, Example 10.14’s solution (10.84) can be expressed as
y(x, t) = 1
2

step(x + 10t −1) −step(x + 10t −2) −step(x −10t −1) + step(x −10t −2)

.
10.5.3 Support
There is another interesting way to look at Example 10.14’s solution: y(x, t) is the sum of
1
2 (x + 10t), a wave traveling to the left, and 1
2 (x −10t), a wave traveling to the right.
In Example 10.14, (x) is zero outside of the interval [1, 2). The support of (x) is defined
to be the set of x for which (x) is nonzero. The interval [1, 2) is the support of (x).
Similarly, the support of 1
2 (x + 10t) is the interval I−
t = [1 −10t, 2 −10t), and the sup-
port of 1
2 (x −10t) is the interval I+
t = [1 + 10t, 2 + 10t). In Figure 10.19, we see how the
intervals I±
t change as t changes. Each horizontal line t = constant intersects Figure 10.19a
0.1
1
x
t
0.1
1
2
2
3
x
t
(a)
(b)
     =[1–10t, 2–10t)
    =[1 + 10t, 2+10t)
FIGURE 10.19
(a) I−
t and (b) I+
t .

Partial Differential Equations Models
889
0.1
(a)
(b)
0.05
1
2
x
t
I t
–   I t
+
     =[1–10t, 2–10t)
    =[1+10t, 2+10t)
FIGURE 10.20
(a) I−
t and (b) I+
t .
in the interval of x values in I−
t
and intersects Figure 10.19b in the interval of x
values in I−
t .
We can combine Figure 10.19a and b to give Figure 10.20’s picture of the support of
y(x, t) = 1
2 (x + 10t) + 1
2 (x −10t).
10.5.4 Zero Initial Displacement
Example 10.15
Use (10.81) to solve the IVP consisting of the wave PDE (10.79) and the special case of
the ICs being y(x, 0) ≡0 and ∂y
∂t (x, 0) = (x), both for −∞< x < ∞.
Method: Using (10.81), that is, solutions in the form y(x, t) = f(x + ct) + g(x −ct), we
calculated in Example 10.13 that ∂y
∂t (x, t) = c f ′(x + ct) −c g′(x −ct).
So, the ICs require that f and g satisfy the system of equations
⎧
⎨
⎩
0 ≡y(x, 0) = f(x) + g(x)
(x) = ∂y
∂t (x, 0) = c

f ′(x) −g′(x)

⎫
⎬
⎭, −∞< x < ∞.
(10.85)
The first equation in (10.85) implies that g(x) ≡−f(x); hence, g′(x) ≡−f ′(x). Substituting
that into the second equation in (10.85) implies that we need
(x) = 2c f ′(x)
The fundamental theorem of calculus for functions of a single variable implies that
f(x) = c1 + 1
2c
x
0
(ξ) dξ,
for some constant c1, and this implies that
g(x) = −c1 −1
2c
x
0
(ξ) dξ.

890
Advanced Engineering Mathematics
So, the solution to the IVP is
y(x, t) = f(x + ct) + g(x −ct) =
c1 + 1
2c
x+ct

0
(ξ) dξ −
c1 −1
2c
x−ct

0
(ξ) dξ,
that is,
y(x, t) = 1
2c
x+ct

0
(ξ) dξ −1
2c
x−ct

0
(ξ) dξ = 1
2c
x+ct

0
(ξ) dξ + 1
2c
0
x−ct
(ξ) dξ.
The solution to this IVP in the special case that y(x, 0) ≡0 is
y(x, t) = 1
2c
x+ct

x−ct
(ξ) dξ. ⃝
(10.86)
We can combine the results of Examples 10.13 and 10.15 to get the next result.
Theorem 10.4
If (x) and (x) are piecewise continuous functions of x for −∞< x < ∞, then d’Alembert
solution of the IVP (10.79) and (10.80), that is,
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂2y
∂t2 = c2 ∂2y
∂x2
y(x, 0) = (x)
∂y
∂t (x, 0) = (x)
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
, −∞< x < ∞,
(10.87)
is given by
y(x, t) = 1
2 (x + ct) −1
2 (x −ct) + 1
2c
x+ct

x−ct
(ξ) dξ.
(10.88)
Why? Denote by y1(x, t) = 1
2 (x + ct) −1
2 (x −ct) and y2(x, t) =
1
2c
 x+ct
x−ct (ξ) dξ, the
solutions of Examples 10.13 and 10.15, respectively. Because PDE (10.79) is linear, y(x, t) =
y1(x, t) + y2(x, t) also satisfies (10.79). Also, y(x, t) satisfies the ICs (10.80) because
y(x, 0) = y1(x, 0) + y2(x, 0) = (x) + 0 = (x)
and
∂y
∂t (x, 0) = ∂y1
∂t (x, 0) + ∂y2
∂t (x, 0) = 0 + (x) = (x). 2

Partial Differential Equations Models
891
Example 10.16
Solve, as explicitly as possible, the IVP consisting of ICs y(x, 0) ≡0 and ∂y
∂t (x, 0) ≜
⎧
⎨
⎩
sin x,
0 ≤x < π
0,
all other x
⎫
⎬
⎭for t > 0. Assume the wave speed is c = 3π m/s. Sketch the
solution y(x, t) versus x at three interesting, positive times t of your choosing.
Method: By Theorem 10.4, the solution is
y(x, t) = 1
2c
x+ct

x−ct
(ξ) dξ,
where (x) = ∂y
∂t (x, 0). Unfortunately, it is not true that y(x, t) = 1
2c
x+ct

x−ct
1 dξ.
Instead, using c = 3π, we can write
y(x, t) =

x−3πt≤ξ≤x+3πt
(ξ) dξ
=

x−3πt≤ξ≤x+3πt and 0≤ξ<π
1 dξ +

x−3πt≤ξ≤x+3πt and (ξ<0 or ξ≥π)
0 dξ,
that is,
y(x, t) =

[x−3πt,x+3πt]∩[0,π)
sin ξ dξ.
(10.89)
The intersection symbol, ∩, says that the integral in (10.89) is performed over only those ξ
that are in the overlap of the interval [x −3πt, x + 3πt] and the interval [0, π).
Formula (10.89) looks nice and compact but to use it requires further analysis! We need
to study the overlap of the intervals [x −3πt, x + 3πt] and the interval [0, π). For any fixed
t > 0, define the interval
Jx ≜[x −3π t, x + 3π t] = {ξ : x −3πt ≤ξ ≤x + 3π t}.
For any fixed t > 0, the interval Jx “slides” along the ξ real line as x varies. In studying the
intersection [x −3πt, x + 3πt] ∩[0, π), ξ must also be located in the interval [0, π), which
does not vary as x varies.
We will study that intersection by letting x increase and thus sliding Jx along until it
overlaps the interval [0, π) and then slides past the interval [0, π). It’s as if Jx is a pulse of
material that moves in relation to the fixed interval [0, π). The latter interval is the support
of the initial velocity datum (x).
Illustrated in Figure 10.21 are the six and only possible relationships the interval Jx can
have with the interval [0, π):
1. Jx is to the left of [0, π).
2. Part of Jx is to the left of [0, π), and part of Jx is contained in, but unequal to, [0, π).

892
Advanced Engineering Mathematics
Jx
0
Case 1
π
Jx
0
π
Jx
0
π
Jx
0
Case 2
π
Jx
0
Case 3
π
Jx
0
Case 4
π
Case 5
Case 6
FIGURE 10.21
Interval cases.
3. Part of Jx is to the left of [0, π), all of [0, π) is contained in Jx, and part of Jx is to the
right of [0, π).
4. All of Jx is contained in, but unequal to, [0, π).
5. Part of Jx is contained in [0, π) and part of Jx is to the right of [0, π).
6. Jx is to the right of [0, π).
Let us analyze if and when each of those six cases occurs, for t > 0: cases (3) and (4) have
a “transitional” nature and will turn out to be useful to study first.
Case (3) can occur only if the length of the interval Jx is greater than or equal to π, and,
conversely, case (4) can occur only if the length of Jx is less than π. The length of Jx =
[x −3πt, x + 3πt] is 6πt. So, case (3) can occur only if t ≥1
6, and case (4) can occur only if
0 < t < 1
6. So, it makes sense to consider separately the cases (a) t ≥1
6 and (b) 0 < t < 1
6.
(a) First, suppose 0 < t < 1
6. Case (1) occurs when the right endpoint of the interval Jx is
to the left of the interval [0, π), that is, when x + 3π t < 0, that is, when x < −3πt.
Case (2) occurs when Jx “straddles” [0, π) “on the left,” specifically the right endpoint of
Jx is at or to the right of the left endpoint of the interval [0, π) and the left endpoint of Jx is
to the left of [0, π). This happens when both x + 3πt ≥0 and x −3πt < 0, that is, when
x ≥−3πt and x < 3πt, that is, when −3πt ≤x < 3πt.
Case (3) cannot occur because we supposed that 0 < t < 1
6.
Case (4) occurs when the left endpoint of Jx is at, or to the right of, 0 and the right end-
point of Jx is to the left of π, that is, when x −3πt ≥0 and x + 3πt < π, that is, when
x ≥3πt and x < (1 −3t)π, that is, when 3πt ≤x < (1 −3t)π.
Case (5) occurs when Jx straddles [0, π) on the right, specifically when (i) the right end-
point of Jx is to the right of the interval [0, π) and (ii) the left endpoint of Jx is in the interval
[0, π). This happens when both (i) x + 3πt ≥π and (ii) 0 ≤x −3πt < π, that is, when (i)
x ≥π −3πt and (ii)(A) x ≥3πt and (ii)(B) x < π + 3πt. Of the inequalities (i) x ≥(1 −3t)π
and (ii)(A) x ≥3πt, (i) is more demanding because (1 −3t)π > 3tπ follows from the
assumption in part (a) that 0 < t < 1
6. So, case (5) occurs when (i) x ≥(1 −3t)π and (ii)(B)
x < (1 + 3t)π, that is, when (1 −3t)π ≤x < (1 + 3t)π.
Case (6) occurs when the left endpoint of Jx is to the right of the interval [0, π), that is,
when x −3πt ≥π, that is, when x ≥(1 + 3t)π.
Integrating
Now that we understand the cases for 0 < t <
1
6, we can evaluate y(x, t), as given in
(10.89):
1. For x < −3πt, Jx ∩[0, π) = ∅, the empty set, so y(x, t) = 0. [The integral over
nothing is nothing.]

Partial Differential Equations Models
893
2. For −3πt ≤x < 3πt, the part of the interval Jx that overlaps [0, π) is [0, x + 3πt], so
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
x+3πt

0
sin ξ dξ =
(
−cos ξ
)x+3πt
0
= 1 −cos(x + 3πt).
3. Case 3 cannot occur if 0 < t < 1
6.
4. For 3πt ≤x < (1−3t)π, all of Jx lies inside [0, π). So, Jx ∩[0, π) = Jx and thus using
the difference of cosines formula, which we saw in Section 4.2 when discussing
the beats phenomenon,
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
x+3πt

x−3πt
sin ξ dξ =
(
−cos ξ
)x+3πt
x−3πt
= −cos(x + 3πt) + cos(x −3πt)
= 2 sin
x + 3πt −(x −3πt)
2

sin
x + 3πt + (x −3πt)
2

= 2 sin(3πt) sin x.
5. For (1 −3t)π ≤x < (1 + 3t)π, the part of the interval Jx that overlaps [0, π) is
[x −3πt, π], so
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
π
x−3πt
sin ξ dξ =
(
−cos ξ
)π
x−3πt = 1 + cos(x −3πt).
6. For x ≥(1 + 3t)π, again Jx ∩[0, π) = ∅, so y(x, t) = 0.
Putting all of the cases that can occur together, for 0 < t < 1
6,
y(x, t) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < −3πt
1 −cos(x + 3πt),
−3πt ≤x < 3πt
2 sin(3πt) sin x,
3πt ≤x < (1 −3t)π
1 + cos(x −3πt),
(1 −3t)π ≤x < (1 + 3t)π
0,
x ≥(1 + 3t)π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(10.90)
(b) Second, suppose t ≥1
6. It makes sense to begin by studying the “transitional” case
that can occur that couldn’t occur in part (a).
Case (3) occurs when part of Jx is to the left of [0, π), all of [0, π) is contained in Jx, and
part of Jx is to the right of [0, π). This occurs when the left endpoint of Jx, that is, x −3πt,
is to the left of 0 and the right endpoint of Jx, that is, x + 3πt, is to the right of π. This case
occurs when both x −3πt < 0 and x + 3πt ≥π, that is, when x < 3πt and x ≥π −3πt. So,
case (3) occurs when (1 −3t)π ≤x < 3πt.

894
Advanced Engineering Mathematics
Case (1) occurs when Jx is to the left of [0, π), that is, when x + 3πt < 0. So, in part (b) as
in part (a), case (1) occurs when x < −3πt.
Case (2) occurs when Jx straddles [0, π) on the left, specifically when (i) the left endpoint
of Jx is to the left of [0, π) and (ii) the right endpoint of Jx is in the interval [0, π). This
happens when (i) x −3πt < 0 and (ii) 0 ≤x + 3πt < π, that is, (i) x < 3πt and (ii)(A)
−3πt ≤x and (ii)(B) x < π −3πt. Of these three inequalities, (i) requires x < 3πt, and
(ii)(B) requires x < (1 −3t)π; but because we assumed that t ≥1
6, (1 −3t) < 0, so (ii)(B)
implies x < 0, which is more demanding than the requirement of (i) that x < 3πt. So, case
(2) occurs when both −3πt ≤x and x < (1 −3t)π, that is, when −3πt ≤x < (1 −3t)π.
Case (4) cannot occur because we supposed that t ≥1
6.
Case (5) occurs when the left endpoint of Jx is in [0, π) and the right endpoint of Jx is to
the right of π. So, this requires (i) 0 ≤x −3πt < π and (ii) x + 3πt ≥π, that is, when (i)(A)
x ≥3πt, and (i)(B) x < (1 + 3t)π, and (ii) x ≥(1 −3t)πt. We are assuming in part (b) that
t ≥1
6, so of the two inequalities (i)(A) x ≥3πt and (ii) x ≥(1 −3t)πt, the more demanding
requirement is that x ≥3πt. So, case (5) occurs when 3πt ≤x < (1 + 3t)π.
The analysis of where case (6) occurs is the same as for part (a): case (6) occurs when
x ≥(1 + 3t)π.
Integrating
Now that we understand the cases for t ≥1
6, we can evaluate y(x, t), as given in (10.89):
1. For x < −3πt, Jx ∩[0, π) = ∅, the empty set, so y(x, t) = 0.
2. For −3πt ≤x < (1 −3t)π, the part of the interval Jx that overlaps [0, π) is
[0, x + 3πt], so
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
x+3πt

0
sin ξ dξ =
(
−cos ξ
)x+3πt
0
= 1 −cos(x + 3πt).
3. For (1 −3t)π ≤x < 3πt, all of [0, π) lies inside Jx, so Jx ∩[0, π) = [0, π), and thus,
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
π
0
sin ξ dξ =
(
−cos ξ
)π
0 = 2.
4. Case 4 cannot occur if t ≥1
6.
5. For 3πt ≤x < (1 + 3t)π,
the part of the interval Jx that overlaps [0, π) is
[x −3πt, π), so
y(x, t) =

Jx∩[0,π)
sin ξ dξ =
π
x−3πt
sin ξ dξ =
(
−cos ξ
)π
x−3πt = 1 + cos(x −3πt).
6. For x ≥(1 + 3t)π, again Jx ∩[0, π) = ∅so y(x, t) = 0.

Partial Differential Equations Models
895
2.0
1.5
1.0
0.5
–10
–5
0
5
10
15
x
2.0
1.5
1.0
0.5
–10
–5
0
5
10
15
x
2.0
1.5
1.0
0.5
–10
–5
0
5
10
15
x
x,——
1
12
y
x,——
1
6
y
y (x,1)
t=——
1
12
t = ——
1
6
t =1
 Case (a)
Case (b)
Case (c)
FIGURE 10.22
y

x, 1
12

, y

x, 1
6

, and y(x, 1).
Putting all of the cases that can occur together, for t ≥1
6,
y(x, t) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < −3πt
1 −cos(x + 3πt),
−3πt ≤x < (1 −3t)π
2,
(1 −3t)π ≤x < 3πt
1 + cos(x −3πt),
3πt ≤x < (1 + 3t)π
0,
x ≥(1 + 3t)π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
(10.91)
The graphs of the solution y(x, t) versus x are given in Figure 10.22 for times t = 1
12, 1
6, 1.
The graphs for those values of t come from, respectively, (10.90), (10.91), and (10.91).
If we ask for the graph of y(x, t) versus x for only one particular value of t, then we
wouldn’t have to do all of the work as in Example 10.16; instead, we would first decide
which of case (a) or (b) is appropriate for that particular value of t.
10.5.5 Problems
1. An infinite string with linear mass density 0.01 kg/m has a tension of 4 N. It has an
initial displacement of (x) m, which is sketched in Figure 10.23 for −2 ≤x < 2
and which is zero for all other x. The string is released from rest with that
2
x
Φ(x)
–2
FIGURE 10.23
Problem 10.5.5.1.

896
Advanced Engineering Mathematics
2
1
Φ(x)
x
–2
–1
1
2
FIGURE 10.24
Problem 10.5.5.2.
displacement. (a) Determine a formula for the subsequent motion of the string
in as explicit a form as possible, (b) sketch the graph of u(x, 0.025), and (c) sketch
the graph of u(x, 0.05).
2. An infinite string with linear mass density 0.01 kg/m has a tension of 4 N. It has an
initial displacement of (x) m, which is sketched in Figure 10.24 for −2 ≤x < 2
and which is zero for all other x. The string is released from rest with that dis-
placement. (a) Determine a formula for the subsequent motion of the string in as
explicit a form as possible, (b) sketch the graph of u(x, 0.025), and (c) sketch the
graph of u(x, 0.05).
3. An infinite string satisfies ∂2u
∂t2 = 100∂2u
∂x2 , ∂u
∂t (x, 0) ≡0, and
u(x, 0) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < −1
x + 1,
−1 ≤x < 0
1,
0 ≤x < 1
0,
1 ≤x
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
Sketch the graph of u(x, 0.07).
4. An infinite string with linear mass density 0.1 kg/m is held taut by a force of 10 N.
Its initial displacement is
u(x, 0) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
x < −1
0.2,
−1 ≤x < 0
0.2(1 −x),
0 ≤x < 1
0,
1 ≤x
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
and is released from rest. Find a formula for the subsequent motion of the string
in as explicit a form as possible, and sketch the solution at t = 0.05.

Partial Differential Equations Models
897
5. An infinite string with wave speed c = 20 m/s is given initial velocity
(x) =
⎧
⎨
⎩
20 m/s,
0 ≤x < 2
0,
all other x
⎫
⎬
⎭
and has zero initial displacement. Find (a) u(x, 0.025) and (b) u(x, t) in as explicit a
form as possible.
6. An infinite string with wave speed c = 20 m/s is given initial velocity
(x) =
⎧
⎨
⎩
4 m/s,
0 ≤x < 1
0,
all other x
⎫
⎬
⎭
and has zero initial displacement. Find (a) u(x, 0.025) and (b) u(x, t) in as explicit a
form as possible.
7. An infinite string with wave speed c = 20 m/s is given initial velocity
(x) =
⎧
⎨
⎩
20 m/s,
2 ≤x < 3
0,
all other x
⎫
⎬
⎭
and has zero initial displacement. Find (a) u(x, 0.025) and (b) u(x, t) in as explicit a
form as possible.
8. Look for plane wave solutions of the form u(x, y, t) = f(k1x+k2y−ct) for the wave
equation in two space dimensions
∂2u
∂t2 = c2
&
∂2u
∂x2 + ∂2u
∂y2
'
.
Find an equation that the pair (k1, k2) must satisfy. By the way,
$
k2
1 + k2
2 is called
the wave number.
10.6 Short Take: Conservation of Energy in a Finite String
If u(x, t) satisfies the PDE of a vibrating string
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
(10.92)

898
Advanced Engineering Mathematics
where c2 = T0/ϱ0, on the interval 0 < x < L, and the BCs
u(0, t) = u(L, t) = 0, t > 0,
(10.93)
then we will explain why the quantity E(t) defined by
E(t) ≜
L
0
1
2
&
T0
∂u
∂x
2
+ ϱ0
∂u
∂t
2'
dx
(10.94)
is, in fact, constant. The natural physical interpretation of
 L
0
1
2ϱ0
∂u
∂t
2
dx is that of total
kinetic energy in the string, and a somewhat (but not entirely) natural physical interpre-
tation of
L
0
1
2T0
∂u
∂x
2
dx is that of the total potential energy in the string. So, if we can
explain why E(t) is constant, we will have explained why the principle of conservation of
energy holds true for this mathematical model.
We calculate that
dE
dt =
L
0
1
2
∂
∂t
&
T0
∂u
∂x
2
+ ϱ0
∂u
∂t
2'
dx =
L
0
T0
∂u
∂x
∂2u
∂x∂t dx +
L
0
ϱ0
∂u
∂t
∂2u
∂t2 dx.
Using integration by parts in the first term and using the PDE in the second term gives
dE
dt =

T0
∂u
∂x
∂u
∂t
L
0
−
L
0
T0
∂2u
∂x2
∂u
∂t dx +
L
0
ϱ0
∂u
∂t c2 ∂2u
∂x2 dx.
Regarding the first terms, that is, ∂u
∂x(L, t)∂u
∂t (L, t) −∂u
∂x(0, t)∂u
∂t (0, t), the BC u(0, t) = 0 for
all t > 0 implies that ∂u
∂t (0, t) = 0 for all t > 0.∗Similarly u(L, t) = 0 for all t > 0 implies
that ∂u
∂t (L, t) = 0. It follows that the first term is zero, so
dE
dt = −
L
0
T0
∂2u
∂x2
∂u
∂t dx +
L
0
ϱ0
∂u
∂t c2 ∂2u
∂x2 dx,
which equals zero because c2 = T0/ϱ0.
So, the principle of conservation of energy holds true for this PDE BVP. Notice that the
initial conditions were never used in establishing the conservation of energy. In fact, using
the initial conditions u(x, 0) = f(x), ∂u
∂t (x, 0) = g(x), 0 < x < L, we see that
∗This makes sense but requires some advanced mathematical justification concerning functions of two variables
u(x, t), specifically why limx→0+

∂u
∂t (x, t)

= ∂
∂t

lim
x→0+ u(x, t)

.

Partial Differential Equations Models
899
E(t) = E(0) =
L
0
1
2
&
T0
 df
dx
2
+ ϱ0

g(x)
2
'
dx
for all t > 0.
While the total energy, E, is constant, the same cannot be said for either the kinetic
energy, KE, or the potential energy, PE, in the string. But, we will see that both KE and PE
are periodic functions of time, t.
In Section 11.2, specifically (11.25), we will see that the general solution of the PDE BVP
is of the form
u(x, t) =
∞
+
n=1

an cos
nπct
L

+ bn sin
nπct
L

sin
nπx
L

.
We assume we can move partial derivatives inside infinite sums to get
∂u
∂t (x, t) =
∞
+
n=1
nπc
L
 
−an sin
nπct
L

+ bn cos
nπct
L

sin
nπx
L

and
∂u
∂x(x, t) =
∞
+
n=1
nπ
L
 
an cos
nπct
L

+ bn sin
nπct
L

cos
nπx
L

.
So, by Parseval’s Theorem 9.20 in Section 9.8, denoting
Bn(t) ≜nπc
L

−an sin
nπct
L

+ bn cos
nπct
L

,
KE(t) ≜
L
0
1
2 ϱ0
∂u
∂t
2
dx = ϱ0
2 · L
2
+
n=1
(Bn(t))2
= ϱ0L
4
∞
+
n=1
nπc
L
2 
−an sin
nπct
L

+ bn cos
nπct
L
2
.
Similarly, by Parseval’s Theorem 9.21 in Section 9.8,
PE(t) ≜
L
0
1
2 T0
∂u
∂x
2
dx = T0L
4
∞
+
n=1
nπ
L
2 
an cos
nπct
L

+ bn sin
nπct
L
2
.

900
Advanced Engineering Mathematics
Incidentally, these calculations offer an alternative way to verify that conservation of
energy holds for this PDE BVP, by explaining why the sum
ϱ0c2

−an sin
nπct
L

+ bn cos
nπct
L
2
+ T0

an cos
nπct
L

+ bn sin
nπct
L
2
is constant.
By the way, at URL
⟨http://proxima.math.wright.edu/People/Larry_Turyn/strings.html⟩
you will find a brief movie showing a vibration of a plucked string and the potential energy
in the string as a function of time. (It’s an old web page and those are the only functioning
parts of the page.)
10.6.1 Problems
1. Let T be the period of PE(t), KE(t) as a function of t. Use Parseval’s theorem from
Section 9.8 to explain why the average value of the potential energy, that is, PE =
1
T
 T
0 PE(t)dt, and the average value of the kinetic energy, KE = 1
T
 T
0 KE(t)dt, are
equal, and find those average values.
2. [R. Haberman] Here’s a model for the vibrations of a string whose right end is
fixed but whose left end is attached to a mass on a spring and is thus free to move
up or down: The same PDE (10.92) holds along with the BC
ku(0, t)−T0
∂u
∂x(0, t)=0, u(L, t) = 0, 0 < t < ∞,
where k is the spring constant. [We assume that the mass of the spring itself is
negligible.] Show there is conservation of total energy with the potential energy
formula modified to include the potential energy of the left-end spring, that is,
KE(t) ≜
L
0
1
2ϱ0
 ∂u
∂t
2 dx and PE(t) ≜
L
0
1
2ϱ0
 ∂u
∂x
2 dx + 1
2k (u(0, t))2.
3. Use the trigonometric identity cos2 θ + sin2 θ = 1 to explain why the function of
time given by (10.94) is constant.
Key Terms
adiabatic conditions: before (10.69) in Section 10.4
body forces: after (10.20) in Section 10.1
Cauchy equation: (10.24) in Section 10.1
composite rod: after Example 10.6 in Section 10.2
contact force: after (10.18) in Section 10.1, before (10.55) in Section 10.4
contact resistance: end of Section 10.2
continuum mechanics: after (10.15) in Section 10.1
d’Alembert wave solutions: (10.81) in Section 10.5

Partial Differential Equations Models
901
deformation, displacement: after (10.19) in Section 10.1
density of heat energy: Example 10.1 in Section 10.1
elastic: before (10.55) in Section 10.4
empty set: after Figure 10.21
energy density: Problem 10.4.6.10
equilibrium temperature: before (10.36) in Section 10.2
Fourier’s law of heat conduction: (10.27) in Section 10.2
heat equation: (10.30) in Section 10.2
heat flux vector: Example 10.1 in Section 10.1
intersection symbol: after (10.89) in Section 10.5
irrotational: after (10.44) in Section 10.3
isotropic: before (10.74) in Section 10.4
Lagrangian description: after (10.15) in Section 10.1
Lamé constants: after (10.74) in Section 10.4
Laplace’s equation: (10.36) in Section 10.2
linear elasticity: after (10.73) in Section 10.4
linear elastostatics: before (10.75) in Section 10.4
linearized strain tensor: after (10.71) in Section 10.4
local elongation: after (10.54) in Section 10.4
magnetostatics: before (10.46) in Section 10.3
Maxwell’s equations of electromagnetism: before (10.6) in Section 10.1
momentum density: Problem 10.4.6.10
plane strain: Definition 10.2 in Section 10.4
plane wave: Problem 10.5.5.8
Poisson’s equation: after (10.36) in Section 10.2
Poisson’s ratio: after (10.76) in Section 10.4
potential equation: (10.36) in Section 10.2
potential flow: (10.44) in Section 10.3
principle of virtual work: Problem 10.4.6.11
principal strains: Definition 10.1 in Section 10.4
reference configuration: before (10.54) in Section 10.4
reference state: after (10.15) in Section 10.1, before (10.54) in Section 10.4
shear modulus: after (10.74) in Section 10.4
solvability condition: (10.50) in Section 10.3
specific heat: beginning of Section 10.2
speed of sound: after (10.68) in Section 10.4
steady-state temperature: before (10.36) in Section 10.2
stress tensor: before (10.19) in Section 10.1
stress vector: after (10.18) in Section 10.1
string: before (10.55) in Section 10.4
support: before Figure 10.19
temperature: beginning of Section 10.2
tension force: (10.55) in Section 10.4
thermal conductivity: (10.27) in Section 10.2
thermal diffusivity: after (10.29) in Section 10.2
tied at the ends: after (10.54) in Section 10.4
total linear momentum: after (10.19) in Section 10.1
traction vector: after (10.18) in Section 10.1
uniaxial stretching: before (10.75) in Section 10.4

902
Advanced Engineering Mathematics
vibrating string: before (10.54) in Section 10.4
vorticity: after (10.44) in Section 10.3
wave number: Problem 10.5.5.8
Young’s modulus: before (10.77) in Section 10.4
Reference
Evans, L.C. Partial Differential Equations. American Mathematical Society, Providence, RI, 1998.

11
Separation of Variables for PDEs
11.1 Heat Equation in One Space Dimension
Here we begin using the “method of separation of variables” to solve models with partial
differential equations (PDEs). Our presentation, based on eigenfunction expansions, is
initially a little different from most books but uses the engineering style of standardization:
Our method uses over and over again off-the-shelf components whose properties are well
understood and then extended in their usefulness.
In Section 10.2, we studied models for heat flux and temperature in heat conduction,
diffusion of substances, and propagation of long electromagnetic waves in a good conduc-
tor. We saw that integral forms of heat balance led to the heat equation in one or more
dimensions, saw how the concept of heat flux is involved in some boundary conditions,
and calculated equilibrium solutions for 1D problems.
The PDE–boundary value problem–initial value problem (PDE-BVP-IVP)
∂T
∂t (x, t) = α ∂2T
∂x2 (x, t), 0 < x < L, t > 0,
(11.1)
T(0, t) = T(L, t) = 0, t > 0,
(11.2)
T(x, 0) = f(x), 0 < x < L,
(11.3)
will be solved by the separation of variables method discussed in the following. The region
in the xt-plane, that is, 0 < x < L, t > 0, is depicted in Figure 11.1. The two boundary con-
ditions (11.2) are on the parallel sides x = 0 and x = L. First, we need some background
knowledge.
The ordinary differential equation–boundary value problem (ODE-BVP)
⎧
⎨
⎩
X′′(x) + λX(x) = 0,
X(0) = X(L) = 0
⎫
⎬
⎭
(11.4)
has infinitely many solutions for the eigenvalues λn =
 nπ
L
	2 , n = 1, 2, . . ., and correspond-
ing eigenfunctions Xn(x) = sin
 nπx
L
	
, n = 1, 2, ...; for all other values of λ, (11.4) has only
the trivial solution, that is, X(x) ≡0. We justified these conclusions in Example 9.14 in
Section 9.3.
Notice that the boundary conditions in (11.4) are the same as the boundary conditions
(11.2) applied to a function X(x) rather than to a function of two variables such as T(x, t).
903

904
Advanced Engineering Mathematics
t
x =0
x= L
FIGURE 11.1
The region 0 < x < L, t > 0 in the xt-plane.
With that background knowledge, we can solve (11.1) through (11.3): Because of the
“homogeneous boundary conditions on parallel sides,” that is, T(0, t) = T(L, t) = 0, t > 0,
we can imagine there are “product solutions” of the PDE in the assumed form T(x, t) =
X(x)G(t), where X solves the ODE-BVP (11.4). Because X(x) satisfies the boundary condi-
tions X(0) = X(L) = 0, such a product solution T(x, t) automatically satisfies the boundary
conditions T(0, t) = T(L, t) = 0. By the method of separation of variables, we mean this
technique of using product solutions in which one (or more) of the function factors satisfies
a pair of boundary conditions on parallel sides.
Let X = Xn(x) = sin
 nπx
L
	
and substitute T(x, t) = sin
 nπx
L
	
G(t) into the PDE ∂
∂tT(x, t) =
α ∂2
∂x2 T(x, t) to get
sin

nπx
L
 dG
dt = −α

nπ
L
2
sin

nπx
L

G(t), n = 1, 2, . . . ;
hence,
dG
dt = −α

nπ
L
2
G(t), n = 1, 2, . . . .
This is the simplest kind of first-order constant coefficients ODE, the solutions being
Gn(t) = bne−α( nπ
L )2t, n = 1, 2, . . . ,
where bn is an arbitrary constant. The subscript n in the formulas indicates that there is
dependence on n, for example, for each value of n, one might have a varying constant bn.
The product solutions are
Tn(x, t) = bn sin

nπx
L

e−α( nπ
L )2t, n = 1, 2, . . . .

Separation of Variables for PDEs
905
What’s left in solving the PDE-BVP-IVP? The initial condition is T(x, 0) = f(x), 0 < x < L,
that is, (11.3). The method is to use all of the product solutions by using the infinite series
T(x, t) =
∞

n=1
Tn(x, t) =
∞

n=1
bn sin

nπx
L

e−α( nπ
L )2t.
(11.5)
This is similar to our method of using Fourier analysis to solve a forced oscillator problem
in Example 9.13 in Section 9.2.
Substitute (11.5) into the initial condition (11.3), that is, T(x, 0) = f(x), 0 < x < L, to get
f(x) = T(x, 0) =
∞

n=1
bn sin

nπx
L

.
(11.6)
Fourier analysis of this sine series yields
bn = 2
L
L
0
f(x) sin

nπx
L

dx.
So, we can say that the solution is given by
T(x, t) =
∞

n=1
⎛
⎝2
L
L
0
f(x) sin

nπx
L

dx
⎞
⎠sin

nπx
L

e−α( nπ
L )2t.
Example 11.1
Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < 5, t > 0,
T(0, t) = T(5, t) = 0, t > 0,
T(x, 0) = f(x) =
⎧
⎨
⎩
56x,
0 < x < 5
2
280 −56x,
5
2 < x < 5
⎫
⎬
⎭
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(11.7)
Method: This problem fits directly into the form of problem (11.1) through (11.3). Note
especially that there are two homogeneous boundary conditions T(0, t) = T(5, t) = 0, so
L = 5 in this problem and the solution of the PDE and BVP is
T(x, t) =
∞

n=1
bn sin

nπx
5

e−α

nπ
5
2
t.

906
Advanced Engineering Mathematics
The initial condition is satisfied by solving f(x) =
∞

n=1
bn sin
 nπx
5
	
, 0 < x < 5. Fourier
sine series analysis implies that this is done by finding the coefficients:
bn = 2
5
5
0
f(x) sin

nπx
5

dx = 2
5
5/2

0
(56x) sin

nπx
5

dx + 2
5
5
5/2
(280 −56x) sin

nπx
5

dx
= 112
5
⎛
⎝
−5x
nπ cos

nπx
5

+
25
(nπ)2 sin

nπx
5
 5
2
0
+
−5(5 −x)
nπ
cos

nπx
5

−
25
(nπ)2 sin

nπx
5
5
5
2

= 112
5
−25
2nπ cos

nπ
2

+
25
(nπ)2 sin

nπ
2

+ 0 −0

+

−0 −0 + 25
2nπ cos

nπ
2

+
25
(nπ)2 sin

nπ
2

= 1120
(nπ)2 sin

nπ
2

.
So the solution is
T(x, t) = 1120
π2
∞

n=1
1
n2 sin

nπ
2

sin

nπx
5

e−α

nπ
5
2
t. ⃝
(11.8)
Notice that we did bother to evaluate the definite integrals and we did state a final
conclusion giving the solution to the whole PDE-BVP-IVP.
For purposes of computation, one would only take a finite number of terms in the series
(11.8). Also, half of the terms in the series are zero, because when n is even, sin
 nπ
2
	
= 0.
So, we need only to include the odd-indexed terms, that is, when n = 2k −1, k =, 1, 2, . . . .
We can rewrite (11.8), after noting that sin

(2k−1)π
2

= sin(kπ −π
2 ) = −cos(kπ) = −(−1)k:
T(x, t) = −1120
π2
∞

k=1
(−1)k
(2k −1)2 sin
(2k −1)πx
5

e
−α

(2k−1)π
5
2
t.
Of physical interest is the decay, as t →∞, of the solution T(x, t). Each of the func-
tions e−α
 nπ
5
	2t →0 as t →∞. Recall that for an ODE, the time constant indicates how
long it takes for a solution to decay to
1
e of its initial value. The time constant for
a heat equation PDE is defined to be the reciprocal of the slowest decay rate rmin.
Because T(x, t) includes many different decaying exponential functions, “weighted” by

2
5

 5
0 T(x, 0) sin
 nπx
5
	
dx

sin
 nπx
5
	
, we can’t guarantee that T(x, τ) = 1
e T(x, 0). Never-
theless, for physical intuition, it is still useful to think of the time constant as being how
long it takes for the solution to decay in a standard way.

Separation of Variables for PDEs
907
The function with n = 1 has the slowest rate of decay, rmin = α
 π
5
	2 = α π2
25 . The corre-
sponding time constant is
τ ≜
1
rmin
=
25
α π2 .
Example 11.2
A 2 m long steel rod, with thermal diffusivity α ≈1.24×10−5m2/s with insulated lateral
surface, has its left end maintained at a temperature of 300◦C and its right end main-
tained at 20◦C. Determine the temperature as a function of x and t if the initial condition
is given by
T(x, 0) = f(x) =

300,
0 < x < 5
2
580 −112x,
5
2 < x < 5

.
(11.9)
Method: The physical problem translates to a mathematical problem, specifically the
PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < 5, t > 0,
T(0, t) = 300, T(5, t) = 20, t > 0,
T(x, 0) =

300,
0 < x < 5
2
580 −112x,
5
2 < x < 5

⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
(11.10)
Unfortunately, this problem does not directly fit into the form of problem (11.1) through
(11.3) because of the inhomogeneous boundary conditions T(0, t) = 300 and T(5, t) = 20.
But we will see that we can find the solution of this problem by a two-step process, and
the second step will turn out to be Example 11.1.
It turns out that the correct form of the solution is not simply the infinite series (11.5)
but instead the sum of a equilibrium solution plus the infinite series (11.5). To be explicit,
the solution will be in the form
T(x, t) = v(x) +
∞

n=1
bn sin

nπx
5

e−α

nπ
5
2
t.
(11.11)
As we saw in Chapter 10, the problem in (11.10), the equilibrium solution, v(x), satisfies
∂v
∂t ≡0, and we want it to also satisfy the PDE in (11.10); hence, 0 = α ∂2v
∂x2 . Because v
is a function only of x, the partial derivatives are really ordinary derivatives, so v(x)
should satisfy ODE 0 = α v′′(x). The equilibrium solution of (11.10) should satisfy also
the boundary conditions (BCs) v(0) = 300, v(5) = 20. So, altogether, v(x) should satisfy
⎧
⎨
⎩
0 = α v′′(x), 0 < x < 5,
v(0) = 300, v(5) = 20.
⎫
⎬
⎭.
(11.12)
Before solving the inhomogeneous ODE-BVP (11.12), let’s see what it will accomplish
for us: If v(x) solves (11.12) and T(x, t) = v(x) + w(x, t), that is, w(x, t) = T(x, t) −v(x), is
to solve the original PDE in (11.10), then applying the BCs to T(x, t) gives
w(0, t) = T(0, t) −v(0) = 300 −300 = 0
and
w(5, t) = T(5, t) −v(5) = 20 −20 = 0.

908
Advanced Engineering Mathematics
Because v(x) will solve the linear homogeneous PDE (11.1) and T(x, t) should solve the
same linear homogeneous PDE, w = T −v must solve it, too. So, putting this all together
implies that w(x, t) should satisfy the PDE-BVP
⎧
⎪⎨
⎪⎩
∂w
∂t = α ∂2w
∂x2 , 0 < x < 5, t > 0
w(0, t) = 0, w(5, t) = 0, t > 0
⎫
⎪⎬
⎪⎭
.
This explains why the solution of the original problem (11.10) should be in the form
(11.11).
To solve (11.12), first note that the general solution of 0 = v′′ is given by v = c1 + c2x,
for some constants c1, c2. Substituting this into the boundary conditions in system (11.12)
gives 300 = v(0) = c1, 20 = v(2) = c1 + c25 = 300 + c25; hence, −280 = c25. So,
v(x) = 300 −56x.
The only thing left is to arrange that the solution of the PDE-BVP, that is,
T(x, t) = 300 −56x +
∞

n=1
bn sin

nπx
5

e−α

nπ
5
2
t,
satisfies the initial condition T(x, 0) =

300,
0 < x < 5
2
580 −112x,
5
2 < x < 5

, that is,
300 −56x +
∞

n=1
bn sin

nπx
5

=

300,
0 < x < 5
2
580 −112x,
5
2 < x < 5

,
that is,
∞

n=1
bn sin

nπx
5

=

56x,
0 < x < 5
2
280 −56x,
5
2 < x < 5

,
(11.13)
just as for Example 11.1. Using the solution of the latter problem, we find that the
solution of Example 11.2 is
T(x, t) = 300 −56x −1120
π2
∞

k=1
(−1)k
(2k −1)2 sin
(2k −1)πx
5

e−1.24×10−5
 (2k−1)π
5
2
t,
using the thermal diffusivity for steel. ⃝
Figure 11.2 depicts the time evolution of T(x, t): Using the truncation of the infinite series
to its first fifty terms, we graphed approximations of T(x, t) versus x for 0 ≤x ≤5, for
t = 0, 103, 2.5 × 104, 5 × 104, 5 × 105, 106. The graphs clearly show the convergence of the
temperature to its equilibrium distribution, that is, T(x, t) →v(x) as t →∞.
Example 11.3
For which values of the constant β does the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 −x, 0 < x < L, t > 0,
∂T
∂x (0, t) = 0, T(L, t) = β, t > 0,
T(x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(11.14)

Separation of Variables for PDEs
909
250
200
150
100
50
1
2
t =0
3
4
5
x
x
x
x
x
x
T300
T300
T300
T300
T300
T300
250
200
150
100
50
1
2
t =103 3
4
5
250
200
150
100
50
1
2
t= 2.5 × 1043
4
5
250
200
150
100
50
1
2
3
4
5
250
200
150
100
50
1
2
3
4
5
250
200
150
100
50
1
2
3
4
5
t =5×104
t = 5 × 105
t =106
FIGURE 11.2
Temperature evolution for Example 11.1.2.
have a solution? What is the solution for such values of β? Your final conclusions should
be in terms of β.
Method: If either of the BCs has an inhomogeneity or if the PDE has an inhomogene-
ity that is a function only of x, then we should use a two-step process, as in Example
11.2: First, find the equilibrium solution v(x), a function only of x that satisfies the PDE
and BCs, and then second, take the solution of the original problem to be in the form
T(x, t) = v(x) + w(x, t). We will see that w(x, t) will satisfy a problem similar to that of
(11.1) through (11.3).
The equilibrium solution, v(x), satisfies ∂v
∂t ≡0, and we want it to also satisfy the PDE
in (11.14); hence, 0 = α ∂2v
∂x2 −x. Because v is a function only of x, the partial derivatives
are really ordinary derivatives, so v(x) should satisfy ODE 0 = α v′′ −x. The equilibrium
solution of (11.14) should satisfy also the BCs v′(0) = 0, v(L) = β. So, altogether, v(x)
should satisfy
⎧
⎨
⎩
0 = α v′′(x) −x, 0 < x < L,
v′(0) = 0, v(L) = β
⎫
⎬
⎭.
(11.15)
The ODE is solvable directly by integrating twice with respect to x:
v′′(x) = x
α ⇒v′(x) =
 x
α dx= 1
2α x2 + c1 ⇒v(x) =
  1
2α x2 + c1

dx= 1
6α x3 + c1x + c2,
where c1, c2 are arbitrary constants. We use this to satisfy also the BCs:
v′(x) = 1
2α x2 + c1 ⇒0 = v′(0) = c1 , β = v(L) = 1
6α L3 + 0 · x + c2.
We take c2 = β −1
6α L3, and the equilibrium solution is
v(x) = 1
6α x3 + β −1
6α L3,
which exists for all β.

910
Advanced Engineering Mathematics
As in Example 11.2, the solution of the PDE and BVP is T(x, t) = v(x) + w(x, t), that is,
w(x, t) = T(x, t) −v(x), so w(x, t) should satisfy the PDE ∂w
∂t = α ∂2w
∂x2 because
∂w
∂t = ∂T
∂t −∂v
∂t =

α ∂2T
∂x2 −x

−0 =

α ∂2(v + w)
∂x2

−x =

α ∂2(v)
∂x2 −x

+ α ∂2(w)
∂x2
= 0 + α ∂2(w)
∂x2 .
Also, w(x, t) should satisfy BCs, for t > 0:
0 = ∂T
∂x (0, t) = ∂(v + w)
∂x
(0, t) = v′(0) + ∂w
∂x (0, t) = 0 + ∂w
∂x (0, t), T(L, t) = β, t > 0;
hence,
(i)
∂w
∂x (0, t) = 0,
and
β = T(L, t) = v(L, t) + w(L, t) = β + w(L, t);
hence,
(ii)
w(L, t) = 0.
The PDE and BCs that w(x, t) should satisfy are
⎧
⎪⎨
⎪⎩
∂w
∂t = α ∂2w
∂x2 , 0 < x < L, t > 0,
∂w
∂x (0, t) = 0, w(L, t) = 0, t > 0
⎫
⎪⎬
⎪⎭
.
(11.16)
In Section 11.1.1 is Table 11.1, that is, Table 9.1, which we will use throughout this
chapter.
Noting the two homogeneous BCs that w(x, t) should satisfy and using the corre-
sponding last line of Table 9.1,
w(x, t) =
∞

n=1
an cos
⎛
⎝

n −1
2

πx
L
⎞
⎠e
−α
 
n−1
2

π
L
2
t
.
So, the solution of the original problem should be in the form
T(x, t) = v(x) + w(x, t) = 1
6α x3 + β −1
6α L3 +
∞

n=1
An cos
⎛
⎝

n −1
2

πx
L
⎞
⎠e
−α
 
n−1
2

π
L
2
t
.
We substitute this into the last remaining aspect of the problem, the initial condition (IC):
0 = T(x, 0) = 1
6α x3 + β −1
6α L3 +
∞

n=1
an cos
⎛
⎝

n −1
2

πx
L
⎞
⎠,

Separation of Variables for PDEs
911
TABLE 11.1
Eigenvalues and Eigenfunctions for X′′ + λX = 0
Boundary Conditions / Values of λ
λ>0
λ=0
λ<0
X(0) = X(L) = 0
λn =

nπ
L
2
,
None
None
Xn(x) = sin

nπx
L

, n = 1, 2, . . .
X′(0) = X′(L) = 0
λn =

nπ
L
2
,
λ0 = 0,
None
Xn(x) = cos

nπx
L

, n = 1, 2, . . .
X0(x)≡1
X(0) = X′(L) = 0
λn =
 
n−1
2

π
L
2
,
None
None
Xn(x) = sin
 
n−1
2

πx
L

, n = 1, 2, . . .
X′(0) = X(L) = 0
λn =
 
n−1
2

π
L
2
,
None
None
Xn(x) = cos
 
n−1
2

πx
L

, n = 1, 2, . . .
that is,
1
6α L3 −β −1
6α x3 =
∞

n=1
An cos
⎛
⎝

n −1
2

πx
L
⎞
⎠.
This is a generalized Fourier series problem, and the coefficients are
An = 2
L
L
0
 1
6α L3 −β −1
6α x3

cos
⎛
⎝

n −1
2

πx
L
⎞
⎠
=
 1
6α L3 −β

2

n −1
2

π
⎡
⎣sin
⎛
⎝

n −1
2

πx
L
⎞
⎠
⎤
⎦
L
0
−2
L
L
0
1
6α x3 cos
⎛
⎝

n −1
2

πx
L
⎞
⎠.
Using integration by parts to do the latter integral and the fact that sin


n −1
2

π

=
−cos nπ = (−1)n+1, we see that
An =
2

n −1
2

π
 1
6α L3 −β

(−1)n+1
−L3
3α
⎛
⎜⎝
6

n −1
2
4
π4
−
6

n −1
2
3
π3
(−1)n+1 +
1

n −1
2

π
(−1)n+1
⎞
⎟⎠
=
2β

n −1
2

π
(−1)n −
2L3
α

n −1
2
4
π4

1 +

n −1
2

(−1)n

.

912
Advanced Engineering Mathematics
The solution of the original problem is
T(x, t) = 1
6α x3 + β −1
6α L3 +
∞

n=1
An cos
⎛
⎝

n −1
2

πx
L
⎞
⎠e
−α
 
n−1
2

π
L
2
t
,
where An’s given earlier. ⃝
11.1.1 Easy Initial Conditions
Recall from our study of Fourier series that there are interesting special cases where a
Fourier series, or Fourier sine series or Fourier cosine series, degenerates to just a finite
sum of trigonometric functions. For example, if f(x) = 2 sin x −sin 4x, then the general
Fourier sine series representation, when L = π,
f(x) =
∞

n=1
bn sin nx,
that is,
2 sin x −sin 4x =
∞

n=1
bn sin nx,
just says that b1 = 2, b4 = −1, and bn = 0 for all n other than n = 1, 4.
Example 11.4
A two meter long steel rod, with thermal diffusivity α, has insulated lateral surface
and its ends maintained at temperature 0◦C. Determine the temperature if the initial
temperature distribution is T(x, 0) = 2 sin(πx) −sin

5πx
2

, 0 < x < 2.
Method: The physical problem translates into the mathematical problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < 2, t > 0,
T(0, t) = T(2, t) = 0, t > 0,
T(x, 0) = 2 sin

2πx
2

−sin

5πx
2

, 0 < x < 2
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
(11.17)
The PDE and homogeneous BCs fit directly into the form of problem (11.1) through
(11.3) with L = 2. The solution of the PDE and BVP is
T(x, t) =
∞

n=1
bn sin

nπx
2

e−α

nπ
2
2
t.
The initial condition gives
2 sin
2πx
2

−sin
5πx
2

=
∞

n=1
bn sin

nπx
2

, 0 < x < 2.

Separation of Variables for PDEs
913
So, b2 = 2, b5 = −1, and bn = 0 for all n other than n = 2 and 5. The solution of the
PDE-BVP-IVP is simply
T(x, t) = 2 sin(πx)e−απ2t −sin
5πx
2

e−α

5π
2
2
t. ⃝
It would be a shame to cover up the simplicity of this solution by stating a final conclusion
in terms of an infinite series. Also, we should not go through the unnecessary work of
calculating the values of bn for all n using the integrals
bn = 2
2
2
0

2 sin
2πx
2

−sin
5πx
2

sin

nπx
2

dx.
Table 11.1 gives for our convenience results about the eigenvalues and eigenfunctions
for ODE X′′(x) + λX(x) = 0 for the four most common sets of separated homogeneous
boundary conditions. Each of the four groups of entries corresponds to a different set of
physical boundary conditions.
11.1.2 Composite Rod
Example 11.5
Solve
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = 1
cϱ
∂
∂x
!
κ(x) ∂T
∂x
"
, 0 < x < 1 and 1 < x < 2, 0 < t < ∞,
∂T
∂x (0, t) = ∂T
∂x (2, t) = 0, 0 < t < ∞
T(x, 0) = f(x), 0 < x < 1 and 1 < x < 2
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
where c, ϱ are constants and the thermal conductivity, κ(x), is defined piecewise by
κ(x) =
⎧
⎨
⎩
1
2,
0 < x < 1
1
3,
1 < x < 2
⎫
⎬
⎭.
As we saw in Section 10.2, the jump in the coefficient κ, as well as the interface con-
ditions (9.75) in Section 9.6 at x = 1, corresponds physically to joining two types of
materials, for example, two metals, at x = 1. This is why we refer to this as a problem
for a “composite” rod.
Method: In Example 9.32 in Section 9.6, we found the eigenvalues, λn, n = 0, 1, . . ., and
corresponding eigenfunctions
Xn(x) =
⎧
⎨
⎩
−
√
2 sin(√3λn) cos(√2λn x),
0 < x < 1
√
3 sin(√2λn) cos(√3λn (2 −x)) ,
1 < x < 2
⎫
⎬
⎭,
n = 0, 1, . . ., for the ODE-BVP with interface conditions (9.75) in Section 9.6. Note that
λ0 = 0 and X0(x) ≡1. We look for product solutions T = Xn(x)Gn(t), and, similar to pre-
vious work, we get ˙Gn = (−λn/(cϱ)) Gn; hence, Gn(t) = exp (−λnt/(cϱ)). The general

914
Advanced Engineering Mathematics
solution of the PDE plus the two BCs and the interface conditions is
T(x, t) = a0
2 +
∞

n=1
anXn(x)e−λnt/(cϱ).
All that remains is to satisfy the initial condition
f(x) = T(x, 0) = a0
2 +
∞

n=1
anXn(x).
Orthogonality, with L = 2, gives us
a0 =
2
0
f(x) dx
and
an = 1
Nn
⎛
⎝
1
0
Xn(x)f(x) dx +
2
1
Xn(x)f(x) dx
⎞
⎠
= 1
Nn
⎛
⎝
1
0
−
√
2 sin(
#
3λn) cos(
#
2λn x)f(x) dx
+
2
1
√
3 sin(
#
2λn) cos(
#
3λn (2 −x))f(x) dx
⎞
⎠,
where Nn =
 2
0

Xn(x)
	2dx = · · · = sin2(
#
3λn) + 3
2 sin2(
#
2λn). ⃝
11.1.3 Time-Dependent Boundary Conditions
For the heat equation, the boundary conditions could depend on time. For example,
consider the following problem.
Example 11.6
A copper rod is 30.0 cm long and has diameter 0.400 cm. Assume that the approxi-
mate physical parameters of copper are mass density ϱ ≈8.96 g/cm3, specific heat
c ≈0.3851 J/(g · K), and thermal conductivity κ ≈3.98 W/(cm · K). Its lateral sur-
face is insulated, its left end is maintained at a temperature of (20.0 −2.00 sin(0.1t))◦C,
and its right end is losing heat at a constant rate of 1 W. Throughout the rod, heat energy
is added to the rod at a rate of 0.200 W/cm3. Initially the temperature in the rod is a
constant, 20.0◦C.
(a) Translate the physical problem into a mathematical problem.
(b) Determine the temperature as a function of x and t.
Method:
(a) Measuring x in cm, the physical problem translates to a mathematical problem con-
sisting of the PDE of a 1D heat equation for temperature T(x, t), with source term
Q(x, t)/(cϱ), a time-dependent boundary condition on T(0, t), a boundary condition
on ∂T
∂x (30, t), and the initial condition on T(x, 0).

Separation of Variables for PDEs
915
First, the physical parameter of thermal diffusivity is
α = κ
cϱ =
3.98 J/s ·cm · K

0.3851 J/(g ·◦K)
	 
8.96 g/cm3	 ≈1.15 cm2
s
,
to the three significant digits of accuracy found in the given physical parameters.
Next, the source term is
Q(x, t)/(cϱ) =
0.200 J/(s · cm3)

0.3851 J/(g · K)
	 
8.96 g/cm3	 ≈0.0580 K/s,
to the three significant digits of accuracy found in the given physical parameters.
The left end boundary condition is T(0, t) = (20.0 −2.00 sin(0.1t))◦C, and the initial
condition is T(x, 0) = 20.0◦C. As for the right end boundary condition, the +1 W
heat flux out of the rod there is −Aκ ∂T
∂x (30.0, t), where A is the area of cross section
of the rod; hence,
∂T
∂x (30.0, t) = −
1 W
π(0.200 cm)23.98 W/(cm ·◦K) ≈−2.00◦C/cm,
to the three significant digits of accuracy found in the physical condition at x = 30.
The mathematical problem is, with approximate physical parameters and sup-
pressing trailing zeros that indicate the number of significant digits,
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = 1.15 ∂2T
∂x2 + 0.058, 0 < x < 30, t > 0,
T(0, t) = 20 −2 sin(0.1t), ∂T
∂x (30, t) = −2, t > 0,
T(x, 0) = 20, 0 < x < 30
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
(b) Unfortunately, this problem does not directly fit into any problem we studied
because of the time-dependent boundary condition. Nevertheless, we should pro-
ceed in a way that is close to the method of writing T(x, t) = v(x) + w(x, t), where
v(x) is a steady state temperature distribution. But, in this problem, we don’t expect
that there is an equilibrium temperature distribution.
Instead, let’s write T(x, t) = v(x, t)+w(x, t), where v(x, t) only satisfies the two inhomo-
geneous boundary conditions. Of course, an infinite number of functions v(x, t) satisfy
the two inhomogeneous boundary conditions. We might as well choose v(x, t) to be as
simple as possible, because of the basic principle to keep things as simple as possible. Also,
we suspect that the simpler the function v(x, t), the easier it will be to find w(x, t).
As far as dependence on x is concerned, the simplest function v would have the form
v(x, t) = β(t) + γ (t)x. The first boundary condition then yields 20 −2 sin(0.1t) = v(0, t) =
β(t), and the second boundary condition gives −2 = ∂v
∂x(30, t) = γ (t), so v(x, t) = 20 −
2 sin(0.1t) −2x.
Now, w(x, t) = T(x, t) −v(x, t) = T(x, t) −20 + 2 sin(0.1t) + 2x. First, we need to find
the PDE to be satisfied by w(x, t); the PDE itself will be affected by v(x, t). We calculate
∂w
∂t = ∂T
∂t −∂v
∂t =

1.15∂2T
∂x2 + 0.058

+ 0.2 cos(0.1t)
= 1.15∂2(w + v)
∂x2
+ 0.058 + 0.2 cos(0.1t),
that is,
∂w
∂t = 1.15∂2w
∂x2 + 0.058 + 0.2 cos(0.1t).
(11.18)

916
Advanced Engineering Mathematics
The boundary conditions that w should satisfy are w(0, t) = T(0, t) −v(0, t) = 20 −
2 sin(0.1t) −(20 −2 sin(0.1t)) = 0 and ∂w
∂x (30, t) = ∂T
∂x (30, t) −∂v
∂x(30, t) = −2 −(−2) = 0,
and the initial condition that w should satisfy is
w(x, 0) = T(x, 0) −v(x, 0) = 20 −(20 −2x) = 2x.
Because of the homogeneous boundary conditions
w(x, 0) = 0,
∂w
∂x (30, t) = 0,
we should take w(x, t) to be in terms of the functions sin
 
n−1
2

πx
30

. But the PDE is not
homogeneous; in fact, it involves a function of time, t. So, we should try to find w in the
form
w(x, t) =
∞

n=1
bn(t) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠.
When we substitute this into PDE (11.18), we get
∞

n=1
dbn
dt (t) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠= ∂w
∂t = 1.15 ∂2w
∂x2 + 0.058 + 0.2 cos(0.1t) =
= 0.058 + 0.2 cos(0.1t) + 1.15
∞

n=1
bn(t)
⎛
⎜⎝−
⎛
⎝

n −1
2

π
30
⎞
⎠
2⎞
⎟⎠sin
⎛
⎝

n −1
2

πx
30
⎞
⎠.
(11.19)
Using a generalized Fourier series to express (0.058 + 0.2 cos(0.1t)) as
0.058 + 0.2 cos(0.1t)
.=
∞

n=1
⎛
⎝2
30
30

0
(0.058 + 0.2 cos(0.1t)) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠dx
⎞
⎠sin
⎛
⎝

n −1
2

πx
30
⎞
⎠
≜
∞

n=1
Rn(t) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠,
and substituting that into (11.19), we get
∞

n=1
dbn
dt (t) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠
=
∞

n=1
Rn(t) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠+ 1.15
∞

n=1
bn(t)
⎛
⎜⎝−
⎛
⎝

n −1
2

π
30
⎞
⎠
2⎞
⎟⎠sin
⎛
⎝

n −1
2

πx
30
⎞
⎠.
Fourier analyzing, that is, breaking down the infinite sum into its components, gives
dbn
dt (t) = 1.15
⎛
⎜⎝−
⎛
⎝

n −1
2

π
30
⎞
⎠
2⎞
⎟⎠bn(t) + Rn(t),
(11.20)

Separation of Variables for PDEs
917
for n = 1, 2, . . ., where
Rn(t) = 2
30
30

0
(0.058 −0.2 cos(0.1t)) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠dx
= 2


30(0.058−0.2 cos(0.1t))
⎡
⎢⎢⎢⎢⎣
cos
 
n−1
2

πx
30

−

n−1
2

π

30
⎤
⎥⎥⎥⎥⎦
30
0
= −0.116 + 0.4 cos(0.1t))

n −1
2

π
.
To solve the first-order linear ODE (11.20), we could use an integrating factor or the
method of undetermined coefficients. Let’s use an integrating factor, after first putting
the ODE into the standard form:
dbn
dt (t) + 1.15
⎛
⎝

n −1
2

π
30
⎞
⎠
2
bn = Rn(t).
The integrating factor is μ(t) = e
1.15
 
n−1
2

π
30
2
t
, so the solution of (11.20) is
bn(t) = bn(0) + e
−1.15
 
n−1
2

π
30
2
t
·
1

n −1
2

π
t
0
(0.116 + 0.4 cos(0.1s))e
1.15
 
n−1
2

π
30
2
s
ds
= bn(0) +
1

n −1
2

π
e
−1.15
 
n−1
2

π
30
2
t
·
⎡
⎢⎢⎢⎢⎢⎣
e
1.15

(n−1
2 )π
30
2
s
·
⎛
⎜⎜⎜⎜⎜⎝
0.116
1.15

(n−1
2 )π
30
2 +
.04 sin(0.1s) + 0.46

(n−1
2 )π
30
2
cos(0.1s)
(.1)2 +

1.15

(n−1
2 )π
30
22
⎞
⎟⎟⎟⎟⎟⎠
⎤
⎥⎥⎥⎥⎥⎦
t
0
= bn(0) +
1
(n −1
2)π
⎛
⎜⎜⎜⎝
0.116
1.15

n−1
2
	
30
2 +
0.04 sin(0.1t) + 0.46

(n−1
2 )π
30
2
cos(0.1t)
0.01 + 1.152

(n−1
2 )π
30
4
⎞
⎟⎟⎟⎠
−
1
(n −1
2)π
e
−1.15

(n−1
2 )π
30
2
t
⎛
⎜⎜⎜⎝
1
1.15

(n−1
2 )π
30
2 +
0.46

(n−1
2 )π
30
2
0.01 + 1.152

(n−1
2 )π
30
4
⎞
⎟⎟⎟⎠

918
Advanced Engineering Mathematics
As for bn(0), the initial condition of w(x, 0) = 2x yields
bn(0) = 2
30
30

0
(2x) sin
⎛
⎝

n −1
2

πx
30
⎞
⎠dx = · · · =
120

n −1
2
2
π2
(−1)n+1.
The solution of the PDE-BVP-IVP, that is, the temperature distribution in the rod, is
T(x, t) = v + w = 20 −2 sin(0.1t) −2x
+
∞

n=1
1
(n −1
2)2π2

120(−1)n+1 +
104.4
1.15(n −1
2)π

sin

(n −1
2)πx
30

+
∞

n=1
1
(n −1
2)π
⎛
⎜⎜⎜⎝
0.04 sin(0.1t) + 0.46

(n−1
2 )π
30
2
cos(0.1t)
0.01 + 1.152

(n−1
2 )π
30
4
⎞
⎟⎟⎟⎠sin

(n −1
2)πx
30

−
∞

n=1
1
(n −1
2)π
e
−1.15

(n−1
2 )π
30
2
t
⎛
⎜⎜⎜⎝
1
1.15

(n−1
2 )π
30
2 +
0.46

(n−1
2 )π
30
2
0.01 + 1.152

(n−1
2 )π
30
4
⎞
⎟⎟⎟⎠·
· sin

(n −1
2)πx
30

. ⃝
Learn More About It
The approach of beginning with eigenfunction expansions is also found in Separation of
Variables for Partial Differential Equations: An Eigenfunction Approach, by George Cain
and Gunter H. Meyer, Chapman and Hall/CRC c⃝2005.
It is an unfortunate reality that we cannot expect to have the existence and unique-
ness of a solution of a heat equation backward in time. See Applied Partial Differen-
tial Equations: With Fourier Series and Boundary Value Problems, 4th edn., by Richard
Haberman, Prentice-Hall, Inc. c⃝2004, specifically Problem 2.5.2.14 and also p. 680.
11.1.4 Problems
1. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = T(π, t) = 0, t > 0,
T(x, 0) = sin x −1
2 sin 3x, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.

Separation of Variables for PDEs
919
2. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = T(π, t) = 0, t > 0,
T(x, 0) = x, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
3. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
∂T
∂x (0, t) = ∂T
∂x (π, t) = 1, t > 0,
T(x, 0) = 2x, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
4. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
∂T
∂x (0, t) = T(π, t) = 0, t > 0,
T(x, 0) = sin x, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
Hint: Use results from the last entries of Table 11.1. Also, you may assume the
orthogonality relation
L
0
cos
⎛
⎝

n −1
2

πx
L
⎞
⎠cos
⎛
⎝

m −1
2

πx
L
⎞
⎠dx =
⎧
⎨
⎩
0,
if n ̸= m
L
2,
if n = m
⎫
⎬
⎭.
5. A five meter long copper rod with insulated surface has both its left and right ends
also insulated. Determine the temperature as a function of x and t if the initial con-
dition is given by T(x, 0) = x, 0 < x < 5. Assume the thermal diffusivity of copper
is about 1.15×10−4 m2/s. Hint: See the second group of entries in Table 11.1. Also,
you may assume the orthogonality relation
L
0
cos

nπx
L

cos

mπx
L

dx =
⎧
⎨
⎩
0,
if n ̸= m
L,
if n = m = 0
L
2,
n = m ≥1
⎫
⎬
⎭.
6. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < 5, t > 0,
T(0, t) = 20, ∂T
∂x (5, t) = 0, t > 0,
T(x, 0) = 0, 0 < x < 5
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.

920
Advanced Engineering Mathematics
Hints: First find the equilibrium solution. After that, continue using the method
of Example 11.2, along with results from the third group of entries of Table 11.1.
Also, you may assume the orthogonality relation
L
0
sin
⎛
⎝

n −1
2

πx
L
⎞
⎠sin
⎛
⎝

m −1
2

πx
L
⎞
⎠dx =
⎧
⎨
⎩
0,
if n ̸= m
L
2,
if n = m
⎫
⎬
⎭.
7. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 −1, 0 < x < 2, t > 0,
T(0, t) = ∂T
∂x (2, t) = 0, t > 0,
T(x, 0) = 0, 0 < x < 2
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
Hint: First find the equilibrium solution. After that, continue using the method
of Example 11.2, along with results from the third group of entries of Table 11.1.
Also, you may assume the orthogonality relation
L
0
sin
⎛
⎝

n −1
2

πx
L
⎞
⎠sin
⎛
⎝

m −1
2

πx
L
⎞
⎠dx =
⎧
⎨
⎩
0,
if n ̸= m
L
2,
if n = m
⎫
⎬
⎭.
8. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < 1
2, t > 0,
∂T
∂x (0, t) = 0, T

1
2, t

= 10, t > 0,
T(x, 0) =
⎧
⎨
⎩
1,
0 < x < 1
4
2 −4x
1
4 < x < 1
2
⎫
⎬
⎭
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
9. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < L, t > 0,
T(0, t) = 0, ∂T
∂x (L, t) = 10, t > 0,
T(x, 0) = f(x), 0 < x < L.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
where the graph of the function f is given in Figure 11.3. Assume f(x) is sinusoidal
for L
2 < x < L.

Separation of Variables for PDEs
921
20
f(x)
L
x
L
2
FIGURE 11.3
Initial temperature distribution for Problem 11.1.4.9.
10. Suppose the solution of the heat equation for a thin homogeneous rod is
T(x, t) = 80 + 20 cos
3πx
L

+
+
∞

n=1
5
(2n−1)2 sin
(2n−1)πx
2L

e
−α

(2n−1)π
2L
2
t :
(a) Identify which part(s) of the solution is the equilibrium solution.
(b) Identify which part(s) of the solution is the transient solution.
(c) Find a formula for the initial temperature distribution.
(d) Find boundary conditions satisfied by T(0, t) and ∂T
∂x (L, t).
11. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 + cos(x), 0 < x < π, t > 0,
∂T
∂x (0, t) = ∂T
∂x (π, t) = 0, t > 0,
T(x, 0) = −x + cos(x), 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
12. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = 20, T(π, t) = 70, t > 0,
T(x, 0) = 20 + 40x, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.

922
Advanced Engineering Mathematics
13. Assume α is a positive constant. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t =
α
t+1
∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = T(π, t) = 0, t > 0,
T(x, 0) = f(x), 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
14. Assume α is a positive constant. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = 20, ∂T
∂x (π, t) = 0, t > 0,
T(x, 0) = 0, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
15. Assuming π
L > 1, solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 + e−t, 0 < x < L, t > 0,
T(0, t) = T(L, t) = 0, t > 0,
T(x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
16. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 + g(t), 0 < x < L, t > 0,
∂T
∂x (0, t) = ∂T
∂x (L, t) = 0, t > 0,
T(x, 0) = f(x), 0 < x < L.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
17. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 + g(t), 0 < x < L, t > 0,
T(0, t) = T(L, t) = 0, t > 0,
T(x, 0) = 0, 0 < x < L.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
18. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t =
α
t+1
∂2T
∂x2 −A, 0 < x < π, t > 0,
T(0, t) = 0, T(π, t) = B, t > 0,
T(x, 0) = f(x), 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.

Separation of Variables for PDEs
923
Assume the positive constant α > 1. Your final conclusion should be stated in
terms of the unspecified constants A, B and the unspecified function f.
19. (a) Find all eigenvalues and eigenfunctions of the ODE-BVP
⎧
⎨
⎩
d2X
dx2 + λX = 0, 0 < x < L
X(−L) = X(L) = 0
⎫
⎬
⎭.
This is not the problem with periodic boundary conditions.
(b) Use the results of part (a) to solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2
T(−L, t) = T(L, t) = 0, 0 < t < ∞
T(x, 0) = 100, 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
20. Suppose we are designing heat flow in a thin rod, modeled by the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2
∂T
∂x (0, t) = 10, T(L, t) = 0, 0 < t < ∞
T(x, 0) = 0, 0 < t < ∞
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
Find a formula for g(t) ≜T

L
2, t

, that is, the temperature measured by a sensor
at the middle of the rod. Your formula should be in terms of L and α.
21. Assume β is a constant. Find a condition of the form β < constant that guarantees
that all solutions of the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t =α ∂2T
∂x2 + βT, 0 < x < L, t > 0
T(0, t) = T(L, t) = 0, t > 0
T(x, 0) = f(x), 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
satisfy limt→∞T(x, t) = 0, for all x in [ 0, L ]. In that case, what is the time constant,
in terms of β?
22. Assume β is a constant. Explain why all solutions of the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t =α ∂2T
∂x2 + βT, 0 < x < L, t > 0
∂T
∂x (0, t) = ∂T
∂x (L, t) = 0, t > 0
T(x, 0) = f(x), 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭

924
Advanced Engineering Mathematics
satisfy limt→∞T(x, t) = 0, for all x in [ 0, L ], if and only if β < 0. What is the time
constant, in terms of β?
11.2 Wave Equation in One Space Dimension
In Section 10.4, we studied models for vibrating strings and membranes and for a vibrating
air column. We saw how linear approximations led to the wave equation in one or more
dimensions. We also saw how to get a linear model that predicted the speed of sound.
The PDE-BVP-IVP for the wave equation is
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
(11.21)
u(0, t) = u(L, t) = 0, t > 0,
(11.22)
u(x, 0) = f(x), 0 < x < L,
(11.23)
∂u
∂t (x, 0) = g(x), 0 < x < L.
(11.24)
In this section, we will see how this problem (11.21) through (11.24) is solvable by the
method of separation of variables. As in our study of the heat equation (11.1) in Section
11.1, because of the “homogeneous boundary conditions” u(0, t) = u(L, t) = 0, t > 0, we
try “product solutions” of the PDE in the assumed form u(x, t) = X(x)G(t), where X solves
the ODE-BVP (11.4) in Section 11.1. Because X(x) satisfies the boundary conditions X(0) =
X(L) = 0, such a product solution u(x, t) automatically satisfies the boundary conditions
u(0, t) = u(L, t) = 0. Let X = Xn(x) = sin
 nπx
L
	
, and substitute u(x, t) = sin
 nπx
L
	
G(t) into
the PDE ∂2u
∂t2 = c2 ∂2u
∂x2 . This yields
sin

nπx
L
 d2Gn
dt2
= −c2 
nπ
L
2
sin

nπx
L

Gn(t), n = 1, 2, . . . ;
hence,
¨Gn + c2 
nπ
L
2
Gn(t) = 0, n = 1, 2, . . . .
This is the famous undamped harmonic oscillator ODE of Section 3.3. The solutions are
Gn(t) = an cos
nπct
L

+ bn sin
nπct
L

, n = 1, 2, . . . ,
where an and bn are arbitrary constants.

Separation of Variables for PDEs
925
The product solutions are
un(x, t)=

an cos
nπct
L

+ bn sin
nπct
L

sin

nπx
L

, n = 1, 2, . . . .
What’s left in solving the PDE-BVP-IVP is the two initial conditions u(x, 0) = f(x) and
∂u
∂t (x, 0) = g(x), 0 < x < L, that is, (11.23) and (11.24). The general method is to use all of
the product solutions by using the infinite series:
u(x, t) =
∞

n=1
un(x, t) =
∞

n=1

an cos

nπct
L

+ bn sin

nπct
L

sin

nπx
L

.
(11.25)
Substitute (11.25) into the first initial condition (11.23), that is, u(x, 0) = f(x), 0 < x < L,
to get
f(x) =
∞

n=1
an sin

nπx
L

.
Fourier analysis of this sine series yields
an = 2
L
L
0
f(x) sin

nπx
L

dx.
Substitute (11.25) into the second initial condition (11.24), that is, ∂u
∂t (x, 0) = g(x), 0 <
x < L. First, take the partial derivative of (11.24) with respect to t,
∂u
∂t (x, t) =
∞

n=1

nπc
L
 
−an sin
nπct
L

+ bn cos
nπct
L

sin

nπx
L

,
and, after that, substitute in t = 0, so the initial condition is satisfied by solving
g(x) = ∂u
∂t (x, 0) =
∞

n=1

nπc
L bn

sin

nπx
L

, 0 < x < L .
This is just a Fourier sine series problem, so for all smooth enough functions g, there is
exactly one solution for the coefficients, implicitly given by

nπc
L
· bn

= 2
L
L
0
g(x) sin

nπx
L

dx .

926
Advanced Engineering Mathematics
The solution of the PDE-BVP-IVP is given by
u(x, t) =
∞

n=1
⎛
⎝2
L
L
0
f(x) sin

nπx
L

dx
⎞
⎠cos
nπct
L

sin

nπx
L

+
∞

n=1
⎛
⎝2
nπc
L
0
g(x) sin

nπx
L

dx
⎞
⎠sin
nπct
L

sin

nπx
L

.
(11.26)
In the special case when the string starts from rest, that is, when ∂u
∂t (x, 0) = 0, 0 < x < L,
we see that bn = 0 for all n; hence, the solution of the PDE-BVP-IVP is
u(x, t) =
∞

n=1
⎛
⎝2
L
L
0
f(x) sin

nπx
L

dx
⎞
⎠cos
nπct
L

sin

nπx
L

.
(11.27)
Example 11.7
A guitar string has mass density of 4.466 × 10−3 kg/m, length 640.0 mm, and is under
tension equal to the weight of a mass of 9.03 kg at sea level. The string is at rest but is
given an initial velocity of sin
 πx
0.640
	
−0.2 sin

2πx
0.640

from its equilibrium position. Find
the displacement of the string.
Method: The tension is found by multiplying 9.03 kg by 9.80665 m/s2, the acceleration
due to gravity at sea level, so T ≈9.03 · 9.80665 N ≈88.554 N. The wave speed is
c ≈
#
T/ϱ =
&
88.554 N
(4.466 × 10−3 kg/m) ≈140.81 m/s.
The physical problem translates to the mathematical problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = 140.812 ∂2u
∂x2 , 0 < x < 0.640, t > 0,
u(0, t) = u(0.640, t) = 0, t > 0,
u(x, 0) = 0, 0 < x < 0.640,
∂u
∂t (x, 0) = sin
 πx
0.640
	
−0.2 sin

2πx
0.640

, 0 < x < 0.640
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
This problem fits directly into the form of problem (11.21) through (11.24), in the special
case of the string starting from the equilibrium position. Note especially that for a guitar
string the BCs are u(0, t) = u(0.640, t) = 0, so L = 0.640 in this problem. Also, we are
given that c = 140.81 m/s. We substitute u(x, 0) ≡0 into the general solution (11.25)
to get
u(x, t) =
∞

n=1
bn sin
 nπct
0.640

sin

 nπx
0.640

.

Separation of Variables for PDEs
927
The initial condition is satisfied by solving
sin

 πx
0.640

−0.2 sin
 2πx
0.640

= ∂u
∂t (x, 0) =
∞

n=1

 nπc
0.640bn

sin

 nπx
0.640

, 0 < x < 0.640.
This is a special case of a Fourier sine series expansion because there are only two Fourier
components in ∂u
∂t (x, 0), specifically the n = 1 and n = 2 modes. So, πcb1
0.640 = 1, 2πcb1
0.640 = 0.2,
and all other bn’s are zero. The solution of the PDE-BVP-IVP gives the displacement of
the string to be
u(x, t) ≈0.00145 sin

 πx
0.640

sin
140.81πt
0.640

−0.000145 sin
 2πx
0.640

sin
281.62πt
0.640

,
to three significant digits. ⃝
Example 11.8
A taut string 4 m long with wave speed c = 60 m/s is initially in the position u(x, 0) =
50(1 −1
5 cos
 πx
2
	
) and is initially at rest. Determine the displacement of the string, as a
function of position and time.
Method: The physical problem translates to a mathematical problem, specifically the
PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = 602 ∂2u
∂x2 , 0 < x < 4, t > 0,
u(0, t) = u(4, t) = 0, t > 0,
u(x, 0) = 50(1 −1
5 cos
 πx
2
	
), 0 < x < 4,
∂u
∂t (x, 0) = 0, 0 < x < 4
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
This problem fits directly into the form of problem (11.21) through (11.24), in the special
case of the string starting from rest. Note especially that the BCs are u(0, t) = u(4, t) = 0,
so L = 4 in this problem. Also, we are given that c = 60 m/s, so the solution of the
PDE-BVP, along with the initial condition on the velocity, is
u(x, t) =
∞

n=1
an sin

nπx
4

cos
60nπt
4

.
The initial condition is satisfied by solving
50

1 −1
5 cos

πx
2

= u(x, 0) =
∞

n=1
an sin

nπx
4

, 0 < x < 4.

928
Advanced Engineering Mathematics
The coefficients in the Fourier sine series are given by
an = 2
4
4
0

50 −10 cos

πx
2

sin

nπx
4

dx
= 1
2
⎛
⎝
4
0
50 sin

nπx
4

dx −
4
0
10 cos

πx
2

sin

nπx
4

dx
⎞
⎠
= 25
'
4 cos
 nπx
4
	
−nπ
(4
0
−5
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
4
0
cos

2πx
4

sin

2πx
4

dx,
n = 2
4
0
cos

2πx
4

sin
 nπx
4
	
dx,
n ̸= 2
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
= 100
1 −(−1)n
nπ

−5
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
4
0
1
2 sin πx dx,
n = 2
4
0
1
2

sin

(n−2)πx
4

+ sin

(n+2)πx
4

dx,
n ̸= 2
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
= · · · = 100
nπ

(−1)n −1
	
−40
π

0,
n = 2
n
n2−4

1 −(−1)n	
,
n ̸= 2

.
To simplify the conclusions, notice that both terms in a2 are zero and further that

1 −
(−1)n	
is zero for all even n. In addition, a little more algebraic simplification for n =
odd = 2k−1 explains why the displacement as a function of position and time is given by
u(x, t) =
∞

k=1
160 ((2k −1)2 −5)
π(2k −1)( (2k −1)2 −4) sin
(2k −1)πx
4

cos

15(2k −1)πt
	
. ⃝
Just as for the heat equation, the 1D wave equation could also have time-dependent
boundary conditions, for example,
u(0, t) = δ(t), u(L, t) = ϵ(t).
Similarly to the method used in Example 11.5 in Section 11.1, we would assume u(x, t) =
v(x, t) + w(x, t), where v would be chosen to satisfy only the boundary conditions, and, in
principle, we would choose v to be in a simple form, for example, v(x, t) ≜β(t) + γ (t)x.
11.2.1 Problems
1. Solve the “plucked” string model
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < 3, t > 0,
u(0, t) = u(3, t) = 0, t > 0,
u(x, 0) =

x,
0 < x < 1
(3 −x)/2,
1 < x < 3

,
∂u
∂t (x, 0) = 0, 0 < x < 3
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.

Separation of Variables for PDEs
929
2. Solve the “plucked” string model
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < 4, t > 0,
u(0, t) = u(4, t) = 0, t > 0,
u(x, 0) =

x,
0 < x < 1
(4 −x)/3,
1 < x < 4

,
∂u
∂t (x, 0) = 0, 0 < x < 4
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
3. Solve, as easily and simply as possible,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
u(0, t) = ∂u
∂x(L, t) = 0, t > 0,
u(x, 0) = sin
 πx
2L
	
−1
3 sin

5πx
2L

, 0 < x < L,
∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
4. Suppose g(x) = 10
n=3 Bn sin nx for some unspecified constants Bn. Find, in as
simple a form as possible, the solution of the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < π, t > 0,
u(0, t) = u(π, t) = 0, t > 0,
u(x, 0) = 0, 0 < x < π
∂u
∂t (x, 0) = g(x), 0 < x < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
Your final conclusion should be in terms of the Bn’s.
In problems 5–8, solve the given PDE-BVP-IVP:
5.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
u(0, t) = u(L, t) = 0, t > 0,
u(x, 0) = 1 −cos
 πx
L
	
, 0 < x < L,
∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭

930
Advanced Engineering Mathematics
6.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < 1, t > 0,
u(0, t) = u(1, t) = 0, t > 0,
u(x, 0) = 4(x −x2), 0 < x < 1,
∂u
∂t (x, 0) = 0, 0 < x < 1
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
7.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
u(0, t) = u(L, t) = 0, t > 0,
u(x, 0) = 0, 0 < x < L,
∂u
∂t (x, 0) = cos

2πx
L

, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
8.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 −u, 0 < x < L, t > 0,
u(0, t) = u(L, t) = 0, t > 0,
u(x, 0) = f(x), 0 < x < L,
∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
9. Solve
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = 4 ∂2u
∂x2 , 0 < x < L, t > 0,
u(0, t) = δ(t), u(L, t) = ϵ(t), t > 0,
u(x, 0) = f(x), 0 < x < L,
∂u
∂t (x, 0) = g(x), 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
in terms of the unspecified functions δ(t), ϵ(t), f(x), and g(x). [Hint: It may be
necessary to use the method of variation of parameters, for solutions of ODEs
in t, or the result of Problem 5.4.1.12, at some point in your work.]
10. Assume h is a positive but unspecified constant. Solve the PDE-BVP-IVP of an
inclined string:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, t > 0,
u(0, t) = 1, u(L, t) = 1 + h, t > 0,
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.

Separation of Variables for PDEs
931
[Hint: First, find the equilibrium solution satisfying the ODE-BVP 0 = v′′, v(0) =
1, v(L) = 1 + h.]
11. Find all solutions of the leaky transmission line modeled by the PDE+BCs
⎧
⎪⎨
⎪⎩
∂2u
∂t2 + 2 ∂u
∂t = c2 ∂2u
∂x2 −u, 0 < x < L, t > 0,
u(0, t) = 0, u(L, t) = 0, t > 0
⎫
⎪⎬
⎪⎭
.
12. A solution of PDE (11.21), BCs (11.22), and IC ∂u
∂t (x, 0) ≡0 is given by
u(x, t) =
∞

n=1
an cos

nπct
L

sin

nπx
L

.
Suppose that the string is further constrained to satisfy u

L
2, t

≡0. What condi-
tion does this impose on the coefficients an? [Problems 11.2.1.12 and 11.2.1.13 are
on page 150 of Pinsky (1998).]
13. A solution of PDE (11.21), BCs (11.22), and IC ∂u
∂t (x, 0) ≡0 is given by
u(x, t) =
∞

n=1
an cos

nπct
L

sin

nπx
L

.
Suppose that the string is further constrained to satisfy u

L
3, t

≡0. What
condition does this impose on the coefficients an?
14. Suppose we model the vibrations of a string forced by the wind by the PDE-BVP
⎧
⎪⎨
⎪⎩
∂2y
∂t2 = c2 ∂2y
∂x2 + cos ωt,
y(0, t) = y(L, t) = 0, 0 < t < ∞
⎫
⎪⎬
⎪⎭
.
For what value(s) of the constant forcing frequency ω can a solution have
max0<x<L |y(x, t)| →∞as t →∞? What phenomenon for forced ODEs is involved
in this situation?
15. Find all eigenvalues and eigenfunctions of the ODE-BVP
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
d4X
dx4 + λX = 0, 0 < x < L,
X(0) = X(L) = 0,
X′′(0) = X′′(L) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
by following the steps as follows:
(a) First, explain why λ = 0 is not an eigenvalue. [Hint: Explain why λ = 0 implies
that X(x) is a third-degree polynomial, and then explain why it can’t satisfy the
four BCs.]

932
Advanced Engineering Mathematics
(b) Second, explain why λ > 0 cannot be an eigenvalue. [Hint: Use a method
similar to that used in Examples 9.29 and 9.30 in Section 9.6. To be more
explicit, multiply through by X(x) the fourth-order ODE that X(x) satisfies,
use two integration by parts, and use the four BCs to get 0 =
 L
0 (X′′(x))2dx +
λ
 L
0 (X(x))2dx.]
(c) Third, find all eigenvalues and eigenfunctions for the case λ < 0. [Hint: Define
ω = (−λ)1/4 for convenience.]
16. Use the results of Problem 11.2.1.15 to solve the PDE-BVP-IVP for the vibrations
of a beam whose ends are “pinned,” modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2y
∂t2 = −EI ∂4y
∂x4
y(0, t) = y(L, t) = 0, 0 < t < ∞
∂2y
∂x2 (0, t) = ∂2y
∂2x(L, t) = 0, 0 < t < ∞,
y(x, 0) = f(x), ∂y
∂t (x, 0) = g(x), 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
where E is the Young’s modulus and I is the moment of inertia.
17. Suppose we model the vibrations of a string forced by the wind by the PDE-BVP
⎧
⎪⎨
⎪⎩
∂2y
∂t2 + 2β ∂y
∂t = c2 ∂2y
∂x2 + cos ωt,
y(0, t) = y(L, t) = 0, 0 < t < ∞
⎫
⎪⎬
⎪⎭
,
where β is a positive constant. What phenomenon analogous to that for forced
ODEs is involved in this situation?
11.3 Laplace Equation in a Rectangle
In Section 10.3, we saw that the equilibrium temperature distribution in a 2- or 3D
region satisfies the Laplace or Poisson equation. In addition, we saw that the electrostatic
potential also satisfies the Laplace or Poisson equation.
Consider the equilibrium solution of the 2D heat equation ∂T
∂t = α∇2T(x, y), that is, the
solutions of ∇2T(x, y) = 0, that is, the functions T(x, y) that satisfy
∂2T
∂x2 + ∂2T
∂y2 = 0.
Shown in Figure 11.4 is a picture of a slab 0 ≤x ≤2, 0 ≤y ≤1, with boundary conditions
indicated on the four sides (1) 0 < x < 2, y = 0, (2) 0 < x < 2, y = 1, (3) 0 < y < 1, x = 0,
(4) 0 < y < 1, x = 2. Assume that the slab’s top and bottom surfaces (in the z-direction) are
insulated.

Separation of Variables for PDEs
933
y
1
T =0°C
T = 50 sin(πy)°C
T =0°C
x
T =0°C
2
FIGURE 11.4
Example 11.9: equilibrium heat problem.
Example 11.9
Solve the mathematical problem to find the equilibrium solution of the heat equation in
the slab that satisfies the boundary conditions given in the picture, that is,
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < 2, 0 < y < 1,
(11.28)
T(x, 0) = T(x, 1) = 0, 0 < x < 2,
(11.29)
T(0, y) = 0, T(2, y) = 50 sin πy, 0 < y < 1.
(11.30)
We will see that we can reuse what we learned about separation of variables and
ODE-BVP eigenvalue problems.
Method: The most important thing to notice first about the problem is that there are two
homogeneous boundary conditions on parallel sides, specifically T(x, 0) = T(x, 1) = 0 on
the horizontal sides with y = 0 and y = 1. This is like Example 11.1 in Section 11.1’s two
homogeneous boundary conditions on parallel sides, specifically T(0, t) = T(L, t) = 0 on
the vertical sides with x = 0 and x = L, which led to an ODE-BVP whose eigenfunctions
were Xn(x) = sin
 nπx
L
	
, n = 1, 2, . . . .
Here, the eigenfunctions are functions of y and L = 1 because the homogeneous
boundary conditions are on the lines y = 0 and y = 1, so the eigenfunctions are
Yn(y) = sin(nπy), n = 1, 2, . . ., and correspondingly we look for solutions of the PDE
in the form T(x, y) = X(x)Yn(y) = X(x) sin(nπy), n = 1, 2, . . . . Such functions T(x, y)
automatically satisfy the first two (homogeneous) boundary conditions of the problem.
Substituting T(x, y) = X(x) sin(nπy) into the PDE gives
0 = ∂2T
∂x2 + ∂2T
∂y2 = d2X
dx2 (x) sin(nπy) + X(x)(−(nπ)2 sin(nπy)), n = 1, 2, . . . ,
which leads to ODE
d2X
dx2 (x) −(nπ)2X(x) = 0,
(11.31)

934
Advanced Engineering Mathematics
whose general solution can be written in the form of either
X(x) = Xn(x) = c1enπx + c2e−nπx
or
X(x) = Xn(x) = d1 cosh(nπx) + d2 sinh(nπx).
We will see that the latter is more convenient than the former.
We now have product solutions Tn(x, y) = (d1 cosh(nπx) + d2 sinh(nπx)) sin(nπy),
where the d1 and d2 in fact can depend on n. Again we use the principle of linear super-
position to “add up” all product solutions to get the general solution of the PDE + (two
homogeneous BCs on parallel sides):
T(x, y) =
∞

n=1
sin(nπy)(an cosh(nπx) + bn sinh(nπx)).
(11.32)
It is this general solution that we substitute into the remaining boundary conditions,
which in this problem play the same role as two initial conditions did in the 1D wave
equation. The third boundary condition is, using the facts cosh(0) = 1 and sinh(0) = 0,
0 = T(0, y) =
∞

n=1
an sin(nπy).
We can either recognize immediately that all an should be zero, or we can go to
the extra work of noting that the Fourier sine series coefficients are given by an =
2
1
 1
0 0 sin(nπy)dy = 0, n = 1, 2, . . . . The fourth boundary condition is, using the facts
that cosh(0) = 1 and sinh(0) = 0,
50 sin πy = T(2, y) =
∞

n=1
bn sinh(2nπ) sin(nπy).
Because the left-hand side is itself a Fourier sine series with L = 1, we don’t need inte-
grals to see that b1 = 50/ sinh(2π) and bn = 0, n = 2, 3, . . . . The final conclusion is that
the equilibrium temperature distribution is given by
T(x, y) = 50 sinh(πx)
sinh(2π) sin(πy). ⃝
What if, instead of using the general solution form (11.32), we had used the general
solution form
T(x, y) =
∞

n=1
sin(nπy)

Anenπx + Bne−nπx	
?
We could still solve the problem, but it would take extra work: The third boundary
condition is
0 = T(0, y) =
∞

n=1
sin(nπy)(An + Bn),

Separation of Variables for PDEs
935
so Fourier sine series analysis implies that for all n ≥1, An + Bn = 0, that is, Bn = −An.
The fourth boundary condition is
50 sin πy = T(2, y) =
∞

n=1
sin(nπy)

Ane2nπ + Bne−2nπ
,
so, for n = 1, 50 = Ane2π + Bne−2π, and, for all n ≥2, 0 = Ane2nπ + Bne−2nπ. Substituting
B1 = −A1 gives 50 = A1

e2π −e−2π	
; hence, A1 = 50/

e2π −e−2π	
.
For n ≥2, Bn = −An gives 0 = An

e2nπ −e−2nπ	
; hence, An = 0/

e2nπ −e−2nπ	
; hence,
An = Bn = 0. Putting this all together, the series solution of the PDE-BVP reduces to a
single term corresponding to n = 1:
T(x, y) = sin πy ·
50
e2π −e−2π ·

eπx −e−πx	
= 50 · sin πy ·
2
e2π −e−2π · eπx −e−πx
2
= 50 · sinh(πx)
sinh(2π) · sin(πy),
just as we found earlier using less work.
In a similar way, we can solve problems with other pairs of homogeneous boundary
conditions on parallel sides by using other lines of Table 11.1.
Example 11.10
Find the electrostatic potential inside the rectangle 0 < x < 2, 0 < y < 1, assuming the
electric field normal to the horizontal sides is zero and the potential is specified on the
vertical sides.
Method: It helps to translate the physical problem into the picture found in Figure 11.5.
The most important thing to notice first about the problem is that there are two
homogeneous boundary conditions on parallel sides. Let V = V(x, y) be the electrostatic
potential. Because the electric field is given by E = ∇V, the boundary conditions are
y
x
1
V = f (y)
V = g(y)
Ey=0
2
Ey=0
FIGURE 11.5
Example 11.10: electrostatic potential.

936
Advanced Engineering Mathematics
∂V
∂y (x, 0) = Ey(x, 0) = 0 and ∂V
∂y (x, 1) = Ey(x, 1) = 0 on the horizontal sides with y = 0
and y = 1, similar to Example 11.9. So, L = 1 in this problem.
The physical problem translates into the mathematical problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2V
∂x2 + ∂2V
∂y2 = 0, 0 < x < 2, 0 < y < 1,
∂V
∂y (x, 0) = ∂V
∂y (x, 1) = 0, 0 < x < 2,
V(0, y) = f(y), V(2, y) = g(y), 0 < y < 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
We should express all of the constants found in the general solution in terms of integrals
involving the functions f(y) and g(y).
This leads to an ODE-BVP whose eigenfunctions are Yn(y) = cos(nπy), n = 0, 1, 2, . . .,
as found in the second group of entries of Table 11.1. Note that λ = 0 is an eigenvalue,
too, and the corresponding eigenfunction, Y0(y) = 1, can be considered an “honorary”
cosine function by taking n = 0 in cos

 nπy
L

. Correspondingly, we look for solutions of
the PDE in the form V(x, y) = X(x)Yn(y) = X(x) cos(nπy), n = 0, 1, 2, . . . . Such functions
of (x, y) automatically satisfy the first two (homogeneous) boundary conditions of the
problem. Substituting V(x, y) = X(x) cos(nπy) into the PDE gives
0 = ∂2V
∂x2 + ∂2V
∂y2 = d2X
dx2 (x) cos(nπy) + X(x)(−(nπ)2 cos(nπy)), n = 0, 1, 2, . . . ,
which leads to the familiar ODE (11.31), that is, X′′(x) −(nπ)2X(x) = 0. For n = 1, 2, . . .,
we get Xn(x) = An cosh(nπx) + Bn sinh(nπx). For n = 0, the solution of X′′(x) −0 = 0 is
X0(x) = A0 + B0x.
The general solution of the PDE and the first two (homogeneous) boundary
conditions is
V(x, y) = a0 + b0x
2
+
∞

n=1
cos(nπy)

an cosh(nπx) + bn sinh(nπx)
	
,
where A0 = a0
2 , B(0) = b0
2 , An = an, Bn = bn, n = 1, 2, . . . . It is this general solution that
we substitute into the remaining boundary conditions. Using the facts cosh(0) = 1 and
sinh(0) = 0, the third boundary condition is
f(y) = V(0, y) = a0
2 +
∞

n=1
an cos(nπy).
This
is
a
standard
Fourier
cosine
series
expansion
problem
with
L = 1,
so
a0 = 2
1
 1
0 f(y) dy and, for n = 1, 2, . . ., an = 2
1
 1
0 f(y) cos(nπy) dy.
The fourth boundary condition is
g(y) = V(2, y) = a0 + 2b0
2
+
∞

n=1
(an cosh(2nπ) + bn sinh(2nπ)) cos(nπy).
This is a standard Fourier cosine series with L = 1, so
a0 + 2b0 = 2
1
1
0
g(y) dy,

Separation of Variables for PDEs
937
and, for n = 1, 2, . . .,
an cosh(2nπ) + bn sinh(2nπ) = 2
1
1
0
g(y) cos(nπy) dy.
It is convenient to denote the coefficients of the Fourier cosine series of g(y) by
g0 = 2
1
1
0
g(y) dy and gn = 2
1
1
0
g(y) cos(nπy) dy, n = 1, 2, . . . ,
(11.33)
and similarly for f(y) by
f0 = 2
1
1
0
f(y) dy and fn = 2
1
1
0
f(y) cos(nπy) dy, n = 1, 2, . . . .
(11.34)
This enables us to solve for the third and fourth boundary conditions in a shorter
notation: V(0, y) = f(y) is equivalent to
a0 = f0,
and
an = fn, n ≥1,
and V(2, y) = g(y) is equivalent to
a0 + 2b0 = g0,
and
an cosh(2nπ) + bn sinh(2nπ) = gn, n ≥1.
Using the value a0 = f0, we find b0 = g0−f0
2
. For n = 1, 2, . . ., using the value an = fn, it
takes a little more work, but we can solve for bn: fn cosh(2nπ) + bn sinh(2nπ) = gn yields
bn sinh(2nπ) = gn −fn cosh(2nπ); hence,
bn = gn −fn cosh(2nπ)
sinh(2nπ)
.
Putting this all together, we know that the solution of the PDE-BVP is
V(x, y) = f0
2 + (g0 −f0)x
4
+
∞

n=1
cos(nπy)

fn cosh(nπx) + gn −fn cosh(2nπ)
sinh(2nπ)
sinh(nπx)

,
(11.35)
where the coefficients fn, gn, n = 0, 1, 2, . . ., are given in (11.33) and (11.34) in terms of
integrals involving the functions f(y), g(y). ⃝
11.3.1 Using Clairvoyance to Choose Alternatives to cosh  nπx
L
 and sinh  nπx
L

Let’s combine all of the terms in (11.35) that have fn as a factor:
fn cosh(nπx) −fn cosh(2nπ)
sinh(2nπ)
sinh(nπx) = fn

cosh(nπx) sinh(2nπ) −cosh(2nπ) sinh(nπx)
	
sinh(2nπ)
=
fn
sinh(2nπ)

sinh(2nπ) cosh(nπx) −cosh(2nπ) sinh(nπx)
	
= sinh(nπ(2 −x))
sinh(2nπ)
fn,
the latter step by using a hyperbolic function identity,
sinh(a −b) = sinh(a) cosh(b) −cosh(a) sinh(b).

938
Advanced Engineering Mathematics
So, we can rewrite the solution (11.35) in a nicer looking form:
V(x, y) = 1
4

f0 · (2 −x) + g0 · x
	
+
∞

n=1
cos(nπy)
sinh(nπ(2 −x))
sinh(2nπ)
fn + sinh(nπx)
sinh(2nπ)gn

,
(11.36)
where the coefficients fn, gn, n = 0, 1, 2, . . . , are given in (11.33) and (11.34) in terms of
integrals involving the functions f(y), g(y).
In (11.36), the final conclusion of the set of functions {sinh(nπ(2−x)), sinh(nπx)} appears
instead of the original basic set of solutions {cosh(nπx), sinh(nπx)} for the ODE X′′(x) −
(nπ)2X(x) = 0. But, if we retrace our steps, we see that we got sinh(nπ(2 −x)) from a
linear combination of the functions cosh(nπx), sinh(nπx). In Chapter 3, we saw that we
can have many basic sets of solutions to the same ODE. In fact, if we choose any two of the
four functions cosh(nπx), sinh(nπx), cosh(nπ(2 −x)), sinh(nπ(2 −x)), we will get a basic
set of solutions of X′′(x)−(nπ)2X(x) = 0. So, in retrospect, it would have been better to use
V(x, y) = a0 + b0x
2
+
∞

n=1
cos(nπy)(cn sinh(nπ(2 −x)) + dn sinh(nπx)),
(11.37)
as the general solution of the PDE + (two homogeneous BCs on parallel sides), rather than
V(x, y) = a0 + b0x
2
+
∞

n=1
cos(nπy)(an cosh(nπx) + bn sinh(nπx)).
But, why would we have known that in advance? The clue is found in the third and
fourth boundary conditions in Example 11.10: V(0, y) = f(y), V(2, y) = g(y), 0 < y < 1.
The function h(x) = sinh(nπx) is convenient because h(0) = 0, and the function H(x) =
sinh(nπ(2 −x)) is convenient because H(2) = 0.
For example,
if the third and fourth boundary conditions were of the forms
∂V
∂x (0, y) = (function of y) and V(2, y) = (function of y), then a convenient basic set of solu-
tions of X′′(x) −(nπ)2X(x) = 0 would be {cosh(nπx), sinh(nπ(2 −x))}. Why? The function
h(x) = cosh(nπx) is convenient because dh
dx(0) = 0, and the function H(x) = sinh(nπ(2 −x))
is convenient because H(2) = 0.
Example 11.11
Solve the PDE-BVP
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < 2, 0 < y < 1,
∂T
∂y (x, 0) = x, T(x, 1) = 20, 0 < x < 2
T(0, y) = ∂T
∂x (2, y) = 0, 0 < y < 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Do evaluate all integrals involved in order to reach a final conclusion.
Method: It helps to translate the mathematical symbols into the picture of the problem
given in Figure 11.6.

Separation of Variables for PDEs
939
y
1
T =0°C
T=20°C
2
x
∂T
∂x =0
∂T
∂y = x
FIGURE 11.6
Example 11.11: equilibrium heat problem.
The most important thing to notice first about the problem is that there are two
homogeneous boundary conditions on parallel sides, specifically T(0, y) = ∂T
∂x (2, y) = 0 on
the horizontal sides with x = 0 and x = 2. So, L = 2 in this problem. This leads to an
ODE-BVP whose eigenfunctions are Xn(x) = sin
 
n−1
2

π
2
x

, n = 1, 2, . . ., as found in
the third group of entries of Table 11.1. Correspondingly, we look for solutions of the
PDE in the form T(x, y) = Xn(x)Y(y) = sin
 
n−1
2

π
2
x

Y(y), n = 1, 2, . . . . Such functions
T(x, y) automatically satisfy the third and fourth (homogeneous) boundary conditions
of the problem. Substituting T(x, y) = sin
 
n−1
2

π
2
x

Y(y) into the PDE gives, for
n = 1, 2, 3, . . .,
0 = ∂2T
∂x2 + ∂2T
∂y2 = −
⎛
⎝

n −1
2

π
2
⎞
⎠
2
sin
⎛
⎝

n −1
2

π
2
x
⎞
⎠Y(y) + sin
⎛
⎝

n −1
2

π
2
x
⎞
⎠d2Y
dy2 ,
which leads to ODE d2Y
dy2 (x) −
 
n−1
2

π
2
2
Y(y) = 0. Because the other two boundary
conditions are of the forms ∂T
∂y (x, 0) = (function of x), T(x, 1) = (function of x), it is most
convenient to use the basic set of solutions
)
cosh

(2n−1)π
4
y

, sinh

(2n−1)π
4
(1 −y)
*
.
A convenient general solution of the PDE+(two homogeneous BCs on parallel sides) is
T(x, y) =
∞

n=1
sin

(2n −1)πx
4
 
An cosh

(2n −1)π
4
y

+ Bn sinh

(2n −1)π
4
(1 −y)

.
Before applying the third boundary condition, we calculate that
∂T
∂y (x, y) =
∞

n=1
sin

(2n −1)πx
4

· (2n −1)π
4
·

An sinh

(2n −1)π
4
y

−Bn cosh

(2n −1)π
4
(1 −y)

.

940
Advanced Engineering Mathematics
The third boundary condition is
x = ∂T
∂y (x, 0) =
∞

n=1
sin
(2n −1)πx
4

· (2n −1)π
4
· (−Bn) cosh
(2n −1)π
4

, 0 < x < 2.
This is a generalized Fourier series with L = 2. Orthogonality gives
−
(2n −1)π
4

Bn cosh
(2n −1)π
4

= 2
2
2
0
x sin
(2n −1)πx
4

dx
= · · · =
sin

(2n−1)π
4
· 2


(2n−1)π
4
2
=
(−1)n+1

(2n−1)π
4
2 ;
hence,
Bn =
(−1)n

(2n−1)π
4
3
cosh

(2n−1)π
4
.
The fourth boundary condition gives
20 = T(x, 1) =
∞

n=1
sin
(2n −1)πx
4
 
An cosh
(2n −1)π
4

.
For this generalized Fourier series with L = 2, orthogonality gives
An cosh
(2n −1)π
4

= 2
2
2
0
20 sin
(2n −1)πx
4

dx = · · · =
20

(2n−1)π
4
;
hence,
An =
20

(2n−1)π
4

cosh

(2n−1)π
4
.
So our final conclusion is that the solution of the PDE-BVP is
T(x, y) =
∞

n=1
sin
(2n −1)πx
4

×
20 cosh

(2n−1)π
4
y

+
(−1)n

 (2n−1)π
4
2 sinh

(2n−1)π
4
(1 −y)


(2n−1)π
4

cosh

(2n−1)π
4

. ⃝
11.3.2 Contour Plot and 3D Plot Using Mathematica
For Example 11.11, we entered a partial sum of the first 100 terms in the sum for T(x, y).
The Mathematica command
ContourPlot[T[x, y], {x, 0, 2}, {y, 0, 1}, Contours →{0, 2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20},
ContourStyle →{Thick, Dashed}]

Separation of Variables for PDEs
941
gave the contour plot shown in Figure 11.7. Because T(x, y) is the temperature, the
contours, that is, level curves of T, are called isotherms.
After that, the Mathematica command
Plot3D[T[x, y], {x, 0, 2}, {y, 0, 1}]
gave the graph shown in Figure 11.8. Using a mouse to rotate Mathematica’s 3D plot,
we get Figure 11.9, which shows the same graph as for Figure 11.8 but from a different
viewpoint.
1.0
y
0.8
0.6
0.4
0.2
0.0
x
0.0
0.5
1.0
1.5
2.0
FIGURE 11.7
Contour plot for Example 11.11.
20
15
10
5
0
0.0
0.5
1.0
1.5
0.0
0.5
1.0
y
z
x
2.0
FIGURE 11.8
Graph of solution for Example 11.11.

942
Advanced Engineering Mathematics
20
2.0
1.5
1.0
0.5
0.0
15
10
5
00.0
0.5
y
z
x
1.0
FIGURE 11.9
Rotated graph of solution for Example 11.11.
y
2
T =0°C
T= 50 cos(πy)°C
x
π
∂T
∂y = π2
2
∂T
∂y = x
FIGURE 11.10
Example 11.12: equilibrium heat problem.
What about other problems, for example, what if our problem specifies that both the
vertical and horizontal pairs of sides each have at least one nonzero boundary condi-
tion? We can use linear superposition, as illustrated in the two pictures in the following.
Figure 11.10 shows a BVP in which there is no pair of homogeneous boundary conditions
on parallel sides.
Figure 11.11 shows a “sum” of two problems: The solution T(x, y) of the original problem
can be expressed as T(x, y) = T1(x, y) + T2(x, y) where T1(x, y) and T2(x, y) solve the two
problems shown in Figure 11.11.
We must emphasize that this decomposition T = T1 + T2, which effectively decouples
the contributions of the inhomogeneities on the horizontal sides from the inhomogeneity
on the vertical sides, is possible because everything in the problem is linear.

Separation of Variables for PDEs
943
∂T2
∂y =
=0
T1
T2
y
2
y
2
T1=0°C
T2=0°C
T2= 0°C
T1= 50 cos(πy)°C
x
π
x
π
+
∂T1
∂y
=0
∂T1
∂y
π2
2
= x
∂T2
∂y
FIGURE 11.11
Example 11.12: sum of equilibrium heat problems.
Example 11.12
Solve the PDE-BVP
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < π, 0 < y < 2,
∂T
∂y (x, 0) = x, ∂T
∂y (x, 2) = π2
2 , 0 < x < π
T(0, y) = 0, T(π, y) = 50 cos(πy), 0 < y < 2
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
Do evaluate all integrals involved in order to reach a final conclusion.
Method: Figure 11.11 indicates that our solution is T(x, y) = T1(x, y) + T2(x, y), where
T1(x, y) solves
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2T1
∂x2 + ∂2T1
∂y2 = 0, 0 < x < π, 0 < y < 2,
∂T1
∂y (x, 0) = ∂T1
∂y (x, 2) = 0, 0 < x < π,
T1(0, y) = 0, T1(π, y) = 50 cos(πy), 0 < y < 2
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭

944
Advanced Engineering Mathematics
and T2(x, y) solves
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2T2
∂x2 + ∂2T2
∂y2 = 0, 0 < x < π, 0 < y < 2,
∂T2
∂y (x, 0) = x, ∂T2
∂y (x, 2) = π2
2 , 0 < x < π
T2(0, y) = T2(π, y) = 0, 0 < y < 2
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
For the solution for T1(x, y), the two homogeneous boundary conditions on the lines
y = 0 and y = 2 give us L = 2. The general solution of the PDE and the two homogeneous
boundary conditions is most conveniently written as
T1(x, y) = a0 + b0x
2
+
∞

n=1
cos

nπy
2
 
an sinh

nπ
2 (π −x)

+ bn sinh

nπx
2

.
We use the general solution also to satisfy the other two boundary conditions T1(0, y) =
0, T1(π, y) = 50 cos(πy), 0 < y < 2:
0 = T1(0, y) = a0
2 +
∞

n=1
cos

nπy
2

sinh

nπ
2

an;
hence, an = 0 for n ≥0, and
50 cos(πy) = T1(π, y) = a0 + b0π
2
+
∞

n=1
cos

nπy
2

bn sinh

nπ2
2

,
where T is measured in ◦C. Without having to calculate integrals, orthogonality gives
a0 + b0π
2
= 0, and, for n ̸= 2, bn sinh

nπ2
2

= 0,
and, for n = 2,
b2 sinh(π2) = 50.
It follows that bn = 0 for n ̸= 2 and
b2 =
50
sinh(π2).
So,
T1(x, y) = 50 cos(πy) · sinh(πx)
sinh(π2) .
When solving for T2(x, y), the two homogeneous boundary conditions on the lines
x = 0 and x = π give us L = π. The general solution of the PDE and these two
homogeneous boundary conditions is
T2(x, y) =
∞

n=1
sin nx

an cosh(ny) + bn cosh(n(2 −y))
	
;

Separation of Variables for PDEs
945
the last term was chosen so as to more easily satisfy the fourth boundary condition.
We use the general solution also to satisfy the third and fourth boundary conditions
∂T2
∂y (x, 0) = x,
∂T2
∂y (x, 2) = π2
2 , 0 < x < π: first, we differentiate T2 with respect to y to get
∂T2
∂y =
∞

n=1
n sin nx

an sinh(ny) −bn sinh(n(2 −y))
	
.
The third boundary condition is
x = ∂T2
∂y (x, 0) =
∞

n=1
sin nx(−n sinh(2n))bn, 0 < x < π.
It follows that
(−n sinh(2n))bn = 2
π
π
0
x sin nx dx = · · · = 2(−1)n+1
n
;
hence,
bn =
2(−1)n
n sinh(2n).
The fourth boundary condition is
π2
2 = ∂T2
∂y (x, 2) =
∞

n=1
sin nx(n sinh(2n))an, 0 < x < π;
hence,
n sinh(2n)an = 2
π
π
0
π2
2 sin nx dx = · · · = π

1 −(−1)n	
n
,
which can be easily solved for bn. So,
T2(x, y) =
∞

n=1
π
n2 sinh(2n) sin nx

1 −(−1)n	
cosh(ny) + 2(−1)n cosh(n(2 −y))
	
.
The solution to the original problem is
T(x, y) = 50 cos(πy) · sinh(πx)
sinh(π2)
+
∞

n=1
π sin nx
n2 sinh(2n)

1 −(−1)n	
cosh(ny)+2(−1)n cosh(n(2 −y))
	
. ⃝
Learn More About It
Problem 11.3.3.13 leads to the concept of “Green’s function.” Much more about
such functions can be found in Green’s Functions within Partial Differential Equa-
tions:
with Complex Variables and Transform Methods, by Hans Weinberger, Dover
Publications Co. c⃝1995, and Green’s Functions with Applications, by Dean G. Duffy,
Chapman & Hall/CRC Press c⃝2001.

946
Advanced Engineering Mathematics
11.3.3 Problems
1. A thin slab measures 2π meters in the x-direction and π meters in the y-direction.
Assume that the top and bottom surfaces (in the z-direction) are insulated and
the temperature on the four edges satisfies ∂T
∂x (0, y) = 0, T(x, 0) = 0, ∂T
∂x (2π, y) =
10, T(x, π) = 0. Find the equilibrium temperature distribution inside the slab.
2. A thin slab measures 2π meters in the x-direction and π meters in the y-
direction. Assume that the top and bottom surfaces (in the z-direction) are
insulated and the temperature on the four edges satisfies ∂T
∂x (0, y) = 0, T(x, 0) =
100, ∂T
∂x (2π, y) = 0, T(x, π) = 0.
Find the equilibrium temperature distribution
inside the slab.
3. A thin slab measures 2π meters in the x-direction and π meters in the y-direction.
Assume that the top and bottom surfaces (in the z-direction) are insulated and
the temperature on the four edges satisfies T(0, y) = 0, T(x, 0) = 100, ∂T
∂y (x, 0) =
−10, T(2π, y) = 0. Find the equilibrium temperature distribution inside the slab.
[No, there is no typographical error ... this is not one of the usual combination of
boundary conditions on four sides.]
4. Find the equilibrium temperature at the center of a square, three of whose sides
are kept at 0◦C and whose fourth side is kept at 100◦C.
5. Suppose f(x) = 10
n=3 αn sin

(n −1
2)x

for some unspecified constants αn. Find,
in as simple a form as possible, the solution of the problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < π, 0 < y < 1,
T(x, 0) = 20, T(x, 1) = f(x), 0<x<π
T(0, y) = ∂T
∂x (π, y) = 0, 0 < y < 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
6. Solve
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < a, 0 < y < b,
T(0, y) = 0, T(a, y) = 0, 0 < y < b
T(x, 0) = 0, 0 < x < a
T(x, b) =

2x
a ,
0 < x < a
2
2(1 −x
a),
a
2 < x < a

⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
7. Find the equilibrium temperature distribution in an a meters by b meters slab if
the top and bottom surfaces (in the z-direction) are insulated, the edge x = 0 is
kept at temperature 0◦C, and all the other edges are kept at temperature 20◦C.

Separation of Variables for PDEs
947
8. Solve
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < 2, 0 < y < 1,
∂T
∂x (0, y) = 2y, ∂T
∂x (2, y) = 2y, 0 < y < 1
∂T
∂y (x, 0) = x, ∂T
∂y (x, 1) = x, 0 < x < 2
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Check that the solvability condition that we discussed in Section 10.3 is veri-
fied in this problem: We have a Neumann problem for the Laplace equation, so

∂D
∂u
∂nds = 0 should hold.
9. Find the equilibrium temperature distribution T = T(x, y) for the problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t =

∂2T
∂x2 + 3 ∂2T
∂y2

, 0 < x < a, 0 < y < b
∂T
∂x (0, y) = ∂T
∂x (a, y) = 0, 0 < y < b,
T(x, 0) = 0, T(x, b) = a
2 −|x −a
2|, 0 < x < a
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
[Hint: Substitute product solutions T(x, y) = cos
 nπx
a
	
Y(y) into the equilibrium
version of the PDE earlier. Be careful about the resulting role played by the “3”
found in the PDE earlier.]
10. Find the equilibrium temperature distribution T = T(x, y) for the problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t =

2 ∂2T
∂x2 + ∂2T
∂y2

, 0 < x < a, 0 < y < b
∂T
∂x (0, y) = ∂T
∂x (a, y) = 0, 0 < y < b,
T(x, 0) = x, T(x, b) = 0, 0 < x < a
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
[Hint: Substitute product solutions T(x, y) = cos
 nπx
a
	
Y(y) into the equilibrium
version of the PDE earlier. Be careful about the resulting role played by the “2”
found in the PDE earlier.]
11. Find the equilibrium temperature distribution T = T(x, y) for the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t =

2 ∂2T
∂x2 + ∂2T
∂y2

, 0 < x < π, 0 < y < 1
∂T
∂x (0, y) = ∂T
∂x (π, y) = 0, 0 < y < 1,
∂T
∂y (x, 0) = 0, 0 < x < π
T(x, 1) = 1 −1
3 cos 2x + 1
5 cos 5x, 0 < x < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.

948
Advanced Engineering Mathematics
12. Solve
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < a, 0 < y < b,
T(0, y) = 0, T(a, y) = 0, 0 < y < b
T(x, 0) = 0, ∂T
∂y (x, b) + hT(x, b) = 1, 0 < x < a
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
13. For the problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2T
∂x2 + ∂2T
∂y2 = 0, 0 < x < a, 0 < y < b
T(0, y) = T(a, y) = 0, 0 < y < b
T(x, 0) = 0, T(x, b) = f(x), 0 < x < a
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
(a) Find the solution in the form of an infinite series in which there appears
integrals involving the unspecified function f(x).
(b) Do the following manipulations on your conclusion for part (a): Change the
name of the variable of integration to z in those integrals mentioned in part (a),
then move the definite integral symbol to the left of the summation notation,
and then move around the order of multiplication of functions in order to
rewrite your solution for T(x, y) in the form
T(x, y) =
a
0
G(x, y, z)f(z) dz.
As an aside, the function G(x, y, z) is known as a Green’s function for the PDE-
BVP in part (a). The integration formula T(x, y) =
 a
0 G(x, y, z)f(z)dz explains
how the only nonzero boundary temperature values in T(x, b) = f(x) influence
the values of T(x, y) inside the rectangle 0 < x < a, 0 < y < b.
11.4 Eigenvalues of the Laplacian and Applications
An eigenvalue problem for the Laplacian has the form
φ + λφ = 0, in a domain D, with homogeneous conditions on its boundary, ∂D.

Separation of Variables for PDEs
949
As an example, the PDE-BVP eigenvalue problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂2φ
∂x2 + ∂2φ
∂y2 + λφ(x, y) = 0, 0 < x < a, 0 < y < b,
φ(x, 0) = φ(x, b) = 0, 0 < x < a
φ(0, y) = φ(a, y) = 0, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(11.38)
has a nontrivial solution for φ exactly when λ is one of the infinite sets of eigenvalues

λ : λ =

mπ
a
2
+

nπ
b
2
, m = 1, 2 . . . , n = 1, 2, . . .

.
It is beyond the level of this course to explain why these are the only eigenvalues, but
we can learn how people found those eigenvalues. If this were the PDE in Section 11.3
instead of the PDE we are considering now, we would see the two homogeneous boundary
conditions for φ on the parallel sides x = 0 and x = a and decide to look for a solution
of (11.38) in the form φ(x, y) = sin
 mπx
a
	
Y(y). Substituting that into the PDE λφ(x, y) =
−φ(x, y) gives
λ sin

mπx
a

Y(y) = −∂2
∂x2
!
sin

mπx
a

Y(y)
"
−∂2
∂y2
!
sin

mπx
a

Y(y)
"
=

mπ
a
2
sin

mπx
a

Y(y) −sin

mπx
a
 d2Y
dy2 , m = 1, 2 . . . .
After dividing out the factor sin
 mπx
a
	
, this gives the ODE-BVP, for m = 1, 2 . . .,
⎧
⎪⎨
⎪⎩
Y” +

λ −
 mπ
a
	2
Y(y) = 0,
Y(0) = Y(b) = 0
⎫
⎪⎬
⎪⎭
,
which is similar to ODE-BVP (11.4) in Section 11.1 except with

λ −
 mπ
a
	2 	
replacing λ,
Y(y) replacing X(x), and b replacing L. Using the results of Table 11.1, there is a nontrivial
solution for Y(y), namely, Yn(y) = sin
 nπy
b
	
, exactly when
λ −

mπ
a
2
=

nπ
b
2
, n = 1, 2 . . . .
Solving for λ gives
λ = λm,n ≜

mπ
a
2
+

nπ
b
2
, m = 1, 2 . . . , n = 1, 2 . . . ,
as we promised.

950
Advanced Engineering Mathematics
For each m = 1, 2 . . . , n = 1, 2, . . ., corresponding to each eigenvalue
λm,n =

mπ
a
2
+

nπ
b
2
,
we have an eigenfunction
φm,n(x, y) = sin

mπx
a

sin

nπy
b

.
In retrospect, we see that the boundary conditions φ(x, 0) = φ(x, b) = 0, 0 < x < a,
could have told us to try the y-dependence of φ(x, y) as sin
 nπy
b
	
, and the boundary con-
ditions φ(0, y) = φ(a, y) = 0, 0 < y < b, could have told us to try the x-dependence
of φ(x, y) as sin
 mπy
a
	
. So, from the four homogeneous boundary conditions, that is, two
pairs of two homogeneous boundary conditions, we could have jumped to the assump-
tion to substitute φ(x, y) = sin
 mπx
a
	
sin
 nπy
b
	
into the “eigenvalues of the Laplacian” PDE
∂2φ
∂x2 + ∂2φ
∂y2 + λφ(x, y) = 0. If we had done that, we would have also arrived at the formula
for the eigenvalues: λ = λm,n ≜
 mπ
a
	2 +
 nπ
b
	2 , m = 1, 2 . . . , n = 1, 2 . . ..
Of course, we can find eigenvalues and eigenfunctions for the same PDE with different
pairs of homogeneous boundary conditions. For example, the PDE-BVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂2φ
∂x2 + ∂2φ
∂y2 + λφ(x, y) = 0, 0 < x < a, 0 < y < b,
∂φ
∂y (x, 0) = φ(x, b) = 0, 0 < x < a,
∂φ
∂x (0, y) = ∂φ
∂x (a, y) = 0, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
has eigenvalues
λm,n =

mπ
a
2
+
⎛
⎝

n −1
2

π
b
⎞
⎠
2
, m = 0, 1, 2 . . . , n = 1, 2, . . . ,
and corresponding eigenfunctions
φm,n(x, y) = cos

mπx
a

cos
⎛
⎝

n −1
2

πy
b
⎞
⎠, m = 0, 1, 2 . . . , n = 1, 2.
To get these results, use Table 11.1.
We can use these eigenvalues and eigenfunctions of the Laplacian in many ways.

Separation of Variables for PDEs
951
11.4.1 Application to Time-Dependent Heat Flow in a Slab
Example 11.13
Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α

∂2T
∂x2 + ∂2T
∂y2

, 0 < x < a, 0 < y < b, t > 0,
T(0, y, t) = T(a, y, t) = 0, 0 < y < b, t > 0,
T(x, 0, t) = T(x, b, t) = 0, 0 < x < a, t > 0,
T(x, y, 0) = f(x, y), 0 < x < a, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(11.39)
Method: We look at the four homogeneous boundary conditions on the four sides of the
rectangle and think that we should try to find product solutions of the PDE in the form
T(x, y, t) = G(t)φm,n(x, y) = G(t) sin

mπx
a

sin

nπy
b

, m = 1, 2 . . . , n = 1, 2, . . . .
Noting that φm,n(x, y) = −λm,nφm,n(x, y) and substituting T(x, y, t) into the PDE gives
dG
dt φm,n(x, y) = ∂T
∂t = α

∂2T
∂x2 + ∂2T
∂y2

= α G(t)

−λm,nφm,n(x, y)
	
,
so we want G(t) to satisfy
dG
dt = −α λm,nG(t).
So, similarly to work in Section 11.1, the general solution of the PDE-BVP is
T(x, y, t) =
∞

n=1
m=1
Tm,n(x, y, t) =
∞

n=1
m=1
bm,n sin

mπx
a

sin

nπy
b

e
−α
 mπ
a
	2+

nπ
b
2
t
.
Applying the initial condition gives
f(x, y) = T(x, y, 0) =
∞

n=1
m=1
bm,n sin

mπx
a

sin

nπy
b

, 0 < x < a, 0 < y < b.
This is an example of a double Fourier series. Very much like the orthogonality used to
find the coefficients in a Fourier sine series, we get the orthogonality relation
b
0
a
0
sin

mπx
a

sin

nπy
b

sin
m′πx
a

sin
n′πy
b

dxdy =
0,
if m ̸= m′ or n ̸= n′
ab
4 ,
if m = m′ and n = n′

.
It follows, after a little more thought, that
bm,n = 4
ab
b
0
a
0
f(x, y) sin

mπx
a

sin

nπy
b

dxdy.
(11.40)

952
Advanced Engineering Mathematics
The solution of the PDE-BVP-IVP is
T(x, y, t) =
∞

n=1
m=1
⎛
⎝4
ab
b
0
a
0
f(x, y) sin

mπx
a

sin

nπy
b

dxdy
⎞
⎠
× sin

mπx
a

sin

nπy
b

e
−α
 mπ
a
	2+

nπ
b
2
t
. ⃝
11.4.2 Special Case
As an aside, if f(x, y) = F(x)G(y), that is, the initial temperature distribution function is
itself a product of a function of x times a function of y, then the double integrals (11.40) are
a lot easier to calculate. In Theorem 7.12 in Section 7.3, we noted that the double integral
on a rectangle of a product function can be calculated as a product of integrals, each with
respect to a single variable. In this situation, that is, if f(x, y) = F(x)G(y), then
f(x, y) sin

mπx
a

sin

nπy
b

=

F(x) sin

mπx
a
 
G(y) sin

nπy
b

;
hence,
bm,n = 4
ab
⎛
⎝
a
0
F(x) sin

mπx
a

dx
⎞
⎠
⎛
⎝
b
0
G(y) sin

nπy
b

dy
⎞
⎠.
11.4.3 Application to Transverse Vibrations of a Rectangular Membrane
Example 11.14
Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2

∂2u
∂x2 + ∂2u
∂y2

, 0 < x < a, 0 < y < b, t > 0,
u(0, y, t) = u(a, y, t) = 0, 0 < y < b, t > 0,
u(x, 0, t) = u(x, b, t) = 0, 0 < x < a, t > 0,
u(x, y, 0) = f(x, y), 0 < x < a, 0 < y < b
∂u
∂t (x, y, 0) = g(x, y), 0 < x < a, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(11.41)
Method: This is like Example 11.13 combined with Example 11.7 in Section 11.2, that is,
the 1D wave equation, that is, (11.21) through (11.24). Use eigenfunctions φmn,n(x, y) ≜
sin
 mπx
a
	
sin

 nπy
b

and substitute u(x, y, t) = Gm,n(t)φmn,n(x, y) into the PDE to get
¨Gm,n(t)φm,n(x, y) = c2Gm,n(t)φmn,n(x, y) = −c2 λm,n Gm,n(t) φm,n(x, y).

Separation of Variables for PDEs
953
So, the PDE-BVP has the general solution
u(x, y, t) =

n=1
m=1
um,n(x, y, t)
=
∞

n=1
m=1

am,n cos
+
mπ
a
2
+

nπ
b
2
ct

+ bm,n sin
+
mπ
a
2
+

nπ
b
2
ct

× sin

mπx
a

sin

nπy
b

.
The two initial conditions give
am,n = 4
ab
b
0
a
0
f(x, y) sin

mπx
a

sin

nπy
b

dxdy
and
bm,n =
4
c ab
, mπ
a
	2 +
 nπ
b
	2
b
0
a
0
g(x, y) sin

mπx
a

sin

nπy
b

dxdy. ⃝
Of course, the method can be applied to the vibrations of a rectangular plate with other
combinations of homogeneous boundary conditions on the four sides.
11.4.4 Application to Steady-State Temperature in a Slab with a Steady Source or Sink
Example 11.15
Solve the PDE-BVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
0 = κ

∂2T
∂x2 + ∂2T
∂y2

+ Q(x, y), 0 < x < a, 0 < y < b
T(0, y) = T(a, y) = 0, 0 < y < b,
T(x, 0) = T(x, b) = 0, 0 < x < a
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
(11.42)
Method: If we assume T(x, y) has the form of the double Fourier series
T(x, y) =
∞

n=1
m=1
bm,n sin

mπx
a

sin

nπy
b

,
then it is guaranteed to satisfy the four homogeneous boundary conditions. Substituting
T(x, y) into the PDE gives
0 = Q(x, y) + κ
∞

n=1
m=1
bm,n

−λm,n
	
sin

mπx
a

sin

nπy
b

,

954
Advanced Engineering Mathematics
that is,
−Q(x, y)
κ
=
∞

n=1
m=1
bm,n

−λm,n
	
sin

mπx
a

sin

nπy
b

,
which is just a double Fourier series problem. So, we find that
bm,n =
4
κab

 mπ
a
	2 +
 nπ
b
	2
b
0
a
0
Q(x, y) sin

mπx
a

sin

nπy
b

dxdy.
The solution for the equilibrium temperature is
T(x, y) =
∞

n=1
m=1
⎛
⎝
4
κab

 mπ
a
	2 +
 nπ
b
	2
b
0
a
0
Q(x, y) sin

mπx
a

sin

nπy
b

dxdy
⎞
⎠
× sin

mπx
a

sin

nπy
b

. ⃝
By the way, a PDE of the form
∂2T
∂x2 + ∂2T
∂y2 = p(x, y)
is called a 2D Poisson equation. It came up in the study of problems with source terms,
for example, the previously mentioned problems of finding an equilibrium temperature
distribution in Section 10.2, but also in electro- or magnetostatics.
11.4.5 Application to Surface Waves
Consider fluid in a 3D rectangular basin whose base is 0 ≤x ≤a, 0 ≤y ≤b, z = −d. The
surface of the fluid is given by z = η(x, y, t). If we assume irrotational potential flow, then
the potential function  = (x, y, z, t) satisfies the PDE ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 = 0 in the spatial
region 0 < x < a, 0 < y < b, −d < z < η(x, y, t), along with the boundary conditions that
the derivative of  in the direction normal to the spatial region is zero on the bottom and
on the sides of the basin, that is, on the planes x = 0, x = a, y = 0, y = b, and z = −d. These
boundary conditions are, for t > 0,
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂
∂x (0, y, z, t) = ∂
∂x (a, y, z, t) = 0, 0 < y < b, −d < z < η(x, y, t),
∂
∂y (x, 0, z, t) = ∂
∂y (x, b, z, t) = 0, 0 < x < a, −d < z < η(x, y, t), and
∂
∂z (x, y, −d, t) = 0, 0 < x < a, 0 < y < b.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
In addition, the potential is specified on the free surface z = η(x, y, t), that is,
(x, y, η(x, y, t)) = p(x, y, t), 0 < x < a, 0 < y < b.

Separation of Variables for PDEs
955
Because of the four homogeneous boundary conditions on the sides of the basin and the
homogeneous boundary condition on the bottom of the basin, the solution for  should
be in the “clairvoyant” form:
(x, y, z, t) =
∞

n=0
m=0
m+n>0
bm,n(t) cos

mπx
a

cos

nπy
b

cosh(
#
λm,n (z + d)),
(11.43)
where the eigenvalues of the Laplacian with the four homogeneous boundary conditions
are given by
λmn =

mπ
a
2
+

nπ
b
2
, m = 0, 1, 2, . . . , n = 0, 1, 2, . . . , with m + n > 0.
We arbitrarily assume that the average value of the potential  is zero, that is, that b0,0 = 0;
the value of b0,0 does not affect the fluid velocity given by u ˆi+v ˆj+w ˆk = ∇. So, we only
need the eigenvalues for m + n > 0.
Now, we will not pretend to solve the free surface boundary value problem, which
involves other, nonlinear PDEs called the “kinematic” condition and the “Bernoulli
equation.” This is because both the potential, u(x, y, t), on the free surface and also the
shape of the free surface itself, z = η(x, y, t), are unknowns.
Nevertheless, we can make some contribution to that work by solving the earlier prob-
lem for (x, y, z, t) in terms of u(x, y, t) for the “nominal” case when the free surface is z = 0,
that is, when η(x, y, t) ≡0. While this solution, denoted 0(x, y, z, t), cannot give an actual
solution for the general case where η is not identically zero, the so-called “perturbation”
methods would use our solution, 0(x, y, z, t), as one ingredient in the “secret sauce” of a
method for the real situation. So, let’s find the solution for 0 in the form of (11.42) that
satisfies the one last boundary condition:
u(x, y, t) = 0(x, y, 0, t) =
∞

n=0
m=0
m+n>0
bm,n(t) cos

mπx
a

cos

nπy
b

cosh(
#
λm,n d),
with b0,0 = 0 assumed, is a double Fourier series. Using the orthogonality relation
b
0
a
0
cos

mπx
a

cos

nπy
b

cos
m′πx
a

cos
n′πy
b

dxdy
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if m ̸= m′ or n ̸= n′
ab,
if m = m = 0′ and n = n = 0′
ab
2 ,
if m = m′ and n = n′, and, m = 0 or n = 0
ab
4 ,
if m = m′ ≥1 and n = n ≥1′
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,

956
Advanced Engineering Mathematics
we see that for m = 0, n ≥1 and for m ≥1, n = 0,
cosh(
#
λm,n d)bm,n(t) = 2
ab
b
0
a
0
cos

mπx
a

cos

nπy
b

u(x, y, t)dxdy,
and for m ≥1 and n ≥1,
cosh(
#
λm,n d)bm,n(t) = 4
ab
b
0
a
0
cos

mπx
a

cos

nπy
b

u(x, y, t) dxdy.
The solution is, for 0 < x < a, 0 < y < b, −d < z < 0, t > 0,
0(x, y, z, t) =
∞

n=1
⎛
⎝2
ab
b
0
a
0
cos

nπy
b

u(x, y, t)dxdy
⎞
⎠cos

nπy
b
 cosh
 nπ
b (z + d)
	
cosh

nπd
b

+
∞

m=1
⎛
⎝2
ab
b
0
a
0
cos

mπx
a

u(x, y, t)dxdy
⎞
⎠cos

mπx
a
 cosh
 mπ
a (z + d)
	
cosh

mπd
a

+
∞

n=1
m=1
⎛
⎝4
ab
b
0
a
0
cos

mπx
a

cos

nπy
b

u(x, y, t)dxdy
⎞
⎠
× cos

mπx
a

cos

nπy
b
 cosh(#λm,n (z + d))
cosh(#λm,n d)
.⃝
11.4.6 Problems
In problems 1 and 2, find the eigenvalues and eigenfunctions of the Laplacian with the
given boundary conditions:
1.
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂2φ
∂x2 + ∂2φ
∂y2 = λφ(x, y), 0 < x < a, 0 < y < b,
φ(x, 0) = φ(x, b) = 0, 0 < x < a
∂φ
∂x (0, y) = ∂φ
∂x (a, y) = 0, 0 < y < b
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
2.
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2φ
∂x2 + ∂2φ
∂y2 = λφ(x, y), 0 < x < a, 0 < y < b,
∂φ
∂y (x, 0) = ∂φ
∂y (x, b) = 0, 0 < x < a
∂φ
∂x (0, y) = ∂φ
∂x (a, y) = 0, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.

Separation of Variables for PDEs
957
3. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α

∂2T
∂x2 + ∂2T
∂y2

, 0 < x < π, 0 < y < 2, t > 0,
T(0, y, t) = ∂T
∂x (π, y, t) = 0, 0 < y < 2, t > 0,
T(x, 0, t) = T(x, 2, t) = 0, 0 < x < π, t > 0,
T(x, y, 0) = sin
 x
2
	
(1 −cos πy), 0 < x < π, 0 < y < 2
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
4. Solve the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 
∂2u
∂x2 + ∂2u
∂y2

, 0 < x < 2, 0 < y < 1, t > 0,
u(0, y, t) = u(2, y, t) = 0, 0 < y < 1, t > 0,
u(x, 0, t) = u(x, 1, t) = 0, 0 < x < 2, t > 0,
u(x, y, 0) = 0, 0 < x < 2, 0 < y < 1
∂u
∂t (x, y, 0) = (1 −|x −1|)y, 0 < x < 2, 0 < y < 1
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
5. Solve
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0 = κ

∂2T
∂x2 + ∂2T
∂y2

−1, 0 < x < a, 0 < y < b
T(0, y) = T(a, y) = 0, 0 < y < b,
T(x, 0) = T(x, b) = 0, 0 < x < a
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
6. Solve
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0 = κ

∂2T
∂x2 + ∂2T
∂y2

−1, 0 < x < a, 0 < y < b
T(0, y) = y, T(a, y) = 0, 0 < y < b,
T(x, 0) = T(x, b) = 0, 0 < x < a
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
[Hint: Let T(x, y) = T1(x, y) + T2(x, y), where T1 solves Problem 11.4.6.5 and T2
solves another problem.]
7. Solve an example of Laplace equation in three dimensions:
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
0 = ∂2T
∂x2 + ∂2T
∂y2 + ∂2T
∂z2 , 0 < x < a, 0 < y < b, 0 < z < c
T(0, y, z) = 0, T(a, y, z) = f(y, z), 0 < y < b, 0 < z < c
T(x, 0, z) = T(x, b, z) = 0, 0 < x < a, 0 < z < c
∂T
∂z (x, y, 0) = ∂T
∂z (x, y, c) = 0, 0 < x < a, 0 < y < b
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.

958
Advanced Engineering Mathematics
8. Suppose T = T(x, y, t) satisfies the heat equation
∂T
∂t = α

∂2T
∂x2 + ∂2T
∂y2

, 0 < x < π, 0 < y < π, t > 0,
the four sides of the square {(x, y) : 0 < x < π, 0 < y < π} are kept at temperature
zero, and T satisfies the initial condition T(x, y, 0) = f(x, y) in the square:
(a) Find the solution T(x, y, t).
(b) Suppose f is symmetric in x and y, that is, f(x, y) ≡f(y, x). Explain why T is
also symmetric in x and y.
9. Solve
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
−1 + 6 cos 4y = ∂2u
∂x2 + ∂2u
∂y2 , 0 < x < π,
0 < y < π
2
u(0, y) = u(π, y) = 0, 0 < y < π
2
u(x, 0) = u(x, π
2 ) = 2, 0 < x < π
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Note that while we could use eigenvalues of the Laplacian, an easier way seems
to be to write the solution in the form u(x, y) = v(y) + w(x, y), where first we find
v(y), the solution of the problem −1 + 6 cos 4y = v′′(y), 0 < y < π
2 , and then let
w(x, y) = u(x, y) −v(y).
11.5 PDEs in Polar Coordinates
Now we will explain how the method of separation of variables can be used in polar
coordinates. First, we recall from Section 6.7 the Laplacian differential operator in polar
coordinates:
u ≜1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 .
(11.44)
We notice that the second term mixes the r and θ variables. So, when using the method of
separation of variables, it may be useful to note that
r2u = r ∂
∂r

r ∂u
∂r

+ ∂2u
∂θ2 ,
(11.45)
because that would help isolate the θ-dependence of u.
11.5.1 Laplace Equation in Polar Coordinates
Example 11.16
Solve u = 0 for the equilibrium temperature, u(r, θ), in the half disk D shown in
Figure 11.12. Assume that the temperature satisfies the boundary conditions as shown
in the picture of D.

Separation of Variables for PDEs
959
y
x
u(a, θ) = f(θ)
a
–a
u=0
u= 0
FIGURE 11.12
Example 11.16: equilibrium heat problem in a half disk.
θ
π
u=0
r
u= f(θ)
u=0
a
FIGURE 11.13
Example 11.16: equilibrium heat problem in polar coordinates.
Method: Solving in D, the PDE
0 = r2u, that is, 0 = r ∂
∂r

r ∂u
∂r

+ ∂2u
∂θ2
(11.46)
seems to have no relationship to the method of separation of variables as we used it
so far in this chapter. But if we rewrite D in polar coordinates, as we did for double
integrals in Section 11.5, the picture becomes clear! Our domain
D = {(r, θ) : 0 ≤r < a, 0 < θ < π}
is actually a rectangle in the rθ-plane, and u satisfies boundary conditions on sides of the
rectangle, as shown in Figure 11.13.
So, just as in Section 11.2, we will use the two homogeneous boundary conditions on
the parallel sides θ = 0 and θ = π to give the form of the product solutions
u(r, θ) = R(r)(θ),
where (θ) satisfies the ODE-BVP eigenvalue problem
⎧
⎨
⎩
′′(θ) + λ(θ) = 0,
(0) = (π) = 0
⎫
⎬
⎭.
The first entry of Table 11.1, with L = π, gives the eigenfunctions and eigenvalues
n(θ) = sin nθ, λn = n2, for n = 1, 2, . . . .

960
Advanced Engineering Mathematics
So we substitute
u(r, θ) = Rn(r) sin nθ
into the PDE (11.46) to get
0 = r ∂
∂r

r ∂
∂rRn(r)

sin nθ + ∂2
∂θ2 [Rn(r) sin nθ]
= sin nθ

r d
dr

r dRn
dr

+ Rn(r) d2
dθ2 sin nθ
= sin nθ

r d
dr

r dRn
dr

−n2Rn(r)

.
So, to have a nontrivial product solution, Rn(r) should satisfy the ODE
0 = r d
dr

r dRn
dr

−n2Rn(r),
that is,
0 = r2 d2Rn
dr2 + r dRn
dr −n2Rn,
(11.47)
for n = 1, 2, . . ., which we recognize as Cauchy–Euler ODEs we studied in Section 3.5.
We substitute R(r) = rm into (11.47) and get the characteristic equation
0 = m(m −1) + m −n2 = m2 −n2,
whose roots are m = ±n and give corresponding solution of (11.47): Rn(r) = c1r−n+c2rn.
The product solutions are of the form
un(r, θ) = (c1r−n + c2rn) sin nθ.
Using linearity, we get the general solution of the PDE plus the two homogeneous
BCs: The general solution of the problem
⎧
⎨
⎩
u(r, θ) = 0, in D = {(r, θ) : 0 ≤r < a, 0 < θ < π}
u(r, 0) = u(r, π) = 0, 0 < r < a
⎫
⎬
⎭
is
u(r, θ) =
∞

n=1
(anr−n + bnrn) sin nθ.
(11.48)
But now, (11.48) stares us in the face with the danger of what happens at r = 0, or,
more properly, what happens as r →0+. Since clearly we don’t want our solution u(r, θ)
to blow up as r →0+, we should require an = 0, for n = 1, 2, . . . . That leaves us with the
general solution
u(r, θ) =
∞

n=1
bnrn sin nθ.
(11.49)

Separation of Variables for PDEs
961
We substitute (11.49) into the remaining BC, u(a, θ) = f(θ), for 0 < θ < π:
f(θ) = u(a, θ) =
∞

n=1
bnan sin nθ =
∞

n=1
Bn sin nθ
(11.50)
where Bn ≜bnan. But, (11.50) is just a standard Fourier sine series expansion problem.
As we learned in Section 9.2,
bnan = Bn = 2
π
π
0
f(θ) sin nθ dθ;
hence,
bn = a−n
⎛
⎝2
π
π
0
f(θ) sin nθ dθ
⎞
⎠.
So the solution to our physical problem is the temperature distribution
u(r, θ) = 2
π
∞

n=1
a−n
⎛
⎝
π
0
f(θ) sin nθ dθ
⎞
⎠rn sin nθ,
that is,
u(r, θ) = 2
π
∞

n=1
⎛
⎝
π
0
f(θ) sin nθ dθ
⎞
⎠

 r
a
n
sin nθ. ⃝
(11.51)
Along the way, we had to deal with the mathematical singularity as r →0+ that was
not in the original problem. The mathematical singularity was an artifact of using polar
coordinates, but we have both mathematical and physical grounds for paying attention
to it. Except for this annoying issue, we have seen once again, just as when doing double
integrals in Chapter 7, how useful polar coordinates can be if appropriate for the geometry
of the domain.
The need to address the issue of the mathematical singularity as r →0+ can be restated
physically: We expect that the temperature, u(r, θ), as well as the flux vector
q = −κ
∂u
∂r ˆer + 1
r
∂u
∂θ ˆeθ

,
should not blow up as r →0+, that is,
| u(0+, θ) | < ∞,
----
∂u
∂r (0+, θ)
---- < ∞, and
---- lim
r→0+
1
r
∂u
∂θ (r, θ)
---- < ∞.
Example 11.17
Solve u = 0 for the equilibrium temperature, u(r, θ), in the disk Da: 0 ≤r < a, depicted
in Figure 11.14.
Method: We depict this problem in the (r, θ) plane in Figure 11.14. Mathematically, this
consists of
(PDE) 0 = r ∂
∂r

r ∂u
∂r

+ ∂2u
∂θ2 , 0 ≤r < a, −π < θ ≤π
(11.52)

962
Advanced Engineering Mathematics
a
r
u = f(θ)
π
–π
θ
FIGURE 11.14
Equilibrium heat problem in a half disk with boundary conditions.
and
(BC) u(a, θ) = f(θ), −π < θ < π.
(11.53)
In addition, the change of variables to (r, θ) introduces two mathematical issues: The
first, as in Example 11.16, is the boundedness of the temperature and the flux vector as
r →0+. The second is that, physically, the ray θ = −π is the same location as the ray
θ = π! It makes sense that the temperature and the flux should be continuous across the
ray θ = ±π, that is,
lim
θ→−π+ u(r, θ) = lim
θ→π−u(r, θ),
and similarly for the flux vector −κ

∂u
∂r ˆer + 1
r
∂u
∂θ ˆeθ

. This can be expressed mathematically
as the additional “continuity” boundary conditions:
u(r, −π+) = u(r, π−) and ∂u
∂θ (r, −π+) = ∂u
∂θ (r, π−).∗
(11.54)
From now on, we will write u(r, −π) instead of u(r, −π+), etc., for convenience.
Let’s solve (11.52) through (11.54) using separation of variables. First, the disk Da is a
rectangle in (r, θ) coordinates. If we substitute a product solution u(r, θ) = R(r)(θ) into
the continuity BCs (11.54), we get
R(r)(−π) = R(r)(π) and R(r)′(−π) = R(r)′(π), 0 < r < a,
that is,
R(r)((π) −(−π)) = R(r)(′(π) −′(−π)) = 0, 0 < r < a.
Since having R(r) ≡0 would be useless for getting product solutions, we get the
periodic boundary conditions
(π) −(−π) = 0
and
′(π) −′(−π) = 0
∗Mathematically, if u(r, θ) is continuously differentiable and u(r, −π+) ≡u(r, π−), then ∂u
∂r (r, −π+) ≡∂u
∂r (r, π−)
follows, because, for example, it is okay to interchange the operations of taking limit as θ →−π+ and the limit
involved in taking the derivative with respect to r.

Separation of Variables for PDEs
963
that we studied in Section 9.3. Indeed, we saw there that the ODE-BVP eigenvalue
problem
⎧
⎨
⎩
′′(θ) + λ(θ) = 0,
(π) = (−π),
′(π) = ′(−π)
⎫
⎬
⎭
has eigenvalues and corresponding eigenfunctions
λ0 = 0 : 0(θ) = 1
λn = n2 : n(θ) : {cos nθ, sin nθ}
for
n = 1, 2, 3, . . . .
Recall from Section 9.3 that the latter means that both cos nθ and sin nθ are eigenfunc-
tions corresponding to the same eigenvalue λn.
Corresponding to eigenvalues λ0 and λn, we get product solutions
u0(r, θ) = R0(r), un,c(r, θ) = Rn,c(r) cos nθ, and un,s(r, θ) = Rn,s(r) sin nθ.
Substituting u0(r, θ) into the PDE (11.52) gives, for n = 1, 2, . . .,
0 = r d
dr

r dR0
dr

+ 0 = r2 d2R0
dr2 + r dR0
dr ,
a Cauchy–Euler ODE. Substituting in R0(r) = rm gives characteristic equation 0 =
m(m −1) + m = m2, so the roots are m = 0, 0. The double root m = 0 gives
R0(r) = c1r0 + c2r0 ln r = c1 + c2 ln r.
Substituting un,c(r, θ) into the PDE (11.52) gives
0 = cos nθ

r d
dr

r dRn,c
dr

−n2Rn,c

= cos nθ

r d2Rn,c
dr2
+ r dRn,c
dr
−n2Rn,c

;
hence, we want
0 = r2 d2Rn,c
dr2
+ r dRn,c
dr
−n2Rn,c,
a Cauchy–Euler ODE. Substituting in Rn,c(r) = rm gives characteristic equation 0 =
m(m −1) + m −n2 = m2 −n2, so the roots are m = ±n. This gives
Rn,c(r) =.c1r−n +.c2rn.
The work for un,s is similar.
Using linearity to include all product solutions, the general solution of the PDE + BCs
(11.52) + (11.54) is
u(r, θ) = a0
2 + c0 ln r
2
+
∞

n=1
(an cos nθ + bn sin nθ)rn
+
∞

n=1
(cn cos nθ + dn sin nθ)r−n.
(11.55)

964
Advanced Engineering Mathematics
At this point, the boundedness of u(0+, θ) yields c0 = 0, cn = 0, dn = 0. The general
solution of (11.52) + (11.54) is
u(r, θ) = a0
2 +
∞

n=1
rn(an cos nθ + bn sin nθ).
We substitute the general solution into the final BC, (11.53), which gives the temperature
on the physical boundary of the disk:
f(θ) = u(a, θ) = a0
2 +
∞

n=1
(anan cos nθ + anbn sin nθ), −π < θ < π.
(11.56)
But (11.56) is just a standard Fourier series expansion problem. As we learned in
Section 9.1,
a0 = 1
π
π
−π
f(θ)dθ, anan = 1
π
π
−π
f(θ) cos nθ dθ, anbn = 1
π
π
−π
f(θ) sin nθ dθ.
So the solution of the problem is
u(r, θ) = 1
2π
π
−π
f(θ)dθ
+ 1
π
∞

n=1

 r
a
n
 π
−π
f(θ) cos nθ dθ

cos nθ +
 π
−π
f(θ) sin nθ dθ

sin nθ

. ⃝
Example 11.18
Solve u = 0 for the equilibrium temperature, u(r, θ), in the annulus Aa,b: a < r < b.
Assume the temperatures on the circles r = a and r = b are given.
Method: Mathematically, this consists of
(PDE)
0 = r ∂
∂r

r ∂u
∂r

+ ∂2u
∂θ2 , a < r < b, −π < θ ≤π,
(11.57)
(BCs) u(a, θ) = f(θ), u(b, θ) = g(θ), −π < θ < π.
(11.58)
As in Example 11.17, continuity of temperature and flux across the ray θ = ±π implies
the additional, periodic boundary conditions (11.54), that is,
u(r, −π+) = u(r, π−)
and
∂u
∂θ (r, −π+) = ∂u
∂θ (r, π−).
At first, the solution proceeds exactly the same as that for Example 11.17, which
we summarize now: The periodic boundary conditions (11.54) imply that the product
solutions are given:
u0(r, θ) = R0(r), un,c(r, θ) = Rn,c(r) cos nθ, and un,s(r, θ) = Rn,s(r) sin nθ.

Separation of Variables for PDEs
965
Substituting them into the PDE of Laplace equation leads to Cauchy–Euler ODEs whose
solutions are the functions R0(r), Rn,c(r), Rn,s(r), and this gives product solutions that
we add up to see that the general solution of (11.52) and (11.54) is (11.55), that is,
u(r, θ)=a0 + c0 ln r
2
+
∞

n=1
(an cos nθ + bn sin nθ)rn+
∞

n=1
(cn cos nθ + dn sin nθ)r−n.
Because r = 0 is not in the domain Aa,b, we do not need to worry about boundedness
of | u(0+, θ) |, so we do not require c0 = 0, cn = 0, dn = 0.
At this point, we could substitute (11.55) into the remaining boundary conditions
(11.58). But, it will make our algebraic work a lot easier if we first use some clairvoy-
ance, as we did in Section 11.3. Instead of writing Rn,c(r) as a linear combination of the
functions r−n and rn, we could write Rn,c(r) as a linear combination of the functions
Hn(r) =

 r
a
n
−

 r
a
−n
, hn(r) =

 r
b
n
−

 r
b
−n
,
for
n = 1, 2, . . . .
Why choose these functions? The functions Hn(r) satisfy Hn(a) = 0, and the functions
hn(r) satisfy hn(b) = 0. And, both Hn(r) and hn(r) are linear combinations of the functions
rn and r−n. Similarly, we can use clairvoyance to rewrite the terms a0+c0 ln r
2
as
C0 ln

b
r

+ A0 ln
 r
a
	
2
.
Why? If we define H0(r) = ln
 r
a
	
and h0(r) = ln
 b
r
	
, then these will be useful in satisfy-
ing BCs at r = a and r = b because H0(a) = 0 and h0(b) = 0. We also see that H0(r) and
h0(r) are linear combinations of 1 and ln r because H0(r) = ln(r)−ln(a) = −ln(a)·1+1·ln r
and similarly h0(r) = ln(b) −ln(r) = ln(b) · 1 −1 · ln r.
So, we take the general solution of (11.57) + (11.54) in the form
u(r, θ) =
C0 ln

b
r

+ A0 ln
 r
a
	
2
+
∞

n=1
(AnHn(r) + Cnhn(r)) cos nθ +
∞

n=1
(BnHn(r) + Enhn(r)) sin nθ,
that is,
u(r, θ) =
C0 ln

b
r

+ A0 ln
 r
a
	
2
+
∞

n=1
Hn(r) (An cos nθ + Bn sin nθ) +
∞

n=1
hn(r) (Cn cos nθ + En sin nθ) ,
(11.59)

966
Advanced Engineering Mathematics
and substitute that into the remaining boundary conditions (11.58):
f(θ) = u(a, θ) =
C0 ln

b
a

+ A0 ln
 a
a
	
2
+
∞

n=1
Hn(a) (An cos nθ + Bn sin nθ) +
∞

n=1
hn(a) (Cn cos nθ + En sin nθ)
=
C0 ln

b
a

2
+
∞

n=1

Cnhn(a) cos nθ + Enhn(a) sin nθ
	
,
because Hn(a) = 0 and ln 1 = 0. This is a standard Fourier series expansion problem
from Section 9.1, so the coefficients satisfy
C0 ln

b
a

= 1
π
π
−π
f(θ)dθ, Cnhn(a)= 1
π
π
−π
f(θ) cos nθ dθ, Enhn(a)= 1
π
π
−π
f(θ) sin nθ dθ .
When we substitute (11.59) into the last boundary condition in (11.58), we get
g(θ) = u(b, θ) =
C0 ln

b
b

+ A0 ln

b
a

2
+
∞

n=1
Hn(b) (An cos nθ + Bn sin nθ) +
∞

n=1
hn(b) (Cn cos nθ + En sin nθ)
= A0 ln( b
a)
2
+
∞

n=1

AnHn(b) cos nθ + BnHn(b) sin nθ
	
.
This is a standard Fourier series expansion problem from Section 9.1, so the coefficients
satisfy
A0 ln
b
a

= 1
π
π
−π
g(θ)dθ, AnHn(b) = 1
π
π
−π
g(θ) cos nθ dθ,
BnHn(b) = 1
π
π
−π
g(θ) sin nθ dθ.
The solution of the whole problem is
u(r, θ) =
ln

b
r
 
π
−π
f(θ)dθ

+ ln

 r
a
  π
−π
g(θ)dθ

2π ln

b
a

+ 1
π
∞

n=1
Hn(r)
Hn(b)
 π
−π
g(θ) cos nθdθ

cos nθ +
 π
−π
g(θ) sin nθdθ

sin nθ

+ 1
π
∞

n=1
hn(r)
hn(a)
 π
−π
f(θ) cos nθdθ

cos nθ +
 π
−π
f(θ) sin nθdθ

sin nθ

. ⃝

Separation of Variables for PDEs
967
11.5.2 Heat Equation in Polar Coordinates
Example 11.19
Suppose the temperature, T, in a disk Da : 0 ≤r < a satisfies the heat equation ∂T
∂t =
T, the temperature is 0 on the circle that bounds the disk, and the initial temperature
distribution, T
--
t=0 = f(r), is circularly symmetric. Find the temperature distribution as a
function of space and time.
Method: Mathematically, this is the PDE-BVP-IVP
∂T
∂t (r, θ, t) = α

1
r
∂
∂r

r ∂T
∂r

+ 1
r2
∂2T
∂θ2

, 0 ≤r < a, −π < θ ≤π,
(11.60)
T(a, θ, t) = 0, −π < θ < π, 0 < t < ∞,
(11.61)
T(r, θ, 0) = f(r), 0 < r < a, −π < θ ≤π.
(11.62)
Because of the circular symmetry of both the boundary condition and the initial condi-
tion, it makes sense to hope that the solution does not depend on θ, that is, T = T(r, t),
where (r, t) lies in the infinite slab, 0 < r < a, 0 < t < ∞. The PDE becomes just
∂T
∂t (r, t) = α 1
r
∂
∂r

r ∂T
∂r

.
(11.63)
The closest things we have to two homogeneous boundary conditions on parallel sides
are the physical BC T(a, t) = 0 and the mathematical/physical BC | T(0+, t) | < ∞. So,
we will try product solutions T(r, t) = R(r)G(t), where R(r) solves the ODE-BVP
⎧
⎪⎪⎨
⎪⎪⎩
1
r
d
dr

r dR
dr

+ λR(r) = 0,
| R(0+) | < ∞, R(a) = 0
⎫
⎪⎪⎬
⎪⎪⎭
.
(11.64)
The ODE can be rewritten as
r d
dr

r dR
dr

+ r2λR(r) = 0, 0 < r < a,
that is,
r2R′′(r) + rR′(r) + λr2R(r) = 0, 0 < r < a,
(11.65)
which is a form of Bessel’s equation of order 0, as we saw in Section 9.6. For λ > 0, the
solutions of (11.65) are given by
R(r) = c1J0(
√
λ r) + c2Y0(
√
λ r), 0 < r < a.
But limr→0+ |Y0(
√
λ r)| = ∞. So, the mathematical/physical BC requires that c2 = 0;
hence, R(r) = c1J0(
√
λ r), 0 < r < a. The second, physical BC gives a characteristic equa-
tion J0(
√
λ a) = 0. This is entirely analogous to the characteristic equation sin(
√
λ L) = 0
that we got for the ODE-BVP (11.4) in Section 11.1, that is,
X′′(x) + λX(x) = 0,
X(0) = X(L) = 0.
When we graph J0(
√
λ a) versus λ in Figure 11.15, we see that there are infinitely many
eigenvalues λn =

 γn,0
a
2
, where γn,0 satisfy J0(γn,0) = 0, for (11.64). (This is analogous

968
Advanced Engineering Mathematics
1.0
π√λ
3
0.8
0.6
0.4
0.2
–0.2
5
J0 ———
10
15
√λ
–0.4
FIGURE 11.15
Finding eigenvalues.
to the eigenvalues λn =
 nπ
L
	2 for the ODE-BVP (11.4) in Section 11.1, found by solving
characteristic equation sin(
√
λ L) = 0.) There are no eigenvalues λ ≤0 for (11.64), as we
explained in Example 9.30 in Section 9.6.1.
The eigenfunctions for the ODE-BVP (11.64) are Rn(r) = J0(γn,0 r/a), corresponding
to eigenvalues λn =

γn,0
a
2
. We substitute our hoped for product solutions u(r, t) =
Rn(r)Gn(t) into the PDE to get
Rn(r)dGn
dt
= α Gn(t) · 1
r
d
dr

r dRn
dr

= α Gn(t) · (−λnRn(r)),
so we need Gn(t) to satisfy the ODE
dGn
dt
= −α λnGn(t).
Solving this gives product solutions
Tn(r, t) = J0

γn,0 r
a

exp

−α

γn,0
a
2
t

,
and linearity allows us to add them up to get the general solution of (11.60) and (11.61)
in the form
T(r, t) =
∞

n=1
anJ0

γn,0 r
a

exp

−α

γn,0
a
2
t

.
We substitute this into the IC, (11.62), to get
f(r) = T(r, 0) =
∞

n=1
anJ0

γn,0 r
a

.
(11.66)
To analyze (11.66), we use the orthogonality relation, as mentioned in (9.71):
a
0
J0

γm,0 r
a

J0

γn,0 r
a

r dr =
 0,
if n ̸= m
Nm,
if n = m

,

Separation of Variables for PDEs
969
where
Nm ≜
a
0

J0

γm,0 r
a
2
rdr = 1
2

J1(γm,0)
	2
(11.67)
does depend on m.
To solve (11.66) for the coefficients an, multiply through by rJ0(γm,0 r/a), and then
integrate with respect to r on the interval 0 < r < a. This gives
a
0
f(r)J0

γm,0 r
a

rdr =
a
0
 ∞

n=1
anJ0

γn,0 r
a

J0

γm,0 r
a

r dr.
We ignore the mathematical issue of justifying the interchange of integration and
summation to get, using (11.67),
a
0
f(r)J0

γm,0 r
a

r dr =
∞

n=1
an
a
0
J0

γm,0 r
a

J0

γn,0 r
a

r dr =
∞

n=1
an
 0,
n ̸= m
Nm,
n = m

= am · 1
2

J1(γm,0)
	2 .
It follows that am = 1
Nm
 a
0 f(r)J0

γm,0 r
a

rdr. So, the solution of the original problem is
T(r, t) =
∞

n=1
a
0
f(r)J0

γn,0 r
a

r dr
1
2

J1(γn,0)
	2
J0

γn,0 r
a

exp

−α

γn,0
a
2
t

. ⃝
Note that T(r, t) →0 as t →∞. This makes sense physically because the equilibrium
temperature distribution is zero.
11.5.3 Problems
1. Solve the problem
⎧
⎪⎪⎨
⎪⎪⎩
1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 = 0, 0 ≤r < a, −π < θ ≤π
u(a, θ) = |θ|, −π < θ < π
⎫
⎪⎪⎬
⎪⎪⎭
,
along with any other boundary condition(s) based on mathematical and/or
physical grounds.
2. Solve Laplace equation for a < r < ∞, that is, in the exterior of the disk 0 ≤r < a.
Assume that the solution u(r, θ) is bounded as r →∞and also satisfies the bound-
ary condition u(a, θ) = 1−2 sin(3θ). Write your final conclusion in as simple a form
as possible.
3. Find the electrostatic potential, V(r, θ), that satisfies Laplace equation in the annu-
lus a < r < b. Assume that the potential is constant on the circles r = a and r = b,
specifically V(a, θ) ≡Va and V(b, θ) ≡Vb, for −π < θ < π.

970
Advanced Engineering Mathematics
4. Solve Laplace equation in the half disk 0 < r < a, 0 < θ < π, shown in Figure 11.12,
assuming instead that the bottom, flat side is insulated and the temperature is
prescribed on the top, half circle side.
5. For the problem
⎧
⎨
⎩
0 = u, 0 < r < a, −π
2 < θ < π
2
u(r, −π
2 ) = u(r, π
2 ) = 0, 0 < r < a
⎫
⎬
⎭,
(a) explain why the general solution for the PDE + (BCs on the rays θ = ± π
2 ) is
given by
u(r, θ) =
∞

k=1

akr2k sin(2kθ) + bkr2k−1 cos(2k −1)θ

,
and (b) find the solution of the problem where the last boundary condition is
u(a, θ) = |θ|, −π
2 < θ < π
2 .
6. Solve
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α 1
r
∂
∂r

r ∂T
∂r

, 0 ≤r < a, 0 < t < ∞
T(a, t) = 0, 0 < t < ∞,
T(r, 0) = J0(γ3,0 r/a) −1
2J0(γ5,0 r/a), 0 < r < a.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
7. Find the equilibrium temperature distribution, T = T(r, z), in a circularly symmet-
ric rod that is insulated everywhere except on the two ends, modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α

1
r
∂
∂r

r ∂T
∂r

+ ∂2T
∂z2

,
0 ≤r < a, 0 < z < L,
∂T
∂r (a, z) = 0, 0 < z < L,
T(r, 0) = f(r), T(r, L) = g(r), 0 < r < a,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
along with any other boundary condition(s) based on mathematical and/or
physical grounds. [Hint: You may assume that d
dx
/
J0(x)
0
≡J1(x).]

Separation of Variables for PDEs
971
8. Solve
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0 = 1
r
∂
∂r

r ∂T
∂r

+ ∂2T
∂z2 , 0 ≤r < 2, 0 < z < 3
T(2, z) = 0, 0 < z < 3,
T(r, 0) = J0(γ1,0 r/2) −1
4J0(γ4,0 r/2), 0 < r < 2
T(r, 3) = −1
2J0(γ2,0 r/2), 0 < r < 2
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
along with any other boundary condition(s) based on mathematical and/or
physical grounds.
9. In Section 6.7, we explained why in polar coordinates the vector differential
operators of grad and div are given by, respectively,
∇u = ∂u
∂r ˆer + 1
r
∂u
∂θ ˆeθ
and
∇• (Fr ˆer + Fθ ˆeθ) = 1
r
∂
∂r
/
rFr
0
+ 1
r
∂
∂θ
/
Fθ
0
.
In Section 10.2, we learned that the equations
0 = ∇• q and q = −κ∇T
model the equilibrium temperature, T, where q is the heat flux vector:
(a) In the special case that the physical domain is the disk Da : 0 ≤r < a and
the thermal conductivity is given by κ = cϱr and c and ϱ are constants, write
down the single second-order PDE that T should satisfy.
(b) Find the equilibrium temperature that satisfies the PDE you wrote in part
(a), along with the mathematical boundary conditions appropriate for a disk
and the physical boundary condition prescribing the temperature on the
boundary, that is, T(a, θ) = f(θ), −π < θ < π.
10. Find the equilibrium temperature distribution in the quarter sector of an annulus
given by a ≤r ≤b, 0 ≤θ ≤π
2 . Assume that the sides θ = 0 and θ = π
2 are kept
insulated, that is, have zero flux in the normal direction, and that the temperature
is prescribed on the inner and outer quarter circles r = a and r = b, for 0 ≤θ ≤π
2 .
Assume that there are no sources or sinks inside the quarter sector of the annulus.
11. Solve
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
0 = 1
r
∂
∂r

r ∂T
∂r

+ ∂2T
∂z2 , 0 ≤r < 2, 0 < z < 4,
T(2, z) = 0, 0 < z < 4,
T(r, 0) = 50, T(r, 4) = 0, 0 < r < 2,
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
,

972
Advanced Engineering Mathematics
along with any other boundary condition(s) based on mathematical and/or
physical grounds. [Hint:

J0(x) x dx = x J1(x).]
12. Let the domain D be the three-quarters of a disk given by

(r, θ) : 0 < r < 1, −3π
4 < θ < 3π
4

.
You may assume that the solution of the problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
u(r, θ) = 0, (r, θ) in D
u(r, −3π
4 ) = u(r, 3π
4 ) = 0, 0 < r < 1
u(a, θ) = f(θ), −3π
4 < θ < 3π
4
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
is given by
u(r, θ) =
∞

n=1
anr2n/3 sin

n

2θ
3 + π
2

.
(a) Use orthogonality to find a formula for the generalized Fourier coefficients an.
(b) Rewrite the solution as the sum of the first term and the sum of the terms for
n ≥2, and then find ∂u
∂r in the same type of form.
(c) Find a condition on f(θ) that must be satisfied if we are to have the flux vector
|| ∇u(0+, θ) || < ∞.
13. Solve the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
0 = 1
r
∂u
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 , a < r < b, −π < θ ≤π
u(a, θ) = f(θ), −π < θ < π
b∂u
∂r (b, θ) = a∂u
∂r (a, θ), −π < θ < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
along with any other boundary condition(s) based on mathematical and/or
physical grounds.
14. Let the domain D be the half annulus given by {(r, θ) : a < r < b, 0 < θ < π}. For
the problem

Separation of Variables for PDEs
973
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
u(r, θ) = 0, (r, θ) in D
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
r
∂u
∂θ (r, 0) = f(r)
1
r
∂u
∂θ (r, π) = g(r)
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
, a < r < b
∂u
∂r (a, θ) = ∂u
∂r (b, θ) = 0, 0 < θ < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
(a) What does the solvability condition for this Neumann problem require of the
functions f and g?
(b) Solve the problem. You may find useful the results of Problem 9.6.4.3.
11.6 PDEs in Cylindrical and Spherical Coordinates
First, in two different examples, we will explain how the method of separation of variables
can be used in cylindrical coordinates. We recall from Section 6.7 the Laplacian differential
operator in cylindrical coordinates:
u ≜1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 + ∂2u
∂z2 .
(11.68)
Suppose the domain is the inside of a cylinder,
D = { (r, θ, z) : −π < θ ≤π, 0 ≤r < a, 0 < z < H },
(11.69)
or the region between two co-axial cylinders,
{ (r, θ, z) : −π < θ ≤π, a < r < b, 0 < z < H },
or the region(s) exterior to either of the aforementioned. When we separate variables, we
will separate the θ-dependence from the r and/or z-dependence at some point. This is
because the mathematical boundary conditions of periodicity with respect to θ imply two
separated, homogeneous boundary conditions (see (11.73) in the following) analogous to
those of (11.54) in Section 11.5.
Example 11.20
Find the equilibrium temperature distribution in the cylinder (11.69), assuming the
lateral surface is insulated and the temperature is given on the two circular ends.
Method: Let T be the temperature. Because it is in equilibrium, ∂T
∂t ≡0, so
0 = 1
r
∂
∂r

r ∂T
∂r

+ 1
r2
∂2T
∂θ2 + ∂2T
∂z2 , −π < θ ≤π, 0 ≤r < a, 0 < z < H.
(11.70)

974
Advanced Engineering Mathematics
Insulation of the lateral surface implies flux in the normal direction is zero, that is,
∂T
∂r (a, θ, z) ≡0, −π < θ ≤π, 0 < z < H.
(11.71)
On physical grounds,
| T(0+, θ, z) | < ∞, −π < θ ≤π, 0 < z < H.
(11.72)
Because of periodicity with respect to θ,
T(r, −π+, z) = T(r, π−, z)
and
∂T
∂θ (r, −π+, z) = ∂T
∂θ (r, π−, z), 0 ≤r < a, 0 < z < H.
(11.73)
Finally, we specify the temperatures on the circular ends:
⎧
⎨
⎩
T(r, θ, 0) = f(r, θ)
T(r, θ, H) = g(r, θ)
⎫
⎬
⎭, −π < θ ≤π, 0 ≤r < a.
(11.74)
The physical cylinder is a box (also known as parallelepiped) in (r, θ, z) coordinates,
so (11.73), combined with (11.71) and (11.72), expresses pairs of “homogeneous” bound-
ary conditions on parallel sides of that box. This suggests separating both the r- and
θ-dependence away from the z-dependence. As in the application to surface waves in
Section 11.4, we can do this by finding eigenvalues of the (r, θ) part of the Laplacian
1
r
∂
∂r
!
r ∂φ
∂r
"
+ 1
r2
∂2φ
∂θ2 + λφ = 0,
(11.75)
along with the boundary conditions
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
φ(r, −π+) = φ(r, π−)
and
∂φ
∂θ (r, −π+) = ∂φ
∂θ (r, π−), 0 ≤r < a,
| φ(0+, θ) | < ∞
and
∂φ
∂r (a, θ) ≡0, −π < θ < π.
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(11.76)
The periodicity in θ suggests looking for solutions in the forms φ(r, θ) = 1 · R0(r) and
φ(r, θ) = {cos nθ, sin nθ}R(r), where we recall from Sections 9.3 and 11.5 that the notation
{cos nθ, sin nθ} means both cos nθ and sin nθ, for n = 1, 2, 3, . . . .
First, if we substitute φ(r, θ) = 1 · R0(r) into the PDE (11.75), we get ODE
1
r
d
dr

r dR0
dr

+ λR0 = 0,
that is,
r2R′′
0 + rR′ + λr2R0 = 0,
plus the boundary conditions | R0(0+) | < ∞, dR0
dr (a) = 0. For λ > 0, the ODE is a form of
Bessel’s equation of order zero. Similar to Example 11.19 in Section 11.5, the solutions of
the ODE are R(r) = c1J0(
√
λ r) + c2Y0(
√
λ r), 0 < r < a. When we apply the boundary
condition | R0(0+) | < ∞, we conclude that c2 = 0, because | Y0(0+) | = ∞. When we
apply the last boundary condition by graphing J′
0(
√
λ a) versus λ > 0, we find there are
infinitely many eigenvalues λm,0, m = 1, 2, . . . . In fact, J′
0(x) = −J1(x), so
λm,0 =

γm,1
a
2
,
where γm,1, m = 1, 2, . . ., are the zeros of the function J1(x).

Separation of Variables for PDEs
975
So, we get solutions
φm,0(r, θ) = J0(
#
λm,0 r), m = 1, 2, . . . .
Second, if we substitute φ(r, θ) = cos nθRn(r) into the PDE (11.75), we get
cos nθ 1
r
d
dr

r dRn
dr

+ Rn(r) 1
r2 · (−n2) cos nθ + λ cos nθRn(r) = 0;
hence, Rn(r) should satisfy the ODE
1
r
d
dr

r dRn
dr

−n2 · 1
r2 Rn + λRn = 0,
that is, the ODE
r2R′′
n + rR′
n + (−n2 + λr2)Rn = 0,
plus the boundary conditions | Rn(0+) | < ∞and dRn
dr (a) = 0. For λ > 0, the ODE is
a form of Bessel’s equation of order n. Similar to Example 11.19, the solutions are
Rn(r) = c1Jn(
√
λ r) + c2Yn(
√
λ r), 0 < r < a. When we apply the boundary condition
| Rn(0+) | < ∞, we conclude that c2 = 0, because | Yn(0+) | = ∞. When we apply the
last boundary condition by graphing J′
n(
√
λ a) versus λ > 0, we find there are infinitely
many eigenvalues λm,n, m = 1, 2, . . . .
The work for φ(r, θ) = sin nθRn(r) is the same as for φ(r, θ) = cos nθRn(r), except for
using the sine function instead of the cosine function.
So, we get solutions
φm,n,c(r, θ) = Jn(#λm,n r) cos nθ, m = 1, 2, . . . , n = 1, 2, . . .
φm,n,s(r, θ) = Jn(#λm,n r) sin nθ, m = 1, 2, . . . , n = 1, 2, . . . .
.
When we substitute product solutions φm,0(r, θ)Z(z) into the original PDE, we get
0 = 1
r
∂
∂r

r ∂
∂rφm,0(r, θ)Z(z)

+ 1
r2
∂2
∂θ2
/
φm,0(r, θ)Z(z)
0
+ ∂2
∂z2
/
φm,0(r, θ)Z(z)
0
= · · · = −λm,0 φm,0(r, θ)Z(z) + φm,0(r, θ)Z′′(z);
hence, Z(z) should satisfy the ODE
Z′′(z) −λm,0Z(z) = 0.
Using clairvoyance, as in Section 11.3, we get product solutions
J0(
#
λm,0 r) sinh(
#
λm,0(H −z)) and J0(
#
λm,0 r) sinh(
#
λm,0 z).
When we substitute product solutions φm,n,c(r, θ)Z(z) into the original PDE, we get
0 = 1
r
∂
∂r

r ∂
∂rφm,n,c(r, θ)Z(z)

+ 1
r2
∂2
∂θ2
/
φm,n,c(r, θ)Z(z)
0
+ ∂2
∂z2
/
φm,n,c(r, θ)Z(z)
0
;

976
Advanced Engineering Mathematics
hence,
0 = −λm,n φm,n,c(r, θ)Z(z) + φm,n,c(r, θ)Z′′(z);
(11.77)
hence, Z(z) should satisfy the ODE
Z′′(z) −λm,nZ(z) = 0.
Using clairvoyance, for n = 1, 2, 3, . . ., we get product solutions
Jn(
#
λm,n r) sinh(
#
λm,n(H −z)){cos nθ, sin nθ}
and
Jn(
#
λm,n r) sinh(
#
λm,n z){cos nθ, sin nθ}.
We can put all product solutions together to get the general solution of (11.70) through
(11.73) in the form
T(r, θ, z) =
∞

m=1
J0(
#
λm,0 r)

am,0
2
sinh(
#
λm,0(H −z)) + cm,0
2
sinh(
#
λm,0 z)

+
∞

m=1
∞

n=1
Jn(
#
λm,n r) sinh(
#
λm,n(H −z))

am,n cos nθ + bm,n sin nθ
	
+
∞

m=1
∞

n=1
Jn(
#
λm,n r) sinh(
#
λm,n z)

cm,n cos nθ + dm,n sin nθ
	
.
It is this general solution that we will substitute into the remaining two, nonhomoge-
neous boundary conditions (11.74):
f(r, θ) = T(r, θ, 0) =
∞

m=1
J0(
#
λm,0 r) am,0
2
sinh(
#
λm,0 H)
+
∞

m=1
∞

n=1
Jn(
#
λm,n r) sinh(
#
λm,n H)

am,n cos nθ + bm,n sin nθ
	
.
Using
π
−π
a
0
Jn

#
λm,n r

cos nθ Jn′

,
λn′,m′ r

cos n′θ r dr dθ =
=
 π
−π
cos nθ cos n′θ dθ

·
⎛
⎝
a
0
Jn

#
λm,n r

Jn′

,
λn′,m′ r

r dr
⎞
⎠

Separation of Variables for PDEs
977
and similar results, we get the appropriate orthogonality relation
π
−π
a
0
Jn

#
λm,n r

cos nθ Jn′

,
λn′,m′ r

cos(n′θ)r dr dθ
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if n ̸= n′ or m ̸= m′
2π
a
0

J0

,
λm,0 r
2
r dr,
if n = n′ = 0 and m = m′
π
a
0

Jn

#
λm,n r
2
r dr,
if n = n′ ≥1 and m = m′
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
π
−π
a
0
Jn

#
λm,n r

sin nθ Jn′

,
λn′,m′ r

sin n′θ r dr dθ
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
n ̸= n′ or m ̸= m′
2π
a
0

J0

,
λm,0 r
2
r dr,
n = n′ = 0 and m = m′
π
a
0

Jn

#
λm,n r
2
r dr,
n = n′ ≥1 and m = m′
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
and
π
−π
a
0
Jn

#
λm,n r

cos nθ Jn′

,
λn′,m′ r

sin n′θ rdr dθ = 0, all n ≥0, n′, m, m′ ≥1.
Using the orthogonality relation, the Fourier–Bessel coefficients satisfy
am,0 =
1
sinh(#λm,0 H)
 π
−π
 a
0 f(r, θ)J0(
#
λm,0 r)rdr dθ
2π
 a
0 (J0(
#
λm,0 r))2 r dr
am,n =
1
sinh(#λm,n H)
 π
−π
 a
0 f(r, θ) cos nθJn(
#
λm,n r)r dr dθ
π
 a
0 (Jn(
#
λm,n r))2 r dr
bm,n =
1
sinh(#λm,n H)
 π
−π
 a
0 f(r, θ) sin nθJn(
#
λm,n r)r dr dθ
π
 a
0 (Jn(
#
λm,n r))2 r dr
.

978
Advanced Engineering Mathematics
Finally, substitute the general solution into the last boundary condition
g(r, θ) = T(r, θ, H) =
∞

m=1
J0(
#
λm,0 r)cm,0
2
sinh(
#
λm,0 H)
+
∞

m=1
∞

n=1
Jn(
#
λm,n r) sinh(
#
λm,nH)

cm,n cos nθ + dm,n sin nθ
	
and use orthogonality to get
cm,0 =
1
sinh(#λm,0 H)
 π
−π
 a
0 g(r, θ)J0(
#
λm,0 r)rdr dθ
2π
 a
0 (J0(
#
λm,0 r))2 r dr
cm,n =
1
sinh(#λm,n H)
 π
−π
 a
0 g(r, θ) cos nθJn(
#
λm,n r)r dr dθ
π
 a
0 (Jn(
#
λm,n r))2 r dr
dm,n =
1
sinh(#λm,n H)
 π
−π
 a
0 g(r, θ) sin nθJn(
#
λm,n r)r dr dθ
π
 a
0 (Jn(
#
λm,n r))2 r dr
.
In the interest of chopping down fewer trees to make paper, this time we won’t bother
to substitute the Fourier–Bessel coefficients in to write the final conclusion explicitly. ⃝
Example 11.21
Suppose the electrostatic potential is given on the lateral side of the cylinder (11.69), that
is, u(a, θ, z) = f(θ, z), −π < θ ≤π, 0 < z < H, and the electric field is zero in the direction
normal to the circular ends, that is, ∂u
∂z (r, θ, 0) ≡0, ∂u
∂z (r, θ, H) ≡0, 0 ≤r < a, −π < θ ≤π.
Find the electric field inside of the cylinder.
Method: The potential does not depend on time in electrostatics, so ∂u
∂t ≡0. We recall that
the electric field E = E(r, θ, z) = ∇u, where u is the electric potential. Mathematically,
our problem translates again to PDE (11.70), that is,
0 = 1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 + ∂2u
∂z2 , −π < θ ≤π, 0 ≤r < a, 0 < z < H.
(11.78)
Periodicity with respect to θ requires (11.73), that is,
u(r, −π+, z) = u(r, π−, z)
and
∂u
∂θ (r, −π+, z) = ∂u
∂θ (r, π−, z), 0 ≤r < a, 0 < z < H.
(11.79)
The zero component of the electric field normal to the circular ends requires
∂u
∂z (r, θ, 0) = ∂u
∂z (r, θ, H) = 0, 0 ≤r < a, −π < θ ≤π.
(11.80)

Separation of Variables for PDEs
979
As usual, on physical grounds, we require that
|u(0+, θ, z)| < ∞, −π < θ ≤π, 0 < z < H.
(11.81)
Finally, we specify the potential specified on the lateral surface of the cylinder:
u(a, θ, z) = f(θ, z), −π < θ ≤π, 0 < z < H.
(11.82)
The physical cylinder is a box (also known as parallelepiped) in (r, θ, z) coordinates,
and so the periodicity in θ, as well as the zero normal electric field on the circular ends,
expresses a pair of homogeneous boundary conditions on parallel sides of that box. This
suggests separating both the θ- and z-dependence away from the r-dependence. Unfor-
tunately, the r- and θ-dependence are tied together in the Laplacian operator. While we
could just separate the θ-dependence from the (r, z)-dependence, instead we will try sep-
arating all of the variables at once (Arscott and Darai 1990). So, let’s try our product
solutions in the form
u(r, θ, z) = R(r)(θ)Z(z).
The boundary conditions on u naturally imply boundary conditions on the factors of the
product solutions:
periodicity in θ =⇒(−π+) = (π−), ′(−π+) = ′(π−)
zero normal electric field on circular ends =⇒Z′(0) = Z′(H) = 0.
So, let’s use eigenfunctions for the ODE-BVPs
⎧
⎨
⎩
′′(θ) + λ(θ) = 0,
(π) −(−π) = 0
and
′(π) −′(−π) = 0
⎫
⎬
⎭
and
Z′′ + μZ = 0, Z′(0) = Z′(H) = 0.
This gives product solutions of the form
u00(r, θ, z) = R00(r) · 1 · 1,
unm(r, θ, z)=Rnm(r){cos nθ, sin nθ} cos
 mπz
H
	
, m=0, 1, 2, . . . , n=0, 1, 2, . . . ; n+m> 0.
When we substitute the first of these into the PDE (11.78), we get
0 = 1
r
d
dr

r dR00
dr

+ R00(r) 1
r2 · 0 + R00(r),
that is,
r2R′′
00 + rR′
00 = 0;
hence, R00(r) = c1 + c2 ln r.
When we substitute unm(r, θ, z) = Rnm(r) cos nθ cos
 mπz
H
	
into the PDE, we get
0 = cos nθ cos

mπz
H
 1
r
d
dr

r dRnm
dr

−n2
r2 Rnm cos nθ cos

mπz
H

−

mπ
H
2
cos

mπz
H

cos nθRnm.

980
Advanced Engineering Mathematics
So, for m = 0, 1, 2, . . . , n = 0, 1, 2, . . . , n + m > 0, Rnm should satisfy
1
r
d
dr

r dRnm
dr

−n2
r2 Rnm −

mπ
H
2
Rnm = 0,
that is,
r2R′′
nm + rR′
nm +

−n2 −

mπ
H
2
r2

Rnm = 0,
an ODE which is, for m = 0, a Cauchy–Euler equation whose solutions are Rn0 = c1rn +
c2r−n. For m ≥1, the ODE is a form of the modified Bessel’s equation of order n, whose
solutions are
Rnm = c1In

mπr
H

+ c2Kn

mπr
H

.
Because of the boundary condition | u(0+, θ, z) | < ∞, −π < θ ≤π, 0 < z < H, we must
leave out the terms involving r−n, ln r and Kn
 mπr
H
	
because they blow up as r →0+.
We can put all product solutions together to get the general solution of (11.78) through
(11.81) in the form
u(r, θ, z) = a0,0
4
+
∞

n=1
rn
a0,n
2
cos nθ + b0,n
2
sin nθ

+
∞

m=1
am,0
2 I0

mπr
H

cos

mπz
H

+
∞

m=1
∞

n=1
In

mπr
H

cos

mπz
H
 
am,n cos nθ + bm,n sin nθ
	
.
We substitute this general solution into the remaining boundary condition, (11.82):
f(θ, z) = u(a, θ, z) = a0,0
4
+
∞

n=1
an
a0,n
2
cos nθ + b0,n
2
sin nθ

+
∞

m=1
am,0
2
I0

mπa
H

cos

mπz
H

+
∞

m=1
∞

n=1
In

mπa
H

cos

mπz
H
 
am,n cos nθ + bm,n sin nθ
	
.
The appropriate orthogonality relation involving double integrals is
π
−π
H

0
cos

mπz
H

cos
m′πz
H

cos nθ cos n′θ dz dθ
=
 π
−π
cos nθ cos n′θ dθ

·
⎛
⎝
H

0
cos

mπz
H

cos
m′πz
H

dz
⎞
⎠
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0,
if n ̸= n′ or m ̸= m′
2πH,
if n = n′ = 0 and m = m′ = 0
πH,
if n = n′ = 0 and m = m′ ≥1, or
if m = m′ = 0
and
n = n′ ≥1
πH
2 ,
if n = n′ ≥1
and
m = m′ ≥1
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭

Separation of Variables for PDEs
981
as well as
π
−π
H

0
cos

mπz
H

cos
m′πz
H

sin nθ sin n′θdz dθ
=
⎧
⎪⎪⎨
⎪⎪⎩
0,
n ̸= n′ or m ̸= m′
πH,
m = m′ = 0 and n = n′ ≥1
πH
2 ,
m = m′ ≥1 and n = n′ ≥1
⎫
⎪⎪⎬
⎪⎪⎭
,
and for all m, m′, n ≥0, n′ ≥1,
π
−π
H

0
cos

mπz
H

cos
m′πz
H

cos nθ sin n′θdz dθ = 0.
It follows that
a0,0 =
1
πH
π
−π
H

0
f(θ, z)dz dθ,
a0,n =
1
πHan
π
−π
H

0
cos nθ f(θ, z)dz dθ,
b0,n =
1
πHan
π
−π
H

0
sin nθ f(θ, z)dz dθ,
am,0 =
1
πH I0
 mπa
H
	
π
−π
H

0
cos

mπz
H

f(θ, z)dz dθ,
and for m = 1, 2, . . . ; n = 1, 2, . . . ,
am,n =
1
πH In
 mπa
H
	
π
−π
H

0
cos

mπz
H

cos nθ f(θ, z)dz dθ,
bm,n =
1
πHIn
 mπa
H
	
π
−π
H

0
cos

mπz
H

sin nθ f(θ, z)dz dθ. ⃝
As an aside, it is known that the modified Bessel functions In(x) are positive for all non-
negative integers n and x > 0, so there is no difficulty in dividing by In

mπa
H

.
11.6.1 Spherical Coordinates
Recall from Section 6.7 the formulas for differential operators in spherical coordinates.
Example 11.22
Suppose the electric field in the normal direction is given on the surface of a sphere,
x2 + y2 + z2 = a2, that is, ρ = a,
(11.83)
and has no dependence on the angle theta, that is, the potential u satisfies
∂u
∂ρ (a, φ, θ) = f(φ), 0 < φ < π, −π < θ ≤π.
(11.84)
Find the electric field inside and outside of the sphere.

982
Advanced Engineering Mathematics
Method: We recall that the electric field E = E(r, θ, z) = ∇u, where u is the electric
potential. Given the lack of θ-dependence in boundary condition (11.84), when we look
for a solution u(ρ, φ) of Laplace equation, u should satisfy
1
ρ2
∂
∂ρ

ρ2 ∂u
∂ρ

+
1
ρ2 sin φ
∂
∂φ

sin φ ∂u
∂φ

≡0, 0 <φ <π, either 0 ≤ρ < a or a <ρ <∞,
(11.85)
depending on whether we are solving inside or outside the sphere.
We will see later that the change of variables from rectangular coordinates to spherical
coordinates brings artificial mathematical boundary conditions that
| u(ρ, 0+) | ≜lim
φ→0+ |u(ρ, φ)| < ∞
and
|u(ρ, π−)| < ∞,
0 ≤ρ < a or a < ρ < ∞.
(11.86)
On physical grounds, this makes sense because where there are no sources, the potential
and electric field should be finite. In addition, when we solve the problem inside the
sphere, we will require
| u(0+, φ) | ≜lim
ρ→0+ |u(ρ, φ)| < ∞, 0 < φ < π
on physical grounds; when we solve the problem on the outside of the sphere, we will
require, instead,
lim
ρ→∞|u(ρ, φ)| = 0, 0 < φ < π.
Our product solutions will be easier to find if we separate the ρ- and φ- dependence
as much as possible in (11.85) by multiplying through by ρ2 to get
∂
∂ρ

ρ2 ∂u
∂ρ

+
1
sin φ
∂
∂φ

sin φ ∂u
∂φ

≡0.
(11.87)
Our physical domain in the (ρ, φ)-plane is either the rectangle 0 ≤ρ < a, 0 < φ < π
or the half-infinite strip a < ρ < ∞, 0 < φ < π. In either case, the closest thing we have
to two separated homogeneous boundary conditions on parallel sides is (11.86). So, our
eigenvalue problem is the ODE-BVP
⎧
⎪⎪⎨
⎪⎪⎩
1
sin φ
d
dφ

sin φ d
dφ

+ λ(φ) = 0,
| (0+) | < ∞, |(π−)| < ∞,
⎫
⎪⎪⎬
⎪⎪⎭
.
(11.88)
First, we note that
1
sin φ
d
dφ

sin φ d
dφ

=
1
sin φ
'
cos φ d
dφ + sin φ d2
dφ2
(
= cos φ
sin φ
d
dφ + d2
dφ2 .
We will see that our ODE is a form of Legendre’s equation after making the change of
variables x = cos φ and defining a new function  by (φ) = (cos φ). We have
d
dφ = d
dφ
/
(cos φ)
0
= −sin φ d
dx (cos φ),
and
d2
dφ2 = −cos φ d
dx (cos φ) + (sin φ)2 d2
dx2 (cos φ),

Separation of Variables for PDEs
983
so our ODE is
0 = λ(cos φ) + cos φ
sin φ

−sin φ d
dx (cos φ)

−cos φ d
dx (cos φ) + (sin φ)2 d2
dx2 (cos φ)
= λ(cos φ) −2 cos φ d
dx (cos φ) + sin2(φ)d2
dx2 (cos φ),
that is,
(1 −x2)′′(x) −2x′(x) + λ(x) = 0,
which is Legendre’s equation, was discussed in Section 9.6, and will be discussed in
Appendix C.
In addition to satisfying the ODE, we have boundary conditions for the function  =
(x) corresponding to the BCs for  = (φ): As φ →0+, x = cos φ →1−, and as
φ →π−, x = cos φ →−1+. The ODE-BVP eigenvalue problem is
⎧
⎨
⎩
(1 −x2)′′(x) −2x′(x) + λ(x) = 0,
|(−1+)| < ∞, |(1−)| < ∞.
⎫
⎬
⎭.
(11.89)
We explained in Appendix C why this ODE-BVP eigenvalue problem has a non-
trivial solution for  for eigenvalues λ = n(n + 1), n = 0, 1, 2, . . . . For nonnegative
integers n, the corresponding eigenfunctions, Pn(x), are in fact polynomials in x, called
the Legendre polynomials. During the nineteenth century, there was extensive mathe-
matical research on special ODEs that have polynomial solutions. During the twentieth
and twenty-first centuries, those special polynomial functions were used in numerical
methods for approximating the solutions of very complicated problems.
The Legendre polynomials automatically satisfy the boundary condition requirements
that they have all of the finite limits as in (11.89) because they are polynomials. Now, the
second-order ODE has two basic solutions, not just one, so one would seem to have, for
λ = n(n + 1), solutions
(x) = c1Pn(x) + c2Qn(x).
But the second solution, Qn(x), is not bounded on the interval (−1, 1), so the BCs (11.89)
force c2 = 0.
Corresponding to eigenvalues λ = n(n + 1), we get eigenfunctions n(φ) = n(x) =
Pn(x) = Pn(cos φ). For future reference, note that Pn is a polynomial of degree n; in
particular, P0(x) ≡1. In addition, for n = 1, 3, 5, . . ., Pn has only odd powers of x, and
for n = 0, 2, 4, . . ., Pn has only even powers of x. You’ll use these last facts in a problem
in this section.
By the way, the first four Legendre polynomials are P0(x) ≡1, P1(x) = x, P2(x) =
1
2(3x2 −1), and P3(x) = 1
2(5x3 −3x).
When we substitute a product solution u(ρ, φ) = Rn(ρ)n(φ) into the original PDE,
we get
0 = 1
ρ2
∂
∂ρ

ρ2 ∂
∂ρ Rn(ρ)n(φ)

+
1
ρ2 sin φ
∂
∂φ

sin φ ∂
∂φ Rn(ρ)n(φ)

= n(φ)
 1
ρ2
d
dρ

ρ2 d
dρ Rn

−n(n + 1)Rn

.

984
Advanced Engineering Mathematics
Omitting the n(φ) factor and multiplying through by ρ2 explain why Rn satisfies the
Cauchy–Euler ODE ρ2R′′
n + 2ρR′
n −n(n + 1)Rn = 0. When n = 0, the solutions are
R0(ρ) = c1 + c2ρ−1.
For n ≥1, the solutions are
Rn(ρ) = c1ρn + c2ρ−(n+1).
For a solution inside the sphere, that is, for 0 < ρ < a, boundedness as ρ →0+
implies we must leave out the terms involving ρ−1, ρ−(n+1); for solutions outside the
sphere, decay to zero as ρ →∞implies we must leave out the terms involving ρn.
Put together all the product solutions Rn(ρ)n(φ) and recall that P0(x) ≡1, so the
general solution for the potential inside the sphere is
u(ρ, φ) = a0 +
∞

n=1
anρnn(φ)
and the general solution for the potential outside the sphere is
u(ρ, φ) = A0 +
∞

n=1
Anρ−(n+1)n(φ).
In order for the solution inside the sphere to match the boundary condition (11.84) that
the electric field in the normal direction, that is, ∂u
∂ρ , is given on the surface of the sphere,
take the partial derivative of the solution with respect to ρ. The constants an, n = 0, 1, . . .,
should be chosen to satisfy
f(φ) = ∂u
∂ρ (a, φ) =
∞

n=1
nanan−1n(φ), 0 < φ < π.
The appropriate orthogonality relation is
π
0
n(φ)n′(φ) sin φ dφ =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
0,
if n ̸= n′
2,
if n = n′ = 0
π
0
(n(φ))2 sin φ dφ,
if n = n′ ≥1
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
So, we calculate that for n = 1, 2, . . .,
an =
 π
0 n(φ)f(φ) sin φ dφ
nan−1  π
0 (n(φ))2 sin φ dφ
,
along with the solvability condition that
0 =
π
0
0(φ)f(φ) sin φ dφ =
π
0
f(φ) sin φ dφ.
In order for the solution outside the sphere to match the boundary condition (11.84)
that the electric field in the normal direction, that is, ∂u
∂ρ , is given on the surface of the

Separation of Variables for PDEs
985
sphere, take the partial derivative of the solution with respect to ρ. The constants An, n =
0, 1, . . ., should be chosen to satisfy
f(φ) = ∂u
∂ρ (a, φ) =
∞

n=1
An(−(n + 1)a−n−2)n(φ), 0 < φ < π.
So, we calculate that for n = 1, 2, . . .,
An = −
 π
0 n(φ)f(φ) sin φ dφ
(n + 1)a−n−2  π
0 (n(φ))2 sin φ dφ
,
again, along with the solvability condition that
0 =
π
0
0(φ)f(φ) sin φ dφ =
π
0
f(φ) sin φ dφ.
The solution for the electric field in the normal direction inside, on, and outside the
sphere is
Eρ(ρ, φ)
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∞

n=1

 π
0 n(φ)f(φ) sin φ dφ
nan−1  π
0 (n(φ))2 sin φ dφ

ρnn(φ),
0<ρ < a, 0<φ < π
f(φ),
ρ = a, 0 < φ < π
∞

n=1

−
 π
0 n(φ)f(φ) sin φ dφ
(n+1)a−n−2  π
0 (n(φ))2 sin φ dφ

ρ−(n+1)n(φ),
a <ρ <∞, 0<φ < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
The solvability condition that we discussed in Section 10.3 applies to Example 11.22
because we have a Neumann problem for the Laplace equation: it is specifically in the
form u = 0 in D,
∂u
∂n = 0 on ∂D.
11.6.2 Polar Coordinates Again
Now that we saw how to use Bessel functions to solve problems in cylindrical coordinates
that involve r, θ, and a third independent variable, we can also solve many problems
in polar coordinates that involve r, θ, and t. We will explore this later in some of the
problems.

986
Advanced Engineering Mathematics
11.6.3 Problems
1. Find the equilibrium temperature distribution in the half cylinder 0 ≤θ ≤π, 0 ≤
r ≤a, 0 < z < H modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0 = 1
r
∂
∂r

r ∂T
∂r

+ 1
r2
∂2T
∂θ2 + ∂2T
∂z2 , 0 < θ < π, 0 ≤r < a, 0 < z < H
T(a, θ, z) = 0, 0 < θ < π, 0 < z < H
T(r, 0, z) = T(r, π, z) = 0, 0 ≤r < a, 0 < z < H
T(r, θ, 0) = f(r, θ), T(r, θ, H) = 0, 0 < θ < π, 0 ≤r < a
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
Assume also any other boundary condition(s) needed in a cylindrical coordinate
system based on mathematical and/or physical grounds.
2. Find the equilibrium temperature distribution in the cylinder (11.69) assuming the
lateral surface is insulated and the temperature is given on the two circular ends.
Assume also that there is heat generation within the cylinder proportional to the
temperature, modeled by the PDE
0 = 1
r
∂
∂r

r ∂T
∂r

+ 1
r2
∂2T
∂θ2 + ∂2T
∂z2 + ηT, −π < θ ≤π, 0 ≤r < a, 0 < z < H,
where η is an unspecified positive constant of proportionality satisfying η < γ1,1
a ,
where γm,1 are the zeros of the Bessel function J1(x). Assume also any other bound-
ary condition(s) needed in a cylindrical coordinate system based on mathematical
and/or physical grounds. [This problem might be a model for the temperature in
a nuclear fuel rod if it is laterally insulated.]
You may assume that all of the zeros, j′
m,n, of the functions J′
n(x) satisfy∗j′
m,n >
γ1,1, for n = 1, 2, . . ., m = 1, 2, . . ..
3. Find the temperature distribution, T = T(r, θ, t), in an annulus whose boundary
circles are kept at 0◦C. temperature, modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α

1
r
∂
∂r

r ∂T
∂r

+ 1
r2
∂2T
∂θ2

, a < r < b, −π < θ ≤π, 0 < t < ∞
T(a, θ, t) = T(b, θ, t) = 0, −π < θ < π, 0 < t < ∞
T(r, θ, 0) = f(r, θ), a < r < b, −π < θ < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
Assume also any other boundary condition(s) needed in a polar coordinate system
based on mathematical and/or physical grounds. [Hint: Theorem 9.17 in Section
9.6 applies to the regular Sturm–Liouville problem involving the ODE r(rR′)′ +

μr2 −n2	
R(r) = 0 on the interval a < r < b.]
∗By Rayleigh’s argument that j′m,n is increasing in n because of the behavior of frequencies of vibration of circular
membranes, as mentioned in Section 15.61 of Watson (1992).

Separation of Variables for PDEs
987
4. Suppose the temperature is given on the lateral side of the cylinder (11.69), that is,
T(a, θ, z) = f(θ, z), −π < θ ≤π, 0 < z < H, and the circular ends are insulated;
hence, ∂T
∂z (r, θ, 0) = ∂T
∂z (r, θ, H) = 0, 0 ≤r < a, −π < θ ≤π. Assume also that
the thermal conductivity in the angular direction is different from that in the other
directions, modeled by the PDE
0 = 1
r
∂
∂r

r ∂T
∂r

+ η 1
r2
∂2T
∂θ2 + ∂2T
∂z2 , −π < θ ≤π, 0 ≤r < a, 0 < z < H,
where η is a positive but unspecified parameter. Find the temperature distribution
inside of the cylinder.
5. Suppose the steady state temperature T = T(r, θ, z) satisfies Laplace equation in
the half cylinder 0 < θ < π, 0 < z < H, 0 ≤r < a. Assume the temperature is 0◦C. on
the side sheet r = a and the rest of the boundary is insulated except for the top
half disk at z = H, where the temperature is specified to be f(r, θ). (a) Write down
the PDE and BCs to model this situation and (b) solve for T in terms of f.
6. Find the solution of the problem of vibrations of an annular membrane held fixed
at zero displacement on the boundary, modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2

1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2

, a < r < b, −π < θ ≤π, 0 < t < ∞
u(a, θ, t) = u(b, θ, t) = 0, −π < θ < π, 0 < t < ∞
⎧
⎪⎨
⎪⎩
u(r, θ, 0) = f(r, θ)
∂u
∂t (r, θ, 0) = g(r, θ)
⎫
⎪⎬
⎪⎭
, a < r < b, −π < θ ≤π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
7. Find the solution of the problem of vibrations of a circular membrane held fixed
at zero displacement on the boundary, modeled by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2

1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2

, 0 ≤r < a, −π < θ ≤π, 0 < t < ∞
u(a, θ, t) = 0, −π < θ < π, 0 < t < ∞
⎧
⎪⎨
⎪⎩
u(r, θ, 0) = 0
∂u
∂t (r, θ, 0) =

1 −r
a
	
cos 2θ
⎫
⎪⎬
⎪⎭
, 0 ≤r < a, −π < θ ≤π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
8. Find the equilibrium temperature distribution in the solid hemisphere 0 < φ < π
2
assuming the flat bottom is insulated and the temperature is prescribed on the
curved hemispherical surface and is independent of θ there. Here’s how you can
do it. Extend the prescribed temperature on the curved hemispherical surface to be
given on the whole sphere by using an even extension: If a function f(φ) is defined for
0 < φ < π
2 and satisfies f ′  π
2
	
= 0, extend f to be even about φ = π
2 . In terms of the

988
Advanced Engineering Mathematics
change of variable x = cos φ that we used to arrive at the Legendre polynomials,
this corresponds to an even extension of a function F(x) defined for 0 < x < 1 that
satisfies F′(0) = 0 to give an even function ˘F(x) defined on −1 < x < 1. Earlier in
Section 11.6, we noted a fact that implies that the Legendre polynomials Pn(x) are
even for n = 0, 2, 4, . . . and odd for n = 1, 3, 5, . . ., so half of the Fourier–Legendre
coefficients should be zero. You may use the result of Problem 11.6.3.9, and some
of the calculations in doing Problem 11.6.3.9 may be useful models for calculations
in doing Problem 11.6.3.8.
9. A function f(φ) is even about φ = π
2 if
f

π
2 + ϕ

≡f

π
2 −ϕ

, for 0 < ϕ < π
2 .
A function f(φ) is odd about φ = π
2 if
f

π
2 + ϕ

≡−f

π
2 −ϕ

, for 0 < ϕ < π
2 .
(a) Explain why sin(φ) is even about φ = π
2 .
(b) Suppose f(φ) is even about φ = π
2 . Explain why, for all odd n, an = 0 in the
expansion of f(φ) in Legendre functions n(φ) given by
f(φ) = a0 +
∞

n=1
ann(φ).
10. Solve the inhomogeneous heat problem
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∇2u + Q = 1
r
∂
∂r

r ∂u
∂r

+ 1
r2
∂2u
∂θ2 + Q(r, θ), 0 ≤r < a, −π < θ ≤π
u(a, θ) = 0, −π < θ < π
⎫
⎪⎪⎬
⎪⎪⎭
.
Assume also any other boundary condition(s) needed in a polar coordinate system
based on mathematical and/or physical grounds.
11. Solve
⎧
⎪⎪⎨
⎪⎪⎩
0 = 1
ρ2
∂
∂ρ

ρ2 ∂V
∂ρ

+
1
ρ2 sin φ
∂
∂φ

sin φ ∂V
∂φ

, 0 ≤ρ < 3, 0 < φ < π
V(3, φ) = 5

1 −cos2 φ
	
, 0 < φ < π
⎫
⎪⎪⎬
⎪⎪⎭
,
along with any other boundary condition(s) needed in a polar coordinate system
based on mathematical and/or physical grounds.

Separation of Variables for PDEs
989
12. Find the solution of the problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α 1
r
∂
∂r

r ∂T
∂r

−4
r2 T, 0 ≤r < a, 0 < t < ∞
∂T
∂r (a, t) = 5, 0 < t < ∞,
T(r, 0) = f(r), 0 < r < a
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
,
along with any other boundary condition(s) based on mathematical and/or
physical grounds.
Key Terms
composite rod: Example 11.5 in Section 11.1
double Fourier series: Example 11.13 in Section 11.4
exterior of disk: Problem 11.5.3.2
Green’s function: Problem 11.3.3.13
isotherms: after Example 11.11 in Section 11.3
method of separation of variables: after (11.4) in Section 11.1
periodic boundary conditions: after (11.54) in Section 11.5
rate of decay: before Example 11.2 in Section 11.1
surface waves: after Example 11.15 in Section 11.4
temperature in a slab with a steady source or sink: Example 11.15 in Section 11.4
time constant: before Example 11.2 in Section 11.1
time dependent boundary conditions: Example 11.6 in Section 11.1
transverse vibrations of a rectangular membrane: Example 11.14 in Section 11.4
spherical coordinates: Example. 11.22 in Section 11.6
Mathematica Commands
ContourPlot[T[x, y], {x, 0, 2}, {y, 0, 1}, Contours →{0, 2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20},
ContourStyle →{Thick, Dashed}] : after Example 11.11 in Section 11.3.1
Plot3D[T[x, y], {x, 0, 2}, {y, 0, 1}]: after Example 11.11
References
F.M. Arscott, and A. Darai, A three-dimensional parabolic punch problem in linear elasticity.
Journal of Engineering Mathematics, 24, 369–383, 1990.
Pinsky, M.A. Partial Differential Equations and Boundary Value Problems, with Applications, 3rd edn.
McGraw-Hill, New York, 1998.
Watson, G.N. The Theory of Bessel Functions. Cambridge University Press, Cambridge, U.K., 1992.


12
Numerical Methods II
12.1 Finite Difference Methods for Heat Equations
For numerical approximation of solutions of ordinary differential equations (ODEs) our
methods have included finite differences in Section 8.8 and spline functions in Section
8.9. In Chapter 12, we will see analogous methods for approximating solutions of partial
differential equations (PDEs).
For a heat equation
∂T
∂t (x, t) = α ∂2T
∂x2 (x, t) + g(x, t),
0 < x < L,
t > 0,
(12.1)
we can use finite differences for both the space variable, x, and the time variable, t.
Partition the interval [ 0, L ] using
0 = x0 < x1 < · · · < xN = L.
Denoting h = xj −xj−1, we have
xj = jh,
j = 0, 1, . . . , N.
We may also write h =△x.
Similarly, we can partition the time interval [0, ∞) or [0, T] using
tm = m △t,
m = 0, 1, 2, . . . .
The first and simplest example of a finite difference method for (12.1) approximates the
time partial derivative using the forward difference, that is,
∂T
∂t (xj, tm) ≈T(xj, tm+1) −T(xj, tm)
△t
,
and approximates the second space derivative using the central difference, that is,
∂2T
∂x2 (xj, tm) ≈T(xj+1, tm) −2T(xj, tm) + T(xj−1, tm)
h2
.
Using the forward difference to approximate the time partial derivative makes this
method analogous to Euler’s method for ODEs.
991

992
Advanced Engineering Mathematics
t
+1
+1
Tj
m
Tj
m
–1
Tj
m
Tj
m
x
FIGURE 12.1
Finite difference stencil for heat equation.
Denote
Tm
j = T(xj, tm)
and
gm
j = g(xj, tm).
It is customary to use the superscript m rather than (m). Keep in mind that Tm
j is not the
mth power of Tj; in fact, Tj without a superscript should never appear in our work.
The stencil or replacement equations for (12.1) are
Tm+1
j
−Tm
j
△t
= α
Tm
j+1 −2Tm
j + Tm
j−1
h2
+ gm
j ,
m = 0, 1, 2, . . . ,
j = 0, 1, . . . , N.
(12.2)
Equivalently,
Tm+1
j
= Tm
j + ϵ ·

Tm
j+1 −2Tm
j + Tm
j−1

+ △t · gm
j ,
m = 0, 1, 2, . . . ,
j = 0, 1, . . . , N, (12.3)
where
ϵ ≜α △t
h2
= α △t
(△x)2 .
Figure 12.1 illustrates that the approximation of the heat equation in (12.2) uses the tem-
perature and source values at time t = tm to explicitly find the temperature values at time
t = tm+1.
Example 12.1
For the PDE–boundary value problem–initial value problem (PDE-BVP-IVP)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
T(0, t) = T(π, t) = 0,
t > 0,
T(x, 0) = 4(πx −x2),
0 < x < π
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
use the preceding finite difference method to find an approximate solution.

Numerical Methods II
993
Method:
The boundary conditions (BCs) on T(x, t) directly imply BCs on Tm
j
≜
T(jh, m △t):
Tm
0 = Tm
N = 0,
m = 1, 2, . . . .
(12.4)
So, we need only to approximate the temperature values at x
=
xj
=
jπ
N , for
j = 1, . . . , N −1. Define
Tm =
⎡
⎢⎣
Tm
1...
Tm
N−1
⎤
⎥⎦.
The initial condition for the temperature directly implies
T 0
j = 4

πxj −x2
j

= 4

π jπ
N −
jπ
N
2
= 4π2j(N −j)
N2
,
j = 0, 1, . . . , N.
For this PDE with g(x, t) ≡0 and BCs T(0, t) = T(π, t) = 0, the replacement equations
(12.3) are
Tm+1
j
= ϵTm
j+1 + (1 −2ϵ)Tm
j + ϵTm
j−1,
j = 1, . . . , N −1,
m = 0, 1, 2, . . . .
(12.5)
As earlier, ϵ = α△t
(△x)2 . In terms of the vector Tm, we see that the replacement equations are
the system of linear, homogeneous partial difference equations
Tm+1 = ATm,
m = 0, 1, . . . ,
(12.6)
where the constant, (N −1) × (N −1), discretization matrix A is given by
A(ϵ) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −2ϵ
ϵ
0
0
.
.
.
0
ϵ
1 −2ϵ
ϵ
0
0
0
ϵ
1 −2ϵ
ϵ
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 −2ϵ
ϵ
0
ϵ
1 −2ϵ
ϵ
0
.
.
.
0
ϵ
1 −2ϵ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Because the matrix A = A(ϵ) depends on the parameter ϵ ≜α
△t
(△x)2 , in principle the
approximation we get could depend on ϵ. As we will see, “... could depend...” turns out
to be “...significantly depends...!”
In effect, the BCs Tm
0 = Tm
N = 0, m = 1, 2, . . ., are “built into” the relationship between
the partial difference equations (12.6) and the matrix A(ϵ):
Tm
j satisfies both the system of partial difference equations (12.6) and the BCs (12.4) ⇐⇒
Tm
j satisfies both (12.4) and (12.5).
The solution of (12.6) is given by
Tm = Am T0,
m = 0, 1, . . . ,
(12.7)
where Am does denote the mth power of the matrix A. ⃝

994
Advanced Engineering Mathematics
Example 12.2
For the PDE-BVP-IVP of Example 12.1 with α
= 1, find the numerical, that is,
approximate, solution for grid size h =△x = π
6 and two values of ϵ of your choice.
Method: Here N = 6, so the solution vector is
Tm =
⎡
⎢⎣
Tm
1...
Tm
5
⎤
⎥⎦.
(a) For △t = 1
8, that is, for ϵ =
1/8
(π/6)2 =
9
2π2 , the 5 × 5 matrix A(ϵ) is
A
 9
2π2

=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −
9
π2
9
2π2
0
0
0
9
2π2
1 −
9
π2
9
2π2
0
0
0
9
2π2
1 −
9
π2
9
2π2
0
0
0
9
2π2
1 −
9
π2
9
2π2
0
0
0
9
2π2
1 −
9
π2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
MathematicaTM says that the eigenvalues of A are λ ≈0.87783, −0.701611, 0.544055,
−0.367836, and 0.0881093, all of which have magnitude less than 1. It follows that,
for all initial temperature distributions and thus any initial vector T0, the approximate
solution has
Tm →0,
as m →∞.
This agrees with the exact solution, T(x, t), using separation of variables, which has
T(x, t) →0,
as t →∞.
In fact, the decay rate for the exact solution is r = 1 because the product solutions are of
the form sin nx · e−n2t. The maximum value T(x, 1) behaves like e−1 times T(x, 0). For the
finite difference solution with △t = 1
8, after eight time steps, we have
⎡
⎢⎢⎣
T
 π
6 , 1

...
T

5π
6 , 1

⎤
⎥⎥⎦≈
⎡
⎢⎣
T8
1...
T8
5
⎤
⎥⎦= T8 = A8T0.
So, the maximum magnitude eigenvalue λ1 of the matrix A

9
2π2

should have
|λ1|8 ≈e−1.
In fact,
|λ1|8 = 0.877838 ≈0.3526008467
versus
e−1 ≈0.3678794412.
So, even this coarse finite difference method, with △x = π
6 and △t = 1
8, gives a good
approximation to the rate of decay of solutions.
Table 12.1 has the numerical results for the approximate solution, which actually was
prepared using an Excel spreadsheet using (12.3).

Numerical Methods II
995
TABLE 12.1
Example 12.2(a): Finite Differences for a Heat Equation with △t = 1
8
i
ti
T m
1
T m
2
T m
3
T m
4
T m
5
0
0
5.483113556
8.77298169
9.869604401
8.77298169
5.483113556
1
0.125
4.483113556
7.77298169
8.869604401
7.77298169
4.483113556
2
0.25
3.939058883
6.77298169
7.869604401
6.77298169
3.939058883
3
0.375
3.435177254
5.980867831
6.869604401
5.980867831
3.435177254
4
0.5
3.029619961
5.225387395
6.05917383
5.225387395
3.029619961
5
0.625
2.649428799
4.604398524
5.298851774
4.604398524
2.649428799
6
0.75
2.33279343
4.029671928
4.665586346
4.029671928
2.33279343
7
0.875
2.042850989
3.545930314
4.085701932
3.545930314
2.042850989
8
1
1.796744622
3.106714667
3.593489239
3.106714667
1.796744622
...
...
...
...
...
...
...
16
2
0.633079741
1.096212821
1.266159481
1.096212821
0.633079741
...
...
...
...
...
...
...
24
3
0.22319749
0.386570987
0.44639498
0.386570987
0.22319749
...
...
...
...
...
...
...
32
4
0.078697932
0.136307736
0.157395864
0.136307736
0.078697932
(b) For △t = 1
4, that is, for ϵ =
1/4
(π/6)2 =
9
π2 , the 5 × 5 matrix A(ϵ) is
A
 9
π2

=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −18
π2
9
π2
0
0
0
9
π2
1 −18
π2
9
π2
0
0
0
9
π2
1 −18
π2
9
π2
0
0
0
9
π2
1 −18
π2
9
π2
0
0
0
9
π2
1 −18
π2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Mathematica says that the eigenvalues of A are λ ≈−2.40322, −1.73567, −0.823781,
0.75566, and 0.0881093, at least one of which has magnitude greater than one. It fol-
lows that, at least for some initial temperature distributions, the approximate solution
vector Tm has
||Tm|| →∞,
as m →∞.
This is very bad! In fact, because the largest magnitude eigenvalue is less than −1, for
some initial conditions, the solution produced by finite differences oscillates between
larger and larger positive and negative values in at least one of the entries Tm
j . This is
true even though the exact solution has T(x, t) →0, as t →∞.
Table 12.2 has the numerical results for the approximate solution, which again was
prepared using an Excel spreadsheet using (12.3). ⃝
In Problem 12.1.5.5, you will address the natural question raised by the aforementioned
“... for some initial conditions ....”

996
Advanced Engineering Mathematics
TABLE 12.2
Example 12.2(b): Finite Differences for a Heat Equation with △t = 1
4
i
ti
T m
1
T m
2
T m
3
T m
4
T m
5
0
0
5.483113556
8.77298169
9.869604401
8.77298169
5.483113556
1
0.25
3.483113556
6.77298169
7.869604401
6.77298169
3.483113556
2
0.5
3.306894862
4.77298169
5.869604401
4.77298169
3.306894862
3
0.75
1.628279222
4.436070815
3.869604401
4.436070815
1.628279222
4
1
2.703865528
1.359116479
4.902715257
1.359116479
2.703865528
5
1.25
−0.988028262
5.81675517
−1.56003395
5.81675517
−0.988028262
6
1.5
6.118163881
−7.115288281
11.89361614
−7.115288281
6.118163881
7
1.75
−11.5283939
22.28621531
−22.77446838
22.28621531
−11.5283939
8
2
29.81946681
−49.63942703
59.40636416
−49.63942703
29.81946681
...
...
...
...
...
...
...
16
4
32489.63605
−56273.47252
64979.22277
−56273.47252
32489.63605
...
...
...
...
...
...
...
24
6
36148921.36
−62611768.41
72297842.71
−62611768.41
36148921.36
...
...
...
...
...
...
...
32
8
40220441029
−69663847365
80440882058
−69663847365
40220441029
12.1.1 Incompatibility of Initial Condition with Boundary Conditions
In Examples 12.1 and 12.2, the initial temperature distribution T(x, 0) = 4(πx −x2), 0 <
x < π, just happened to also satisfy the BCs T(0, t) = T(π, t) = 0, at t = 0, in the limits
as x →0+ and as x →π−. We do not demand that this always be true. If the initial
temperature distribution T(x, 0) happens to not satisfy one or more of the BCs at t = 0, then
the replacement equations would have to be broken into two cases: m = 0 versus m ≥1.
Example 12.3
With N = 3, for the PDE-BVP-IVP
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 , 0 < x < π, t > 0,
T(0, t) = T(π, t) = 0, t > 0,
T(x, 0) = f(x), 0 < x < π
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
set up, but do not solve, the replacement equations.
Method: With N = 3, hence h = π
3 , and ϵ =
α△t
(π/3)2 , the replacement equations for the
PDE-BVP-IVP are (12.5), that is,
Tm+1
j
= ϵTm
j+1 + (1 −2ϵ)Tm
j + ϵTm
j−1,
j = 1, . . . , N −1,
for m ≥1 but are
T1
j = ϵf(xj+1) + (1 −2ϵ)f(xj) + ϵf(xj−1),
j = 1, . . . , N −1,
for m = 0. (If, for example, f(x) is undefined at x = 0, then for j = 1, replace f(x0) by
limx→0+ f(x).) ⃝

Numerical Methods II
997
12.1.2 Time-Dependent Boundary Conditions
Instead of the homogeneous Dirichlet BCs T(0, t) = T(L, t) = 0, the temperature might
satisfy a nonhomogeneous Dirichlet BC T(0, t) = φ(t) and/or T(L, t) = ψ(t). This is an
extension of our previous work by replacing
Tm
0 = φ(tm)
and/or
Tm
N = ψ(tm), respectively.
The matrix A(ϵ) is not affected, but (12.6) is replaced by the nonhomogeneous system of
difference equations
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Tm+1
1
Tm+1
2...
Tm+1
N−2
Tm+1
N−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= A(ϵ)
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Tm
1
Tm
2...
Tm+1
N−2
Tm
N−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
+ ϵ
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
φ(tm)
0
...
0
ψ(tm)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
12.1.3 Other Boundary Conditions
What is less straightforward is what to do with Neumann or Robin BCs. In Problem
8.6.1.8 in Section 8.6, we found that the central difference approximation
∂T
∂x (xj, tm) ≈
Tm
j+1 −Tm
j−1
2h
(12.8)
has accuracy O(h2). So, for example, we can replace the Neumann BC
∂T
∂x (0, tm)φ(tm) ≜φm
by
Tm
1 −Tm
−1
2h
= φm.
(12.9)
But then it appears that we need to know Tm
−1 ≈T(−h, tm). To deal with this, we also need
to use the replacement equation for the PDE at x = 0, that is,
Tm+1
0
= Tm
0 + ϵ

Tm
1 −2Tm
0 + Tm
−1

+ △t · gm
0 .
(12.10)
Note that when we had Dirichlet BCs we never used the replacement equation for the PDE
at x = 0 or x = L, even though (12.2) did allow for j = 0 and j = N, respectively.
We can combine (12.9) and (12.10) by solving (12.9) for Tm
−1 to get
Tm
−1 = Tm
1 −2h φm.

998
Advanced Engineering Mathematics
Substitute this into (12.10) to get
Tm+1
0
= Tm
0 + ϵ

Tm
1 −2Tm
0 + Tm
1 −2h φm
+ △t · gm
0 ,
that is,
Tm+1
0
= (1 −2ϵ)Tm
0 + 2ϵTm
1 −2h ϵ φm + △t · gm
0 .
(12.11)
We include (12.11) in the system of difference equations.
Example 12.4
For the PDE-BVP-IVP with N = 3
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
∂T
∂x (0, t) = φ(t),
T(π, t) = 0,
t > 0,
T(x, 0) = f(x),
0 < x < π
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
set up, but do not solve, the replacement equations in vector form.
Method: With N = 3, hence h = π
3 , and ϵ =
α△t
(π/3)2 , the PDE-BVP-IVP are discretized by
Tm+1 =
⎡
⎢⎢⎢⎢⎣
1 −2ϵ
2ϵ
0
ϵ
1 −2ϵ
ϵ
0
ϵ
1 −2ϵ
⎤
⎥⎥⎥⎥⎦
Tm −2hϵ
⎡
⎢⎢⎢⎢⎣
φm
0
0
⎤
⎥⎥⎥⎥⎦
,
where Tm ≜
⎡
⎣
Tm
0
Tm
1
Tm
2
⎤
⎦and the initial condition is T0 ≜
⎡
⎣
f(x0)
f(x1)
f(x2)
⎤
⎦. ⃝
12.1.4 Nonlinearity
As for ODEs, numerical methods for PDEs also handle nonlinear PDEs. For example, for
a PDE of the form
∂T
∂t = α ∂2T
∂x2 + q

x, t, T, ∂T
∂x

,
we can define g(x, t) ≜q

x, t, T(x, t), ∂T
∂x (x, t)

, which could have replacement equations
Tm+1
j
= ϵTm
j+1 + (1 −2ϵ)Tm
j + ϵTm
j−1 + q

xj, tm, Tm
j ,
Tm
j+1−Tm
j−1
2h

,
j = 1, . . . , N −1,
m = 0, 1, 2, . . . .

Numerical Methods II
999
Handling a nonlinear BC is a trickier matter. In principle it would require solving a
nonlinear equation at each time step. For example, a BC of the form
T(0, t) −η
∂T
∂x (xj, t)
3
= φ(t)
would require solving
Tm
0 −η
Tm
1 −Tm
−1
2h
3
−φm = 0
for Tm
−1 in terms of Tm
0 , Tm
1 , and φm ≜φ(tm), in order to substitute for Tm
−1 in the replacement
equation at j = 0, m −1, 2, . . ..
Example 12.5
For the nonlinear PDE-BVP-IVP that follows, find the numerical, that is, approximate,
solution for grid size h =△x = π
6 and △t = 1
8:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 + 1.5T(1 −T),
0 < x < π,
t > 0,
T(0, t) = T(π, t) = 0,
t > 0,
T(x, 0) = πx −x2,
0 < x < π
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Method: Here ϵ =
1/8
(π/6)2 =
9
2π2 and N = 6, so the solution vector is
Tm =
⎡
⎢⎣
Tm
1...
Tm
5
⎤
⎥⎦.
The replacement equations are
Tm+1
j
= Tm
j + ϵ ·

Tm
j+1 −2Tm
j + Tm
j−1

+ △t · Tm
j (1 −Tm
j ),
m = 0, 1, 2, . . . ,
j = 1, 2, . . . , N −1.
Table 12.3 has the numerical results for the approximate solution, which again was
prepared using an Excel spreadsheet.
It appears that the approximate solution agrees with T(x, t) →v(x) as t →∞, where
the equilibrium temperature distribution v(x) satisfies the nonlinear ODE-BVP
⎧
⎨
⎩
0 = v′′ + 1.5v(1 −v),
0 < x < π,
v(0) = v(π) = 0
⎫
⎬
⎭. ⃝

1000
Advanced Engineering Mathematics
TABLE 12.3
Example 12.5: Finite Differences for a Nonlinear Heat Equation with △t = 1
8
i
ti
T m
1
T m
2
T m
3
T m
4
T m
5
0
0
1.370778389
2.193245422
2.4674011
2.193245422
1.370778389
1
0.125
1.025480576
1.452542911
1.538526021
1.452542911
1.025480576
2
0.25
0.747735232
1.173778558
1.304768271
1.173778558
0.747735232
3
0.375
0.636428921
1.001004452
1.110760232
1.001004452
0.636428921
4
0.5
0.555863729
0.884632053
0.987607201
0.884632053
0.555863729
...
...
...
...
...
...
...
8
1
0.395780439
0.645035453
0.726666641
0.645035453
0.395780439
...
...
...
...
...
...
...
16
2
0.289638291
0.479833303
0.543923601
0.479833303
0.289638291
...
...
...
...
...
...
...
128
16
0.212938681
0.356952076
0.406565109
0.356952076
0.212938681
...
...
...
...
...
...
...
296
37
0.212908658
0.356903395
0.406510433
0.356903395
0.212908658
...
...
...
...
...
...
...
320
40
0.212908658
0.356903395
0.406510432
0.356903395
0.212908658
12.1.5 Problems
In problems 1–4,
set up the replacement equations for the PDE-BVP-IVP using
N = 3 and ϵ =
9
2π2 :
(a) Find the eigenvalues of the matrix A(ϵ).
(b) In the special case φ(t) ≡0 and ψ(t) ≡0, discuss whether the decay rate of the
numerical solutions should agree well with the decay rate for the exact solution.
1.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
T(0, t) = 0,
∂T
∂x (π, t) = ψ(t),
t > 0,
T(x, 0) = f(x),
0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
2.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
T(0, t) = φ(t),
∂T
∂x (π, t) = ψ(t),
t > 0,
T(x, 0) = f(x),
0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
3.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
∂T
∂x (0, t) = φ(t),
∂T
∂x (π, t) = ψ(t),
t > 0,
T(x, 0) = f(x),
0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭

Numerical Methods II
1001
4.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
3T(0, t) −∂T
∂x (0, t) = φ(t),
∂T
∂x (π, t) = ψ(t),
t > 0,
T(x, 0) = f(x), 0 < x < π
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
5. In Example 12.2(b), we saw that for some initial temperature distributions, the
approximate solution has ||Tm|| →∞, as m →∞. For the PDE and BCs of
Example 12.2(b), are there some nonzero initial temperature distributions, T0, for
which ||Tm|| →0, as m →∞? If so, find one such nonzero T0.
6. For the following nonlinear PDE-BVP-IVP, find the numerical, that is, approxi-
mate, solution for grid size h =△x = π
6 and △t = 1
8:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = ∂2T
∂x2 + 2T(1 −T),
0 < x < π,
t > 0,
T(0, t) = T(π, t) = 0,
t > 0,
T(x, 0) = πx −x2,
0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
12.2 Numerical Stability
As we remarked in Chapter 8, astute users of numerical methods noticed that if the
time step size, △t, is too large, then a numerical solution might behave qualitatively in
a significantly incorrect way(s).
For example, we saw in Example 12.2 in Section 12.1 that while the exact solution of a
heat equation should have T(x, t) →0, as t →∞, an approximate solution using finite
differences can oscillate between larger and larger positive and negative values as m, the
number of time steps, increases.
There are three general ways to discuss the accuracy of an approximation solution
method that depends on a time step size, △t, and/or a space grid size, h. A method is con-
sistent if a limiting process results in the terms in the difference equation converging to
the appropriate terms in the original PDE. It is usually easy to design a numerical method
to be consistent. For example, in Section 8.6, we saw how to get finite differences that were
consistent approximations of derivatives, and then in Section 8.7, we replaced terms in
ODEs by those finite differences to get the “replacement equations,” that is, a numerical
method.
A numerical method is convergent if all numerical solutions converge to exact solutions
by a limiting process. For example, Theorem 8.6(c) in Section 8.3 implies that the numerical
integration method of Simpson’s rule is convergent because Sn →I as n →∞, that is, as
h = b−a
n
→0.
A convergent method is successful in the sense that theoretically it converges to the exact
solution for a broad set of conditions. For example, Simpson’s rule is a convergent method
for all functions f(x) that are four times continuously differentiable on [ a, b ].

1002
Advanced Engineering Mathematics
We italicize the word method to emphasize that its success is not “hit or miss,” that is, its
success is predictably related to the conditions under which it is asked to work.
In Example 12.2(b) in Section 12.1, we saw that a numerical method could be consistent
yet fail to be convergent.
There is a third way to discuss the accuracy of an approximate solution method: a method
is stable if it does not catastrophically magnify errors as the number of time steps increases.
Consider the heat equation in Section 12.1. We used the approximation
T(xj, tm) ≈Tm
j .
A stable method is allowed to have a certain amount of magnification of errors.
The solution of the heat problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂T
∂t = α ∂2T
∂x2 ,
0 < x < π,
t > 0,
T(0, t) = T(π, t) = 0,
t > 0,
T(x, 0) = f(x),
0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(12.12)
depends linearly on the initial temperature distribution f(x): Suppose T1(x, t) solves (12.12)
when the initial condition is T(x, t) = f1(x) and T2(x, t) solves (12.12) when the initial
condition is T(x, t) = f2(x). Then, for any constants β1, β2, when the initial condition is
T(x, t) = β1 f2(x) + β2 f2(x), then the solution of (12.12) is
T(x, t) = β1T1(x, t) + β2T2(x, t).
Likewise, the approximate solution given by the finite difference method, that is,
Tm = AmT0,
(12.13)
depends linearly on the initial vector T0.
Suppose there is an error in the initial condition, that is, we use an initial vector
T0 + (initial error)
rather than T0. The approximate solution depends linearly on the initial vector, so the
initial error leads to an error
Am(initial error)
in the approximate solution given by (12.13).
We see that the method of approximating the solution of (12.12) by (12.13) is numeri-
cally unstable exactly when the discretization matrix A = A(ϵ) has an eigenvalue whose
magnitude is greater than 1 or a deficient eigenvalue whose magnitude is 1. This happens
exactly when the linear system of difference equations (12.6) in Section 12.1 is unstable.
So, let us study the eigenvalues of the matrix A(ϵ). We could find a formula for the char-
acteristic polynomial 0 = | A(ϵ) −λI | using the technique of Example 4.38 in Section 4.6.

Numerical Methods II
1003
Instead, we will work in a way similar to what we did in Chapter 11: substitute a product
solution,
Tm
j = Qm sin njπ
N ,
j = 1, . . . , N −1,
(12.14)
into the partial difference equation (12.5) in Section 12.1 and then divide by Qm to get
Q sin njπ
N = sin njπ
N + ϵ

sin n(j + 1)π
N
−2 sin njπ
N + sin n(j −1)π
N

,
j = 1, . . . , N −1.
(12.15)
We use a sum of sines formula to rewrite
sin n(j + 1)π
N
+ sin n(j −1)π
N
= 2 sin
1
2
n(j + 1)π
N
+ n(j −1)π
N

cos
1
2
n(j + 1)π
N
−n(j −1)π
N

= 2 sin njπ
N cos nπ
N .
After dividing through by sin njπ
N , we have the amplification factor
Qn = 1 + 2ϵ

−1 + cos nπ
N

,
(12.16)
where n can be any integer other than an integer multiple of N.
Because n is an integer, Tm
j
= Qm sin njπ
N , j = 1, . . . , N −1, satisfies the BCs (12.4)
in Section 12.1, that is, Tm
0
= Tm
N = 0, m = 1, 2, . . .. This guarantees that the vector
[ sin njπ
N ]N−1
j=1
is an eigenvector of the matrix A(ϵ) if, and only if, Tm
j
satisfies the partial
difference equation (12.5) in Section 12.1.
Note that the logical equivalence between Q satisfying (12.15) and Q satisfying (12.16)
does not allow n to be an integer multiple of N.
Theorem 12.1
(von Neumann–Goldstine criterion)
(a) If 1
2 ≥ϵ ≜α
△t
(△x)2 > 0, then the numerical method of linear partial difference
equations (12.5) in Section 12.1 and BCs (12.4) in Section 12.1 is stable.
(b) If 1
2 < ϵ, then the numerical method of linear partial difference equations (12.5) in
Section 12.1 and BCs (12.4) in Section 12.1 is unstable if N is sufficiently large.
(c) If ϵ < 0, that is, we are going backward in time, then the numerical method of
linear partial difference equations (12.5) in Section 12.1 and BCs (12.4) in Section
12.1 is unstable.

1004
Advanced Engineering Mathematics
Why? First, for each of n = 1, n = 2, . . . , n = N −1, the vector T(n) ≜[ sin njπ
N ]N−1
j=1 is an
eigenvector of the matrix A(ϵ). This gives a complete set of n basic solutions
Qm
n T(n),
n = 1, . . . , N −1,
for the system of difference equations Tm+1 = ATm and thus assures the stability of the
numerical method if, and only if,
|Qn| =
1 + 2ϵ

−1 + cos nπ
N
 ≤1,
for all n = 1, . . . , N −1.
(12.17)
Note that −1 + cos nπ
N < 0 for 1 ≤n ≤N −1, so for ϵ > 0,
Qn = 1 + 2ϵ

−1 + cos nπ
N

< 1.
So, in cases (a) and (b), stability is decided by whether Qn ≥−1 for all n = 1, . . . , N −1.
(a) Suppose 0 < ϵ ≤1
2. Then for 1 ≤n ≤N −1, we have cos nπ
N > −1, so
Qn = 1 + 2ϵ

−1 + cos nπ
N

≥1 + 2 · 1
2

−1 −1

= −1;
thus, the numerical method is stable.
(b) Suppose ϵ > 1
2. Then for n = N −1 and sufficiently large N, nπ
N = π(N−1)
N
will be
sufficiently close to π that we have
cos π(N −1)
N
sufficiently close to the smallest value it can be, −1. Then
QN−1 = 1 + 2ϵ

−1 + cos nπ
N

< 1 + 2ϵ

−1 + (≈−1)

< −1,
because 2ϵ > 1. Thus, the numerical method is unstable for sufficiently large N.
(c) Suppose ϵ < 0, that is, △t < 0, that is, we are going backward in time. Then
because −1 + cos π
N < 0, for n = 1, we have
Q1 = 1 + 2ϵ

−1 + cos π
N

> 1,
which implies that the numerical method is unstable. 2
In case (b), that is, when the time step size is too large, the “mechanism” of instabil-
ity is that A has an eigenvalue QN−1 < −1. Because Qm
N−1 as a function of m alternates

Numerical Methods II
1005
in sign, this causes the numerical solution to oscillate between positive and nega-
tive values of larger and larger magnitude, exactly what we saw in Table 12.2. It
is great when the numerical results show exactly what we expect from a theoretical
analysis.
To check numerical results often one takes a finer and finer grid. Theorem 12.1 tells us
that to avoid numerical instability, we must take the time step size to be very small if
the grid size is small. For example, if α = 1 and h =△x = 10−7, then we need to use
△t = 5.0 × 10−15. If time is measured in seconds, then numerically we must work in
femtoseconds, and we need 2 petasteps to predict what happens during 1 s on time. If we
insist on a very accurate description in space, then our numerical work may take as much
computer time as the age of the universe. Another danger is that if we take a huge number
of time steps, then the accumulated round off errors would make the numerical results
meaningless.
12.2.1 Crank–Nicholson Method
We saw in Section 8.4 that solving a system of linear algebraic equations can be accom-
plished using an implicit, iterative method such as Gauss–Seidel’s. Here we will see that
an implicit method can help us overcome the numerical instability issue.
Consider the heat equation, ∂T
∂t = ∂2T
∂x2 . Instead of approximating
∂2T
∂x2 (xj, tm) ≈T(xj+1, tm) −2T(xj, tm) + T(xj−1, tm)
h2
,
use the Crank–Nicholson method of approximating the PDE:
∂2T
∂x2 (xj, tm) ≈1
2
Tm
j+1 −2Tm
j + Tm
j−1
h2
+ 1
2
Tm+1
j+1 −2Tm+1
j
+ Tm+1
j−1
h2
.
(12.18)
Substitute this, along with the usual forward difference approximation of ∂T
∂t (xj, tm), into
the heat equation. This gives the implicit partial difference equation
−ϵ
2 Tm+1
j−1 + (1 + ϵ)Tm+1
j
−ϵ
2 Tm+1
j+1 = ϵ
2 Tm
j+1 + (1 −ϵ)Tm
j + ϵ
2 Tm
j−1,
j = 1, . . . , N −1,
(12.19)
where, as usual, ϵ ≜
△t
(π/N)2 . We call (12.19) “implicit” because it does not explicitly give
a formula for Tm+1
j
, the temperature at time tm+1 and space point xj, in terms of the
temperature values at the earlier time, tm.

1006
Advanced Engineering Mathematics
We can express the vector Tm+1 in terms of Tm:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 + ϵ
−1
2ϵ
0
.
.
.
0
−1
2ϵ
1 + ϵ
−1
2ϵ
.
0
−1
2ϵ
1 −ϵ
.
.
.
.
.
.
.
.
.
.
.
0
.
−1
2ϵ
1 + ϵ
−1
2ϵ
0
.
.
0
−1
2ϵ
1 + ϵ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Tm+1
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −ϵ
1
2ϵ
0
.
.
.
0
1
2ϵ
1 −ϵ
1
2ϵ
.
0
1
2ϵ
1 −ϵ
.
.
.
.
.
.
.
.
.
.
.
0
.
1
2ϵ
1 −ϵ
1
2ϵ
0
.
.
0
1
2ϵ
1 −ϵ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Tm.
(12.20)
To study numerical stability of the Crank–Nicholson method, again we substitute into
the partial difference equation (12.19) the product solution (12.14), that is,
Tm
j = Qm sin njπ
N ,
j = 1, . . . , N −1,
for the partial difference equation (12.5) in Section 12.1. Substitute that into (12.20) to get
Q ·

sin njπ
N −ϵ
2

sin n(j + 1)π
N
−2 sin njπ
N + sin n(j −1)π
N

= sin njπ
N + ϵ
2

sin n(j + 1)π
N
−2 sin njπ
N + sin n(j −1)π
N

,
j = 1, . . . , N −1. (12.21)
In Problem 12.2.2.1, you will derive that (12.21) can be rewritten as
Qn = 1 + ϵ

−1 + cos nπ
N

1 −ϵ

−1 + cos nπ
N
,
for n = 1, . . . , N −1.
(12.22)

Numerical Methods II
1007
Theorem 12.2
(a) If ϵ > 0, then the Crank–Nicholson method of linear partial difference equations
(12.20) and BCs (12.4) in Section 12.1 is stable.
(b) If ϵ < 0, that is, we are going backward in time, then the numerical method of
linear partial difference equations (12.20) and BCs (12.4) in Section 12.1 is unstable.
Why? You will explain (a) in Problem 12.2.2.2 and (b) in Problem 12.2.2.3. 2
12.2.2 Problems
1. Explain why ϵ > 0 implies Qn given in (12.22), that is, for the Crank–Nicholson
method, satisfies −1 < Qn < 1 for n = 1, . . . , N −1. Also, explain why Theorem
12.2(a) is true.
2. Derive that (12.21) can be rewritten as (12.22).
3. Use the result of Problem 12.2.2.2 and define β = −ϵ > 0 to help explain why
Theorem 12.2(b) is true.
12.3 Finite Difference Methods for Potential Equations
On a spatial rectangle 0 ≤x ≤L, 0 ≤y ≤H, we can discretize the potential equation
F(x, y) = ∂2u
∂x2 + ∂2u
∂y2
(12.23)
by defining
xn ≜n △x,
ym ≜m △y,
un,m ≜u(xn, ym),
and
Fn,m ≜F(xn, ym).
We can use the central difference approximations
∂2u
∂x2 (xn, ym) ≈(△x)−2 
un−1,m −2un,m + un+1,m

and
∂2u
∂y2 (xn, ym) ≈(△y)−2 
un,m−1 −2un,m + un,m+1

.
Let N be an integer with L = N △x and M be an integer with H = M △y. The replacement
equations for (12.23) are, for n = 1, . . . , N −1, and m = 1, . . . , M −1,
Fn,m = (△x)−2 
un−1,m −2un,m + un+1,m

+ (△y)−2 
un,m−1 −2un,m + un,m+1

.
(12.24)

1008
Advanced Engineering Mathematics
Example 12.6
Set up and solve a finite differences approximate solution of
−1 + 6 cos 4y = ∂2u
∂x2 + ∂2u
∂y2 , 0 < x < π,
0 < y < π
2 ,
(12.25)
along with BCs
⎧
⎨
⎩
u(0, y) = u(π, y) = 0,
0 < y < π
2
u(x, 0) = u(x, π
2 ) = 2,
0 < x < π
⎫
⎬
⎭.
(12.26)
Method: The instructions did not say how fine or coarse we should make the grids in the
x and y directions. N = 8 and M = 4 look large enough to make the work informative
and also seems to be a reasonable compromise between accuracy and tediousness in the
calculations.
So, xn = n · π
8 , ym = m · π
8 , and un,m = u(xn, ym), n = 1, . . . , 7, m = 1, 2, 3. For
convenience, define h =△x =△y =
π
8 . The central difference approximation of the
Laplacian is, in this example,
∇2u(xn, ym) ≈h−2 
un,m−1 + un,m+1 + un−1,m + un+1,m −4un,m

,
(12.27)
because △x = h =△y. This five-point Laplacian stencil ∗is illustrated in Figure 12.2.
The replacement equations are
−1 + 6 cos(4ym) = Fn,m ≈∇2u(xn, ym)
≈h−2 
un,m−1 + un,m+1 + un−1,m + un+1,m −4un,m

,
along with the BCs
u0,m = u8,m = 0,
m = 1, 2, 3,
and
un,0 = un,4 = 2,
n = 1, . . . , 7.
The approximate solution’s values and the boundary values can be displayed in a
rectangular array:
2
2
2
2
2
2
2
0
u13
u23
u33
u43
u53
u63
u73
0
0
u12
u22
u32
u42
u52
u62
u72
0
0
u11
u21
u31
u41
u51
u61
u71
0
2
2
2
2
2
2
2
.
un,m+1
un,m
un–1,m
un+1,m
un,m–1
FIGURE 12.2
Five-point Laplacian.
∗Discretizations of the Laplacian and other partial differential operators are found in Section 25.3 of Abramowitz
and Stegun (1964).

Numerical Methods II
1009
We could set up a system of 21 equations in the 21 unknowns un,m, n = 1, . . . , 7, m =
1, 2, 3. For that purpose, it would help to define the great vector
u = [u11
...
u71  u12
...
u72  u13
...
u73]T
in
R21.
But we can use the apparent symmetry satisfied by both the BCs (12.26) and the
inhomogeneity in the PDE (12.25): We suspect that the solution u(x, y) satisfies
u(x, y) = u(π −x, y)
and
u(x, y) = u

x, π
2 −y

,
0 < x < π,
0 < y < π
2 ,
which would manifest in the approximate solution satisfying
un,m = uN−n,m
and
un,m = un,M−m,
n = 1, . . . , 7,
m = 1, 2, 3.
Using symmetry, the rectangular array would display the solution as
2
2
2
2
2
2
2
0
u11
u21
u31
u41
u31
u21
u11
0
0
u12
u22
u32
u42
u32
u22
u12
0
0
u11
u21
u31
u41
u31
u21
u11
0
2
2
2
2
2
2
2
.
For example, at (n, m) = (1, 2), using u1,3 = u1,1, the BC u0,2 = 0, and the inhomogene-
ity value F1,2 and multiplying through the replacement equations by h2, the replacement
equation becomes
π2
64 · (−7) = π2
64

−1 + 6 cos

4 · π
4

= π2
64

−1 + 6 cos(4y2)

= h2F1,2
≈h2∇2u(x1, y2) ≈u1,1 + u1,3 + u0,2 + U2,2 −4u1,2 = 2u1,1 + 0 + u2,2 −4u1,2.
Define the vector
U = [u11
u21
u31
u41
u12
u22
u32
u42]T in R8.
Incorporating h = π
8 , the symmetry assumption, the BCs, and the inhomogeneity values
Fn,m = −1 + 6 cos(4ym) and multiplying through the replacement equations by h2 gives
us the system
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−4
1
0
0
1
0
0
0
1
−4
1
0
0
1
0
0
0
1
−4
1
0
0
1
0
0
0
2
−4
0
0
0
1
2
0
0
0
−4
1
0
0
0
2
0
0
1
−4
1
0
0
0
2
0
0
1
−4
1
0
0
0
2
0
0
2
−4
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
u11
u21
u31
u41
u12
u22
u32
u42
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−2
−2
−2
−2
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
+ π2
64
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
−1
−1
−1
−7
−7
−7
−7
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(12.28)

1010
Advanced Engineering Mathematics
The numerical solution of (12.28) was done by Mathematica, although MATLAB® would
have done just as well. The numerical results are in the rectangular array
2
2
2
2
2
2
2
0
1.46580
2.10184
2.36989
2.44390
2.36989
2.10184
1.46580
0
0
1.60714
2.41748
2.77959
2.88162
2.77959
2.41748
1.60714
0
0
1.46580
2.10184
2.36989
2.44390
2.36989
2.10184
1.46580
0
2
2
2
2
2
2
2
. ⃝
By the way, the exact solution can be found using separation of variables in the form
u(x, y) = v(y)+w1(x, y)+w2(x, y), where first we find v(y) = −1
2 y2−3
8 cos(4y), the solution
of the problem where the BCs have been replaced by zero on all four sides of the rectangle.
The exact solution values on our rectangular array are, after truncating the infinite series
for w1 and w2 at 80 terms each,
1.9625
1.9797
1.98446
1.98564
1.98446
1.9797
1.9625
−0.100339
1.41052
2.06223
2.32217
2.39234
2.32217
2.06223
1.41052
−0.100339
−0.007893
1.5117
2.30692
2.65895
2.75649
2.65895
2.30692
1.5117
−0.007893
0.078020
1.44556
2.06953
2.32375
2.39296
2.32375
2.06953
1.44556
0.078020
1.97532
1.98664
1.98977
1.99055
1.98977
1.98664
1.97532
.
Increasing the number of terms in each series from 80 to 200 improves the agreement with
the exact BCs but has little effect on the values inside the rectangle. So, we see that the finite
difference values are correct to within an error of about 5%. The chief problem seems to
be that using △y = π
8 is too coarse to capture the oscillation of the term cos(4y) in the
inhomogeneity in the PDE.
12.3.1 Other Boundary Conditions
If a BC on an edge of the rectangle involves the normal derivative, for example,
∂u
∂y

x, π
2

,
0 < x < π,
on the horizontal boundary y = π
2 , we can approximate that derivative in many different
ways. Pick any n with 0 < n < N. At a boundary point (xn, yM), we can use ∂u
∂y(xn, yM) ≈

Numerical Methods II
1011
(a)
un,M −un,M−1
h
,
using forward difference
(b)
un,M+1 −un,M
h
,
using forward difference
(c)
un,M+1 −un,M−1
2h
,
using central difference
The simplest choice is to use (a) because it uses only approximate solution values at
points that will also be used in the five-point Laplacian at (xn, yM−1), that is,
∇2u(xn, yM−1) ≈h−2 
un,M−2 + un,M + un−1,M−1 + un+1,M−1 −4un,M−1

.
Both (b) and (c) involve the fictitious approximate solution value un,M+1 ≈u(xn, yM+1)
at a point that is not in the rectangle. But the central difference approximation, (c), is
appealing because it has higher accuracy than a forward difference approximation. If we
use either (b) or (c), we must supplement the equations by using an approximation of the
PDE ∇2u = F(x, y) at the boundary point (xn, yM), for example,
Fn,M ≈F(xn, yM) = ∇2u(xn, yM) ≈h−2 
un,M−1 + un,M+1 + un−1,M + un+1,M −4un,M

.
Remarks
We note that the methods we used in two space dimensions extend to finite difference
methods for regions in three space dimensions.
We also note that we can use iterative methods mentioned in Section 8.4, for example,
SOR or SSOR, to solve the system of linear algebraic equations produced by finite dif-
ference methods for potential equations. Indeed, such systems of PDEs were the original
context for the SOR method of David M. Young, Jr., whose thesis, “Iterative methods for
solving PDEs of elliptical type,” can be downloaded from the Internet.
12.3.2 Problems
In problems 1–7, find a numerical approximate solution of the problem using the specified
△x and △y. If possible, use symmetry to reduce the tedium of the work.
1.
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 ,
0 < x < 1,
0 < y < 1
u(0, y) = 0,
u(1, y) = 1,
0 < y < 1
u(x, 0) = u(x, 1) = 0,
0 < x < 1
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,
with △x =△y = 1
3.
2. 0 = ∂2u
∂x2 + ∂2u
∂y2 ,
(x, y) in D, with △x =△y = 1
2, where Figure 12.3 shows the shaded
region D, which is a rectangle from which the lower right has been cut out. Con-
secutive dots in Figure 12.3 are 1
2 unit apart. The numerical values of u(x, y) on the
boundary of D are shown in Figure 12.3.

1012
Advanced Engineering Mathematics
y
1
0
1
0
1
x
1
0
1
2
2
1
3
0
2
x= 2
FIGURE 12.3
Problem 12.3.2.2.
y
1
x
1
1
4
3
4
FIGURE 12.4
Problem 12.3.2.3.
3.
⎧
⎪⎨
⎪⎩
−1 = ∂2u
∂x2 + ∂2u
∂y2 ,
(x, y) in D
u = 0 on the boundary of D
⎫
⎪⎬
⎪⎭
,
with △x =△y = 1
4, where Figure 12.4 shows the shaded region D, which is a 1 × 1
square from which the lower right 1
4 × 1
4 corner square has been cut out. The
dashed diagonal line shows symmetry in the problem.
4.
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
−1 = ∂2u
∂x2 + ∂2u
∂y2 ,
(x, y) in D
u = 0 on the outer boundary of D
u = 1 on the inner boundary of D
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
,

Numerical Methods II
1013
y
1
x
1
FIGURE 12.5
Problem 12.3.2.4.
with △x =△y =
1
4, where Figure 12.5 shows the shaded region D, which is a
1×1 square from which a 1
4 × 1
4 square has been cut out. The dashed diagonal line
shows symmetry in the problem.
5.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
−1 = ∂2u
∂x2 + ∂2u
∂y2 ,
0 < x < π,
0 < y < π
u(0, y) = u(π, y) = 4
u(x, 0) = 0,
∂u
∂y(x, π) = 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
with △x =△y = π
3 .
6.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
−1 = ∂2u
∂x2 + ∂2u
∂y2 , 0 < x < π, 0 < y < π
u(0, y) = u(π, y) = 2
∂u
∂y(x, 0) = ∂u
∂y(x, π) = 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
,
with △x =△y = π
3 .
7.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 ,
0 < x < π,
0 < y < π
u(0, y) = u(π, y) = 0
∂u
∂y(x, 0) = −5,
3u(x, π) + ∂u
∂y(x, π) = 6
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
with △x =△y = π
3 .

1014
Advanced Engineering Mathematics
12.4 Finite Difference Methods for the Wave Equation
The wave equation is
∂2u
∂t2 = c2 ∂2u
∂x2 ,
and we know from Section 10.5 that it has D’Alembert wave solutions
u(x, t) = f(x + ct) + g(x −ct),
whose two wave speeds are ∓c, respectively. We have seen also that the linearized
equations of gas dynamics, the system of two first-order PDEs in (10.66) in Section 10.4,
that is,
⎧
⎪⎨
⎪⎩
∂ϱ1
∂t = −ϱ0
∂u1
∂x
ϱ0
∂u1
∂t = −P′(ϱ0) ·
∂ϱ1
∂x
⎫
⎪⎬
⎪⎭
,
(12.29)
can be rewritten as the wave equation, where c =

P′(ϱ0).
Now we will rewrite (12.29) in another way that is useful for our discussion of numerical
methods: Define w1(x, t) = ϱ0u1(x, t) and w2(x, t) = c ϱ1(x, t). In Problem 12.4.3.1, you will
explain why (12.29) can be rewritten as
∂
∂t
w1
w2

= −c B ∂
∂x
w1
w2

,
(12.30)
where B =
0
1
1
0

.
12.4.1 Scalar Hyperbolic Problem
We will begin by studying the scalar hyperbolic problem
∂w
∂t = −c ∂w
∂x ,
that is,
∂w
∂t + c ∂w
∂x = 0.
(12.31)
Here, the wave speed c may be either positive or negative.
As we used for the heat equation, discretize space and time by
xj = jh,
j = 0, 1, 2, . . . ,
and
tm = m △t,
m = 0, 1, 2, . . . .,

Numerical Methods II
1015
and define
wm
j = w(xj, tm).
We may also write h =△x.
It is customary to use the superscript m rather than (m); keep in mind that wm
j is not the
mth power of wj.
The natural way to discretize (12.31) would be to use the forward Euler scheme: use the
forward time difference approximation
∂w
∂t (xj, tm) ≈
wm+1
j
−wm
j
△t
.
(12.32)
It is also natural to use the central space difference approximation
∂w
∂x (xj, tm) ≈
wm
j+1 −wm
j−1
2h
,
(12.33)
which we know from Problem 8.6.1.8 has accuracy O(h2).
The replacement equations,
wm+1
j
−wm
j
△t
+ c
wm
j+1 −wm
j−1
2h
= 0,
(12.34)
are consistent with the PDE (12.31) and simple to implement; unfortunately, if c ̸= 0, they
are unstable and thus useless! This is an example of the saying that “For every problem,
there is a simple answer that is wrong.”
There are no BCs, so for the stability analysis, we can substitute into the PDE (12.34) a
product solution in the general form
wm
j = Qmeiαj,
for j = 0, 1, 2, . . . ,
m = 0, 1, 2, . . . .,
(12.35)
where i =
√
−1 and α is real. From (12.34), this gives
Qm+1eiαj −Qmeiαj + ϵ
2

Qmeiα(j+1) −Qmeiα(j−1)
= 0,
where
ϵ ≜c △t
△x .
Dividing through by Qmeiαj and solving for Q gives the expression for the amplification
factor, as in Section 12.2, also known as the dispersion equation,
Q = 1 −ϵ
2

eiα −e−iα
= 1 −iϵ sin α;
(12.36)

1016
Advanced Engineering Mathematics
hence
|Q|2 = 1 + ϵ2 sin2 α.
Because we are free to choose α ̸= 0, |Q|2 > 1; hence, |Q| > 1, which shows that the
numerical scheme is unstable.
In Problem 12.4.3.2 you will explain why using a forward Euler scheme consisting
of the forward time difference approximation (12.32) and the forward space difference
approximation,
∂w
∂x (xj, tm) ≈
wm
j+1 −wm
j
h
,
(12.37)
is also unstable if the wave speed c > 0.
If the wave speed c < 0, then the forward Euler scheme consisting of the forward time
difference approximation (12.32) and the forward space difference approximation (12.37)
is stable.
The latter suggests the scheme of the forward time difference approximation (12.32) and
the backward space difference approximation,
∂w
∂x (xj, tm) ≈
wm
j −wm
j−1
h
.
(12.38)
In fact, this is stable, as long as the wave speed c > 0, as you will explain in Problem
12.4.3.3.
12.4.2 Lax Scheme
Peter Lax proposed using an averaged approximation of wm
j in approximating the time
derivative:
∂w
∂t (xj, tm) ≈(△t)−1 ·

wm+1
j
−
wm
j+1 + wm
j−1
2

.
(12.39)
Along with using the central space difference approximation for the space derivative, this
gives the Lax scheme for (12.31):
(△t)−1 ·

wm+1
j
−
wm
j+1 + wm
j−1
2

+ c
wm
j+1 −wm
j−1
2h
= 0.
(12.40)
The replacement equations are
(Lax scheme)
wm+1
j
= 1 −ϵ
2
wm
j+1 + 1 + ϵ
2
wm
j−1,
(12.41)
where, as earlier,
ϵ ≜c △t
△x .

Numerical Methods II
1017
In Problem 12.4.3.4, you will explain why the Lax scheme is numerically stable as long as
|ϵ| ≤1, which is the Courant–Friedrichs–Lewy (CFL) criterion.
Learn More About It
Computational Fluid Mechanics and Heat Transfer, 2nd edn., by John C. Tannehill, Dale
A. Anderson, and Richard H. Pletcher, Taylor & Francis,
c⃝1997, and Numerical
Recipes: The Art of Scientific Computing, 3rd edn., by William H. Press et al., Cambridge
University Press c⃝2007, are among many good resources for learning about finite dif-
ference methods for wave equations and, more generally, conservation laws such as
the Euler PDE (7.41) in Section 7.6, that is, (10.2) in Section 10.1. As you may see, our
study barely scratch the surface of this topic’s practical and theoretical aspects, which
include the CFL criterion.
12.4.3 Problems
1. Explain why (12.29) can be rewritten as (12.30).
2. Use the following steps to help explain why using a forward Euler scheme con-
sisting of the forward time difference approximation (12.32) and the forward space
difference approximation (12.37) is unstable if the wave speed c > 0.
(a) Substitute into the replacement a product solution of the form (12.35), perform
algebraic manipulations to arrive at the dispersion equation
Q = 1 + ϵ −ϵ eiα,
and then find the real and imaginary parts of Q.
(b) Perform algebraic manipulations to get an expression for the square of the
modulus of the amplification factor, namely,
|Q|2 = 1 + 2ϵ(1 + ϵ)(1 −cos α),
and use that to show that the numerical scheme is unstable if the wave speed
c > 0.
(c) Show that the numerical scheme is stable if c < 0.
3. Explain why using the scheme for (12.31) of using the forward time difference
approximation (12.32) and the backward space difference approximation (12.38),
is stable, as long as the wave speed c > 0.
4. Explain why the Lax scheme (12.41), that is, replacement equations
wm
j+1 = 1 −ϵ
2
wm
j+1 + 1 + ϵ
2
wm
j−1,
is numerically stable as long as |ϵ| ≤1.

1018
Advanced Engineering Mathematics
5. Study the stability of the scheme for (12.31) of using the backward time difference
approximation
∂w
∂t (xj, tm) ≈
wm
j −wm−1
j
△t
and the forward space difference approximation (12.38).
12.5 Short Take: Galerkin Method
In this section, we will use “Galerkin methods” to approximate the solution of an ODE-
BVP or a PDE-BVP using a finite dimensional subspace of functions.
Suppose L < ∞and functions f(x) and g(x) are square integrable on the interval [ 0, L ],
for example,
L
0
| f(x)|2 dx < ∞.
In particular, if f(x) is continuous on [ 0, L ], then f(x) is square integrable on [ 0, L ].
Define an inner product by
⟨f, g⟩≜
L
0
f(x) g(x) dx.
Suppose u(x) satisfies an ODE-BVP of the form
⎧
⎨
⎩
−

p(x)u′′ + q(x)u = f(x)
u(0) = u(L) = 0
⎫
⎬
⎭.
(12.42)
For convenience, we define a differential operator A on a function u by
(Au)(x) ≜−

p(x)u′(x)
′ + q(x)u(x).
(12.43)
Define a space of functions V ≜C2
0[0, L] by
u is in V if u(x) is twice continuously differentiable on [0, L] and u(0) = u(L) = 0. (12.44)
In the notation C2
0, the subscript 0 refers to u satisfying the zero Dirichlet BCs u(0) =
u(L) = 0 and the superscript 2 refers to u being twice continuously differentiable.
ODE-BVP (12.42) can be stated abstractly as
Au = f
and
u is in V.
(12.45)

Numerical Methods II
1019
The Galerkin method is to pick an N-dimensional vector subspace VN and a set of
functions {φ1(x), . . . , φN(x)} that is a basis for VN ⊆V. We approximate
u(x) ≈uN(x) ≜
N

n=1
cnφn(x)
and substitute that into the abstract form of the ODE-BVP to get
N

n=1
cnAφn(x) = f(x),
(12.46)
using the linearity of A. Take the inner product of both sides of (12.46) with φm(x) to get
N

n=1
⟨φm, Aφn⟩cn = ⟨φm, f⟩,
m = 1, . . . , N.
(12.47)
This is a system of N linear equations in N unknowns c1, . . . , cN.
But (12.47) can be rewritten as
A c = f,
(12.48)
where c = [ c1
...
cN ]T and
f = [ ⟨φ1, f⟩
...
⟨φN, f⟩]T
are vectors in RN and A is the N × N matrix defined by
A ≜[ ⟨φm, Aφn⟩] 1 ≤m ≤N
1 ≤n ≤N
.
(12.49)
Definition 12.1
Operator A is coercive if there exists a positive constant α such that for all v in V,
⟨Av, v⟩≥α⟨v, v⟩.
(12.50)
Note that if the vector space is Rn and A is a real n × n matrix, then A being coercive is
the same as A being positive definite, as in Definition 2.19 in Section 2.6.
Theorem 12.3
If A coercive and {φ1(x), . . . , φN(x)} is a linearly independent set of functions in V, then
the N × N matrix A given by (12.49) is invertible; hence, we can solve (12.48) for c.

1020
Advanced Engineering Mathematics
Why? If A is not invertible, then there must exist a vector c ̸= 0 such that Ac = 0. Defining
a function v(x) ≜N
n=1 cnφn(x), we have
0 =
N

n=1
⟨φm, Aφn⟩cn =

φm, A
 N

n=1
cnφn

= ⟨φm, Av⟩,
for m = 1, . . . , N;
(12.51)
the latter follows from using linearity of A. From (12.51) we conclude that
0 + · · · + 0 =
N

m=1
cm⟨φm, Av⟩=
N

m=1
⟨cmφm, Av⟩=

N

m=1
cmφm, Av

= ⟨v, Av⟩≥α⟨v, v⟩= α||v||2 ≥0;
hence, ||v|| = 0; hence, 0 = v ≜N
n=1 cnφn. But this would contradict the linear indepen-
dence of {φ1(x), . . . , φN(x)} because c ̸= 0. So, A must be invertible. 2
Example 12.7
Use the Galerkin method to approximate the solution of the ODE-BVP
⎧
⎨
⎩
−y′′(x) + xy = 1 + x,
0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭.
(12.52)
Method: It makes sense for the functions φn(x) to satisfy the BCs. Let’s use
φ1(x) = sin πx,
φ2(x) = sin 2πx,
φ3(x) = sin 3πx,
that is, the approximate solution is to be of the form
y(x) = c1 sin πx + c2 sin 2πx + c3 sin 3πx.
The differential operator A is defined by
Ay = −y′′(x) + xy
and the inner product is given by ⟨u, v⟩≜
 1
0 u(x)v(x)dx. Define f(x) = 1 + x. Using
Mathematica, for example, to calculate
a12 ≜⟨φ1, Aφ2⟩=
1
0
sin πx(−(sin 2πx)′′ + x sin 2πx)dx
=
1
0
(−4 + x) sin πx sin 2πx dx = −8
9π2 ,
we calculated the matrix
A ≜[⟨φm, Aφn⟩] 1 ≤m ≤3
1 ≤n ≤3
=
⎡
⎢⎢⎢⎢⎢⎣
1
4 + π2
2
−8
9π2
0
−8
9π2
1
4 + 2π2
−24
25π2
0
−24
25π2
1
4 + 9π2
⎤
⎥⎥⎥⎥⎥⎦

Numerical Methods II
1021
0.15
y
0.10
0.05
x
0.2
0.4
0.6
0.8
1.0
FIGURE 12.6
Example 12.7.
and the vector
f = [ ⟨φ1, f⟩... ⟨φN, f⟩]T =
 3
π
−1
2π
1
π
T
.
So,
c = [ c1
c2
c3 ]T = A−1f ≈[ 0.184055
−0.00709816
0.00711143 ]T .
In Figure 12.6, we graph the approximate solution
y(x) ≈0.184055 sin(πx) −0.00709816 sin(2πx) + 0.00711143 sin(3πx)
as a dashed curve; Mathematica produced a numerical approximate solution of the ODE-
BVP using NDSolve, and then we graphed it as the solid curve. ⃝
So, even the Galerkin method using only a three dimensional subspace V of the infi-
nite dimensional space C2
0[0, L] can produce a very good approximate solution of an
ODE-BVP.
12.5.1 A Generalization of the Galerkin Method
Consider again (12.45), that is, Au = f, u in V, where A is linear. As earlier, suppose we
approximate
u(x) ≈uN(x) ≜
N

n=1
cnφn(x),
and substitute that into Au = f to get (12.46), that is,
N

n=1
cnAφn(x) = f(x).

1022
Advanced Engineering Mathematics
Let {ψm}N
m=1 be a basis for an N dimensional subspace of V. Take the inner product of both
sides of (12.46) with ψm(x) to get
N

n=1
⟨ψm, Aφn⟩cn = ⟨ψm, f⟩,
m = 1, . . . , N.
(12.53)
This is a system of N linear equations in N unknowns c1, . . . , cN.
A particularly useful case is when we choose the functions {ψm}N
m=1 to be a dual basis
for the subspace spanned by the set of functions {Aφn}N
n=1, meaning that
⟨ψm, Aφn⟩= δmn =
1,
if m = n
0,
if m ̸= n
 
.
In this very special case, the solution of (12.53) is given simply by
cn = ⟨ψn, f⟩;
hence, the approximate solution is
u(x) ≈uN(x) ≜
N

n=1
⟨ψn, f⟩φn(x).
(12.54)
As an aside, suppose that, instead of a function space, we have V = Rm with the usual,
Euclidean inner product, so an element of V would be a vector u and the right-hand side
of Au = f would be a vector f. In that situation, (12.54) would be
u =
 N

n=1
φnψT
n

f.
This formula is reminiscent of formula (2.61) in the context of the SVD (singular value
decomposition).
12.5.2 The Galerkin Method for PDEs
Of course, we can approximate the solution of a PDE, too.
Example 12.8
Use the Galerkin method to approximate the solution of the PDE-BVP
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂2u
∂x2 + ∂2u
∂y2 = −(1 + x + y), 0 < x < 1, 0 < y < 1
u(0, y) = u(1, y) = 0
u(x, 0) = u(x, 1) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(12.55)

Numerical Methods II
1023
Method: It makes sense for the functions φn(x) to satisfy the BCs. Let’s use
φ1(x, y) = xy(1 −x)(1 −y),
φ2(x, y) = x2y(1 −x)(1 −y),
φ3(x, y) = xy2(1 −x)(1 −y),
and so the approximate solution is to be of the form
y(x) = c1xy(1 −x)(1 −y) + c2x2y(1 −x)(1 −y) + c3xy2(1 −x)(1 −y).
The Laplacian differential operator A =  is defined by
Ay = ∂2y
∂x2 + ∂2y
∂y2
for functions that satisfy the zero Dirichlet BCs in (12.55). The inner product is given by
⟨u, v⟩≜
 1
0
 1
0 u(x, y)v(x, y)dx dy. Using Mathematica we calculated the matrix
A ≜[⟨φm, Aφn⟩] 1 ≤m ≤3
1 ≤n ≤3
=
⎡
⎢⎢⎢⎢⎣
−1
45
−1
90
−1
90
−1
90
−4
525
−1
180
−1
90
−1
180
−4
525
⎤
⎥⎥⎥⎥⎦
and the vector
f = [ ⟨φ1, f⟩
⟨φ2, f⟩
⟨φ3, f⟩]T =

−1
18
−
7
240
−
7
240
T
,
where f(x, y) = −(1 + x + y). So,
c = [ c1
c2
c3 ]T = A−1f =
 95
52
35
52
35
52
T
.
So, the approximate solution is
u(x, y) = 5
52 xy(1 −x)(1 −y)

19 + 7x + 7y

. ⃝
Figure 12.7a shows the graph of the approximate solution. Figure 12.7b shows the graph
of the absolute value of the error, u(x, y) −v(x, y), where v(x, y) is a truncation of the exact
solution obtained by the method of Section 11.4. The exact solution would be
v(x, y) =
∞

m=1
∞

m=1
4(1 −2(−1)m −2(−1)n + 3(−1)m+n)
mn

m2 + n2
π4
sin(mπx) sin(nπy) .
It appears that the error is not very big, although the error is relatively great near the sides
x = 1 and y = 1.
12.5.3 Nonlinear Problems
Certainly Galerkin method is useful for problems such as Example 12.7 where the non-
constant coefficients ODE does not have simple solutions. Even more useful is Galerkin

1024
Advanced Engineering Mathematics
0.15
1.0
0.5
0.0
x
0.10
0.05
z
0.00
0.0
0.5
y
0.008
1.0
0.5
0.0
x
0.006
0.004
z
0.000
0.002
0.0
0.5
y
(a)
(b)
1.0
1.0
FIGURE 12.7
Example 12.8.
method for nonlinear problems. While we will not include such an example, we note that
having a nonlinear differential operator A means that
A
 N

n=1
cnφn(x)

= f(x)
cannot be reduced to (12.46). Here, the Galerkin method is to solve the system of nonlinear
equations
!
φm, A
 N

n=1
cnφn
"
= ⟨φm, f⟩,
m = 1, . . . , N,
(12.56)
for c1, . . . , cN. Newton’s method or its variants, such as the secant method, can be used to
solve (12.56). Or you can use Mathematica’s NSolve command to solve a nonlinear system
of equations.
12.5.4 Problems
In problems 1–5, use Galerkin method to approximate the solution of the ODE- or
PDE-BVP.
1.
⎧
⎨
⎩
−y′′(x) + x2y = 1 + x,
0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭
2.
⎧
⎨
⎩
−y′′(x) + xy = 1 + x,
0 < x < 1
y(0) = y′(1) = 0
⎫
⎬
⎭
[Hint: See Table 9.1, which implicitly gives functions that satisfy the BCs in
Problem 12.5.4.2.]

Numerical Methods II
1025
3.
⎧
⎨
⎩
−y′′(x) + xy = 1 + x, 0 < x < 1
y′(0) = y(1) = 0
⎫
⎬
⎭
[Hint: See Table 9.1, which implicitly gives functions that satisfy the BCs in
Problem 12.5.4.3.]
4.
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂2u
∂x2 + ∂2u
∂y2 = −(1 + xy), 0 < x, y < 1
∂u
∂x(0, y) = u(1, y) = 0
u(x, 0) = u(x, 1) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
[Hint: φ3(x, y) = x2y(1 −x)(1 −y) is one example of a function that satisfies all
of the BCs. See Table 9.1, which implicitly gives product functions that satisfy the
BCs in Problem 12.5.4.4.]
5.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂2u
∂x2 + ∂2u
∂y2 = −(1 + xy), 0 < x, y < 1
∂u
∂x(0, y) = u(1, y) = 0
∂u
∂yu(x, 0) = u(x, 1) = 0
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
[Hint: φ1(x, y) = cos
 πx
2

cos
 πy
2

and φ3(x, y) = x2y2(1−x)(1−y) are two examples
of a function that satisfies all of the BCs.]
Key Terms
amplification factor: (12.16) in Section 12.2
coercive: Definition 12.1 in Section 12.5
consistent numerical method: beginning of Section 12.2
convergent numerical method: beginning of Section 12.2
Crank–Nicholson method: (12.18) in Section 12.2
discretization matrix: after (12.6) in Section 12.1
dispersion equation: (12.36) in Section 12.4
dual basis: after (12.53) in Section 12.5
five point Laplacian: (12.27) in Section 12.3
forward Euler scheme: (12.32) in Section 12.4
Galerkin method: before and in (12.47) in Section 12.5
generalization of the Galerkin method: (12.53) in Section 12.5
implicit partial difference equation: (12.19) in Section 12.2
Lax scheme: (12.40) in Section 12.4
replacement equations: (12.2) in Section 12.1
scalar hyperbolic problem: (12.31) in Section 12.4
stable numerical method: beginning of Section 12.2
stencil: (12.2) in Section 12.1
Reference
Abramowitz, M. and Stegun I. (eds.). Handbook of Mathematical Functions. Dover Publications,
New York, 1964.


13
Optimization
13.1 Functions of a Single Variable
In this chapter, we will study methods for some of the more practical problems engineers
face to optimize use of resources, performance characteristics, or financial gain. In addi-
tion, some of the techniques and ideas are relevant to the subject of calculus of variations
in Chapter 14.
Companies spend millions of dollars on research to save 1% or 2% of the weight of an
automobile or get a 1% or 2% improvement of the fuel efficiency of an airplane engine.
Often there is an interplay between numerical methods for modeling and solution of
equations and the optimization of a performance characteristic(s).
Definition 13.1
Suppose x is a single real variable and f(x) is a function whose domain is an interval I.
(a) x⋆is a global minimizer of f(x) on I if
f(x⋆) ≤f(x) for all x in I.
In this case, we say f(x⋆) is the global minimum (value) of f(x) on I.
(b) x⋆is a local (or relative) minimizer of f(x) on I if there is a δ > 0 such that
f(x⋆) ≤f(x) for all x in I with |x −x⋆| < δ.
In this case, we say f(x⋆) is a local minimum (value) of f(x) on I.
The definitions of “global maximizer” and “local (or relative) maximizer” are similar,
for example, x⋆is a global maximizer of f(x) on I if f(x⋆) ≥f(x) for all x in I.
Theorem 13.1
Suppose f(x) is a continuous function on a closed, finite interval I = [a, b]. Then f has a
global minimizer (global maximizer) in I.
1027

1028
Advanced Engineering Mathematics
Definition 13.2
f(x) has a critical number x0 if f ′(x0) is either zero or does not exist.
Theorem 13.2
(Fermat’s theorem) Suppose f(x) is a function on an interval I and x⋆is strictly inside I,
that is, there is a δ > 0 such that the interval (x⋆−δ, x⋆+ δ) ⊆I. If f has a local minimum
or maximum at x⋆, then x⋆is a critical number of f.
Corollary 13.1
If f(x) is a continuous function defined on a closed, finite interval [ a, b ] then each of its
global minimizer(s) and maximizer(s) is either a critical number or an endpoint a or b.
Corollary 13.1 gives us a theoretical method for finding the global minimizer and
maximizer of a continuous function on a closed, finite interval.
Example 13.1
A cylinder of radius ri and length L is carrying matter whose temperature is Ti. The cylin-
der has insulation from radius ri to radius ro, where ro > ri. As depicted in Figure 13.1,
the temperature outside the cylinder is To < Ti. Here, i stands for the inside of the
insulation, and o stands for its outside.
According to the classical theory, the rate of convective and conductive heat loss from
the insulated cylinder is (See Aziz, 1997)
q =
β
h ln (ro/ri) +

k/ro
,
where
h is the convective heat transfer coefficient for the insulation material
k is the conductive heat transfer coefficient for the insulation material
β = 2πkhL(Ti −T∞)
T∞is the (constant) temperature of the medium
Discuss the dependence of the convective heat loss on the outer radius, ro.
T=To
T=Ti
ro
ri
FIGURE 13.1
Example 13.1.

Optimization
1029
Method: Let r = ro for notational convenience and define the function
f(r) ≜
β
h ln (r/ri) +

k/r
.
Let h, k, L, ri, Ti, and To be constant parameters. On its domain ri ≤r < ∞, we calculate
f ′(r) = · · · = −
β

h ln (r/ri) +

k/r
2 ·
h
r −k
r2

,
so the only possible critical point is at r⋆=
k
h, which is called the critical radius of
insulation. It exists only if k
h > ri.
Because
f ′(r) = · · · = −
β(hr −k)
r2 
h ln (r/ri) +

k/r
2 ,
we see that f ′(r) > 0 for r < r⋆and f ′(r) < 0 for r ≥r⋆. By Theorem 13.3 mentioned later,
the global maximum of the convective heat transfer is at ro = r⋆, assuming the critical
radius of insulation exists.
The paradoxical result is that if ri < r⋆= k
h, then an insulated cylinder can lose more
heat by convection than an uninsulated cylinder. This can be useful if the cylinder is an
electrical wire that needs to dissipate heat.
On the other hand, if ri > r⋆= k
h, then adding insulation has the usually desired effect
of decreasing heat dissipation from a pipe of radius ri carrying, for example, hot water
or steam. ⃝
Aziz goes on to discuss a more recent theory of forced convection and shows that the
classical theory greatly underestimates the maximum heat transfer rate.
13.1.1 Global Optimization Result
In Section 4.2, we discussed the maximum amplitude of steady-state oscillations for a sinu-
soidally forced second-order linear ordinary differential equation. At that time, we stated
essentially the same result as the next theorem.
Theorem 13.3
Suppose a function f(x) is defined and differentiable for x in the interval (a, ∞), for some
a. If f ′(x) is positive for a < x < x⋆and f ′(x) is negative for x⋆< x < ∞, then the global
maximum value of f on (a, ∞) is f(x⋆).
13.1.2 Convex Functions and Optimization
Definition 13.3
A function f = f(x) defined on an interval I is
(a) Convex if
f

(1 −t)x + ty

≤(1 −t)f(x) + tf(y), for all 0 ≤t ≤1, x, y in I
(13.1)

1030
Advanced Engineering Mathematics
f(x)
x
a
b
FIGURE 13.2
Secant line above graph of convex function.
(b) Strongly (or strictly) convex if
f

(1 −t)x + ty

< (1 −t)f(x) + tf(y), for all 0 < t < 1, x ̸= y in I
(13.2)
Figure 13.2 illustrates that the graph of a convex function on an interval [ a, b ] lies below
the secant line connecting the points (a, f(a)), (b, f(b)).
In Problem 13.1.3.9, you will explain why a twice continuously differentiable function
of a single variable defined on an interval is convex if, and only if, its second derivative is
non-negative on that interval. So, “convex” is what is called “concave up” in Calculus I.
Theorem 13.4 ∗
Suppose f(x) is a convex function on an interval I and x⋆is in I. If x⋆is a local minimizer
for f, then it is a global minimizer for f on I.
Why? Suppose, to the contrary, that there is an x⋆that is a local minimizer for f but is not
a global minimizer for f on I. x⋆being a local minimizer implies there is a δ > 0 such that
f(x⋆) ≤f(x) for all x in I with |x −x⋆| < δ. x⋆not being a global minimizer implies there is
ax such that f(x) < f(x⋆), which implies that
x⋆−x
 ≥δ. Define t = 0.9δ/
x⋆−x
. We see
that 0 < t < 1. Note for future reference that
x⋆−x
 = 0.9δ/t.
Define x ≜tx + (1 −t)x⋆. By the convexity of f,
f(x) = f

tx + (1 −t)x⋆
≤tf
x

+ (1 −t)f

x⋆
< tf

x⋆
+ (1 −t)f

x⋆
= f

x⋆
.
(13.3)
But, x = tx + (1 −t)x⋆satisfies
|x⋆−x| =
x⋆−tx −(1 −t)x⋆ =
t

x⋆−x
 = t ·
x⋆−x
 = t · (0.9δ/t) = 0.9δ;
by the local minimizer assumption, this implies f(x) ≥f(x⋆), contradicting (13.3). So, the
local minimizer must be the global minimizer on the interval I. 2
13.1.3 Problems
1. A page of a book is to contain 40 in.2 of print. If the margins of both the top and
bottom sides must be 2 in. and the margins of both the left and right sides must be
1 in., find the dimensions of the paper that minimize its area.
∗This theorem is from Cheney (1982).

Optimization
1031
2. Inside a circular pipe whose inner diameter is 25 cm, a machine part is rotating
anomalously because its motion is not perfectly circular. Suppose a point on it
describes the parametric motion
r(t) = (10 cos t + sin3 t) ˆj + (10 sin t + cos3 t) ˆj.
Find the closest that the point gets to the inner wall of the pipe. At what times
does that happen?
3. A rectangular beam is to be cut from a log with a circular cross section of diam-
eter 1 m. If the strength of the beam is proportional to the product of its cross
section’s width and the square of its cross section’s length, find the cross-section
dimensions of the strongest beam.
4. A closed rectangular box has a square base and a volume of 20 ft3. Because of the
weight of its contents, the box is constructed from two different types of material.
The material for the sides costs $2/ft2, and the stronger material for the top and
bottom costs $3/ft2. Design the dimensions of the box whose total material cost is
the least.
5. Suppose that in a small town, the number of people who realize they have con-
tracted a severe influenza is N(t) ≜
2000
1+999e−2t , where t is measured in days. Then
˙N(t) measures the rate at which people become aware that they have the illness.
When will the town’s small hospital have to be ready for the greatest influx of
patients?
6. In the paper by Aziz mentioned in Example 13.1, he also studied heat dissipation
from an insulated sphere. He stated that the rate of convective and conductive
heat loss is
q = β/
h(ro −ri)
riro
+ k
r2o

,
where
β = 4πkh(Ti −T∞),
h is the convective heat transfer coefficient for the insulation material
k is the conductive heat transfer coefficient for the insulation material
Discuss the dependence of the convective heat loss on the outer radius, ro.
7. In the tropical island nation of Quovideo, every person is entitled to a plot of land
in the shape of a right triangle and having 1000 m2 area. Find the dimensions of
the plot that uses the least amount of perimeter fencing.
8. Suppose that a rectangle has its base on the x-axis and two of its vertices on the
curve y =
1
x2+1, and suppose that the rectangle is symmetric about the y-axis. Find
the largest possible area of such a rectangle.
9. Explain why a twice continuously differentiable function of a single variable
defined on an interval is convex if, and only if, its second derivative is nonnegative
on that interval.
10. (a) Use the result of Problem 13.1.3.9 to explain why the function f(x) ≜−ln x is
convex on the interval (0, ∞).

1032
Advanced Engineering Mathematics
(b) Use the result of part (a) to explain why ln

(1 −t)x + ty

≥(1 −t) ln(x) +
t ln(y), for all 0 ≤t ≤1, positive x, y.
(c) Use the result of part (b) to explain why x1−tyt ≥(1 −t)x + ty, for all 0 ≤t ≤1,
positive x, y.
11. Suppose f(x) is a convex function of x in an interval I and M is any real number.
Explain why S ≜{x in I : f(x) ≤M} is either an interval or empty.
12. (a) Suppose both f(x) and g(x) are convex functions defined on a common interval
I. Explain why f(x) + g(x) is also convex on I.
(b) Give an example where f(x) is convex on an interval J , g(x) is convex on an
interval I, and g(I) ⊆J, yet f(g(x)) is not convex on I.
13. Explain why a twice continuously differentiable function of a single variable
defined on an interval is strictly convex if its second derivative is greater than
or equal to a positive constant γ on that interval.
14. Suppose f(x) is strictly convex on an interval I. Explain why f cannot have two
distinct global minimizers in I.
13.2 Functions of Several Variables
Suppose f(x) is a function which has domain D in Rn. The minimization problem is
⎧
⎨
⎩
Minimize
f(x)
Subject to
x is in D
⎫
⎬
⎭.
(13.4)
We call f = f(x) an objective function.
Definition 13.4
Suppose f(x) is a function which has domain D in Rn.
(a) x⋆is a global minimizer of f(x) on D if
f(x⋆) ≤f(x) for all x in D.
In this case we say f(x⋆) is the global minimum (value) of f(x) on D.
(b) x⋆is a local (or relative) minimizer of f(x) on D if there is a δ > 0 such that
f(x⋆) ≤f(x) for all x in D with ||x −x⋆|| < δ.
In this case, we say f(x⋆) is a local minimum (value) of f(x) on D.
(c) A unit vector d is a feasible direction at x0 in D if there is an α > 0 such that the
line segment {x0 + α ˆd : 0 ≤α ≤α} is in D.

Optimization
1033
FIGURE 13.3
Feasible directions.
Definition 13.4(c) is illustrated in Figure 13.3. The concept of “feasible direction” is based
only on the nature of D, that is, is not influenced by the choice of objective function f.
Theorem 13.5
If x⋆is a local minimizer for (13.4) and d is a feasible direction at x⋆in D, then either
d • ∇f(x⋆) ≥0
or ∇f(x⋆) does not exist.
Why? If ∇f(x⋆) exists, define a function of a single variable by g(α) = f(x⋆+αd). The chain
rule gives
g′(α) = d • ∇f(x⋆+ αd),
which is the directional derivative of f in the direction of d at the point x⋆+ αd. Because x⋆
is a local minimizer for (13.4),
0 ≥lim
α→0+
g(α) −g(0)
α
= g′(0+) = d • ∇f(x⋆). 2
Theorem 13.5 says that there is an interaction between the domain and the objective
function at a minimizer.
Definition 13.5
Suppose D is a set in Rn and x0 is in D. If there is some solid, open ball Br(x0) ≜{x :
||x −x0|| < r} that is contained in D for some r > 0, then we say x0 is in the interior of D.
If x0 is in the interior of D, then every direction is feasible at x⋆in D, by using α = r
2.
Definition 13.6
(a) x0 is a critical point of f = f(x) if ∇f(x0) is either 0 or does not exist.
(b) x0 is a stationary point of f = f(x) if ∇f(x0) = 0.

1034
Advanced Engineering Mathematics
Example 13.2
Find all of the critical points of f(x, y) ≜(2 −x2)(3y −y2).
Method: ∇f(x) exists at all x, so we need only to find all (x, y) for which both ∂f
∂x = 0 and
∂f
∂y = 0, that is,
⎧
⎨
⎩
−2x(3y −y2)
= 0
(2 −x2)(3 −2y)
= 0
⎫
⎬
⎭.
(13.5)
The first equation is true when (1) x = 0 or (2) y = 0 or (3) y = 3. The second equation
is true when (4) x = −
√
2 or (5) x =
√
2 or (6) y = 3
2. For both equations in (13.5) to be
simultaneously true, we must have both one of (1), (2), or (3) and one of (4), (5), or (6).
This is like a strangely dictatorial restaurant that insists that you buy exactly one item
from “column A” and exactly one item from “column B.”
In principle, there are nine possibilities:
(1)
and
(4) :
x = 0
and
x = −
√
2 : impossible
(1)
and
(5) :
x = 0
and
x =
√
2 : impossible
(1)
and
(6) :
x = 0
and
y = 3
2 : (x, y) = (0, 3
2)
(2)
and
(4) :
y = 0
and
x = −
√
2 : (x, y) = (−
√
2, 0)
(2)
and
(5) :
y = 0
and
x =
√
2 : (x, y) = (
√
2, 0)
(2)
and
(6) :
y = 0
and
y = 3
2 : impossible
(3)
and
(4) :
y = 3
and
x = −
√
2 : (x, y) = (−
√
2, 3)
(3)
and
(5) :
y = 3
and
x =
√
2 : (x, y) = (
√
2, 3)
(3)
and
(6) :
y = 3
and
y = 3
2 : impossible.
So, there are exactly five critical points: (x, y) = (±
√
2, 0), (±
√
2, 3), and (0, 3
2). ⃝
Theorem 13.6
(Fermat’s theorem) If x⋆is in the interior of D and is a local minimizer for (13.4), then x⋆is
a critical point of f.
Why? Because x⋆is in the interior of D, for every j = 1, . . . , n, both the standard unit basis
vector e(j) and its opposite, −e(j), are feasible direction. Theorem 13.5 implies
e(j) • ∇f(x⋆) ≥0
and

−e(j)
• ∇f(x⋆) ≥0,
or ∇f(x⋆) does not exist.
The latter of the two inequalities is equivalent to e(j) • ∇f(x⋆) ≤0, so we get
0 ≤e(j) • ∇f(x⋆) ≤0;

Optimization
1035
hence,
∂f
∂xj
(x⋆) = e(j) • ∇f(x⋆) = 0.
Because this is true for j = 1, . . . , n, ∇f(x⋆) is either 0 or does not exist. 2
A local maximizer for f(x) that lies in the interior of D must also be a critical point.
Definition 13.7
Suppose x0 is a critical point of f = f(x) and there are two feasible directions d1, d2 such
that for some α,
f(x0 + αd1) < f(x0) < f(x0 + αd2)
for 0 < α ≤α. Then we say f has a saddle point at x0.
A critical point may be a local minimizer, a local maximizer, a saddle point, or “none
of the above.” To find out the nature of a critical point, it helps to use the multivariable
generalization of Taylor’s theorem. We will state it for x = [ x
y ]T in R2 and then indicate
how it generalizes to Rn. Denote △r =
x −a
y −b

and recall the O notation from Definition
8.5 in Section 8.6.
Definition 13.8
If f = f(x, y) is at least twice differentiable at (a, b), the Hessian matrix is
[D2f(a, b)] ≜
⎡
⎢⎢⎢⎢⎣
∂2f
∂x2 (a, b)
∂2f
∂x∂y(a, b)
∂2f
∂x∂y(a, b)
∂2f
∂y2 (a, b)
⎤
⎥⎥⎥⎥⎦
.
The 2 × 2 real, symmetric Hessian matrix [D2f(a, b)] is a generalization of the concept of
second derivative to a function of two variables.
Note that
1
2!△rT[D2f(a, b) ]△r
= 1
2

∂2f
∂x2 (a, b)(x −a)2 + 2 ∂2f
∂x∂y(a, b)(x −a)(y −b) + ∂2f
∂x∂y(a, b)(y −b)2

.

1036
Advanced Engineering Mathematics
Theorem 13.7
(Taylor’s theorem in R2) Suppose f = f(x, y) is (N + 1) times continuously differentiable at
(a, b) for some N ≥2. Then for (x, y) sufficiently near (a, b),
f(x, y) = f(a, b) + 1
1! △r • ∇f(a, b) + 1
2! △rT[ D2f(a, b) ]△r
+ · · · + 1
N!
N

i=0
N
i

∂Nf
∂x i∂y N−i (a, b)(x −a)i(y −b)N−i + RN+1(a, b; x, y),
(13.6)
where the remainder term RN+1(a, b; x, y) satisfies
|RN+1(a, b; x, y)| = O


x −a
y −b
 

N+1
and depends upon the (N + 1)-th order derivatives
∂N+1f
∂x i∂y N+1−i (x, y) for (x, y) near (a, b).
Theorem 13.8
(Second derivative test) Suppose f = f(x, y) has a stationary point at (a, b) and is at least
three ∗times continuously differentiable at (a, b).
(a) If the Hessian matrix [D2f(a, b)] is positive definite,† that is, has only positive
eigenvalues, then (a, b) is a local minimizer.
(b) If the Hessian matrix [D2f(a, b)] is negative definite, that is, has only negative
eigenvalues, then (a, b) is a local maximizer.
(c) If the Hessian matrix [D2f(a, b)] has one positive eigenvalue and one negative
eigenvalue then (a, b) is a saddle point.
(d) If the Hessian matrix has a zero eigenvalue, then further information is needed
in order to determine the nature of the critical point at (a, b).
Why? If (a, b) is a stationary point of f, then (13.6) with N = 2 gives
f(x, y) = f(a, b) + 1
2! △rT[ D2f(a, b) ] △r + R3(a, b; x, y).
When discussing local behavior of f, as long as [D2f(a, b)] has no zero eigenvalue, then we
may assume that (x, y) is so close to (a, b) that the remainder term, R3(a, b; x, y), is insignifi-
cant compared to the quadratic terms. Then parts (a), (b), and (c) make sense. For example,
if [D2f(a, b)] has only positive eigenvalues, then positive definiteness of the 2 × 2 matrix
∗It is enough to assume that f is two, rather than three, times continuously differentiable, but “three times” is
appropriate when using the O notation, as in Definition 8.5 in Section 8.6.
† See Section 2.6 for the definition of positive definite.

Optimization
1037
[D2f(a, b)] = [ aij ] implies that a11(x −a)2 + 2a12(x −a)(y −b) + a22(y −b)2 > 0 for (x, y) ̸=
(a, b); hence, f(x, y) > f(a, b), that is, (a, b) is a local minimizer for f. 2
This theorem generalizes to Rn in place of R2 in a clear way, except in (c) we change the
hypothesis to “have at least one positive eigenvalue and at least one negative eigenvalue.”
By Theorem 2.31 in Section 2.6 and discussion before it, a 2 × 2 symmetric matrix [ aij ] is
positive definite if, and only if, a11 > 0 and 0 < det

[ aij ]

= a11a22 −a2
12.
On the other hand, if a symmetric 2×2 matrix has negative determinant you will explain
in Problem 13.2.3.9 why the matrix has one negative and one positive eigenvalue.
Example 13.3
Use Theorem 13.8 to determine, if possible, the nature of the critical points of f(x, y) ≜
(2 −x2)(3y −y2).
Method: In Example 13.2, we found that the critical points are (x, y) = (±
√
2, 0), (±
√
2, 3),
and (0, 3
2). The Hessian matrix is
D2f(x, y) =
−2(3y −y2)
−2x(3 −2y)
−2x(3 −2y)
−2(2 −x2)

,
so
det D2f(±
√
2, 0) =

0
∓6
√
2
∓6
√
2
0
 = −72 < 0,
so both (
√
2, 0) and (−
√
2, 0) are saddle points.
Also,
det D2f(±
√
2, 3) =

0
±6
√
2
±6
√
2
0
 = −72 < 0,
so both (
√
2, 3) and (−
√
2, 3) are saddle points.
Finally,
det A = det D2f

0, 3
2

=

−9
2
0
0
−4
 = 18 > 0,
and a11 < 0. It follows that the matrix −A is positive definite; hence, A is negative
definite. So,

0, 3
2

is a local maximum point. ⃝
Figure 13.4 illustrates the saddle point at (−
√
2, 0) and the local maximum point at (0, 3
2)
on the graph of z = f(x, y) = (2 −x2)(3y −y2).
For functions of more than one variable, all sorts of remarkable things can happen. For
example, in Problem 13.2.3.8, you will derive that the function f(x, y) ≜e−x sin(x + y) has
no critical points yet takes on all values between −∞and ∞. In Problem 13.2.3.14, you
will derive that the function k(x, y) ≜5xey −x5 −e5y has a local maximum at (x, y) = (1, 0),
but the function does not have a global maximum there, despite the fact that the function
has no other critical point.

1038
Advanced Engineering Mathematics
20
Saddle point
Local maximum point
z
10
0
–10
–20
–2
0
0
1
2
x
2
4
y
6
FIGURE 13.4
Example 13.3.
13.2.1 Global Optimization and Lagrange Multipliers
Suppose we want to find the minimum (or maximum) value that f = f(x) has on its
domain D and we want to find a global minimizer (or global maximizer). As in the study
of functions of one variable, we have a basic existence result.
Theorem 13.9
Suppose f = f(x) is continuous on a closed, bounded domain D in Rn. Then there exist
both a global minimizer x⋆
0 and a global maximizer x⋆
1 in D, that is, for all x in D
f(x⋆
0) ≤f(x) ≤f(x⋆
1).
This seems to be a great result, but unfortunately it does not give a method for finding a
global minimizer or maximizer.
One of the most powerful theoretical methods uses geometry: For example, suppose D
is a bounded closed domain in R3 and suppose that a surface g(x, y, z) = 0 is the boundary
of D. The global maximizer of an objective function f = f(x, y, z) is either in the interior of
D or is on its boundary. If the global maximizer is in the interior, then (Fermat’s) Theorem
13.6 implies it must be a critical point. So, one method of finding the global maximizer
is, in principle, to evaluate f(x, y, z) at all critical points and also minimize f(x, y, z) at all
(x, y, z) satisfying g(x, y, z) = 0. The latter gives us a constrained optimization problem:

Minimize
f(x, y, z)
Subject to
g(x, y, z) = 0

.
(13.7)
Theorem 13.10
(Lagrange multiplier) Suppose constrained minimization problem (13.7) has solution x⋆=
(x⋆, y⋆, z⋆). Then there exists a Lagrange multiplier λ such that

Optimization
1039
∇f(x⋆) = λ∇g(x⋆),
(13.8)
as long as
∇g(x⋆) ̸= 0.
(13.9)
Why? Because ∇g(x⋆) ̸= 0, at the point x⋆on the surface g(x, y, z) = 0, there exists the
tangent plane whose normal vector is ∇g(x⋆).
On the other hand, let C : x = x(t) be any continuously differentiable curve on the
surface g(x, y, z) = 0 that passes through x⋆at t = 0, that is, x(0) = x⋆. Because f(x, y, z) is
minimized at x⋆, the function of a single variable given by
φ(t) ≜f(x(t))
must have a local minimum at t = 0; hence, by the chain rule
0 = ˙φ(0) = ∇f(x⋆) • ˙x(0).
(13.10)
Denote by T = ˙x(0), the tangent vector to the curve C at the point x⋆. Because of (13.10),
∇f(x⋆) is orthogonal to every tangent vector to the surface g(x, y, z) = 0 at x(0); hence,
∇f(x⋆) is either (a) the zero vector or (b) normal to the surface g(x, y, z) = 0 at x(0). In case
(a), ∇f(x⋆) = 0·∇g(x⋆), and in case (b), ∇f(x⋆) is parallel to ∇g(x⋆). In either case, (a) or (b),
there exists a scalar λ such that ∇f(x⋆) = λ∇g(x⋆), that is, (13.8) is true. 2
We may refer to (13.9) as a technical requirement. Note that, in principle, it can be
verified only after the candidate for the global minimizer has been found.
The method of Lagrange multipliers generalizes to Rn:
Corollary 13.2
Suppose x is in Rn, n ≥2, and the constrained optimization problem
⎧
⎨
⎩
Minimize
f(x)
Subject to
g(x) = 0
⎫
⎬
⎭
has a solution x⋆. Then there exists a Lagrange multiplier λ such that
∇f(x⋆) = λ∇g(x⋆),
as long as
∇g(x⋆) ̸= 0.

1040
Advanced Engineering Mathematics
Lemma 13.1
Suppose f(x) is continuous on closed, bounded sets D and 
D in Rn. If D ⊆
D, that is, D is
contained in 
D, then
max
x in D f(x) ≤max
x in 
D
f(x).
Let’s see how Theorem 13.10, Corollary 13.2, and Lemma 13.1 can help us solve a “real-
world” problem.
Example 13.4
The U.S. Postal Services “Global Express Guaranteed” international shipping option will
deliver a rectangular boxed package as long as its height is at least 5.5 in. and no more
than 46 in., its width is no more than 35 in., its length is at least 9.5 in. and no more than
46 in., its weight is no more than 70 lb, and the sum of its girth, that is, the perimeter of
the longitudinal side, and its length is no more than 108 in. Find the dimensions of the
most voluminous box that can be shipped.
Method: Let h, w, and ℓbe the height, width, and length of the box, in inches. A direct
translation of the problem is to
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Maximize hwℓ
Subject to 5.5 ≤h ≤46
0 ≤w ≤35
9.5 ≤ℓ≤46
0 ≤2w + 2h + ℓ≤108
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
(13.11)
Here, our closed, bounded domain is D = {(h, w, ℓ) : 5.5 ≤h ≤46, 0 ≤w ≤35, 9.5 ≤
ℓ≤46, 0 ≤2h + 2w + ℓ≤108}. Physically, it is nonsense to allow the width to be zero,
but, mathematically, allowing w to be zero, puts our problem in the category discussed
in Theorem 13.9.
We used the MathematicaTM command
RegionPlot3D[2h + 2w + ℓ≥0&&108 ≥2w+ 2h + ℓ, {h, 5.5, 46}, {w, 0, 35}, {ℓ, 9.5, 46},
AxesLabel →{h, w, ℓ}, PlotStyle →Thick,LabelStyle →(FontSize →14),
ColorFunction →“LightTemperatureMap,′′AxesStyle →Thickness[0.00315],
PlotPoints →100]
to produce the picture of D in Figure 13.5. As we see, the boundary of D consists of
many pieces, so maximizing
f(h, w, ℓ) ≜hwℓ
over D would at least involve, in principle, many constrained maximization problems,
one for each 2D piece of the boundary of D, as well as evaluating f(h, w, ℓ) at all critical
points in the interior of D. And, at least one of those 2D pieces of the boundary of D
itself has many 1D line segments bounding it.

Optimization
1041
40
30
20
10
w
h
0
30
20
10
10
20
30
40
ℓ
FIGURE 13.5
Example 13.4.
Instead, we will maximize f(h, w, ℓ) over the tetrahedron

D ≜{(h, w, ℓ) : 0 ≤h ≤54, 0 ≤w ≤54, 0 ≤ℓ≤108, 0 ≤2h + 2w + ℓ≤108}
by evaluating f at all of its critical points in the interior of 
D and maximizing f on
the boundary of 
D. It will turn out that the maximum of f(h, w, ℓ) over 
D occurs at
(h⋆, w⋆, ℓ⋆) = (18, 18, 36). Because the maximizer of f on 
D actually is a point in D and
Lemma 13.1 says that the maximum of f on D is less than or equal to the maximum of
f on 
D, we conclude that the maximum of f on D equals the maximum of f on 
D and
occurs at (h⋆, w⋆, ℓ⋆) = (18, 18, 36), too.
The boundary of the tetrahedron consists of three different triangles in the planes
h = 0, w = 0, and ℓ= 0 and part of the plane 2h⋆+ 2w⋆+ ℓ⋆= 108. But, we know the
maximizer can’t be on any of the planes h = 0, w = 0, and ℓ= 0 because the volume of
the box is zero there.
We claim that a maximizer x⋆= (h⋆, w⋆, ℓ⋆) must satisfy 2h⋆+2w⋆+ℓ⋆= 108: because,
if not, that is, 2h⋆+ 2w⋆+ ℓ⋆< 108, then it cannot be true that h⋆= 46, w⋆= 35,
and ℓ⋆= 46. So, we could increase the height to be h > h⋆or the width to be w > w⋆
or the length ℓ> ℓ⋆to get a box with a higher volume and still satisfy the constraint
2w+2h+ℓ≤108. So, we may assume that 2h⋆+2w⋆+ℓ⋆= 108. This gives us a problem
of the form (13.7):
⎧
⎨
⎩
Maximize
hwℓ
Subject to
2w + 2h + ℓ= 108
⎫
⎬
⎭,
(13.12)
along with the additional requirements that 0 ≤h ≤54, 0 ≤w ≤54, and 0 ≤ℓ≤108.
With f(h, w, ℓ) = hwℓand g(h, w, ℓ) = 2h + 2w + ℓ−108, the Lagrange multiplier
theorem, 13.10, tells us to solve
⎧
⎪⎨
⎪⎩
wℓˆw + hℓˆh + hw ˆℓ= ∇f = λ∇g
= λ

2 ˆw + 2 ˆh + ˆℓ

2h + 2w + ℓ
= 108
⎫
⎪⎬
⎪⎭
;
hence, wℓ= 2λ, hℓ= 2λ, and hw = λ, along with 2h + 2w + ℓ= 108. The first two
equations imply wℓ= 2λ = hℓ, so ℓ> 0 implies h = w. Substituting into the girth plus

1042
Advanced Engineering Mathematics
length equation gives 2h + 2w + ℓ= 108; hence,
ℓ= 108 −4w.
Also, λ = hw = w2. Substituting the latter two equations into wℓ= 2λ gives
w(108 −4w) = 2w2;
hence, 0 = w(6w −108). This implies 18 = w = h and ℓ= 36. The fact that λ = 182 is
irrelevant.
The critical points of the function f(h, w, ℓ) = hwℓlie only on the lines h = w = 0, w =
ℓ= 0, and w = ℓ= 0, as you will justify in Problem 13.2.3.15. Because f(18, 18, 36) > 0, it
follows that the maximum of f is at (h⋆, w⋆, ℓ⋆) = (18, 18, 36). The most voluminous box
that can be shipped this way has those dimensions and has a volume of 27
4 ft3. ⃝
Theorem 13.11
(Lagrange multipliers) Suppose a constrained minimization problem
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
f(x)
Subject to
f1(x) = 0
f2(x) = 0
⎫
⎪⎪⎬
⎪⎪⎭
(13.13)
has solution x⋆= (x⋆, y⋆, z⋆). If ∇f1(x⋆) ̸= 0 or ∇f2(x⋆) ̸= 0, then there exist Lagrange
multipliers λ, μ such that
∇f(x⋆) = λ∇f1(x⋆) + μ∇f2(x⋆).
(13.14)
Example 13.5
Find the point on the curve y = x2 −x closest to the curve y = 1
2 x −6.
Method: The square of the distance between a point (x, y) on the curve x2 −x −y = 0
and a point on (X, Y) on the curve 12 −X + 2Y = 0 is (X −x)2 + (Y −y)2. We consider
X, Y, x, y to be the variables of the problem, which we state as
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
(X −x)2 + (Y −y)2
Subject to
x2 −x −y = 0
12 −X + 2Y = 0
⎫
⎪⎪⎬
⎪⎪⎭
.
(13.15)
Note that the distance is minimized exactly where the square of the distance is
minimized.
Define f(X, Y, x, y) ≜(X −x)2 + (Y −y)2, f1(X, Y, x, y) ≜x2 −x −y, f2(X, Y, x, y) ≜
12 −X + 2Y and let λ, μ be the Lagrange multipliers. [ X
Y
x
y ]T is a point in R4,
and let ˆI, ˆJ, ˆı, ˆj be the corresponding unit basis vectors. So,
∇= ˆI ∂
∂X + ˆJ ∂
∂Y + ˆı ∂
∂x + ˆj ∂
∂y.

Optimization
1043
The candidates (X, Y, x, y) for the minimization and the Lagrange multipliers λ, μ
satisfy the six equations
∇f = λ∇f1 + μ∇f2
and
f1(X, Y, x, y) = f2(X, Y, x, y) = 0,
that is,
⎡
⎢⎢⎣
2(X −x)
2(Y −y)
−2(X −x)
−2(Y −y)
⎤
⎥⎥⎦= λ
⎡
⎢⎢⎣
0
0
2x −1
−1
⎤
⎥⎥⎦+ μ
⎡
⎢⎢⎣
−1
2
0
0
⎤
⎥⎥⎦,
x2 −x −y = 0,
12 −X + 2Y = 0.
The first four equations imply
(1) λ(2x −1) = −2(X −x) = μ
and
(2) λ = 2(Y −y) = 2μ;
hence,
0 = 2μ −2μ = 2λ(2x −1) −λ = λ

2(2x −1) −1

.
It follows that either λ = 0 or x = 3
4.
In the case when x = 3
4, y = x2 −x = −3
16 so that (X, Y, x, y) satisfies the constraint
0 = f1(X, Y, x, y).
On the other hand, suppose λ = 0. Then (1) and (2) imply X = x and Y = y, which
is only possible if the two curves x2 −x = y and y = 1
2 x −6 intersect. But, they don’t,
because x2 −x = y = 1
2 x−6 implies that x must satisfy the quadratic equation x2 −3
2 x+
6 = 0, which has no real number solution.
So, (x, y) = ( 3
4, −3
16) is the point on the curve y = x2 −x closest to the curve y =
1
2 x −6. ⃝
In Problem 13.2.3.13, you will continue the work of Example 13.5 to conclude that the
closest approach of the two curves is where (x, y) = ( 3
4, −3
16) and (X, Y) =

117
40 , −363
80

, and
the minimum distance is 87
√
5
40 .
The results of Example 13.5 are illustrated in Figure 13.6.
Example 13.6
Find where the curves xy −x + 2y = 0 and x −2y + 6 = 0 are closest to each other.
Method: (X −x)2 + (Y −y)2 is the square of the distance between a point (x, y) on the
curve xy −x + 2y = 0 and a point on (X, Y) on the curve X −2Y + 6 = 0. We consider
X, Y, x, y to be the variables of the problem, which we state as
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
(X −x)2 + (Y −y)2
Subject to
xy −x + 2y = 0
X −2Y + 6 = 0
⎫
⎪⎪⎬
⎪⎪⎭
.
(13.16)
Define f(X, Y, x, y) ≜(X−x)2+(Y−y)2, f1(X, Y, x, y) ≜xy−x+2y, f2(X, Y, x, y) ≜X−2Y+6
and let λ, μ be the Lagrange multipliers.
As in Example 13.5, [ X
Y
x
y ]T is a point in R4 and
∇= ˆI ∂
∂X + ˆJ ∂
∂Y + ˆı ∂
∂x + ˆj ∂
∂y.
(13.17)

1044
Advanced Engineering Mathematics
–4
–2
2
2
–2
–4
4
y
x
4
6
–6
FIGURE 13.6
Example 13.5.
The candidates (X, Y, x, y) for the minimization and the Lagrange multipliers satisfy
the six equations
∇f = λ∇f1 + μ∇f2
and
f1(X, Y, x, y) = f2(X, Y, x, y) = 0,
that is,
⎡
⎢⎢⎣
2(X −x)
2(Y −y)
−2(X −x)
−2(Y −y)
⎤
⎥⎥⎦= λ
⎡
⎢⎢⎣
0
0
y −1
x + 2
⎤
⎥⎥⎦+ μ
⎡
⎢⎢⎣
1
−2
0
0
⎤
⎥⎥⎦,
xy −x + 2y = 0,
X −2Y + 6 = 0.
The first four equations imply
(1) λ(y −1) = −2(X −x) = −μ
and
(2) λ(x + 2) = −2(Y −y) = 2μ;
hence,
0 = 2μ −2μ = λ(x + 2) + 2(λ(y −1)) = λ

2(y −1) + (x + 2)

.
It follows that either λ = 0 or 2(y −1) + (x + 2) = 0.
In the latter case, x = −2y; substitute that into f1 = 0 to get (−2y)y −(−2y) + 2y = 0;
hence, 0 = 2y(y −2). So, our candidates so far have either (a) y = 0, in which case x = 0,
or (b) y = 2, in which case x = −4.
Substitute x = y = 0 into (1) and (2) to get
2μ = 4X
and
2μ = −2Y;
hence, Y = −2X. Substitute that into f2 = 0 to get 0 = X −2Y + 6 = X −2(−2X) + 6;
hence, X = −6
5 and thus Y = 12
5 . The square of the distance between the respective
points (X, Y) = (−6
5, 12
5 ), (x, y) = (0, 0) is f(−6
5, 12
5 , 0, 0) = 36
5 .

Optimization
1045
4
y
2
0
–2
–4
x
–4
–2
0
2
4
FIGURE 13.7
Example 13.6.
Substitute x = −4, y = 2 into (1) and (2) to get
2μ = 4(X + 4) = 4X + 16
and
2μ = −2(Y −2) = −2Y + 4;
hence, −2Y + 4 = 4X + 16, and hence, Y = −2X −6. Substitute that into f2 = 0 to
get 0 = X −2Y + 6 = X −2(−2X −6) + 6; hence, X = −18
5 and thus Y =
6
5. The
square of the distance between the respective points (X, Y) = (−18
5 , 6
5), (x, y) = (−4, 2) is
f(−18
5 , 6
5, −4, 2) = 4
5.
On the other hand, suppose λ = 0. Then (1) and (2) imply X = x and Y = y, which
is only possible if the two curves xy −x + 2y = 0 and x −2y + 6 = 0 intersect. But,
they don’t, because x = 2y −6 substituted into the equation of the first curve gives
0 = (2y −6)y −(2y −6) + 2y = 2(y2 −3y + 3), which has no real solution.
So, the only candidates for the closest points are (a) (X, Y) = (−6
5, 12
5 ), (x, y) = (0, 0),
and (b) (X, Y) = (−18
5 , 6
5), (x, y) = (−4, 2). The latter gives the closest approach, and the
minimum distance between the curves is
2
√
5. ⃝
The results of Example 13.6 are illustrated in Figure 13.7, which shows that a solution of
the “Lagrange equations” (13.14) may not be a global minimizer.
13.2.2 Numerical Minimization and Steepest Descent Methods
Suppose we want to find the minimum (or maximum) value that f = f(x) has on domain
D = Rn. We will briefly discuss the ideas behind one numerical method.
Suppose the level curves of a function f(x) are as shown in Figure 13.8a. Suppose x0
is our first guess for a global minimizer of f. We recall that at the point x0 on a level set
f(x) = k, the negative of the gradient vector, g ≜−∇f(x0), points in the direction of the
greatest decrease of f. Let g denote the unit vector in the direction of g. Our goal is to
minimize f, so a numerical search along the normal line segment
x(α) ≜x0 + αg, 0 ≤α ≤δ

1046
Advanced Engineering Mathematics
x0
(x0, f(x0))
(x–∞, f(x–∞))
x–∞
4
2
0
x
–2–4
–4
–2
0
2
0
100
200
z
300
y
(a)
(b)
FIGURE 13.8
(a) Method of steepest descent, (b) Surface z = f(x, y).
should help: Solve the single variable calculus problem
f

x(α⋆)

= min
0≤α≤δ f (x(α)) ,
(13.18)
and take as our next guess for the global minimizer x1 ≜x(α⋆). By the way, the single
variable calculus problem (13.18) is called a line search.
What we described previously is called the method of steepest descents. In principle,
it is a reliable, cautious method. But it can be very slow because it ignores some of the
geometry. Suppose that we had chosen as our initial guess the point x−∞also shown in
Figure 13.8a. Look also at the graph of f in Figure 13.8b. It would take many iterations of
the steepest descent method to get from x−∞to x⋆on a path down a long, gently sloping
valley.
The conjugate gradient method modifies the steepest descent method by replacing the
normal line’s choice x(α) with many choices of x(α) = x0 + αpj, j = 1, . . .. The conju-
gate gradient method enables us to behave more like a mountain goat than a weekend
hiker.
Learn More About It
Numerical Recipes: The Art of Scientific Computing, 3rd edition, by William H. Press et al.,
Cambridge University Press c⃝2007 is a good resource for learning about the method
of steepest descents and the conjugate gradient method.
13.2.3 Problems
In problems 1–3, (a) find all of the critical points, and (b) explain why each of the critical
points is a local maximum, local minimum, or saddle point or cannot be classified by the
second derivative test.

Optimization
1047
1. f(x, y) = x2 + xy2 + 3y2
2. f(x, y) = (2 −x2)(1 −y2)
3. f(x, y) = y3 + 3x2y −3x2 −3y2
4. For f(x, y) = xe−x2 sin2 y, defined only on the domain {(x, y) : −1 ≤x ≤1, −π
4 ≤
y ≤5π
4 }, find all of the critical points. Do not classify them into the categories of
local maximum, local minimum, or saddle point.
5. Find the absolute maximum value of f(x, y) = 4 + x −x2 −2y2 over the set of (x, y)
satisfying x2 + y2 = 2.
6. Find the absolute maximum value and absolute minimum values of f(x, y) = x2 +
2y2 over the domain 0 ≤3x2 + y2 ≤6.
7. Find the absolute maximum value and absolute minimum values of f(x, y) = 2x2+
y2 over the domain 0 ≤x2 + y2 ≤4.
8. Show that f(x, y) ≜e−x sin(x + y) has no critical points, but that f(x, y) takes on all
values between −∞and ∞.
9. If a symmetric 2×2 matrix A has negative determinant, explain why the matrix has
one negative and one positive eigenvalue. [Hint: | A −λI | =

a11 −λ
a12
a12
a22 −λ
 =
(λ1 −λ)(λ2 −λ) = λ2 −(a11 + a22)λ + |A|.]
10. In solving the least squares solution problem in Section 2.5, we used linear algebra
to conclude that if x⋆is the global minimizer of f(x) ≜||Ax −b||2, then necessarily
x⋆solves the normal equations ATAx = ATb. Now, using the theory of Section
13.2, re-derive this result that a global minimum of f(x) on domain Rn must satisfy
the normal equations.
11. Suppose W is a real, symmetric, positive definite n × n matrix. As in Problem
2.6.3.17, define ⟨x, y⟩W ≜⟨Wx, y⟩≜xT WTy and || x ||W ≜√⟨x, x⟩W. Suppose real,
m × n matrix A and vector b in Rm are given.
Consider the problem of finding x⋆that minimizes f(x) ≜||Ax−b||2
W. We called
this a generalized or weighted least squares problem. Use the theory of Section
13.2 to re-derive the result that if x⋆is a global minimum of f(x) on domain Rn,
then x⋆must satisfy the generalized normal equations
AT WTAx = AT WTb.
12. Derive that the function f(x, y) ≜e−x sin(x + y) has no critical points. Also, find
points at which (a) ∂f
∂x > 0 and ∂f
∂y > 0, (b) ∂f
∂x > 0 and ∂f
∂y < 0, (c) ∂f
∂x < 0 and ∂f
∂y > 0,
(d) ∂f
∂x < 0 and ∂f
∂y < 0.
13. Continue the work of Example 13.5 to conclude that the closest approach of the
two curves is where (x, y) = ( 3
4, −3
16) and (X, Y) =

117
40 , −363
80

and the minimum
distance is 87
√
5
40 .
14. Derive that the function k(x, y) ≜5xey −x5 −e5y has a local maximum at (x, y) =
(1, 0), but the function does not have a global maximum there despite the fact that
the function has no other critical point.
15. Why do the critical points of the function f(h, w, ℓ) = hwℓlie only on the lines
h = w = 0, w = ℓ= 0, and ℓ= h = 0?

1048
Advanced Engineering Mathematics
16. (Small project) In the paper by Aziz mentioned in Section 13.1, he also studied the
case of pure radiative heat transfer from an insulated cylinder to a large medium
at temperature T∞. Define dimensionless quantities
φ0 ≜To
Ti
, φ∞≜T∞
Ti
, P ≜ro
ri
, Bi ≜ϵσT3
i ri
k
,
the latter being the Biot number. Given a Bi and φ∞, the problem of finding the
maximum heat transfer rate reduces to finding the φ0 and P to
⎧
⎨
⎩
Maximize
Q ≜(1 −φ0)/ ln P
Subject to
1 −φ0 −BiP ln P

φ4
0 −φ4
∞

= 0
⎫
⎬
⎭.
Using realistic values of Bi and φ∞from, for example, Example 4 of Aziz’s paper,
maximize Q.
17. Suppose c is a constant vector. Do the calculations to explain why ∇

cTx

= c.
13.3 Linear Programming Problems
If we are manufacturing things of various sizes and shapes, we need various raw materials,
machines, and labor tasks to make them. The allocation of resources naturally leads to
optimization problems.
In a linear programming (LP) problem, suppose we have quantities x1, . . . , xk of things
that are needed. These quantities are assumed to be nonnegative, that is, xi ≥0 for i =
1, . . . , k. We will model the demand for these quantities, or limitations on their availability,
using linear inequalities of the form d1x1 + · · · + dkxk ≤b or d1x1 + · · · + dkxk ≥b or linear
equations of the form d1x1 + · · · + dkxk = b.
The last part of an LP problem is that we have a function f that is to be minimized or
maximized. We assume that the objective function f is also linear, that is, f(x1, . . . , xk) =
c1x1 + · · · + ckxk.
Here is an example of an LP problem:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Maximize
x + 2y
Subject to
x + 6y
≤24
23x + 12y ≤92
x ≥0, y ≥0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(13.19)
In Figure 13.9, we see a drawing of the set
C ≜{(x, y) : x + 6y ≤24, 23x + 12y ≤92, x ≥0, y ≥0}.
The dotted lines are of the form x+2y = k, where k is a constant. A graphical solution of this
LP problem is found by drawing those lines to get closer and closer until one just touches C.

Optimization
1049
4
y
2
x
2
4
FIGURE 13.9
Convex set and lines x + 2y = k.
The process of getting closer and closer to C is carried out by taking the constant k smaller
and smaller until at k = k⋆; there is a point (x⋆, y⋆) in C where 2x⋆+ y⋆= k⋆.
Example 13.7
The Acme Company manufactures windows in model numbers 1 through k. Model #j
requires gj cm2 of glass, Aj grams of aluminum, wj cm3 of wood, vj cm2 of vinyl plastic,
and Lj hours of labor. Suppose the costs of glass, aluminum, wood, vinyl, and labor
are cg, cA, cw, cv, and cL, in dollars per appropriate unit. Suppose the company has a
business plan that expects it to produce at least 800 windows per week. (If the company
falls below this level of production, an investment bank may declare them in violation
of their loan covenant and demand immediate repayment.) The plan also demands that
they manufacture at least mj of model #j per week, where each mj ≥0. (Some of the
window models are not bought often so some of the mj may be zero.) Assume that the
company president has declared a freeze on hiring and overtime so that the total number
of hours of labor available is at most 650, and assume that the company has a long term
contract with a supplier that demands that it buy at least 150 kg of aluminum per week. If
the Acme Company gets paid pj dollars for each window model #j produced, formulate
the problem of maximizing its profit.
Method: By the way, the amount of glass used per window by model # may take into
account the average waste of glass due to breakage. Our model has several unrealistic
assumptions by omission. For example, we are assuming that every week is the same
and that the company sells every window it makes. In spite of this, let’s formulate the
problem as best we can using the given information.
Let xj be the amount of window model #j produced per week. The minimum demand
for windows, by model, translates to xj ≥mj for j = 1, . . . , k, and the business plan
requires x1 + · · · + xk ≥800.
The total amount of glass used per week is g1x1 + · · · + gkxk, and there are similar
calculations for aluminum, wood, vinyl, and labor. The latter requires L1x1+· · ·+Lkxk ≤
650 because of the labor freeze. The total cost of the glass used in making one model #j
window is cggj, and there are similar costs for the other resources.

1050
Advanced Engineering Mathematics
Our problem can be formulated as
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Maximize
n
j=1

pj −cggj −cAAj −cwwj −cvvj −cLLj

xj
Subject to
x1+
· · ·
+xk
≥800
L1x1+
· · ·
+Lkxk ≤650
A1x1+
· · ·
+Akxk ≥1.5 × 105
xj
≥mj, j = 1, . . . , k.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
(13.20)
An LP problem in “preliminary standard form” has the form
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
c1x1 + · · · + ckxk
Subject to
a11x1 + · · · + a1kxk ≤b1
...
...
am1x1 + · · · + amkxk ≤bm
x1, . . . , xk ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
(13.21)
The letter “c” stands for the word “cost.”
If our original problem has an inequality d1x1 + · · · + dkxk ≥γ , then we can change it to
the equivalent inequality −d1x1 −· · · −dkxk ≤−γ . Similarly, if our original problem asked
us to maximize a function g(x) = γ1x1 + · · · + γkxk, then we change it to the equivalent
objective of minimizing f(x) ≜−g(x) = −γ1x1 −· · · −γkxk. So, one theory and methods for
solving (13.21) will be enough to solve many different types of LP problems.
13.3.1 Slack Variables and Standard Form
We learned how to solve linear systems of algebraic equations in Section 1.1. Some very
clever people realized that in solving LP problems, we can take advantage of row reduc-
tion techniques by converting an inequality to an equality by introducing a slack variable:
if a11x1+ · · · +a1kxk ≤b1, then there is a nonnegative number ε such that a11x1+ · · · +a1kxk +
ε = b1, specifically ε = b1 −(a11x1 + · · · + a1kxk). So, we introduce the slack variable xk+1,
and get the equation a11x1 + · · · +a1kxk +xk+1 = b1 along with the constraint that xk+1 ≥0;
the latter is the same as the nonnegativity constraint on the original variables x1, . . . , xk.
Definition 13.9
An LP problem is in standard form if it is written as
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
c1x1 + · · · + ckxk
Subject to
a11x1 + · · · + a1kxk = b1
...
...
am1x1 + · · · + amkxk = bm
x1, . . . , xk ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
.
(13.22)

Optimization
1051
Example 13.8
Put into standard form the LP problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
5x1 + x2 + 2x3
Subject to
x1
+x2
+x3
≥20
2x1
+x2
−x3
≤30
3x1
+x2
+x3
= 60
x1, . . . , x3 ≥0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
Method: First, put the first inequality into the preliminary standard form, that is, −x1 −
x2 −x3 ≤−20. Next, introduce slack variables into the two inequalities. The LP problem
in standard form is
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
5x1 + x2 + 2x3
Subject to
−x1
−x2
−x3
+x4
= −20
2x1
+x2
−x3
+x5
=
30
3x1
+x2
+x3
=
60
x1, . . . , x5 ≥0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
. ⃝
(13.23)
Denoting
c =
⎡
⎢⎣
c1
...
cn
⎤
⎥⎦,
A =
⎡
⎢⎢⎢⎢⎣
a11
.
.
.
a1n
.
.
.
.
.
.
.
.
.
am1
.
.
.
amn
⎤
⎥⎥⎥⎥⎦
,
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦,
and
b =
⎡
⎢⎢⎢⎢⎣
b1
.
.
.
bm
⎤
⎥⎥⎥⎥⎦
,
we can rewrite (13.22), an LP in standard form, in “compact, standard form,”
⎧
⎨
⎩
Minimize
cTx
Subject to
Ax = b, x ≥0
⎫
⎬
⎭,
(13.24)
where
x and c are in Rn
b are in Rm
A is a real, m × n matrix
Here, the notation (x1, . . . , xn) = x ≥0 means xj ≥0 for j = 1, . . . , n.
Definition 13.10
Suppose (13.24) is an LP in standard form. Then x is defined to be a
(a) Feasible solution if Ax = b and x ≥0;
(b) Basic feasible solution if Ax = b, x ≥0, and no more than m of the xj are nonzero.

1052
Advanced Engineering Mathematics
Example 13.9
Find a basic feasible solution of the LP problem of Example 13.8.
Method: The augmented matrix, with variables labeled above it, is
x1
x2
x3
x4
x5
⎡
⎣
−1
−1
−1
1
0
|
−20
2
1
−1
0
1
|
30
3
1
1
0
0
|
60
⎤
⎦
∼
R3 + R2 →R2
R3 + R1 →R1
x1
x2
x3
x4
x5
⎡
⎣
2
0
0
1
0
| 40
5
2
0
0
1
| 90
3
1
1
0
0
| 60
⎤
⎦.
So, we have a basic feasible solution (x1, x2, x3, x4, x5) = (0, 0, 60, 40, 90). ⃝
Example 13.10
We want to find the proportions of wheat bran, oat flour, and rice flour to be mixed
to produce emergency rations. Nutritional information ∗about these cereals is in
Table 13.1. Assume the mixture should have, per 100 g, at least 370 kcal, 12 g of pro-
tein, and 2 g polyunsaturated fat. Set up the linear programming problem, in standard
form, to minimize the grams of carbohydrates per 100 g of mixture, and find a basic
feasible solution.
Method: Let x1, x2, and x3 be the proportions of wheat bran, oat flour, and rice flour used
in the mixture. It follows that 0 ≤xi ≤1 for i = 1, 2, 3 and x1 + x2 + x3 = 1.
In the mixture, the energy in kcal requirement means that 216x1 +404x2 +363x3 ≥370.
The protein requirement means that 15.55x1 + 14.66x2 + 7.23x3 ≥12. The dietary fiber
requirement means that 42.8x1+6.5x2+4.6x3 ≥12. The polyunsaturated fat requirement
means that 2.212x1 + 3.329x2 + 0.996x3 ≥2. The amount of carbohydrates per 100 g of
mixture is f = 64.51x1 + 65.70x2 + 76.48x3.
This directly translates to
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
64.51x1 + 65.70x2 + 76.48x3
Subject to
216x1
+ 404x2
+ 363x3
≥370
15.55x1
+14.66x2
+ 7.23x3
≥12
2.212x1
+3.329x2
+0.996x3
≥
2
x1
+
x2
+
x3
=
1
x1, . . . , x3 ≥0
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
TABLE 13.1
Nutrients per 100 g
Wheat Bran
Oat Flour
Rice Flour
Kilocalories(kcal)
216
404
363
g Protein
15.55
14.66
7.23
g Carbs
64.51
65.70
76.48
g Dietary fiber
42.8
6.5
4.6
g Polyunsaturated fats
2.212
3.329
0.996
∗From
the
document
sr22fg20.pdf,
“CerealGrains
and
Pasta,”
available
from
the
USDA
website,
⟨http://www.ars.usda.gov/ Services/docs.htm?docid=18878⟩.

Optimization
1053
The requirement xi ≤1 for i = 1, 2, 3 will be automatically satisfied because x1+x2+x3 =
1 and x1, x2, x3 ≥0.
Turn all of the ≥inequalities to ≤inequalities and introduce slack variables to get the
equivalent problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
64.51x1 + 65.70x2 + 76.48x3
Subject to
−216x1
−404x2
−363x3
+x4
= −370
−15.55x1
−14.66x2
−7.23x3
+x5
= −12
−2.212x1
−3.329x2
−0.996x3
+x6
= −2
x1
+
x2
+
x3
=
1
x1, . . . , x6 ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
. ⃝
In solving LP problems by hand, people have used a tableau version of the problem. In
Table 13.2, the top row lists the cost coefficients c4, c5, c6, c1, c2, c3 for the materials used
in the amounts x4, x5, x6, x1, x2, x3. The second row has those variable names. The next
four rows give the augmented matrix for Ax = b except that the columns were permuted
corresponding to listing the variables as x4, x5, x6, x1, x2, x3.
After the row operations 3.329R4 + R3 →R3, 14.66R4 + R2 →R2, 404R4 + R1 →R1,
we can find a basic feasible solution for x4, x5, x6, x2 as the basic variables. Permuting the
columns corresponding to x1 and x2 gives Table 13.3. The last column is empty except
for the first two entries that will have the total cost f for a known basic feasible solu-
tion. The 4 × 4 identity matrix in the first four columns shows that (x4, x5, x6, x2) =
(34, 2.66, 1.129, 1).
The total cost is
cTx = [ c4
c5
c6
c2
c1
c3 ][ x4
x5
x6
x2
x1
x3 ]T
= 0(34) + 0(2.66) + 0(1.129) + 65.70(1) + 64.512(0) + 76.80(0) = 65.70.
So, we have a basic feasible solution (x1, x2, x3, x4, x5, x6) = (0, 1, 0, 34, 2.66, 1.129). ⃝
TABLE 13.2
Before Finding a Basic Feasible Solution
0
0
0
64.51
65.70
76.80
x4
x5
x6
x1
x2
x3
y
1
0
0
−216
−404
−363
−370
0
1
0
−15.55
−14.66
−7.23
−12
0
0
1
−2.219
−3.329
−0.996
−2
0
0
0
1
1
1
1
TABLE 13.3
After Finding the First Basic Feasible Solution
0
0
0
65.70
64.51
76.80
f
x4
x5
x6
x2
x1
x3
y
65.70
1
0
0
0
188
41
34
0
1
0
0
−0.89
7.43
2.66
0
0
1
0
1.11
2.333
1.129
0
0
0
1
1
1
1

1054
Advanced Engineering Mathematics
Note that the basic feasible solution we found is clearly suboptimal because it uses all oat
flour and no wheat bran, and the latter would use fewer carbohydrates than the former.
We will see how to find the optimal solution in Section 13.4.
13.3.2 Application: Structural Optimization
In structural optimization (Kirsch, 1993), Uri Kirsch models the collapse of structures in
order to find the maximum load a structure can withstand. In the regime of plasticity, it
is assumed that until reaching the yield stress, the stress is proportional to the strain, as
we assumed in Section 10.4 when discussing linear elasticity. Also in the plastic regime,
the bending moment of a section of material is proportional to its curvature, until the
magnitude of the bending moment reaches the fully plastic moment, MPℓ.
The linear theory assumes rotations are small and that the basic geometry remains the
same as for the undeformed structure, for example, that a triangular strut hasn’t collapsed
into a straight line. So, the geometry imposes linear, static equilibrium equations relating
the structure’s members’ bending moments, M1, . . . , Mn.
Let λ · P be the magnitude of a compressive force, where λ is a parameter that later will
be maximized in order to optimize the structure. Let ℓbe a characteristic length of the
structure, so λPℓwill be a bending moment. The optimization problem will be
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Maximize
λ
Subject to
static equilibrium equations in λ, M1, . . . , Mn
−MPℓ≤Mj ≤MPℓ, j = 1, . . . , n
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(13.25)
13.3.3 Problems
In problems 1–3, use row reduction to find all basic feasible solutions. You may use tech-
nology but write down all augmented matrices that you use and label them as to which
variable you are solving for.
1.

x1
−x2
+x3
= −7
3x1
−4x2
+x4
= 11

2.
⎧
⎪⎨
⎪⎩
x1
−x2
+x3
+x4
= 0
−2x1
+2x2
−x3
= 0
3x2
+2x3
= 4
⎫
⎪⎬
⎪⎭
3.
⎧
⎪⎨
⎪⎩
x1
+2x2
−2x3
+x4
= −6
−2x1
−4x2
+5x3
= 17
x1
+2x2
−x3
= 0
⎫
⎪⎬
⎪⎭
In problems 4–5, (a) set up, but do not solve, the LP problem in standard form, and (b)
find all basic feasible solutions. You may use technology but write down all augmented
matrices that you use and label them as to which variable you are solving for.

Optimization
1055
4.

x1
−x2
≤4
3x1
−4x2
≥5

5.

x1
+x2
≤
2
−2x1
−x2
≤−3

In problems 6–7, (a) set up, but do not solve, the LP problem in standard form to produce
emergency rations that have the desired optimization, and (b) also find a basic feasible
solution.
6. Maximize the kcal content per 100 g of mixture, with the requirements that the
mixture has, per 100 g, at least 10 g of protein and 2 g of polyunsaturated fat, as
well as no more than 70 g of carbohydrates.
7. Maximize the protein content in g per 100 g of mixture, with the requirements
that the mixture has, per 100 g, at least 300 kcal, 10 g of dietary fiber, and 2.5 g of
polyunsaturated fat.
13.4 Simplex Procedure
Here we will study Dantzig’s Simplex Procedure, a basic but powerful method for solving
LP problems. Unlike a least squares problem or the problem of minimization of a function
of several variables, the difficulty is not in the function to be optimized but, instead, in a
“combinatorial explosion” of candidates for the minimizer.
We saw in Section 13.2 that even with a few variables, the search for an optimum can be
difficult due to searching both in the interior and on the boundary of the domain. As the
number of variables increases, the difficulty increases terribly.
For an LP problem, it turns out that the work can be reduced tremendously because of
the following result concerning convex geometrical analysis.
Theorem 13.12
If x⋆is in Rn and is a minimizer for LP problem (13.22) in Section 13.3, then at most m of
the entries x⋆
j can be nonzero.
This fundamental result reduces optimization to a search among
n
m

candidates x,
because that is the number of ways of choosing m distinct objects from a menu of n dis-
tinct objects. Unfortunately, even searching the finite number
n
m

≜
n!
m!(n−m) of candidates
may take too much time! For example,
1000
950

≈9.46046 × 1084. This is what we mean by a
combinatorial explosion of candidates.
The simplex procedure decides how to improve a basic feasible solution by replacing
one of the basic variables by a new choice. This is called the pivoting process. The decision
process first chooses a variable to leave; after that, the process chooses which variable to
enter into the list of basic variables.

1056
Advanced Engineering Mathematics
TABLE 13.4
Before Replacing a Basic Variable
ci1
ci2
ci3
cj1
cj2
f
xi1
xi2
xi3
xj1
xj2
y
yiici1 + yi2ci2 + yi3ci3
1
0
0
αi1
β1
yii
0
1
0
αi2
β2
yi2
0
0
1
αi3
β3
yi3
zj1 −cj1
zj2 −cj2
13.4.1 Unit Cost Reduction
If we move a variable into the set of basic variables, then we will, in effect, exchange spe-
cific amounts of the old basic variables for 1 unit of the new variable. If the new variable is
increased from xj = 0 to xj = 1, then the process will increase our costs by cj but decrease
our cost by zj, the total cost of the replaced amounts of the old quantities. We will explain
how to find zj −cj, the unit cost reduction.
Suppose we begin with, or have arrived at, the tableau in Table 13.4. At this stage, the
3 × 3 identity matrix in the first three columns shows that the basic feasible solution has
(xi1, xi2, xi3) = (yi1, yi2, yi3), all other xk = 0, and the total cost is f = y • [ ci1
ci2
ci3 ]T. If
we increase the amount of variable xj1 from 0 to 1, that is, add 1 unit of xj1, at a cost of 1·cj1,
then keeping the system Ax = b satisfied implies we must reduce xi1 by αi1, reduce xi2 by
αi2, and reduce xi3 by αi3, changing costs by
zj1 ≜αi1ci1 + αi2ci2 + αi3ci3.
So, simultaneously increasing xj1 from 0 to 1 while changing∗xi1, . . . , xi3 from yi1, . . . , yi3 to
yi1 −αi1, . . . , yi3 −αi3, respectively, increases costs by cj1 but also changes costs by zj1. The
net unit cost saving is zj1 −cj1.
When deciding which variable to enter into the list of basic variables, the simplex proce-
dure says to choose the variable with the maximum positive unit reduced cost; if none of
the unit reduced costs are positive, then we have already arrived at the minimizer!
Suppose we decided to move variable xj1 into the list of basic variables. By Theorem
13.12, we must move out one of the old basic variables, that is, reduce it to zero, if we want
our new basic feasible solution to be a minimizer. To decide which variable to move out,
note that if we increase xj1 from 0 to θ > 0, then we will change xi1, . . . , xi3 from yi1, . . . , yi3
to yi1 −θαi1, . . . , yi3 −θαi3, respectively. If we reduce one of the old basic variables to a
negative value, then we have “gone too far” because we no longer have a feasible solution.
So, we need yi1 −θαi1 ≥0, . . . , yi3 −θαi3 ≥0.
If, say, αi1 < 0, then yi1 −θαi1 ≥yi1 ≥0 for all θ > 0, which imposes no restriction
on θ. The actual restrictions on θ are that yiℓ−θαiℓ≥0 for all ℓfor which αiℓ> 0. The
minimum reduction is
θ =
min
for which αiℓ>0
yiℓ
αiℓ
.
(13.26)
If all αiℓ≤0, then we define θ = 0 and the simplex procedure is complete.
∗If an α < 0, then we are increasing the corresponding variable by |α|.

Optimization
1057
Let L be the choice of index ℓthat achieves the minimum in (13.26), that is, θ =
yiL
αiL .
Assuming θ > 0, to improve on our basic feasible solution we increase xj1 from 0 to θ
and simultaneously reduce xiL to 0; at the same time, we change the other basic variables
xik from yik to yik −θαik, respectively.
Example 13.11
Use the simplex procedure to find the minimum value and a minimizer of the LP
problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
5x1 + x2 + 2x3
Subject to
x1
+x2
+x3
≥20
2x1
+x2
−x3
≤30
3x1
+x2
+x3
= 60
x1, . . . , x3 ≥0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
Method: In Section 13.3, we used slack variables x4, x5 to put this problem in standard
form as (13.22) in Section 13.3, that is,
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
5x1 + x2 + 2x3
Subject to
−x1
−x2
−x3
+x4
=
−20
2x1
+x2
−x3
+x5
=
30
3x1
+x2
+x3
=
60
x1, . . . , x5 ≥0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
First, we will find a basic feasible solution. In matrix-tableau form, the problem is
5
1
2
0
0
f
x1
x2
x3
x4
x5
⎡
⎢⎣
−1
−1
−1
1
0
| −20
2
1
−1
0
1
|
30
3
1
1
0
0
|
60
⎤
⎥⎦
∼
R3 + R2 →R2
R3 + R1 →R1
5
1
2
0
0
f
x1
x2
x3
x4
x5
120
⎡
⎢⎣
2
0
0
1
0
|
40
5
2
0
0
1
|
90
3
1
1
0
0
|
60
⎤
⎥⎦
z1
z2
.
The underlined ones in the first and second rows correspond to the basic variables
x4, x5. After that, the elementary row operations make x3 to be the third basic variable.
After permuting the variables, the tableau is in Table 13.5, whose bottom row contains
the unit costs reduction information zj −cj, which we are about to calculate. Also, later
we will explain why the “5” is circled.
TABLE 13.5
After Finding the First Basic Solution
0
0
2
5
1
f
x4
x5
x3
x1
x2
y
120 = 0 · 40 + 0 · 90 + 2 · 60
1
0
0
2
0
40
0
1
0
5⃝
2
90
0
0
1
3
1
60
1
1

1058
Advanced Engineering Mathematics
So far, we have a basic feasible solution (x1, x2, x3, x4, x5) = (0, 0, 60, 40, 90). The next
thing to do is to decide whether we should pivot by choosing a variable to enter the list
of basic variables, and after that, by choosing a variable to move out.
We use the “maximum unit reduced cost” criterion for choosing which variable, if
any, to move in. The unit reduced cost of x1 is
z1 −c1 =
⎡
⎣
c4
c5
c3
⎤
⎦•
⎡
⎣
α4
α5
α3
⎤
⎦−c1 =
⎡
⎣
0
0
2
⎤
⎦•
⎡
⎣
2
5
3
⎤
⎦−5 = 6 −5 = 1.
Note that the α column vector used to calculate z1 sits under the x1 variable in Table 13.5.
Similarly
z2 −c2 =
⎡
⎣
0
0
2
⎤
⎦•
⎡
⎣
0
2
1
⎤
⎦−1 = 2 −1 = 1.
The maximum reduced unit cost is 1, and so we are free to choose either x1 or x2 to move
into the list of basic variables. Let’s choose x1 to move in.
The choice to move in x1 will, in principle, affect the choice of which variable to move
out. We decide which variable to move out by analyzing the column [ α4
α5
α3 ]T =
[ 2
5
3 ]T corresponding to variable x1. We calculate that minimum reduction is
θ = min
αiℓ>0
yiℓ
αiℓ
= min
 y4
α4
, y5
α5
, y3
α3
!
= min
 40
2 , 90
5 , 60
3
!
= 18,
achieved at index L = 5. So, to improve on our basic feasible solution, we increase x1
from 0 to θ = 18 and reduce x5 to 0; at the same time, the other basic variables xik change
from yik to yik −θαik, respectively.
Circle the “pivot position” 5⃝in Table 13.5 and now do row operations 0.2R2 →R2,
−2R2+R1 →R1, and −3R2+R3 →R3 to get the tableau in Table 13.6. After that, we per-
mute the columns, specifically by exchanging the columns corresponding to variables x5
and x1, to put the tableau into standard form for using unit costs reduction to discuss
the next round of possibly swapping variables:
z5 −c5 =
⎡
⎣
0
5
2
⎤
⎦•
⎡
⎣
−0.4
0.2
−0.6
⎤
⎦−0 = −0.2 −0 = −0.2
and
z2 −c2 =
⎡
⎣
0
5
2
⎤
⎦•
⎡
⎣
−0.8
0.4
−0.2
⎤
⎦−1 = 1.6 −1 = 0.6.
TABLE 13.6
After Doing Row Operations on Table 13.5
0
0
2
5
1
x4
x5
x3
x1
x2
f
1
−0.4
0
0
−0.8
4
0
0.2
0
1
0.4
18
0
−0.6
1
0
−0.2
6

Optimization
1059
TABLE 13.7
After Finding the Second Basic Solution
0
5
2
0
1
f
x4
x1
x3
x5
x2
y
102 = 0 · 4 + 5 · 18 + 2 · 6
1
0
0
−0.4
−0.8
4
0
1
0
0.2
.4⃝
18
0
0
1
−0.6
−0.2
6
TABLE 13.8
After Finding the Third Basic Solution
0
1
2
0
5
f
x4
x2
x3
x5
x1
y
75 = 0 · 40 + 1 · 45 + 2 · 15
1
0
0
0
0.8
40
0
1
0
0.5
2.5
45
0
0
1
−0.5
0.2
15
−0.5
−2.1
The maximum unit cost reduction is 0.6, so we move variable x2 in. To decide which
variable to move out, we calculate the minimum positive reduction, using ∗to denote
quantities not calculated because α < 0:
θ = min
αiℓ>0
yiℓ
αiℓ
= min
 y4
α4
, y1
α1
, y3
α3
!
= min
 
∗, 18
0.4, ∗
!
= 45,
which is achieved at index L = 1. So, to improve on our basic feasible solution, we
increase x2 from 0 to θ = 45 and reduce x1 to 0; at the same time, the other basic variables
xik change from yik to yik −θαik.
Circle the pivot position
.4⃝in the tableau in Table 13.7, and do row operations
2.5R2 →R2, 0.8R2 + R1 →R1, and 0.2R2 + R3 →R3; after that, permute the columns to
get the tableau in Table 13.8.
The unit reduced costs of x5 and x1 are, respectively,
z5 −c5 =
⎡
⎣
0
1
2
⎤
⎦•
⎡
⎣
0
0.5
−0.5
⎤
⎦−0 = −0.5 −0 = −0.5
and
z1 −c1 =
⎡
⎣
0
1
2
⎤
⎦•
⎡
⎣
0.8
2.5
0.2
⎤
⎦−5 = 2.9 −5 = −2.1.
Because all of the unit cost reductions are negative, we have arrived at a minimizer! The
solution is (x1, x2, x3) = (0, 45, 15). The slack variable values of x4 = 40, x5 = 0 are not
part of the solution to the original problem but do indicate how much “wiggle room” is
left in the inequalities at the optimum solution. ⃝

1060
Advanced Engineering Mathematics
By the way, the Mathematica command
FindMinimum[{5x1 + x2 + 2x3, −x1 −x2−x3 + x4 == −20 &&
2x1 + x2 −x3 + x5== 30 && 3x1 + x2 + x3 == 60 &&
x1 ≥0 && x2 ≥0 && x3 ≥0 && x4 ≥0 && x5 ≥0}, {x1, x2, x3, x4, x5}]
gave output
{75., {x1 →0., x2 →45., x3 →15., x4 →40., x5 →0.}}
that checks our conclusion for Example 13.11. While Mathematica can easily solve our small
LP problems, we needed to work through them by hand in order to understand how the
simplex procedure works.
13.4.2 Problems
In problems 1–8 solve the LP problem.
1.
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
x1 + x2
Subject to
x1 + 2x2 ≥3
x1, x2 ≥0.
⎫
⎪⎪⎬
⎪⎪⎭
2.
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
2x1 + x2 + x3
Subject to
x1 + 2x2 −x3
= 20
−x1 −x2 + 2x3 + x4
= 16
2x1 + x2 + 2x3
= 12
x1, . . . , x4 ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
3.
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
3x1 + x2 + 2x3
Subject to
−x1 −x2 −x3 + x4
= −10
2x1 + x2 + x3 + x5
=
40
3x1 + x2 + x3
=
50
x1, . . . , x5 ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
4.
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
5x1 + 2x2 + x3
Subject to
2x1
+x2
+x3
= 60
3x1
+x2
+x3
≤80
2x1
+x2
−x3
≥40
x1, . . . , x3 ≥0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭

Optimization
1061
5.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
2x1 + 3x2 + x3
Subject to
2x1
+x2
+x3
= 60
3x1
+x2
+x3
≤80
2x1
+x2
−x3
≥40
x1, . . . , x3 ≥0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
6.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
3x1 −2x3
Subject to
x1
+3x2
+4x3 = 80
3x1
−x2
+x3 ≤160
x1
+x2
+x3 ≥40
x1, . . . , x3 ≥0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
7.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
x1 + x2 + 2x3
Subject to
x1
+3x2
+4x3 ≤70
3x1
−x2
+x3 ≤110
x1
+x2
+x3 ≥40
x1, . . . , x3 ≥0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
8.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
3x1 + x2 −2x3
Subject to
−x1
+3x2
−x3
= 7
−2x2
+x3
+x4
= 12
−4x2
+3x3
+x5
= 10
x1, . . . , x5 ≥0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
In problems 9–11, solve a structural optimization problem of a continuous beam. [The
model comes from the book by Kirsch referred to in Section 13.2.] [Hint: Substitute λPℓ= μ
and xj = Mj/MPℓ, j = 1, 2, 3.]
9.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Maximize
λ
Subject to
4M1 + 2M2
= λPℓ
2M2 + 4M3 = 2λPℓ
−MPℓ≤Mj ≤MPℓ, j=1, 2, 3
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
10.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Maximize
λ
Subject to
4M1 + 2M2
= λPℓ
2M2 + 4M3 = λPℓ
−MPℓ≤Mj ≤MPℓ, j=1, 2, 3
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
11.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Maximize
λ
Subject to
4M1 + 2M2
= 5λPℓ
2M2 + 4M3 = λPℓ
−MPℓ≤Mj ≤MPℓ, j=1, 2, 3
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
12. Solve the LP problem of Problem 13.3.3.6.
13. Solve the LP problem of Problem 13.3.3.7.

1062
Advanced Engineering Mathematics
13.5 Nonlinear Programming
Suppose x is in Rn and f(x) and fi(x), i = 1, . . . , m, are functions which have a common
domain C in Rn. We want to minimize the value of the objective function f(x). The func-
tions f1, . . . , fm give constraints concerning which x can be put into the objective function.
The nonlinear programming problem is
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
f(x)
Subject to
f1(x) ≤0
...
fm(x) ≤0
x is in C
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(13.27)
Definition 13.11
A set C in Rn is convex if for every pair of points x, y in C the line segment
[x, y] ≜{(1 −t)x + ty : 0 ≤t ≤1}
is contained in C.
In R2, Figure 13.10 shows two examples of a convex set C.
4
y
2
0
–2
–4
–6
x
–4
–2
0
2
4
6
FIGURE 13.10
Example 13.12.

Optimization
1063
Definition 13.12
Suppose a function f = f(x) is defined on a convex set C in Rn. We say f(x) is
(a) Convex if
f

(1 −t)x + ty

≤(1 −t)f(x) + tf(y), for all 0 ≤t ≤1, x, y in C
(13.28)
(b) Strongly (or strictly) convex if
f

(1 −t)x + ty

< (1 −t)f(x) + tf(y), for all 0 < t < 1, x ̸= y in C
(13.29)
In Problem 13.1.3.9, you explained why a twice continuously differentiable function of
a single variable defined on an interval is convex if, and only if, its second derivative is
nonnegative on that interval. So, “convex” is what we called “concave up” for a function
of a single variable in Calculus I.
Recall the definition of the Hessian matrix,
D2f(x) ≜
"
∂2f
∂xi∂xj
(x)
#
1 ≤i ≤n
1 ≤j ≤n
.
Theorem 13.13
Suppose f(x) is a function whose domain is a convex set C in Rn and whose gradient
vector, ∇f(x), and Hessian matrix, D2f(x), are continuous on C. Then the following are
equivalent:
(a) f is convex on C.
(b) f(x2) −f(x1) ≥∇f(x1) • (x2 −x1) for all x1, x2 in C.
(c) D2f(x) is positive semi-definite at all x in C.
Corollary 13.3
Suppose A is an n × n real, constant, positive semi-definite matrix and c is a real, constant
vector in Rn. Then f(x) ≜xTAx and g(x) ≜cTx are convex on all of Rn.
Why? You will derive these results in Problem 13.5.3.14.
Recall from Section 13.3 the notation (x1, . . . , xn) = x ≥0, which means xj ≥0 for
j = 1, . . . , n. Similarly, (y1, . . . , ym) = y < 0 means yj < 0 for j = 1, . . . , m.

1064
Advanced Engineering Mathematics
Definition 13.13
Suppose f(x) and fi(x), i = 1, . . . , m, are functions whose common domain is a convex set
C in Rn. Denote f(x) = [ f1(x)
· · ·
fm(x) ]T in Rm.
(a) The feasible region is the set R ≜{x in C : f(x) ≤0}.
(b) The Lagrangian is the function F(x, λ) ≜f(x) + m
j=1 λj fj(x) = f(x) + λf(x), where
λ = [ λ1
· · ·
λm ]T is a vector of Lagrange multipliers.
(c) Slater’s condition is that there exists an x0 such that f(x0) < 0. Such an x0 is called
a strictly feasible solution.
Theorem 13.14
Suppose f(x) is a convex function whose domain is a closed, convex set C in Rn. Then f
achieves its minimum in C, that is, there exists an x⋆in C satisfying
f(x⋆) ≤f(x) for all x in C
as long as either
(a) C is bounded∗or
(b) f(x) is both strongly convex on C and twice continuously differentiable on C.
Definition 13.14
Suppose f(x) and fi(x), i = 1, . . . , m, are convex functions whose common domain is a
convex set C. A convex programming (CP) problem is
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
f(x)
Subject to
f1(x) ≤0
...
fm(x) ≤0
x is in C
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(13.30)
Theorem 13.15
(Karush–Kuhn–Tucker (KKT) necessary conditions) If x⋆is a global minimizer for the
nonlinear programming problem (13.27), then there exists a vector of Lagrange multipliers
∗See Section 2.10 for the definitions of “closed” and “bounded.”

Optimization
1065
λ⋆= [ λ⋆
1
· · ·
λ⋆
m ]T such that
λ⋆≥0
and
λ⋆
i fi(x⋆) = 0 for i = 1, . . . , m,
(13.31)
and
∇f(x⋆) +
 ∂fi
∂xj
(x⋆)
T
λ⋆= 0,
(13.32)
in addition to the original requirements of the CP problem that
x⋆is in C
and
f(x⋆) ≤0.
(13.33)
(13.32) is called the stationarity condition, and (13.31)’s conclusion “λ⋆
i fi(x⋆) = 0, i =
1, . . . , m”, is called the complementarity condition. As we will see, the stationarity con-
clusion is the same as the “vector field” conclusion of the method of Lagrange multipliers
we saw in Section 13.2 for problems with equality constraints.
Conclusions (13.31) and (13.32) of Theorem 13.15 are also known as the KKT or Kuhn–
Tucker (KT) conditions.
Theorem 13.16
(KKT sufficient conditions) Suppose that Slater’s condition holds, that is, that there is a
strictly feasible solution, and the functions f(x) and fi(x), i = 1, . . . , m, are all convex func-
tions on a common, convex domain C. If there exists x⋆in C with f(x⋆) ≤0 and a vector
of Lagrange multipliers λ⋆such that the KKT conditions are satisfied, then x⋆is a global
minimizer for the convex programming problem (13.30).
Our first example is similar to Example 13.5 in Section 13.2.
Example 13.12
Use the KT conditions to solve the CP problem of finding where the regions x2−x−y ≤0
and 12 −X + 2Y ≤0 are closest to each other.
Method: (X −x)2 + (Y −y)2 is the square of the distance between a point (x, y) in the
region x2 −x −y ≤0 and a point on (X, Y) in the region 12 −X + 2Y ≤0. We consider
X, Y, x, y to be the variables of the problem, which we state as
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
(X −x)2 + (Y −y)2
Subject to
x2 −x −y ≤0
12 −X + 2Y ≤0
⎫
⎪⎪⎬
⎪⎪⎭
.
(13.34)
Define f(X, Y, x, y) ≜(X−x)2+(Y−y)2, f1(X, Y, x, y) ≜x2−x−y, f2(X, Y, x, y) ≜12−X+2Y
and let λ1, λ2 ≥0 be the Lagrange multipliers.
Note that f, f1, and f2 are convex functions everywhere, as we can see from using either
Theorem 13.13 or Corollary 13.3.

1066
Advanced Engineering Mathematics
As in Examples 13.5 and 13.6 in Section 13.2, the gradient is
∇= ˆI ∂
∂X + ˆJ ∂
∂Y + ˆı ∂
∂x + ˆj ∂
∂y.
The stationarity condition, (13.32), is
⎡
⎢⎢⎣
2(X −x)
2(Y −y)
−2(X −x)
−2(Y −y)
⎤
⎥⎥⎦= ∇f(X, Y, x, y) = −
⎡
⎢⎣
∂f1
∂X
∂f1
∂Y
∂f1
∂x
∂f1
∂y
∂f2
∂X
∂f2
∂Y
∂f2
∂x
∂f2
∂y
⎤
⎥⎦
T
λ
= −λ1
⎡
⎢⎢⎣
0
0
2x −1
−1
⎤
⎥⎥⎦−λ2
⎡
⎢⎢⎣
−1
2
0
0
⎤
⎥⎥⎦,
(13.35)
which also appeared in Example 13.5 in Section 13.2, with λ = −λ1 and μ = −λ2.
Equation (13.35) implies
(1) λ1(2x −1) = 2(X −x) = λ2
and
(2) λ1 = −2(Y −y) = 2λ2;
hence,
0 = 2λ2 −2λ2 = 2λ1(2x −1) −λ1 = λ1

2(2x −1) −1

.
It follows that
either λ1 = 0
or
x = 3
4.
(13.36)
The complementarity conditions are
λ1(x2 −x −y) = λ2(12 −X + 2Y) = 0.
In effect, the complementarity conditions say that either the multiplier λ1 is zero or the
point (x, y) lies on the boundary of the region x2 −x −y ≤0, and either the multiplier λ2
is zero or the point (X, Y) lies on the boundary of the region 12 −X + 2Y ≤0.
Combine the first complementarity condition and (13.36) to imply that
λ1 = 0
or
y = x2 −x =
3
4
2
−3
4 = −3
16.
So, either λ1 = 0 or (x, y) = ( 3
4, −3
16).
Suppose λ1 = 0. Then (1) and (2) imply X = x, Y = y, and λ2 = 0. The second
complementarity condition is then satisfied because λ2 = 0. The only facts we have left
to work with are the feasibility requirements in (13.33), specifically f(x⋆) ≤0, that is, that
x2 −x −y ≤0 and 12 −X + 2Y ≤0. It follows that
y ≥x2 −x
and
Y ≤−6 + 1
2 X.
(13.37)
But X = x and Y = y, so x2−x ≤y ≤−6+ 1
2 x; hence, 0 ≥x2−3
2 x+6 =

x −3
4
2
+ 87
16 ≥87
16,
which is impossible. So, we conclude that λ1 ̸= 0.

Optimization
1067
So far, we have concluded that (x, y) = ( 3
4, −3
16) and λ1 > 0. To find (X, Y), note that
(1) and (2), along with λ1 > 0 and the fact that 2x −1 = 2 · 3
4 −1 ̸= 0, together imply
2Y+ 3
8 = 2

Y+ 3
16

= 2(Y−y)= −λ1= −2(X−x)
(2x−1) = −2(X−3
4)
(2 · 3
4 −1)
= −2X−3
2
1
2
=−4X + 3;
hence,
Y = −2X + 21
16.
The second complementarity condition is that either λ2 = 0 or 12−X+2Y = 0. If λ2 = 0
then, again, (1) and (2) imply X = x and Y = y, leading eventually to a contradiction as
in the preceding argument. If 12 −X + 2Y = 0, then we use the fact that Y = −2X + 21
16,
hence
0 = 12 −X + 2Y = 12 −X + 2

−2X + 21
16

= 12 −5X + 21
8 ;
hence, X = 117
40 , and thus Y = −2 117
40 + 21
16 = −363
80 . The closest approach of the two
regions is where (x, y) = ( 3
4, −3
16), (X, Y) =

117
40 , −363
80

, and the minimum distance is
$117
40 −3
4
2
+

−363
80 + 3
16
2
=
$
872
402 + 3482
802 = 87
√
5
40
. ⃝
The results of Example 13.12 are depicted in Figure 13.10.
Example 13.13
Solve
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
−ln x −3 ln y
Subject to
2x + y ≤4
x > 0, y > 0
⎫
⎪⎪⎬
⎪⎪⎭
.
(13.38)
Method: The objective function, f(x, y) = −ln x −3 ln y, is convex on the convex set
C ≜{(x, y) : x > 0, y > 0} by Problems 13.1.3.10 and 13.1.3.12(a). The constraint function
f1(x, y) = 2x+y−4 is also convex on C, by a result in Corollary 13.3, so we can use the KT
conditions to solve the CP problem (13.38). The feasible region is shown in Figure 13.11.
Let λ ≥0 be the Lagrange multiplier.
The feasibility conditions are 2x + y −4 ≤0, x > 0, y > 0. The complementarity
condition is
λ · (2x + y −4) = 0.
(13.39)
The stationarity condition is
⎡
⎢⎣
−1
x
−3
y
⎤
⎥⎦+ λ
⎡
⎣
2
1
⎤
⎦=
⎡
⎣
0
0
⎤
⎦.
(13.40)

1068
Advanced Engineering Mathematics
4
2
x
y
2
FIGURE 13.11
Example 13.13.
But, (13.40) implies λ ̸= 0, because −1
x cannot be zero for a feasible (x, y). So, the
complementarity condition implies
2x + y −4 = 0.
(13.41)
(13.40) also implies
y = 3
λ
and
x = 1
2λ.
Substitute those into (13.41) to get
4 = 2
 1
2λ

+
 3
λ

= 4
λ;
hence, λ = 1, and hence, x = 1
2 and y = 3. The minimum value of the objective function
with these constraints is f( 1
2, 3) = −ln 1
2 −3 ln 3 = −ln 27
2 . ⃝
13.5.1 Dual LP Problem
Suppose we have an LP problem in the preliminary standard form (13.21) in Section 13.3,
that is,
⎧
⎨
⎩
Minimize
cTx
Subject to
Ax ≤b
and
x ≥0
⎫
⎬
⎭,
where
x and c are in Rk
b are in Rm
A is a real, m × k matrix.
We can refer compactly to (13.21) in Section 13.3 as the “LP problem for the
‘triple’[ A, b, c ].”

Optimization
1069
Definition 13.15
The LP problem [ A, b, c ] has dual LP problem [ −AT, c, −b ], that is, to find y in Rm to
⎧
⎨
⎩
Minimize
−bTy
Subject to
−ATy ≤c
and
y ≥0
⎫
⎬
⎭.
(13.42)
Note that the dual LP problem (13.42) can be restated as
⎧
⎨
⎩
Maximize
bTy
Subject to
ATy ≥−c
and
y ≥0
⎫
⎬
⎭.
Theorem 13.17
(LP duality theorem) Suppose that either the LP problem [A, b, c] has an optimal solu-
tion x⋆or its dual, [ −AT, c, −b ], has an optimal solution y⋆. Then both problems have an
optimal solution and
cTx⋆= −bTy⋆.
Why? Define the convex set C = {x : x ≥0}.
First, we will use the KT conditions to explain why [A, b, c] having an optimal solution
x⋆implies [ −AT, c, −b ] has an optimal solution y⋆and cTx⋆= −bTy⋆.
Define f(x) ≜cTx and f(x) ≜Ax −b, and define the CP problem
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
f(x)
Subject to
f(x) ≤0
x is in C
⎫
⎪⎪⎬
⎪⎪⎭
.
By the KT conditions, there exists a vector of Lagrange multipliers λ⋆= [ λ⋆
1
· · ·
λ⋆
m ]T
such that
λ⋆≥0
and
λ⋆
i fi(x⋆) = 0 for i = 1, . . . , m
(13.43)
and
∇f(x⋆) = −
 ∂fi
∂xj
(x⋆)
T
λ⋆,
(13.44)
in addition to the original requirements of the CP problem that
x⋆is in C
and
f(x⋆) ≤0,
that is, that x⋆≥0
and
Ax⋆≤b.
(13.45)

1070
Advanced Engineering Mathematics
From f(x) ≜cTx, we calculated in Problem 13.2.3.17 that
∇f(x⋆) = c.
From f(x) ≜Ax −b, we calculate that
 ∂fi
∂xj
(x⋆)
T
= (A)T = AT.
So, the stationarity condition is
c = −ATλ⋆.
The complementarity conditions in (13.43) are
λ⋆
i ·

ai1x⋆
1 + · · · + aikx⋆
k −bi

= 0, for i = 1, . . . , m.
Summing up zeros over i = 1, . . . , m implies
bTλ⋆= (Ax⋆)Tλ⋆= (x⋆)TATλ⋆= −(x⋆)Tc,
that is,
−bTλ⋆= cTx⋆.
If we define y0 ≜λ⋆, then the KT conditions and the preceding work explain why y0
satisfies
y0 ≥0,
−ATy0 = c,
and
−bTy0 = cTx⋆.
This shows that the dual problem, [−AT, c, −b], has a feasible solution, y0, and therefore,
there is a y⋆that is both feasible, that is, satisfies y⋆≥0 and −ATy⋆≤c, and minimizes the
dual problem’s objective function, bTy. [Note that there may be no relationship between
y0 = λ⋆and y⋆.]
It follows from the minimality of bTy⋆that
−bTy⋆≤−bTy0 = cTx⋆.
(13.46)
But, the preceding argument can be used if we exchange the roles of the two prob-
lems: [A, b, c] is the dual of [−AT, c, −b]. The KT conditions for the global minimizer y⋆for
[−AT, c, −b] imply there is a μ⋆≥0 for which stationarity holds, that is, b = −(−AT)Tμ⋆=
Aμ⋆and the complementarity conditions
μ⋆
i ·

−(a1jy⋆
1 + · · · + akjy⋆
k) −cj

= 0, for j = 1, . . . , k
hold. As before, summing up zeros over i = 1, . . . , m implies
cTμ⋆= (−ATy⋆)Tμ⋆= (y⋆)T(−A)μ⋆= −(y⋆)Tb,
that is,
cTμ⋆= −(y⋆)Tb.

Optimization
1071
If we define x0 = μ⋆, we see that x0 is a feasible solution of [A, b, c], and thus the original
problem’s optimal solution x⋆satisfies
cTx⋆≤cTμ⋆= −bTy⋆.
(13.47)
This and (13.46) imply that
cTx⋆≤cTμ⋆= −bTy⋆≤−bTy0 = cTx⋆;
hence, equality holds throughout, and hence,
cTx⋆= −bTy⋆. 2
13.5.2 Application: Geometric Tolerancing
Geometric Dimensioning and Geometric Tolerancing are sets of ANSI standards for the
quantitative description of machine parts and constraints on errors in their manufacture.
For example, an engine design might require that the difference between a piston radius
and its cylinder bore radius vary by a magnitude of at most 0.0002 in.
One basic problem is to check if a particular part’s dimensions are within the margin of
error. In order to do that, it may be necessary to rotate and/or translate a part to see if it
fits within an allowed region. This tests the straightness tolerancing of the part.
For example, in Figure 13.12a, we see a curved side of a part. We want to quantify how
close it is to being straight. The curviness of the side is exaggerated for convenience in
illustrating the concepts involved.
After rotating the plane by an angle θ and translating the origin by (X, Y), we get the
curve in Figure 13.12b. The number ts measures how “unstraight” the side is. For example,
the whole manufactured part might be rejected if ts > 0.01 mm.
In order to measure the straightness of a curved side, we need to choose the rotation
angle θ and the vertical translation Y so that points (xi, yi) on the curve are mapped to
points (x′
i, y′
i) lying in the strip 0 ≤y′ ≤ts. After rotation by θ and translation by (X, Y), the
point (x, y) is mapped to the point
x′
i
y′
i

=
cos θ
−sin θ
sin θ
cos θ
 xi
yi

+
X
Y

;
(xi, yi)
(xi΄, yi΄)
Y
y΄
  x΄
ts
θ
(a)
(b)
FIGURE 13.12
(a) Curved side, (b) Side after rotation and translation to check straightness.

1072
Advanced Engineering Mathematics
hence, y′
i = xi sin θ + yi cos θ + Y. Testing the part involves mapping some points (xi, yi),
i = 1, . . . , N on the original curve to the points (x′
i, y′
i). The straightness of a curve can be
measured by ts, the objective function of the nonlinear programming problem (Chen and
Fan, 2002)
⎧
⎨
⎩
Minimize
ts
Subject to
0 ≤xi sin θ + yi cos θ + Y ≤ts, i = 1, . . . , N
⎫
⎬
⎭.
(13.48)
There is no sign restriction on either θ or Y. The paper referred to “Tolerance evaluation
of minimum zone straightness· · · ” goes on to solve the nonlinear programming problem
using a commonly used minimization algorithm that is available via the Excel software
package.
Learn More About It
Example 13.13 is a version of an example in “A duality theorem for convex programs,”
W. S. Dorn, IBM J. Res. Dev. 4 (1960) pp. 407-413.
There are many sources to learn the justification of the KKT conditions, including
Linear Programming: Methods and Applications, 5th edn., by Saul I. Gass, McGraw-Hill,
Inc. c⃝1985, Linear and Nonlinear Programming, 3rd edn., by David G. Luenberger and
Yinyu Ye, Springer c⃝2008, and Elements of Applicable Functional Analysis, by Charles
W. Groetsch, Marcel Dekker, Inc. c⃝1980.
13.5.3 Problems
1. Use the KT conditions to find where the regions x2 −2x −y ≤0 and 8 −X + Y ≤0
are closest to each other.
2. Use the KT conditions to find where the regions x2−2x+1−y ≤0 and 6−X+Y ≤0
are closest to each other.
3. Solve
⎧
⎨
⎩
Minimize
−2 ln x −3 ln y
Subject to
2x + y ≤4
x > 0, y > 0
⎫
⎬
⎭.
4. Solve
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
−ln x −2 ln y
Subject to
−x + y ≤1
2x + y ≤3
x > 0, y > 0
⎫
⎪⎪⎬
⎪⎪⎭
.
5. Given xj, j = 1, . . . , p in a convex set C and nonnegative real numbers λj satisfying
λ1 + · · · + λp = 1, explain why λ1x1 + · · · + λpxp also must be in C.
6. Suppose x0 is in Rn and r > 0. Explain why each of the following are convex sets
in Rn:
(a) The closed solid ball {x : ||x −x0|| ≤r}
(b) The open solid ball {x : ||x −x0|| < r}
(c) A vector subspace V.

Optimization
1073
7. Suppose f(x) is a convex function of x on a convex set C and M is any real number.
Explain why S ≜{x : f(x) ≤M} is either a convex set or empty.
8. (a) Suppose f(x) and g(x) are convex functions defined on a common, convex set
C in Rn. Explain why f(x) + g(x) is also convex on C.
9. We know from Problem 13.1.3.12(b) that in R1, the composition of two convex
functions need not be convex. A vector-valued function g(x) on Rn is affine
if g(x) = Ax + b for some matrix A and b in Rn. Suppose f(y) is convex on
Rm and g(x) is affine on Rn and takes on values in Rm. Explain why f

g(x)

is
convex on Rn.
10. Suppose that C is a convex set in Rn and for = 1, . . . , m, fi(x) is a convex function
on common domain C. Explain why the set of feasible points, that is,
{x in C : f(x) ≤0 and x ≥0}
is a convex set in Rn.
11. Suppose C1 and C2 are convex subsets of Rm. Explain why their intersection, C1 ∩
C2 ≜{x : x is in both C1 and C2}, is also convex.
12. Explain why the results in Corollary 13.3 are true.
13. Suppose that C is a convex set in Rn, f(x) is a convex function on C, and x⋆is a
local minimizer of f on C. Explain why x⋆must be a global minimizer of f on C.
[Hint: This is a generalization of Theorem 13.4 in Section 13.1.]
14. Here you will establish the results of Corollary 13.3 without using the results of
Theorem 13.13: Suppose A is an n × n real, constant, positive semi-definite matrix
and c is a real, constant vector in Rn. Explain why f(x) ≜xTAx and g(x) ≜cTx are
convex on all of Rn.
15. Suppose that A is an n × n real, constant, positive definite matrix. Explain why
f(x) ≜xTAx is strictly convex on all of Rn.
16. Suppose f(x) is strictly convex on a convex set C. Explain why f cannot have two
distinct global minimizers in C.
13.6 Rayleigh–Ritz Method
Recall from Definition 2.23 in Section 2.9 that the Rayleigh quotient of a real, symmetric
matrix A is defined by
RA(x) ≜⟨x, Ax⟩
⟨x, x⟩= xTAx
|| x ||2 ,
for x ̸= 0.
(13.49)
Also, recall from Theorem 2.45 in Section 2.9 that if A is a real symmetric matrix, then the
values
λ1 ≜max{RA(x) : x satisfying x ̸= 0}
and
λn ≜min{RA(x) : x satisfying x ̸= 0}

1074
Advanced Engineering Mathematics
exist and are eigenvalues of A, and there are eigenvectors x(1) and x(n), respectively, that
“achieve” the values λ1 and λn, that is,
λ1 = RA(x1)
and
λn = RA(xn).
Recall that RA(βx) = RA(x) for all nonzero scalars β. In Section 2.9, we used this to
argue that we need only study the Rayleigh quotient for || x || = 1.
Note that if A is n × n and 1 = || x ||2 = xTx = n
i=1 x2
j then
RA(x) = xTAx =
n

i=1
n

j=1
aijxixj .
It follows that we can, at least theoretically, find λ1 and x(1) by solving a maximization
problem and similarly find λn and x(n) by solving a minimization problem.
Example 13.14
Use Lagrange multipliers to explain why the minimum and the maximum of the
Rayleigh quotient of a real, symmetric matrix A are eigenvalues.
Method: Our two optimization problems are of the form (13.7) in Section 13.2, specifically
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Maximize
n

i=1
n

j=1
aijxixj
Subject to
−1 +
n

i=1
x2
i = 0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(13.50)
and
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
n

i=1
n

j=1
aijxixj
Subject to
−1 +
n

i=1
x2
i = 0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
(13.51)
Note that (13.50) could be rewritten as to call for minimizing

−n
i=1
n
j=1 aijxixj

, hence
could be rewritten in the form of (13.7) in Section 13.2.
Define
f(x) ≜
n

i=1
n

j=1
aijxixj.
For example, when n = 2, f(x1, x2) = a11x2
1 + a12x1x2 + a21x2x1 + a22x2
2.
So far we have not used the fact that A is symmetric. For example, when n = 2,
symmetry of A implies f(x1, x2) = a11x2
1 + 2a12x1x2 + a22x2
2. Similarly, for any n ≥2,
f(x) =
n

i=1
aiix2
i + 2

j̸=i
aijxixj.

Optimization
1075
This helps us to calculate
∂f
∂xi
= 2aiixi + 2

j̸=i
aijxj = 2(Ax)i,
that is, twice the ith component of the vector Ax.
Define g(x) = −1 + n
i=1 x2
i . We calculate that
∂g
∂xi
= 2xi = 2(x)i,
twice the ith component of the vector x.
Because ∇g = 2x ̸= 0 for all x that satisfy the constraint g(x) = || x ||2 −1 = 0, (13.50)
has Lagrange multiplier λ satisfying
2Ax⋆= ∇f(x⋆) = λ ∇g(x⋆) = 2λ x⋆,
where x⋆is a maximizer. It follows that
Ax⋆= λx⋆.
Because x⋆̸= 0, we conclude that x⋆is an eigenvector of A. We note that the maximizer
x⋆for problem (13.50) satisfies
f(x⋆) = (x⋆)TAx⋆= (x⋆)T(λx⋆) = λ ||x⋆||2 = λ · 1 = λ.
But, as we saw in Theorem 2.45 in Section 2.9, both the maximum and minimum
values of RA(x) are eigenvalues of A. So, the maximum is an eigenvalue, with cor-
responding eigenvector x⋆. The same reasoning explains why the minimum is an
eigenvalue, with corresponding eigenvector being a minimizer. ⃝
In Example 2.37 in Section 2.9 we saw that the Rayleigh quotient for the matrix
A =
⎡
⎣
−4
2
3
2
−5
1
3
1
−8
⎤
⎦
produced approximate eigenvalues λ1 ≈−1.10996, for the maximum eigenvalue, and
λn ≈−9.60653, for the minimum eigenvalue.
13.6.1 Other Eigenvalues
Suppose that for a symmetric matrix A we have found λ1, the maximum eigenvalue, and
a corresponding eigenvector x(1) ≜[x(1)
1
· · ·
x(1)
n ]T with ||x(1)||2 = 1. After that, we can
look for another eigenvalue λ2 ≤λ1 and an eigenvector x that is orthogonal to x(1) by using
the Rayleigh–Ritz method of solving the maximization problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Maximize
xTAx
Subject to
xTx −1 = 0
xTx(1) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(13.52)

1076
Advanced Engineering Mathematics
Again, we can use the method of Lagrange multipliers, except now with two constraints
and thus two multipliers. Define f(x) ≜xTAx, f1(x) ≜xTx−1, and f2(x) ≜xTx(1). Symmetry
of A implies that, as in Example 13.14, we have
∇f = 2Ax,
∇f1 = 2x,
and
∇f2 = x(1).
We conclude that there exist multipliers μ1, μ2 and global minimizer x⋆for which
stationarity holds, that is,
2Ax⋆= 2μ1x⋆+ μ2x(1)
(13.53)
and
||x⋆||2 −1 = 0 =

x⋆T x(1).
(13.54)
Take the dot product of (13.53) with x⋆and use (13.54) to get
2

x⋆T Ax⋆= 2μ1||x⋆||2 + μ2

x⋆T x(1) = 2μ1 · 1 + μ2 · 0 = 2μ1.
This shows that μ1 is the global maximum value of xTAx subject to the constraints (13.54).
On the other hand, take the dot product of (13.53) with the unit vector x(1) and use (13.54)
to get
2

x(1)T
Ax⋆= 2μ1

x(1)T
x⋆+ μ2||x(1)||2 = 2μ1 · 0 + μ2 · 1 = μ2.
(13.55)
But, symmetry of A implies

x(1)T
Ax⋆=

x(1)T
ATx⋆=

Ax(1)T
x⋆=

λ1x(1)T
x⋆= λ1

x(1)T
x⋆= λ1 · 0,
so (13.55) implies μ2 = 0. It follows that stationarity, (13.53), actually implies
2Ax⋆= 2μ1x⋆+ 0 · x(1) = 2μ1x⋆;
hence, λ2 ≜μ1 is an eigenvalue of A with corresponding eigenvector x⋆.
The last thing to notice is that λ2 ≤λ1 follows from
λ1 = max{RA(x) : x satisfying || x || = 1}
≥max{RA(x) : x satisfying || x || = 1 and xTx(1) = 0} = λ2.
This is true because the maximum over a set is greater than or equal to the maximum over
its subset, as we announced in Lemma 13.1 in Section 13.2.
We can continue in this way: After having found eigenvalues λ1 ≥λ2 ≥· · · ≥λk and
corresponding orthonormal set of eigenvectors x(1), . . . , x(k), we can pose the problem

Optimization
1077
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Maximize
xTAx
Subject to
xTx −1 = 0
xTx(1) = 0
...
xTx(k) = 0
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(13.56)
In Problem 13.6.3.3, you will explain why the global minimizer of (13.56) gives an
eigenvector corresponding to an eigenvalue λk+1 ≤λk.
13.6.2 Hilbert Space Eigenvalue Problems
The next result for a linear operator on Hilbert space generalizes the Rayleigh method for
a matrix on Rn.
Theorem 13.18
Suppose A is a bounded, self-adjoint operator on a Hilbert space H and define
RA(x) ≜⟨Ax, x⟩
⟨x, x⟩.
Assume there exists ∗a specific vector x(1) satisfying
RA(x(1)) = λ1 = max{RA(x) : x satisfying x ̸= 0}.
Then x(1) is an eigenvector of A.
Why? By Theorem 2.44(a) in Section 2.9, without loss of generality, we may assume that
||x(1)|| = 1. “Perturb” x(1) a little by considering
x = x(1) + εy,
where ε > 0, || y || = 1 and y ⊥x(1); the latter and the Pythagorean theorem guarantee that
|| x ||2 = ||x(1)||2 + || εy ||2 = 12 + | ε |2 || y ||2 = 1 + ε2.
Because λ1 is the maximum,
λ1 ≥RA(x) = ⟨Ax, x⟩
⟨x, x⟩= ⟨A(x(1) + εy), x(1) + εy⟩
||x(1) + εy||2
= ⟨A(x(1) + εy), x(1) + εy⟩
1 + ε2
.
(13.57)
∗In an infinite dimensional Hilbert space, if the operator A is “compact,” then an analytical argument can show
that x(1) exists.

1078
Advanced Engineering Mathematics
Multiply through (13.57) by 1 + ε2; also, expand the numerator and use the fact that A is
self-adjoint. This gives
(1 + ε2)λ1 ≥⟨Ax(1), x(1)⟩+ 2ε⟨Ay, x(1)⟩+ ε2⟨Ay, y⟩= λ1 + 2ε⟨Ay, x(1)⟩+ ε2⟨Ay, y⟩.
Subtract λ1 from both sides to get
ε2λ1 ≥2ε⟨Ay, x(1)⟩+ ε2⟨Ay, y⟩.
Divide both sides by ε; because it’s positive, this won’t change the direction of the
inequality. So,
ελ1 ≥2⟨Ay, x(1)⟩+ ε⟨Ay, y⟩.
Take the limit as ε →0+ to get
0 ≥⟨Ay, x(1)⟩.
(13.58)
Equation (13.58) is true as long as || y || = 1 and y ⊥x(1). So, we can replace y by −y
to get
0 ≥⟨A(−y), x(1)⟩.
Multiply through by (−1), which changes the direction of the inequality, so we get 0 ≤
⟨Ay, x(1)⟩, that is,
⟨Ay, x(1)⟩≥0.
(13.59)
Combining (13.58) and (13.59), we get
0 ≥⟨Ay, x(1)⟩≥0.
Using this and self-adjointness of A, it follows that
0 = ⟨Ay, x(1)⟩= ⟨y, Ax(1)⟩= ⟨Ax(1), y⟩.
So, every unit vector y that is orthogonal to x(1) is also orthogonal to Ax(1). It follows
from a Hilbert space generalization of Corollary 2.9 in Section 2.4 that x(1) and Ax(1) are
parallel; hence,
Ax(1) = αx(1)
for some scalar α. But we can explain why α = λ1:
α · 1 = α · ||x(1)||2 = α⟨x(1), x(1)⟩= ⟨Ax(1), x(1)⟩= λ1⟨x(1), x(1)⟩= λ1.
So,
Ax(1) = λ1x(1). 2

Optimization
1079
13.6.3 Problems
1. Explain why the global maximizer of (13.56) gives an eigenvector corresponding
to an eigenvalue λk+1 ≤λk.
In problems 2 and 3, use the Rayleigh quotient to find approximations of the minimum and
maximum eigenvalues of the 3 × 3 matrix found in problems 2.9.2.3 and 2.9.2.4, respec-
tively. You may use Mathematica or MATLAB® as in Example 2.37 in Section 2.9. After
that, use the Rayleigh–Ritz method to find an approximation of the middle eigenvalue of
the matrix.
2. A =
⎡
⎣
0
1
1
1
0
0
1
0
2
⎤
⎦
3. A =
⎡
⎣
2
√
3
0
√
3
0
0
0
0
−1
⎤
⎦
Key Terms
affine function: Problem 13.5.3.9
basic feasible solution: Definition 13.10 in Section 13.3
complementarity condition: (13.31) in Section 13.5
conjugate gradient method: after (13.18) in Section 13.2
constrained optimization problem: (13.7) in Section 13.2
convex function: Definition 13.3 in Section 13.1, Definition 13.12 in Section 13.5
convex programming (CP) problem: (13.30) in Section 13.5
convex set: Definition 13.11 in Section 13.5
critical number: Definition 13.2 in Section 13.1
critical point: Definition 13.6 in Section 13.2
critical radius of insulation: Example 13.1 in Section 13.1
Dantzig’s simplex procedure: Section 13.4
dual LP problem: Definition 13.15 in Section 13.5
feasible direction: Definition 13.4 in Section 13.2
feasible region: Definition 13.13 in Section 13.5
feasible solution: Definition 13.10 in Section 13.3
fully plastic moment: before (13.25) in Section 13.3
generalized normal equations: Problem 13.2.3.11
generalized or weighted least squares: Problem 13.2.3.11
geometric dimensioning, geometric : tolerancing: end of Section 13.5
global minimizer, global minimum (value): Definition 13.1 in Section 13.1, Definition
13.4 in Section 13.2
global maximizer: after Definition 13.1 in Section 13.1
Hessian matrix: Definition 13.8 in Section 13.2
interior: Definition 13.5 in Section 13.2
Lagrange multiplier(s): (13.8) in Section 13.2, (13.14) in Section 13.2, Definition 13.13 in
Section 13.5
Lagrangian: Definition 13.13 in Section 13.5

1080
Advanced Engineering Mathematics
line search after (13.18) in Section 13.2
line segment: Definition 13.11 in Section 13.5
linear inequalities: beginning of Section 13.3
Linear programming (LP) problem: beginning of Section 13.3
local (or relative) minimizer, local minimum (value): Definition 13.1 in Section 13.1,
Definition 13.4 in Section 13.2
matrix-tableau form: beginning of Example 13.11 in Section 13.4
method of steepest descents: after (13.18) in Section 13.2, Figure 13.8
minimum reduction: (13.26) in Section 13.4
minimization problem: (13.4) in Section 13.2
nonlinear programming problem: (13.27) in Section 13.5
objective function: (13.4) in Section 13.2
pivoting process: after Theorem 13.12 in Section 13.4
plasticity: before (13.25) in Section 13.3
Rayleigh quotient: (13.49) in Section 13.6
Rayleigh–Ritz method: (13.52) in Section 13.6
saddle point: Definition 13.7 in Section 13.2
slack variable: before Definition 13.9 in Section 13.3
Slater’s condition: Definition 13.13 in Section 13.5
standard form for LP problem: Definition 13.9 in Section 13.3
stationarity: (13.32) in Section 13.5
stationary point: Definition 13.6 in Section 13.2
straightness tolerancing: end of Section 13.5
strictly feasible solution: Definition 13.13 in Section 13.5
strongly
(or strictly) convex: Definition 13.3 in Section 13.1, Definition 13.12 in
Section 13.5
unit cost reduction: beginning of Section 13.4.1
yield stress: before (13.25) in Section 13.3
Mathematica Commands
After Example 13.11 in Section 13.4:
FindMinimum[{5x1 + x2 + 2x3, −x1 −x2 −x3 + x4 == −20 &&
2x1 + x2 −x3 + x5 == 30 && 3x1 + x2 + x3 == 60 &&
x1 ≥0 && x2 ≥0 && x3 ≥0 && x4 ≥0 && x5 ≥0}, {x1, x2, x3, x4, x5}]
References
Aziz, A. The critical thickness of insulation. Heat Transfer Engineering 18, 61–91, 1997.
Chen, M.-C. and Fan, S.-K.S. Tolerance evaluation of minimum zone straightness using non-linear
programming techniques: A spreadsheet approach. Computers and Industrial Engineering 43,
437–453, 2002.
Cheney, E.W. Approximation Theory. AMS Chelsea Publishing, New York, 1982, pp. 25–26.
Kirsch, U. Structural Optimization: Fundamentals and Applications. Springer-Verlag, New York, 1993.

14
Calculus of Variations
14.1 Minimization Problems
The Calculus of variations, as a systematic way of modeling and solving physical
problems, is historically relevant but was also reinvigorated in the twentieth century in
the subjects of both control theory and finite element methods.
Very natural contexts for Calculus of variations include engineering mechanics and elec-
tromagnetism where we use knowledge of the energy in the system. For example, it makes
sense that if we deform a solid object then its new equilibrium shape should minimize its
potential energy.
There are many physical problems that are modeled by a Calculus of variations problem
of finding an “admissible” function y(x) so as to
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
b
a
F

x, y(x), y′(x)

dx
Subject to
y(a) = ya
y(b) = yb
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(14.1)
or subject to conditions on or including y′(a), y′(b). Also, the integral may involve higher
order derivatives of y. Often the problem has a natural interpretation in terms of energy in
a system.
In Chapter 13 we studied minimization of a function that depends on several inde-
pendent variables, that is, unknowns to be solved for. In Chapter 14 we will minimize
an integral which depends on a function which is the unknown to be solved for. That
is inherently a more mind boggling problem, but results from Chapter 13 will still be
relevant.
A function is admissible if it is continuous and piecewise continuously differentiable
on the interval [ a, b ]. If higher order derivatives are in the integrand then the class of
admissible functions may be further restricted to involve higher order differentiability.
A functional is a mapping from a vector space to scalar values. For example, in (14.1)
the mapping
y →
b
a
F

x, y(x), y′(x)

dx
is a functional.
1081

1082
Advanced Engineering Mathematics
We will see in the rest of the chapter that there are techniques for turning a problem such
as (14.1) into a differential equation with boundary conditions.
Here are some examples.
Example 14.1
Suppose a beam has rest state y(x) ≡0 for 0 ≤x ≤L. A linear theory for the vertical
deformation y = y(x) of the beam subject to a constant external force field f(x) in the
vertical direction assumes the total potential energy stored in the beam is
J = J[ y ] =
L
0
1
2 EI(y′′)2 + yf(x)

dx.
The first term in the integral is the strain energy. Assume that the beam is simply sup-
ported at the ends, so u(0) = u(L) = 0. Physically, it makes sense that the total potential
energy should be at a minimum, and we will see in Section 14.2 that this works out well
mathematically too. The calculus of variations problem is
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
L
0
1
2 EI(y′′)2 + yf(x)

dx
Subject to y(0) = y(L) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(14.2)
We will see in Section 14.2 that by varying y(x) we will arrive at the ODE-BVP
⎧
⎨
⎩
−EIy′′′′ + f(x) = 0
y(0) = y(L) = 0,
y′′(0) = y′′(L) = 0
⎫
⎬
⎭.
The natural boundary conditions y′′(0) = y′′(L) = 0 will appear as a consequence of the
minimization.
Example 14.2
Suppose a thin, “Euler–Bernoulli” rod has angular deflection θ = θ(s), for 0 ≤s ≤L,
and is subject to a compressive end force P, as illustrated in Figure 14.1. The potential
energy of the rod is
J[ θ ] =
L
0

1
2 EI
dθ
ds
2
−P(1 −cos θ)

ds.
The first term is the potential energy due to bending and the second term comes from
the load on the rod. Assume that the rod is pinned at the ends, so θ ′(0) = θ ′(L) = 0,
θ
P
FIGURE 14.1
Example 14.2.

Calculus of Variations
1083
where ′ = d
ds. Physically, it makes sense that the total potential energy should be at a
minimum, and we will see in Section 14.2 that this works out well mathematically too.
The calculus of variations problem is
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
L
0

1
2 EI
dθ
ds
2
−P(1 −cos θ)

dx
Subject to θ ′(0) = θ ′(L) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(14.3)
We will see in Section 14.2 that by varying θ(s) we will arrive at the nonlinear ODE-BVP
⎧
⎨
⎩
EIθ ′′ + P sin θ = 0
θ ′(0) = θ ′(L) = 0
⎫
⎬
⎭.
Example 14.3
Suppose a thin membrane has displacement z = u(x, y, t) for (x, y) in some nice set D in
R2. Suppose the edge of the membrane is kept at zero displacement, that is, u = 0, on the
boundary of D, and assume the force of gravity is negligible. By assuming a solution in
the form
u(x, y, t) = eiωtφ(x, y),
we are led to the eigenvalue problem for the Laplacian given by
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2φ
∂x2 + ∂2φ
∂y2 + ω2φ
0 ≡φ

(x,y) on boundary of D
⎫
⎪⎪⎬
⎪⎪⎭
.
We will see in Section 14.4 that this problem is equivalent to the calculus of variations
problem
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize

D
∂φ
∂x
2
+
∂φ
∂y
2
dx dy
Subject to

D
|φ(x, y)|2 dx dy = 1
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
(14.4)
Example 14.3 is a generalization of work on the Rayleigh Quotient in Section 9.5.
Example 14.4
(The brachistochrone problem) Suppose a bead starts at rest and then, under the influ-
ence of the force of gravity, slides (not rolls!) without friction down the curve z = f(x),
as shown in Figure 14.2. Set up a calculus of variations problem whose solution would
give the curve that will minimize the time it takes for the bead to reach the origin.
Method: Recall from Section 7.2 that the arclength, s, along the curve satisfies
ds
dx =

1 + ( f ′(x))2.

1084
Advanced Engineering Mathematics
z
z0
x
x0
FIGURE 14.2
Example 14.4.
hence
ds =

1 + ( f ′(x))2 dx.
(14.5)
Suppose the bead has mass m and that during its slide its position vector is
r = r(t) = x(t) ˆı + z(t) ˆk.
The total energy, that is, sum of the kinetic and potential energies, is
E = 1
2mv2 + mgz.
This is conserved, so

v(t)
2 + 2gz(t) ≡02 + 2gz0,
(14.6)
after recalling that the bead starts from rest at position (x, z) = (x0, z0). But, (14.6) implies
that the velocity of the particle is
0 < ds
dt = ||˙r(t)|| = |v(t)| =

2g (z0 −z(t)),
and z(t) = f(x(t)), so
ds =

2g

z0 −f(x(t)) dt,
hence
dt
ds =
1

2g

z0 −f(x(t))
.
It follows from (14.5) that
dt = dt
ds ds =
1

2g

z0 −f(x)

1 + ( f ′(x))2 dx.

Calculus of Variations
1085
So, the total time required for the journey is
t =
0
x0
1

2g

z0 −f(x)

1 + ( f ′(x))2 dx.
(14.7)
The calculus of variations problem is to minimize
J[ f ] ≜
0
x0
1

2g

z0 −f(x)

1 + ( f ′(x))2 dx
(14.8)
subject to f(x0) = z0 and f ′(x) is continuously differentiable on [0, x0]. 2
It turns out that the curve that gives the shortest time is not a straight line from (x0, z0)
to (0, 0) despite the fact that that would give the shortest path. Why not? For one thing, the
velocity and the speed of the bead are not constant during its journey.
14.1.1 The Rayleigh–Ritz Method
Suppose that abstractly we have a problem
Au = f
and
u is in V.
(14.9)
Then we will see that we can use the corresponding calculus of variations problem of
minimizing
J[ u ] ≜⟨Au, u⟩−2⟨u, f⟩
(14.10)
to find an approximate solution. Moreover, in some problems the approximate minimum
value of J[ u ] has physical significance.
For example, in Section 12.5 the problem

p(x)y′(x)
′ + q(x)y(x) = f(x), y(0) = y(L) = 0
was rewritten in the form (14.9) using a differential operator A defined by
(Au)(x) ≜−

p(x)u′(x)
′ + q(x)u(x)
on functions u = u(x) in the space V ≜C2
0[0, L] defined by
u is in V if u(x) is twice continuously differentiable on [0, L] and u(0) = u(L) = 0.
The Rayleigh–Ritz method we used in Section 9.7 for eigenvalue problems can be used
to find an approximate solution for a problem in the form (14.9): If u is a minimizer for the
problem
J[ y ] =
L
0

p(x)

y′(x)
2 + q(x)

y(x)
2 −2y(x)f(x)

dx
and satisfies the BCs u(0) = u(L) = 0 then it should satisfy the ODE

p(x)u′(x)
′ +
q(x)u(x) = f(x).

1086
Advanced Engineering Mathematics
Example 14.5
Use the Rayleigh–Ritz method to find an approximate solution of

−

ϱ(x)y′′ = f(x)
y(0) = y(1) = 0

,
(14.11)
where mass density ϱ(x) ≜3+cos(πx) and load f(x) = 0.2x. This models a loaded string
that is fixed at the ends x = 0 and x = 1.
Method:
It makes sense to look for an approximate solution that also satisfies the
boundary conditions, for example, in the form
y(x) = c1x(1 −x) + c2x2(1 −x).
(14.12)
In Section 9.7 we referred to this as using a family of “trial” functions.
Using MathematicaTM we found that
f(c1, c2) ≜
1
0

ϱ(x)

y′(x)
2 −2y(x)f(x)

dx
=
1
0

3 + cos(πx)

c1(1 −2x) + c2(2x −3x2)
2
−2

c1x(1 −x) + c2x2(1 −x)

0.2x

dx
= c2
1 + c1

−1
30 + c2 + 144 c2
π4
−16 c2
π2

+ c2
50

−1 + 20 c2(180 −20π2 + π4)
π4

(14.13)
and the approximate minc1,c2 f(c1, c2) is achieved at c1 = 0.008216334900156332, c2 =
0.019716988798084503. In Figure 14.3 we show the approximate solution of the ODE-
BVP, namely,
y⋆(x) ≜0.008216334900156332x(1 −x) + 0.019716988798084503x2(1 −x)
0.004
0.003
0.002
0.001
x
y
y(x)
y*(x)
0.0
0.2
0.4
0.6
0.8
1.0
FIGURE 14.3
Example 14.5.

Calculus of Variations
1087
as a dashed graph and also Mathematica’s approximate solution y(x) in the solid, graph.
The minimization in the Rayleigh–Ritz method was done by the command
FindMinimum

c2
1 + c1

−1
30 + c2 + 144 c2
π4
−16 c2
π2

+ c2
50

−1 + 20 c2(180 −20π2 + π4)
π4

, c1, c2

and Mathematica’s approximate solving of the ODE-BVP was done using
s = NDSolve[{y′′[x] ==
1
3 + Cos[πx])

y′[x] ∗π ∗Sin[πx] −x
5

,
y[0] ==0, y[1]==0}, {y}, x]//Quiet
followed by
b=Plot[Evaluate[{y[x]}/.s], {x, 0, 1}, PlotStyle →{Blue, Thick},
LabelStyle →Directive[FontSize →16],
AxesLabel →{x, y}, AxesStyle →Thickness[0.00315]].
14.1.2 Problems
1. Use the Rayleigh–Ritz method to find an approximate solution of
⎧
⎨
⎩
−

3 + cos x

y′′ = 0.1x
y′(0) = y(π) = 0
⎫
⎬
⎭
by using an approximate solution of the form y(x) = c1x2(π −x) + c2 cos x
2.
In problems 2 and 3, use the Rayleigh–Ritz method to find an approximate solution of the
ODE-BVP.
2.
⎧
⎪⎨
⎪⎩
−d
dr

r du
dr

−u = r3
| u(0+) | < ∞, u(1) = 0
⎫
⎪⎬
⎪⎭
3.
⎧
⎪⎨
⎪⎩
−d
dr

r du
dr

−4r2u = r3
| u(0+) | < ∞, u(1) = 0
⎫
⎪⎬
⎪⎭

1088
Advanced Engineering Mathematics
14.2 Necessary Conditions
Example 14.6
Suppose we have a twice continuously differentiable solution function θ0 = θ0(s) for
(14.3) in Section 14.1, that is, J[ θ0 ] is the minimum of
J[ θ ] ≜
L
0

1
2 EI
dθ
ds
2
−P(1 −cos θ)

ds
subject to the requirements that θ ′(0) = θ ′(L) = 0. Explain why θ0(s) must satisfy the
nonlinear ODE
EIθ ′′ + P sin θ = 0,
0 < s < L.
(14.14)
Method: Let ε0 > 0. Consider a family of functions given by
θ(s) ≜θ0(s) + εφ(s),
−ε0 ≤ε ≤ε0,
(14.15)
where φ(s) is continuously differentiable on (0, L) and φ ′(0) = φ ′(L) = 0. The boundary
conditions on φ(s) are imposed so that θ ′(0) = θ ′(L) = 0.
The values of J[ θ ] on that family give a function of the single variable ε:
f(ε) ≜J[ θ0(s) + εφ(s)] =
L
0
1
2 EI

θ ′
0(s) + εφ′(s)
2 −P

1 −cos

θ0(s) + εφ(s)

ds
=
L
0
1
2 EI

θ ′
0(s)
2 + 2εθ ′
0(s)φ′(s) +

εφ′(s)
2
ds −
L
0
P

1 −cos

θ0(s)

ds,
−
L
0
P

sin

θ0(s)

εφ(s)
+ 1
2 cos

θ0(s)

(εφ(s))2 + · · ·

ds,
for small ε, by using the MacLaurin series for the cosine function. So,
f(ε) = f(0) + ε
L
0

EIθ ′
0(s)φ ′(s) −P sin

θ0(s)

φ(s)

ds + · · · .
We assumed that θ0(s) is a global minimizer of J[ θ ], so we must have
df
dε (0) = 0
and
d2f
dε2 (0) ≥0.
The first conclusion, the equality f ′(0) = 0, is called “stationarity.” It implies that
0 = f ′(0) =
L
0

EIθ ′
0(s)φ ′(s) −P sin

θ0(s)

φ(s)

ds
=
L
0
EIθ ′
0(s)φ ′(s) ds −
L
0
P sin

θ0(s)

φ(s) ds.

Calculus of Variations
1089
Using integration by parts and the assumption that θ0 is twice continuously differen-
tiable, this gives
0 = f ′(0) =

EIθ ′
0(s)φ(s)
L
0 −
L
0
EIθ ′′
0 (s)φ(s) ds −
L
0
P sin

θ0(s)

φ(s) ds.
We assumed that the minimizer satisfies the boundary conditions, so the first two terms
are zero. This gives
0 = f ′(0) =
L
0

−EIθ ′′
0 (s) −P sin

θ0(s)

φ(s) ds.
Since this is true for all functions φ(s) that are continuously differentiable on [ 0, L ] and
satisfy φ ′(0) = φ ′(L) = 0, the terms in the parentheses must add up to be identically
zero, that is,
−EIθ ′′
0 (s) −P sin

θ0(s)

≡0,
0 < s < L.
This says that if θ0(s) is a global minimizer then necessarily it must satisfy the nonlinear
ODE (14.14), that is,
EIθ ′′ + P sin θ = 0, 0 < s < L. ⃝
So, analogously to our work in Chapter 10 for partial differential equations, we see that
a model involving integration leads to a differential equation.
14.2.1 Euler–Lagrange Equations
The method used to apply stationarity to the minimization of
J[ θ ] =
L
0

1
2 EI
dθ
ds
2
−P(1 −cos θ)

ds
was based on varying the function θ to be (14.15), that is,
θ(s) ≜θ0(s) + ε φ(s),
where φ(s) is continuously differentiable on (0, L) and φ ′(0) = φ ′(L) = 0. It helps to think
of the function εφ(s) as being a small change of the function θ(s). Thinking of ε as an
arbitrarily small real number, we define the “variation” of θ(s) as being ε φ(s), that is,
δθ ≜ε φ(s).
Even though we suppress the “(s)” from the notation of δθ, we must keep in mind that δθ
is a function that is free to vary, at least within the requirements that δθ(s) be continuously
differentiable on (0, L) and δθ ′(0) = δθ ′(L) = 0.
Similarly, the variation δθ′ is a function that is free to vary, at least within the require-
ments that it be continuous on (0, L) and have
 L
0 δθ′(s) ds = 0. The latter follows from
 L
0 δθ′(s) ds = [δθ(s)]L
0, which follows from a consistency assumption that
(δθ(s))′ = δ(θ ′)(s).
(14.16)

1090
Advanced Engineering Mathematics
The idea of linear approximation was used in calculus of functions of a single variable and
calculus of functions of several variables. For example, in R3, f(r), that is, f(x, y, z), has
f(r0 + △r) ≈f(r0) + ∇f(r0) • (△r) = f(r0) + ∂f
∂x(r0)△x + ∂f
∂y(r0)△y + ∂f
∂z(r0)△z.
As we saw in Chapter 9 when studying Fourier series, varying a function can be done in
infinitely many “directions,” not just the two directions of R2 or the n directions of Rn.
Analogously, we have
J[ θ + δθ ] ≈J[ θ ] + δJ.
Examining the terms in the integral, we have
δ

1
2 EI
dθ
ds
2
=

EIθ′(s)

· δθ ′
and
δ

−P

1 −cos θ(s)

=

−P sin θ(s)

· δθ,
so
δJ =
L
0

(EIθ ′(s)) · δθ ′ −(P sin θ(s)) · δθ

ds.
Stationarity requires 0 = δJ. After use of integration by parts, this becomes
0 = δJ =

(EIθ ′(s)) · δθ
L
0 −
L
0

EIθ ′(s)
′ · δθ −

P sin θ(s)

· δθ

ds
= EIθ ′(L) · δθ(L) −EIθ ′(0) · δθ(0) −
L
0

EIθ ′′(s) + P sin θ(s)

· δθ ds.
Because of the boundary conditions satisfied by θ(s), we get
0 =
L
0

EIθ ′′(s) + P sin θ(s)

· δθ ds.
Because the variation δθ(s) is arbitrary except for the requirements that it be continuously
differentiable on (0, L) and satisfy δθ ′(0) = δθ ′(L) = 0, we conclude that
EIθ ′′(s) + P sin θ(s) ≡0,
0 < s < L.

Calculus of Variations
1091
Note that
δJ = ∂J
∂ε[ θ + εφ ]|ε=0 .
(14.17)
Let’s generalize this “variational” method.
Theorem 14.1
Suppose F = F(x, y, y′) is continuously differentiable and suppose y = y0(x) solves (14.1),
in Section 14.1, that is,
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
b
a
F

x, y(x), y′(x)

dx
Subject to
y(a) = ya
y(b) = yb
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Then necessarily, at

x, y(x), y′(x)

=

x, y0(x), y′
0(x)

, we have the Euler–Lagrange equation
∂F
∂y −d
dx
 ∂F
∂y′

≡0, a < x < b.
(14.18)
Why is (14.18) true? We are allowed to vary δy = δy(x) and δy′ = δy′(x) arbitrarily, except
for requiring that δy be continuously differentiable on (a, b), δy(a) = δy(b) = 0, δy′ be piece-
wise continuous on (a, b), and
 b
a δy′(x) dx = 0. The second assumption comes from the
need for y0(x) + δy(x) to satisfy the boundary conditions y(a) = ya, y(b) = yb, and the last
assumption follows from the second assumption and consistency, that is, (δy)′ = δ(y′).
Analogous to linear approximation in R3, we have
δF(x, y, y′) = ∂F
∂y(x, y, y′) · δy + ∂F
∂y′ (x, y, y′) · δy′.
There is no partial derivative of F with respect to x term because x is not being varied in
the minimization process. So,
δJ =
b
a
δF(x, y, y′)dx =
b
a
∂F
∂y

x, y(x), y′(x)

· δy(x) + ∂F
∂y′

x, y(x), y′(x)

· δy′(x)

dx.

1092
Advanced Engineering Mathematics
Concerning the last term, integration by parts gives
b
a
∂F
∂y′

x, y(x), y′(x)

· δy′(x) dx =
=
 ∂F
∂y′

x, y(x), y′(x)

· δy(x)
b
a
−
b
a
 d
dx
 ∂F
∂y′

x, y(x), y′(x)
 
· δy(x) dx.
But, δy(a) = δy(b) = 0, so
δJ =
b
a
∂F
∂y

x, y(x), y′(x)

· δy(x) dx −
b
a
 d
dx
 ∂F
∂y′

x, y(x), y′(x)
 
· δy(x) dx.
Stationarity at a global minimum gives
0 = δJ =
b
a
∂F
∂y

x, y0(x), y ′
0(x)

−d
dx
 ∂F
∂y′

x, y0(x), y ′
0(x)
 
· δy(x) dx.
Because δy(x) is arbitrary except for the requirements that it be continuously differentiable
and satisfy δy(a) = δy(b) = 0, we conclude that (14.18) holds, that is,
∂F
∂y

x, y0(x), y ′
0(x)

−d
dx
 ∂F
∂y′

x, y0(x), y ′
0(x)
 
≡0,
a < x < b. 2
Example 14.7
Explain why the result of Example 14.6 agrees with the Euler–Lagrange equation of
Theorem 14.1.
Method:
In Example 14.6, F(x, y, y′) =
1
2 EI(y′)2 −P(1 −cos y), where y = θ(s) and
y′ = θ ′(s), s is the independent variable instead of x, and ′ =
d
ds. The Euler–Lagrange
equation is
0 = ∂F
∂y −d
ds
 ∂F
∂y′

= −P sin y −d
ds

EIy′ 
= −P sin θ −EIy ′′ = −P sin θ −EIθ ′′.
This agrees with our conclusion in Example 14.6. ⃝
Theorem 14.1’s result is powerful and enables us to avoid having to “reinvent the
wheel.” In the subject of Advanced Engineering Mathematics this is exactly what we want
to have in our toolbox.
Thus, we also see why much of engineering’s foundations in the physical sciences can
be formulated on the basic principle of minimizing potential energy.

Calculus of Variations
1093
Example 14.8
Assuming D is a nice planar domain, find a PDE satisfied by the solution of the
minimization problem
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize

D
∂u
∂x
2
+
∂u
∂y
2
dA
Subject to
u = g(x, y) on ∂D
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(14.19)
Method: Define
J[ u ] =

D
∂u
∂x
2
+
∂u
∂y
2
dA.
Let u0(x, y) be a solution of (14.19) and δu(x, y) be its variation. Both u0 and u = u0 + δu
must satisfy the Dirichlet boundary condition u ≡g(x, y) on ∂D, so δu ≡0 on ∂D. We
calculate
J[ u0 + δu ] =

D
∂u0
∂x + ∂(δu)
∂x
2
+
∂u0
∂y + ∂(δu)
∂y
2
dA
=

D
∂u0
∂x
2
+ 2∂u0
∂x · ∂(δu)
∂x
+
∂(δu)
∂x
2
+
∂u0
∂y
2
+ 2∂u0
∂y · ∂(δu)
∂y
+
∂(δu)
∂y
2
dA,
so
δJ = 2

D
∂u0
∂x · ∂(δu)
∂x
+ ∂u0
∂y · ∂(δu)
∂y

dA = 2

D
(∇u0) • (∇δu) dA.
Corollary 6.1 in Section 6.7 implies ∇u0 •∇δu = ∇•(δu ∇u0)−δu∇2u0, so the divergence
theorem implies
δJ = 2

∂D

δu ∇u0)

• n ds −2

D
δu ∇2u0 dA = 0 −2

D
δu ∇2u0 dA,
because δu ≡0 on ∂D. Other than the requirement that δu ≡0 on ∂D, arbitrariness of δu
implies stationarity, that is, δJ = 0, which implies
∇2u0 = 0 in D,
that is, u0 satisfies Laplace’s equation in D. ⃝
Theorem 14.1 and Example 14.8 generalize to problems in which the minimizer is a
function of two independent variables:

1094
Advanced Engineering Mathematics
Corollary 14.1
Suppose F = F

x, y, u, ∂u
∂x, ∂u
∂y

is continuously differentiable and u = u0(x, y) solves
⎧
⎪⎪⎨
⎪⎪⎩
Minimize
J[ u ] ≜

D
F

x, y, u, ∂u
∂x, ∂u
∂y

dx dy
Subject to
u = g(x, y) on ∂D
⎫
⎪⎪⎬
⎪⎪⎭
.
Then necessarily we have the Euler–Lagrange equation
∂F
∂u −∂
∂x

∂F
∂(∂u/∂x)

−∂
∂y

∂F
∂(∂u/∂y)
 
at

x,y,u0(x,y)
 ≡0, (x, y) in D.
(14.20)
14.2.2 Natural Boundary Condition
Example 14.9
Explain why the solution of the minimization problem
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
J[ y ] ≜−
L
0

y′(x)
2 + 2y(x)f(x)

dx
Subject to y(0) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(14.21)
must solve the ODE-BVP

y′′(x) = f(x)
y(0) = y′(L) = 0

.
(14.22)
The latter is a model for physical problem of the equilibrium configuration of a loaded
string with free right end. Notice that the ODE-BVP includes both y(0) = 0, the fixed end
boundary condition in the minimization problem, and also y′(L) = 0, the free end
boundary condition. The latter is also called a natural boundary condition.
Method: Notice that our problem is not directly addressed by Theorem 14.1 because the
minimization problem does not assume a fixed end condition at the endpoint x = L.
Suppose y0(x) solve the minimization problem (14.21). Within the admissible class
of piecewise continuously differentiable functions y(x) defined on [ 0, L ] we are free to
vary both
y(x) = y0(x) + δy(x),
0 < x < L,
and
y(L) = y0(L) + δy(L).
The two kinds of variation are independent of each other. But, in order for y0(x) + δy(x)
to continue to satisfy the boundary condition y(0) = 0, we must have
δy(0) = 0.
(14.23)

Calculus of Variations
1095
We have
J[ y0 + δy ] = −
L
0

y′
0(x) + δy′(x)
2 + 2

y0(x) + δy(x)

f(x)

dx
= −
L
0

y′
0
2 + 2y′
0(δy′) +

δy′2 + 2y0f + 2(δy)f

dx
= −
L
0

y′
0
2 + 2y0f

dx −
L
0

2y′
0(δy′) + 2(δy)f

dx −
L
0

δy′2.
So, stationarity of J at y0 requires that
0 = δJ = −
L
0

2y′
0

δy′(x)

+ 2(δy)f

dx.
Integration by parts gives that the first term can be rewritten using
−
L
0

2y′
0 δy′(x) dx = −

2y′
0(x)

δy(x)
 L
0 +
L
0
2y′′
0 δy(x) dx.
Using this and (14.23), that is, δy(0) = 0, we conclude that stationarity of J at y0
requires that
0 = −2y′
0(L)δy(L) + 2
L
0

y′′
0(x) −f(x)

δy(x) dx.
Varying δy(x), 0 < x < L, and δy(L) independently implies that, respectively, both
y′
0(L) = 0
and

y′′
0(x) −f(x)

≡0,
0 < x < L.
Putting together (a) the fixed end BC y0(0) = 0, (b) the “natural boundary condition”
y′
0(L) = 0, and (c) the ODE y′′
0(x) −f(x) ≡0, we conclude that the solution of the calculus
of variations problem, y0, satisfies the ODE-BVP (14.22). ⃝
14.2.3 Hamilton’s Principle
Problems of dynamics can be stated as Hamilton’s equations in terms of convenient state
variables, and Hamilton’s principle implies that the solution is a stationary point of a
functional.
For example, the motion of a mass on the end of a rod free to move in a plane is con-
strained to lie on a circle. This is called a pendulum. We studied the potential energy in
this system in Problem 7.2.5.21, as an application of line integrals.
If a system has total potential energy V and kinetic energy T, the Lagrangian L is
defined by
L ≜T −V.

1096
Advanced Engineering Mathematics
The total action during a time interval [t1, t2] is defined to be
I ≜
t2
t1
L dt.
Hamilton’s principle states that the motion makes the action stationary, that is, has
δI = 0.
Example 14.10
Study Hamilton’s principle for the motion of a mass m on an undamped nonlinear pen-
dulum of length ℓ. As in Figure 7.17, define ϕ to be the angle with respect to vertical,
with ϕ = 0 for the pendulum normally at rest and ϕ = π for the inverted pendulum.
Then the total potential energy is V = mgz = mgℓ(1 −cos ϕ) and the kinetic energy is
T = 1
2 m(ℓ˙ϕ)2.
Method: The action is
I =
t2
t1
1
2 mg(ℓ˙ϕ)2 −mgℓ(1 −cos ϕ)

dt,
and, by integration by parts, its variation is
δI =
t2
t1
mℓ2 ˙ϕ(δ ˙ϕ) dt +
t2
t1
mgℓ(−sin ϕ)(δϕ) dt
=

mℓ2 ˙ϕ(δϕ)
t2
t1
−
t2
t1
mℓ2 ¨ϕ(δϕ) dt +
t2
t1
mgℓ(−sin ϕ)(δϕ) dt
= mℓ2 ˙ϕ(t2)δϕ(t2) −mℓ2 ˙ϕ(t1)δϕ(t1) −
t2
t1
mℓ

ℓ¨ϕ + g sin ϕ)(δϕ) dt .
Among all motions that start and end at fixed endpoints ϕ(t1) = ϕ1, ϕ(t2) = ϕ2, the
variations δϕ satisfy δϕ(t1) = δϕ(t2) = 0, hence
δI = −
t2
t1
mℓ

ℓ¨ϕ + g sin ϕ)(δϕ) dt.
So, stationarity implies

ℓ¨ϕ + g sin ϕ) ≡0,
t1 < t < t2.
This is the familiar equation of motion of an undamped nonlinear pendulum. ⃝

Calculus of Variations
1097
14.2.4 Hamilton’s Principle for Continuous Media
Recall from Section 10.6 the potential and kinetic energies of linear vibrations of a string
satisfying boundary conditions, that is,
PE =
L
0
1
2 T0
∂u
∂x
2
dx
and
KE =
L
0
1
2 ϱ0
∂u
∂t
2
dx,
where
the constant tension is T0
the constant mass density is ϱ0
Again, the Lagrangian is L ≜KE −PE and the action is I ≜
 t2
t1 L dt, that is,
I[ u ] =
t2
t1
L
0
1
2

ϱ0
∂u
∂t
2
−T0
∂u
∂x
2
dx dt.
Hamilton’s principle of least action gives
0 = δI =
t2
t1
L
0

ϱ0
∂u
∂t ·
∂(δu)
∂t

−T0
∂u
∂x ·
∂(δu)
∂x

dx dt.
Integration by parts with respect to t for the first term and integration by parts with respect
to x for the second term gives
0 = δI =
t2
t1
L
0

−∂
∂t

ϱ0
∂u
∂t

+ ∂
∂x

T0
∂u
∂x

· (δu) dx dt.
This being true for all admissible variations δu gives us the one dimensional wave
equation PDE
∂
∂t

ϱ0
∂u
∂t

= ∂
∂x

T0
∂u
∂x

,
that is,
ϱ0
∂2u
∂t2 = T0
∂2u
∂x2 .

1098
Advanced Engineering Mathematics
The results can be generalized to the case when the tension and the mass density are not
constant.
Learn More About It
Calculus of Variations, with Applications to Physics and Engineering, by Robert Weinstock,
Dover Publications, Inc. c⃝1974, is an excellent reference. It includes Problem 14.2.5.8.
14.2.5 Problems
In problems 1–4 find the ODE- or PDE -BVP satisfied by a solution of the calculus of
variations problem.
1. Minimize −
 L
0

p(x)

y′(x)
2 + 2y(x)f(x)

dx
Subject to y(0) = y(L) = 0
2. Minimize
 L
0

−

y′(x)
2 + x2
y(x)
2
dx
Subject to y(0) = y(L) = 0.
[Note: This is an example of a boundary value problem modeling a cantilever
beam clamped at the end x = L, as in Problem 9.7.1.7.]
3. Minimize

D

∂u/∂x
2 +

∂u/∂y
2 −2uf(x, y)

dA
Subject to u ≡g(x, y) on ∂D
4. Minimize

D

∂u/∂x
2 +

∂u/∂y
2 −2uf(x, y)

dA
Subject to
∂u/∂n ≡0 on ∂D
5. Express the solution of
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
3
1

−x

y′2 + xy2
dx
Subject to
y(1) = 0, y(3) = −1
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
in terms of Bessel functions of order zero.
6. Use the Euler–Lagrange ODE necessary condition to explain why there is no
admissible, that is, continuous and piecewise continuously differentiable on
[−1, 1], function to solve
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
1
−1
x2 
y′2 dx
Subject to
y(−1) = −1, y(1) = 1
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.

Calculus of Variations
1099
7. Find the first variation of
J[ y, v ] ≜
b
a
F(x, y(x), y′(x), v(x), v′(x)) dx,
where y, v are continuously differentiable and satisfy BCs y(a) = ya, y(b) = yb,
v(a) = va, v(b) = vb.
8. Study Hamilton’s principle for the motion of a mass m on an undamped spheri-
cal pendulum of length ℓ. Let ϕ be the angle with respect to vertical, with ϕ = 0
for the pendulum normally at rest and ϕ = π for the inverted pendulum, and
define θ to be the usual angle in the xy-plane. Then the total potential energy is
V = mgz = mgℓ(1 −cos ϕ) and the kinetic energy is T = 1
2 m

(ℓ˙ϕ)2 + (ℓ(sin ϕ) ˙θ)2
.
9. You will explain why the minimizer of (14.2) in Section 14.1, that is,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
L
0
1
2 EI(y′′)2 + yf(x)

dx
Subject to
y(0) = y(L) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
must satisfy the fourth order ODE-BVP
(⋆)
⎧
⎨
⎩
EIy′′′′ + f(x) = 0
y(0) = y(L) = 0,
y′′(0) = y′′(L) = 0
⎫
⎬
⎭.
The natural boundary conditions y′′(0) = y′′(L) = 0 will appear as a consequence
of the minimization.
(a) Define J[ y ] ≜
 L
0
1
2 EI(y′′)2 + yf(x)

dx and explain why at a minimizer y0(x)
the variation of J satisfies
0 = δJ =
L
0

EI(y′′
0(x) δy′′(x) + f(x) δy(x)

dx.
(b) In the result of part (a), integrate twice by parts and use the fact that δy(0) =
δy(L) = 0 (Why is that true?) to get
0 =
L
0

EI(y′′′′
0 (x) + f(x)

δy(x) dx + EIy′′
0(L) δy′(L) −EIy′′
0(0) δy′(0).
(c) From the result of part (b), argue that y0 satisfies (⋆) by varying independently
(1) the function δy(x) for 0 < x < L, (2) the number δy′(L), and (3) the number
δy′(0).

1100
Advanced Engineering Mathematics
10. Partially generalize the result of Problem 14.2.5.9 to explain why the minimizer
y0 of
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize
L
0
F

x, y, y′, y′′
dx
Subject to
y(0) = y(L) = 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
at

x, y, y′, y′′
=

x, y0(x), y′
0(x), y′′
0(x)

must satisfy the ODE, for 0 < x < L,
∂F
∂y −d
dx
 ∂F
∂y′

+ d2
dx2
 ∂F
∂y′′

≡0.
11. Theorem 14.1 generalizes to problems in which the minimizer is a vector valued
function y: Suppose F = F(x, y, y′) is continuously differentiable and suppose y =
y0(x) is in Rn and solves (14.1) in Section 14.1, that is,
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
b
a
F

x, y(x), y′(x)

dx
Subject to
y(a) = ya
y(b) = yb.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
Find the necessary conditions satisfied by y0(x).
12. Study Hamilton’s principle for the motion of the three springs and two masses
system in Example 5.5 in Section 5.1. The action is
I = 1
2
t2
t1

m1˙x2
1 + m2˙x2
2 −k1x2
1 −k2(x2 −x1)2 −k3x2
2

dt.
14.3 Problems with Constraints
We may need to find the minimum of a functional J[ y ] over all functions y that satisfy an
integral constraint
K[ y ] ≜
b
a
G

x, y(x), y′(x)

dx = c.
(14.24)
Such a minimization problem is called an isoperimetric problem in honor of an historic
problem: Find the curve of minimum length that encloses a given area. We will mention
that historic problem in Example 14.12.

Calculus of Variations
1101
Theorem 14.2
Suppose F = F(x, y, y′) is continuously differentiable and suppose y = y0(x) solves
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
b
a
F

x, y(x), y′(x)

dx
Subject to
K[ y ] ≜
b
a
G

x, y(x), y′(x)

dx = c
y(a) = ya
y(b) = yb
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
(14.25)
where c is a constant. Then necessarily there is a constant Lagrange multiplier λ such that
δ( J −λK) = ∂(F −λG)
∂y
−d
dx
∂(F −λG)
∂y′

≡0,
a < x < b,
(14.26)
as long as the problem is not degenerate in the sense that at the minimizer
∂G
∂y −d
dx
∂G
∂y′

̸≡0,
a < x < b.
(14.27)
Why? Instead of using a single parameter family of variations as in (14.15) in Section 14.2,
we use a two parameter family of variations
y(ε1, ε2)(x) ≜y0(x) + ε1φ1(x) + ε2φ2(x),
−ε0 ≤ε1,
ε2 ≤ε0
(14.28)
so that we can have y(x) satisfy the integral constraint (14.24) and still have enough free-
dom of variation to be able to imply a necessary condition. Here φ1(x), φ2(x) are linearly
independent functions, in that sense that 0 ≡c1φ1(x) + c2φ2(x) on the interval [ a, b ] for
constants c1, c2 only if c1 = c2 = 0, and also φj(a) = φj(b) = 0 for j = 1, 2 so that y(ε1, ε2)(x)
satisfies the boundary conditions for all ε1, ε2.
Define functions of the two real variables ε1, ε2 by
f(ε1, ε2) ≜J

y(ε1, ε2)

and
g(ε1, ε2) ≜K

y(ε1, ε2)

.
The minimization problem
⎧
⎨
⎩
Minimize
f(ε1, ε2)
Subject to
g(ε1, ε2) = c
⎫
⎬
⎭
(14.29)
is covered by Corollary 13.2 in Section 13.2, where the vector ε ≜−−−−→
(ε1, ε2) is in R2.
Because f(0, 0) = J[ y0 ] and we assumed y0 is a global minimizer for (14.25), we know that

1102
Advanced Engineering Mathematics
(ε1, ε2) = (0, 0) is a global minimizer for (14.29) in R2. So, there exists a constant Lagrange
multiplier λ so that
∇ε( f −λg)

(ε1,ε2)=(0,0) = 0.
(14.30)
In Problem 14.3.2.5 you will explain why (14.30) implies
0 =
b
a
∂(F −λG)
∂y

x, y0(x), y ′
0(x)

−d
dx
∂(F −λG)
∂y′

x, y0(x), y ′
0(x)

φ1(x) dx
(14.31)
and
0 =
b
a
∂(F −λG)
∂y

x, y0(x), y ′
0(x)

−d
dx
∂(F −λG)
∂y′

x, y0(x), y ′
0(x)

φ2(x) dx.
(14.32)
(14.31) and (14.32) redundantly give the same requirement that
∂(F −λG)
∂y

x, y0(x), y ′
0(x)

−d
dx
∂(F −λG)
∂y′

x, y0(x), y ′
0(x)

≡0,
that is, (14.26), as long as (14.27) is true. 2
The technical requirement (14.27) is analogous to the technical requirement ∇g(x⋆) ̸= 0
of Theorem 13.10 in Section 13.2.
By combining the calculation of variation implicit in Corollary 14.1 in Section 14.2 with
the result of Theorem 14.2 we have
Corollary 14.2
Suppose F = F

x, y, u, ∂u
∂x, ∂u
∂y

is continuously differentiable and u = u0(x, y) solves
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
J[ u ] ≜

D
F

x, y, u, ∂u
∂x, ∂u
∂y

dx dy
Subject to
K[ u ] ≜

D
G

x, y, u, ∂u
∂x, ∂u
∂y

dx dy = c
u = g(x, y) on ∂D
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
where c is a constant. Then necessarily u satisfies
∂(F −λG)
∂u
−∂
∂x
∂(F −λG)
∂(∂u/∂x)

−∂
∂y
∂(F −λG)
∂(∂u/∂y)
 
at

x,y,u0(x,y)
 ≡0, (x, y) in D
(14.33)

Calculus of Variations
1103
for some constant λ, as long as
∂G
∂u −∂
∂x

∂G
∂(∂u/∂x)

−∂
∂y

∂G
∂(∂u/∂y)
 
at

x,y,u0(x,y)
 ̸≡0, (x, y) in D.
Theorem 14.3
Suppose F = F(x, y, y′) is continuously differentiable and suppose y = y0(x) solves
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
J[ y, z ] ≜
b
a
F(x, y(x), y′(x), z(x), z′(x)) dx
Subject to
G(x, y, z) = c
y(a) = ya
y(b) = yb
z(a) = za
z(b) = zb
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
where c is a constant. Then necessarily there is a Lagrange multiplier function λ = λ(x)
such that
∂(F −λ(x)G)
∂y
−d
dx
∂(F −λ(x)G)
∂y′

≡0,
a < x < b,
(14.34)
and
∂(F −λ(x)G)
∂z
−d
dx
∂(F −λ(x)G)
∂z′

≡0,
a < x < b,
(14.35)
unless the problem is degenerate in the sense that at the minimizer
∂G
∂y (x, y(x), z(x)) = ∂G
∂z (x, y(x), z(x)) ≡0,
a < x < b.
(14.36)
Why? Please see the explanation in the book by Weinstock mentioned in the “Learn More
About It” below.
The technical requirement (14.36) is analogous to the technical requirement ∇g(x0) ̸= 0
of Theorem 13.10 in Section 13.2.
14.3.1 Differential Equation Constraints
If we are to minimize an integral functional subject to constraints expressed as differential
equations, we can move the constraints into the functional using Lagrange multipliers that
are functions of the independent variable.

1104
Advanced Engineering Mathematics
Example 14.11
The problem to find the optimum path of a single stage rocket to put a satellite into orbit
can be expressed as a calculus of variations problem (Thomson, 1986) of
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Maximize
J[ φ ] ≜u0 +
T
0
F(t)
m
cos φ dt
Subject to
˙w −F(t)
m
sin φ + g = 0
˙y −w = 0
y(0) = y0, φ(0) = θ0
u0 = v0 cos φ0
w0 = v0 sin φ0
w(T) = 0, y(T) = Y
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
,
where φ = φ(t) is the “thrust attitude,” that is, the rocket’s angle of inclination with
respect to horizontal. Also, T is the unspecified time when the rocket’s fuel runs out,
y(t) is the altitude, g is the acceleration due to gravity (assumed to differ negligibly from
a constant), F(t) is the magnitude of the propulsive force of the rocket, m(t) is the rocket’s
mass, and u(t) and w(t) are the rocket’s horizontal and vertical velocities, respectively.
The same techniques that derive Theorem 14.3 say that there are Lagrange multiplier
functions λ1(t) and λ2(t) such that the constraints can be moved into the functional to
reformulate the problem as
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Maximize
J2[φ] ≜u0 +
T
0
F(t)
m
cos φ + λ1(t)( ˙w −F
m sin φ + g) + λ2(t)(˙y −w)

dt
Subject to
y(0) = y0, φ(0) = θ0
u0 = v0 cos φ0
w0 = v0 sin φ0
w(T) = 0, y(T) = Y
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
Thomson derives that the optimum thrust attitude is given implicitly by
tan φ(t) =

1 −t
T

tan θ0.
Example 14.12
Perhaps the first isoperimetric problem in history was to find the “nice,” that is, simple,
closed, piecewise smooth, plane curve of length L = 2π that encloses the greatest area.
Given a nice curve
C : r(t) = x(t) ˆı + y(t) ˆj,
0 ≤t ≤b,
its length is
L[ x, y ] =
b
0

˙x2 + ˙y2 dt

Calculus of Variations
1105
and the area it encloses is
A[ x, y ] =
b
0
xdy
dt dt .
So, the problem is to
⎧
⎨
⎩
Maximize
A[ x, y ]
Subject to
L[ x, y ] = 2π
⎫
⎬
⎭.
It turns out∗that the maximum area is π and it is achieved by a circle of radius one.
Learn More About It
Calculus of Variations, with Applications to Physics and Engineering, by Robert Weinstock,
Dover Publications, Inc. c⃝1974, is an excellent reference. It includes a derivation of
Theorem 14.3 in its Sections 4 and 5, “Restrictions Imposed through Finite or Differ-
ential Equations.” Another good reference is Section IV-7 of Methods of Mathematical
Physics, Volume I, by R. Courant and D. Hilbert, John Wiley & Sons, c⃝1989.
The derivation for the isoperimetric problem of Example 14.12 can be found in
Section III.2 of Elements of Applicable to Functional Analysis, by Charles W. Groetsch,
Marcel Dekker, Inc. c⃝1980.
14.3.2 Problems
In problems 1–4 use Theorem 14.2 or Corollary 14.2 find a BVP satisfied by the minimizer.
1. Minimize
 L
0

u′)2 −q(x)u2
dx
Subject to
 L
0 |u(x)|2dx = 1 and u(0) = u′(L) = 0
2. Minimize
 L
0

u′)2 −q(x)u2
dx
Subject to
 L
0 σ(x)|u(x)|2 dx = 1 and u(0) = u′(L) = 0
3. Minimize

D
∂u
∂x
2
+
∂u
∂y
2
−f(x, y) u2
dA
Subject to

D |u(x, y)|2 dA = 1 and u ≡g(x, y) on ∂D
4. Minimize

D
∂u
∂x
2
+
∂u
∂y
2
−u4
dA
Subject to

D |u(x, y)|2dA = 1 and u ≡0 on ∂D
5. Explain why (14.30) implies (14.31) and (14.32).
∗It is convenient to reparametrize the curve by arclength, expand x(t) and y(t) in Fourier series, and use a
Parseval Theorem.

1106
Advanced Engineering Mathematics
14.4 Eigenvalue Problems
Suppose we have an eigenvalue problem
Ax = λx,
where
x ̸= 0,
(14.37)
where x is in an inner product space (V, ⟨·, ·⟩). As we have seen in Sections 2.9 and 13.6, one
of the forms of the Rayleigh–Ritz method for finding an eigenvalue(s) of a square matrix
A effectively replaces Problem 14.3.2.7 with
⎧
⎨
⎩
Minimize
⟨Ax, x⟩
Subject to
⟨x, x⟩= 1
⎫
⎬
⎭.
(14.38)
14.4.1 An ODE-BVP
Given a finite interval [ a, b ], define a norm on functions y(x) by
||y||2
s ≜
b
a

dsy
dxs

2
dx
and define a normed vector space Vs(a, b) to consist of all s-times piecewise continuously
differentiable functions y(x) on [ a, b ], which satisfy
||y||2
s < ∞
and
the boundary conditions y(a) = y′(b) = 0.
For functions defined on [ a, b ] define also an inner product by
⟨y, z⟩≜
b
a
y(x) z(x) dx,
at least for functions that are square integrable on [ a, b ], that is, that satisfy
∞> ⟨y, y⟩= ||y||2
0 =
b
a
|y(x)|2 dx.
Example 14.13
Consider the eigenvalue problem
⎧
⎨
⎩
y′′(x) + q(x)y = λy(x), a < x < b
y(a) = y′(b) = 0
⎫
⎬
⎭.
(14.39)
Turn (14.39) into a calculus of variations problem.

Calculus of Variations
1107
Method: Define the operator A =
d2
dx2 + q(x) on V2(a, b), which consists of all twice
piecewise continuously differentiable functions y(x) on [ a, b ] which satisfy the boundary
conditions y(a) = y′(b) = 0, by
Ay ≜d2
dx2 [ y ] + q(x)y
and define
J[ y ] ≜⟨Ay, y⟩=
b
a

y′′(x) + q(x)y

y(x) dx =
b
a
y′′(x) y(x) dx +
b
a
q(x)y(x) y(x) dx.
Using integration by parts and the boundary conditions we have
J[y] =

y′(x) y(x)
b
a −
b
a

y′(x)
2 dx +
b
a
q(x)

y(x)
2dx
= 0 + 0 +
b
a

−

y′(x)
2 + q(x)

y(x)
2
dx.
Here, the calculus of variations problem (14.38) is
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J[ y ] ≜
b
a

−

y′(x)
2 + q(x)

y(x)
2
dx
Subject to K[ y ] ≜
b
a

y(x)
2dx = 1
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
,
(14.40)
where y is in V2(a, b), hence satisfies the boundary conditions y(a) = y′(b) = 0.
Define
F

x, y(x), y′(x)

= −

y′(x)
2 + q(x)

y(x)
2
and
G

x, y(x), y′(x)

=

y(x)
2.
Theorem 14.2 in Section 14.3 enables us to conclude that there exists a Lagrange
multiplier λ such that
2q(x)y −2λy + d
dx[ 2y′] = ∂(F −λG)
∂y
−d
dx
∂(F −λG)
∂y′

≡0,
a < x < b,
as long as at the minimizer we have
2y(x) −(0) = ∂G
∂y −d
dx
 ∂G
∂y′

̸≡0,
a < x < b.
(14.41)
Because y(x) satisfies
 b
a

y(x)
2dx = 1, it is impossible to have 2y(x) ≡0 on a < x < b. So,
condition (14.41) is automatically true at a solution of the calculus of variations problem.
So, the ODE-BVP eigenvalue problem (14.39), that is, y′′(x)+q(x)y−λy(x) = 0, a < x < b,
y(a) = y′(b) = 0, does indeed correspond to the calculus of variations problem (14.40). ⃝

1108
Advanced Engineering Mathematics
Because y(x) satisfies Ay = y′′(x) + q(x)y = λy and y ̸≡0 for a < x < b, we can refer
to λ as an eigenvalue of the differential operator A on the space V(a, b). Note that the two
boundary conditions, y(a) = y′(b) = 0, are part of the definition of the space V2(a, b).
14.4.2 An Eigenvalue Problem for the Laplacian
Similarly, given a nice spatial region D in R2, define a norm on functions φ(x, y) by
||φ||2
s ≜

D
∂sφ
∂xs
2
+
∂sφ
∂ys
2
dx dy.
Define a normed vector space W to consist of s-times continuously differentiable functions
φ(x, y) on D for which
||φ||2
s < ∞
and which satisfy the boundary condition
0 ≡φ

(x,y) on boundary of D.
Unfortunately, this space is not a complete normed vector space, as defined in Section 2.10.
But there is a concept called “completion of a metric space” that considers “functions” ψ
that are limits, in the norm || · ||s, of sequences of functions in W. This completion is called
a Sobolev (Lusternik and Sobolev 1974) space and is denoted by Hs
0(D). The superscripts
refers to the squares of the s order derivatives in the norm and the subscript 0 refers to
the zero Dirichlet boundary condition. For functions defined on D, define also an inner
product by
⟨φ, ψ⟩≜

D
φ(x, y) ψ(x, y) dx dy,
at least for functions that are square integrable on D, that is, that satisfy
∞> ⟨φ, φ⟩= ||φ||2
0 =

D
|φ(x, y)|2 dx dy.
Example 14.14
We have seen in Example 14.3 in Section 14.1 that vibration of a membrane over a planar
region D can lead to the eigenvalue problem
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2φ
∂x2 + ∂2φ
∂y2 + λφ, (x, y) in D
0 ≡φ

(x,y) on boundary of D
⎫
⎪⎪⎬
⎪⎪⎭
,
(14.42)

Calculus of Variations
1109
assuming the boundary of the membrane has displacement u = 0. Look for a solution in
the Sobolev space H2
0(D) by turning (14.42) into a calculus of variations problem.
Method: Define the operator A = − on H2
0(D) by
Aφ ≜−

∂2φ
∂x2 + ∂2φ
∂y2

= −∇• ∇φ.
Consider the corresponding calculus of variations problem (14.38). We have
⟨Aφ, φ⟩= −

D

∇• ∇φ(x, y)

φ(x, y) dx dy.
Use the result of Corollary 6.1 in Section 6.7 and the Divergence Theorem to
conclude that
⟨Aφ, φ⟩= −

D ∇•

φ ∇φ

dx dy +

D

∇φ

•

∇φ

dx dy
= −

∂D
φ ·

n • ∇φ

ds +

D

∇φ

•

∇φ

dx dy
= 0 +

D
∂φ
∂x
2
+
∂φ
∂y
2 
dx dy,
after using the boundary condition 0 ≡φ

(x,y) on boundary of D. So, the calculus of varia-
tions problem is
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize

D
∂φ
∂x
2
+
∂φ
∂y
2
dx dy
Subject to

D
|φ(x, y)|2 dx dy = 1
and
φ ≡0 on ∂D
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
(14.43)
14.4.3 Sturm–Liouville Problem
Assume s(x) > 0 on the interval (a, b). Assume ϵ0, ϵ1, γ0, γ1 are given constants (scalars). We
will assume always that either ϵ0 ̸= 0 or ϵ1 ̸= 0 and either γ0 ̸= 0 or γ1 ̸= 0. Below we will
look for minimizers X(x) that are continuous and whose derivative is piecewise continuous
on the interval (a, b).
Example 14.15
The minimizer of
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize
J[ X ] ≜−

p(x)X(x)X′(x)
b
a +
b
a

p(x)

X′(x)
2 −q(x) (X(x))2
dx
Subject to
K[ X ] ≜
b
a
s(x)(X(x))2 dx = 1
ϵ0X(a) −ϵ1X′(a) = 0
γ0X(b) + γ1X′(b) = 0
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭

1110
Advanced Engineering Mathematics
solves the Sturm–Liouville eigenvalue problem consisting of ODE (9.63) in Section 9.6,
that is,

p(x)X′(x)
′ + (λs(x) + q(x))X(x) = 0, a < x < b,
and boundary conditions (9.64) in Section 9.6, that is,
⎧
⎨
⎩
ϵ0X(a) −ϵ1X′(a) = 0
γ0X(b) + γ1X′(b) = 0
⎫
⎬
⎭.
An explanation for this is in Section 8.1 of the book by Weinstock mentioned in Section
14.3’s “Learn More About It”.
Note that the technical requirement (14.27) in Section 14.3 is automatically verified
because
∂G
∂X −d
dx
 ∂G
∂X′

= 2s(x)X(x) ̸≡0
because s(x) > 0 on the interval (a, b), and a minimizer X(x) is continuous on (a, b) and
satisfies
 b
a s(x)(X(x))2 dx = 1.
Learn More About It
Good expositions of Sobolev spaces are in Section 2.5 of Elements of Functional Analysis,
by L. A. Lusternik and V. J. Sobolev, Hindustan Publishing Co., Halsted Press/John
Wiley & Sons, Inc., c⃝1974.
14.4.4 Problems
In problems 1–4, find a calculus of variations problem whose solution is an eigenfunction
for the given eigenvalue problem.
1.
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 +λu, (x, y) in D
0 ≡∂u
∂n on ∂D
⎫
⎪⎪⎬
⎪⎪⎭
2.
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 +f(x, y)u +λu, (x, y) in D
0 ≡u on ∂D
⎫
⎪⎪⎬
⎪⎪⎭
3.
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 + f(x, y)u + λσ(x, y)u, (x, y) in D
0 ≡u on ∂D
⎫
⎪⎪⎬
⎪⎪⎭

Calculus of Variations
1111
4.
⎧
⎪⎪⎨
⎪⎪⎩
0 = ∂2u
∂x2 + 2∂2u
∂y2 + f(x, y)u + λu, (x, y) in D
0 ≡u on ∂D
⎫
⎪⎪⎬
⎪⎪⎭
In problems 5 and 6, use the result of Example 14.15 to find an ODE-BVP eigenvalue
problem satisfied by the minimizer.
5. Minimize
 L
0

u′)2 −q(x)u2
dx
Subject to
 L
0 u2 dx = 1 and u(0) = u(L) = 0
6. Minimize
 L
0

u′)2 −q(x)u2
dx
Subject to
 L
0 σ(x)u2dx = 1 and u(0) = u′(L) = 0
14.5 Short Take: Finite Element Methods
If we combine variational methods with the approximation of a solution using a basis of
splines, which we learned about in Section 8.9, then we have “Finite Element Methods
(FEM).”
Abstractly, the Rayleigh–Ritz method for a boundary value problem uses the variational
formulation
⎧
⎨
⎩
Minimize
J[ u ] ≜⟨u, Au⟩−2⟨u, f⟩
Subject to
u = M
n=1 cnφn
⎫
⎬
⎭,
(14.44)
where the “trial functions” φn are chosen appropriately. For example, if the operator A
has boundary conditions built into its definition then the trial functions should satisfy the
boundary conditions.
For example, if A = −d2
dx2 + q(x) is defined to include the boundary conditions y(0) =
y(L) = 0 then we want the trial functions to satisfy φn(0) = φn(L) = 0, for n = 1, . . . , M.
Recall from Section 8.8 that the uniform tent basis functions were defined in (8.86) in
Section 8.9, that is,
Tj(x) ≜
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
 x−xj
h
+ 1

,
xj−1 ≤x ≤xj

1 −
x−xj
h

,
xj ≤x ≤xj+1
0,
|x −xj| ≥h
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
= φ
x −xj
h

,
for j = 0, . . . , N + 1. If h = L
N and xj = jh for j =−1, . . . , N + 1, then for n = 1, . . . , N −1 ≜M
the uniform tent basis spline functions T1(x), . . . , TN−1(x) will satisfy the boundary
conditions y(0) = y(L) = 0.

1112
Advanced Engineering Mathematics
Example 14.16
Use the uniform tent basis functions and the Rayleigh–Ritz method to approximate the
solution of the ODE-BVP
⎧
⎨
⎩
−y′′(x) + xy = 1 + x, 0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭.
(14.45)
Method: Define the linear differential operator A by
Ay ≜

−d2
dx2 + x

[ y ] = −y′′ + xy
on functions that satisfy the boundary conditions y(0) = y(1) = 0. Using integration by
parts and the boundary conditions we calculate
⟨Ay, y⟩=
1
0

−y′′ + xy

y dx =

−y′(x)y(x)
1
0 +
1
0
(y′)y′ dx +
1
0
(xy)y dx
=
1
0
(y′)y′ dx +
1
0
(xy)y dx.
So, the Rayleigh–Ritz method using the uniform tent basis functions is the equivalent
optimization problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
1
0

(y′)2 + xy2 −2(1 + x)y

dx
Subject to
y(x) = M
n=1 cnTn(x)
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
where
M = N −1
h = 1
N
We used Mathematica (see the commands below) to solve for the constants c1, . . . , c7 in
the special case N = 8, that is, grid size h = 0.125, and got the approximate dashed solu-
tion graph in Figure 14.4. Figure 14.4 also displays Mathematica’s approximate solution
graph in a solid curve.
0.20
0.15
0.10
0.05
0.00
x
y(x)
0.0
0.2
0.4
0.6
0.8
1.0
FIGURE 14.4
Example 14.16.

Calculus of Variations
1113
14.5.1 Mathematica Commands
After defining h = 0.125, the basic tent function T1(x) in Example 14.16 was defined by
T1[t_]:=Piecewise[{{ t
h, 0< t ≤h}, {1 −t −h
h
, h< t ≤2h}, {0, −1< t ≤0}, {0, 2h< t ≤1}}].
Noting that T2(x) = T1(x −h), T3(x) = T1(x −2h), etc., we defined
y[x_, c1_, c2_, c3_, c4_, c5_, c6_, c7_] := c1 ∗T1[x] + c2 ∗T1[x −h] + c3 ∗T1[x −2h]
+c4 ∗T1[x −3h] + c5 ∗T1[x −4h] + c6 ∗T1[x −5h] + c7 ∗T1[x −6h]
and the functional by
J[c1_, c2_, c3_, c4_, c5_, c6_, c7_]
:=Evaluate
 1
0

(D[y[x, c1, c2, c3, c4, c5, c6, c7], x])2 + x (y[x, c1, c2, c3, c4, c5, c6, c7])2
−2 ∗(1 + x) ∗y[x, c1, c2, c3, c4, c5, c6, c7]

dx

.
Then we used
FindMinimum[J[c1, c2, c3, c4, c5, c6, c7], {{c1, .1}, {c2, 0.1}, {c3, 0.1}, {c4, .1}, {c5, 0.1},
{c6, 0.1}, {c7, 0.1}}].
to get output
{−0.176893, {c1 →0.0722715, c2 →0.127121, c3 →0.162938, c4 →0.178214,
c5 →0.171418, c6 →0.140859, c7 →0.0845438}}.
We substituted those values into y to get a function
y[x, 0.07227152517438032‘, 0.1271210999916466‘, 0.16293835769807058‘,
0.17821421372289542‘, 0.17141750813321335‘, 0.14085942298213375‘,
0.08454383690012605‘]
whose graph we plotted as the dashed curve in Figure 14.4.
To get and plot Mathematica’s approximate solution, we used the command
s = NDSolve[{x′′[t] == tx[t] −1 −t, x[0] == 0,x[1] == 0}, {x}, {t, 0, 1}]

1114
Advanced Engineering Mathematics
followed by
bb = Plot[Evaluate[x[t]/.s], {t, 0, 1}, PlotStyle →{Blue, Thick},
AxesLabel →{Text[Style[′′x′′, Italic, 18]], Text[Style[′′y(x)′′, Italic, 18]]}]
to get the solid curve in Figure 14.4.
Example 14.17
Use cubic spline uniform basis functions and the Rayleigh–Ritz method to approximate
the solution of the ODE-BVP
⎧
⎨
⎩
−y′′(x) + x2y = x, 0 < x < 1
y(0) = y′(1) = 0
⎫
⎬
⎭.
(14.46)
Method: Define the linear differential operator A by
Ay ≜

−d2
dx2 + x

[ y ] = −y′′ + x2y
on functions that satisfy the boundary conditions y(0) = y′(1) = 0. Using integration by
parts and the boundary conditions we calculate
⟨Ay, y⟩=
1
0

−y′′ + xy

y dx =

−y′(x)y(x)
1
0 +
1
0
(y′)y′ dx +
1
0
(x2y)y dx
=
1
0
(y′)y′ dx +
1
0
(x2y)y dx.
The instructions said to use cubic spline uniform basis functions because having
a boundary condition involving the derivative of the solution, for example, at x = 1,
contraindicates use of tent spline uniform basis functions, which lack smoothness.
Let h =
1
N be the grid size, and define xj = jh, for j = −1, 0, . . . , N + 1. Recall that
the cubic spline uniform basis functions Cj(x) were defined in (8.90) in Section 8.9, for
j = −1, 0, . . . , N + 1. Using the data in (8.93) in Section 8.9, specifically
Cj(xj−2) = 0,
Cj(xj−1) = 1
6,
Cj(xj) = 2
3, Cj(xj+1) = 1
6,
Cj(xj+2) = 0,
Cj′(xj−2) = 0,
Cj′(xj−1) = 1
2h,
Cj′(xj) = 0, Cj′(xj+1) = −1
2h,
Cj′(xj+2) = 0,
we see that an approximate solution y(x) ≜N+1
j=−1 cjCj(x) will satisfy the boundary con-
dition 0 = y(0) if, and only if, 1
6c−1+ 2
3c0+ 1
6c1 = 0, that is, c−1+4c0+c1 = 0. Also, y(x) will
satisfy the boundary condition 0 = y′(1) = y′(xN) if, and only if, −1
2hcN−1 + 1
2hcN+1 = 0,
that is, −cN−1 + cN+1 = 0.

Calculus of Variations
1115
0.30
0.25
0.20
0.15
0.10
0.05
0.00
x
y(x)
0.0
0.2
0.4
0.6
0.8
1.0
FIGURE 14.5
Example 14.17.
So, the Rayleigh–Ritz method using the cubic spline uniform basis functions is the
equivalent optimization problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize
J[ y ] ≜
1
0

(y′)2 + x2y2 −2xy

dx
Subject to
y(x) =
N+1

j=−1
cjCj(x), c−1 + 4c0 + c1 = 0,
and
−cN−1 + cN+1 = 0
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
Note that it is not true that each of the individual test functions Cj(x) satisfies the bound-
ary conditions y(0) = y′(1) = 0. But, all test functions N+1
j=−1 cjCj(x) do satisfy the
boundary conditions as long as the coefficients c−1, c0, . . . , cN+1 satisfy the constraints
c−1 + 4c0 + c1 = 0 and −cN−1 + cN+1 = 0.
We used Mathematica (see the commands below) to solve for the constants
c−1, c0, . . . , c3 in the special case N = 3, that is, grid size h = 1
3, and got the approx-
imate dashed solution graph in Figure 14.5. Figure 14.5 also displays Mathematica’s
approximate solution graph in a solid curve. ⃝
To create the cubic spline B3[x] ≜ψ(x) defined in (8.88) in Section 8.9, we used
Mathematica commands
BB[x ] := Piecewise
3x3 −6x2 + 4
6
, 0 < x ≤1

,
(2 −x)3
6
, 1 < x < 2}

B3[x ] := Piecewise[{{BB[x], 0 < x < 2}, {BB[−x], −2 < x < 0}, {0, x ≤−2}, {0, x ≥2}}]
After that we used Mathematica commands
h = 1
3; CM1[x ] := B3
x + h
h

; C0[x ] := B3
x
h

; C1[x ] := B3
x −h
h

;
C2[x ] := B3
x −2h
h

; C3[x ] := B3
x −3h
h

; C4[x ] := B3
x −4h
h

;

1116
Advanced Engineering Mathematics
y[x , cm1 , c0 , c1 , c2 , c3 , c4 ] :=
cm1 CM1[x] + c0 C0[x] + c1 C1[x] + c2 C2[x] + c3 C3[x] + c4 C4[x]
f[cm1 , c0 , c1 , c2 , c3 , c4 ] := Evaluate
 1
0

(D[y[x, cm1, c0, c1, c2, c3, c4], x])2
+ x2 ∗(y[x, cm1, c0, c1, c2, c3, c4])2 −2x ∗y[x, cm1, c0, c1, c2, c3, c4]

dx

FindMinimum[{f[cm1, c0, c1, c2, c3, c4], {−c2 + c4 == 0, cm1 + 4c0 + c1 == 0}},
{{cm1, .1}, {c0, .1}, {c1, .1}, {c2, 0.1}, {c3, .1}, {c4, .1}}]
14.5.2 Rayleigh–Ritz, Galerkin, and Least Squares
Suppose A is a linear differential operator on a vector space that includes in its definition
boundary conditions. A could be an ordinary differential operator or a partial differential
operator, such as the Laplacian.
The Galerkin method is, abstractly, as in (12.47) in Section 12.5, to find c1, . . . , cM that
solves
M
 
j=1
⟨φi, Aφj⟩cj = ⟨f, φi⟩, i = 1, . . . , M,
(14.47)
which can be rewritten as (12.48) in Section 12.5, that is,
Ac = f,
(14.48)
where c = [c1
...
cM]T and
f =

⟨φ1, f⟩
...
⟨φM, f⟩
T
(14.49)
are vectors in RM, and A is the M × M matrix defined by (12.49) in Section 12.5, that is,
A ≜

⟨φi, Aφj⟩

1 ≤i ≤M
1 ≤j ≤M
.
(14.50)
Definition 14.1
A linear operator A on an inner product space is symmetric if
⟨Au, v⟩= ⟨u, Av⟩
for all u, v.
The definition of “symmetric” is similar to Definition 2.36 in Section 12.10 of
“self-adjoint” for a bounded linear operator on a Hilbert space.

Calculus of Variations
1117
For example, the differential operator in Example 14.16 is symmetric, because for all
functions u and v that satisfy the boundary conditions, that is, satisfy u(0) = u(1) = 0,
v(0) = v(1) = 0, and are sufficiently differentiable, integration by parts twice gives
⟨Au, v⟩=
1
0
(−u′′(x) + xu)v dx =

−u′(x)v(x)
1
0 +
1
0

u′(x)v′(x) + xu(x)v(x)

dx
= −0 + 0 +

u(x)v′(x)
1
0 +
1
0

−u(x)v′′(x) + xu(x)v(x)

dx = 0 −0 + ⟨u, Av⟩.
Theorem 14.4
If A is linear and symmetric, then the Galerkin method and the Rayleigh–Ritz method
given in (14.44) give the same approximate solution of Au = f.
Why? It will suffice to show that u = M
n=1 cnφn minimizes J[ u ] ≜⟨u, Au⟩−2⟨u, f⟩if, and
only if, c = [c1
...
cM]T satisfies (14.48).
We calculate
q(c) ≜J
 M
 
n=1
cnφn

=
! M
 
i=1
ciφi, A
M
 
j=1
cjφj
"
−2
! M
 
i=1
ciφi, f
"
=
M
 
i=1
M
 
j=1
cicj⟨φi, Aφj⟩−2
M
 
i=1
ci⟨φi, f⟩= cTAc −2cTf,
where A and f are as in (14.50) and (14.49), respectively.
Because we assumed A is symmetric, the matrix A is symmetric, that is, AT = A.
Using symmetry of A, (Fermat’s) Theorem 13.6 in Section 13.2, that is, stationarity, states
that a minimizer c for j(c) must satisfy
0 = 1
2 0 = 1
2
∂q
∂c = Ac −f,
which is (14.48) from Galerkin’s method. So, the minimizer u = M
n=1 cnφn for the
Rayleigh–Ritz method is the same as the solution given by Galerkin’s method. 2
The Rayleigh–Ritz method was given in (14.44) and used the functional
J[ u ] ≜⟨u, Au⟩−2⟨u, f⟩.
The least squares method is to, instead, find constants c1, . . . , cM to
⎧
⎨
⎩
Minimize
J2[ u ] ≜||Au −f||2 = ⟨Au −f, Au −f⟩
Subject to
u = M
n=1 cnφn
⎫
⎬
⎭.
(14.51)

1118
Advanced Engineering Mathematics
In Problem 14.5.4.6 you will explain why the least squares method gives the same solution
as a generalization of Galerkin’s method, assuming A is a symmetric linear operator.
14.5.3 Finite Elements for PDEs
For PDEs we can use surface splines and triangular surface patches, which were men-
tioned in Section 8.9, analogously to the way we used tent functions to approximate the
solution of an ODE earlier in this section. There are a lot of sophisticated computational
science problems associated with decomposing a planar or spatial region into triangles
or tetrahedra, or into rectangles or boxes. But after that hard work, the idea of using the
Rayleigh–Ritz or Galerkin method is abstractly the same as for ODEs.
Like many other chapters or even sections of this book, the topic of finite elements for
PDEs deserves a shelf of books to discuss either the technicalities of practical implementa-
tion or the mathematical theory that justifies the hope of convergence to a solution.
Learn More About It
Among the many good references for finite element methods for PDEs are Finite
Elements for Electrical Engineers, 3rd ed., by Peter P. Silvester and Ronald l. Ferrari,
Cambridge University Press, 1996, Introduction to Finite Elements in Engineering, 3rd
ed., by Tirupathi R. Chandrupatla and Ashok D. Belegundu, Prentice Hall, Inc., 2002,
and An Analysis of the Finite Element Method, 2nd ed., by Gilbert R. Strang and George
J. Fix, Wellesley-Cambridge, 2008.
14.5.4 Problems
In problems 1 and 2, use the uniform tent basis functions and the Rayleigh–Ritz method to
approximate the solution of the ODE-BVP, using the given value of N.
1.
⎧
⎨
⎩
−y′′(x) + x2y = x, 0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭, N = 4
2.
⎧
⎨
⎩
−y′′(x) + y cos(πx) = x, 0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭, N = 5
In problems 3 and 4, use the cubic spline uniform basis functions and the Rayleigh–
Ritz method to approximate the solution of the ODE-BVP, using the given value
of N.
3.
⎧
⎨
⎩
−y′′(x) + xy = 1 + x, 0 < x < 1
y(0) = y′(1) = 0
⎫
⎬
⎭, N = 2
4.
⎧
⎨
⎩
−y′′(x) + xy = 1 + x, 0 < x < 1
y′(0) = y(1) = 0
⎫
⎬
⎭, N = 2

Calculus of Variations
1119
5. For the nonlinear ODE-BVP
⎧
⎨
⎩
−y′′(x) + y −y3 = x, 0 < x < 1
y(0) = y(1) = 0
⎫
⎬
⎭,
(a) explain why its solution is a minimizer for the problem
⎧
⎨
⎩
Minimize
J[ y ]
Subject to
y(0) = y(1) = 0
⎫
⎬
⎭,
where
(⋆⋆)[y] ≜
1
0
1
2 (y′)2 + 1
2 y2 −1
4 y4 −xy

dx.
(b) Use the uniform tent basis functions and the Rayleigh–Ritz method to
approximate the solution of the ODE-BVP, using N = 4.
6. You will explain why the least squares method gives the same solution as a
generalization of Galerkin’s method, assuming A is a symmetric linear operator.
(a) Use ||Au−f||2 = ⟨Au−f, Au−f⟩, u = M
n=1 cnφn, and symmetry of A to explain
why
||Au −f||2 =
M
 
i=1
M
 
j=1
cicj⟨Aφi, Aφj⟩−2
M
 
i=1
ci⟨Aφi, f⟩+ || f||2.
(b) Define j ≜Aφj, A ≜

⟨i, Aφj⟩

1 ≤i ≤M
1 ≤j ≤M
, and f ≜

⟨1, f⟩· · · ⟨M, f⟩
T.
Explain why the vector c =

c1 · · · cM
T minimizes ||Au −f||2 if, and only
if, c solves the system of equations
Ac =f,
which is a generalization of Galerkin’s method found in (12.53) in Section 12.5.
Key Terms
action: before Example 14.10 in Section 14.2
admissible function: after (14.1) in Section 14.1
brachistochrone problem: Example 14.4 in Section 14.1
calculus of variations: before (14.1) in Section 14.2
Euler–Lagrange equation: (14.18) in Section 14.2
fixed end boundary condition: after (14.23) in Section 14.2
free end boundary condition: after (14.23) in Section 14.2
functional: after (14.1) in Section 14.1
Hamilton’s principle: before Example 14.10 in Section 14.2
integral constraint: (14.24) in Section 14.3
isoperimetric problem: after (14.24) in Section 14.3

1120
Advanced Engineering Mathematics
least squares method: (14.46) in Section 14.5
natural boundary condition: after (14.2) in Section 14.1, after (14.23) in Section 14.2
pendulum: before Example 14.10 in Section 14.2
Rayleigh–Ritz method: after (14.10) in Section 14.1
Sobolev space: before (14.42) in Section 14.4
spherical pendulum: Problem 14.2.5.8
symmetric: Definition 14.1 in Section 14.5
variation: before (14.16) in Section 14.2
Mathematica Commands
After Example 14.5 in Section 14.1:
s = NDSolve[{y′′[x] ==
1
3 + Cos[πx])

y′[x] ∗π ∗Sin[πx] −x
5

,
y[0] == 0, y[1] == 0}, {y}, x]//Quiet
b = Plot[Evaluate[{y[x]}/.s], {x, 0, 1}, PlotStyle →{Blue, Thick},
LabelStyle →Directive[FontSize →16], AxesLabel →{x, y},
AxesStyle →Thickness[0.00315]].
In Section 14.5:
T1[t_]:=Piecewise[{{ t
h, 0< t ≤h}, {1 −t −h
h
, h< t ≤2h}, {0, −1< t ≤0}, {0, 2h< t ≤1}}]
y[x_, c1_, c2_, c3_, c4_, c5_, c6_, c7_] := c1 ∗T1[x] + c2 ∗T1[x −h] + c3 ∗T1[x −2h]
+c4 ∗T1[x −3h] + c5 ∗T1[x −4h] + c6 ∗T1[x −5h] + c7 ∗T1[x −6h]
J[c1_, c2_, c3_, c4_, c5_, c6_, c7_]
:=Evaluate
 1
0

(D[y[x, c1, c2, c3, c4, c5, c6, c7], x])2 + x (y[x, c1, c2, c3, c4, c5, c6, c7])2
−2 ∗(1 + x) ∗y[x, c1, c2, c3, c4, c5, c6, c7]

dx

FindMinimum[J[c1, c2, c3, c4, c5, c6, c7], {{c1, .1}, {c2, 0.1}, {c3, 0.1}, {c4, .1}, {c5, 0.1},
{c6, 0.1}, {c7, 0.1}}]
s = NDSolve[{x′′[t] == tx[t] −1 −t, x[0] == 0, x[1] == 0}, {x}, {t, 0, 1}]
bb=Plot[Evaluate[x[t]/.s], {t, 0, 1}, PlotStyle→{Blue, Thick},
AxesLabel →{Text[Style[′′x′′, Italic, 18]], Text[Style[′′y(x)′′, Italic, 18]]}]
Further Mathematica commands are found after Example 14.17 in Section 14.5.

Calculus of Variations
1121
References
Lusternik, L.A. and Sobolev, V.J. Elements of Functional Analysis, Section 2.5. John Wiley & Sons, Inc.,
New York, 1974.
Thomson, W.T. Introduction to Space Dynamics, Section 8.3. Dover Publications, Inc., Mineola, NY,
1986.


15
Functions of a Complex Variable
15.1 Complex Numbers, Roots, and Functions
We begin by briefly reviewing complex numbers and their properties.
In Section 2.1, we defined a number z to be complex if z = x + iy, where x and y are
real and i ≜
√
−1. Given a complex number z = x + iy, where x and y are real, we call
x the real part of z and write x = Re(z), and we call y the imaginary part of z and write
y = Im(z). Both Re(z) and Im(z) are real numbers. If Re(z) = 0, we say z is imaginary; if
Im(z) = 0, we say z is real. From now on, if we write a > 0 or say that “a is positive,” then
we are implicitly assuming that a is real.
Given a complex number z = x + iy, where x and y are real, we denote z = x −iy and
call it the complex conjugate of z. We have that
Re(z) = z + z
2
and
Im(z) = z −z
2i .
Note that
zz = (x + iy)(x −iy) = x2 −(iy)2 = x2 + y2,
which is the square of the distance in the xy-plane between the points (x, y) and (0, 0).
Figure 15.1 shows a complex number z, its complex conjugate z, and the quantity
|z| ≜

x2 + y2,
which is called the modulus or absolute value of x+iy, assuming x and y are real. Note that
if x is real, then the modulus of x is just the absolute value of x. Also, note that
 |z|
 = |z|,
that is, the modulus of the modulus of z is just the modulus of z.
We have the important geometric property that |z−w| is the distance between the points
z and w in the complex plane.
15.1.1 Polar Forms
To solve difference equations in Section 4.6, we used the polar form of a complex number:
x + iy = r cos θ + ir sin θ.
(15.1)
1123

1124
Advanced Engineering Mathematics
Im(z)=y
|z|
z=x + iy
x= Re(z)
z=x – iy
FIGURE 15.1
Complex numbers.
Im(z)
Re(z)
r
θ
FIGURE 15.2
Complex numbers and the polar form.
This is just another example of using polar coordinates, as shown in Figure 15.2. Note that
r =

x2 + y2 = |z|
and
tan θ = y
x,
if
x ̸= 0.
(15.2)
To solve harmonic oscillator problems in Section 3.3, we used
Euler′s formula:
eiθ ≜cos θ + i sin θ .
(15.3)
Combining (15.1) through (15.3), we have
z = x + iy = r(cos θ + i sin θ) = reiθ = |z| eiθ,
(15.4)
where
• r = |z| ≥0
• θ is real
• −π < θ ≤π

Functions of a Complex Variable
1125
The polar exponential form of z is |z| eiθ, where θ is real and −π < θ ≤π. Familiar laws of
exponents also hold for positive integer powers of z = reiθ:
zn =

reiθn
= rn 
eiθn
= rneinθ = rn (cos nθ + i sin nθ)
= |z|n (cos nθ + i sin nθ) .
(15.5)
Note that nθ may not be in the interval (−π, π], so the polar exponential form of zn may
not be |z|neinθ.
In the special case that r = 1, we have
DeMoivre′s theorem:
(cos θ + i sin θ)n = cos nθ + i sin nθ.
For all integers k,
ei2πk = 1,
eikπ = (−1)k,
and
eiθ+i2πk = eiθ.
Further,
Theorem 15.1
For real numbers α and β,
eiα = eiβ
⇐⇒
α + 2πk = β
for some integer k.
(15.6)
Also,
e−iθ = cos(−θ) + i sin(−θ) = cos θ −i sin θ.
Example 15.1
Find the exact polar exponential and polar forms of the numbers
(a) −
√
3 + i,
(b) (−
√
3 + i)5,
(c) (−
√
3 + i)(1 −i),
(d) −
√
3+i
1−i .
Method:
(a) Figure 15.3a shows the number −
√
3 + i in the complex plane, so we see that r =

(−3)2 + (1)2 = 2 and the angle θ = 5π
6 . So, in the polar form −
√
3 + i = 2 cos 5π
6 +
i2 sin 5π
6 and in the polar exponential form −
√
3 + i = 2ei5π/6.
(b) Using the result of part (a), we have
(−
√
3 + i)5 =

2ei5π/65
= 25ei25π/6 = 32ei25π/6 = 32eiπ/6
as the polar exponential form. It follows that its polar form is
(−
√
3 + i)5 = 32 cos π
6 + i32 sin π
6 .

1126
Advanced Engineering Mathematics
Im(z)
Re(z)
Im(z)
Re(z)
1
1
θ
–1
1–i
r
–√3
–√3+i
(a)
(b)
FIGURE 15.3
Example 15.1: (a) −
√
3 + i and (b) 1 −i.
(c) Figure 15.3b shows the number 1 −i in the complex plane, so we see that r =

(1)2 + (−1)2 =
√
2 and the angle θ = −π
4 . So, 1 −i =
√
2 e−iπ/4. Using this and a
result from part (a), we have
(−
√
3 + i)(1 −i) =

2ei5π/6 √
2e−iπ/4
= 2
√
2 ei

5π
6 −π
4

= 2
√
2 ei7π/12
is the polar exponential form. The polar form is 2
√
2 cos 7π
12 + i2
√
2 sin 7π
12 .
(d) Using results from parts (a) and (c),
−
√
3 + i
1 −i
=
2ei5π/6
√
2e−iπ/4 =
√
2 ei 5π
6 −

−i π
4

=
√
2 ei13π/12 =
√
2 e−i11π/12
is
the
polar
exponential
form,
and
the
polar
form
is
√
2 cos

−11π
12

+
i
√
2 sin

−11π
12

. ⃝
Example 15.2
Explain why cos 3θ = cos3 θ −3 cos θ sin2 θ.
Method: cos 3θ = Re

ei3θ
= Re

eiθ3
= Re

(cos θ + i sin θ)3
= Re

cos3 θ + i3 cos2 θ sin θ −3 cos θ sin2 θ −i sin3 θ

= cos3 θ −3 cos θ sin2 θ. ⃝
Because we have
eiθ = cos θ + i sin θ
and
e−iθ = cos(−θ) + i sin(−θ) = cos θ −i sin θ,
(15.7)
it follows that
e−iθ = eiθ.
Using that, we have
cos θ = Re

eiθ
= eiθ + e−iθ
2
(15.8)

Functions of a Complex Variable
1127
and
sin θ = Im

eiθ
= eiθ −e−iθ
2i
= i · −eiθ + e−iθ
2
.
(15.9)
15.1.2 Roots
We are familiar with the fact that the equation z2 = 9 has two solutions, z = ±3 = ±
√
9.
Now we will see how this fits into the general calculation of roots that are complex
numbers.
Example 15.3
For each of the given equations, find all exact solutions in the form a + ib:
(a) z2 = −5,
(b) z2 = −1 + i
√
3,
(c) z3 = −8,
(d) z4 = 81.
Method: Each of problems (a), . . . , (d) asks us to solve an equation of the form zn = w
where z is the unknown and w ̸= 0 is given. Our solution method is to use the polar
exponential forms of both z = reiθ and w = ρeiω, so the equation we are to solve is
(⋆)
ρeiω = w = zn = (reiθ)n = rneinθ.
Taking the modulus of both sides and noting that θ is real gives us
ρ =
ρeiω =
rneinθ = rn,
where ρ = |w| > 0 and r = |z| are both real. This implies that r = ρ1/n. Using ρ = rn and
(⋆) implies that we need to have
eiω = einθ.
Using the result of Theorem 15.1, we get ω+2πk = nθ for some integer k for all solutions
z = reiθ.
(a) Using polar exponential forms z = reiθ and w = −5 = 5eiπ, we see that the equation
z2 = −5 is equivalent to 5 eiπ = −5 = z2 =

reiθ2 = r2 ei2θ. As in the general
discussion of the method earlier, we need to solve (1) 5 = |w| = r2 for r = |z| and
(2) eiπ = ei2θ for real θ. This gives us r =
√
5, which, by definition, is the unique
nonnegative number whose square is 5.
By (15.6), (2) is true if, and only if, π +2πk = 2θ, for some integer k. This is equivalent
to θ = π
2 + πk. So, the solutions are
z = reiθ =
√
5ei

π
2 +πk

,
k = any integer.
At first glance, we seem to have infinitely many solutions, and we know from pre-
vious experience with high school algebra that there should be only two solutions,
z = ±i
√
5. We rewrite the solutions as
z =
√
5 · ei π
2 eiπk =
√
5 · i · (−1)k.
Since there are only two possibilities for (−1)k, what looks like infinitely many
solutions does reduce to two solutions, z = ±i
√
5.

1128
Advanced Engineering Mathematics
(b) Using polar exponential forms z = reiθ and w = −1 + i
√
3 = 2 ei 2π
3 , we see that the
equation z2 = −1 + i
√
3 is equivalent to 2 ei 2π
3 = −1 + i
√
3 = z2 =

reiθ2 = r2 ei2θ.
As in the general discussion of the method earlier, we need to solve (1) 2 = |w| = r2
for r = |z| and (2) ei 2π
3 = ei2θ for real θ. This gives us r =
√
2.
By (15.6), (2) is true if, and only if, 2π
3 + 2πk = 2θ, that is, θ = π
3 + πk, for some
integer k. So, the solutions are
z = reiθ =
√
2 ei

π
3 +πk

,
k = any integer.
Again, what appears to be infinitely many solutions reduces to two solutions:
z =
√
2 · eiπ/3eiπk =
√
2 ·
	
−1
2 + i
√
3
2

· (−1)k = ±
	
−1
√
2
+ i
√
3
√
2

.
The set of all solutions is

−1
√
2
+ i

3
2, −1
√
2
−i

3
2

.
(c) Using polar exponential forms z = reiθ and w = −1 + i
√
3 = 2 ei 2π
3 , we see that the
equation z3 = −8 is equivalent to 8 eiπ = −8 = z3 =

reiθ3 = r3 ei3θ. This gives both
(1) 8 = r3, which tells us r = 2, and (2) eiπ = ei3θ.
By (15.6), (2) is equivalent to π + 2πk = 3θ, that is, θ = π
3 + 2π
3 k, for some integer k.
So, the solutions are
z = reiθ = 2 ei

π
3 + 2π
3 k

,
k = any integer.
What appears to be infinitely many solutions reduces to three solutions:
z0 = 2 ei

π
3 +0

= 2

1
2 + i
√
3
2

= 1 + i
√
3,
z1 = 2 ei

π
3 + 2π
3

= 2 eiπ = −2,
and
z2 = 2 ei

π
3 + 4π
3

= 2 ei 5π
3 = 2
	
1
2 −i
√
3
2

= 1 −i
√
3.
The set of all solutions is {1 + i
√
3, −2, 1 −i
√
3} and is displayed in Figure 15.4a.
(d) Using polar exponential forms z = reiθ and w = 81 = 81 ei·0, we see that the equation
z3 = 81 is equivalent to 81 ei·0 = 81 = z4 =

reiθ4 = r4 ei4θ. This gives both (1)
81 = r4, which tells us r = 3, and (2) ei·0 = ei4θ.
By (15.6), (2) is equivalent to 0 + 2πk = 4θ, that is, θ = π
2 k, for some integer k. So,
the solutions are
z = reiθ = 3 ei π
2 k,
k = any integer.
What appears to be infinitely many solutions reduces to four solutions:
z0 = 3 ei0 = 3,
z1 = 3 ei π
2 = i3,
z2 = 3 eiπ = −3,

Functions of a Complex Variable
1129
Im(z)
Re(z)
Im(z)
i3
–i3
3
–3
Re(z)
–2
1+i √3
1–i √3
(a)
(b)
FIGURE 15.4
Example 15.3 (a) and (b).
and
z3 = 3 ei 3π
2 = −i3.
The set of all solutions is {3, i3, −3, −i3} and is displayed in Figure 15.4b. ⃝
Even though the instructions asked for the solutions in the form z = a + ib, it helped to
use the polar exponential form of z.
It is not an accident that the solutions of z3 = w are the vertices of an equilateral triangle,
if w ̸= 0. You will explain why in Problem 15.1.4.11.
By the way, we can check that the solutions in part (c) agree with what we know from
high school algebra: the solutions of
0 = z3 + 8 = (z + 2)(z2 −2z + 4)
are z = −2 and, by the quadratic formula, z = 2±√
(−2)2−4·1·4
2·1
= 2±
√
−12
2
= 2
2 ± i2
√
3
2
=
1 ± i
√
3, as we found earlier.
In general, if w is any complex number and n is an integer ≥2, then we can solve zn = w
by using polar exponential forms w = ρeiα and z = reiθ:
ρeiα = w = zn =

reiθn
= rneinθ
has solutions
z = ρ1/nei(α+2πk)/n = ρ1/neiα ei2πk/n = ρ1/neiα 
ei2π/nk
,
k = 0, 1, . . . , n −1.
Define the principal nth root of unity by
ωn = ei2π/n

1130
Advanced Engineering Mathematics
because, first, ω n
n = 1 and, second, all of the roots of 1 are given by
1, ωn, ω 2
n , . . . , ω n−1
n
.
Further, all of the solutions of zn = w are given by
ρ1/neiα/n,
ρ1/neiα/nωn , . . . ,
ρ1/neiα/nω n−1
n
.
15.1.3 Functions
A function of a complex variable is a “machine” that gives a single complex number
output, f(z), for every allowed complex number input z, as depicted in Figure 15.5. The
word mapping is synonymous with the word “function.”
Given a function f and a set of allowed inputs, A, the image of A under f is the set
{f(z) : z in A}.
Example 15.4
Under the mapping f(z) ≜1/z, find the image of the set {z : |z| = 2}.
Method: Define w = u + iv by w = f(z), where u = Re

f(z)

and v = Im

f(z)

. Define
set A = {z : |z| = 2}. For every z in A, the corresponding image point w = f(z) = 1/z has
1/w = z satisfy
2 = |z| =

1
w
 = |1|
|w| =
1
|w|,
so |w| = 1
2. So, the image set f(A) satisfies
f(A) ⊆

w : |w| = 1
2

.
Is f(A) = {w : |w| = 1
2}? That is, does the image of A include all of the points in that
circle in the w-plane? For any w satisfying |w| = 1
2, if we let z = 1
w, then
f(z) = 1
z = 1
1
w
= w
and |z| =
1
|w| =
1
1/2 = 2 so that z is in A. So, f(A) includes all of the points of the circle
|w| = 1
2. That fact, combined with the inclusion f(A) ⊆{w : |w| = 1
2}, gives us the
conclusion that f(A) = {w : |w| = 1
2}. ⃝
Geometrically, the set A, that is, {z : |z| = 2}, is the circle in the z-plane of radius 2 whose
center is at the origin. Similarly, the set f(A) = {w : |w| = 1
2} is the circle in the w-plane of
radius 1
2 whose center is at the origin.
z
f
f(z)
FIGURE 15.5
Function as a machine.

Functions of a Complex Variable
1131
C
B
D
x
y
2
1
1
2
1
1
C
D
B
C΄
C΄
D΄
D΄
v
u
B΄
B΄
FIGURE 15.6
Mapping of a circle by the inversion mapping.
By the way, implicit in our way of solving Example 15.4 was our use of the inverse
function f −1(w) because
w = f(z) = 1
z ⇐⇒z = f −1(w) = 1
w .
While we cannot draw a graph of f(z) versus z for z in A, we can show the effect of the
mapping f on A using a transformation picture like the ones we showed in Section 6.6 for
linear transformations of the plane. The transformation picture is shown in Figure 15.6.
The mapping f(z) = 1/z is called the inversion mapping. It is possible to show that
whenever A is a circle in the z-plane that does not pass through the origin, z = 0, then f(A)
is a circle in the w-plane.
Under the inversion mapping, what is the image of a circle A that does pass through the
origin? The inversion mapping is undefined at the origin, so we should omit it from A
by defining A = {z ̸= 0 : z is in A} and then finding the image of A under the inversion
mapping.
Example 15.5
Find the image under the inversion mapping of the set A, which is the circle in the
z-plane of radius two whose center is at −i2.
Method: Define w = u + iv by w = f(z), where u = Re

f(z)

and v = Im

f(z)

. The
set A consists of z = x + iy whose distance from the point z0 ≜−i2 is two, that is,
2 = |x + iy −(0 −i2)| =

(x −0)2 + (y −(−2))2. So,
A = {z : |z + i2| = 2},
and every z in A is of the form
z = −i2 + 2eit
for some real t. We define
A = {z ̸= 0 : z is in A} =

−i2 + 2eit : π
2 < t < 5π
2

.

1132
Advanced Engineering Mathematics
C
B
D
x
y
–2
–3
–1
1
1
C
D
B
C΄
C΄
D΄
D΄
v
1
4
v=—
u
B΄
B΄
FIGURE 15.7
Example 15.5.
It follows that for every z in A,
u + iv = w = f(z) =
1
−2i + 2eit =
1
−2i + 2 cos t + i2 sin t = 1
2 ·
1
cos t + i(−1 + sin t).
Rationalizing the denominator gives
u + iv = 1
2 · cos t −i(−1 + sin t)
cos2 t + (−1 + sin t)2 = 1
2 ·
cos t −i(−1 + sin t)
cos2 t + (1 −2 sin t + sin2 t)
= 1
2 · cos t + i(1 −sin t)
2 −2 sin t
= 1
4

cos t
1 −sin t + i 1 −sin t
1 −sin t

=
cos t
4(1 −sin t) + i 1
4 .
So, f(A) ⊆{u + iv : v = 1
4}; the latter is a horizontal line in the w-plane. The ultimate
results are illustrated in Figure 15.7.
Is f(A) = {u + iv : v = 1
4}? That is, does the image of A include all of the points in that
line in the w-plane? For any w = u+iv satisfying Im(w) = 1
4, we need to show that there
is at least one value of t in the interval π
2 < t < 5π
2 for which g(t) ≜
cos t
4(1−sin t) = u. We
calculate
lim
t→π
2
+
cos t
4(1 −sin t) = lim
t→π
2
+
(1 + sin t) cos t
4(1 + sin t)(1 −sin t) = lim
t→π
2
+
(1 + sin t)
4 cos t
= lim
t→π
2
+
≈2
≈0−= −∞,
(15.10)
and similarly
lim
t→5π
2
−g(t) =
lim
t→5π
2
−
cos t
4(1 −sin t) = ∞.
(15.11)
Because g(t) is continuous for π
2 < t < 5π
2 , (15.10) and (15.11) imply g(t) takes on all values
in the interval (−∞, ∞). This concludes the explanation why f(A) = {u + iv : v = 1
4}, a
horizontal line in the w-plane. ⃝
In general, if A is a circle passing through the origin in the z-plane, then the image of A
under the inversion mapping is a line in the w-plane. We will explain this in Chapter 16
when we need this general result.

Functions of a Complex Variable
1133
15.1.4 Problems
1. Why is wz = w z for all complex numbers z ̸= 0 and w?
2. Why is
 w
z

= w
z for all complex numbers w, z?
3. Explain why z1z2 −z2z1 is imaginary for all complex numbers z1, z2.
4. Why is |z −w| the distance between the points z and w in the complex plane?
5. Explain why the only complex number z for which z = 2 −1
z is z = 1. [Hint:
Multiply both sides by z.]
6. Find the exact polar exponential form of (a) −1+i
√
3−i, (b) (−1 + i)3, and (c) (
√
3 −i)7.
7. Find the exact polar exponential and x + iy forms of (a) (−
√
3 −i)2,
(b)

3
5 −i 4
5
2
, and (c) −
√
3−i
1+i .
8. Explain why sin 4θ ≡4 cos θ sin θ(cos2 θ −sin2 θ) is an identity.
9. Explain why (a) sin 3θ
≡
3 cos2 θ sin θ −sin3 θ and (b) cos 4θ
≡
cos4 θ −
6 cos2 θ sin2 θ + sin4 θ are identities.
10. The number π is irrational, that is, cannot be written as a ratio of integers. Use
this fact to explain why z ≜ei has zn ̸= 1 for all integers n.
11. Suppose ρ > 0 and α is real. Explain why the cube roots of any number
0 ̸= ρeiα are the vertices of an equilateral triangle. What is the length of each of its
sides?
12. Suppose ρ > 0 and α is real. Explain why the sixth roots of any number ρeiα
are the vertices of an equilateral hexagon. What is the length of each of its
sides?
13. Find the exact solutions of the equations. Give your final conclusions in both polar
exponential and x + iy forms. (a) z2 = i4, (b) z2 = −2 −i2
√
3, (c) z3 = 27,
(d)
z3 = −2 −i2, (e) z3 = −4 + i4
√
3, (f) z4 = −81.
14. Solve z3 = −1 + i and express the solution(s) in polar exponential form.
15. Solve z3 = −1 −i
√
3 and express the solution(s) in polar exponential form.
16. Find the exact solutions of the equations. Give your final conclusions in both polar
exponential and x + iy forms:
(a) z2 + 4z + 8 = 0,
(b) z3 −z2 + z = 0.
17. Find all solutions of the equations:
(a) 1 −z
1 + z = z,
(b) (z −i)3 = z3.
18. For each of (a), (b), and (c), describe in words involving geometric concepts the set
of points and draw it in the complex plane:
(a) |z + 1 −i
√
3| = 1,
(b) |z −1| = |z −i2|,
(c) |z −i| = |z + 1|.

1134
Advanced Engineering Mathematics
19. Find the image of the given sets under the inversion mapping. Draw an appropri-
ate transformation picture and give the image in set notation:
(a) {z : |z| = 3},
(b) {z : |z −1| = 1},
(c) {z : |z + 2| = 2},
(d) {z : |2z + i| = 1}.
20. Find the image of the given sets under the mapping f(z) = z2. Draw an appropri-
ate transformation picture and give the image in set notation. (a) The half of the
circle |z| = 2 with Re(z) ≥0; (b) the half of the circle |z| = 2 with Im(z) ≤0; (c)
the quarter disk {z = x + iy : 0 ≤x2 + y2 ≤4, x ≥0, y ≥0};
(d) the line
Im(z) = −1, (e) the line Re(z) = 1, (f) the line Im(z) = β, where β is an unspeci-
fied real constant, (g) the line Re(z) = α, where α is an unspecified real constant,
(h) the line Re(z) = Im(z).
21. Find the image of the given sets under the mapping f(z) = iz −3. Draw an appro-
priate transformation picture and give the image in set notation. (a) {z : |z| = 1},
(b) {z : Re(z) = 2}, (c) {z : Im(z) = −1}, (d) {z : Re(z) = −Im(z)}.
22. Find the exact value of (−
√
3+i)5 in the form x+iy, where x, y are real. Do evaluate
exact trigonometric function values.
23. Suppose a and R are unspecified real numbers. Using the formula |x + iy| =

x2 + y2, explain why, for all real θ, both (a) |a + Reiθ| = |a + Re−iθ| and (b)
|a + Reiθ| = |aeiθ + R|.
15.2 Derivative and the Cauchy–Riemann Equations
Just as for a function of a single real variable, there are concepts of limit, continuity, and
derivative for a function of a single complex variable. Such functions usually take on com-
plex values. For example, if z = x + iy where x and y are real, then the function defined by
f(z) = z2 can also be expressed as
f(z) ≜z2 = (x + iy)2 = (x2 −y2) + i2xy ≜u(x, y) + iv(x, y),
whose real part is u(x, y) = Re

f(z)

= x2 −y2 and whose imaginary part is v(x, y) =
Im

f(z)

= 2xy.
So, a function of a single complex variable z = x + iy can be expressed in terms of
two real-valued functions of the two real variables x, y. Our work with functions of a sin-
gle complex variable will have a lot in common with the multivariable calculus study of
vector fields and thus will be richer than Calculus I study of a function of a single real
variable.
For example, the function of a single real variable x given by g(x) = |x|2 is differentiable
at all x, but the function of a single complex variable z given by f(z) = |z|2 is differentiable
only at z = 0, as we will see in Example 15.12.

Functions of a Complex Variable
1135
Example 15.6
Express f(z) ≜1/z in terms of two functions of two real variables.
Method: Let z = x + iy where x and y are real. Using rationalization of the denominator,
f(z) ≜1
z =
1
x + iy =
(x −iy)
(x + iy)(x −iy) = x −iy
x2 + y2 =
x
x2 + y2 + i
−y
x2 + y2 ≜u(x, y) + iv(x, y),
whose real part is u(x, y) =
x
x2 + y2 and whose imaginary part is v(x, y) = −
y
x2 + y2 . ⃝
An open disk around the complex number z0 is a set of the form
Dr(z0) ≜{z : |z −z0| < r},
where r > 0. The punctured disk of positive radius r about z0 is the set {z : 0 < |z−z0| < r}.
Intuitively, the complex output values w ≜f(z) tend to a limit w0 if f(z) gets closer and
closer to w0 as the input values z get closer and closer to z0. Because modulus measures
distance between two complex numbers, having such a limit means that
|f(z) −w0|
goes to zero
as |z −z0|
goes to zero.
Here is a precise definition.∗
Definition 15.1
Suppose the domain of f(z) includes a punctured disk about z0. Suppose that for all ε > 0
there is a δ > 0 such that 0 < |z −z0| < δ implies |f(z) −w0| < ε. Then we say that the
limit of f(z), as z approaches z0, exists and equals w0, and we write
lim
z→z0 f(z) = w0
(15.12)
or
f(z) →w0
as
z →z0.
(15.13)
In the definition, given a function f, the δ > 0 may depend on either or both of ε and z0.
The existence of the limit in (15.12) means that no matter how small an open disk (of
radius ε) is drawn around w0, there is a sufficiently small, positive radius δ such that f
maps the punctured disk {z : 0 < |z −z0| < δ} into the disk {w : |w −w0| < ε}. This is
depicted in Figure 15.8, where complex numbers w = u + iv with u and v being real.
∗There is a more general definition of limit when the domain of f(z) does not include a punctured disk about z0:
we say z0 is a limit point of a set A if for all r > 0, there is a point zr ̸= z0 in the intersection A∩Dr(z0). Suppose
z0 is a limit point of the domain of f(z) and for all ε > 0 there exists a δ > 0 such that z in both the domain
of f and the punctured disk of radius δ about z0 implies |f(z) −w0| < ε. Then we say that the limit of f(z),
as z approaches z0, exists and equals w0, and we write limz→z0 f(z) = w0 or f(z) →w0
as
z →z0. This more
general definition is used when we consider boundary values, for example, for partial differential equations
(PDEs) rewritten as problems in the complex plane.

1136
Advanced Engineering Mathematics
y
x
v
u
f
z0
w0
ε
δ
FIGURE 15.8
Limit in terms of ε and δ.
Theorem 15.2
Suppose limz→z0 f(z) and limz→z0 g(z) exist and c is a complex constant. Then there exist
(a) limz→z0

f(z) + g(z)

= limz→z0 f(z) + limz→z0 g(z).
(b) limz→z0

c f(z)

= c limz→z0 f(z).
(c) limz→z0

f(z)g(z)

=

limz→z0 f(z)
 
limz→z0 g(z)

.
(d) lim
z→z0
g(z)
h(z) = limz→z0 g(z)
limz→z0 h(z), as long as there exists limz→z0 h(z) ̸= 0.
Example 15.7
Explain why limz→i
3(z2+1)
z−i
= i6.
Method: Let z0 = i and define f(z) ≜3(z2 + 1)
z −i
for z ̸= i. For z ̸= i,
f(z) = 3(z2 + 1)
(z −i)
= 3(z + i)
(z −i)

(z −i)
= 3(z + i).
The problem’s narrative gave us the clue that f(z) should be approaching i6 = w0 as
z →i. For z ̸= i, we calculate
|f(z) −w0| = |3(z + i) −i6| = |3z −3i| = |3(z −i)| = |3| |z −i| = 3 |z −i|.
So, for any ε > 0, we can choose δ = 1
3 ε to see that 0 < |z −i| < δ implies |f(z) −i6| < ε,
as we desired. ⃝
The results of Example 15.7 are illustrated in Figure 15.9.
In Example 15.7, the limit exists as z →i even though f(z) is undefined at z = i. When we
say that the complex numbers f(z) approach i6, it is a completely separate issue whether f
does or does not take on the value i6, or any value at all, at z = i.
In general, for limz→z0 f(z) = w0 to exist, it must be true that the complex numbers f(z)
approach w0 for all paths of complex numbers z approaching z0.

Functions of a Complex Variable
1137
y
i
δ
x
f
v
ε
i6
u
FIGURE 15.9
Example 15.7.
Definition 15.2
A function f is continuous at z0 if f is defined on a sufficiently small open disk around z0
and limz→z0 f(z) = f(z0).
Definition 15.3
A function f whose domain is D has an extensionf if both (a) the domain off is a set 
D
that contains D and (b) f(z) =f(z) for all z in D.
Example 15.8
Because f(z) ≜3(z2 + 1)
z −i
is undefined at z = i, f is not continuous at i. Find an extension
of f that is continuous at i.
Method: The natural domain of f is D = {z : z ̸= i}. Since we wantf(z) = f(z) for all z ̸= i
and we know from Example 15.7 that limz→i f(z) = i6, we should try
f(z) ≜
⎧
⎪⎪⎨
⎪⎪⎩
3(z2 + 1)
z −i
,
if z ̸= i
i6,
if z = i
⎫
⎪⎪⎬
⎪⎪⎭
.
We constructedf to agree with f on D and the domain off is the whole z-plane, sof is
an extension of f.
Becausef is defined at z = i and on an open disk around z = i, and
lim
z→i f(z) = lim
z→i
f(z) = i6,
we conclude thatf is continuous at z = i and thus is as desired. ⃝

1138
Advanced Engineering Mathematics
By a polynomial in z of degree n, we will mean a function of the form p(z) = a0 + a1z +
· · · + anzn, where a0, . . . , an are constant complex numbers and an ̸= 0.
Theorem 15.3
(a) If f(z) is a polynomial, then f(z) is continuous at every z.
(b) If p(z) and q(z) are polynomials, then f(z) ≜p(z)/q(z) is continuous at every z
where q(z) ̸= 0.
(c) If f(z) is a polynomial defined on a punctured disk {z : 0 < |z −z0| < δ} whose
radius is positive, then there exists limz→z0 f(z).
15.2.1 Derivatives
The definition of the derivative of a function of a single complex variable is analogous to
the definition of derivative of a function of a single real variable.
Throughout the rest of this section, we will assume that a function f(z) is defined at
least for all z in some open disk about a point z0, that is, the domain of f includes a disk
{z : |z −z0| < δ} for some δ > 0, possibly very small.
Definition 15.4
The derivative of f at z0 exists and is given by
f ′(z0) ≜lim
z→z0
f(z) −f(z0)
z −z0
,
if the limit exists. If it does, then we say that f is differentiable at z0.
An alternative notation for the limit in the definition of the derivative is
f ′(z) = lim
△z→0
f(z+ △z) −f(z)
△z
,
if the limit exists.
We begin with an example and some properties of derivatives that are familiar to us
from our Calculus I courses.
Example 15.9
Assume a is an unspecified complex number. Explain why f(z)
≜
3(z2 + a) is
differentiable everywhere and find a formula for its derivative as a function of z.
Method: For any z0,
f ′(z0) = lim
z→z0
f(z) −f(z0)
z −z0
= lim
z→z0
3(z2 + a) −3(z2
0 + a)
z −z0
= lim
z→z0
3(z2 −z2
0)
z −z0
,

Functions of a Complex Variable
1139
so, using Theorem 15.3(c), there exists
f ′(z0) = lim
z→z0
3(z + z0)

(z −z0)


(z −z0)
= lim
z→z0 3(z + z0) = 3(z0 + z0) = 6z0.
As a function of z, the derivative is f ′(z) = 6z. ⃝
Theorem 15.4
(a) If n is a positive integer,
then zn is differentiable everywhere and

zn′ = nzn−1.
(b) If f(z) is a polynomial, then f(z) is differentiable everywhere.
(c) If p(z) and q(z) are polynomials, then f(z) ≜p(z)/q(z) is differentiable at
every z for which q(z) ̸= 0.
Theorem 15.5
If f(z) is differentiable at z0, then f(z) is continuous at z0.
Theorem 15.6
Suppose f(z) and g(z) are differentiable at z and c is a complex constant. Then the following
derivatives at z exist:
(a)

f(z) + g(z)
′ = f ′(z) + g′(z).
(b)

c f(z)
′ = c f ′(z).
(c)

f(z)g(z)
′ = f ′(z)g(z) + f(z)g′(z).
(d)
d
dz
 f(z)
g(z)

= f ′(z)g(z) −f(z)g′(z)

g(z)
2
, as long as g(z) ̸= 0.
(e) (f ◦g)′(z) = d
dz

f

g(z)
 
= f ′(g(z))g′(z), as long as f is differentiable at g(z).
The result of Theorem 15.6(c) is the product rule, the result of Theorem 15.6(d) is the
quotient rule, and the result of Theorem 15.6(e) is the chain rule for (f ◦g)(z) = f

g(z)

, the
composition of two functions of complex variables.
Example 15.10
At what value(s) of z is (z + i)/(z −i) differentiable? Find its derivative where it exists.
Method: The denominator is zero only at z = i, so the derivative of (z + i)/(z −i) fails to
exists at z = i. At every other z in the complex plane, the derivative exists and is given by
z + i
z −i
′
= (z + i)′(z −i) −(z + i)(z −i)′
(z −i)2
= (z −i) −(z + i)
(z −i)2
= −
2i
(z −i)2 . ⃝

1140
Advanced Engineering Mathematics
15.2.2 Cauchy–Riemann Equations
Here we begin to see how the results for a function f(z) of one complex variable z = x + iy
are richer than the results for a function of one real variable or even the results for a planar
vector field, that is, a vector valued function of two real variables (x, y).
Theorem 15.7
Suppose f(z) = u(x, y)+iv(x, y), where z = x+iy, and x, y, u, v are real. If f(z) is differentiable
at z0 = x0 + iy0, then ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y all exist at (x0, y0) and the Cauchy–Riemann equations
hold at (x0, y0), that is,
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂u
∂x(x0, y0) = ∂v
∂y(x0, y0)
∂u
∂y(x0, y0) = −∂v
∂x(x0, y0)
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
(15.14)
Further
f ′(z0) = ∂u
∂x(x0, y0) + i ∂v
∂x(x0, y0)
(15.15)
and
f ′(z0) = ∂v
∂y(x0, y0) −i ∂u
∂y(x0, y0) .
(15.16)
Why? First, we see that the Cauchy–Riemann equations, (15.14), follow from comparing
(15.15) with (15.16). So, it will suffice to show that both (15.15) and (15.16) are true and that
the four partial derivatives that appear in those equations exist.
Assume f(z) is differentiable at z0 = x0 + iy0. Then, by definition,
f ′(z0) = f ′(x0 + iy0) =
lim
x+iy→x0+iy0
f(x + iy) −f(x0 + iy0)
(x + iy) −(x0 + iy0)
(15.17)
exists and is the same complex number, no matter what path in the z-plane along which
we choose to have x + iy approach x0 + iy0.
In particular, along horizontal paths with y ≡y0, we calculate
f ′(z0) = lim
x →x0
y ≡y0
f(x + iy0) −f(x0 + iy0)
(x + iy0) −(x0 + iy0)
= lim
x→x0
u(x, y0) + iv(x, y0) −

u(x0, y0) + iv(x0, y0)

x −x0
,
that is,
(⋆) f ′(z0) = lim
x→x0
u(x, y0) −u(x0, y0)
x −x0
+ i v(x, y0) −v(x0, y0)
x −x0

.

Functions of a Complex Variable
1141
In order for the limit of a complex-valued expression to exist, the limits of both the real
and imaginary parts must exist. So, if f ′(z0) exists, then there exist both
lim
x→x0
u(x, y0) −u(x0, y0)
x −x0
= ∂u
∂x(x0, y0)
and
lim
x→x0
v(x, y0) −v(x0, y0)
x −x0
= ∂v
∂x(x0, y0).
So, from the existence of f ′(z0) and (⋆), it follows that (15.15) is true, that is,
f ′(z0) = ∂u
∂x(x0, y0) + i ∂v
∂x(x0, y0) .
Along vertical paths with x ≡x0, we calculate
f ′(z0) = lim
x ≡x0
y →y0
f(x0 + iy) −f(x0 + iy0)
(x0 + iy) −(x0 + iy0)
= lim
y→y0
u(x0, y) + iv(x0, y) −

u(x0, y0) + iv(x0, y0)

i(y −y0)
,
that is,
(⋆⋆) f ′(z0) = lim
y→y0
u(x0, y) −u(x0, y0)
i(y −y0)
+ i v(x0, y) −v(x0, y0)
i(y −y0)

.
In order for the limit of a complex-valued expression to exist, the limits of both the real
and imaginary parts must exist. So, if f ′(z0) exists, then there exist both
lim
y→y0
v(x0, y) −v(x0, y0)
y −y0
= ∂v
∂y(x0, y0)
and
−lim
y→y0
u(x0, y) −u(x0, y0)
x −x0
= −∂u
∂y(x0, y0).
So, from the existence of f ′(z0) and (⋆⋆), it follows that (15.16) is true, that is,
f ′(z0) = ∂v
∂y(x0, y0) −i ∂u
∂y(x0, y0) .
Equating (15.15) and (15.16) establishes the rest of the theorem. 2

1142
Advanced Engineering Mathematics
So, differentiability of f = u + iv implies that the Cauchy–Riemann equations, relating
partial derivatives of u and v, are satisfied. This suggests the possibility that a function f
satisfying the Cauchy–Riemann equations would be differentiable. Unfortunately, satis-
fying the Cauchy–Riemann equations, while a necessary consequence of differentiability,
does not, by itself, guarantee differentiability. We do have a partially satisfying result:
Theorem 15.8
Suppose f(z) = u(x, y) + iv(x, y), where z = x + iy, and x, y, u, v are real. If ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y are
all continuous at (x0, y0) and the Cauchy–Riemann equations, (15.14), hold at (x0, y0), then
f(z) is differentiable at z0 = x0 + iy0 and f ′(z0) is given by (15.15) and by (15.16).
We may refer to the properties that “ ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y are all continuous at (x0, y0)” as the
“technical hypotheses.”
Example 15.11
At what value(s) of z is f(z) ≜2xy + i(y2 −x2) differentiable? Find a formula for f ′(z)
where it exists.
Method: u(x, y) = 2xy and v(x, y) = y2 −x2 are polynomials in real variables x and y, so
∂u
∂x = 2y, ∂u
∂y = 2x, ∂v
∂x = −2x, and ∂v
∂y = 2y exist and are continuous everywhere. Thus,
the technical hypotheses of Theorem 15.8 are satisfied. This implies that the Cauchy–
Riemann equations are “decisive” in this example, in the sense that f ′(x0 + iy0) exists if,
and only if, the Cauchy–Riemann equations are satisfied at (x0, y0).
For any x + iy, we verify the Cauchy–Riemann equations:
∂u
∂x (x, y) −∂v
∂y(x, y) = 2y −2y ≡0
and
∂u
∂y (x, y) + ∂v
∂x(x, y) = 2x + (−2x) ≡0.
So, f is differentiable everywhere. By (15.15), f ′(z) = ∂u
∂x +i ∂v
∂x = 2y −i2x = −i2z. ⃝
Example 15.12
At what value(s) of z is f(z) ≜|z|2 differentiable? Find a formula for f ′(z) where it exists.
Method: f(z) = x2 + y2, so u(x, y) = x2 + y2 and v(x, y) ≡0 are polynomials in real
variables x and y, so ∂u
∂x = 2x, ∂u
∂y = 2y, ∂v
∂x = ∂v
∂y ≡0 exist and are continuous everywhere.
So the technical hypotheses of Theorem 15.8 are satisfied. This implies that the Cauchy–
Riemann equations are decisive in this example. For any x + iy, we have
∂u
∂x (x, y) −∂v
∂y(x, y) = 2x −0 = 2x
and
∂u
∂y (x, y) + ∂v
∂x(x, y) = 2y + 0 = 2y,
so f is differentiable only where both 2x = 0 and 2y = 0. So, f is differentiable only at the
origin, z = 0, where (15.15) gives f ′(0) = ∂u
∂x (0, 0) + i ∂v
∂x (0, 0) = 0 + i 0 = 0. ⃝

Functions of a Complex Variable
1143
Example 15.13
At what value(s) of z is f(z) ≜y2 −x2 + x + i(−2xy + y2) differentiable? Find a formula
for f ′(z) where it exists.
Method: u(x, y) = y2 −x2 + x and v(x, y) = −2xy + y2 are polynomials in real variables
x and y, so ∂u
∂x = −2x + 1, ∂u
∂y = 2y, ∂v
∂x = −2y, ∂v
∂y = −2x + 2y exist and are continuous
everywhere. So, the technical hypotheses of Theorem 15.8 are satisfied. This implies that
the Cauchy–Riemann equations are decisive in this example.
For any x + iy, we have
∂u
∂x (x, y) −∂v
∂y(x, y) = −2x + 1 −(−2x + 2y) = 1 −2y
and
∂u
∂y (x, y) + ∂v
∂x(x, y) = 2y + (−2y) ≡0,
so f is differentiable only if 1−2y = 0. So, f is differentiable only on the line y = 1
2, where
(15.15) gives f ′(x+i 1
2) = ∂u
∂x (x, 1
2)+i ∂v
∂x (x, 1
2) = (1−2x)+i(−2· 1
2) = 1−2x−i = 1−2z. ⃝
In the appendix of this section, we will study two examples to further understand
complications involving the Cauchy–Riemann equations.
15.2.3 Orthogonal Families of Curves and an Application to Fluid Flow
Suppose we have a function of two real variables  = (x, y) and a point P = (x0, y0) that
lies on a level curve {(x, y) : (x, y) = k1}; hence, (x0, y0) = k1. Recall from Section 6.5
that the vector
∇(x0, y0) = ∂
∂x (x0, y0)ˆı + ∂
∂y (x0, y0) ˆj
is normal to the level curve at point P, as long as ∇(x0, y0) ̸= 0.
As usual, write z = x + iy. Suppose further that (x, y) is the real part of a function f(z)
that is differentiable at z0 = x0 + iy0, that is, there is a function (x, y) such that
f(z) ≜(x, y) + i(x, y)
is differentiable at z0 = x0 + iy0. By the Cauchy–Riemann equations, at (x, y) = (x0, y0),
∇ • ∇ =
∂
∂x ˆı + ∂
∂y ˆj

•
∂
∂x ˆı + ∂
∂y ˆj

= ∂
∂x
∂
∂x + ∂
∂y
 
∂
∂y
= ∂
∂y
∂
∂x −∂
∂x
 
∂
∂y ≡0 .
(15.18)
Let k1 = (x0, y0) and k2 = (x0, y0), so the level curves (x, y) = k1 and (x, y) = k2 inter-
sect at the point (x0, y0). [We’ve dropped the set notation here for brevity.] By (15.18), those
two level curves intersect orthogonally, as long as ∇(x0, y0) ̸= 0.
The two families of level “curves”
F1 = {(x, y) = k1 : k1 in R},
F2 = {(x, y) = k2 : k2 in R}

1144
Advanced Engineering Mathematics
are orthogonal families, that is, at all points where a level curve (x, y) = k1 from family
F1 intersects a level curve (x, y) = k2 from family F2, these two level curves intersect
orthogonally.
Example 15.14
Fluids that have potential flow have particle trajectories
˙x(t)
˙y(t)

whose velocities are
given by
⎡
⎢⎢⎣
˙x(t)
˙y(t)
⎤
⎥⎥⎦=
⎡
⎢⎢⎢⎣
∂
∂x (x, y)
∂
∂y (x, y)
⎤
⎥⎥⎥⎦(x, y) = (x(t), y(t))
= ∇(x, y)(x, y) = (x(t), y(t))
for some scalar potential function (x, y). An equipotential curve is a level set (x, y) =
k1, for some constant k1. In 2D fluid flow, we denote the velocity by
v = ˙x(t)ˆı + ˙y(t) ˆj = ∇(x, y).
The fluid is incompressible if ∇• v = 0, in which case
0 = ∇• v = ∇•

∇(x, y)

= ∂
∂x
∂
∂x

+ ∂
∂y
∂
∂y

;
hence,
∂2
∂x2 + ∂2
∂y2 = 0,
that is, (x, y) satisfies Laplace’s equation. We will have more to say about solutions of
Laplace’s equation when we discuss “harmonic functions” in Section 15.3 and when we
use complex variables to solve Laplace’s equation in Chapter 16. ⃝
As usual, we write z = x + iy. Suppose further that a given potential function (x, y) is
the real part of a function f(z) that is continuously differentiable, that is, there is a function
(x, y) such that
f(z) ≜(x, y) + i(x, y)
is continuously differentiable; hence, ,  satisfy the Cauchy–Riemann equations and all of
their first partial derivatives are continuous functions. As we saw, the equipotential curves
(x, y) are orthogonal to the streamlines, defined to be the level sets (x, y) = k2.
Let us understand why the name “streamlines” makes sense: because (15.18) is true and
∇ = v,
0 = v • ∇.
Thus, automatically, a nonzero vector ∇ is normal to the streamline, that is, to the level
curve, (x, y) = k2, and (15.18) says that ∇ is orthogonal to the velocity vector, v. This
implies that at a point where there is a tangent vector, a nonzero velocity vector is tangent
to the streamline, by the result of Problem 15.2.5.15.

Functions of a Complex Variable
1145
15.2.4 Appendix
Here we will explore further the technical hypotheses or conclusions involved in differen-
tiability of a function f(z), where, as usual, z = x + iy, x, y being real. We will assume in
the two examples in the succeding text that f(z) = u(x, y) + iv(x, y) and that u(x, y), v(x, y)
have their first partial derivatives with respect to x and y existing at (x0, y0).
Example 15.15
In Problem 15.2.5.14∗you will explain why the function
f(z) ≜
⎧
⎪⎪⎨
⎪⎪⎩
x2 sin
1
x

+ i x y sin
1
x

,
if x ̸= 0
0,
if x = 0
⎫
⎪⎪⎬
⎪⎪⎭
is differentiable at z0 = 0 even though
∂u
∂x (x, y) =
⎧
⎪⎪⎨
⎪⎪⎩
2 x sin
1
x

−cos
1
x

,
if x ̸= 0
0,
if x = 0
⎫
⎪⎪⎬
⎪⎪⎭
is not continuous at (x, y) = (0, 0). So, f(z) being differentiable at a point z0 = x0 + iy0
does not imply that the first partial derivatives of u, v must be continuous at (x0, y0).
Example 15.16
Define
f(z) ≜
⎧
⎨
⎩
z3/|z|2,
z ̸= 0
0,
z = 0
⎫
⎬
⎭.
In Problem 15.2.5.16, you will explain why f is not differentiable at the point z0 = 0
even though u, v satisfy the Cauchy–Riemann equations at (0, 0). So, u, v satisfying the
Cauchy–Riemann equations at a point z0 are not enough to imply that f = u + iv is
differentiable at z0.
Learn More About It
An explanation of Theorem 15.8 can be found in Section 2.1 of Complex Variables, 2nd
ed., by Stephen D. Fisher, Dover Publications c⃝1999.
15.2.5 Problems
1. Evaluate the limit, if it exists:
(a) limz→−1
3(z2 −1)
z + 1
,
∗This example was created by my colleague, Professor James T. Vance, Jr.

1146
Advanced Engineering Mathematics
(b) limz→−1
2(z3 + 1)
z + 1
,
(c) limz→−1
2(z3 + 1)
3(z2 −1).
2. Evaluate the limit, if it exists:
(a) limz→−i
3(z2 + 1)
z + i
,
(b) limz→−i
z4 + 1
z + i ,
(c) limz→−i
z4 + 1
3(z2 + 1).
In problems 3 and 4, for the given function use results from problems 1 and 2, respectively,
to find an extension that is continuous everywhere.
3. (a) f(z) = 3(z2 −1)
z + 1
,
(b) g(z) = 2(z3 + 1)
z + 1
.
4. f(z) = 3(z2 + 1)
z + i
.
5. For the given function, find the derivative where it exists:
(a) f(z) = iz2 + 2z −(1 + i),
(b) g(z) = π(2z −i)2,
(c) h(z) = (z2 + 1)(z3 + 2z + 4),
(d) k(z) = z + i2
z −i ,
(e) ℓ(z) =
1
z2 −i2z −4,
(f) m(z) =
π2
z2(z −1).
In problems 6–12, a function f(z) is given. For each, (a) where is f differentiable? (b) where
is f not differentiable? and (c) find a formula for f ′(z), where it exists, in terms of z. As
usual, we write z = x + iy, where x and y are real.
6. f(z) = |z|2.
7. f(z) = (−1 + 2x)y + i(x −x2 + y2).
8. f(z) = (1 + 2x)y + i(x −x2 + y2).
9. f(z) = (−1 −2x)y + i(x −x2 −y2).
10. f(z) = 3x2 + y2 + i2xy.
11. f(z) = z Re(z).
12. f(z) = x2 + 3y2 + i2xy.

Functions of a Complex Variable
1147
13. Suppose f(z) = (x −iy)(2 −x2 −y2). Explain why f is differentiable only at the
points on the circle x2 + y2 = 1.
14. Define
f(z) ≜
⎧
⎪⎨
⎪⎩
x2 sin

1
x

+ ixy sin

1
x

,
if x ̸= 0
0,
if x = 0
⎫
⎪⎬
⎪⎭
.
(a) Explain why f(z) is differentiable at z0 = 0.
(b) Explain why
∂u
∂x(x, y) =
⎧
⎪⎨
⎪⎩
2x sin

1
x

−cos

1
x

,
if x ̸= 0
0,
if x = 0
⎫
⎪⎬
⎪⎭
is not continuous at (x, y) = (0, 0).
(c) Why do these results not contradict Theorem 15.7?
15. Explain why ∇ both being orthogonal to a nonzero vector v and normal to the
curve  = k2 in R2 implies that v is tangent to the streamline  = k2, at a point
where there is a tangent vector to this streamline.
16. Define f(z) ≜
⎧
⎨
⎩
z3/|z|2,
if z ̸= 0
0,
if z = 0
⎫
⎬
⎭.
Explain why f is not differentiable at a point z = 0 even though u, v satisfy the
Cauchy–Riemann equations at (0, 0). So, u, v satisfying the Cauchy–Riemann
equations at a point z0 are not enough to imply that f = u+iv is differentiable at z0.
15.3 Analyticity, Harmonic Function, and Harmonic Conjugate
We saw that a function f(z) may be differentiable everywhere, or everywhere except at
some isolated point(s), or only on a line. In principle, the set of points at which a func-
tion is differentiable could be anything. In this section, we will study the set of points of
differentiability.
A set A in the complex plane is open if at every point z0 in A there is some open disk
Dr(z0) contained in A. The radius r may depend on the point z0. For example, let A be the
first quadrant, that is, A = {x + iy : x > 0, y > 0}. For any z0 = x0 + iy0 in A, if we choose
r < min{x0, y0}, then Dr(z0) is contained in A. This is illustrated in Figure 15.10a.
To study open sets, it helps to have a property of complex numbers.
Theorem 15.9
(Triangle inequality) For all complex numbers z1, z2, |z1 + z2| ≤|z1| + |z2|.

1148
Advanced Engineering Mathematics
y
x
z0
z0
z1
z
R
r
y
x
r
y0
x0
(a)
(b)
FIGURE 15.10
(a) The first quadrant is an open set. (b) An open disk is an open set.
Example 15.17
Explain why every open disk is an open set.
Method: The picture in Figure 15.10b says a thousand words: suppose z1 is any point in
an open disk Dr(z0). We will explain why Dr(z0) is an open set by finding an open disk
DR(z1) that is contained in Dr(z0).
Because z1 is in Dr(z0), |z1−z0| < r, by definition of “open disk.” Define R = r−|z1−z0|,
and choose any z in DR(z1). We will explain why z must also be in Dr(z0), so we will have
shown that DR(z1) is contained in Dr(z0) and then we will be done.
To explain why z in DR(z1) is also in Dr(z0), the triangle inequality and the definition
of R allow us to calculate
|z −z0| = |(z −z1) + (z1 −z0)| ≤|z −z1| + |z1 −z0| < R + |z1 −z0|
= (r −|z1 −z0|) + |z1 −z0| = r. ⃝
Definition 15.5
A function is analytic on an open set O if the function is differentiable at every point in O.
Definition 15.6
A function is analytic at a point z0 if the function is differentiable at every point in some
open disk Dr(z0).
Definition 15.5 is more important than Definition 15.6. In fact, a function is analytic at a
point only if the function is analytic on some open disk centered at that point.
Many authors use the word “holomorphic” to mean the same thing as “analytic.”

Functions of a Complex Variable
1149
Example 15.18
Find where f(z) ≜2xy + i(y2 −x2) is analytic.
Method: In Example 15.11 in Section 15.2, we explained why f is analytic everywhere.
So, f is analytic on the whole complex plane, C. ⃝
Example 15.19
Find where f(z) ≜
1
z(z −i) is analytic.
Method: By Theorem 15.6 in Section 15.2, f is differentiable at all z except where its
denominator is zero, that is, except at z = 0 and z = i. By the result of Problem 15.3.3.24,
the set A = {z : z ̸= 0 and z ̸= i} is open, so f is analytic on A. ⃝
Example 15.20
Where is f(z) ≜y2 −x2 + i(−2xy + y2) analytic?
Method: In Example 15.13 in Section 15.2, we saw that f is differentiable only on the line
A ≜{x + iy : y = 1
2}. A contains no open disk, as illustrated in Figure 15.11: there is
no point z0 such that f is differentiable on an open disk centered at z0 because no matter
how small r is, f is not differentiable at z0 + i
3r. So, f is analytic nowhere. ⃝
Definition 15.7
A function is entire if the function is analytic on the whole complex plane, C.
Theorem 15.10
Polynomials in z are entire functions.
For future reference, we state
y
x
y= 1
2
z0
r
FIGURE 15.11
A function differentiable only on y = 1
2.

1150
Advanced Engineering Mathematics
Theorem 15.11
Suppose A is an open set in C. If, at every point in A, f(z) = u(x, y)+iv(x, y) both satisfies the
Cauchy–Riemann equations and has ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y each continuous, then f is analytic
on A.
Theorem 15.12
(L’Hôpital’s rule) Suppose f and g are analytic at a point z0. If either, f(z0) = g(z0) = 0 or
limz→z0 |f(z)| = limz→z0 |g(z)| = ∞, then
lim
z→z0
f(z)
g(z) = lim
z→z0
f ′(z)
g′(z),
assuming the latter limit exists.
15.3.1 Harmonic Functions
We need further study of sets of points.
Definition 15.8
(a) A directed line segment is a parameterized curve C : z = z(t) = z0 + tz1, where
z0 and z1 are complex constants, t is real, and a ≤t ≤b. We say that this directed
line segment connects z(a) to z(b), and we may notate it as z(a)  z(b).
(b) A polygonal curve connecting z0 to zn is either a finite sequence of directed line
segments z0  z1, z1  z2, . . . , zn−1  zn or a single directed line segment
z0  zn.
Definition 15.9
(a) A set A in C is connected if for every pair of distinct points z0, zn in A, there is a
polygonal curve C that satisfies all of the requirements that it lies in A, connects
the point z0 to zn, and is simple, that is, does not intersect itself.
(b) A domain in the complex plane is a set A that is both open and connected.
The requirement that the polygonal curve C be simple means that no two of its directed
line segments have a point in common other than the end of the k-th directed line segment
being the beginning of the k + 1st directed line segment and, possibly, z0 = zn. Examples
of domains include the sets illustrated in Figure 15.12.
An example of a set that is not a domain is the set
B ≜{z : |z| < 1 or |z −2| < 1}

Functions of a Complex Variable
1151
FIGURE 15.12
Three examples of domains.
y
x
2
FIGURE 15.13
Set B.
shown in Figure 15.13. B is the union of two open disks that do not “communicate,” that is,
B is like two circular rooms with no connecting “doorway.” Note that the point z = 1 is
not in B, so there is no way to get from a point in the disk |z| < 1 to the disk |z −2| < 1 by
a polygonal curve that remains in B.
Definition 15.10
A function (x, y) is harmonic on an open set D if  satisfies Laplace’s equation,
∂2
∂x2 + ∂2
∂y2 = 0,
at all points (x, y) in D.

1152
Advanced Engineering Mathematics
Clearly, harmonic functions are important in the study of partial differential equations.
That is one of the reasons the next result is so valuable.
Theorem 15.13
As usual, we write z = x + iy, where x and y are real. If f is analytic on an open set D,
then both
u(x, y) ≜Re

f(z)

and
v(x, y) ≜Im

f(z)

are harmonic on D.
Why? Suppose f is analytic on an open set D and z = x + iy is in D. Theorem 15.40 in
Section 15.9 will guarantee that f ′ and f ′′ are also analytic on D. This implies that all of the
second order partial derivatives of u and v exist and are continuous at (x, y) and u and v
satisfy the Cauchy–Riemann equations at z. We calculate that
∂2u
∂x2 = ∂
∂x
 ∂u
∂x

≡∂
∂x
 ∂v
∂y

≡∂
∂y
 ∂v
∂x

≡∂
∂y

−∂u
∂y

= −∂2u
∂y2 ;
hence, u is harmonic at z. A similar calculation explains why v is harmonic at z. 2
Example 15.21
Explain why u(x, y) = 3x2y −y3 is harmonic on C.
Method: u(x, y) = Re

f(z)

where f(z) ≜−iz3 is an entire function, so u(x, y) is harmonic
on C. ⃝
Alternatively, we could substitute u(x, y) into Laplace’s equation and verify that it is
satisfied at all (x, y) in R2.
15.3.2 Harmonic Conjugate
Definition 15.11
Given a function u(x, y) whose first partial derivatives are continuous on a domain D, we
say v is a harmonic conjugate of u if f(z) ≜u(x, y) + iv(x, y) is analytic on D.
Given a function u, part of the procedure for finding a harmonic conjugate, v, uses the
same technique for finding a scalar potential function as we used in Section 6.4. In order
for v to be a harmonic conjugate for the given u, the Cauchy–Riemann equations (15.14) in
Section 15.2, for f = u + iv demand that

Functions of a Complex Variable
1153
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂v
∂y ≡∂u
∂x
∂v
∂x ≡−∂u
∂y
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
on D.
Because u is given, we are effectively given
∇v = ∂v
∂x ˆı + ∂v
∂y ˆj = −∂u
∂y ˆı + ∂u
∂x ˆj.
Given vector field ∇v, we are asked to find a “scalar potential function,” v. Such a function
v exists only if the given function u is harmonic. Why? By Clairaut’s theorem, such a
function v(x, y) exists only if ∇v satisfies the exactness criterion, that is,
∂
∂x
 ∂v
∂y

≡∂
∂y
 ∂v
∂x

;
hence,
∂2u
∂x2 = ∂
∂x
 ∂u
∂x

≡∂
∂x
 ∂v
∂y

≡∂
∂y
 ∂v
∂x

≡∂
∂y

−∂u
∂y

= −∂2u
∂y2 .
(15.19)
So, for u to have a harmonic conjugate, it is necessary that u be harmonic on D.
The terminology makes sense! Only a harmonic function can have a harmonic conjugate.
Example 15.22
Find a harmonic conjugate for u(x, y) = 2 + x + x3 −3xy2, if it is possible to do so.
Method: Since the domain D is not specified in the problem, we should assume that it
is the whole complex plane or, perhaps, the largest set in the complex plane on which u
and its first and second partial derivatives are continuous. Later, we may have to specify
the domain D.
The given function u must be a real-valued function, as should be the desired
harmonic conjugate function v.
We want v to satisfy the Cauchy–Riemann equations; hence,
∂v
∂y = ∂u
∂x = ∂
∂x

2 + x + x3 −3xy2 
= 1 + 3x2 −3y2,
so
v =

(1 + 3x2 −3y2)∂y = y + 3x2y −y3 + g(x),
where g(x) is an arbitrary function of x alone. Substitute v into the other Cauchy–
Riemann equation to get
−6xy = ∂
∂y

2 + x + x2 −3xy2 
= ∂u
∂y = −∂v
∂x = −∂
∂x

y + 3x2y −y3 + g(x)

= −6xy −g′(x);

1154
Advanced Engineering Mathematics
hence, g′(x) ≡0. So, v(x, y) = y + 3x2y −y3 + c is a harmonic conjugate of u, for any real
constant c. ⃝
Theorem 15.14
If u = u(x, y) is harmonic on a domain D and z0 is any point in D, then u(x, y) has a
harmonic conjugate on some open ball Bδ(z0) contained in D.
Because Theorem 15.14 guarantees only that there is a harmonic conjugate near z0, we
may say that this is a local harmonic conjugate. For example, if z0 ̸= 0 is not on the negative
x-axis, then we will see in Section 15.4 that the function u(x, y) ≜1
2 ln(x2 + y2) has a local
harmonic conjugate arctan(y/x) corresponding to f(z) = u(x, y) + i v(x, y) = 1
2 ln(x2 + y2) +
arctan(y/x) ≜Log(z).
Theorem 15.37 in Section 15.8, an improved version of Theorem 15.14, relies on concepts
we will define in Section 15.8.
Example 15.23
Find a streamline function for potential flow for the velocity vector field:
v(x, y) = dx
dt ˆı + dy
dt ˆj =
	
1 +
y2 −x2

x2 + y22

ˆı −
2xy

x2 + y22 ˆj.
Method: This is potential flow for potential function (x, y) if the velocity is
v(x, y) = ∂
∂x ˆı + ∂
∂y ˆj.
As discussed in Example 15.14 in Section 15.2, the streamline function (x, y) is to be
chosen so that
f(z) = (x, y) + i(x, y)
is analytic on some domain. As we saw in the present section,  is a harmonic conjugate
of . So, by the Cauchy–Riemann equations, we want  to satisfy
∂
∂y ≡∂
∂x = 1 +
y2 −x2

x2 + y22
(15.20)
and
∂
∂x ≡−∂
∂y =
2xy

x2 + y22
(15.21)
on some domain D. From (15.21), using the substitution w = x2 + y2 and calculating
∂w = 2x ∂x, we integrate to get
 =

2xy

x2 + y22 ∂x =
 2xy
w2 · ∂w
2x = y

w−2 ∂w = −y
w + g(y)
= −
y
x2 + y2 + g(y),
(15.22)

Functions of a Complex Variable
1155
where g(y) is an arbitrary function of y alone. [We were able to pull out the “y” factor
from the integral with respect to w because the original integral was with respect to x; y
and x are independent variables.]
Substitute this into (15.20), the first Cauchy–Riemann equation, to get
1 +
y2 −x2

x2 + y22 = ∂
∂x = ∂
∂y = ∂
∂y

−
y
x2 + y2 + g(y)

= −
1
x2 + y2 +
2y2

x2 + y22 + g′(y).
At first glance, it looks hopeless to find g′(y) as a function of y alone, but patient algebraic
manipulation gives us
g′(y) = 1 +
y2 −x2

x2 + y22 −
	
−
1
x2 + y2 +
2y2

x2 + y22

= 1 +

y2 −x2

x2 + y22 −
⎛
⎝

−x2 + y2

x2 + y22
⎞
⎠= 1,
so g(y) = y + c, where c is an arbitrary real constant. For convenience, choose c = 0 to
get the streamline function:
(x, y) = −
y
x2 + y2 + y.
All of the earlier calculations made sense for x2 + y2 > 0, so we may take as the domain
the punctured plane D = {z : z ̸= 0}. ⃝
By the way, if we take instead the domain to be D = {z : |z| ≥1}, then the streamlines,
that is, the level sets (x, y) = k for real constants k, give the 2D Stokes potential fluid
flow around a cylinder.
15.3.3 Problems
1. Suppose, as usual, that z = x+iy where x and y are real. Where, if anywhere, does
f(z) ≜x −y + i(x + y) satisfy the Cauchy–Riemann equations? Find f ′(z) where it
exists. Is f an entire function?
2. Define f(z) = 1
2

z + 3
z

. Use Definition 15.4 in Section 15.2 and calculations such
as those done in Example 15.9 in Section 15.2 to find f ′(z), where it exists. Where
is f analytic?
3. Define f(z) = 1/(z −i). Use Definition 15.4 in Section 15.2 and calculations such as
those done in Example 15.9 in Section 15.2 to find f ′(z), where it exists. Where is f
analytic?

1156
Advanced Engineering Mathematics
In problems 4–6, a function u(x, y) is given. Find a harmonic conjugate.
4. u(x, y) = (−1 + 2x)y
5. u(x, y) = x −2xy
6. u(x, y) = 8x3y −8xy3 + 1
In problems 7–10, a function u(x, y) is given. Find an entire function f(z) such that
Re(f(z)) = u(x, y). Express f(z) and f ′(z) as functions of z. (Your final conclusions should
be formulas that do not involve x or y.)
7. u(x, y) = x −2xy
8. u(x, y) = 4xy3 −4x3y
9. u(x, y) = y2 + x3 −x2 −3xy2 + 2
10. u(x, y) = 3xy2 −y3 + 2xy −y
11. For the function u(x, y) = y/x2 + y2, find a domain in which it is harmonic and
find a harmonic conjugate for u in that domain.
12. Derive the Cauchy–Riemann equations in polar coordinates:
given f(z) =
f(x + iy)
=
u(x, y) + iv(x, y), define U(r, θ)
≜
u(r cos θ, r sin θ), V(r, θ)
≜
v(r cos θ, r sin θ). If f(z) is differentiable at a point, then
∂U
∂r = 1
r
∂V
∂θ
and
∂V
∂r = −1
r
∂U
∂θ
there. Why? Use the chain rule, as in Chapter 6, x = r cos θ, y = r sin θ, and the
Cauchy–Riemann equations in x, y to get
∂U
∂r = ∂u
∂x · ∂x
∂r + ∂u
∂y · ∂y
∂r = cos θ · ∂u
∂x + sin θ · ∂u
∂y
= cos θ · ∂v
∂y −sin θ · ∂v
∂x
and similar chain rule calculations of
1
r
∂V
∂θ ,
∂V
∂r ,
−1
r
∂U
∂θ .
13. Use the Cauchy–Riemann equations in polar coordinates to find a harmonic con-
jugate for U(r, θ) = v∞
	
r + a2
r

cos θ, where a and v∞are unspecified positive
constants. This function is used in a model of potential flow past a circular cylinder.
14. Use Cauchy–Riemann equations in polar coordinates to find a harmonic conjugate
for U(r, θ) = v∞
	
r + a2
r

cos θ −κθ, where a, v∞, and κ are unspecified positive
constants. This function is used in a model of potential flow with circulation κ past
a circular cylinder.
15. For the function u(x, y) =
2xy
(x2+y2)2 , use the Cauchy–Riemann equations in polar
coordinates to find a domain in which it is harmonic and to find a harmonic
conjugate for u in that domain.

Functions of a Complex Variable
1157
16. Suppose n is a positive integer. Explain why rn cos nθ has harmonic conjugate
rn sin nθ.
17. Suppose n is a positive integer. Find a harmonic conjugate for rn sin nθ.
18. Find an analytic function f(z) for which Re(f(z)) =
y
(x + 2)2 + y2 .
19. Suppose f(z) = eαz, where α is a complex constant, specifically α = a + ib, where
a, b are real:
(a) Find a formula for f(z) in terms of real exponential functions of x and y and
real sine and cosine trigonometric functions of x and y.
(b) Where is f(z) analytic?
20. Suppose g(z) = u(x, y) + iv(x, y) is analytic on a domain D and define U(x, y) =

u(x, y)
2 −

v(x, y)
2. Find an analytic function f(z) for which Re(f(z)) = U(x, y).
21. If u(x, y) has harmonic conjugate v(x, y), why does v(x, y) have harmonic conjugate
−u(x, y)?
22. Let f(z) = z3. Use this to solve the system of ODEs
⎡
⎣
˙x(t)
˙y(t)
⎤
⎦=
⎡
⎣
3(x2 −y2)
−6xy
⎤
⎦.
23. Find solution curves in the xy-plane for the system of ODEs
⎡
⎣
˙x(t)
˙y(t)
⎤
⎦=
⎡
⎣
2y
2x
⎤
⎦.
24. Suppose O is an open subset of the complex plane and z1, . . . , zn are complex
numbers in O. Explain why
{z : z is in O but z ̸= zk for k = 1, . . . , n}
is open.
15.4 Elementary Functions
All of the commonly used functions of a real variable, such as the exponential, logarithmic,
trigonometric, and power functions, have generalizations to functions of a single complex
variable. Many of their properties are the same. But there are some nonroutine aspects of
the generalization to functions of a complex variable, and a new type of function, Arg(z),
plays a crucial role.
15.4.1 Arg(z)
Given a complex number z, there are infinitely many values of the real number θ for which
z = |z|eiθ. This defines a new notation:

1158
Advanced Engineering Mathematics
arg(z) ≜{θ : z = |z|eiθ
and θ is real},
for
z ̸= 0.
(15.23)
For example, arg(i5) = { π
2 + 2πk : k is any integer} because i5 = 5eiπ/2. For any nonzero
complex number z, arg(z) is a set of infinitely many real numbers.
It will be very useful to select just one of the real numbers out of the infinite set arg(z):
Arg(z) ≜the unique θ in arg(z) that satisfies −π < θ ≤π, for z ̸= 0.
(15.24)
Example 15.24
Find the exact value of Arg
	
−
√
3 + i
1 −i

.
Method:
As in Example 15.1(d) in Section 15.1,
−
√
3 + i
1 −i
=
2ei5π/6
√
2e−iπ/4
=
√
2 ei

5π
6 −(−i π
4 )

=
√
2 ei13π/12. So,
arg(z) =
13π
12 + 2πk : k is any integer

.
Figure 15.14 shows the set arg(z) written in an explicit form.
Arg
	
−
√
3 + i
1 −i

= −11π
12 ,
using k = −1,
is the unique θ in arg(z) satisfying
−π < θ ≤π. ⃝
As illustrated in Figure 15.15,
Arg(z) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
cos−1
⎛
⎝
x

x2 + y2
⎞
⎠,
y ≥0
−cos−1
⎛
⎝
x

x2 + y2
⎞
⎠,
y < 0
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(15.25)
For example, z = −
√
3 −i = x + iy has
Arg(z) = −cos−1
	
−
√
3
(−
√
3)2 + (−1)2

= −cos−1
	
−
√
3
2

= −5π
6 .
arg(z) =
11π
12
k≤–2
k ≥2
k=–1 k = 0
k = 1
12
12
13π
37π
–
...,
...
,
,
,
.
FIGURE 15.14
Explicit set notation for arg(z).

Functions of a Complex Variable
1159
y
x
r
θ
y
x
r
θ
–θ
θ=cos–1
, y ≥0
x )
( r
θ= – cos–1
, y< 0
x )
( r
FIGURE 15.15
Arg in terms of inverse trigonometric functions.
Arg(x0– iy)≈–π
y
x
Arg (x0+ iy)≈π
FIGURE 15.16
Discontinuity of Arg.
The function Arg(z) is continuous at all z0 except those z0 that lie on the nonpositive real
axis. Why? Regarding the lack of continuity, suppose first that z0 = x0 < 0. Then for small
positive y, Arg(x0 + iy) ≈π and Arg(x0 −iy) ≈−π, as illustrated in Figure 15.16. So,
lim
y→0+(Arg(x0 + iy) = π
and
lim
y→0−(Arg(x0 + iy) = −π,
which explains why Arg(z) is discontinuous at all z of the form z = x0 with x0 < 0.
The Arg(z) function is not even defined at z = 0, so Arg(z) is not continuous at 0. So,
Arg(z) is discontinuous on the nonpositive real axis.
Formulas (15.25) and continuity of the function cos−1 explain why Arg(z) is continuous
off the positive x-axis. In Problem 15.4.6.26, you will establish the continuity of Arg(z) at
points on the positive real axis.
15.4.2 Exp(z)
Definition 15.12
Given z = x + iy, where x and y are real, we can define
exp(z) ≜ex (cos y + i sin y).
(15.26)
Another notation for this is ez.

1160
Advanced Engineering Mathematics
Theorem 15.15
(a) ez satisfies the laws of exponents, that is,
(i) ez1+z2 = ez1ez2
and
(ii) ez1−z2 = ez1/ez2
and
(iii)

ezn = enz
(15.27)
for all complex numbers z1, z2, z and integers n, and
(b) ez is an entire function, and d/dz

ez
= ez.
Why? Using trigonometric identities, you will explain (a) in Problem 15.4.6.24. For (b),
write u + iv = ez ≜ex cos y + iex sin y, that is, u(x, y) = ex cos y and v(x, y) = ex sin y. We
can verify that the Cauchy–Riemann equations are true at all (x, y):
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂u
∂x −∂v
∂y = ex cos y −ex cos y = 0
∂u
∂y + ∂v
∂x = −ex sin y + ex sin y = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
The functions ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y exist and are continuous everywhere, so the technical hypothe-
ses of Theorem 15.8 in Section 15.2 are satisfied. This establishes why ez is differentiable
everywhere and thus entire. Further, by formula (15.15) in Section 15.2,
d
dz

ez 
= ∂u
∂x + i ∂v
∂x = ex cos y + iex sin y = ez. 2
15.4.3 Log(z)
Given any z ̸= 0, the set {w : ew = z} contains infinitely many complex numbers because
cos y and sin y are periodic functions.
Example 15.25
Find the set of all exact solutions of the equation ew = 1 −i.
Method: 1−i =
√
2 ei

−π
4

= eln
√
2 ei

−π
4

= eln
√
2+i

−π
4

, where ln x is the usual Calculus
I function defined for positive real numbers x. The equation to solve is
ew = 1 −i = eln
√
2+i

−π
4

.
So, there is at least one exact solution: w = ln
√
2 + i

−π
4

. By (15.6) in Section 15.1, that
is, eiθ−i2πk = eiθ for all integers k, there are infinitely many solutions for w. The set of all
solutions is
1
2 ln 2 −i π
4 + i2πk : k is any integer

. ⃝

Functions of a Complex Variable
1161
...,
...
– i
– i
+ i
1
2 In(2)
π
1 In(2)
1
2 In(2)
9π
7π
4
4
4
2
,
,
,
.
k≤–2
k ≥2
k=–1
k= 0
k = 1
FIGURE 15.17
Example 15.25.
It may help to write such a set in a more explicit form (Figure 15.17).
Definition 15.13
We define
log(z) ≜
+
ln |z| + iArg(z) + i2πk : k is any integer
,
, for z ̸= 0.
(15.28)
The solution set in Example 15.25 was log(1 −i).
When z = 0, there is no solution w for the equation ew = 0, as you will see in Problem
15.4.6.19. So, log(0) is undefined.
When it is defined, log(z) is an infinite set of values w, so it would seem that we have
not successfully generalized the familiar concept of logarithm of a positive real numbers,
which produces a single value. In algebra and calculus of functions of a real variable, ln(x)
is truly a function for x > 0. In fact, in this book, the natural logarithm function, ln, will
allow only inputs that are positive, real numbers.
But, we have already faced a similar issue: arg(z) is an infinite set but we defined Arg(z),
which is truly a function in that it gives a single output for every nonzero input z. In fact,
this enables us to do the same for logarithms!
Definition 15.14
We define
Log(z) ≜ln |z| + iArg(z),
for
z ̸= 0.
(15.29)
So, Log(z) chooses from log(z) the unique element w that satisfies −π < Im(w) ≤π, as
you will explain in Problem 15.4.6.22. Note that
log(z) =
+
Log(z) + i2πk : k is any integer
,
.
(15.30)
Note also that if x is a real, positive number, then Log(x) = ln x, as you will explain in
Problem 15.4.6.23.

1162
Advanced Engineering Mathematics
Example 15.26
Find the exact values of Log

−
√
3 + i

and eLog

−
√
3+i

.
Method: Log

−
√
3 + i

= ln

| −
√
3 + i|

+ iArg

−
√
3 + i

= ln 2 + i 5π
6 . Because w =
Log

−
√
3 + i

is one of the infinitely many complex numbers that satisfy ew = −
√
3 + i,
it follows with no effort (other than thinking!) that eLog

−
√
3+i

= −
√
3 + i. ⃝
Theorem 15.16
eLog(z) = z for all z ̸= 0.
Theorem 15.17
d/dz

Log(z)

= 1/z for all z in D ≜{z : Re(z) > 0 or Im(z) ̸= 0}.
Note that D consists of all z in C except z on the nonpositive real axis, so we can also
notate D ≜{z : Arg(z) ̸= π}. The set D is sometimes called a “cut” plane and is illustrated
in Figure 15.18.
Why is Theorem 15.17 true? Choose any z = x + iy where x and y are real and either x > 0
or y ̸= 0. (By the way, the “or” is the “inclusive or,” that is, the case where both x > 0 and
y ̸= 0 are included.) Denote
u + iv ≜Log(z) = ln |z| + iArg(z) = ln(

x2 + y2) + iArg(z) = 1
2 ln(x2 + y2) + iArg(z).
y
x
FIGURE 15.18
Cut plane.

Functions of a Complex Variable
1163
We have
∂u
∂x = ∂
∂x
1
2 ln(x2 + y2)

=
x
x2 + y2
and
∂u
∂y =
y
x2 + y2 ,
so both ∂u
∂x and ∂u
∂y are defined and continuous everywhere except at (x, y) = (0, 0). We will
see that the tricky part of the justification is in studying the derivatives of v(x, y) ≜Arg(z).
To do that, we will use the formulas for Arg(z) in (15.25).
Case (1): If (x, y) lies in the first or second quadrant or lies on the positive y-axis, that is,
has y > 0, then v(x, y) = Arg(z) = cos−1

x
√
x2+y2

, hence, cos v =
x
√
x2+y2 . Using implicit
differentiation, we calculate that
−sin v · ∂v
∂x = ∂
∂x

cos v

= ∂
∂x
⎡
⎣
x

x2 + y2
⎤
⎦=
1

x2 + y2
−
x2

x2 + y23/2 =
y2

x2 + y23/2 .
(15.31)
Because y > 0, hence 0 < v < π,
sin v = +

1 −cos2 v =
-
1 −
x2
x2 + y2 =

y2

x2 + y2
=
|y|

x2 + y2
=
y

x2 + y2
.
(15.32)
Substituting (15.32) into (15.31), and noting that

y2 = |y| = y for y > 0, we get
∂v
∂x = −
1
sin v ·
y2

x2 + y23/2 = −
⎛
⎝
y

x2 + y2
⎞
⎠
−1
·
y2

x2 + y23/2 = −

x2 + y2
y
·
y2

x2 + y23/2
= −
y
x2 + y2 = −∂u
∂y.
This shows that the second of the Cauchy–Riemann equations is satisfied.
Similarly,
∂v
∂y = −
1
sin v · ∂
∂y

cos v

= −
1
sin v · ∂
∂y
⎡
⎣
x

x2 + y2
⎤
⎦= −
1
sin v ·
	
−
xy

x2 + y23/2

= −

x2 + y2
y
·
	
−
xy

x2 + y23/2

=
x
x2 + y2 = ∂u
∂x,
which confirms the first of the Cauchy–Riemann equations. Thus, if (x, y) has y > 0, then
the Cauchy–Riemann equations,

1164
Advanced Engineering Mathematics
∂v
∂x = · · · = −
y
x2 + y2 = −∂u
∂y
and
∂v
∂y = · · · =
x
x2 + y2 = ∂u
∂x ,
(15.33)
are satisfied. Because all of the first partial derivatives ∂u
∂x, ∂u
∂y, ∂v
∂x, ∂v
∂y are defined and con-
tinuous everywhere except at (x, y) = (0, 0), Theorem 15.8 in Section 15.2 allows us to
conclude that Log(z) is differentiable at such a point z = x + iy.
Case (2): If (x, y) lies in the third or fourth quadrant or lies on the negative y-axis, that
is, has y < 0, then the calculations concerning derivatives of cos v work just as well for
derivatives of −cos v. Using the same reasoning using Theorem 15.8 in Section 15.2 as in
Case (1), we conclude that Log(z) is differentiable at such a point z = x + iy.
Case (3): If (x, y) lies on the positive x-axis, that is, has x > 0 and y = 0, you will calculate
in Problem 15.4.6.25 that again (15.33) holds. By the same reasoning, using Theorem 15.8
in Section 15.2 as in Case (1), we conclude that Log(z) is differentiable at such a point
z = x + iy.
In all of these cases, formula (15.15) in Section 15.2 gives the formula for the derivative
of Log(z):
d
dz

Log(z)

= ∂u
∂x + i ∂v
∂x =
x
x2 + y2 + i

−
y
x2 + y2

= x −iy
x2 + y2 =
z
|z|2 = z
z z = 1
z.
This generalizes the Calculus I fact that (ln x)′ = 1/x for real numbers x > 0. 2
15.4.4 Branches of Logarithms
The function Log(z) gives the unique value w in log(z) that satisfies −π < Im(w) ≤π. For
any real number σ, we can define a different “branch” of log(z) by
Logσ(z) = w is the unique w in log(z) satisfying −π + σ < Im(w) ≤π + σ.
(15.34)
So, Log(z) = Log0(z).
The subscript σ in Logσ(z) does not denote a base for a logarithm. This is different from
the notation ℓog2(x) = t ⇔x = 2t, for real numbers x > 0.
The usefulness of defining such branches Logσ(z) is that
d
dz

Logσ(z)

= 1
z, for all z ̸= 0 for which σ + π is not in arg(z).
For example, Log−π
4 (z) has a cut on the ray θ = 3π
4 , which is illustrated in Figure 15.19.
Log−π
4 (z) is differentiable on D ≜{z : Arg(z) ̸= 3π
4 }, which is the rotation by angle −π
4 of
the cut plane for Log0(z).
15.4.5 Power Functions
If n is an integer, f(z) ≜zn is easy to define and calculate. For any positive integer n, the
binomial theorem gives
zn = (x + iy)n =
n
.
k=0
n
k

xn−k(iy)k,

Functions of a Complex Variable
1165
y
x
FIGURE 15.19
Another cut plane.
and for any negative integer m, we can define zm ≜
1
z−m . We also want to have a definition
for zα.
First, let’s recall from (15.30) that log(z) is an infinite set. If α is a complex number, we
can define
α log(z) ≜
+
α · (Log(z) + i2πk) : k is any integer
,
,
and, similarly,
eαlog(z) ≜
/
eα·(Log(z)+i2πk) : k is any integer
0
.
This set could be an infinite set, meaning that it could contain infinitely many distinct
numbers.
If α is any complex number, we would want to define zα in such a way that zα = elog(zα).
This suggests defining the power function by
zα ≜eα log(z),
(15.35)
which could be an infinite set.
The principal branch, or principal value, of zα is defined to be
eα Log(z)
and will also be denoted by z\α\. The latter is not a standard notation, but we hope it will
become popular.
If α = 1
n, we will also denote the principal value of z1/n by
n√
z ≜e
1
n Log(z) = z\ 1
n \.

1166
Advanced Engineering Mathematics
Example 15.27
For each of the numbers z in the following, evaluate z\(1+i)\ and z1+i and express the
number or set of numbers in the form a + ib:
(a) z = −3,
(b) z = i3,
(c) z = −
√
3 + i.
Method: Note that
zα = eα log(z) =
/
eα (Log(z)+i2πk) : k is any integer
0
=
/
z\α\ · eα(i2πk) : k is any integer
0
.
(15.36)
(a) Log(−3) = ln | −3| + iArg(−3) = ln 3 + iπ, so
(−3)\(1+i)\ = e(1+i)Log(−3) = e(1+i)(ln 3+iπ) = eln 3 e−π ei ln 3 eiπ = 3 · e−π · ei ln 3 · (−1)
= −3 e−π · ei ln 3 = −3 e−π ·

cos(ln 3) + i sin(ln 3)

= −3 e−π cos(ln 3) −i3 e−π sin(ln 3).
Using (15.36) and the earlier calculation of (−3)\(1+i)\, we have
(−3)1+i =
/
(−3)\(1+i)\ · e(1+i)i2πk : k is any integer
0
=
/
(−3)\(1+i)\ · ei2πk · e−2πk : k is any integer
0
=
/
−3 e−π · ei ln 3 · 1 · e−2πk : k is any integer
0
=
/
−3 e−(1+2k)πei ln 3 : k is any integer
0
=
/
−3 e−(1+2k)π cos(ln 3) −i3 e−(1+2k)π sin(ln 3) : k is any integer
0
.
(b) Log(i3) = ln |i3| + iArg(i3) = ln 3 + i π
2 , so
(i3)\(1+i)\ = e(1+i)Log(i3) = e(1+i)(ln 3+i π
2 ) = eln 3 e−π
2 ei ln 3 eiπ/2 = 3 · e−π
2 · ei ln 3 · (i)
= 3ie−π
2 · ei ln 3 = −3e−π
2 sin(ln 3) + i3e−π
2 cos(ln 3).
Using (15.36) and the earlier calculation of (i3)\(1+i)\, we have
(i3)1+i =
/
(i3)\(1+i)\ · e(1+i)i2πk : k is any integer
0
=
/
(i3)\(1+i)\ · ei2πk · e−2πk : k is any integer
0
=
/
i3 e−π
2 · ei ln 3 · e−2πk : k is any integer
0
=
/
i3 e−( 1
2 +2k)πei ln 3 : k is any integer
0
=
/
−3 e−( 1
2 +2k)π sin(ln 3) + i3 e−( 1
2 +2k)π cos(ln 3) : k is any integer
0
.

Functions of a Complex Variable
1167
(c) Log(−
√
3 + i) = ln | −
√
3 + i| + iArg(−
√
3 + i) = ln 2 + i 5π
6 , so
(−
√
3 + i)\(1+i)\ = e(1+i)Log(−
√
3+i) = e(1+i)(ln 2+i 5π
6 ) = eln 2 e−5π/6 ei ln 2 ei5π/6
= 2 · e−5π/6 · ei ln 2 ·
1
2 · (−
√
3 + i)

= e−5π/6 · ei ln 2 · (−
√
3 + i)
= e−5π/6 ·

cos(ln 2) + i sin(ln 2)

· (−
√
3 + i)
= −e−5π/6 √
3 cos(ln 2) + sin(ln 2)

+ ie−5π/6 
−
√
3 sin(ln 2) + cos(ln 2)

.
Using (15.36) and the earlier calculation of (−
√
3 + i)\(1+i)\, we have
(−
√
3 + i)1+i =
/
(−
√
3 + i)\(1+i)\ · e(1+i)i2πk : any integer k
0
=
/
(−
√
3 + i)\(1+i)\ · ei2πk · e−2πk : any integer k
0
=
/
e−5π/6 · ei ln 2 · (−
√
3 + i) · 1 · e−2πk : k is any integer
0
=
/
e−( 5
6 +2k)π · ei ln 2 · (−
√
3 + i) : k is any integer
0
=

−e−( 5
6 +2k)π √
3 cos(ln 2) + sin(ln 2)

+ ie−( 5
6 +2k)π 
−
√
3 sin(ln 2) + cos(ln 2)

: any integer k

. ⃝
Example 15.28
For each of the numbers z in the following, evaluate (i) √z, (ii) z1/2, and (iii) z2/3 in
“rectangular” form a + ib or in polar exponential form:
(a) z = −5,
(b) z = −
√
3 + i.
Method: Recall that polar exponential form is reiθ with −π < θ ≤π.
(a) Similar to Example 15.27(a), Log(−5) = ln 5 + iπ, so
(i)
√
−5 = (−5)\ 1
2 \ ≜e
1
2 Log(−5) = e
1
2 (ln 5+i π) = e
1
2 ln 5 · eiπ/2 = eln
√
5 · (i) = i
√
5
in rectangular form. This agrees with what we learned in a precalculus course.
On the other hand, using (15.36),
(−5)1/2 = e
1
2 log(−5) =
/
e
1
2 Log(−5)e
1
2 (i2πk) : k is any integer
0
=
/
i
√
5 · eiπk : k is any integer
0
=
/
i(−1)k√
5 : k is any integer
0
.
At first glance, it seems as if (−5)1/2 is an infinite set. But since (−1)k = ±1, we see
that in “rectangular” form,
(ii)
(−5)1/2 = {±i
√
5} = {i
√
5, −i
√
5}
is a finite set. We also see that, as in Example 15.3(a) in Section 15.1, (−5)1/2 is the set
of all solutions, that is, both solutions, of the equation z2 = −5.

1168
Advanced Engineering Mathematics
Further,
(−5)2/3 = e
2
3 log(−5) =
/
e
2
3 (Log(−5)+i2πk) : k is any integer
0
=
/
e
2
3 (ln 5+i(π+2πk)) : k is any integer
0
=
/
3√
25 · ei2π/3 · ei4πk/3 : k is any integer
0
=
/
3√
25 ei2π/3,
3√
25 ei2π,
3√
25 ei10π/30
.
In polar exponential form,
(iii)
(−5)2/3 =
/
3√
25 ei2π/3,
3√
25,
3√
25e−i2π/30
.
(b) Log(−
√
3 + i) = ln 2 + i 5π
6 , so

−
√
3 + i = (−
√
3 + i)\ 1
2 \ ≜e
1
2 Log(−
√
3+i) = e
1
2 (ln 2+i 5π
6 ) = e
1
2 ln 2 · ei5π/12
= eln
√
2 · ei5π/12.
In polar exponential form,
(i)

−
√
3 + i =
√
2 ei5π/12.
On the other hand, using (15.36),
(−
√
3 + i)1/2 =
/
e
1
2 Log(−
√
3+i)e
1
2 (i2πk) : k is any integer
0
=
/√
2 ei5π/12 · eiπk : k is any integer
0
.
Because eiπk = (−1)k, in polar exponential form,
(ii)
(−
√
3 + i)1/2 =
/√
2ei5π/12,
√
2e−i7π/120
,
which is a finite set. We also see that, as in Example 15.3(a) in Section 15.1,
(−
√
3 + i)1/2 is the set of all solutions, that is, both solutions, of the equation
z2 = −
√
3 + i.
Finally,
(−
√
3 + i)2/3 =
/
e
2
3 Log(−
√
3+i)e
2
3 (i2πk) : k is any integer
0
=
/ 3√
4 · ei5π/9 · ei4πk/3 : k is any integer
0
=
/ 3√
4 ei5π/9,
3√
25 ei17π/9,
3√
4 ei29π/90
.
In polar exponential form,
(iii)
(−
√
3 + i)2/3 =
/ 3√
4 ei5π/9,
3√
4 e−iπ/9,
3√
4 e−i7π/90
. ⃝
Example 15.29
Where is √z differentiable, and what is its derivative there?
Method: By the chain rule,
√
z ≜e
1
2 Log(z)

Functions of a Complex Variable
1169
is differentiable everywhere Log(z) is differentiable, that is, for all z in D ≜{z :
Arg(z) ̸= π} = {z = x + iy : x > 0 or y ̸= 0}. At those z,
d
dz
1√z
2
= d
dz
1
e
1
2 Log(z)2
= e
1
2 Log(z) · d
dz
1 1
2 Log(z)
2
= e
1
2 Log(z) · 1
2 · 1
z = √z · 1
2z =
1
2√z.
This generalizes the Calculus I fact that (√x)′ =
1
2√x for real numbers x > 0. ⃝
This tells us everything about differentiability of √z for z in D. But what about z not in
D? We will explain why √z is not differentiable on the nonpositive real axis.
First, recall that in this book the natural logarithm function, ln, will allow only inputs
that are positive, real numbers, such as |z|1/2 when z ̸= 0.
Define
f(z) =
√
z = · · · = e
1
2 ln |z| ei Arg(z)/2 = eln(|z|1/2) ei Arg(z)/2 = |z|1/2 ei Arg(z)/2.
We recall that Arg(z) is not continuous at z = x + i0 for x ≤0. In fact, Arg(0) is undefined,
so f(z) is not even defined at z = 0. Further, if z0 = x0 + i0 and x0 < 0, then the facts that
lim
y→0+ Arg(x0 + iy) = π
and
lim
y→0−Arg(x0 + iy) = −π
imply that
lim
y→0+ f(x0 + iy) = ln(|x0|1/2)ei π
2 = i

|x0| and
lim
y→0−f(x0 + iy) = ln(|x0|1/2)e−i π
2 = −i

|x0|,
so f(z) is not continuous at z0 = x0 + i0 and thus cannot be differentiable there. ⃝
We recall in passing that we know that we can define for the logarithm differ-
ent branches Logσ (z). Using them, we can define a function fσ(z) ≜e
1
2 Logσ (z) that is
differentiable except on the ray Arg(z) = σ + π.
15.4.6 Problems
1. Find a complex number z for which |z −i| = 2 and Arg(z) = 3π
4 .
2. Find a complex number z for which |z −i| = 3 and Arg(z) = 5π
6 .
3. For each of the numbers z in the following, find arg(z) and Arg(z):
(a) −1 + i,
(b) −
√
3 −i,
(c) 2ei 5π
3 ,
(d) −
√
3 −i
−1 + i .
4. Find the Arg of the complex numbers:
(a) −2
√
2 −i2
√
2,
(b) (1 + i)(−
√
3 −i),
(c)
1+i
√
3
−1+i .

1170
Advanced Engineering Mathematics
5. For each of (a) and (b), decide whether the given statement is true or false. If it’s
true, explain why; if it’s false, give a specific, explicit counterexample:
(a) Log(z2) = 2 Log(z) for all z ̸= 0.
(b) Log(√z) = 1
2 Log(z), where √z = z\1/2\, that is, the principal value of z1/2, for
all z ̸= 0.
6. For which value(s) of z is Log
1
z

= −Log(z)?
7. For which value(s) of z is (a) Log

ez
= z, and (b) eLog(z) = z?
8. Solve the equation
e(1−z
2) = −1 + i.
9. Solve the equation
ez
ez + 1 = i.
10. We know that Log(z1z2) may not equal Log(z1) + Log(z2). Explain why
Log(z1z2) + i2kπ = Log(z1) + Log(z2),
where k must be either −1, 0, or 1.
11. Define the principal square root function by √z, that is, z\1/2\ ≜√|z| ei Arg(z)/2.
Find a number z and an integer n for which Arg

zn1/2
̸= n Arg

z1/2
.
12. Define a function
f(z) ≜|z|α2ei(3α−2)Arg(z).
Find all real values of the constant α for which f(z) satisfies the Cauchy–Riemann
equations for z ̸= 0. For each such value of α, write down the corresponding
functions f ′(z).
13. Evaluate (2i)i.
14. Find the exact value(s) of (−1 −i)1/3 and express it or them in polar exponential
form.
15. Evaluate
3
−
√
3 + i and express your final conclusion in polar exponential form.
16. Evaluate (−
√
3+i)1/3 and express your final conclusion in polar exponential form.
17. Where is
3√z differentiable?
18. Where is
3√
z2 differentiable?
19. Why is there no solution of the equation ew = 0?
20. Define f(z) = Arg(z) and g(z) =

f(z)
2. Where is g(z) differentiable? At such z,
find a formula for g′(z).
21. Why does the power function defined in (15.35) give only a single value when the
exponent α is an integer?
22. Why does Log(z) choose from log(z) the unique element w satisfying −π
<
Im(w) ≤π?
23. If x is a real, positive number, explain why Log(x) = ln x.

Functions of a Complex Variable
1171
24. Using trigonometric identities, explain why ez satisfies the laws of exponents,
(15.27), that is, (i) ez1+z2 = ez1ez2, (ii) ez1−z2 = ez1/ez2, and (iii)

ezn = enz for all
complex numbers z1, z2, z and integers n.
25. Establish (15.33) in Case (3) of the explanation of Theorem 15.17: that is, if (x, y)
lies on the positive x-axis, that is, has x > 0 and y = 0, explain why ∂v
∂x = · · · =
−
y
x2+y2 = −∂u
∂y and ∂v
∂y = · · · =
x
x2+y2 = ∂u
∂x. [Caution:

y2 = |y|.]
26. Explain why Arg(z) is continuous on the positive real axis.
15.5 Trigonometric Functions
Based on Euler’s formula for eiθ and our previous experience with the sine and cosine
functions of a real variable as solutions of ODEs, for all complex numbers z, we define
cos z ≜eiz + e−iz
2
and
sin z ≜eiz + e−iz
i2
.
(15.37)
Theorem 15.18
For all complex numbers z1 and z2,
cos(z1 + z2) = cos z1 cos z2 −sin z1 sin z2
(15.38)
and
sin(z1 + z2) = sin z1 cos z2 + cos z1 sin z2 .
(15.39)
Why? In justifying an identity, it is usually easier to start on the more complicated side and
use facts and/or algebraic manipulations to get to the other side. For example, starting on
the right-hand side of (15.38), we have
cos z1 cos z2 −sin z1 sin z2 = eiz1 + e−iz1
2
· eiz2 + e−iz2
2
−eiz1 −e−iz1
i2
· eiz2 −e−iz2
i2
= 1
4

ei(z1+z2) +
ei(−z1+z2) +
ei(z1−z2) + e−i(z1+z2)
+ 1
4

ei(z1+z2) −
ei(−z1+z2) −
ei(z1−z2) + e−i(z1+z2)
= 1
4 · 2 ·

ei(z1+z2) + e−i(z1+z2)
= ei(z1+z2) + e−i(z1+z2)
2
≜cos(z1 + z2),
thus establishing (15.38). You will establish (15.39) in Problem 15.5.1.13 using similar
work. 2

1172
Advanced Engineering Mathematics
Example 15.30
Find the exact value of sin(−
√
3 + i).
Method: It’s actually easier to not use the polar exponential form of −
√
3 + i, and it’s
easier to use (15.39) rather than the definition of sin(−
√
3 + i) given by (15.37).
First, it helps to get two more formulas: for any real number y, using (15.37), we have
cos(iy) = ei(iy) + e−i(iy)
2
= e−y + ey
2
≜cosh y
(15.40)
and
sin(iy) = ei(iy) −e−i(iy)
i2
= e−y −ey
i2
= −i · e−y −ey
2
= i · ey −e−y
2
≜i sinh y .
(15.41)
So,
sin(−
√
3 + i) = sin(−
√
3) cos(i) + cos(−
√
3) sin(i)
= −sin(
√
3) cosh(1) + i cos(
√
3) sinh(1). ⃝
Example 15.31
Solve the equations (a) 2 sin z = −1, and (b) cos z = i3.
Method: (a) For z = x + iy, where x, y are real, the equation is
−1
2 + i0 = sin(x + iy) = sin x cos(iy) + cos x sin(iy) = sin x cosh y + i cos x sinh y.
Separating the real and imaginary parts gives the system of equations
⎧
⎨
⎩
(1)
−1
2 = sin x cosh y
(2)
0 = cos x sinh y
⎫
⎬
⎭.
Equation (2) is easier to solve than the first because 0 is on the left-hand side: 0 =
cos x sinh y
⇐⇒
(i) 0 = cos x or (ii) 0 = sinh y
⇐⇒
(i) x = (n −1
2)π for some
integer n or (ii) y = 0.
Substitute each of these possibilities (i), (ii) into the equation (1): First, using (i), hence
x = (n −1
2)π,
(1)(i)
−1
2 = sin x cosh y = sin

n −1
2

π

cosh y = (−1)n+1 cosh y
and the fact that cosh(y) ≥1 for all real y shows that there is no solution of (1)(i) for y.
So far, there is no solution.
Second, using (ii), hence y = 0,
(1)(ii)
−1
2 = sin x cosh y = sin x cosh 0 = sin x · 1 = sin x,
which gives x =

−π
2 ± π
3

+ 2kπ for any integer k.
So, the set of solutions is
/
−π
2 ± π
3 + 2kπ + i0 : k is any integer
0
.

Functions of a Complex Variable
1173
These solutions are exactly the ones we would find in a trigonometry course.
(b) For z = x + iy, where x, y are real, the equation is
i3 = cos(x + iy) = cos x cos(iy) −sin x sin(iy) = cos x cosh y −i sin x sinh y.
Separating the real and imaginary parts gives the system of equation
⎧
⎨
⎩
(1)
0 =
cos x cosh y
(2)
3 =
−sin x sinh y
⎫
⎬
⎭.
Equation (1) is easier to solve than (2): 0 = cos x cosh y
⇐⇒(i) 0 = cos x or (ii) 0 =
cosh y. But (ii) is impossible to satisfy because cosh y ≥1 for all y. The solutions of (1)
have x = (n −1
2)π.
Substitute x = (n −1
2)π into the second equation (2):
3 = −sin x sinh y = −sin

n −1
2

π

sinh y = (−1)n sinh y.
For n = even = 2k, (2) becomes sinh y = 3, whose solution would be y = arcsinh(3) if
we had defined such a function. Instead, there is a solution technique of independent
interest:
3 = sinh y ≜ey −e−y
2
⇐⇒6 = ey −e−y ⇐⇒6ey = ey · (ey −e−y) = (ey)2 −1,
so substituting w = ey turns that equation into 6w = w2 −1, that is, w2 −6w −1 = 0,
whose solutions are
ey = w = 6 ±
√
40
2
= 3 ±
√
10.
This gives us only y = ln

3 +
√
10

, because ey > 0 for any real y, hence, ey cannot equal
3 −
√
10 < 0.
So far, our only solutions are
z = x + iy =

2k −1
2

π + i ln(3 +
√
10).
For n = odd = 2ℓ−1, (2) becomes sinh y = −3, whose solutions are found using
−3 = sinh y ≜ey −e−y
2
⇐⇒−6 = ey −e−y ⇐⇒−6ey = ey · (ey −e−y) = (ey)2 −1,
so substituting w = ey turns that equation into −6w = w2 −1, that is, w2 + 6w −1 = 0,
whose solutions are
ey = w = −6 ±
√
40
2
= −3 ±
√
10.
Again, this gives us only y = ln

−3 +
√
10

.

1174
Advanced Engineering Mathematics
Putting everything together, the set of solutions is

2k −1
2

π + i ln(3 +
√
10) : any integer k

∪

2ℓ−1 −1
2

π + i ln(−3 +
√
10) : any integer ℓ

. ⃝
The ∪symbol, that is, set union, means that the solutions consist of all z that are either in
the first set or in the second set.
Notice that we found all of the exact solutions.
Example 15.31(a) shows that solving a trigonometric equation for the complex vari-
able z may turn out to only give real solutions that we are familiar with from solving a
trigonometric equation for a real variable.
But, as in Example 15.31(b), solving a trigonometric equation for a complex variable z
may give infinitely many complex solutions even though the corresponding real trigono-
metric equation has no solution, such as sin x = 2, as you will see in Problem 15.5.1.2 in the
following.
15.5.1 Problems
1. Express sin

π+i4
4

in the form a + ib.
2. Find the solutions of sin z = 2. Note that sin x = 2, the corresponding trigonomet-
ric equation for real variable x, has no solution!
3. Find the solutions of cos z = 2. Note that cos x = 2, the corresponding trigonomet-
ric equation for real variable x, has no solution!
In problems 4–11, solve the equation, that is, find all solutions of the equation.
4. 2 cos z = −1,
5. sin z = i3,
6. cos z = −cosh π.
7. sin z = −i sinh π
2 .
8. sin z = −i sinh 3.
9. cos z = i sinh 3.
10. cos z = −cosh π.
11. sin z = −cos z.
12. For complex numbers z = x + iy, where x, y are real, define
cosh(z) ≜ez + e−z
2
.
(a) Explain why cosh(z) = cosh x cos y + i sinh x sin y.
(b) Find all solutions of cosh z = i sinh z.
(c) Solve cosh z = −e.
13. Establish (15.39).

Functions of a Complex Variable
1175
14. Suppose b is a real number and |b| ≤1. Explain why the equation sin z = b has
only real solutions for z.
15. Suppose b is a real number and |b| ≤1. Explain why the equation cos z = b has
only real solutions for z.
16. Suppose b is a real number and |b| ≤1. Explain why the equation sinh z = b
has only imaginary solutions for z.
17. Suppose b is a real number and |b| ≤1. Explain why the equation cosh z = b has
only imaginary solutions for z.
15.6 Taylor and Laurent Series
Similar to the definition in calculus of real variables,
∞
.
j=0
aj (z −z0)j
(15.42)
is a power series about z0. The aj’s can be complex constants. As in calculus, convergence
of such an infinite series at particular values of z is an issue. Also, similar to calculus, study
of series begins with study of series of complex constants 3∞
j=0 cj.
Some very useful background information is from precalculus: if r is a real number, then
the sum of a geometric series is given by
∞
.
j=0
rj = 1 + r + r2 + r3 + · · · =
1
1 −r, as long as |r| < 1 .
(15.43)
Definition 15.15
(a) A series of complex numbers 3∞
j=0 cj converges absolutely if the corresponding
real series 3∞
j=0 |cj| converges.
(b) A power series (15.42) converges absolutely at a specific z1 if the complex series
3∞
j=0 cj converges absolutely, where cj ≜aj (z1 −z0)j.
(c) A series 3∞
j=0 cj diverges if the sequence of partial sums
+
sN
,∞
N=0 does not
converge (to a finite number), where sN ≜3N
j=0 cj.
(d) A power series (15.42) diverges at z1 if the complex series 3∞
j=0 cj diverges, where
cj ≜aj(z1 −z0)j.
Example 15.32
For which value(s) of the complex constant α is the geometric series 3∞
j=0 αj absolutely
convergent?

1176
Advanced Engineering Mathematics
Method: Define cj = αj. We calculate that |cj| = |αj| = |α|j, so we see that 3∞
j=0 cj is a
geometric series that converges absolutely if, and only if, |α| < 1. So our original series,
3∞
j=0 αj, converges absolutely if, and only if, |α| < 1. ⃝
Example 15.33
Does the series 3∞
j=0 e(−0.1+i)j converge absolutely?
Method: Define cj = e(−0.1+i)j =

e−0.1j eij = αj, where α = e−0.1 ei, and note that |α| =
|e−0.1|·|ei| = e−0.1·1 < 1. So, by Example 15.32, 3∞
j=0 e(−0.1+i)j is an absolutely convergent
geometric series. ⃝
Theorem 15.19
Suppose 3∞
j=0 cj and 3∞
j=0 bj are absolutely convergent series and 3∞
j=0 hj is not absolutely
convergent. Then for all complex constants β and γ ,
(a) 3∞
j=0(βcj + γ bj) is absolutely convergent.
(b) 3∞
j=0(βcj + γ hj) is not absolutely convergent unless γ = 0.
Why? (a) follows from the triangle inequality for complex numbers because |βcj + γ bj| ≤
|βcj| + |γ bj| = |β| |cj| + |γ | |bj|. (b) Assume contrarily that γ ̸= 0 and 3∞
j=0(βcj + γ hj) is
absolutely convergent. Then the result of part (a) would imply that
∞
.
j=0
hj =
∞
.
j=0
1
γ

(βcj + γ hj) + (−βcj)

is absolutely convergent, which would imply that 3∞
j=0 hj is absolutely convergent, giving
a contradiction. 2
Theorem 15.20
Suppose there exists
α ≜lim
j→∞

aj+1
aj
 .
(15.44)
(a) If 0 < α < ∞, then the power series (15.42), that is, 3∞
j=0 aj (z −z0)j, converges
absolutely for |z −z0| < R ≜1
α and diverges for |z −z0| > R.
(b) If α = 0, then the power series (15.42) converges absolutely at all z.
Why? (a) The absolute convergence follows from the absolute convergence of geomet-
ric series along with an (unstated) “comparison theorem” based on a similar result from

Functions of a Complex Variable
1177
calculus of real variables. Similar reasoning explains divergence in part (a) and absolute
convergence in part (b). 2
We note that it is possible for a power series (15.42) to converge only at z0.
Definition 15.16
(a) A finite number R is called the radius of convergence for (15.42) if the power series
both (a) converges absolutely for |z −z0| < R and (b) diverges for |z −z0| > R.
(b) The radius of convergence is defined to be R = ∞, and we say the radius of
convergence is infinite, if the power series (15.42) converges absolutely at all z.
(c) The radius of convergence is defined to be R = 0 if the power series (15.42)
converges only at z0.
We need one further concept of convergence
Definition 15.17
Let z0 be a complex number and r > 0. A power series (15.42) converges uniformly on a
closed disk Dr(z0) ≜{z : |z −z0 ≤r} if for all ε > 0, there exists an integer N, possibly
dependent on ε, such that
for all n ≥N and for all z in Dr(z0),

∞
.
j=n+1
aj (z −z0)j < ε .
(15.45)
For uniform convergence, N is not allowed to depend on z in Dr(z0).
Intuitively, 3∞
j=N+1 aj (z −z0)j is a “tail” of the series and we hope that the tail is unim-
portant. One way to remember this is that “the tail does not wag the dog.” In applications
of mathematics to engineering, usually computer software will truncate a series at a finite
number of terms; the reliability of the truncation process may depend on a mathematical
theorem that guarantees uniform convergence.
Theorem 15.21
Suppose a power series (15.42) has a positive radius of convergence, R, possibly infinite.
Then
(a) The power series converges uniformly in every closed disk of finite radius that is
contained in the open disk DR(z0) = {z : |z −z0| < R}.
(b) The power series defines a function

1178
Advanced Engineering Mathematics
f(z) ≜
∞
.
j=0
aj (z −z0)j
(15.46)
that is analytic on the open disk DR(z0).
Example 15.34
Find the radius of convergence of the series 3∞
j=0

−z
2
j, and find a simple formula for
an analytic function, which equals the sum of the series.
Method: The power series is 3∞
j=0

−1
2
j
zj, so aj =

−1
2
j
, and we calculate
lim
j→∞

aj+1
aj
 = lim
j→∞


−1
2
j+1

−1
2
j

= lim
j→∞
−1
2
 = 1
2 = α,
so R = 1
α = 2 is the radius of convergence.
Alternatively, we note that 3∞
j=0

−z
2
j is a geometric series that converges for
−z
2
 =
|z|
|2| = |z|
2 < 1, that is, for |z| < 2, and diverges for
−z
2
 > 1, that is, for |z| > 2.
The conclusions about the analyticity of f(z) ≜3∞
j=0

−z
2
j follow from Theorem 15.21.
The formula for f(z) follows from the fact that the sum of a geometric series 3∞
j=0 rj is
1
1−r, as long as |r| < 1. Here, r = −z
2 has |r| < 1 as long as |z −0| < 2, in which case
f(z) =
1
1 −

−z
2
 =
1
1 + z
2
=
1
1 + z
2
· 2
2 =
2

1 + z
2

· 2 =
2
2 + z. ⃝
The fact that the function f(z) =
2
z+2 is undefined at z = −2 reassures us that the radius
of convergence being R = 2 makes sense. On the other hand, while the function f(z) is
defined at z = 2, that point is not within the radius of convergence, because the sequence
of partial sums for
∞
.
j=0

−2
2
j
=
∞
.
j=0
(−1)j
is {sN}∞
j=0 = 1, 0, 1, 0, . . ., which does not converge due to oscillation.
15.6.1 Taylor Series
Earlier, we explained how a power series can define an analytic function. The reverse
process also works! From a function analytic on an open disk, we can get a power series
that equals the function on that disk. The process is familiar to us from calculus of a real
variable.
If f(z) is analytic on an open disk about z0, Theorem 15.40 in Section 15.9, a consequence
of Cauchy’s integral formula, will guarantee that all of its derivatives f ′, f ′′, f (3), . . . , f (n), . . .

Functions of a Complex Variable
1179
are defined, continuous, and differentiable on that open disk about z0. So, we can calculate
the numbers
aj = 1
j! f (j)(z0),
for
j = 0, 1, 2, . . . .
(15.47)
Theorem 15.22
(Taylor series) Suppose f(z) is analytic on an open disk D centered at z0. Then at all z in D,
f(z) =
∞
.
j=0
aj (z −z0)j =
∞
.
j=0
1
j! f (j)(z0) (z −z0)j.
(15.48)
Also, the convergence in (15.48) is absolute inside the open disk D and uniform on any
finite closed disk contained in D.
Example 15.35
Find the Taylor series centered at z0 = 0 for the function f(z) ≜e3z.
Method: We calculate f ′(z) = 3e3z, f ′′(z) = 9e3z, . . . , f (j) = 3je3z, so aj = f (j)(0) = 3je3·0 =
3j. Let z0 = 0. Theorem 15.22 on Taylor series implies that
f(z) =
∞
.
j=0
1
j! f (j)(0) (z −0)j =
∞
.
j=0
1
j! 3j zj =
∞
.
j=0
(3z)j
j!
.
(15.49)
As for convergence of the series in (15.48), f(z) = e3z is analytic on the whole complex
plane C, so Theorem 15.22 guarantees that the convergence is absolute everywhere and
uniform on any finite closed disk. ⃝
The Taylor series for e3z agrees with the Taylor series for e3x with real variable x replaced
by complex variable z. This is not a coincidence!
In general, we define a function g(x) of the real variable x to be real analytic on an
interval a < x < b if for some function f(z) we have that g(x) = f(x + i0) and f(z) is
analytic on some set in the complex plane that is open and contains the points z = x + i0
for a < x < b.
15.6.2 Laurent Series
There can be more than one way to expand the same function into a power series, if we
include negative powers.
Example 15.36
Expand
1
1 −z into (a) a power series that converges inside an open disk, and (b) a series
in nonpositive powers of z that converges outside an open disk.

1180
Advanced Engineering Mathematics
Method:
(a) We could use the Taylor series found in (15.48) but it’s easier to use geometric series
and possibly manipulations of geometric series: first, we have
f(z) ≜
1
1 −z =
∞
.
j=0
zj,
converges for |z| < 1
(15.50)
just by substituting r = z into the formula for geometric series, (15.43). The power
series for f(z) is defined and analytic on the set {z : |z| < 1}, that is, inside an open
disk.
(b) Instead of writing
1
1 −z in powers of z, we can write
g(z) ≜
1
1 −z =
1
−z

1 −1
z
 = 1
−z ·
1
1 −1
z
= −1
z ·
	
1 + 1
z +
1
z
2
+
1
z
3
+ · · ·

= −1
z −
1
z
2
−
1
z
3
−
1
z
4
−· · · ,
where we substituted r = 1/z into the geometric series formula (15.43). The
geometric series for
1
1 −1
z
converges for 1 > |r| =

1
z
 = 1
|z|, that is, for |z| > 1. So,
g(z) ≜
1
1 −z = −
∞
.
j=1
z−j
converges for |z| > 1 .
(15.51)
The series in nonpositive powers of z for g(z) is defined and analytic on the set {z :
|z| > 1}, that is, outside an open disk. ⃝
It is true that we were being a little fussy by giving different names to the functions f
and g. While both of them give a formula for
1
1 −z, the two functions have different
domains, so giving them different names helps to separate them in our minds.
Neither the series for f(z) given in (15.50), nor the series for g(z) given in (15.51), were
asked to converge at z = 1. This is good because
1
1 −z is undefined at z = 1. As for other
points z on the circle |z| = 1, such as z = −1, neither series converges there even though
the original, given function,
1
1 −z is defined at all z ̸= 1.
Partial differential equations in Sections 11.5 and 11.6, specifically Problem 11.5.3.2 and
Example 11.22 in Section 11.6, modeled physical problems that take place outside of a disk
or a ball.
Definition 15.18
An open annulus is a set of the form
Ar,R(z0) ≜{z : r < |z −z0| < R},
(15.52)
where R > r ≥0; R = ∞is allowed.

Functions of a Complex Variable
1181
When r = 0, the “annulus” A0,R(z0) is actually a punctured disk, and when R = ∞, the
“annulus” A0,∞(z0) is actually the punctured plane C\{z0} ≜{z:0 < |z −z0| < ∞}.
Definition 15.19
A Laurent series is an expansion of the form
f(z) ≜
∞
.
j=1
a−j (z −z0)−j +
∞
.
j=0
aj (z −z0)j .
(15.53)
Theorem 15.23
(Laurent series) If a Laurent series (15.53) converges absolutely for all z in an open annulus
Ar,R(z0) (or in some open disk DR(z0)), then the series converges uniformly in any closed
annulus contained in Ar,R(z0), that is, in any set
{z : r ≤|z −z0| ≤R}, where r <r < R < R
(or, respectively, in any closed disk {z : |z −z0| ≤R}, where 0 < R < R).
Example 15.37
Find a Laurent series for
1
3 −z in each of several significantly different open sets that are
annuli or disks.
Method: A trivial way to get a Laurent series in several different open sets that are annuli
or disks would be to get a Laurent series in one such set and then use the same Laurent
series in some subsets of the first such set.
Instead, we will get sets that are significantly different from each other because this
will help later in this section and chapter.
First,
1
3 −z =
1
3

1 −z
3
 = 1
3 ·
1
1 −z
3
= 1
3 ·
∞
.
j=0
 z
3
j
=
∞
.
j=0
3−( j+1)zj, for |z| < 3,
because
 z
3
 < 1 ⇐⇒|z| < 3.
Second,
1
3 −z =
1
−z

1 −3
z
 = −1
z ·
1
1 −3
z
= −1
z ·
∞
.
j=0
3
z
j
=
∞
.
j=1
−(3 j−1)z−j, for |z| > 3,
because
 3
z
 < 1 ⇐⇒|z| > 3.
Third, we can get a Laurent series in an annulus centered at a z0 ̸= 0, for example,
1
3 −z =
1
1 −(z −2) =
∞
.
j=0
(z −2)j , for |z −2| < 1.

1182
Advanced Engineering Mathematics
Each of the three corresponding sets, D3(0) = {z : |z| < 3}, A3,∞(0) = {z : |z| > 3},
and D1(2) = {z : |z −2| < 1}, is an open annulus or open disk. ⃝
Example 15.38
Find Laurent series for f(z) =
1
z + 1 −
2
z −2 in each of several significantly different
open sets that are annuli or disks.
Method: We were not told what z0 should be, so we might as well try to use z0 = 0,
for convenience. Each of the two terms in f(z) can be written in many choices of Laurent
series, as we saw in Example 15.37. But when we add the two terms together to get f(z),
we need both of the two Laurent series to converge in the same disk or annulus. To solve
this problem, it will help to be systematic!
We will get sets that are significantly different from each other because this will help
later in this section and chapter.
The first term,
1
z + 1, can be written as either
1
z + 1 =
1
1 −(−z) =
∞
.
j=0
(−z)j =
∞
.
j=0
(−1)jzj, for |z| < 1,
(15.54)
because |−z| < 1 ⇐⇒|z| < 1, or
1
z + 1 = 1
z ·
1
1 −

−1
z
 = 1
z ·
∞
.
k=0

−1
z
k
=
∞
.
j=1
(−1)j−1z−j, for |z| > 1,
(15.55)
because
−1
z
 < 1 ⇐⇒|z| > 1.
The second term, −
2
z −2, can be written as either
−2
z −2 =
2
2 −z =
2
2

1 −z
2
 =
∞
.
j=0
 z
2
j
, for |z| < 2,
(15.56)
because
 z
2
 < 1 ⇐⇒|z| < 2, or
−2
z −2 = −2
z ·
1
1 −

2
z
 = −2
z ·
∞
.
k=0
2
z
k
=
∞
.
j=1
−

2j
z−j, for |z| > 2,
(15.57)
because
 2
z
 < 1 ⇐⇒|z| > 2.
If we combine (15.54) and (15.56), we can write f(z) as the sum of two terms, each of
which converges for |z| < 1. If we combine (15.55) and (15.56), we can write f(z) as the
sum of two terms, each of which converges for 1 < |z| < 2. If we combine (15.55) and
(15.57), we can write f(z) as the sum of two terms, each of which converges for |z| > 2.
Trying to combine (15.54) and (15.57) does not work because there is no z that satisfies
both |z| < 1 and |z| > 2.

Functions of a Complex Variable
1183
Combining (15.54) and (15.56), we get
f(z) =
1
z + 1 −
2
z −2 =
∞
.
j=0

(−1)j + 1
2j

zj, for |z| < 1,
that is, in the disk D1(0).
On the other hand, combining (15.55) and (15.56), we get
f(z) =
1
z + 1 −
2
z −2 =
∞
.
j=1
(−1) j−1z−j +
∞
.
j=0
 z
2
j
, for 1 < |z| < 2,
that is, in the annulus A1,2(0).
A third correct final conclusion comes from combining (15.55) and (15.57) to get
f(z) =
1
z + 1 −
2
z −2 =
∞
.
j=1

(−1) j−1 −2j
z−j, for |z| > 2,
that is, in the annulus A2,∞(0). ⃝
In Example 15.38 we were able to express a function f(z) as a Laurent series in each
of the sets D1(0), A1,2(0), and A2,∞(0). This gives us a way to work with the function
everywhere except on the circles z = 1 and z = 2. The three domains are shown in
Figure 15.20. ⃝
Example 15.39
Find a Laurent series for f(z) = sin z/z3.
Method: The Taylor series expansion for sin z about z0 = 0 is the same as the Maclaurin
series for sin x except with the real variable x replaced by the complex variable z. So,
y
x
y
x
y
x
D1(0)
(a)
A1,2(0)
(b)
A2,∞(0)
(c)
FIGURE 15.20
Example 15.38’s Laurent series domains.

1184
Advanced Engineering Mathematics
f(z) = 1
z3 · sin z = 1
z3 ·
∞
.
k=0
(−1)k
(2k + 1)! z2k+1 = 1
z3 ·
	
z −z3
3! + z5
5! ± · · ·

= 1
1!z−2 −1
3!z0 + 1
5!z2 −1
7!z4 ± · · · = z−2 +
∞
.
ℓ=0
(−1)ℓ+1
(2ℓ+ 3)! z2ℓ
is its Laurent series about z0 = 0.
The Laurent series is undefined at z = 0. Because the Maclaurin series for sin z con-
verges for all z—this follows from the ratio test, that is, Theorem 15.20–we see that the
Laurent series for f(z) about z0 = 0 converges for all z with |z| > 0, that is, in the annulus
A0,∞(0), a punctured plane. ⃝
Example 15.40
Find a Laurent series for f(z) = sin z
z
.
Method: The Taylor series expansion for sin z about z0 = 0 is the same as the Maclaurin
series for sin x except with the real variable x replaced by the complex variable z. So,
f(z) = 1
z · sin z = 1
z ·
∞
.
k=0
(−1)k
(2k + 1)! z2k+1 =
∞
.
k=0
(−1)k
(2k + 1)! z2k = 1
1!z0 −1
3!z2 + 1
5!z4 + · · ·
=
∞
.
ℓ=0
(−1)ℓ
(2ℓ+ 1)! z2ℓ, for 0 < |z| < ∞
is its Laurent series.
In fact, this Laurent series can also be called a power series because it only has
nonnegative powers of (z −z0).
The Laurent series is also defined at z = 0. By the ratio test, that is, Theorem 15.20, we
see that the Laurent series for f(z) about z0 = 0 converges for all z. The original function
f(z) = sin z
z
wasn’t even defined at z = 0: nevertheless, this Laurent series is defined
everywhere. ⃝
We’ll see in Section 15.7 that Example 15.40 illustrates the concept of a “removable”
singularity.
Example 15.41
Find a Laurent series for f(z) = sin2 z
z
and its domain of convergence.
Method: It is possible to find the product of two Taylor series, but in this problem, it is
easier and more effective to use the trigonometric identity
sin2 z = 1 −cos 2z
2
.
Using the Maclaurin series for cos x and replacing x by 2z, we have that
f(z) = 1
z · sin2 z = 1
z · 1 −cos 2z
2
= 1
2z
⎛
⎝1 −
∞
3
k=0
(−1)k (2z)2k
(2k)!
⎞
⎠=
∞
3
k=1
(−1)k+1 22k−1z2k−1
(2k)!
(15.58)

Functions of a Complex Variable
1185
is a Laurent series for f(z). As for the Maclaurin series for cos x, the ratio test shows that
the Maclaurin series for cos z converges for all z. So, the Laurent series (15.58) converges
for |z| < ∞, that is, the whole plane. While the original function is undefined at z = 0,
its Laurent series is a continuous extension of f(z).
In fact, this Laurent series can also be called a power series because it only has non-
negative powers of (z −z0). ⃝
15.6.3 Product of Taylor Series
If
f(z) =
∞
.
j=0
aj (z −z0)j
and
g(z) =
∞
.
j=0
bj (z −z0)j
both converge for |z −z0| < R, then
f(z)g(z) =
∞
.
j=0
⎛
⎝
j
.
ℓ=0
aℓbj−ℓ
⎞
⎠(z −z0)j
converges for |z −z0| < R. This makes sense, for example, for z0 = 0, because
f(z)g(z) =

a0 + a1z + a2z2 + · · ·
 
b0 + b1z + b2z2 + · · ·

= a0b0 + (a0b1 + a1b0)z + (a0b2 + a1b1 + a2b0)z2 + · · · .
For example, we could calculate that
e3zez =
⎛
⎝
∞
.
j=0
(3z)j
j!
⎞
⎠
⎛
⎝
∞
.
j=0
zj
j!
⎞
⎠=
∞
.
j=0
⎛
⎝
j
.
ℓ=0
3ℓ
ℓ! ·
1
(j −ℓ)!
⎞
⎠zj,
(15.59)
but it is a lot easier to write
e3zez = e4z =
∞
.
j=0
(4z)j
j!
.
By the way,
j
.
ℓ=0
aℓbj−ℓis called a discrete convolution of the sequences of coefficients.
We saw this also in Section 4.7 when studying the z-transform and in Section 9.5 when
studying the discrete Fourier transform.
15.6.4 Problems
1. Find a Laurent series for f(z) = 3
z +
1
z−2 in each of the domains (a) 0 < |z| < 2 and
(b) 2 < |z|.

1186
Advanced Engineering Mathematics
2. Find a Laurent series for f(z) =
3
z−1 +
1
z−2 in each of the domains (a) 1 < |z −1|
and (b) 0 < |z −1| < 1.
3. Find a Laurent series for f(z) =
1
z−1 +
1
z+2 in each of the domains (a) 1 < |z| < 2
and (b) 0 < |z| < 1.
In each of problems 4–7, find a Laurent series for the given function in two domains that
are disjoint, that is, have no points in common. It may help to use partial fractions.
4. f(z) =
1
z+1 +
1
z−2
5. f(z) =
1
z(z−1)
6. f(z) = −
5
(z+2)(z−3)
7. f(z) = −
z+5
(z+2)(2z+1)
In each of problems 8–10, for the given function, find a Laurent series and a domain on
which it converges.
8. f(z) = cos2 z
z
9. f(z) = z e1/z
10. f(z) = arcsin2 z
z
. [Hint: The Maclaurin series for arcsin x is x + 1
2
x3
3 + 1·3
2·4
x5
5 +
1·3·5
2·4·6
x7
7 + · · · .]
11. A consequence of (15.59) is the identity
(⋆)
4j
j! =
j
.
ℓ=0
3ℓ
ℓ! ·
1
(j −ℓ)!,
for all integers j ≥0. Explain (⋆) by applying the binomial theorem, (x + y)j =
3j
ℓ=0
j!
ℓ!(j−ℓ)! xℓyj−ℓ, in the particular case of x = 3 and y = 1, that is, using the
binomial theorem on the right-hand side of 4j = (3 + 1)j. Next, generalize this to
give an explanation of why the law of exponents (15.27)(i) in Section 15.4, that is,
ez1+z2 = ez1ez2, is true.
15.7 Zeros and Poles
Definition 15.20
Suppose f(z) is analytic at z0. We say f(z) has a zero of order m at z0 if
f(z0) = f ′(z0) = · · · = f (m−1)(z0) = 0
and
f (m)(z0) ̸= 0 .
(15.60)

Functions of a Complex Variable
1187
Note that Theorem 15.40 in Section 15.9 in the following will justify that f, f ′, . . . , f (m)
exist at z0.
Example 15.42
Find the zero(s) and their order(s) for f(z) ≜(z −z0)2.
Method:
f(z0) = (z0 −z0)2 = 0
f ′(z) = 2(z −z0),
f ′(z0) = 2(z0 −z0) = 0
f ′′(z) = 2,
f ′′(z0) = 2 ̸= 0,
so f(z) has a zero of order two at z0. That is the only point at which f(z) = 0, so z0 is the
only zero of f(z). ⃝
Example 15.43
Find the order of the zero of f(z) ≜(sin z −z) at z = 0.
Method: The “at z = 0” tells us that z0 = 0. We calculate
f(0) = sin 0 −0 = 0
f ′(z) = cos z −1,
f ′(0) = cos 0 −1 = 0
f ′′(z) = −sin z,
f ′′(0) = −sin 0 = 0
f ′′′(z) = −cos z,
f ′′′(0) = −cos 0 = −1 ̸= 0,
so f(z) has a zero of order three at z = 0. ⃝
Theorem 15.24
(A “preparation theorem”) Suppose f(z) is analytic at z0. Then f(z) has a zero of order m at
z0 if, and only if,
f(z) = (z −z0)mg(z)
for some function g(z) that both is analytic at z0 and has g(z0) = f (m)(z0)
m!
̸= 0.
Why? Given that f(z) has a zero of order m at z0, the Taylor series expansion of f(z) gives
f(z) = f(z0) + f ′(z0)
1!
(z −z0) + · · · + f (m−1)(z0)
(m −1)! (z −z0)m−1 + f (m)(z0)
m!
(z −z0)m + · · · ,

1188
Advanced Engineering Mathematics
so knowing that f(z) has a zero of order m at z0 implies
f(z) = 0 + 0
1!(z −z0) + · · · +
0
(m −1)!(z −z0)m−1 + f (m)(z0)
m!
(z −z0)m
+ f (m+1)(z0)
(m + 1)! (z −z0)m+1 + · · ·
= (z −z0)m
	
f (m)(z0)
m!
+ f (m+1)(z0)
(m + 1)! (z −z0) + · · ·

= (z −z0)mg(z),
where
g(z) ≜f (m)(z0)
m!
+ f (m+1)(z0)
(m + 1)! (z −z0) + · · ·
converges and satisfies
g(z0) = f (m)(z0)
m!
+ f (m+1)(z0)
(m + 1)! (0) + · · · = f (m)(z0)
m!
̸= 0. 2
15.7.1 Singularities
Here we will study the ways a function can fail to be analytic at z0.
Definition 15.21
f(z) has an isolated singularity at z0 if f(z) is not analytic at z0 but is analytic on some
punctured disk A0,r(z0) = {z : 0 < |z −z0| < r}.
Example 15.44
By Theorem 15.4 in Section 15.2, f(z) ≜
1
(z −1)2 is not analytic at z = 1 but is analytic
for 0 < |z −1| < ∞, so f(z) has an isolated singularity at z = 1. ⃝
Example 15.45
f(z) ≜sin z
z
is not analytic at z = 0 because it is undefined at z = 0. But, by Theorem
15.6(d) in Section 15.2, f(z) is analytic for 0 < |z| < ∞, so f(z) has an isolated singularity
at z = 0. ⃝
Example 15.46
f(z) ≜e−1/z is not analytic at z = 0 because it is undefined there. But f(z) is analytic for
0 < |z| < ∞, because f(z) = g(h(z)), where h(z) ≜−1
z is analytic everywhere except at
z = 0, and g(z) ≜ez is analytic everywhere. So f(z) has an isolated singularity at z = 0. ⃝

Functions of a Complex Variable
1189
Definition 15.22
Suppose f(z) has an isolated singularity at z0 and, in some punctured disk A0,r(z0), f(z) has
Laurent series
f(z) ≜
∞
.
j=1
a−j (z −z0)−j +
∞
.
j=0
aj (z −z0)j.
(a) If a−j = 0, for all j ≥1, then we say f(z) has a removable singularity at z0.
(b) If a−j ̸= 0 for infinitely many j ≥1, then we say f(z) has an essential singularity
at z0.
(c) If a−j = 0 for all j > m ≥1 and a−m ̸= 0, then we say f(z) has a pole of order m
at z0.
Example 15.47
f(z) ≜sin z
z
has what kind of singularity at z0 = 0?
Method: From Example 15.45 we know f(z) has an isolated singularity at z0 = 0. From
Example 15.40 in Section 15.6, the Laurent series of f(z) about z0 = 0 is
f(z) = 1
1!z0 −1
3!z2 + 1
5!z4 + · · · , for 0 < |z| < ∞.
By definition, f(z) has a removable singularity at z0 = 0. ⃝
The phrase “removable singularity” suggests that there exists some sort of “removal
process.” Indeed, for Example 15.47, we can define an extension of f(z) by
f(z) ≜
⎧
⎨
⎩
f(z),
if z ̸= 0
1,
if z = 0
⎫
⎬
⎭.
We choose f(0) = 1 because L’Hôpital’s rule, that is, Theorem 15.12 in Section 15.3,
calculates that
lim
z→0 f(z) = lim
z→0
sin z
z
= lim
z→0
(sin z)′
(z)′
= lim
z→0
cos z
1
= 1.
Theorem 15.25
(An “extension theorem” or “singularity removal process”) Suppose f(z) has a remov-
able singularity at z0 and a Laurent series (15.53) in Section 15.6 for z in some punctured
disk A0,r(z0). Then

1190
Advanced Engineering Mathematics
f(z) ≜
⎧
⎨
⎩
f(z),
if z ̸= z0
a0,
if z = z0
⎫
⎬
⎭
defines an extension of f(z) that is analytic on the disk Dr(z0).
Why? The reasoning is similar to what we used when removing the singularity of sin z
z
at
z = 0. 2
Example 15.48
f(z) ≜e−1/z has what kind of singularity at z0 = 0?
Method: From Example 15.46, we know f(z) has an isolated singularity at z = 0. Using
the Maclaurin series of ez and replacing z by −1
z , we see that f(z) has a Laurent series
f(z) = exp

−1
z

=
∞
.
n=0
1
n!

−1
z
n
= 1 +
∞
.
j=0
(−1)j
j!
z−j.
Because a−j =
(−1)j
j!
̸= 0 for infinitely many j ≥1, in fact all j ≥1, f(z) has an
essential singularity at z0 = 0. ⃝
Example 15.49
f(z) ≜
1
(z −1)2 has what kind of singularity at z = 1?
Method: From Example 15.44, we know f(z) has an isolated singularity at z = 1. This
function “is its own Laurent series,” that is,
f(z) = · · · + 0 · (z −1)−4 + 0 · (z −1)−3 + 1(z −1)−2 + 0 · (z −1)−1 + 0 · (z −1)0
+ 0 · (z −1)1 + · · · ,
for all z ̸= 1. f(z) fits the definition of having a pole of order two at z0 = 1. ⃝
It is often easier to find the location of a pole and its order by studying the zeros of
the denominator and numerator of a function. Intuitively, the “algebra of poles” is the
reciprocal of the “algebra of zeros.”
Theorem 15.26
Suppose m is a positive integer. Any one of the following “data” is enough to imply that
f(z) has a pole of order m at z0:
(a) f(z) =
g(z)
(z −z0)m , where g(z) is analytic at z0 and has g(z0) ̸= 0.

Functions of a Complex Variable
1191
(b) f(z) = g(z)
h(z), where h(z) has a zero of order m at z0, g(z0) ̸= 0, and both g(z) and
h(z) are analytic at z0.
(c) f(z) = f1(z) + f2(z), where f1(z) has a pole of order m at z0 and f2(z) is analytic at z0
or has a pole of order less than m at z0.
Why? (a) The assumption that g(z) is analytic at z0 guarantees that g(z) has Taylor series
given as in (15.48) in Section 15.6, that is,
g(z) =
∞
.
j=0
cj (z −z0)j
that converges in some open disk Dr(z0). We also assumed that c0 = g(z0) ̸= 0. So,
f(z) =
g(z)
(z −z0)m = (z −z0)−m 
c0 + c1(z −z0) + c2 (z −z0)2 + · · ·

= c0(z −z0)−m + c1(z −z0)−m+1 + c2 (z −z0)−m+2 + · · · .
According to Definition 15.22(c), f(z) has a pole of order m at z0.
(b) and (c) will be in your homework Problems 15.7.2.11 and 15.7.2.12, respectively. 2
Example 15.50
Find the order of the pole f(z) ≜
z −5
(sin z)2 has at z = 0.
Method: Let h(z) ≜(sin z)2. In order to use Theorem 15.26(b), we will need to find the
order of the zero of h(z) at z = 0. We calculate
h(0) = sin2 0 = 0
h′(z) = 2 sin z cos z = sin 2z,
h′(0) = sin(2 · 0) = 0
h′′(z) = 2 cos 2z,
h′′(0) = 2 cos 0 = 2 ̸= 0,
so h(z) has a zero of order two at z = 0. Because g(z) ≜z −5 is analytic at z = 0 and has
g(0) = −5 ̸= 0, Theorem 15.26(b) implies that the original function, f(z) ≜g(z)
h(z), has a
pole of order two at z = 0. ⃝
Theorem 15.27
(A second “preparation theorem”) Suppose f(z) has an isolated singularity at z0. Then f(z)
has a pole of order m at z0 if, and only if,
f(z) = (z −z0)−mk(z)
for some function k(z) that is both analytic at z0 and has k(z0) ̸= 0.

1192
Advanced Engineering Mathematics
Why? By definition, f(z) has a pole of order m at z0 when for z in some punctured disk
A0,r(z0), it has a Laurent series (15.53) in Section 15.6 with a−j = 0 for all j > m and a−m ̸= 0.
In “explicit” form,
f(z) = · · · + 0 +
a−m
(z −z0)m +
a−m+1
(z −z0)m−1 + · · · + a0 + a1(z −z0) + a2(z −z0)2 + · · ·
= (z −z0)−m
a−m + a−m+1(z −z0) + a−m+2(z −z0)2 + · · ·

= (z −z0)−mk(z).
Note that k(z0) = a−m + a−m+1(z0 −z0) + · · · = a−m + 0 + · · · = a−m ̸= 0. 2
To make Theorem 15.26 more useful, it helps to have corresponding results about zeros.
Theorem 15.28
Suppose m > 0 and n are integers:
(a) If g(z) and h(z) are analytic at z0, g(z) has a zero of order m at z0, and h(z) has a
zero of order n at z0, then the product f(z) ≜g(z)h(z) has a zero of order (m + n)
at z0.
(b) If g(z) has a pole of order m at z0 and h(z) is analytic and has a zero of order n at
z0, then
(i) if m > n, then the product f(z) ≜g(z)h(z) has a pole of order (m −n) at z0;
(ii) if m = n, then f(z) ≜g(z)h(z) has a removable singularity at z0;
(iii) if m < n, then f(z) has a removable singularity at z0 and an extension of
f(z) ≜g(z)h(z) has a zero of order (n −m) at z0.
Why? For example, for (b)(i), if g(z) has a pole of order m at z0, then the second
“preparation” Theorem 15.27 implies that g(z) = (z −z0)−mk(z) for some function
k(z) that both is analytic at z0 and has k(z0) ̸= 0. Also, if h(z) is analytic and has
a zero of order n at z0, then the first preparation Theorem 15.24 implies that h(z) =
(z −z0)nℓ(z) for some function ℓ(z) that is analytic at z0 and has ℓ(z0) ̸= 0. It follows
that
f(z) = g(z)h(z) = (z −z0)−mk(z) · (z −z0)nℓ(z) = (z −z0)−(m−n) · k(z)ℓ(z).
Because k(z0)ℓ(z0) ̸= 0, the second preparation theorem implies that f(z) has a pole of
order (m −n) at z0, as we wanted to explain.
As for explaining the other results in Theorem 15.28, that is left to you in Problems
15.7.2.13 through 15.7.2.15. 2

Functions of a Complex Variable
1193
Corollary 15.1
If g(z) has a pole of order m at z0, then
4
1
g(z) ≜
⎧
⎪⎪⎨
⎪⎪⎩
1
g(z),
z ̸= z0
0,
z = z0
⎫
⎪⎪⎬
⎪⎪⎭
has a zero of order m at z0.
Why? You will explain this in Problem 15.7.2.16. 2
15.7.2 Problems
1. Find a function that has a pole of order one at each of z = ±i and a pole of order
three at z = −1.
In problems 2 and 3, what is the order of the pole of the given function f(z) at the given
point z0?
2. f(z) =
1
ez −1 −z at z0 = 0?
3. f(z) =
1
(ez −1)2 at z0 = 0?
In problems 4–6, find and classify the isolated singularities of the given function.
4. f(z) =
1
z2(z + 1).
5. f(z) =
1
ez + 1.
6. f(z) =
z
z2 + 4 + 1
z.
7. Construct a function f(z) that has a zero of order one at each of z = ±i and a pole
of order three at z = −1.
8. Explain why f ′(z) has a zero of order m −1 at z0 if f(z) has a zero of order m at z0,
assuming m is an integer ≥2. What happens if m = 1?
9. Explain why f ′(z) has a pole of order m + 1 at z0 if f(z) has a pole of order m at z0,
assuming m is a positive integer.
10. Suppose that R > 0 and for k = 1, ..., 4 each of the functions fk(z) has a zero of
order two at z0, is analytic at z0, and is nonzero for 0 < |z −z0| < R. Define
h(z) = det
f1(z)
f2(z)
f3(z)
f4(z)

and H(z) = f4(z)
h(z) . Investigate the nature of the singularity of H(z) at z0.

1194
Advanced Engineering Mathematics
11. Explain why Theorem 15.26(b) is true.
12. Explain why Theorem 15.26(c) is true.
13. Explain why Theorem 15.28(a) is true.
14. Explain why Theorem 15.28(b)(ii) is true.
15. Explain why Theorem 15.28(b)(iii) is true.
16. Explain why Corollary 15.1 is true.
15.8 Complex Integration and Cauchy’s Integral Theorem
In the complex plane, line integrals of functions of a complex variable have properties that
are wonderful and more amazing than we would have expected from our experience with
line integrals of vector fields in R2 or R3.
Definition 15.23
In the complex plane,
C : z = z(t) = x(t) + iy(t), a ≤t ≤b
is a parametrized curve if z(t) is continuous on interval [ a, b ]. In this case, z(a) is the
initial point and z(b) is the terminal point of C. A parametrized curve is piecewise smooth
if x(t) and y(t) are piecewise smooth functions of the real variable t in the interval [ a, b ].
A parametrized curve is closed if z(a) = z(b). A parametrized curve is simple if z(s) ̸= z(t)
for all s, t with a ≤s < t ≤b except possibly z(a) = z(b).
Recall from Section 7.2 our definition that a simple, closed, parametrized curve in the
xy-plane is positively oriented if, while standing in the xy-plane with head in the ˆk direc-
tion, the inside of the curve is on our left as we travel on the curve in the direction of its
parametrization. While this definition is admittedly imprecise, it will suffice for our needs.
We will say C : z = z(t) = x(t) + iy(t), a ≤t ≤b, a simple, closed, parametrized curve
in the complex plane, is positively oriented if C : r(t) = x(t)ˆı + y(t) ˆj, the corresponding
curve in R2, is positively oriented.
We may abuse the definitions by referring to C as both (1) a parametrized curve, C : z =
z(t), a ≤t ≤b, and (2) the set {z(t) : a ≤t ≤b}, that is, the set consisting of all of the points
z(t), a ≤t ≤b.
Definition 15.24
In the complex plane, a contour is a chain of parametrized, piecewise smooth curves
C1, . . . , CN where the initial point of Ck+1 is the terminal point of Ck : z = zk(t), ak ≤t ≤bk,
for k = 1, . . . , N −1. We denote such a contour by C1 + · · · + CN. A contour is simple if

Functions of a Complex Variable
1195
for k ̸= ℓ, Ck and Cℓhave no points in common except that the initial point of Ck+1 is the
terminal point of Ck and, possibly, z1(a1) = zN(bN). If the latter is true, then we say the
contour is closed.
A simple, closed, contour in the complex plane will be called positively oriented if
C1 + · · · + CN, its chain of parametrized curves, Ck : z = zk(t), ak ≤t ≤bk, k = 1, . . . , N, is
positively oriented when rewritten as a single parametrized curve:
z(t) =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
z1(t),
if a1 ≤t ≤b1
z2(t + a2 −b1),
if b1 ≤t ≤b1 + b2 −a2
...
zN

t + 3N−1
k=1 (ak+1 −bk)

,
if b1 + 3N−1
k=2 (bk −ak) ≤t ≤b1 + 3N
k=2(bk −ak)
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
An example of a contour is depicted in Figure 15.21.
We may abuse the definitions by referring to C as both (1) a contour, that is, a chain of
parametrized curves, Ck : z = zk(t), ak ≤t ≤bk, k = 1, . . . , N, and (2) the set union of all of
the points zk(t), ak ≤t ≤bk, k = 1, . . . , N.
We will define a line integral in the complex plane analogous to the line integral of a vec-
tor field over a curve in R2. Given a parametrized curve C : z = z(t) = x(t)+iy(t), a ≤t ≤b,
partition the interval [ a, b ] into a = t0 < t1 < · · · < tn = b. This gives an approximation of
C by a polygonal curve∗consisting of directed line segments z(t0)  z(t1)  ...  z(tn),
as shown in Figure 15.22.
z3
C3
C2
x
y
z1
C1
z2
FIGURE 15.21
Contour C1 + C2 + C3.
z(t1)
z(t0)
z(tn)
FIGURE 15.22
Polygonal curve approximation.
∗See Definition 15.8(b) in Section 15.3.

1196
Advanced Engineering Mathematics
Define △zk = z(tk) −z(tk−1). Given a function f(z), we can form the Riemann sum
n3
k=1
f (z(tk)) △zk. If we can take its limit as both n →∞and

max1≤k≤n |△zk|

→0, then
we can define the line integral of a function of a complex variable over a curve in the
complex plane by

C
f(z) dz ≜lim
n→∞
n3
k=1
f (z(tk)) △zk .
(15.61)
Because both z and f(z) have real and imaginary components, the concept defined in
(15.61) is like that of a line integral of a vector field rather than of a scalar field.
Theorem 15.29
Suppose C : z = z(t) = x(t) + iy(t), a ≤t ≤b, is a piecewise smooth, parametrized curve
and f (z(t)) is a continuous function of t in the interval [ a, b ]. Then there exists

C f(z) dz as
defined in (15.61) and it can be calculated using

C
f(z) dz =
b
a
f (z(t)) dz
dt (t) dt .
(15.62)
Similar to the result of Problem 7.2.5.22, the line integral of a function of a complex
variable over a curve in the complex plane does not depend on the parametrization of the
curve.
Corollary 15.2
Suppose the function f(z) is continuous at every point on a contour C1 + · · · + CN. Then
there exists

C1+···+CN
f(z) dz ≜

C1
f(z) dz + · · · +

CN
f(z) dz .
(15.63)
Example 15.51
Evaluate

C
1
z −1 dz where the circle C is shown in Figure 15.23.
Method: The circle C can be parametrized by
C : z = z(t) = (1 + 2 cos t) + i2 sin t = 1 + 2eit, 0 ≤t ≤2π,

Functions of a Complex Variable
1197
y
x
–1
1
2
FIGURE 15.23
Example 15.51.
so (15.62) calculates that

C
1
z −1 dz =
2π

0
1
z(t) −1 z′(t) dt =
2π

0
1
1 + 2 cos t + i2 sin t −1
(−2 sin t + i2 cos t) dt
=
2π

0
−2 sin t + i2 cos t
2 cos t + i2 sin t dt =
2π

0
i(2 cos t + i2 sin t)
2 cos t + i2 sin t
dt =
2π

0
i dt = 2πi. ⃝
By the way, a streamlined version of the earlier calculation is

C
1
z −1 dz =
2π

0
1
z(t) −1 z′(t) dt =
2π

0
1
(1 + 2eit) −1 (i2eit)dt =
2π

0
1

2eit (i
2eit)dt
=
2π

0
i dt = 2πi.
We will see that Example 15.51 and related calculations are very useful.
Theorem 15.30

C
1
(z −z0)m dz =
⎧
⎨
⎩
2πi,
m = 1
0,
m ̸= 1
⎫
⎬
⎭,
(15.64)
where m is any integer and C is any positively oriented circle centered at z0.
Why? You will establish the case m = 1 in Problem 15.8.3.8; it’s similar to the calculation
of Example 15.51. For integer m ̸= 1 and circle C : z(t) = z0 + reit, 0 ≤t ≤2π,

1198
Advanced Engineering Mathematics

C
1
(z −z0)m dz =
2π

0
1



z0 + reit −

z0
m

ireit
dt =
2π

0
r−meit(−m) (ir)eitdt
= i r1−m
2π

0
eit(1−m) dt = i r1−m
5
eit(1−m)
i(1 −m)
62π
0
= i r1−m
	

1
i(1 −m) −

1
i(1 −m)

= 0. 2
Example 15.52
Let C be the straight line from z = 1 to z = i. Evaluate

C
z dz.
Method: C can be parametrized by z = z(t) = 1 + (i −1)t, 0 ≤t ≤1. On C, z′(t) = i −1
and z(t) = 1 −t + it = 1 + (−i −1)t, so, by (15.62),

C
z dz ≜
1
0
z(t) z′(t) dt =
1
0
(1 + (−i −1)t) (i −1) dt
=
1
0
(i −1 + 2t) dt =

(i −1)t + t2 1
0 = (i −1) + 1 = i. ⃝
Definition 15.25
Given a parametrized curve C : z = z(t), a ≤t ≤b, the opposite curve is the parametrized
curve defined by −C : z = z(a + b −t), a ≤t ≤b.
Theorem 15.31
If

C f(z) dz exists, then so does the integral over the opposite parametrized curve, and

−C
f(z) dz = −

C
f(z) dz .
(15.65)
So, in this respect, integration on a parametrized curve in the complex plane behaves
like the line integral of a vector field.
15.8.1 Integration on a Closed Contour
We will study integration on closed contours and learn about deep connections with sin-
gularities of functions. First, recall the definition of “connected” from Section 15.3. Next,
we define a set A in the complex plane to be bounded if there is an M > 0 such that |z| ≤M
for all z in A. For example, the square {x + iy : −1 ≤x ≤1, −1 ≤y ≤1} is bounded with
M =
√
2.
First, we need a major theorem of pure mathematics.

Functions of a Complex Variable
1199
Theorem 15.32
(Jordan curve theorem) A simple closed contour C separates the complex plane into two
open connected sets, Ui and Uo with the properties that (i) C equals the disjoint union
Ui ∪C ∪Uo, (ii) Ui is bounded, and (iii) Uo is not bounded.
We call the bounded set Ui the interior of C, we say Ui is inside C, and we call the
unbounded set Uo the exterior of C.
Example 15.53
C : z = 1 + 2eit, 0 ≤t ≤2π has positive orientation, and the interior of C is the open disk
D2(1), as shown in Figure 15.23. ⃝
Recall from Definition 15.9 in Section 15.3 that a “domain” is an open, connected set.
Definition 15.26
A function f(z) has an antiderivative F(z) on a domain D if F′(z) = f(z) for all z in D.
Theorem 15.33
Suppose f(z) has an antiderivative F(z) on a domain D and C : z = z(t), a ≤t ≤b is a
contour contained in D. Then

C
f(z) dz = F(z(b)) −F(z(a)) .
(15.66)
Why? The explanation is the same as for the fundamental theorem of line integrals,
Theorem 7.8 in Section 7.2.
Example 15.54
Evaluate

C
1
z dz where C is shown in Figure 15.24a.
Method: Recall from Theorem 15.17 that d/dz

Log(z)

= 1/z for all z in any domain D as
long as no point on the nonpositive real axis is in D. An example of such a domain D is
the interior of the dashed ellipse illustrated in Figure 15.24b. So, Theorem 15.33 allows
us to calculate

C
1
z dz = Log(i3) −Log(2) =

ln 3 + i π
2

−

ln 2 + i0

= ln
3
2

+ i π
2 . ⃝

1200
Advanced Engineering Mathematics
1
1
2
3
y
x
2
1
1
2
3
y
x
2
(a)
(b)
FIGURE 15.24
(a) C, (b) C in D.
Corollary 15.3
Suppose f(z) has an antiderivative F(z) on a domain D and C is a simple closed contour
contained in D. Then

C f(z) dz = 0.
Why? Because F(z) is an antiderivative for f(z) on D,

C f(z) dz = F(z(b)) −F(z(a)) =
F(z(a)) −F(z(a)) = 0, where C has the same initial as terminal point z(b) = z(a). 2
Theorem 15.34
(Path independence) Suppose f(z) has an anti-derivative F(z) on a domain D, zI and zT are
in D, and contained in D are any two contours C1 and C2 both having initial point zI and
terminal point zT. Then

C2
f(z) dz =

C1
f(z) dz .
(15.67)
Why? By Theorem 15.33,

C2
f(z) dz = F(zT) −F(zI) =

C1
f(z) dz. 2
15.8.2 Cauchy–Goursat Integral Theorem
Here we will learn about a basic but powerful result for calculating contour integrals. First,
we need a definition.

Functions of a Complex Variable
1201
Definition 15.27
A deformation in domain D of a contour C0 to a contour C1, denoted by C0 ↭C1 or
z(t, s) : C0 ↭C1, is a map
z = z(t, s),
a ≤t ≤b,
0 ≤s ≤1
that satisfies
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
z0(t) ≜z(t, 0), a ≤t ≤b, parametrizes C0
z1(t) ≜z(t, 1), a ≤t ≤b, parametrizes C1
z(t, s) is continuous in (t, s) for a ≤t ≤b, 0 ≤s ≤1
z(t, s) remains in D for all a ≤t ≤b, 0 ≤s ≤1
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
We say C0 can be deformed in D to C1 if there exists a deformation C0 ↭C1 in D.
Figure 15.25 illustrates a deformation with the arrows and dotted lines depicting the
process of deforming C0 to C1. This specific deformation is
z(t, s) =
1
√
2

(2 −s)2 cos t + (2 −s) sin t + i

−(2 −s)2 cos t + (2 −s) sin t

, 0 ≤t ≤2π.
Note that if there exists a deformation z(t, s) : C0 ↭C1 in D, then there exists a
deformation z(t, 1 −s) : C1 ↭C0 in D.
C0
C1
FIGURE 15.25
Deformation of C0 to C1.

1202
Advanced Engineering Mathematics
Theorem 15.35
(Deformation theorem) Suppose f(z) is analytic on a domain D and contour C0 can be
deformed in D to contour C1. Then

C0
f(z) dz =

C1
f(z) dz .
(15.68)
Definition 15.28
A domain D is simply connected if every simple, closed contour contained in D can be
deformed in D to a single point.
Every open disk is simply connected, but every annulus and punctured disk is not
simply connected.
Theorem 15.36
(Cauchy–Goursat integral theorem) Suppose f(z) is analytic on a simply connected domain
D and C is a simple, closed contour contained in D. Then

C
f(z) dz = 0.
Why? Given a simple closed contour C contained in simply connected domain D where
f(z) is analytic, find a deformation C ↭{z0} in D, that is, deformation to the parametrized
curve C1 : z = z(t) ≡z0, a ≤t ≤b. By the Deformation Theorem 15.35,

C
f(z) dz =

{z0}
f(z) dz =
b
a
f (z(t)) · 0 dt = 0. 2
Example 15.55
Evaluate

|z−2|=1
1
z dz.
Method: Recall that the arrow direction in the symbol

indicates that the parametriza-
tion of the curve is positively oriented. Define a parametrized curve by C : z = z(t) =
2 + eit, 0 ≤t ≤2π, and a domain by D ≜D1+ε(2). Then f(z) ≜1
z is analytic in D as long
as ε < 1, and C is contained in D as long as ε > 0. Choose ε = 1
2, for example, to see that
Theorem 15.36 implies that

C f(z) dz = 0. ⃝

Functions of a Complex Variable
1203
2
–2
x
C
D
y
2
–2
x
y
2
–2
x
y
2
–2
x
C1
C0
y
(a)
(b)
(c)
(d)
FIGURE 15.26
Example 15.56: using theorems.
Example 15.56
Evaluate

C
2z + i
z3 + z dz, where C is shown in Figure 15.26a.
Method: The ×’s in Figure 15.26a show the location of the singularities of f(z) ≜2z + i
z3 + z;
everywhere else f(z) is analytic. We will use a combination of Theorems 15.30, 15.35, and
15.36 to evaluate the integral.
First, it helps to get the partial fractions expansion of f(z). Because the denominator is
z3 + z = z(z2 + 1) = z(z −i)(z + i), the correct form of the expansion is
2z + i
z3 + z = 2z + i
z3 + z = A
z +
B
z −i +
C
z + i,
where A, B, C are constants to be determined. As discussed in Appendix A, we can
calculate A, B, C, specifically that
f(z) = i
z + −i3
2
z −i +
i
2
z + i,
so

C
2z + i
z3 + z dz = i

C
1
z dz −i3
2

C
1
z −i dz + i
2

C
1
z + i dz.
The third term is zero, by Theorem 15.36, because f(z) =
1
z+i is analytic on the domain
D shown in Figure 15.26b.
The first term can be calculated by the deformation C ↭C0 in the same domain
D —see Figure 15.26c:
i

C
1
z dz = i

C0
1
z dz = i · 2πi = −2π,
using Theorem 15.30.
The second term can be calculated by the deformation C ↭C1 in the same domain
D –(see Figure 15.26d):
−i3
2

C
1
z −i dz = −i3
2

C1
1
z −i dz = −i3
2 · 2πi = 3π.
Finally, we conclude that

C
2z + i
z3 + z dz =

C

i i
z −i3
2 ·
1
z −i + i
2 ·
1
z + i

dz = −2π + 3π + 0 = π. ⃝

1204
Advanced Engineering Mathematics
As we promised in Section 15.3, we have a result that also relies on the concept of simple
connectedness.
Theorem 15.37
If u = u(x, y) is harmonic on a simply connected domain D, then u(x, y) has a harmonic
conjugate on D.
15.8.3 Problems
In problems 1–4, evaluate the integral.
1.

|z|=3
z dz
2.

|z−i|=1
z dz
3.

|z|= 3
2
z
(z −i)(z −2i) dz
4.

|z|= 5
2
z
(z −i)(z −2i) dz
5. Evaluate the integral, where the parametrized curve C is shown in Figure 15.27b.
[Hint: Use a suitable branch of the logarithm.]

C
1
z dz
6. Evaluate the integral, where the parametrized curve C is shown in Figure 15.27a:

C
1
z dz.
1
–1
–1
1
1
2
–2
3
x
x
y
y
(a)
(b)
FIGURE 15.27
(a) Problem 15.8.3.6, (b) Problem 15.8.3.5.

Functions of a Complex Variable
1205
7. Theorem 15.34 on path independence in the complex plane is analogous to the
fundamental theorem of line integrals, Theorem 7.8 in Section 7.2, in R2 or R3.
Theorem 7.8 in Section 7.2 applies when a vector field F is exact. It turns out that
if we study path independence in the complex z = x + iy plane by rewriting a
contour integral in terms of real variables (x, y), then we will see in this problem
that the Cauchy–Riemann equations naturally appear:
(a) Given a function f(z), write f(z) = u(x, y) + iv(x, y) and z = x + iy and thus

C
f(z) dz =

C

u(x, y) + iv(x, y)

d(x + iy)
= · · · =

C
(u dx −v dy) + i

C
(vdx + udy).
Fill in those details.
(b) Use the result of part (a) to argue that in order to have path independence
of the contour integral

C f(z) dz, we need to have path independence of both

C (u dx −v dy) and

C (v dx + u dy).
(c) Argue that path independence of

C (udx −vdy) implies that the vector field
F = u ˆı −v ˆj must be exact. Argue similarly why the vector field G = v ˆı + u ˆj
must be exact.
(d) Use the results of part (c) to argue that path independence of the con-
tour integral

C f(z) dz implies that u, v must satisfy the Cauchy–Riemann
equations.
(e) Study Example 15.54 and Problems 7.2.5.19 and 7.2.5.20 and discuss the rel-
ative advantages and/or disadvantages of applying path independence of
contour integrals in the complex plane versus applying path independence
of line integrals in R2.
8. Establish the case m = 1 of Theorem 15.30; it’s similar to the calculation of
Example 15.51.
15.9 Cauchy’s Integral Formulas and Residues
Theorem 15.38
(Cauchy’s integral formula) Suppose f(z) is analytic on a simply connected domain D that
contains both a simple, closed, positively oriented contour C and the interior of C. If z0 is
inside C, then

C
f(z)
z −z0
dz = 2πi f(z0).
(15.69)

1206
Advanced Engineering Mathematics
Suppose z is inside C. If we replace z by ζ and then replace z0 by z, we get
1
2πi

C
f(ζ)
ζ −z dζ = f(z) .
(15.70)
Example 15.57
Evaluate

C
sin(z2)
z −i2 dz, where C is (a) the circle |z| = 3 oriented positively and (b) the
circle |z| = 1 oriented positively.
Method:
(a) By Cauchy’s integral formula with f(z) ≜sin(z2) and D = C,

|z|=3
sin(z2)
z −i2 dz = 2πi · f(i2) = 2πi · sin

(i2)2
= 2πi · sin(−4) = −i2π sin 4.
(b) By the Cauchy–Goursat Theorem 15.36 with, for example, D = D1.5(0),

|z|=1
sin(z2)
z −i2 dz = 0
because the function g(z) ≜sin(z2)
z −i2 is analytic on D1.5(0). ⃝
Example 15.58
Evaluate

|z+i|=1.5
z
z2 + 1 dz.
Method:
z
z2 + 1 =
z
(z −i)(z + i) is analytic everywhere on and inside the circle |z + i| =
1.5, except at z = −i. Figure 15.28 shows the contour |z + i| = 1.5 oriented positively,
with the ×’s marking the poles of the function
z
z2+1. Define f(z) ≜z
z−i, which is analytic
on D = D1.6(−i), for example. By Cauchy’s integral formula on D, we calculate that

|z+i|=1.5
z
z2 + 1 dz =

|z+i|=1.5
f(z)
z + i dz = 2πi · f(−i) = 2πi ·
−i
−i −i = iπ. ⃝
Example 15.59
Evaluate

|z|=R
eiωz
z2 + 1 dz, where ω is an unspecified constant and the constant R > 1.
Method: Partial fractions give
1
z2 + 1 =
A
z −i +
B
z + i = . . . = −i/2
z −i + i/2
z + i.
Define f(z) = eiωz. Partial fractions and then Cauchy’s integral formula give

|z|=R
eiωz
z2 + 1 dz = −i
2

|z|=R
f(z)
z −i dz + i
2

|z|=R
f(z)
z + i dz = −i
2 · 2πi f(i) + i
2 · 2πi f(−i)
= πf(i) −πf(−i) = π

eiωi −eiω(−i)
= 2π · e−ω −eω
2
= −2π sinh ω. ⃝

Functions of a Complex Variable
1207
y
x
–1
1
–1
–2
–3
FIGURE 15.28
Example 15.58.
15.9.1 Use of a Dumb-Bell Contour
We present an alternative method for calculating integrals by using “dumb-bell” contours,
as shown in Figure 15.29. Here, C0 is the circle |z| = 1
3, C1 is the circle |z−i| = 1
4, and they are
“joined” by the two line segments (a) L : z = z(t) = i

1
3 + 5t
12

, 0 ≤t ≤1, and (b)−L. Note
that the opposite parametrized curves ±L have opposite direction arrows in Figure 15.29.
Defining the contour C ≜C0 + L + C1 + (−L), if f(z) is analytic on a simply connected
domain D, which contains C, then

C
f(z) dz =

C0
f(z) dz +

L
f(z) dz +

C1
f(z) dz +

−L
f(z) dz
=

C0
f(z) dz +


L
f(z) dz +

C1
f(z) dz −


L
f(z) dz;
–2
C0
–L
L
C1
2
y
x
FIGURE 15.29
Dumb-bell contour.

1208
Advanced Engineering Mathematics
hence,

C
f(z) dz =

C0
f(z) dz +

C1
f(z) dz .
(15.71)
The latter is the sum of integrals over two disjoint circles.
Example 15.60
(Example 15.56 in Section 15.8 again) Evaluate

C
2z + i
z3 + z dz, where C is shown in
Figure 15.26a.
Method:
Using the dumbbell contour shown in Figure 15.29, (15.71) and Cauchy’s
integral formula allows us to calculate

C
2z + i
z3 + z dz =

C0
2z + i
z3 + z dz +

C1
2z + i
z3 + z dz =

C0
2z + i
z2 + 1
z
dz +

C1
2z + i
z(z + i)
z −i
dz
= 2πi
 2z + i
z2 + 1

at z=0

+ 2πi
 2z + i
z(z + i)

at z=i

= 2πi
 i
1

+ 2πi
 2i + i
i(i + i)

= −2π + 3π = π. ⃝
One advantage of this method is that we didn’t have to use a partial fractions expansion.
Theorem 15.39
(Cauchy’s integral formula in general) Suppose f(z) is analytic on a simply connected
domain D that contains a simple, closed, positively oriented contour C and its interior.
If z0 is inside C and m is a nonnegative integer, then

C
f(z)
(z −z0)m+1 dz = 2πi
m! f (m)(z0).
(15.72)
Suppose z is inside C. If we replace z by ζ and then replace z0 by z, we get
m!
2πi

C
f(ζ)
(ζ −z)m+1 dζ = f (m)(z) .
(15.73)
When m = 0, (15.72) reduces to (15.69) and (15.73) reduces to (15.70). An important
consequence of (15.73) and thus a consequence of Theorem 15.39 are the following.

Functions of a Complex Variable
1209
–1
–1
1
y
x
1
FIGURE 15.30
Example 15.61.
Theorem 15.40
If f(z) is analytic at z0, then so are f ′, f ′′, . . ..
Example 15.61
Evaluate

C
sin 2z

z2 + 1
2 dz, where C : z = z(t) = −i
2 + eit, 0 ≤t ≤2π.
Method:
sin 2z

z2+1
2 is analytic everywhere on and inside C except at z = −i. Figure 15.30
shows the contour |z + i
2| = 1 oriented positively, with the ×’s marking the poles of the
function
sin 2z

z2+1
2 . Define f(z) ≜sin 2z
(z−i)2 . Using Theorem 15.39 with m = 1, we calculate

C
sin 2z

z2 + 1
2 dz =

C
sin 2z
(z + i)2(z −i)2 dz =

C
(sin 2z)/(z−i)2
(z+i)2
dz = 2πi
1! · d
dz
1 sin 2z
(z−i)2
2
at z=−i
= 2πi ·
2 cos 2z
(z −i)2 −2 sin 2z
(z −i)3
 
at z=−i = 2πi ·
2 cos(−i2)
(−i −i)2 −2 sin(−i2)
(−i −i)3

= 2πi ·
2 cosh 2
−4
−−2i sinh 2
i8

= iπ

−cosh 2 + 1
2 sinh 2

. ⃝
Alternatively, it would take more work to calculate the partial fractions expansion
1

z2 + 1
2 =
i
4
z −i +
−1
4
(z −i)2 −
i
4
z + i +
−1
4
(z + i)2 ,
but after that we could use Theorem 15.39 with m = 0 and m = 1.

1210
Advanced Engineering Mathematics
15.9.2 Integration of a Laurent Series
If f(z) has a Laurent series (15.53) in Section 15.6, that is,
f(z) ≜
∞
.
j=1
a−j (z −z0)−j +
∞
.
j=0
aj (z −z0)j,
and is an analytic function in an annulus Ar,R(z0), then we can integrate term by term on a
positively oriented circle C : |z −z0| = ρ, where r < ρ < R, to get

C
f(z) dz = · · · + a−3

C
dz
(z −z0)3 + a−2

C
dz
(z −z0)2
+ a−1

C
dz
z −z0
+ a0

C
1 dz + a1

C
(z −z0) dz + · · · = · · · + 0 + 0 + a−1 · 2πi
+ 0 + 0 + · · · = 2πi a−1,
by using Theorem 15.30 in Section 15.8. This motivates a definition:
Definition 15.29
Res[ f; z0 ] = a−1 is called the residue of f at z0, assuming f has Laurent series (15.53) in
Section 15.6.
Example 15.62
Define g(z) ≜sin z
z4 . Evaluate (a) Res[ g; 0 ] and (b)

|z|=0.5
g(z) dz.
Method:
(a) Similar to the work in Example 15.39 in Section 15.6, the Maclaurin series of sin z
gives
g(z) = sin z
z4
= z−3 −1
3! z−1 +
∞
.
ℓ=1
(−1)ℓ+1
(2ℓ+ 3)!z2ℓ−1,
so Res[ g; 0 ] = a−1 = −1
3!.
(b) Method I: By the earlier calculation that motivated the definition of “residue,”

|z|=0.5
g(z) dz = 2πi · a−1 = −πi
3 .
Method II: Using Cauchy’s integral formula with m = 3,

|z|=0.5
g(z) dz = 2πi
3! ·
	
d3
dz3

sin z



at z=0 = 2πi
3! · (−cos z)

at z=0 = 2πi
3! · (−1) = −πi
3 . ⃝
In general, calculating f (m)(z) may involve a lot of work. Even though this was not true
in Example 15.62, Method I was still easier than Method II. In some problems, residues
may provide a relatively easy way to evaluate integrals.

Functions of a Complex Variable
1211
Theorem 15.41
Suppose f(z) has an isolated singularity at z0 and m is a nonnegative integer.
(a) If z0 is a removable singularity of f, then Res[ f; z0 ] = 0.
(b) If z0 is a pole of order m + 1 of f, then
Res[ f; z0 ] = 1
m! lim
z→z0
 dm
dzm
1
(z −z0)m+1 f(z)
2
.
(15.74)
(c) If z0 is a “simple pole” of f, that is, a pole of order one of f, then
Res[ f; z0 ] = lim
z→z0(z −z0) f(z) .
(15.75)
Note that (c) is a special case of (b) with m = 0.
Example 15.63
Find Res[ z cot2 z; 0 ].
Method: Define f(z) ≜z cot2 z = z · cos2 z
sin2 z
. While it may be possible to find a Laurent
series of f, it is easier to use Theorem 15.41 to find the residue. But, to do that, we must
first find the order of the pole of f(z) at z = 0.
First, we can find that sin z has a zero of order one at z = 0 by using the definition of
“zero.” After explaining that, we can use “preparation” Theorem 15.24 in Section 15.7 to
see that
sin z = (z −0)k(z)
(15.76)
for some function k(z) that is analytic at z = 0 and has k(0) ̸= 0. In fact, (15.76) implies
k(z) =
sin z
z
for z ̸= 0, and L’Hôpital’s rule, that is, Theorem 15.12 in Section 15.3,
calculates that
lim
z→0
sin z
z
= lim
z→0
(sin z)′
(z)′
= lim
z→0
cos z
1
= 1,
so
k(z) ≜
⎧
⎪⎨
⎪⎩
sin z
z
,
if z ̸= 0
1,
if z = 0
⎫
⎪⎬
⎪⎭
.
Next, we substitute (15.76) into the formula for f(z) to see that, for z ̸= 0,
f(z) = z ·
cos2 z
z2 
k(z)
2 =
cos2 z
z

k(z)
2 = 1
z ·
cos z
k(z)
2
≜g(z)
z
.
(15.77)
Note that g(z) ≜cos2 z

k(z)
2 is analytic at z = 0, because cos z and k(z) are analytic at z = 0
and k(0) ̸= 0; also, g(0) = 1. By Theorem 15.26(b) in Section 15.7 and (15.77), f(z) has a
pole of order one at z = 0.

1212
Advanced Engineering Mathematics
So, Theorem 15.41(c) allows us to calculate
Res[ z cot2 z; 0 ] = lim
z→0 z f(z) = lim
z→0 z · g(z)
z
= lim
z→0 g(z) = g(0) = 1. ⃝
15.9.3 Cauchy’s Residue Theorem
Theorem 15.42
Suppose C is a simple, closed, and positively oriented contour and D is a simply connected
domain that contains both C and the interior of C. Suppose f(z) is analytic on D except at
finitely many points or at a sequence of points z1, . . . , zk, . . .. If f(z) is analytic on the interior
of C except at finitely many points z1, . . . , zn, then

C
f(z) dz = 2πi
n
.
k=1
Res[ f; zk ] .
(15.78)
Example 15.64
Find

|z|=1.5
1
z(z + 1)(z −2) dz.
Method: The singularities of f(z) ≜
1
z(z + 1)(z −2) are at z = 0, −1, 2. Figure 15.31 depicts
the singularities as ×’s and the contour C : |z| = 1.5 with positive orientation. Two of
the singularities of f(z) are contained inside C, while z = 2 is not. By Theorems 15.41
and 15.42,

C
f(z) dz = 2πi (Res[ f; 0 ] + Res[ f; −1 ])
= 2πi

z ·
1
z(z + 1)(z −2)
 
at z=0 +


(z + 1) ·
1
z
(z + 1)(z −2)
 
at z=−1

= 2πi

−1
2 + 1
3

= −πi
3 . ⃝
y
x
2
–1
FIGURE 15.31
Example 15.62: C : |z| = 1.5 and poles of f(z).

Functions of a Complex Variable
1213
Learn More About It
The book Fundamentals of Complex Analysis, with Applications to Engineering and Science,
3rd edition, by E. B. Saff and A. D. Snider, Prentice Hall/Pearson Education, Inc., c⃝
2003, pages 207–209, has an explanation for Theorem 15.40.
15.9.4 Problems
1. Find the isolated singularities of f(z)=
1
z(z+1)(z−2) and compute the corresponding
residues.
2. Find the isolated singularities of f(z)=
1
z2(z+1) and compute the corresponding
residues.
3. Define f(z)=
1
z sin z only for z inside the disk D2( π
2 ). Find the isolated singularities
of f and compute the corresponding residues.
In problems 4–8, evaluate the integral.
4.

|z|=3
z
(z −i)(z −2i) dz
5.

|z|= 3
2
 z cos z
(z −1)2 + z cos z
z + 2

dz
6.

|z|=3
 z cos z
(z −1)2 + z cos z
z + 2

dz
7.

|z|=3
ez
z −iπ dz
8.

|z|=4
ez
z −iπ dz
In problems 9–12, evaluate the integral using Cauchy’s residue theorem.
9.

|z−1|=3
1
z2 −9 dz
10.

|z|=3
z
sin z dz
11.

|z−π
2 |=2
z
sin z dz
12.

|z|=2
z
ez + 1 dz
13. Evaluate the integral, assuming ω and ξ are unspecified constants and R > ω > 0:

z| = R
zeiξz
z2 + ω2 dz.

1214
Advanced Engineering Mathematics
14. Evaluate the integral, assuming ω and ξ are unspecified constants and R > ω > 0:

z| = R
zeiξz
z2 −ω2 dz.
15.10 Real Integrals by Complex Integration Methods
Theorem 15.29 in Section 15.8 gave our first technique for evaluating line integrals in the
complex plane by parametrizing the contour and then calculating a definite integral with
respect to a real variable.
By “real definite integral” we mean an integral with respect to a real variable. We can
use Theorem 15.29 in Section 15.8 in reverse to evaluate some difficult real definite inte-
grals by using Cauchy’s residue theorem. This will be a good example of the mathematical
“culture” of using the same results in many ways by “looking at it from many angles.”
15.10.1 Integration of Periodic Functions
Example 15.65
Evaluate
2π

0
dθ
1 + cos2 θ .
Method: At first, this seems to have no connection with complex variables. But, inte-
grating over the interval 0 ≤θ ≤2π suggests a connection with a line integral over a
circle.
Parametrize the unit circle |z| = 1 by C : z = z(θ) = eiθ, 0 ≤θ ≤2π, which has positive
orientation. For z on C,
cos θ = eiθ + e−iθ
2
= z + z−1
2
= (2z)−1(z2 + 1).
On C, we have dz
dθ (θ) = ieiθ = iz; hence, dθ = dz
iz . By Theorem 15.29 in Section 15.8,
2π

0
1
1 + cos2 θ dθ =

|z|=1
1
1 +

(2z)−1(z2 + 1)
2
dz
iz =

|z|=1
(2z)2
(2z)2 +

z2 + 1
2
dz
iz
= −4i

|z|=1
z
4z2 +

z2 + 1
2 dz = −4i

|z|=1
z
z4 + 6z2 + 1 dz.
The four singularities are where the denominator is zero, that is, where 0 = z4+6z2+1 =

z22 + 6z2 + 1, that is, where
z2 = −6 ±

62 −4
2
= −3 ± 2
√
2,

Functions of a Complex Variable
1215
that is,
z = ±i

3 + 2
√
2,
±i

3 −2
√
2.
Of these singularities, only z3 = i

3 −2
√
2 and z4 = −i

3 −2
√
2 are inside C. Because
f(z) ≜
z
z4 + 6z2 + 1 =
z

z2 + 3 + 2
√
2
 
z −i

3 −2
√
2
 
z + i

3 −2
√
2
 ,
both z3 and z4 are simple poles of f(z). It is relatively straightforward to use Theorem
15.41(c) in Section 15.9 to calculate that
2π

0
1
1 + cos2 θ dθ = −4i

|z|=1
f(z) dz = −4i · 2πi

Res[ f(z); z3 ] + Res[ f(z); z4 ]

= · · · = 8π

1
8
√
2
+
1
8
√
2

= π
√
2. ⃝
In principle, this technique will work for any real integral of the form
2π

0
h(cos θ, sin θ) dθ,
where h(cos θ, sin θ), for example, 1 + sin θ + 3 cos2 θ
2 + sin2 θ
, is a rational function of cos θ and
sin θ whose denominator is never zero for 0 ≤θ ≤2π.
15.10.2 Improper Integrals over (−∞, ∞), [0, ∞), or (−∞, 0]
Example 15.66
Evaluate
∞

0
dx

x2 + 3
2 .
Method: Define g(x) =
1

x2+3
2 and f(z) =
1

z2+3
2 . We will explain why the improper
integral
 ∞
−∞g(x) dx exists and equals
2πi Res[ f(z); i
√
3 ] = · · · =
π
12
√
3
.
First, the improper integral
 ∞
−∞g(x) dx exists because both
lim
b→∞
b
0
dx

x2 + 3
2
and
lim
a→−∞
0
a
dx

x2 + 3
2

1216
Advanced Engineering Mathematics
y
x
R
–R
CR,1
–i√3
CR,2
i√3
FIGURE 15.32
Complex integration to evaluate a real integral.
exist, by a comparison theorem for definite integrals. Because both of the improper
integrals
 ∞
0
g(x)dx and
 0
−∞g(x)dx are convergent, we have
∞

−∞
g(x) dx = lim
R→∞
0
−R
g(x) dx + lim
R→∞
R
0
g(x) dx = lim
R→∞
R
−R
g(x) dx.
Consider the contour CR = CR,1 + CR,2 shown in Figure 15.32, which also indicates by
× the singularities of f(z) at z = ±i
√
3. We calculate

CR,2
f(z) dz =

CR,2
1

z2 + 3
2 dz =
π
0
1

Reiθ2 + 3
2 iReiθ dθ =
π
0
1

R2ei2θ + 3
2 iReiθ dθ.
Using Lemma 15.1 in the following,


CR,2
f(z) dz

=

π
0
1

R2ei2θ + 3
2 iReiθ dθ

≤
π
0

1

R2ei2θ + 3
2 iReiθ
 dθ
≤
π
0
R

R2 −3
2 dθ →0,
as
R →∞.
Also, CR,1 : z = x + i0, −R ≤x ≤R, so

CR,1
f(z) dz =
R
−R
g(x) dx.
Putting things together, we have
lim
R→∞

CR,1+CR,2
f(z) dz = lim
R→∞
⎛
⎝

CR,2
f(z) dz +
R
−R
g(x) dx
⎞
⎠= 0 +
∞

−∞
g(x) dx.
For R >
√
3,

CR,1+CR,2
f(z) dz = 2πi Res[ f(z); i
√
3 ].

Functions of a Complex Variable
1217
Finally, use the fact that g(x) is an even function to get
∞

0
dx

x2 + 3
2 = 1
2
∞

−∞
g(x) dx = 1
2
lim
R→∞

CR,1+CR,2
f(z) dz
= 1
2 2πi Res[ f(z); i
√
3 ] = · · · = π
√
3
36 .
In the earlier calculations, the last step was to use the fact that f(z) =
1

z−i
√
3
2
z+i
√
3
2
has a pole of order two at z = i
√
3, along with Theorem 15.41(b) in Section 15.9. ⃝
Lemma 15.1
For R >
√
3, and all θ in the interval [0, π],

1

R2ei2θ + 3
2 ieiθ
 ≤
1

R2 −3
2 .
(15.79)
Why? The triangle inequality, |z1 + z2| ≤|z1| + |z2|, implies that |z1| ≥|z1 + z2| −|z2|. So,
defining z1 = R2ei2θ + 3 and z2 = −3, we have
|R2ei2θ + 3| = |z1| ≥|z1 + z2| −|z2| = |R2ei2θ + 3 −3| −| −3| = |R2ei2θ| −3
= |R2| |ei2θ| −3 = R2 · 1 −3 = (R2 −3).
It follows that
1
|R2ei2θ + 3| ≤
1
(R2 −3)
and thus that

1

R2ei2θ + 3
2 ieiθ
 =
1
|R2ei2θ + 3|2 · |ieiθ| =

1
|R2ei2θ + 3|
2
· 1 ≤
1
(R2 −3)2 . 2
The reasoning and techniques we used in Example 15.66 generalize to a theorem.
Theorem 15.43
Suppose p(x)/q(x) is a rational function, degree

q(x)

≥2 + degree

p(x)

, and q(x) has no
real zeros. Then
∞

−∞
p(x)
q(x) dx = 2πi
n
.
k=1
Res
1 p(z)
q(z); zk
2
,
(15.80)
where z1, . . . , zn are the zeros of q(z) in the upper half plane Im(z) ≥0.

1218
Advanced Engineering Mathematics
15.10.3 Cauchy Principal Value
As we saw in Section 7.1, an improper integral
 ∞
−∞g(x) dx may fail to exist even
though limR→∞
 R
−R g(x) dx

exists. Nevertheless, it can be useful in solving differential
equations and other subjects to acknowledge the latter limit’s value.
Definition 15.30
The Cauchy principal value of
 ∞
−∞g(x) dx is defined by
P. v.
∞

−∞
g(x) dx ≜lim
R→∞
⎛
⎝
R
−R
g(x) dx
⎞
⎠,
if the latter exists.
If |g(x)| is integrable on (−∞, ∞), then the principal value equals
 ∞
−∞g(x) dx.
Example 15.67
As we saw in Example 7.3 in Section 7.1,
P. v.
∞

−∞
2x
x2 + 1 dx = 0
even though the improper integral is divergent. ⃝
Example 15.68
Find P. v.
 ∞
−∞
cos ωx
x2 + 1 dx, where ω is a real constant.
Method: Because cos ωx = Re

eiωx
, we have
P. v.
∞

−∞
cos ωx
x2 + 1 dx = Re
	
P. v.
∞

−∞
1
x2 + 1 eiωx dx

.
Let f(z) ≜
1
z2 + 1 eiωz and use the same contour CR = CR,1 + CR,2 found in Example 15.66
and shown in Figure 15.32. At any point on CR,2 given by z = Reiθ = x + iy that lies in
the upper half plane, that is, has y > 0, we have
eiωz =
eiω(x+iy) =
eiωx
e−ωy = 1 · e−ωy < 1.

Functions of a Complex Variable
1219
So,


CR,2
1
z2 + 1 eiωz dz

=

π
0
1

Reiθ2 + 1
eiωReiθ iReiθ dθ

≤
π
0
eiω(x+iy)
|R2ei2θ + 1| ·
iReiθ dθ
<
π
0
R
R2 −1 dθ →0,
as
R →∞.
So, similar to the work of Example 15.66, the convergent improper integral is given by
∞

−∞
cos ωx
x2 + 1 dx = P. v.
∞

−∞
cos ωx
x2 + 1 dx ≜lim
R→∞
⎛
⎝
R
−R
cos ωx
x2 + 1 dx
⎞
⎠
= lim
R→∞Re
⎛
⎝

CR,1
1
z2 + 1 eiωz dz
⎞
⎠= Re
	
2πi Res
1
eiωz
z2 + 1; i
2
= Re
	
2πi
	
eiωz
z + i

at z=i


= Re
	
2πi eiω i
i + i

= π e−ω. ⃝
15.10.4 Hilbert Transform
Suppose f(z) = u(x, y)+iv(x, y) is analytic on the upper half plane, y > 0, and is continuous
on the real axis, y = 0. Suppose also that

CR,2

f(z)
z
 dz →0
as R →∞,
where, as previously mentioned, CR,2 : z = R eiθ, 0 ≤θ ≤π. Using Cauchy’s integral
formula, we have
f(z) =
1
2πi

CR
f(ζ)
ζ −z dζ,
for z in the upper half plane, where, as previously mentioned CR = CR,1 + CR,2 and CR,1 :
z = x + i0, −R ≤x ≤R. Further details in a book by Bernard Friedman explain why
u(x, 0+) = P. v. 1
π
∞

−∞
v(ξ)
ξ −x dξ,
(15.81)
and
v(x, 0+) = −P. v. 1
π
∞

−∞
u(ξ)
ξ −x dξ,
(15.82)

1220
Advanced Engineering Mathematics
for −∞< x < ∞. Equation (15.81) says that u(x, 0+) is the Hilbert transform of its har-
monic conjugate v(x, y) and v(x, 0+) is the Hilbert transform of its harmonic conjugate
−u(x, y); the latter uses the result of Problem 15.3.3.21. So, (15.82) really says the same
thing as (15.81). Hilbert transforms are useful in signal processing. An example of this is
discussed in a book by Saff and Snider.
Learn More About It
Useful discussions of the Hilbert transform are in Section 5.1 of Lectures on Applications-
Oriented Mathematics, by Bernard Friedman, ed. by Victor Twersky, Holden-Day Inc.,
c⃝1969 (reprinted by Wiley Classics Library, c⃝1991), and in Section 8.5 of Funda-
mentals of Complex Analysis, with Applications to Engineering and Science, 3rd edn., by
E. B. Saff and A. D. Snider, Prentice Hall/Pearson Education, Inc., c⃝2003.
If an integral involves a square root, then one needs to integrate over a contour
which bends around the branch cut on a half line. Good references for this include
Operational mathematics, 3rd edn., by Ruel V. Churchill, McGraw-Hill c⃝1971, and the
book by Saff and Snider cited earlier.
15.10.5 Problems
In problems 1–9, evaluate, that is, find the exact value, of the integral.
1.
 2π
0
1
1 + sin2 θ
dθ

Hint: Recall sin θ = 1
2i

eiθ −e−iθ
2.
 π
0
1
1 + cos2 θ dθ [Hint: Use symmetry of cos2 θ on the interval [0, π] vs. [π, 2π].]
3.
 π
0
1
1 + sin2 θ
dθ [Hint: Use symmetry of sin2 θ on the interval [0, π] vs. [π, 2π].]
4.
 ∞
−∞
1

x2 + 4
2 dx
5.
 ∞
−∞
x2

x2 + 4
2 dx
6.
 2π
0
sin2 θ
1 + cos2 θ dθ
7.
 2π
0
1
2 + cos2 θ dθ
8. P. v.
 ∞
−∞
x sin ωx
x2 + 1 dx
9. P. v.
 ∞
−∞
x2 cos ωx
x4 + 1
dx

Functions of a Complex Variable
1221
Key Terms
absolute value: beginning of Section 15.1
analytic: Definition 15.5 in Section 15.3 and Definition 15.6 in Section 15.3
anti-derivative: Definition 15.26 in Section 15.8
arg(z): (15.23) in Section 15.4
Arg(z): (15.24) in Section 15.4
bounded: before Theorem 15.32 in Section 15.8
“branch” of log(z): (15.34) in Section 15.4
Cauchy principal value: Definition 15.30 in Section 15.10
Cauchy-Riemann equations: (15.14) in Section 15.2
complex: beginning of Section 15.1
complex conjugate: beginning of Section 15.1
connected: Definition 15.9 in Section 15.3
continuous: Definition 15.2 in Section 15.2
contour, simple, closed: Definition 15.24 in Section 15.8
converges absolutely: Definition 15.15 in Section 15.6
converges uniformly: Definition 15.17 in Section 15.6
deformation: Definition 15.27 in Section 15.8
DeMoivre’s Theorem: after (15.5) in Section 15.1
derivative: Definition 15.4 in Section 15.2
differentiable: Definition 15.4 in Section 15.2
directed line segment: Definition 15.8 in Section 15.3
discrete convolution: end of Section 15.6
diverges: Definition 15.15 in Section 15.6
domain: Definition 15.9 in Section 15.3
entire: Definition 15.7 in Section 15.3
equipotential: Example 15.14 in Section 15.2
essential singularity: Definition 15.22 in Section 15.7
exp(z): (15.26) in Section 15.4
Euler’s formula: (15.3) in Section 15.1
extension: Definition 15.3 in Section 15.2
exterior: after Theorem 15.32 in Section 15.8
function of a complex variable: before Example 15.4 in Section 15.1
geometric series: (15.43) in Section 15.6
harmonic: Definition 15.10 in Section 15.3
harmonic conjugate: Definition 15.11 in Section 15.3
Hilbert transform: (15.81) in Section 15.10, (15.82) in Section 15.10
holomorphic: after Definition 15.6 in Section 15.3
image: before Example 15.4 in Section 15.1
imaginary: beginning of Section 15.1
imaginary part: beginning of Section 15.1
incompressible: Example 15.14 in Section 15.2
initial point: Definition 15.23 in Section 15.8
interior: after Theorem 15.32 in Section 15.8
inversion mapping: before Example 15.5 in Section 15.1
irrational: Problem 15.1.4.10

1222
Advanced Engineering Mathematics
isolated singularity: Definition 15.21 in Section 15.7
Laurent series: Definition 15.19 in Section 15.6
limit of f(z) exists: Definition 15.1 in Section 15.2
log(z): (15.28) in Section 15.4
Log(z): (15.29) in Section 15.4
Logσ(z): (15.34) in Section 15.4
mapping: before Example 15.4 in Section 15.1
modulus: beginning of Section 15.1
open: before Theorem 15.9 in Section 15.3
open disk: before Definition 15.1 in Section 15.2
open annulus: Definition 15.18 in Section 15.6
opposite curve: Definition 15.25 in Section 15.8
orthogonal families: before Example 15.14 in Section 15.2
parametrized curve: Definition 15.23 in Section 15.8
piecewise smooth curve: Definition 15.23 in Section 15.8
polar exponential form: (15.4) in Section 15.1
polar form: (15.1) in Section 15.1
pole of order m: Definition 15.22 in Section 15.7
polygonal curve: Definition 15.8 in Section 15.3
polynomial in z of degree n: before Theorem 15.3 in Section 15.2
positively oriented: Definition 15.23 in Section 15.8 and Definition 15.24 in Section 15.8
potential flow: Example 15.14 in Section 15.2
power function: (15.35) in Section 15.4
power series about: (15.42) in Section 15.6
principal branch or principal value of the power function: after (15.35) in Section 15.4
principal n-th root of unity: before Example 15.4 in Section 15.1
punctured disk: before Definition 15.1 in Section 15.2
punctured plane: Example 15.23 in Section 15.3
radius of convergence: Definition 15.16 in Section 15.6
real: beginning of Section 15.1
real analytic: after (15.49) in Section 15.6
real part: beginning of Section 15.1
removable singularity: Definition 15.22 in Section 15.7
residue of f: Definition 15.29 in Section 15.9
set union: after Example 15.31 in Section 15.5
simple, closed curve: Definition 15.23 in Section 15.8
simply connected: Definition 15.28 in Section 15.8
2-D Stokes potential fluid flow: after Example 15.23 in Section 15.3
streamlines: after Example 15.14 in Section 15.2
Taylor Series: Theorem 15.22 in Section 15.6
terminal point: Definition 15.23 in Section 15.8
truncate: before Theorem 15.21 in Section 15.6
zero of order m: Definition 15.20 in Section 15.7

16
Conformal Mapping
16.1 Conformal Mappings and the Laplace Equation
16.1.1 Linear Mappings
A linear mapping is a function defined by f(z) = az + b, where a ̸= 0 and a and b are
constants that may be complex. Here are special cases:
1. If f(z) = z + b, then f is a translation.
2. If f(z) = αz and α is real, then f is a magnification (or compression or dilation). In
the future we may refer to a mapping as a magnification even if |α| < 1.
3. If f(z) = eiϕz, where ϕ is real, then f is a rotation, because f(reiθ) = rei(θ+ϕ).
Figure 16.1 illustrates this for ϕ = π
6 .
4. If f(z) = az, where a = αeiϕ and α and ϕ are real, then f is a rotation followed by a
magnification.
Finally, we see that a linear mapping f(z) = az + b is a rotation, followed by a
magnification, followed by a translation, as illustrated in Figure 16.2.
We observe that a linear mapping “preserves angles and orientation between curves.”
Let’s check this symbolically: Suppose we have two parametrized curves that intersect at
a point z0, that is,
C1 : z = z1(t), −1 ≤t ≤1
and
C2 : z = z2(t), −1 ≤t ≤1
with
z0 = z1(0) = z2(0) .
(16.1)
At z0 the respective tangent vectors to C1 and C2 are ˙z1(0) and ˙z2(0), assuming both are
nonzero. This is illustrated in Figure 16.3.
Under the linear mapping f(z) = az + b, the images of the parametrized curves C1 and
C2 are f(C1) ≜{ f(z1(t)) : −1 ≤t ≤1} and f(C2) ≜{ f(z2(t)) : −1 ≤t ≤1}, so at the point
f(z0), the tangent vectors are a˙z1(0) and a˙z2(0), respectively. (We assume that a ̸= 0 as part
of the definition of “linear mapping.”)
Write ˙z1(0) = |˙z1(0)| eiθ1 and ˙z2(0) = |˙z2(0)| eiθ2. Without loss of generality, −π < θ1 <
θ2 ≤θ1 + π. Then the angle between ˙z1(0) and ˙z2(0) is θ2 −θ1. Writing a = αeiϕ, where α
1223

1224
Advanced Engineering Mathematics
y
y
B
C
D
O
O
B΄
C΄
D΄
x
x
1
1
π
6—
√3
1
2
– —
FIGURE 16.1
f(z) = eiπ/6 z.
and ϕ are real, the angle between a˙z1(0) = α|˙z1(0)| ei(θ1+ϕ) and a˙z2(0) = α|˙z2(0)| ei(θ2+ϕ) is
(θ2 + ϕ) −(θ1 + ϕ) = θ2 −θ1. This explains why all linear maps preserve angles between
curves as well as the orientation sgn(θ2 −θ1).
We will say that f is “conformal at z0” if for every two curves C1 and C2 that intersect at
z0, the angle from C1 to C2 equals the angle from f(C1) to f(C2) at f(z0). A conformal map
need not be linear.
Theorem 16.1
Suppose f is differentiable at z0. Then f is conformal at z0 if, and only if, f ′(z0) ̸= 0.
Why? Suppose C1 and C2 are parametrized as in (16.1). By the chain rule,
d
dt

f(z1(t))

=
f ′(z1(t))˙z1(t). So, the tangent vectors to f(C1) and f(C2) at f(z0) are f ′(z0)˙z1(0) and f ′(z0)˙z2(0),
respectively, if f ′(z0) ̸= 0; however, if f ′(z0) = 0, then there are no tangent vectors to the
parametrized curves f(C1) and f(C2) at f(z0).
Similar to the explanation why a linear mapping preserves angles, if f ′(z0) ̸= 0, then the
angle between f(C1) and f(C2) at f(z0) is

Arg

f ′(z0)

+ θ2

−

Arg

f ′(z0)

+ θ1

= θ2 −θ1,
so f does preserve angles. 2
Recall Definition 15.9(b) in Section 15.3.
Definition 16.1
Suppose f(z) is defined on a domain D.
(a) f is conformal on D if f is analytic on D and f ′(z0) ̸= 0 at every z0 in D.
(b) f is conformal at a boundary point z0 if
lim
z is in D and z→z0
f(z) −f(z0)
z −z0
exists and is ̸= 0.

Conformal Mapping
1225
y
y
B
C
D
O
O
B΄
C΄
D΄
x
x
1
1
π
6—
y
O
B˝
C˝
D˝
x
1
2
– —
π
6—
√3
√3
y
O
B˝΄
C˝΄
D˝΄
x
1
2
– —
√3
1
2
– —
FIGURE 16.2
f(z) = az + b.

1226
Advanced Engineering Mathematics
z2(0)
.
z1(0)
.
FIGURE 16.3
Tangent vectors ˙z1(0), ˙z2(0).
Example 16.1
f(z) ≜1+z
1−z is defined on the domain D1(0) = {z : |z| < 1}. Study conformality of f(z) on
its domain and at its boundary points.
Method: f ′(z) =
2
(1−z)2 , so f is conformal on domain D1(0) and also at every boundary
point of D1(0) except at z = 1. ⃝
Theorem 16.2
If g is conformal at z0 and f is conformal at g(z0), then f(g(z)) is conformal at z0.
Why? This is a straightforward consequence of the chain rule. 2
16.1.2 Harmonic Functions
Recall that a function φ is harmonic on a domain D if φ(x, y) satisfies Laplace’s equation
∂2φ
∂x2 + ∂2φ
∂y2 = 0
on D. Also, recall that if F(x, y) = φ(x, y) + iψ(x, y) is analytic on D, then both φ and its
harmonic conjugate ψ are harmonic on D.
A conformal mapping can transform Laplace’s equation in a complicated domain into
Laplace’s equation in a simpler domain. After solving Laplace’s equation in the simpler
domain, we can use the inverse of the conformal mapping to get a solution of Laplace’s
equation in the original domain. This fits into a general mathematical method of reducing
a new difficult problem to a simpler problem or even to a problem for which the solution
is well known.
To use this method, we need two results:
Theorem 16.3
If w = f(z) is conformal on domain D in the z-plane then (a) f(D) is a domain in the
w- plane, and (b) there exists an inverse map z = f −1(w) that is conformal on f(D).

Conformal Mapping
1227
Under the assumptions of Theorem 16.3, f(f −1(w)) = w on f(D) and f −1(f(z)) = z on D.
Theorem 16.4
If f(z) is conformal on domain D, w = u + iv = f(z), and 	 = 	(u, v) is harmonic on
domain E = f(D), then φ(x, y) ≜	

f(x + iy)

is harmonic on D.
Why? Let z0 be any point in D and denote w0 = f(z0). Because 	 is harmonic on E, Theorem
15.14 in Section 15.3 implies that there exists a local harmonic conjugate 
 such that g(w) ≜
	(u, v) + i
(u, v) is analytic on some open ball Bδ(w0) contained in E. This implies that
	

f(x + iy)

+ i

f(x + iy)

= g

f(x + iy)

is analytic on f −1 (Bδ(z0)); hence, 	

f(x + iy)

is harmonic on f −1 (Bδ(z0)). Since this is true
for any z0 in D, φ(x, y) is harmonic on D. 2
16.1.3 Elementary Functions
We will study the conformality of some familiar functions: exponential, trigonometric,
logarithmic, and a new class of functions called “Möbius transformations.”
Example 16.2
Why is ez conformal everywhere?
Method: At all z, there exists (ez)′ = ez, and for all z = x + iy with x, y real, |(ez)′| =
|ex+iy| = |ex eiy| = |ex| |eiy| = ex · 1 ̸= 0. ⃝
Example 16.3
Where is sin z conformal?
Method: At all z, there exists (sin z)′ = cos z, so all we have to do is to find the solutions
of cos z = 0. For all real x, y, cos(x + iy) = · · · = cos x cosh y −i sin x sinh y. Separating
the real and imaginary parts of the equation cos(x + iy) = 0 + i0 gives the system of
equations
(1)
0 = cos x cosh y
(2)
0 = sin x sinh y

.
Because cosh y > 0 at all real y, the only solutions of equation (1) have x =

n −1
2
	
π.
Substitute that into equation (2) to get
0 = sin


n −1
2
	
π

sinh y = (−1)n+1 sinh y,
so only y = 0 solves equation (2) for those values of x.
So, sin z is conformal everywhere except at the points z =

n −1
2
	
π + i0. ⃝

1228
Advanced Engineering Mathematics
16.1.4 Möbius Transformations
Möbius transformations are defined by
w = M(z) ≜az + b
cz + d ,
(16.2)
where a, b, c, d are constants that may be complex and ad −bc ̸= 0. The latter assumption
implies at least one of c and d is nonzero. We will see in Section 16.2 that having Möbius
transformations in our “library” will help us find a mapping to circles and lines, and in
Section 16.3, we will use this to solve Laplace’s equation in some complicated domains.
The assumption that ad −bc ̸= 0 guarantees that M(z) is not a constant function. Why?
As we can see in the case when c ̸= 0,
M(z) =
a
c (cz + d) + b −ad
c
cz + d
= a
c −ad −bc
c(cz + d) .
(16.3)
In Problem 16.1.5.1, you will discuss the criterion ad −bc ̸= 0 in the case when c = 0
but d ̸= 0.
We calculate that
M′(z) = ad −bc
(cz + d)2
is defined and nonzero everywhere except at z = −d
c .
Theorem 16.5
Suppose a Möbius transformation is defined by (16.2):
(a) If c ̸= 0, then M(z) is conformal except at z = −d
c .
(b) If c = 0, then M(z) is a linear mapping that is conformal everywhere.
Note that the basic assumption ad −bc ̸= 0 implies a ̸= 0 in case (b), that is, if c = 0.
The logarithmic transformation w = Log(z) = ln |z| + iArg(z) is clearly conformal every-
where except on the nonpositive real axis.
Example 16.4
Where is the mapping w = h(z) ≜Log

1+z
1−z
	
conformal?
Method: Define ζ = g(z) ≜
1+z
1−z. By Theorem 16.2, h(z) is conformal wherever g(z) is
both conformal and not equal to a nonpositive real number. By Theorem 16.5, g(z) is
conformal everywhere except at z = 1.

Conformal Mapping
1229
–8
–6
–4
–2
–2
–4
2
4
6
z
ζ
–6
FIGURE 16.4
Example 16.4.
For which z is ζ = 1+z
1−z a nonpositive real number? To answer this, it helps to invert
the dependence to get z as a function of ζ:
ζ = 1 + z
1 −z ⇐⇒(1 −z)ζ = 1 + z ⇐⇒ζ −1
= (1 + ζ)z ⇐⇒z = k(ζ) ≜ζ −1
ζ + 1 = 1 −
2
ζ + 1.
The set of all z for which ζ is real and ≤0 is the set k

(−∞, 0]

, as shown in Figure 16.4.
Basic calculus graphical techniques give that ζ is real and ≤0 ⇐⇒z ≤−1 or z > 1.
So, the set of z for which ζ is not a nonpositive real number is everywhere except the
intervals (−∞, −1] and (1, ∞) on the real axis.
Finally, we conclude that g(z) is conformal on
{z : Im(z) ̸= 0} ∪{z : z is real and −1 < z < 1} . ⃝
Often, using the inverse of a mapping is a valuable technique for understanding the
original mapping.
16.1.5 Problems
1. In the case when c = 0 and d ̸= 0, why is the Möbius transformation given in (16.2)
not constant if, and only if, ad −bc ̸= 0?
In problems 2–5, find where the given function is conformal.
2. Log

2−z
2+z
	
3. Log

3+z
3−z
	
4. cos z

1230
Advanced Engineering Mathematics
5. cosh z
6. Use the fact that sin z = 1
2i(eiz −e−iz) to solve Example 16.3, that is, find where sin z
conformal, by another method.
16.2 Möbius Transformations
Visualizing the effects of a mapping of the complex plane will help us find and use map-
pings to solve the Laplace equation in Section 16.3. As we saw in Section 15.1, one way to
visualize a mapping is to show a region A in the z-plane along with its image, f(A), in the
w-plane. In particular, it is useful to know the images of lines and circles under commonly
used mappings such as Möbius transformations, given by (16.2) in Section 16.1, that is,
w = M(z) = az + b
cz + d .
16.2.1 Circles, Lines, and Möbius Transformations
Other than linear mappings, the simplest Möbius transformation is the inversion mapping
given by
w = f(z) = 1
z .
(16.4)
Lemma 16.1
For any R > 0 and z0 in C, define the circle C = {z : |z −z0| = R}.
(a) If 0 is not in C, that is, |z0| ̸= R, then the image of C under the inversion mapping
is the circle
f(C) =

w : |w −w0| =
R
 |z0|2 −R2 

(16.5)
whose center is at
w0 =
z0
|z0|2 −R2 .
(16.6)
(b) If 0 is in C, that is, |z0| = R > 0, then the image of 
C ≜{z ̸= 0 : |z −z0| = R} under
the inversion mapping is

w =
1
1 + eiϕ · 1
z0
: 0 < ϕ ≤2π, ϕ ̸= π

,
(16.7)
which is all of the points on a line that passes through
1
2z0 .

Conformal Mapping
1231
y
z0
x
1
2z0
FIGURE 16.5
Lemma 16.1(b).
Why? In Problem 16.2.5.23, you will derive the result in part (a). For part (b), we note that
every z in 
C can be written in the form
z = z0 + |z0| eiθ = z0 + z0 e−iArg(z0) eiθ = z0 + z0 eiϕ
for some ϕ ̸= π with 0 < ϕ ≤2π. Under the inversion map, its image is
w = 1
z = 1
z0
1
1 + eiϕ = 1
z0
1 + e−iϕ
|1 + eiϕ|2 = · · · = 1
z0

 1 + cos ϕ
2(1 + cos ϕ) −i
sin ϕ
2(1 + cos ϕ)

=
1
2z0
−i

sin ϕ
1 + cos ϕ

1
2z0
.
Figure 16.5 illustrates the image, which is a line whose direction vector is −i
2z0 . Multiplica-
tion of a complex number by −i rotates it by −π
2 , and in Problem 16.2.5.25, you will explain
why

sin ϕ
1+cos ϕ : 0 < ϕ ≤2π with ϕ ̸= π

= (−∞, ∞).
So, the image of 
C under the inversion map is a line passing through the point
1
2z0 . 2
Here are special cases: If z0, the center of the circle that passes through the origin, is real,
then under the inversion map the image of the circle is a vertical line in the w-plane; if z0
is imaginary, then the image is a horizontal line.
Recall the basic assumption ad−bc ̸= 0. Assuming also c ̸= 0, the Möbius transformation
defined in (16.2) in Section 16.1, that is, w = M(z) = az+b
cz+d, can be written as in (16.3) in
Section 16.1, that is,
M(z) = a
c −ad −bc
c(cz + d).

1232
Advanced Engineering Mathematics
Briefly we will explain why the image of every circle is either a circle or a line: Suppose C
is any circle and −d
c is not in C. By (16.3) in Section 16.1, M can be written as composition
of linear mappings and the inversion mapping, specifically
M(z) = g3

g2

f

g1(z)

,
where
g1(z) ≜cz + d
f(w1) ≜
1
w1
g2(w2) ≜ad−bc
c
w2
g3(w3) ≜a
c −w3
Each of the linear maps g1, g2, g3 maps circles to circles and maps lines to lines. In addi-
tion, because we assumed −d
c is not in C, w1 ≜cz + d ̸= 0 is not in the image g1(C).
By Lemma 16.1(a), f(g1 (C)) is a circle. Because g2 and g3 are linear mappings, M(C) =
g3

g2

f

g1(C)

is a circle.
On the other hand, suppose C is any circle and −d
c is in C. Then w1 = 0 is in the image
g1 (C), so Lemma 16.1(b) implies f


g1 (C)
	
is a line, where

g1 (C) ≜

w1 ̸= 0 : w1 = g1(z) for some z in C

.
Because g2 and g3 are linear mappings, M(C) = g3

g2

f


g1 (C)
			
is a line in the w-plane.
If L is a line in the z-plane, then there are likewise two cases depending on whether 0 is
in g1 (L). As before, assume f is the inversion mapping. We define a punctured line to be
all points on a line except for one excluded point.
Lemma 16.2
Define a line L = {z = z0 + tz1 : −∞< t < ∞}, where z0 and z1 are complex numbers:
(a) If 0 is not in L, then f(L) ∪{0} is a circle through the origin.
(b) If 0 is in L, then, defining L = {z ̸= 0 : z is in L}, we have f(L) as a punctured line.
Why?
u + iv = w = 1
z ⇐⇒x + iy = z = 1
w =
1
u + iv =
u
u2 + v2 + i
−v
u2 + v2
is on the line L, that is, has x + iy = z = z0 + tz1 = x0 + tx1 + i(y0 + ty1) for some t. It
follows that
u
u2 + v2 −x0 = tx1
and
ty1 =
−v
u2 + v2 −y0;

Conformal Mapping
1233
hence,
uy1
u2 + v2 −x0y1 = tx1y1 =
−vx1
u2 + v2 −y0x1.
So,
y1u + x1v =

x0y1 −y0x1

(u2 + v2) .
(16.8)
(a) If 0 is not in L, then the vectors z0 and z1 are linearly independent, that is,
0 ̸=

x0
x1
y0
y1
 = x0y1 −x1y0.
In this case, we can divide (16.8) by

x0y1 −x1y0

to get
u2 + v2 =
1
x0y1 −x1y0

y1u + x1v

,
which is a circle whose center is at
u0 + iv0 =
y1
2(x0y1 −x1y0) + i
x1
2(x0y1 −x1y0),
passes through the origin (u, v) = (0, 0), and has radius

x2
1 + y2
1
2 |x0y1 −x1y0| .
(b) If 0 is in L, then L = {tz1 : −∞< t < ∞}. Define L = {tz1 : t ̸= 0} = {z ̸= 0 :
z is in L}. Then
f(L) =
1
t · 1
z1
: t ̸= 0

is a punctured line. 2
16.2.2 Mapping Two Given Circles to Two Concentric Circles
In Section 11.5, we saw how to solve the Laplace equation inside a disk, outside a disk, or
inside an annulus. The latter is the region between two concentric circles. Suppose we want
to solve the Laplace equation in a region D that lies between two nonconcentric circles, that
is, between two eccentric circles.
Here we will see that we can find a Möbius transformation M that maps D to an annulus
in the w-plane. In Section 16.3, we will solve the Laplace equation in the annulus and thus
find a solution of the Laplace equation in the original region, D, using Theorem 16.4 in
Section 16.1.

1234
Advanced Engineering Mathematics
Lemma 16.3
If three points w1, w2, and w3 do not lie on a line, that is, are not collinear, then there is
exactly one circle that contains w1, w2, and w3.
Why? You will explain why in Problem 16.2.5.24.
Theorem 16.6
Suppose real numbers δ ̸= 0 and γ > 0 are given, and denote circles C1 : |z| = 1 and
C2 : |z −δ| = γ . Suppose C1 and C2 have no point in common. Then we can find real
numbers α, β such that the Möbius transformation∗
w = M(z) = z −α
z −β
(16.9)
maps each of the circles C1 and C2 to concentric circles centered at 0 in the w-plane, as long
as it turns out that
z = β is on neither C1 nor C2 .
(16.10)
Here,
β = 1
α ,
(16.11)
the real number α satisfies
δα2 + (γ 2 −δ2 −1)α + δ = 0,
(16.12)
and the images of the circles are
M(C1) : |w| = |α|
and
M(C2) : |w| =

δ −α
γ
 .
(16.13)
Why? Note that δ ̸= 0 and (16.12) imply α ̸= 0. Dividing (16.12) by (−α) and substituting
in (16.11) implies that
(δ −α)(δ −β) = γ 2 .
(16.14)
In Problem 16.2.5.19, you will explain why the image of C1 : |z| = 1 is M(C1) : |w| = |α|.
To explain why the image M(C2) is the circle |w| =
 δ−α
γ
, first we note that M(C2) is a
circle by Lemma 16.1 and the assumption that z = β is not on C2. By Lemma 16.3, it will
∗This theorem is an interpretation of results in Keener (1988). It appears that the conclusions and derivation of
theorem 16.6 give a specific and useful new method.

Conformal Mapping
1235
suffice to show that M maps three distinct (hence, not collinear) points on the circle C2 to
three distinct (hence, not collinear) points on the circle |w| =
 δ−α
γ
.
The points z = δ + γ and δ ± iγ are distinct and lie on the circle C2. Our calculations as
follows will establish that |M(δ + γ )| = |M(δ ± iγ )| =
 δ−α
γ
. First,
|M(δ ± iγ )| =

δ ± iγ −α
δ ± iγ −β
 =

(δ −α) ± iγ
(δ −β) ± iγ
 =

((δ −α) ± iγ )γ
((δ −β) ± iγ )γ
 =

(δ −α)γ ± iγ 2
((δ −β) ± iγ )γ

=

(δ −α)γ ± i(δ −α)(δ −β)
((δ −β) ± iγ )γ
=

(δ −α)(γ ± i(δ −β))
((δ −β) ± iγ )γ
= |δ −α|((((((
(
|γ ± i(δ −β)|
|γ |((((((
|δ −β) ± iγ |
= |δ −α|
|γ |
,
because |γ ± i(δ −β)| =

γ 2 + (δ −β)2 = |(δ −β) ± iγ |.
Also, we calculate
|M(δ + γ )|=

δ + γ −α
δ + γ −β
=

((δ −α) + γ )γ
((δ −β) + γ )γ
=

(δ −α)γ + γ 2
((δ −β) + γ )γ
=

(δ −α)γ + (δ −α)(δ −β)
((δ −β) + γ )γ

=

(δ −α)(γ + (δ −β))
((δ −β) + γ )γ
 = |δ −α| ((((((
|γ + (δ −β)|
|γ | ((((((
|δ −β) + γ |
= |δ −α|
|γ |
.
All that remains is to explain why the points M(δ ± iγ ), M(δ + γ ) are distinct.
In Problem 16.2.5.20 you will explain why β = 1
α , along with Theorem 16.6’s hypothesis
(16.10), that is, “z = β is on neither C1 nor C2,” imply that the solutions of quadratic
equation (16.12) are not α = ±1. In Problem 16.2.5.21, you will derive that
M(δ + iγ ) = M(δ −iγ ) ⇐⇒α = β ⇐⇒α = ±1,
so hypothesis (16.10) guarantees that M(δ +iγ ) ̸= M(δ −iγ ). In Problem 16.2.5.22, you will
derive that
M(δ + iγ ) = M(δ + γ ) ⇐⇒(δ −α)γ −γ 2 = ∓iαγ ,
which is impossible because δ, γ , and α are real and α ̸= 0.
Having seen that M maps three noncollinear points on the circle C2 to three noncollinear
points on the circle |w| =
 δ−α
γ
 we conclude that M is as desired. ⃝
The quadratic equation in (16.12) can be rewritten as
α2 + γ 2 −δ2 −1
δ
α + 1 = 0,
(16.15)
so α−· α+ = 1 and α−1
± = α∓.

1236
Advanced Engineering Mathematics
Example 16.5
(a) Find a Möbius transformation that maps each of the circles |z| = 1 and |z −1| = 3
to a circle whose centers are at 0 in the w-plane. (b) Does your transformation map the
region between the circles |z| = 1 and |z −1| = 3 to an annulus?
Method: (a) With δ = 1 and γ = 3, Theorem 16.6 says to let M(z) = z−α
z−β , where
β = 1
α
and
1 · α2 + (32 −12 −1)α + 1 = 0.
The latter is a quadratic equation whose solutions are
α = α± ≜−7 ± 3
√
5
2
.
As expected, α−· α+ = 1 so α−1
± = α∓.
(1) With α = α+ = −7+3
√
5
2
, we get the Möbius transformation
M+(z) ≜z −−7+3
√
5
2
z −−7−3
√
5
2
= 2z + 7 −3
√
5
2z + 7 + 3
√
5
.
By Theorem 16.6’s conclusion (16.13), M+ maps
C1 : |z| = 1
→
M+(C1) : |w| = 7−3
√
5
2
,
and
C2 : |z −1| = 3
→
M+(C2) : |w| = 3−
√
5
2
.
C1 is inside C2. Because 3 −
√
5 > 7 −3
√
5, M+(C1) is inside M+(C2). We see that M+
preserves the interior versus exterior relationship between C1 and C2 (Figure 16.6).
3
2
1
–2
–1
1
2
3
 M +(z)= 2z +7–3√5
2z +7+3√5
4
–0.3
0.3
u
v
0.3
–0.3
x
y
–1
–2
–3
FIGURE 16.6
Example 16.5(a)(1).

Conformal Mapping
1237
3
2
1
1
2
3
4
x
y
M–(z)= 2z+7+3√5
2z+7–3√5
–1
–1
–3
–2
–2
6
4
2
2
4
6
u
v
–2
–2
–6
–4
–4
–6
FIGURE 16.7
Example 16.5(a)(2).
(2) With α = α−= −7−3
√
5
2
, we get the Möbius transformation
M−(z) ≜z −−7−3
√
5
2
z −−7+3
√
5
2
= 2z + 7 + 3
√
5
2z + 7 −3
√
5
.
By Theorem 16.6’s conclusion (16.13), M−maps
C1 : |z| = 1
→
M−(C1) : |w| = 7+3
√
5
2
and
C2 : |z −1| = 3
→
M−(C2) : |w| = 3+
√
5
2
.
Because C1 is inside C2 and M−(C2) is inside M−(C1), we see that M−reverses the interior
versus exterior relationship between C1 and C2 (Figure 16.7).
(b) M+ has a pole only at z = −7+3
√
5
2
, which is not in the connected set R consisting
of the circles C1 : |z| = 1, C2 : |z −1| = 3 and all of the points between them. So, M+,
is continuous on R. It follows that the image, M+(R), is the connected set consisting of
M+(C1), M+(C2), and all of the points between them, that is, the annulus 7−3
√
5
2
< r <
3−
√
5
2
.
Similarly, M−has a pole only at z = −7+3
√
5
2
, which is not in the same region R as
defined in the paragraph previously. By the same reasoning process, it follows that the
image, M+(R), is the annulus 3+
√
5
2
< r < 7+3
√
5
2
. ⃝
What if the given circles are not of the form |z| = 1 and |z−δ| = γ ? The general situation
can be handled by Theorem 16.6 after performing some preliminary manipulations.

1238
Advanced Engineering Mathematics
Example 16.6
Find a Möbius transformation that maps each of the circles |z| = 5 and |z −2| = 2 to
circles whose centers are at 0 in the w-plane.
Method: Note that 0 being on one of the given circles is not a problem because we will
not use the inversion mapping on the given circle.
The change of variables z = 5z maps the given circles to
|5z | = 5
and
2 = |z −2| = |5z −2|,
that is,
|z| = 1
and
z −2
5
 = 2
5 .
With δ = 2
5 and γ = 2
5, Theorem 16.6 and (16.15) say to let M(z) = z−α
z−β , where
β = 1
α
and
0 = α2 −5
2 α + 1 = (α −2)

α −1
2

,
so α+ = 2 and α−= 1
2. As expected, α−· α+ = 1 and α−1
± = α∓.
(1) With α = α+ = 2, we get the Möbius transformation
M+(z) ≜z −2
z −1
2
= 2z −4
2z −1 ;
substituting backz = z/5, we get the Möbius transformation
M+(z) ≜(2z/5) −4
(2z/5) −1 = 2z −20
2z −5 .
By Theorem 16.6’s conclusion (16.13), M+ maps
C1 : |z| = 5
→
M+(C1) : |w| = 2
and
C2 : |z −2| = 2
→
M+(C2) : |w| = 4.
Because C2 is inside C1 and M+(C1) is inside M+(C2), we see that M+ reverses the interior
versus exterior relationship between C1 and C2.
In fact, we note for future reference that if we parametrize C1 with positive orientation,
then M+(C1) is parametrized with negative orientation. So, the interior of the closed
parametrized curve C1 is mapped by M+ to the exterior of the closed parametrized curve
M+(C1).
Similarly, if we parametrize C2 with negative orientation, then M+(C2) is parametrized
with positive orientation. So, the exterior of the closed, parametrized curve C2 is mapped
by M+ to the interior of the closed curve M+(C2). The orientations are also shown in
Figure 16.8.
(2) With α = α−= 1
2, we get the Möbius transformation
M−(z) ≜z −(1/2)
z −2
,
that is,
M−(z) = (z/5) −(1/2)
(z/5) −2
= 2z −5
2z −20.

Conformal Mapping
1239
4
y
M +(z)= 2z –20
2z –5
2
–2
–2
–4
2
4
x
–4
4
v
2
–2
–2
–4
2
4
u
–4
FIGURE 16.8
Example 16.6(1).
4
4
x
y
2
2
–2
–2
–4
–4
–0.4
0.4
u
v
0.4
–0.4
M – (z) = 2z– 5
2z– 20
FIGURE 16.9
Example 16.6(2).
By Theorem 16.6’s conclusion (16.13), M−maps
C1 : |z| = 5
→
M−(C1) : |w| = 1
2
and
C2 : |z −2| = 2
→
M−(C2) : |w| = 1
4.
Because C2 is inside C1 and M−(C2) is inside M−(C1), we see that M−preserves the
interior versus exterior relationship between C1 and C2 (Figure 16.9). ⃝
What if neither of the given circles is centered at the origin? This situation requires an
additional preliminary step: translate the center of one of the given circles to the origin by

1240
Advanced Engineering Mathematics
a linear mapping z = z + (constant). For example, suppose we wanted to map the circles
|z −3| = 2 and |z −1| = 5 to circles centered at w = 0. Let z =z + 3, so the given circles are
|z| = 2 and |z + 2| = 5. After that, let ˘z = 2z, so the given circles are |˘z| = 1 and |˘z + 1| = 5
2.
After that, we can use the formulas of Theorem 16.6.
What if the given circles are of the forms |z| = 1 and |z −μ| = γ , where μ is not real? A
preliminary rotation of the complex plane takes care of this: Let ϕ = Arg(μ) andz = ze−iϕ,
so the circle γ = |z−μ| becomes γ =
zeiϕ −|μ|eiϕ =
z−|μ|
·|eiϕ| = |z−δ|, where δ = |μ|.
16.2.3 Some Useful Facts about Möbius Transformations
Given four distinct points z, z1, z2, z3, their cross ratio is defined to be
(z, z1, z2, z3) ≜(z −z1)(z2 −z3)
(z −z3)(z2 −z1).
Lemma 16.4
Suppose z, z1, z2, z3 are distinct points and w, w1, w2, w3 are distinct points. Define a
mapping w = g(z) implicitly by (z, z1, z2, z3) = (w, w1, w2, w3), that is, by
(z −z1)(z2 −z3)
(z −z3)(z2 −z1) = (w −w1)(w2 −w3)
(w −w3)(w2 −w1) .
(16.16)
Then w = g(z) is a Möbius transformation and wj = g(zj) for j = 1, 2, 3.
Why? The details are tedious but easy: Cross multiplication of (16.16) gives
w(w2 −w3)(z −z3)(z2 −z1) −w1(w2 −w3)(z −z3)(z2 −z1)
= w(w2 −w1)(z −z1)(z2 −z3) −w3(w2 −w1)(z −z1)(z2 −z3);
hence,
w =
w1(w2 −w3)(z −z3)(z2 −z1) −w3(w2 −w1)(z −z1)(z2 −z3)
(w2 −w3)(z −z3)(z2 −z1) −(w2 −w1)(z −z1)(z2 −z3)
≜az + b
cz + d ≜g(z).
(16.17)
In this generality, we will not bother to derive the formulas for the constants a, b, c, d in
terms of z1, z2, z3, w1, w2, w3. The reader may easily verify from (16.17) that g(zj) = wj for
j = 1, 2, 3.
The reader may also derive the formulas for the constants a, b, c, d and check that
ad −bc ̸= 0 follows from the given information that z1, z2, z3 are distinct and w1, w2, w3
are distinct.

Conformal Mapping
1241
Example 16.7
Find a Möbius transformation that maps the points −1, 1, i to the points −1, 3, (1 −i2),
respectively.
Method: Let z1, z2, z3 be i, 1, −1 and let w1, w2, w3 be (1 −i2), 3, −1; we chose this order
because the orders z1, z2, z3 and w1, w2, w3 correspond to travel counterclockwise on the
circles |z| = 1 and |w −1| = 2. Formula (16.17) gives
w =
(1 −i2)

3 −(−1)

z −(−1)

(1 −i) −(−1)

3 −(1 −i2)

(z −i)

1 −(−1)


3 −(−1)

z −(−1)

(1 −i) −

3 −(1 −i2)

(z −i)

1 −(−1)

= · · · = −i2z −i4
−i2z
= z + 2
z
= 1 + 2
z ≜g(z). ⃝
Notice that Lemma 16.4 gives us a method for finding a map that takes one arbitrary
circle to another arbitrary circle. Theorem 16.6 gives us a method for finding a map that
takes two given circles to two circles centered at the origin but whose radii we did not
specify.
A resource of information about Möbius and other transformations is the Dictionary
of Conformal Representation, by H. Kober. For example, in its Section 3.3 on page 8, we
find that
M(z) ≜z −α
z −β
accomplishes the mapping of circle to circle given by
|z −z0| = r →|w −w0| = R, where w0 = z0 −α −r2s
z0 −β
, R = r|s|, and s =
−β + α
|z0 −β|2 −r2 ,
assuming z = β is not on the circle |z −z0| = r.
On the other hand, item (iii) on page 7 of Kober’s “dictionary" states that if z = β is on
the circle |z −z0| = r, that is, if |β −z0| = r, then M accomplishes the mapping of circle to
the line given by
|z −z0| = r, z ̸= β
→Re

 α −β
z0 −β

· w

= −|α −β|2 + 2Re ((z0 −α)(α −β))
2r2
.
The results of Lemma 16.1(a) are found on page 9 of Kober’s dictionary.
16.2.4 Möbius Transformation to or from a Line
Suppose M(z) is a Möbius transformation and L is a line. If the “point at infinity” is on
the image M(L), then M(L) cannot be a circle and therefore must be a line. The cross-ratio
method can be adapted to this situation. In an informal way, consider
(w, w1, w2, w3) = (w −w1)(w2 −w3)
(w −w3)(w2 −w1).

1242
Advanced Engineering Mathematics
If w3 →∞, then w2 −w3
w −w3
→1. This motivates defining
(w, w1, w2, ∞) ≜(w −w1)
(w2 −w1).
Example 16.8
Find a Möbius transformation that maps the region |z| < 1 to the region Re(w) < 0.
Method: Let M(z) be the desired Möbius transformation. First, let’s choose M(z) so that
it maps the circle |z| = 1 to the line Re(w) = 0; this follows the general principle that
if we want to understand an inequality, it helps to first understand the corresponding
equality.
We can consider the point ∞to be on the line Re(w) = 0, so the cross-ratio method
defines M implicitly by
(z, z1, z2, z3) = (w, w1, w2, ∞) = (w −w1)
(w2 −w1) ,
where the circle |z| = 1 has points z1, z2, z3 mapped to w1, w2, ∞, respectively, on the
line Re(w) = 0. The mapping we get depends on the orientations of z1, z2, z3, and w1, w2.
Figure 16.10 illustrates what are essentially the only two possibilities, although we could
choose different magnitudes of w2.
(a) With z1 = 1, z2 = i, z3 = −1, w1 = 0, w2 = i, we get
−iw = w −0
i −0 = (w, w1, w2, ∞) = (z, z1, z2, z3) = (z −1)

i −(−1)


z −(−1)

(i −1) = · · · = −i z −1
z + 1 ,
that is,
w = M+(z) ≜z −1
z + 1 .
(16.18)
(b) With z1 = 1, z2 = i, z3 = −1, w1 = 0, w2 = −i, we get
iw = w −0
−i −0 = (w, w1, w2, ∞) = (z, z1, z2, z3) = (z −1)

i −(−1)


z −(−1)

(i −1) = · · · = −i z −1
z + 1 ,
z3=–1
z2= i
z1= 1
w3= ∞
w2= i
w1= 0
v
x
u
…
y
w1= 0
v
u
w3= ∞
…
z3= –1
z2= i
z1= 1
x
y
w2=–i
FIGURE 16.10
Example 16.8: Two orientations for M(z).

Conformal Mapping
1243
that is,
w = M−(z) ≜−z −1
z + 1 .
(16.19)
Each of the Möbius transformations M± maps the circle |z| = 1 to the line Re(w) = 0,
as shown in Figure 16.10. Which, if either, maps the inside of the circle to the left half
plane, Re(w) < 0?
We can test the maps by calculating a “test value”:
M±(0) = ±(−1) = ∓1.
Because M−(0) = 1, M−(z) is not satisfactory.
M+(0) is in the left half-plane, Re(w) < 0. In fact, for any z = reiθ with |z| < 1,
M+(z) = reiθ −1
reiθ + 1 = (reiθ −1)(re−iθ + 1)
(reiθ + 1)(re−iθ + 1) = r2 −1 + i2r sin θ
r2 + 1 + 2r cos θ
=
r2 −1
r2 + 1 + 2r cos θ + i
2r sin θ
r2 + 1 + 2r cos θ ,
so r = |z| < 1 implies
Re (M+(z)) =
r2 −1
x2 + y2 + 1 + 2x =
r2 −1
(x + 1)2 + y2 < 0.
So, M+(z) is a Möbius transformation M± that maps the inside of the circle to the left
half-plane, Re(w) < 0. ⃝
Any positive multiple of M+(z), for example, M(z) = 179(z−1)
z+1
, also accomplishes what
M+(z) does in Example 16.8.
Learn More About It
Much useful information about Möbius and other transformations and their applica-
tions to solving PDEs is found in Dictionary of Conformal Representation, by H. Kober,
c⃝1957, Dover Publication, Inc., New York.
16.2.5 Problems
In each of problems 1–8 find a Möbius transformation that maps both of the given circles
to concentric circles centered at the origin.
1. |z| = 1 and |z + 1| = 5
2
2. |z| = 1 and |z + 1| = 3
3. |z| = 3 and |z −1| = 1
4. |z| = 3 and |z + 1| = 1
5. |z −2| = 1 and |z −1| = 3
6. |z −1| = 2 and |z −2| = 4

1244
Advanced Engineering Mathematics
7. |z| = 1 and |z −i| = 5
2
8. |z| = 1
2 and |z −i| = 2
In each of problems 9–12, find a Möbius transformation that maps the given circle in the
z-plane to the given circle or line in the w-plane.
9. |z| = 1 →|w −i| = 2
10. |z| = 2 →|w −i2| = 1
11. |z| = 1 →Im(w) = 0
12. |z| = 2 →Im(w) = 1
In each of problems 13 and 14, find a Möbius transformation that maps the given line in
the z-plane to the given circle or line in the w-plane.
13. Re(z) = 1 →|w| = 2
14. Re(z) = 1 →Im(w) = 2
In each of problems 15–18, find a Möbius transformation that maps the given region in the
z-plane to the given region in the w-plane.
15. |z| < 2 →Re(w) < 0
16. |z| < 2 →Re(w) < 1
17. |z| > 2 →Re(w) < 1
18. |z| > 2 →Im(w) < 1
19. Explain why the image of C1 : |z| = 1 is M(C1) : |w| = |α| in the derivation of
Theorem 16.6, assuming α is real.
20. (a) Explain why β =
1
α , along with Theorem 16.6’s hypothesis (16.10), that is,
“z = β is on neither C1 nor C2,” imply that the solutions of quadratic equation
(16.12) are not α = ±1.
(b) Explain why β = 1
α , along with α = 1 or α = −1, would imply M±(z) does not
map the two circles to circles.
21. (a) In the context of the derivation of Theorem 16.6, derive that
M(δ + iγ ) = M(δ −iγ ) ⇐⇒α = β.
(b) Derive that α = β ⇐⇒α = ±1.
(c) Conclude that
M(δ + iγ ) = M(δ −iγ ) ⇐⇒α = ±1.
22. Derive that
M(δ + iγ ) = M(δ + γ ) ⇐⇒(δ −α)γ −γ 2 = ∓iαγ
in the context of the derivation of Theorem 16.6.

Conformal Mapping
1245
23. Explain why Lemma 16.1(a) is true, that is, explain why if 0 is not in C, that is,
|z0| ̸= R, then the image of C under the inversion mapping is the circle given in
(16.5), that is, f(C) =

w : |w −w0| =
R
|w0|2−R2

, whose center is given in (16.6),
that is, is at w0 =
z0
|z0|2−R2 .
24. Explain why Lemma 16.3 is true, that is, explain why if three points w1, w2, and w3
do not lie on a line, then there is exactly one circle that contains w1, w2, and w3.
(a) First, consider (w2−w1) = (x2−x1)+i(y2−y1) and (w3−w1) = (x3−x1)+i(y3−
y1) as vectors in the complex plane. Use linear independence to conclude that
0 ̸= x1(y2 −y3) −x2(y1 −y3) + x3(y1 −y2).
(b) Discover where the center (h, k) and radius r of the unique circle on which
w1, w2, and w3 lie by explaining why
(x1 −h)2 + (y1 −k)2 = r2 = (x2 −h)2 + (y2 −k)2
and
(x1 −h)2 + (y1 −k)2 = r2 = (x3 −h)2 + (y3 −k)2
and then using those equations to arrive at a linear algebraic system of
equations for unknowns h and k.
25. Explain why
 sin ϕ
1+cos ϕ : 0< ϕ ≤2π with ϕ ̸=π

=(−∞, ∞).
16.3 Solving Laplace’s Equation Using Conformal Maps
Here we will use conformal maps to solve Laplace’s equation in a region in the z-plane by
mapping the region into one in the w-plane that is easier to work with. Additionally, even
if the original region is not difficult to work with, the solution we get using a conformal
map may be in a form much simpler to use than an infinite series, the latter having been
produced in Section 11.3 or Section 11.5.
As usual, let z = x + iy, where x, y are real. In this section, we will consider confor-
mal maps ζ = g(z), that is, the image will be in the ζ = ξ + iη plane, where ξ, η are
real. (We do not write u + iv = w = g(z) because we reserve for later use in fluid flow
problems the symbols u, v to stand for the components of fluid velocity in two dimen-
sions, and we reserve w for the “complex velocity,” which we will define later in this
section.)

1246
Advanced Engineering Mathematics
Lemma 16.5
Suppose g(z) is a conformal map from a domain D. Then E = g(D), the image of D under
g, is a domain in the ζ-plane.
Why? g being conformal on D includes the property that g is analytic and thus continuous
on D. Because D is open, a famous topological result called “the open mapping theorem”
implies that E is an open set in the ζ−plane. The continuity of g and the property that D is
connected imply E is connected, by another topological theorem. 2
Lemma 16.6
Suppose g(z) is a conformal map from a domain D, that is, an open, connected set in the
z-plane, to a domain E ≜f(D) in the ζ-plane. Locally, that is, in an open ball Bδ in E, g(z)
has an inverse function, that is,
z = x + iy = g−1(ζ).
Suppose 	 = 	(ζ) = 	(ξ, η) solves Laplace’s equation in E and define
φ(x, y) ≜	

ξ(x, y), η(x, y)

= 	

g(ζ)

.
Then φ(x, y) satisfies Laplace’s equation in D.
Also, suppose a smooth contour  is part of the boundary of E.
(a) If 	 satisfies the Dirichlet boundary condition that 	(ξ, η) ≡	0 is a constant on
, then φ(x, y) ≡	0 on g−1().
(b) If 	 satisfies the Neumann boundary condition that
∂	
∂nζ (ξ, η) ≡0 on , then
∂φ
∂nz (x, y) ≡0 on g−1().
Why? (a) is obvious; (b) follows from the fact that the conformal map g−1(ζ) preserves
angles, for example, the right angle between the vector nζ, written as a complex number,
and ∂	
∂ξ +i ∂	
∂η . The latter is referred to as the gradient of 	, written as a complex number. 2
Example 16.9
In the left-hand side of (the earlier) Figure 16.8, the region between two eccentric circles
is shaded. Find the electrostatic potential in the region, assuming the potential is one on
the outer circle, |z| = 5, and zero on the inner circle, |z −2| = 2.
Method: From Example 16.6 in Section 16.2 we know that the Möbius transformation
M = M+ given by
ζ = M(z) ≜2z −20
2z −5

Conformal Mapping
1247
maps the circle |z| = 5 to the circle |ζ| = 2 and maps the circle |z −2| = 2 to the circle
|ζ| = 4. Does M map
D ≜{z : |z| < 5 and |z −2| > 2},
the region between the curves |z| = 5, |z −2| = 2, onto
E ≜{ζ : 2 < |ζ| < 4},
the region between the curves |ζ| = 2, |ζ| = 4?
Yes. Parametrize C1 : |z| = 5 with positive orientation and parametrize C2 : |z−2| = 2
with negative orientation. As discussed in Example 16.6 in Section 16.2, M(z) maps the
interior of C1 to the exterior of M(C1) and maps the exterior of C2 to the interior of M(C2),
so M(D) ⊆E. We can check that the inverse map,
z = M−1(ζ) ≜5ζ −20
2ζ −2 ,
maps the exterior of M(C1) to the interior of C1 and maps the interior of M(C2) to the
interior of C2, so M−1(E) ⊆D. That and M(D) ⊆E explain why M(D) = E, as we
desired.
It is easy to solve Laplace’s equation 	(ξ, η) = 0 in the annulus E with boundary
conditions 	 = 0 on the circle R = 4 and 	 = 1 on the circle R = 2, where polar coor-
dinates for (ξ, η) are ξ = R cos ϑ, η = R sin ϑ. The boundary conditions are constants,
so we can look for a solution of Laplace’s equation that is independent of ϑ: Laplace’s
equation that 	 = 	(R) should satisfy is
1
R
d
dR

R d	
dR

= 0.
This gives
	 = c1 + c2 ln R,
where c1, c2 are arbitrary constants. The boundary conditions are
1 = 	(2) = c1 + c2 ln 2
and
0 = 	(4) = c1 + c2 ln 4,
and it’s easy to see that the solution is c1 = 2, c2 = −1
ln 2. The solution in the ζ-plane is
	 = 2 −
1
ln 2 ln R. Noting that R =

ξ2 + η2, we have ln R = 1
2 ln(ξ2 + η2), so
	 = 	(ξ, η) = 2 −
1
2 ln 2 ln(ξ2 + η2) .
(16.20)
To transform back to the original problem in the z = x + iy-plane, we note that
ξ + iη = ζ = M(z) = 2z −20
2z −5 = 2(x + iy) −20
2(x + iy) −5 = (2x −20) + i2y
(2x −5) + i2y
=

(2x −20) + i2y

(2x −5) −i2y

(2x −5)2 + (2y)2
=
(2x −20)(2x −5) + 4y2 + i2y

(2x −5) −(2x −20)

(2x −5)2 + (2y)2
= 4x2 + 4y2 −50x + 100 + i30y
(2x −5)2 + (2y)2
.

1248
Advanced Engineering Mathematics
So,
ξ = 4x2 + 4y2 −50x + 100
(2x −5)2 + (2y)2
,
η =
30y
(2x −5)2 + (2y)2 .
The solution of the original problem is
φ = φ(x, y) = 2 −
1
2 ln 2 ln

(4x2 + 4y2 −50x + 100)2 + (30y)2

(2x −5)2 + (2y)22

. ⃝
By the way, this could enable us to calculate the electric field E between the eccentric
circles using
E = −∇φ = −∂φ
∂x ˆı −∂φ
∂y ˆj.
If φ = φ(z) has a harmonic conjugate ψ(z), then the function
f(z) ≜φ(z) + i ψ(z)
(16.21)
is analytic. The curves φ(x, y) = constant are the equipotentials; the curves ψ(x, y) =
constant are the electric field lines. From (16.20) and
	 = 	(ξ, η) = 2 −
1
ln 2 ln |ζ| = 2 −
1
ln 2 Re(Log ζ) ,
so it follows that its harmonic conjugate is 
 = 
(ξ, η) = −1
ln 2 Im(Log ζ). So the electric
field lines are the level sets
constant = ψ(z) = −1
ln 2 Arg(ξ + iη) = −1
ln 2 Arg(4x2 + 4y2 −50x + 100 + i30y) .
16.3.1 Boundary Values on a Circle
Suppose −π = α0 < α1 < α2 < · · · < αN−1 < αN = π. The corresponding points zk = Reiαk
on the circle |z| = R divide it into N arcs. The problem we want to solve is to find a solution
φ = φ(r, θ) of Laplace’s equation inside the circle that also has given constant potentials
on those arcs, that is, satisfies
φ(R, θ) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
φ1,
−π = α0 ≤θ < α1
φ2,
α1 ≤θ < α2
...
φN,
αN−1 ≤θ < αN = π
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
(16.22)

Conformal Mapping
1249
Let’s map the open disk |z| < R to the upper half-plane, that is, Im(ζ) > 0, and apply
a theorem that gives the solution to the corresponding Dirichlet problem in the ζ-plane.
Thus, instead of an infinite series solution to the original problem in the xy-plane as we
would get in Section 11.5, we will get a simpler solution formula.
First, we will find a Möbius transformation that maps the open disk |z| < R to the half-
plane Im(ζ) > 0 by using cross ratios. How? We want to both (a) map the three points
z = R, iR, −R, on the boundary of the disk, to ζ = 0, 1, ∞, respectively, on the boundary
of the half plane, and (b) map the interior, that is, points on the “left,” of the positively
oriented circle |z| = R to the interior, that is, points on the left, of the positively ori-
ented line Im(ζ) = 0. The calculations are similar to those we did in Example 16.8 in
Section 16.2:
ζ = ζ −0
1 −0 = ζ −ζ1
ζ2 −ζ1
= (ζ, 0, 1, ∞) = (ζ, ζ1, ζ2, ∞) = (z, z1, z2, z3) = (z, R, iR, −R)
= (z −z1)(z2 −z3)
(z −z3)(z2 −z1) = (z −R)

iR −(−R)


z −(−R)

(iR −R) = z −R
z + R · iR + R
iR −R = · · · = −i z −R
z + R .
So, we define a mapping by
ζ = M(z) ≜−i z −R
z + R .
(16.23)
We will need the images of the points zk = Reiαk. In general,
M(Reiθ) = −i Reiθ −R
Reiθ + R = −i eiθ −1
eiθ + 1 = −i · 2i
2i · eiθ −1
eiθ + 1 · e−iθ/2
e−iθ/2 · 2
2
=

eiθ/2 −e−iθ/2
2i
 " 
eiθ/2 + e−iθ/2
2

= sin(θ/2)
cos(θ/2) = tan θ
2 ,
assuming θ is not an odd multiple of π.
So,
ξk + iηk = ζk = M(zk) = tan αk
2 + i0, k = 1, . . . , N −1,
(16.24)
and
ζ0 = −∞+ i0, ζN = ∞+ i0,
at least as limits.

1250
Advanced Engineering Mathematics
The function 	

ξ, η)

≜φ

ξ(x, y), η(x, y)

, where ξ + iη = M(x + iy), should satisfy the
Dirichlet problem
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(PDE)
0 = ∂2	
∂ξ2 + ∂2	
∂η2 , for η > 0
(BC)
	(ξ, 0) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
φ1,
−∞< ξ < ξ1
φ2,
ξ1 ≤ξ < ξ2
...
φN−1,
ξN−2 ≤ξ < ξN−1
φN,
ξN−1 ≤ξ < ∞
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(16.25)
Fortunately, Matthews and Howell (2006) have solved this problem:
Theorem 16.7
The solution of Dirichlet problem (16.25) is
	(ξ, η)=φN + 1
π
N−1
#
k=1
(φk −φk+1)Arg(ζ −ξk)
=φN + 1
π
N−1
#
k=1
(φk −φk+1) cos−1

η

(ξ −ξk)2 + η2

.
(16.26)
Note that η > 0 so Arg(ξ + iη) = cos−1 
η/

ξ2 + η2
	
.
The original Dirichlet problem on the disk |z| ≤R is mapped to the Dirichlet problem
(16.25) on the half-plane Im(ζ) ≥0 by
ξ + iη = ζ = M(z) = −i z −R
z + R = −i (x −R) + iy
(x + R) + iy = −i

(x −R) + iy
 
(x + R) −iy

(x + R)2 + y2
= · · · =
2Ry
(x + R)2 + y2 + i R2 −(x2 + y2)
(x + R)2 + y2 .
To use (16.26), it will help to simplify
η
ξ −ξk
=
R2 −(x2 + y2)
2Ry −ξk

(x + R)2 + y2 =
R2 −(x2 + y2)
2Ry −

(x + R)2 + y2
tan(αk/2) .
(16.27)

Conformal Mapping
1251
Using this, the solution to the original Dirichlet problem in the disk x2 + y2 < R2 is
φ(x, y) = φN + 1
π
N−1
#
k=1
(φk −φk+1)
cos−1
⎛
⎝
R2 −(x2 + y2)

2Ry −

(x + R)2 + y2
tan(αk/2)
2 +

R2 −(x2 + y2)
2
⎞
⎠.
(16.28)
16.3.2 The Joukowsky Map
We will see that one special mapping gives beautiful images of Zhukovskii airfoils, wing-
like profiles that were important in the history of flight. Those profiles are not only of
historical interest but also guides to further interplay between theory and practice.
Zhukovskii, or Joukowsky or Joukowski in older transliterations from the Cyrillic
alphabet, used the Joukowsky map
z = J(ζ) ≜1
2

ζ + a2
ζ

,
(16.29)
where a is a positive constant. We will see that it produces the images shown in
Figure 16.11.
But first, we will see some routine-looking images. Suppose ρ is a positive constant.
When we map the circle centered at the origin given by
Cρ : |ζ| = ρ, −π < ϕ ≤π,
we get an ellipse, as long as ρ ̸= a: Writing ζ = ρeiϕ, we have
x + iy = z = J(ζ) = 1
2

ρ eiϕ + a2
ρ e−iϕ

,
(a)
(b)
–1.0
–0.5
0.5
1.0 x
0.10
y
–1.0
–0.5
0.5
1.0 x
0.1
0.2
0.3
–0.1
y
FIGURE 16.11
Images under the Joukowsky map (a) a Zhukovskii airfoil and (b) not a Zhukovskii airfoil.

1252
Advanced Engineering Mathematics
so
⎧
⎪⎪⎨
⎪⎪⎩
x = 1
2

ρ cos ϕ + a2
ρ cos ϕ
	
= 1
2

ρ + a2
ρ
	
cos ϕ
y = 1
2

ρ sin ϕ −a2
ρ sin ϕ
	
= 1
2

ρ −a2
ρ
	
sin ϕ
⎫
⎪⎪⎬
⎪⎪⎭
.
(16.30)
So, for 0 < ρ ̸= a, (16.30) shows that J

Cρ

≜Eρ, that is, the ellipse
Eρ : 1 ≡cos2 ϕ + sin2 ϕ =
x2
1
4

ρ + a2
ρ
	2 +
y2
1
4

ρ −a2
ρ
	2 .
(16.31)
Note that the ellipse has major axis of length g(ρ) ≜

ρ + a2ρ−1
and minor axis of length
f(ρ) ≜
ρ −a2ρ−1.
For ρ = a, the image of Ca under the Zhukovskii map is a line segment on the x-axis.
Why? For ρ = a, we can think of the image intuitively as being a degenerate ellipse whose
minor axis has length f(a) = 0 and major axis has length g(a) = 2a. In fact, because of
(16.30), the image J(Ca) consists of the points
(x, y) =

1
2 g(a) cos ϕ, 1
2 f(a) sin ϕ

= (a cos ϕ, 0), −π < ϕ ≤π,
that is, is the line segment −a ≤x ≤a, y = 0.
We can think of the line segment as a limiting or “degenerate” case of an ellipse.
Thinking this way helps because in the study of fluid flow, we can think of one copy
of the line segment, specifically the image of the half circle |ζ| = a, 0 ≤ϕ ≤π, as being
the “upper” boundary of an infinitesimally thin plate and another copy of the line seg-
ment as the “lower” boundary of the thin plate because it is the image of the half circle
|ζ| = a, −π ≤ϕ ≤0.
Supposing 0 < ρ < a, in Problem 16.3.5.9 you will explain why the points in the interior
of the circle Cρ are mapped to the exterior of the ellipse Eρ and the points in the exterior of
Cρ are mapped onto the whole z-plane.
We have already seen that there are rich geometrical aspects of the Joukowsky mapping.
In addition, we will see that the image of rays Arg(ζ) = ϕ0 is hyperbolas in the z-plane,
as long as ϕ0 is not an integer multiple of π
2 , that is, as long as the ray does not lie on the
ξ−or η-axis: By (16.30), x + iy = z = J(ζ) = J(ρeiϕ0) implies
x = 1
2

ρ + a2
ρ

cos ϕ0
and
y = 1
2

ρ −a2
ρ

sin ϕ0.
We will eliminate ρ by using
2x
cos ϕ0
= ρ + a2
ρ
and
2y
sin ϕ0
= ρ −a2
ρ ,

Conformal Mapping
1253
from which it follows that
2x
cos ϕ0
+
2y
sin ϕ0
= 2ρ
and
2x
cos ϕ0
−
2y
sin ϕ0
= 2a2
ρ .
This gives
x
cos ϕ0
+
y
sin ϕ0
= ρ
and
x
a2 cos ϕ0
−
y
a2 sin ϕ0
= 1
ρ .
The product of those two identities is
1 ≡

x
cos ϕ0
+
y
sin ϕ0
 
x
a2 cos ϕ0
−
y
a2 sin ϕ0

,
that is, the hyperbola
1 =
x2
a2 cos2 ϕ0
−
y2
a2 sin2 ϕ0
.
16.3.3 Zhukovskii Airfoils
Recall that J(ζ) maps circles Cρ centered at the origin to ellipses, as long as 0 < ρ ̸= a,
and maps the circle Ca to a line segment on the real axis. We will see that the Zhukovskii
airfoils are the images of circles that are not centered at the origin, specifically given by
C : |ζ −ζ0| = |a −ζ0|,
where Arg(ζ0) = δ and |ζ0| ≪a. Here is a brief version of the derivation in Panton (2005).
If ζ is a point on the circle C, the law of cosines implies
|ζ|2 −(2|ζ0| cos ν)|ζ| −(a2 −2a|ζ0| cos δ) = 0,
(16.32)
where ν = δ −γ and Arg(ζ) = γ . You will explain this in Problem 16.3.5.10.
Define a nondimensional parameter by
ρ ≜|ζ|
a > 0,
and define a “small”∗parameter by
ε ≜|ζ0|
a .
∗Figure 16.12 does not appear to have |ζ0| ≪a in order to make room for labeled quantities.

1254
Advanced Engineering Mathematics
y
a
x
C
C:|ζ–ζ0|=|a – ζ0|
ζ0=|ζ0|eiδ
|a – ζ0|
y
a
x
v γ
C
ζ
ζ0
(a)
(b)
FIGURE 16.12
(a) C centered at |ζ0|eiδ (b) using the law of cosines.
Using these and then solving the quadratic equation ρ2 −(2ε cos ν)ρ −(1 −2ε cos δ) = 0
gives
ρ ≈1 + ε ·
cos ν −cos δ,
ν ̸= δ
0,
ν = δ

.
(16.33)
You will explain this in Problem 16.3.5.11.
For ν ̸= δ, thinking of the Joukowsky map leads us to calculate
ζ ≈aeiγ 
1 + ε(cos ν −cos δ)

and
a2
ζ ≈ae−iγ 
1 −ε(cos ν −cos δ)

.
Define K(ζ) ≜1
2

ζ + 1
ζ
	
. Then the behavior of the Joukowsky map on the circle C is
given by
x + iy = z = J(ζ) = aK

ζ
a

= aK

ρeiγ 	
= · · · ≈a

cos γ + iε(cos ν −cos δ) sin γ
	
.
So,
x
a ≈a cos γ
and
y
a ≈ε(cos ν −cos δ) sin γ .
(16.34)
Note that
sin γ = ±
(
1 −
x
a
	2
and
cos ν −cos δ = cos(δ −γ ) −cos δ = · · · = sin δ sin γ −cos δ(1 −cos γ ),

Conformal Mapping
1255
and define
q(x) ≜ε sin δ

1 −
x
a
	2
and
t(x) ≜ε| cos δ|

1 −
x
a
		 (
1 −
x
a
	2
.
Then (16.34) can be rewritten as
y
a ≈q(x) ± t(x) .
(16.35)
The parabola y
a = q(x) is called the camber line and the term t(x) is called the thickness
distribution, in the book by Panton.
The camber line y
a = q(x) reaches its maximum at x = 0, and its maximum value is
H ≈ε sin δ = |ζ0| sin δ
a
,
which is called the camber ratio.
The thickness distribution t(x) reaches its maximum value
T ≈3
√
3ε | cos δ|
4
= 3
√
3|ζ0| | cos δ|
4a
,
called the thickness ratio, at x = −a
2, known as the quarter-chord point.
In terms of nondimensionalized quantities X = x
a, Y = y
a, the approximate Zhukovskii
airfoil is given by
Y = H(1 −X2) ±
4
3
√
3
T(1 −X)

1 −X2 .
(16.36)
It can be shown that the left end of the airfoil is the parabolic “nose” and the right end
of the airfoil is a cusped “tail.”
16.3.4 Lift on Zhukovskii Airfoils
Later in this section, we will study 2D fluid flow around Zhukovskii airfoils. As mentioned
earlier in this section, denote by
v = v(t, ξ, η) = u ˆı + v ˆj,
the fluid velocity vector field for 2D steady flows in the ξη-plane. Potential flow is when
there is a scalar potential function 	 such that
v = ∇	,
that is,
u(ξ, η) ˆı + v(ξ, η) ˆj = v = ∂	
∂ξ ˆı + ∂	
∂η ˆj .
(16.37)

1256
Advanced Engineering Mathematics
As usual, in the study of complex variables, we denote ξ + iη = ζ, so (16.37) becomes
v = v(ζ) = u(ζ) + iv(ζ) = ∂	
∂ξ + i ∂	
∂η .
If 	 = 	(ζ) has a harmonic conjugate 
(ζ), then the complex potential function
f(ζ) ≜	(ζ) + i 
(ζ)
is analytic, and we define the complex velocity υ = υ(ζ) by
υ ≜df
dζ .
The Cauchy–Riemann equations tell us that, in fact,
υ = ∂	
∂ξ + i ∂
∂ξ = ∂	
∂ξ −i ∂	
∂η = v(ζ),
so the “complex velocity” is the complex conjugate of the 2D velocity vector written as a
complex number.
By the way, we wrote the Zhukovskii mapping as z = J(ζ), that is, with independent
variable ζ = ξ + i η rather than independent variable w = u + iv, because we wanted to
reserve u and v for the components of fluid velocity in 2D flow.
Let
v = v(ζ)
be a velocity vector field in the ζ-plane corresponding to complex scalar potential
	(ζ) + i 
(ζ) ≜v∞

e−iαζ + a2eiα
ζ
+ κ Log

e−iα ζ
	
,
(16.38)
where v∞is a real, positive constant. The velocity vector field derived from the potential
in (16.38) both has circulation κ around the circle ζ = a and corresponds to an angle of
attack α. Why is the latter true? Denote ζ = ξ + i η, so
	(ζ) + i 
(ζ) = 	(ξ, η) + i 
(ξ, η)
= v∞Re

cos(−α)ξ −sin(−α)η + i(cos(−α)η + sin(−α)ξ) + a2(cos α + i sin α)
	
× (ξ −i η)
ξ2 + η2 + v∞Re

κ

Log(e−iα) + Log(ζ)
		
;
hence,
	(ξ, η) = v∞

ξ cos α + η sin α + a2 (ξ cos α + η sin α)
ξ2 + η2
+ κ ln(ξ2 + η2)

.

Conformal Mapping
1257
It follows that
u(ξ, η) + iv(ξ, η) = v(ξ, η) = ∂	
∂ξ + i ∂	
∂η
= v∞

cos α + i sin α + a2 cos α
ξ2 + η2 −a2 cos(α)2ξ2
(ξ2 + η2)2
+ i

a2 sin α
ξ2 + η2 −a2 sin(α)2η2
(ξ2 + η2)2

+ v∞κ

2ξ
ξ2 + η2 + i
2η
ξ2 + η2

.
So, indeed, the velocity “at ∞” is
lim
ζ→∞

u(ξ, η)ˆı + v(ξ, η) ˆj

= v∞cos α ˆj + v∞sin α ˆj,
which has fluid flow at an angle α with respect to horizontal.
It is known that the lift force on the airfoil is given by
Fy = ϱ0 v∞κ,
where ϱ0 is the air density. But it seems that any value of κ is allowed, so the lift force is
unspecified. It turns out that there is physical reasoning, called the “Kutta condition,” that
determines the uniquely correct value of κ and thus the lift. As discussed in Panton’s book
cited earlier, the airfoil has a stagnation point, that is, a place where the complex velocity
is zero, at the tail of the airfoil, that is, the point z = a. The streamline connected to that
point makes an angle α + β, where α is the angle of attack and β ≈2H as ϵ = |z0|
a
→0+.
Here, H is the camber ratio, and
sin(α + β) =
κ
4πr0v∞
,
where
r0 ≈

1
4 +
T
3
√
3

· 2a.
The chord length is ℓ= 2a, if you are reading other sources.
It follows that
κ = 4πr0v∞sin(α + β),
and so the lift is
Fy = 4πϱ0 r0 v2
∞.
Further,
β ≈ϵ sin δ ≈2H, as ϵ →0+.

1258
Advanced Engineering Mathematics
In terms of the parameters H and T with which we partially characterized Zhukovskii
airfoils, the lift is
Fy ≈4πa

1
2ϱ0 v2
∞
 
1 + 4T
3
√
3

sin(α + 2H), as ϵ →0+.
Because air density decreases as altitude increases, the lift force decreases as altitude
increases, if the angle of attack and the airspeed stay the same.
Of course, real airplane wings are only approximately Zhukovskii airfoils, in part,
because a cusped tail is impossible to machine precisely. Also, fluids don’t exactly have
2D potential flow. Nevertheless, Zhukovskii airfoil theory can play a useful role in
understanding wings, especially at moderate airspeeds.
Example 16.10
In Section 6.3, we saw a Zhukovskii airfoil that was the image of the circle
C : ζ ≜(−0.07 + ρ0 cos(t)) + i(0.05 + ρ0 sin(t)),
where ρ0 =

(1 + 0.07)2 + (0.05)2 = |a −ζ0|, under the map (6.30) in Section 6.3, that is,
z = x + iy ≜ξ · 1 + ξ2 + η2
2(ξ2 + η2) + iη · −1 + ξ2 + η2
2(ξ2 + η2)
.
Find the approximate camber ratio, H, and the thickness ratio, T, and indicate them on a
picture of the airfoil. Also, find the lift in terms of the unspecified wind speed at infinity,
the air density, and the angle of attack.
Method: From the form of C, we see that its center is at ζ0 = −0.07 + i0.05 = |ζ0| cos δ +
i|ζ0| sin δ = |ζ0|eiδ and a = 1. So, the camber ratio is
H ≈|ζ0| sin δ
a
= 0.05
1
= 0.05
and the thickness ratio is
T ≈3
√
3 |ζ0| | cos δ|
4a
= 3
√
3 | −0.07|
4 · 1
≈0.0909326674.
The airfoil is shown in Figure 16.13. This is the same airfoil as in Figure 16.11a, except
stretched to make room to show H and T. Figure 16.13 was drawn using the approxima-
tions in (16.36), while Figure 16.11a was drawn using the exact Joukowsky map, as in
(6.30) in Section 6.3. The lack of a cusp at the tail is an artifact of drawing thick curves in
order to be visible in the printed copy.
The lift is
Fy ≈4πa

1
2ϱ0 v2
∞
 
1 + 4T
3
√
3

sin(α + 2H) = 2π · 1 · ϱ0 v2
∞(1 + 0.07) sin(α + 0.1)
= 2.14πϱ0 v2
∞sin(α + 0.1). ⃝

Conformal Mapping
1259
0.10
T
H
–0.5
0.5
1.0
Camber line
–1.0
FIGURE 16.13
Zhukovskii airfoil of Example 16.10.
Learn More About It
Complex Variables for Mathematics and Engineering, 5th ed., by John H. Mathews and
Russell W. Howell, Jones and Bartlett Publ., Inc.,
c⃝2006, has the derivation of
Theorem 16.7’s solution of a Dirichlet problem.
Also, much additional, useful information about conformal maps, including the
Joukowsky map, can be found in Volume I of Applied and Computational Complex Anal-
ysis, by Peter Henrici, John Wiley & Sons, Inc. c⃝1986; Conformal Mapping, by Zeev
Nehari, McGraw-Hill Co., Inc. c⃝1952; the books by Keener, Panton, Saff, and Snider;
and An Informal Introduction to Theoretical Fluid Mechanics, by James Lighthill, Oxford
University Press c⃝1986.
16.3.5 Problems
In problems 1–6, solve Laplace’s equation for φ = φ(, y) in the region between the given
circles and satisfying the given boundary conditions.
1. φ = 0 on |z| = 1 and φ = 1 on |z + 1| = 5
2.
2. φ = 0 on |z| = 1 and φ = 0 on |z + 1| = 3.
3. φ = 1 on |z| = 3 and φ = −1 on |z −1| = 1.
4. φ = 1 on |z −1| = 2 and ∂φ
∂n = 0 on |z −2| = 4.
5. φ = 1 on |z| = 1 and ∂φ
∂n = 0 on |z −i| = 5
2.
6. φ = 0 on |z| = 1
2 and φ = −1 on |z −i| = 2.
In problems 7 and 8, solve (16.25) with the given boundary conditions.
7. φ(R, θ) =
⎧
⎨
⎩
1,
−π ≤θ < −π
3
−1,
−π
3 ≤θ < π
3
1,
π
3 ≤θ < π
⎫
⎬
⎭.
8. φ(R, θ) =
⎧
⎨
⎩
1,
−π ≤θ < −π
3
2,
−π
3 ≤θ < π
3
1,
π
3 ≤θ < π
⎫
⎬
⎭.
9. Suppose 0 < ρ ̸= a. Recall that (16.31) implies that if ζ is on the circle Cρ the
Joukowsky map’s image point z = J(ζ) lies on the ellipse Eρ. Recall that Eρ

1260
Advanced Engineering Mathematics
has major axis of length g(ρ) ≜

ρ + a2ρ−1
and minor axis of length f(ρ) ≜
ρ −a2ρ−1.
(a) Explain why g(ρ) is decreasing for 0 < ρ < a and increasing for a < ρ.
(b) Explain why f(ρ) = −ρ + a2ρ−1 for 0 < ρ < a and f(ρ) = ρ −a2ρ−1 for a < ρ.
Explain why f(ρ) is decreasing for 0 < ρ < a and increasing for a < ρ.
(c) If a < ρ, explain why the points in the exterior of Cρ are mapped to the exterior
of Eρ.
(d) Explain why the interior of the circle Ca, that is, the disk Da, is mapped onto
the whole z-plane.
(e) If 0 < ρ < a, explain why the points in the interior of the circle Cρ are mapped
to the exterior of the ellipse Eρ.
(f) If 0 < ρ < a, explain why the exterior of the circle Cρ is mapped onto the whole
z-plane.
10. Derive (16.32). [Hints: First, consider the triangle whose vertices are the points 0,
ζ, and ζ0. Opposite the central angle whose measure is ν is the side whose length
is |ζ −ζ0|. Use the law of cosines for this angle. Later, use |ζ −ζ0| = |a −ζ0| and
ζ0 = |ζ0|eiδ.]
11. Derive (16.33) starting from (16.32).
In problems 12–14 use (6.30) in Section 6.3, that is, x(ξ, η) ≜ξ · 1 + ξ2 + η2
2(ξ2 + η2) , y(ξ, η) =
η · −1 + ξ2 + η2
2(ξ2 + η2) , to draw the Zhukovskii airfoil with the given center and a = 1. Find the
camber ratio, H; the thickness ratio, T; and the lift in terms of the unspecified wind speed
at infinity, the air density, and the angle of attack.
12. ζ0 = 0.05 −i0.07
13. ζ0 = −0.07 + i0.07
14. ζ0 = −0.07 + 0.10
Key Terms
angle of attack: before Example 16.10 in Section 16.3
camber line: after (16.35) in Section 16.3
camber ratio: after (16.35) in Section 16.3
chord length: before Example 16.10 in Section 16.3
collinear: Lemma 16.3 in Section 16.2
complex velocity: after (16.37) in Section 16.3
conformal: before Theorem 16.1 in Section 16.1; Definition 16.1 in Section 16.1
conformal at a boundary point: Definition 16.1 in Section 16.1
conformal on D: Definition 16.1 in Section 16.1
cross-ratio: before Lemma 16.4 in Section 16.2
eccentric circles: before Lemma 16.3 in Section 16.2
electric field lines: after (16.21) in Section 16.3
inversion mapping: (16.4) in Section 16.2

Conformal Mapping
1261
Joukowsky map: (16.29) in Section 16.3
linear mapping: beginning of Section 16.1
magnification: beginning of Section 16.1
Möbius transformations: (16.2) in Section 16.1
punctured line: before Lemma 16.2 in Section 16.2
quarter chord point: after (16.35) in Section 16.3
rotation: beginning of Section 16.1
thickness distribution: after (16.35) in Section 16.3
thickness ratio: after (16.35) in Section 16.3
translation: beginning of Section 16.1
Zhukovskii airfoils: beginning of Section 16.3.2
References
Keener, J.P., Principles of Applied Mathematics. Transformation and Approximation, Section 6.3. Addison-
Wesley Publ. Co., New York, 1988.
Mathews, J.H. and Howell, R.W., Complex Variables for Mathematics and Engineering, 5th edn. Jones
and Bartlett Publ., Inc., Sudbury, MA, 2006.
Panton, R.L., Incompressible Flow, 3rd edn., Section 18.13. John Wiley & Sons, Inc., New York, 2005.


17
Integral Transform Methods
17.1 Fourier Transform
Recall Definition 9.3 in Section 9.4, that is, that a function is absolutely integrable on R
if the improper integral
 ∞
−∞|f(x)| dx converges. Recall Definition 9.4 in Section 9.4, that
is, that if f(x) is absolutely integrable on R, then its Fourier transform is defined by the
improper integral
F(ω) ≜
1
√
2π
∞

−∞
f(x) e−iωx dx.
(17.1)
Theorem 9.7 (Fourier inversion theorem) in Section 9.4 states that if f(x) is absolutely
integrable on R and is piecewise smooth on every finite interval, then
f(x) .=
1
√
2π
∞

−∞
F(ω) eiωxdω,
(17.2)
in the sense that at every x0,
f(x+
0 ) + f(x−
0 )
2
=
1
√
2π
∞

−∞
F(ω) eiωx0dω.
(17.3)
The Fourier transform and its inversion theorem are very useful for solving some physi-
cal problems, as we will see in Section 17.2. A reference for a rigorous proof of the Fourier
inversion theorem was given in “Learn More About It” found at the end of Section 9.4.
Just as the symbol L stands for the Laplace transform operator and L−1 for the inverse
Laplace transform operator, we denote the Fourier transform operator and its inverse
operator by
F[ f(t)] = F(ω) ≜
1
√
2π
∞

−∞
f(t) e−iωtdt
(17.4)
1263

1264
Advanced Engineering Mathematics
and
F−1[F(ω)] ≜
1
√
2π
∞

−∞
F(ω) eiωtdt.
(17.5)
As we mentioned in Section 9.4, we caution that there are many slightly different
definitions of the Fourier transform and corresponding inverse transform in current use.
In this section, we concentrate on Fourier transforms and inverse transforms for which
complex variable integration methods are particularly needed or useful.
Example 17.1
Find the Fourier transform of
f(t) =
1
a2 + t2 ,
where the constant a ̸= 0.
Method: In effect, we will consider four sub-cases corresponding to a > 0 versus a < 0
and ω ≥0 versus ω ≤0.
First, assume a > 0 and ω ≥0. We will calculate the Fourier transform using
residues and the “real integral” method of Section 15.10, especially Example 15.68 in
Section 15.10. Let CR = CR,1 + CR,2, as shown in Figure 17.1a.
Let z = t + iy and extend f = f(z) =
1
a2+z2 to be a function of the complex variable z.
We chose CR,2 to be the bottom half circle because the factor
e−iωz = e−iω(t+iy) = eωye−iωt →0, as y →−∞.
Noting that CR is negatively oriented, we calculate

CR
e−iωz
a2 + z2 dz = −2πi Res

e−iωz
a2 + z2 ; −ia

= −2πi lim
z→−ia (z + ia) · e−iωz
z2 + a2
= −2πi lim
z→−ia
e−iωz
z −ia = −2πi · e−aω
−i2a = πe−aω
a
.
z=t+iy
y
R
t
CR,2
CR,1
y
R
t
CR,2
CR,1
z=t+iy
(a)
(b)
FIGURE 17.1
Large semicircular contours in (a) the lower half plane and (b) the upper half plane.

Integral Transform Methods
1265
Continuing with this case of a > 0, now we will use the line integral over CR to explain
why the Fourier transform of f(t) is given by
F

1
a2 + t2

(ω) =
1
√
2π
· πe−aω
a
, for ω ≥0.
Suppose R > a. As R →∞,
1
√
2π
· πe−aω
a
=
1
√
2π

CR,1
f(z) e−iωz dz =
1
√
2π
R
−R
f(t) e−iωtdt →F[f(t)](ω)
because (1) the improper integrals
 R
0
1
a2+t2 e−iωtdt and
 0
−R
1
a2+t2 e−iωtdt converge and
(2)

CR,2
1
a2+z2 e−iωz dz →0, as R →∞. The latter follows from the calculating on CR,2 :
z = Re−iθ, 0 ≤θ ≤π, that

e−iωz
a2 + z2
 =

e−iωRe−iθ
a2 + R2e−i2θ
 =
1
|a2 + R2e−i2θ| · |e−iωR cos θ| · |e−ωR sin θ|
=
1
|R2e−i2θ + a2| · 1 · e−ωR sin θ ≤
1
R2 −a2 · 1 · 1 =
1
R2 −a2
because 0 ≤θ ≤π and we assumed ω ≥0. So,


CR,2
e−iωz
a2 + z2 dz

≤
π
0
1
R2 −a2 dθ =
π
R2 −a2 →0, as R →∞.
We conclude that if a > 0, then, for ω ≥0,
F[f(t)] =
1
√
2π
· πe−aω
a
.
Continuing with a > 0, if ω ≤0, then we can use the same method but applied to the
positively oriented contour CR in the upper half plane shown in Figure 17.1b, to get
F[f(t)](ω) =
π
2 · eaω
a
for ω ≤0. Putting the two cases for ω together, we have that for a > 0,
F

1
a2 + t2

(ω) =
π
2 · e−a|ω|
a
.
(17.6)
On the other hand, if a < 0, define α = |a| = −a > 0. It follows from (17.6) that
F

1
a2 + t2

(ω) = F

1
α2 + t2

(ω) =
π
2 · e−α|ω|
α
=
π
2 · e−|a| |ω|
−a
=
π
2 · e−|a| |ω|
|a|
.
To summarize,
F

1
a2 + t2

(ω) =
π
2 · e−|a| |ω|
|a|
. ⃝
(17.7)

1266
Advanced Engineering Mathematics
Example 17.2
Find the Fourier transform of f(x) = e−x2/2.
Method: The improper integral F[e−x2/2](ω) =
1
√
2π
 ∞
−∞e−x2/2 e−iωxdx exists because
the improper integrals
 0
−∞e−x2/2 e−iωxdx and
 ∞
0
e−x2/2 e−iωxdx converge. To find a
formula for the Fourier transform, it helps to first complete the square in the exponent
−1
2 x2 −iωx = −1
2

x2 + i2ωx
	
= −1
2

(x + iω)2 −(iω)2	
= −1
2 (x + iω)2 −ω2
2 ,
so
F[e−x2/2](ω) = e−ω2/2
√
2π
·
∞

−∞
e−(x+iω)2/2 dx.
(17.8)
At this point, we recall from Problem 7.7.2.8 that
∞

−∞
e−u2/2 du =
√
2π.
(17.9)
We will use this to evaluate the integral in (17.8). In fact, we will use a contour integral to
explain why the integrals in (17.8) and (17.9) are equal, as we would guess by making the
non-rigorous substitution u = x + iω
2 . Unfortunately, the latter would lead to nonsense
such as writing −∞+ iω“ = ”−∞.
It will turn out that integrating over the large, rectangular contour (Fisher, 1999,
pp. 100–101) shown in Figure 17.2 works better than a large semicircular contour in
this problem. Finding a useful contour is as much of an art as a science, so it helps to be
familiar with different examples.
Let CR,1 : z = −x + iω, −R ≤x ≤R.
We define a function of the complex variable z by f(z) ≜e−z2/2, which is analytic
everywhere. The Cauchy–Goursat theorem implies that
0 =

CR,1+···+CR,4
e−z2/2 dz =

CR,1
e−z2/2 dz + · · · +

CR,4
e−z2/2 dz.
(17.10)
By taking R →∞, the integrals over CR,2 and CR,4 will go to zero. Why? For z = ±R+iy
on CR,2 or CR,4,
|e−z2/2| =
e−(±R+iy)2/2 =
e−(R2−y2)/2
e∓iRy = e−(R2−y2)/2 · 1.
CR,1
CR,2
CR,4
Re z
Im z
Im z = w
CR,3
FIGURE 17.2
Large rectangular contour.

Integral Transform Methods
1267
It follows that |e−z2| ≤e−(R2−ω2) on CR,2 and on CR,4 because 0 ≤y ≤ω. On CR,4 :
z = R + iy, 0 ≤y ≤ω, we calculate that


CR,4
e−z2/2 dz

≤
ω
0
|e−z2/2| dy ≤
ω
0
e−(R2−ω2)/2 dy = ωe−(R2−ω2)/2 →0,
as R →∞. A similar calculation holds on CR,2.
By (17.10) and the facts that limR→∞

−CR,2 e−z2/2dz = limR→∞

−CR,4 e−z2/2 dz = 0,
lim
R→∞

−CR,1
e−z2/2 dz = −lim
R→∞

CR,1
e−z2/2 dz = lim
R→∞

CR,3
e−z2/2 dz =
∞

−∞
e−x2/2 dx =
√
2π.
From (17.8),
F[e−x2/2](ω) = e−ω2/2
√
2π
∞

−∞
e−(x+iω)2/2 dx = e−ω2/2
√
2π
· lim
R→∞

−CR,1
e−z2/2 dz;
hence,
F

e−x2/2
(ω) = e−ω2/2
√
2π
lim
R→∞
R
−R
e−x2/2 dx = e−ω2/2
√
2π
√
2π = e−ω2/2. ⃝
We recall some other results about Fourier transforms and apply them to transforms we
have calculated in this section.
Theorem 9.8 in Section 9.4 states that if a is a nonzero constant and F(ω) = F[f(x)](ω)
exists, then we have (9.43) in Section 9.4, that is,
F[f(ax)](ω) = 1
a F
ω
a
	
and
F−1 
F
ω
a
	
(x) = a F−1[F(ω)](x).
Example 17.3
Find F

e−βx2
(ω).
Method: Rewrite e−βx2 = f(ax), where we choose f(x) ≜e−x2/2 because we know its
Fourier transform. To find 1/a, note that f(ax) = e−a2x2/2, so we need a2/2 = β, so a =

2β.
By Theorem 9.8 in Section 9.4 and the conclusion of Example 17.2,
F[e−βx2](ω) = 1
a e−(ω/a)2/2 =
1

2β
· e−(ω/√
2β )2/2 =
1

2β
· e−ω2/(4β). ⃝
A general result doubles the usefulness of tables of Fourier transforms because we can
reverse the roles of Fourier transform and inverse Fourier transform. This was not possible
for the Laplace transform.
Theorem 9.9(a) in Section 9.4 states that we have (9.44) in Section 9.4, that is,
(a) F−1[F(ω)](x) = F[F(ω)](−x)
and
(b) F[f(x)](ω) = F−1[f(x)](−ω),
where F[f(x)](ω) and F[F(ω)](x) are assumed to exist.

1268
Advanced Engineering Mathematics
Example 17.4
Find F
π
2 · e−|ax|
|a|

(ω).
Method: Exchanging the roles of x and ω in (17.7), that is, the conclusion of Example 17.1,
gives
F
π
2 · e−|ax|
|a|

(ω) = F−1
π
2 · e−|ax|
|a|

(−ω) =
1
a2 + (−ω)2 =
1
a2 + ω2 . ⃝
17.1.1 Convolution
Recall that we defined the convolution on the real line by
(f ∗g)(x) =
1
√
2π
∞

−∞
g(x −ξ) f(ξ) dξ.
Theorem 9.10 in Section 9.4 (convolution) states that we have (9.45) in Section 9.4, that is,
F[(f ∗g)(x)] = F[f(x)] · F[g(x)]
and
(f ∗g)(x) = F−1[F(ω) G(ω)],
assuming F(ω) ≜F[f(x)](ω) and G(ω) ≜F[g(x)](ω) exist.
17.1.2 Problems
In problems 1–4, use the definition to find the Fourier transform of the given function.
1. f(x) = e−|x|
2. f(x) = e−a|x|, where a is a positive constant
3. f(x) =
x
1+x2
4. f(x) =
x
a2+x2
5. Find the inverse Fourier transform of F(ω) = e−|ω|.
In problems 6 and 7 use the method of Theorem 9.11 in Section 9.4(b) and the results of
previous examples to find the Fourier transform or the inverse Fourier transform of the
given function.
6. F

e−a|x|
, where a is a positive constant
7. F−1
e−βω2
8. Give a plausible explanation why F[(f ∗g)(x)] = F[f(x)]·F[g(x)], that is, Table 17.1
entry F.12, is true. Your explanation may be a calculation in which you do not pay
attention to issues of convergence, that is, it does not have to be mathematically
rigorous.
9. If f(x) is an odd, real-valued function on (−∞, ∞), find a simpler expression for
F[f(x)] in terms of an integral over 0 ≤x < ∞involving sin(ωx).

Integral Transform Methods
1269
TABLE 17.1
Fourier Transforms: β > 0, k > 0, τ > 0, a, c, t0 Are Nonzero Constants
Formula
f(t)
F(ω) = F[f(t)] or F(ω) = F[f(x)]
F.1
f ′(x)
iωF(ω)
F.2
eiat f(t)
F(ω −a)
F.3
f(t) =
⎧
⎪⎨
⎪⎩
1,
−a < t < a
0,
|t| > a
⎫
⎪⎬
⎪⎭
a·

2
π ·
⎧
⎪⎪⎨
⎪⎪⎩
sin(aω)
aω
,
ω ̸= 0
1,
ω = 0
⎫
⎪⎪⎬
⎪⎪⎭
F.4
sinc(t) ≜sin t
t
, t ̸= 0

π
2 ·
⎧
⎪⎨
⎪⎩
1,
−1 < ω < 1
1
2,
ω = ± 1
0,
|ω| > 1
⎫
⎪⎬
⎪⎭
F.5
1
a2+t2

π
2 · e−|aω|
|a|
F.6
e−|ax|
|a|

2
π ·
1
a2+ω2
F.7
e−x2/2
e−ω2/2
F.8
f(ax)
1
a F
 ω
a

F.9
e−βx2
1
√
2β · e−ω2/(4β)
F.10
f(t −t0)
e−iωt0 F(ω)
F.11
e−at Step(t), where a > 0
1
√
2π
1
a+iω
F.12
(f ∗g)(t)
F(ω)G(ω)
F.13
f(x) cos kx
1
2

F(ω −k) + F(ω + k)

F.14
f(x) sin kx
1
i2

F(ω −k) −F(ω + k)

F.15
f(x) =
⎧
⎪⎨
⎪⎩
sin kx,
0 ≤x ≤τ
0,
all other x
⎫
⎪⎬
⎪⎭

2
π ·
1
k2−ω2

e−iωτ 
−iω sin kτ −k cos kτ

+ k

10. Suppose a is a positive constant. Use Table 17.1 entry F.11 and convolution to
explain why
F−1

1
(a + iω)2

= te−at Step(t).
11. If f(x) is an even, real-valued function on (−∞, ∞), explain why
F[f(x)](ω) =

2
π
∞

0
f(x) cos(ωx) dx,
which is called the Fourier cosine transform, denoted by Fc[f(x)](ω). [Hints:
Explain why f(x) being even and real valued on (−∞, ∞) implies both (a)
Fc[f(x)](ω) = Re

F[f(x)](ω)

and (b) F[f(x)](ω) is real.]

1270
Advanced Engineering Mathematics
17.2 Applications to Partial Differential Equations
In order to use Fourier transforms to solve partial differential equations (PDEs) or ordi-
nary differential equations (ODEs) we need to get a formula for the Fourier transform of a
derivative.
Theorem 17.1
Assuming f(x) and f ′(x) are absolutely integrable on the real line, and limx→±∞f(x) = 0,
F[f ′(x)](ω) = iωF[f(x)](ω).
(17.11)
Why? Integration by parts gives
F[f ′(x)](ω) =
1
√
2π
lim
R→∞
R
−R
f ′(x)e−iωx dx
=
1
√
2π
lim
R→∞
⎛
⎝

f(x)e−iωxR
−R −
R
−R
f(x)(−iωe−iωx)dx
⎞
⎠,
and use of limR→∞

f(R)e−iωR −f(−R)eiωR
= 0 explains why
= 0 +
iω
√
2π
lim
R→∞
R
−R
f(x)e−iωx dx = iωF[f(x)](ω). 2
The method of separation of variables that we used in Chapter 11 works well for a spatial
variable in a finite interval. But when a spatial variable is in an infinite interval, it helps a
lot to use integral transforms.
Example 17.5
Solve the heat equation in one space dimension with no sources or sinks. Assume
limx→±∞temperature = 0 and limx→±∞heat flux = 0.
Method: The PDE is
∂u
∂t = α ∂2u
∂x2 ,
where α is a positive constant and u = u(x, t). We will write the subscript in Fx to indicate
the Fourier transform of the x dependence. As in the use of Laplace transforms, we will
denote the transform of a function of x by the corresponding capital letter function of ω:
U(ω, t) ≜Fx[u(x, t)](ω) ≜
∞

−∞
u(x, t) e−iωx dx.

Integral Transform Methods
1271
Take the Fourier transform of both sides of the PDE to get
Fx
∂u
∂t (x, t)

(ω) = α Fx

∂2u
∂x2 (x, t)

(ω).
The right-hand side is simplified by using Theorem 17.1, and we assume that we can
interchange the two operations of (a) partial differentiation with respect to t and (b)
taking the Fourier transform with respect to x to get
∂U
∂t (ω, t) = α (iω)2U(ω, t);
hence,
∂U
∂t = −α ω2U,
(17.12)
for short.
We can solve (17.12) as if it were the first-order ODE of exponential decay:
U(ω, t) = A(ω) e−αω2t,
for an arbitrary function A(ω).
In terms of the initial heat distribution u(x, 0) for −∞< x < ∞, we have
A(ω) = U(ω, 0) = Fx
∂u
∂t (x, 0)

(ω) =
1
√
2π
∞

−∞
u(x, 0) e−iωx dx.
This is good progress on finding the solution. But just as for solving ODEs using
Laplace transforms, we need to transform back to get the solution as a function of (x, t),
not (ω, t):
u(x, t) = F−1
x

U(ω, t)

(x) = F−1
x

e−αω2t · A(ω)

(x).
The latter is the inverse Fourier transform of a product, so we can use convolution
Theorem 9.12 in Section 9.4 to express the solution as a convolution:
u(x, t) =

F−1
x

e−αω2t
∗F−1
x

A(ω)
	
(x) =
1
√
2π
∞

−∞
g(x −ξ, t)u(ξ, 0) dξ.
Here,
g(x, t) ≜F−1
x

e−αω2t
(x) =
1
√
2αt
· e−x2/(4αt),
by using the result of Example 17.3 in Section 17.1 with β = αt. So, the solution of the
problem is
u(x, t) =
1
2
√
παt
∞

−∞
e−(x−ξ)2/(4αt) u(ξ, 0) dξ. ⃝
(17.13)
The function
G(x, t) ≜
1
√
2π
· g(x, t) ≜
1
2
√
παt
e−x2/(4αt)

1272
Advanced Engineering Mathematics
is called the heat kernel or Green’s function or propagator, and (17.13) can be rewritten as
u(x, t) =
∞

−∞
G(x −ξ, t) u(ξ, 0) dξ.
The earlier calculations worked well for a PDE on the real line −∞< x < ∞. Other
problems take place on a half line and use the next “tool” in our toolbox.
17.2.1 Fourier Cosine and Sine Transforms
Recall from Problem 17.1.2.11 the definition of the Fourier cosine transform, which came
up naturally when taking the Fourier transform of an even function. Problem 17.1.2.11
asked you to explain why
F

f(x)

(ω) =
1
√
2π
∞

−∞
f(x) e−iωx dx =
1
√
2π
∞

−∞
f(x) (cos(ωx) −i sin(ωx)) dx
reduces to the Fourier cosine transform, defined by
Fc[f(x)] ≜

2
π
∞

0
f(x) cos(ωx) dx,
(17.14)
if f(x) is even, that is, f(−x) ≡f(x). If f(x) were to satisfy some differential equation on
the half line 0 < x < ∞and satisfy f ′(0) = 0, a requirement automatically met by an even,
differentiable function, then the Fourier cosine transform may be a relevant tool.
Similarly, if f(x) is odd, then the Fourier transform reduces to the Fourier sine trans-
form, defined by
Fs[f(x)] ≜

2
π
∞

0
f(x) sin(ωx) dx.
(17.15)
If f(x) were to satisfy some differential equation on the half line 0 < x < ∞and satisfy
f(0) = 0, a requirement automatically met by an odd, continuous function, then the Fourier
sine transform may be a relevant tool.
Another motivation comes from calculating transforms of the second derivative.
Theorem 17.2
Assuming f(x), f ′(x), and f ′′(x) are absolutely integrable on [0, ∞), limx→∞f(x) = 0, and
limx→∞f ′(x) = 0,

Integral Transform Methods
1273
(a)
Fc[f ′′(x)](ω) = −

2
π · f ′(0) −ω2Fc[f(x)](ω),
(17.16)
(b)
Fs[f ′′(x)](ω) = ω ·

2
π · f(0) −ω2Fs[f(x)](ω),
(17.17)
where Fc[f ′′(x)](ω) or Fs[f ′′(x)](ω) is assumed to exist, respectively.
Why? (a) Using integration by parts twice and ignoring issues of convergence,
Fc[f ′′(x)](ω) =

2
π · lim
R→∞
R
0
f ′′(x) cos(ωx) dx
=

2
π · lim
R→∞


f ′(x) cos(ωx) + ωf(x) sin(ωx)
R
0
−ω2
R
0
f(x) cos(ωx) dx

=

2
π · lim
R→∞

f ′(R) cos(ωR) + ωf(R) sin(ωR)
−f ′(0) −0 −ω2
R
0
f(x) cos(ωx) dx

=

2
π ·

0 + 0 −f ′(0) −ω2
R
0
f(x) cos(ωx) dx

= −

2
π · f ′(0) −ω2Fc[f(x)](ω).
To explain (b) in Problem 17.2.2.10 as follows, you will use arguments similar to the
earlier arguments for (a). 2
A problem that specifies a nonzero value for f ′(0) suggests using the Fourier cosine trans-
form in order to incorporate the value of f ′(0). Similarly, a problem that specifies a nonzero
value for f(0) suggests using the Fourier sine transform.
In order to use the Fourier cosine or sine transform, it helps to know more of their
properties.
Theorem 17.3
(Fourier cosine and sine transform inversion theorems) Assume f(x) is absolutely inte-
grable on [0, ∞) and piecewise smooth on every finite interval contained in [0, ∞).
(a) Assume f is continuous at x and Fc(ω) ≜Fc[f(x)](ω) exists. Then
f(x) = F−1
c
[Fc(ω)](ω) ≜

2
π
∞

0
Fc(ω) cos(ωx) dω.
(17.18)
(b) Assume f is continuous at x and Fs(ω) ≜Fs[f(x)](ω) exists. Then
f(x) = F−1
s
[Fs(ω)](ω) ≜

2
π
∞

0
Fs(ω) sin(ωx) dω.
(17.19)

1274
Advanced Engineering Mathematics
Why? The reader can find references for explanations in the “Learn More About It”
section as follows. Note also that, as for the convergence of Fourier series and for the
Fourier inversion theorem, at a point where f(x) has a finite jump, we replace f(x) by
1
2

f(x−) + f(x+)

.
Analogous to Theorem 9.8 in Section 9.4 we have the following:
Theorem 17.4
For j = s or j = c, suppose a is a nonzero constant and Fj(ω) ≜Fj[f(x)](ω) exists. Then
Fj[f(ax)](ω) = 1
a Fj
ω
a
	
and
F−1
j

Fj
ω
a
	 
(x) = aF−1
j
[Fj(ω)](x),
(17.20)
where Fj[f(x)](ω) is assumed to exist.
Theorem 17.5
(Convolutions) Suppose there exist both the Fourier sine transform Fs(ω) ≜Fs[f(x)](ω) and
the Fourier cosine transforms Gc(ω) ≜Fc[g(x)](ω), and Hc(ω) ≜Fc[h(x)](ω). Then
F−1
s
[Gc(ω)Fs(ω)](ω) = 1
2
∞

0

g(|x −ξ|) −g(x + ξ)

f(ξ) dξ,
(17.21)
and
F−1
c
[Gc(ω)Hc(ω)](ω) = 1
2
∞

0

g(|x −ξ|) + g(x + ξ)

h(ξ) dξ.
(17.22)
Why? See the “Learn More About It” section as follows.
Example 17.6
Solve the problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
u(0, t) = φ(t), 0 < t < ∞
u(x, 0) = f(x), 0 < x < ∞
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
,
(17.23)
along with the boundary conditions that lim|x|→∞u(x, t) = lim|x|→∞∂u
∂x (x, t) = 0.

Integral Transform Methods
1275
This problem corresponds to diffusion from a source at the left end of the half line
0 < x < ∞.
Method: Because a possibly nonzero u(0, t) is specified, it makes sense to use the Fourier
sine transform
U(ω, t) ≜Fs,x[u(x, t)](ω) ≜

2
π
∞

0
u(x, t) sin(ωx) dx.
Take the Fourier transform of both sides of the PDE to get
Fs,x
∂u
∂t (x, t)

(ω) = αFs,x

∂2u
∂x2 (x, t)

(ω).
We assume that we can interchange the two operations of (a) partial differentiation
with respect to t and (b) taking the Fourier sine transform with respect to x. So, Theorem
17.1 applied to this PDE implies
∂U
∂t (ω, t) = α ω ·

2
π · u(0, t) −α ω2U,
that is,
∂U
∂t = α ω ·

2
π · φ(t) −α ω2U,
(17.24)
using the assumptions that both u(x, t) →0 and ∂u
∂x (x, t) →0 as |x| →∞.
We can solve (17.24) as if it were a first-order linear ODE:
∂U
∂t (ω, t) + α ω2U = α ω ·

2
π · φ(t)
has integrating factor μ(t) = eαω2t, so the ODE can be rewritten as
d
dt

eαω2tU

= αω ·

2
π · eαω2t φ(t),
whose solutions are, implicitly,
eαω2tU(ω, t) = α ω ·

2
π
t
0
eαω2τ φ(τ) + C(ω).
Explicitly,
U(ω, t) = α ω ·

2
π
t
0
e−αω2(t−τ) φ(τ) + C(ω)e−αω2t,
where C(ω) is an arbitrary function of ω.
Substitute in the initial condition to get
Fs(ω) ≜Fs,x

f(x)

(ω) = U(ω, 0) = 0 + C(ω),
so
U(ω, t) = α ω ·

2
π
t
0
e−αω2(t−τ) φ(τ) + Fs(ω)e−αω2t.

1276
Advanced Engineering Mathematics
To find the solution u(x, t), use the inverse Fourier sine transform
u(x, t) =

2
π F−1
s,x
⎡
⎣
t
0
α ωe−αω2(t−τ) φ(τ) dτ
⎤
⎦(x) + F−1
s,x

Fs(ω)e−αω2t
(x).
(17.25)
The second term in (17.25) is relatively easy to calculate using the Convolution The-
orem result (17.21). To prepare for that, we note that the function g(x) ≜
1
√
2αt e−x2/(4αt)
is an even, real-valued function of x, so Problem 17.1.2.11 implies that its Fourier cosine
transform equals its Fourier transform. Using the result of Example 17.3 in Section 17.1,
the latter can be calculated to be
F

1
√
2αt
e−x2/(4αt)

= e−αω2t,
so
Gc(ω) = Fc[g(x)](ω) = F

g(x)

= e−αω2t.
Using (17.21)’s convolution result and our definition that Fs(ω) = Fs,x

f(x)

, we have
F−1
s,x

Fs(ω)e−αω2t
(x) = 1
2
∞

0

g(|x −ξ|) −g(x + ξ)

f(ξ) dξ
= 1
2
∞

0
#
1
√
2αt
e−(x−ξ)2/(2αt) −
1
√
2αt
e−(x+ξ)2/(2αt)
$
f(ξ) dξ
=
1
2
√
2αt
∞

0

e−(x−ξ)2/(2αt) −e−(x+ξ)2/(2αt)
f(ξ) dξ.
To calculate the first term in (17.25), Example 17.7 in the following has
F−1
s,x

ωe−αω2t
(x) =
x
(2αt)3/2 e−x2/(4αt),
so it follows that
F−1
s,x

α ωe−αω2(t−τ)
(x) =
αx

2α(t −τ)
3/2 e−x2/(4α(t−τ)).
This enables us to calculate the first term in (17.25):

2
π · F−1
s,x
⎡
⎣
t
0
α ωe−αω2(t−τ) φ(τ) dτ
⎤
⎦(x) =

2
π ·
t
0
F−1
s,x

αωe−αω2(t−τ)
(x)φ(τ) dτ
=

2
π
t
0
αx

2α(t −τ)
3/2 e−x2/(4α(t−τ))φ(τ) dτ.
Put the first and second terms together to conclude that the solution of the problem is
u(x, t) =

2
π
t
0
αx

2α(t −τ)
3/2 e−x2/(4α(t−τ))φ(τ) dτ
+
1
2
√
2αt
∞

0

e−(x−ξ)2/(2αt) −e−(x+ξ)2/(2αt)
f(ξ) dξ. ⃝

Integral Transform Methods
1277
Example 17.7
Explain why
(a)
F−1
s

ωe−αω2t
(x) =
x
(2αt)3/2 e−x2/(4αt)
(17.26)
and
(b)
F−1
c

e−αω2t
(x) =
1
√
2αt
e−x2/(4αt).
(17.27)
Method: In Problem 17.2.2.11, you will do part (b), which has been discussed earlier.
For part (a), first, we use symmetry to get an integral over the real line in order to later
apply a contour integral method in the complex plane:
F−1
s

ωe−αω2t
(x) ≜

2
π
∞

0
ωe−αω2t sin(ωx) dω = 1
2 ·

2
π
∞

−∞
ωe−αω2t sin(ωx) dω,
because ωe−αω2t sin(ωx) is an even function of the real variable ω.
For convenience, define
w(x, t) ≜F−1
s

ωe−αω2t
(x),
which is what we are calculating. Because sin(ωx) = 1
2i

eiωx −e−iωx
,
w(x, t) = 1
2i ·
1
√
2π
∞

−∞
ωe−αω2t 
eiωx −e−iωx	
dω = 1
2i

v+(x, t) + v−(x, t)

,
(17.28)
where
v±(x, t) ≜
1
√
2π
∞

−∞
ωe−αω2t e±iωxdω.
(17.29)
First, by Theorem 9.8 in Section 9.4, v+(x, t) is an inverse Fourier transform:
v+(x, t) = F−1
F
ω
a
	 
= af(ax) =
1
√
2αt
· f
#
x
√
2αt
$
,
(17.30)
where f(x) = F−1
F(ω)

, a =
1
√
2αt, and
F
ω
a
	
= ωe−αt ω2 =⇒F(ω) = aω e−αt a2 ω2 =
1
√
2αt
ωe−ω2/2.
So, we need to calculate
f(x) = F−1[F(ω)] = F−1

1
√
2αt
ωe−ω2/2

=
1
√
2π
∞

−∞
1
√
2αt
ωe−ω2/2 eiωxdω.
As usual, we complete the square to get
f(x) = e−x2/2 ·
1
√
2π
∞

−∞
1
√
2αt
ωe−(ω−ix)2/2 dω;

1278
Advanced Engineering Mathematics
CR,1
CR,2
CR,4
Re w
Im w
Im w=x
–R
CR,3
R
FIGURE 17.3
Large rectangular contour.
hence,
f(x) =
1
√
2αt
e−x2/2 ·
1
√
2π
 ∞

−∞
(ω −ix)e−(ω−ix)2/2dω + ix
∞

−∞
e−(ω−ix)2/2 dω

.
(17.31)
From now on, we consider ω to be a complex variable so that we can use contour
integration methods.
For the first integral in (17.31), we use a contour integration as in Example 17.2 in
Section 17.1. The large rectangular contour is shown in Figure 17.3. The function G(ω) ≜
(ω −ix)e−(ω−ix)2/2 is analytic in ω everywhere, so the Cauchy–Goursat theorem and the
calculation that


CR,4
ωe−(ω−ix)2 dω

→0
and


CR,2
ωe−(ω−ix)2 dω

→0
as R →∞,
imply that
−
∞

−∞
(ω −ix)e−(ω−ix)2/2 dω = lim
R→∞

−CR,1
ωe−ω2 dω = lim
R→∞

CR,3
ωe−ω2 dω = lim
R→∞0 = 0,
because ωe−ω2 is an odd function on the interval −R < ω < R. Similarly, the function
H(ω) ≜e−(ω−ix)2/2 is analytic in ω everywhere, so the Cauchy–Goursat theorem and the
calculation that


CR,4
e−(ω−ix)2 dω

→0
and


CR,2
e−(ω−ix)2 dω

→0
as R →∞,
imply that
∞

−∞
e−(ω−ix)2/2 dω = lim
R→∞

−CR,1
e−ω2 dω = lim
R→∞

CR,3
e−ω2 dω =
∞

−∞
e−ω2/2 dω =
√
2π,
so
f(x) =
1
√
2αt
· e−x2/2 ·
1
√
2π
· (ix) ·
√
2π =
1
√
2αt
· (ix) · e−x2/2.

Integral Transform Methods
1279
It follows that
v+(x, t) =
1
√
2αt
· f
#
x
√
2αt
$
=
1
√
2αt
·

1
√
2αt
· (ix) · e−x2/2
x→
x
√
2αt

=
1
√
2αt
·
1
√
2αt
·
#
i
x
√
2αt
$
· e−x2/(4αt) =
ix
(2αt)3/2 · e−x2/(4αt).
Similar calculations explain why, again,
v−(x, t) =
ix
(2αt)3/2 · e−x2/(4αt).
So, what we have calculated is, by (17.28),
F−1
s

ωe−αω2t
(x) = w(x, t) = 1
2i

v+(x, t) + v−(x, t)

=
x
(2αt)3/2 · e−x2/(4αt). ⃝
Theorem 17.6
(Parseval’s
theorem)
Suppose
there
exist
Fc(ω) = Fc[f(x)](ω),
Fs(ω) = Fs[f(x)](ω),
Gc(ω) = Fc[g(x)](ω), and Gs(ω) = Fc[g(x)](ω). Then
∞

0
f(x) g(x) dx =
∞

0
Fc(ω) Gc(ω) dω
and
∞

0
f(x) g(x) dx =
∞

0
Fs(ω) Gs(ω) dω.
(17.32)
Why? See the “Learn More About It” section as follows.
Learn More About It
Problem 17.2.2.6 is from Applied Partial Differential Equations, by Paul DuChateau and
David Zachmann, Harper & Row Publishers, Inc.,
c⃝1989. Problems 17.2.2.8 and
17.2.2.9 are based on unpublished lecture notes of a course given by Melvin Baron
at Columbia University in 1975. A derivation of (17.37), found in Problem 17.2.2.9 as
follows, is on pp. 18–20 of Lectures on Applications-Oriented Mathematics, by Bernard
Friedman, ed., by Victor Twersky, Holden-Day Inc.,
c⃝1969 (reprinted by Wiley
Classics Library, c⃝1991).
Derivations of Theorems 17.3, 17.5, and 17.6 are in Section 1.3 of Fourier Transforms,
by Ian N. Sneddon, Dover Publications, Inc., c⃝1995.
17.2.2 Problems
Throughout these problems, assume the solution of the PDE satisfies the boundary
condition(s) limx→±∞|u(x, t)| < ∞, limx→±∞|u(x, y)| < ∞, or limy→±∞|u(x, y)|< ∞, as
appropriate for the spatial domain on which the solution is to be defined.

1280
Advanced Engineering Mathematics
In problems 1 and 2 solve the problem by using the Fourier sine or cosine transform with
respect to x.
1.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
u(0, t) = 0, 0 < t < ∞
u(x, 0) =
%2,
0 < x < π
0,
π < x < ∞
&
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
2.
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
∂u
∂x(0, t) = 0, 0 < t < ∞
u(x, 0) =
%1,
0 < x < 3
0,
3 < x < ∞
&
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
3. Solve the heat equation in R2, that is,
⎧
⎨
⎩
∂u
∂t = α

∂2u
∂x2 + ∂2u
∂y2
	
, (x, y) in R2, 0 < t < ∞
u(x, y, 0) = f(x, y), (x, y) in R2
⎫
⎬
⎭.
Hint: Take the Fourier transforms with respect to both x and y, that is, use
U(ω, ν, t) ≜1
2π
∞

−∞
∞

−∞
u(x, y, t)e−iωxe−iνydxdy.
4. Use the Fourier cosine transform to solve
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
∂u
∂x(0, t) = ψ(t), 0 < t < ∞
u(x, 0) = 0, 0 < x < ∞
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
.
5. Use a Fourier cosine transform and (17.22) to solve the problem in an infinite strip
given by
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
0 = ∂2u
∂x2 + ∂2u
∂y2 , 0 < x < ∞, 0 < y < H
∂u
∂x(0, y) = 0, 0 < y < H
%u(x, 0) = f(x)
u(x, H) = 0
&
, 0 < x < ∞
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
.
You may assume that F−1
c,x
cosh(aω)
cosh(bω)

=
√
2π cos

πa/(2b)

cosh

πx/(2b)

b

cosh(πx/b) + cos(πa/b)
 , as
long as |Re(a)| < |Re(b)|.

Integral Transform Methods
1281
6. Use the Fourier transform to explain why the 1D wave equation
⎧
⎪⎨
⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , −∞< x < ∞, −∞< t < ∞
% u(x, 0) = f(x)
∂u
∂t (x, 0) = g(x)
&
, −∞< x < ∞
⎫
⎪⎬
⎪⎭
has solutions given by d’Alembert’s formula (10.88) in Section 10.5.
7. Explain why (17.27) is true.
Recall from Definition 7.21 in Section 7.7 that the error function erf is given by
erf(θ) ≜
2
√π
θ
0
e−y2 dy.
Define the complementary error function, erfc, by
erfc(θ) ≜1 −erf(θ).
(17.33)
8. Define I(x) ≜
 ∞
0
e−aω2 cos ωx dω, where a is a positive constant. Then
I(x) = 1
2 ·
π
a e−x2/(4a).
(17.34)
Establish (17.34) by the following:
a. Take the ordinary derivative of I(x) with respect to x and move the derivative
inside the integral by noting that the improper integral converges absolutely.
[Inside the integral, the derivative is the partial derivative with respect to x.]
After that, use integration by parts for
 b
0 . . . dω to eventually get
dI
dx + x
2a I = 0,
and the initial condition
I(0) =
∞

0
e−aω2 dω = 1
2 ·
π
a ,
the latter by using the fact that the error function satisfies limθ→∞erf(θ) = 1.
b. Solve part (a)’s ODE and initial condition in so as to evaluate I(x).
9. Let A be a constant. Here you will solve a special case of Example 17.6,
⎧
⎪⎪⎨
⎪⎪⎩
∂u
∂t = α ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
u(0, t) = A, 0 < t < ∞
u(x, 0) = 0, 0 < x < ∞
⎫
⎪⎪⎬
⎪⎪⎭
(17.35)
by using a derivation of an inverse Fourier sine transform.

1282
Advanced Engineering Mathematics
a. Explain why U(ω, t), the Fourier sine transform of the solution, satisfies an IVP
whose solution is
U(ω, t) =

2
π · A · 1
ω

1 −e−αω2t	
.
b. Break up u(x, t) = F−1
s
[U(ω, t)](x) as a sum of two inverse Fourier sine trans-
forms. The first involves
2
π
∞

0
sin ωx
ω
dω = 1, for x > 0,
(17.36)
which you may assume is true. [Its derivation is done using principal value and
residues using an “indented” contour to work around the difficulty at ω = 0.]
c. The second of the inverse Fourier sine transforms involves
2
π
∞

0
sin ωx
ω
e−αω2t dω = erf
#
x
2
√
αt
$
.
(17.37)
Establish (17.37) by using the following method: Problem 17.2.2.8 gives the
fact that
1
2 ·
π
a e−x2/(4a) = I(x) ≜
∞

0
e−aω2 cos ωx dω.
Use interchange of the order of integrations to obtain
x
0
I(u) du =
x
0
⎛
⎝
∞

0
e−aω2 cos ωu dω
⎞
⎠du =
∞

0
sin ωx
ω
e−αω2 dω.
(17.38)
After that, use the substitution a = αt and the left-hand side of (17.38) to finish
establishing (17.37).
d. Finish up by finding that the solution of the original problem (17.35), is
u(x, t) = A erfc
#
x
2
√
αt
$
.
10. Explain why Theorem 17.2(b) is true.
11. Use the result of Example 17.3 in Section 17.1 to explain why (17.27)(b) is true.

Integral Transform Methods
1283
17.3 Inverse Laplace Transform
We used the Laplace transform in an operational way in Chapter 4 by using the fact that
F(s) = L[f(t)] ≜
∞

0
f(t) e−st dt, s > a ⇐⇒
f(t) = L−1[F(s)], t ≥0.
(17.39)
In Example 4.20 in Section 4.4, we solved the IVP ˙y + 2y = 3e−4t, y(0) = 5, by calculating
that
y(t) = L−1[Y(s)] = L−1
13
2
1
s + 2 −3
2
1
s + 4

= 13
2 e−2t −3
2 e−4t
because, for example,
L[e−2t] =
1
s + 2, s > −2 ⇐⇒L−1

1
s + 2

= e−2t t ≥0.
In effect, we found y(t) = L−1[Y(s)] by using a table of Laplace transforms, such as
Table 4.1 in Section 4.5, by writing F(s) as a linear combination of Laplace transforms
whose inverse transforms we could look up.
The “table lookup” method works well for many problems, although we may need to
manipulate an F(s) into a form that is on the table. Also, a relatively exotic F(s) may require
us to consult long, extensive tables of expert knowledge of inverse Laplace transforms.
But we may need to find the inverse Laplace transform of an F(s) that is neither on
our table nor can be manipulated into a form on our table. Indeed, finding new inverse
transforms is how those long, expert level tables were created!
Here is a nonexpert level, hypothetical example of the historical process: After people
learned that L−1 
1
s+2

= e−2t, they realized that they would also need to find L−1
1
(s+2)2

,
for example, in order to solve the IVP ˙y + 2y = e−2t, y(0) = 0. In the process of finding the
latter, they created a new table entry:
L−1

1
(s + a)k+1

= tk
k! e−at, s > −a,
for any nonnegative integer k.
We need a method to find inverse Laplace transforms beyond the table lookup method.
This will help in more theoretical issues. For example, it would be good to know in general
which functions have an inverse Laplace transform.
From now on, we will consider functions F(s) to depend on a complex variable s. This
will allow us to use contour integration to find inverse Laplace transforms.
We need some technical definitions first.

1284
Advanced Engineering Mathematics
s0
Re(s)
Re(s)=s0
FIGURE 17.4
Left half circles.
Definition 17.1
An unbounded family of left half circles (Figure 17.4) is the collection of sets
Ss0,k ≜{s : |s −s0| = Rk and Re(s) ≤s0},
where s0 is a fixed real number and the positive radii Rk →∞as k →∞.
A set S is countably infinite if we can list the distinct elements of S as a sequence
'
sk
(∞
k = 1.
Definition 17.2
Suppose Ss0,k is an unbounded family of left half circles. We say F(s)
→
0 uni-
formly on that family if for every ε > 0, there is a K such that for all k ≥K,
|F(s)| < ε
for all s in Ss0,k.
Definition 17.3
F(s) is of class L if (a) F(s) →0 on some unbounded family of left half circles centered
at the real number s0 and (b) F(s) is analytic, except possibly at finitely many poles or a
countably infinite set of poles σ, all of which satisfy Re(σ) < s0.
Recall that by Definition 15.22 in Section 15.7, all poles are isolated singularities.

Integral Transform Methods
1285
Im s
C
Re s
s0
FIGURE 17.5
Bromwich contour.
Definition 17.4
A Bromwich contour C for F(s) is a vertical line contour
C : s = γ + iy, −∞< y < ∞,
for some real number γ , which lies to the right of all singularities of F(s).
Figure 17.5 illustrates a Bromwich contour for a function, each of whose poles is
indicated by an ×.
Theorem 17.7
(Laplace inversion theorem) Suppose F(s) is of class L using an unbounded family of left
half circles centered at the real number s0. Then there exists a real number γ > s0 such that
the Bromwich contour C : s = γ + iy, −∞< y < ∞, gives
L−1
F(s)

(t) ≜
1
2πi · P.V.

C
F(s) est ds, t ≥0.
(17.40)
Further,
L−1
F(s)

(t) =
)
k
Res

F(s) est; sk

, t ≥0,
(17.41)
where {sk} are the poles of F(s).

1286
Advanced Engineering Mathematics
Definition 17.5
f(t) is of exponential order if there are a positive number M and real numbers α, t0
such that
| f(t) | ≤Meαt,
for all t ≥t0.
Corollary 17.1
Suppose f(t) is of exponential order, with α as in Definition 17.5, and is piecewise
continuous on the interval [0, ∞). Then there exists
F(s) ≜L

f(t)

(t) ≜
∞

0
f(t) e−st dt,
for all s > α,
and F(s) is of class L, and
L−1
F(s)

(t) = f(t), t ≥0.
It is traditional to write (17.40), the contour integral for the inverse Laplace transform, as
L−1
F(s)

(t) ≜
1
2πi · P.V.
γ +i∞

γ −i∞
F(s) est ds, t ≥0.
(17.42)
When we are learning a new method that extends what we know, it is usually a good
idea to understand how the old results can be obtained by the new method.
Example 17.8
Evaluate L−1
 1
s−3

using a Bromwich contour.
Method: The only singularity of F(s) ≜
1
s−3 is at s = 3, so C : s = 4 + iy, −∞< y < ∞,
is a Bromwich contour. We use the residue theorem and the positively oriented curve
CR = CR,1 + CR,2 shown in Figure 17.6:
CR,1 : z = 4 + iy, −R ≤y ≤R
and
CR,2 : z = 4 + Reiθ, π
2 ≤θ ≤3π
2 .
We calculate, for any R > 0, that
1
2πi
⎛
⎝

CR,1
F(s)est ds +

CR,2
F(s)est ds
⎞
⎠=
1
2πi

CR
F(s)est ds =
1
2πi

CR
1
s −3 est ds
= Res
 est
s −3; 3

= e3t.

Integral Transform Methods
1287
Im s
Re s
4+iR
4–iR
CR,2
CR,1
FIGURE 17.6
Example 17.8.
For s on the left half circle CR,2 : s = 4 + Reiθ,
| F(s) | =

1
s −3
 =
1
|1 + Reiθ| ≤
1
R −1 →0,
as R →∞,
and
Re(s) = Re

4 + Reiθ	
≤4; hence, | est | ≤e4t,
for t ≥0.
Choose left half circles with R = Rk. Similarly to the argument in Example 17.1 in Section
17.1, we see that
1
|1 + Rkeiθ| ≤
1
Rk −1; hence,


CR,2
F(s) est ds
 ≤

CR,2
| F(s) | |est| ds ≤
3π
2
π
2
1
|1 + Rkeiθ| e4t dθ ≤πe4t ·
1
Rk −1 →0,
as k →∞. So,
L−1

1
s −3

(t) =
1
2πi P.V.
4+i∞

4−i∞
1
s −3 est ds =
1
2πi
lim
R→∞

CR,1
1
s −3 est ds
= lim
R→∞
1
2πi

CR
F(s)est ds = lim
R→∞Res
 est
s −3; 3

= lim
R→∞e3t = e3t. ⃝
Clearly, in a sophomore ODE course, it makes sense to find L−1 
1
s−3

(t) by using the
table lookup method rather than the Bromwich contour method. But we will see that the
generality of the latter is useful.

1288
Advanced Engineering Mathematics
Theorem 17.8
Suppose a function of class L can be written in the form
F(s) = p(s)
q(s).
Suppose F(s) is analytic except at {sk}, which is either a finite or a countably infinite set,
and that each sk is both a simple pole of F(s) and satisfies p(sk) ̸= 0. Then
L−1
F(s)

(t) =
)
k
p(sk)
q′(sk) eskt.
(17.43)
Why? The second “preparation” theorem (15.27) in Section 15.7 implies that q(s) =
(s−sk)g(s) where g(s) is analytic at sk and q′(sk) = g(sk) ̸= 0. When we calculate the residues
called for in Theorem 17.7, we get (17.43). 2
Formula (17.43) is a generalization of Heaviside’s method for partial fractions expan-
sion for a rational function F(s) = p(s)/q(s). Note that a rational function F(s) can be of
class L only if the degree of the polynomial p(s) is strictly less than the degree of the
polynomial q(s).
17.3.1 Solving a Wave Equation
Example 17.9
A flat string of length L is tied at its right end at x = L. Beginning at time t = 0, its left end
at x = 0 moves up and down with height sin t. Assuming that initially the string is flat
and at rest, find the motion of the string for t ≥0. Assume that c =

ϱ/T is constant and
L
πc is not an integer.
Method: The mathematical problem is
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, 0 < t < ∞
u(0, t) = sin t, 0 < t < ∞
u(L, t) = 0,
0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(17.44)
Let Lt denote the Laplace transformation operation with respect to t and define
U(x, s) ≜Lt[u(x, t)](s) =
∞

0
u(x, t) e−st dt.
By a property of Laplace transforms, the PDE in (17.44) and the initial conditions imply
Lt
∂2u
∂t2 (x, t)

(s) = s2U(x, s) −su(x, 0) −∂u
∂t (x, 0) = s2U(x, s) −s · 0 −0;

Integral Transform Methods
1289
hence,
s2U(x, s) = c2 ∂2U
∂x2 , 0 < x < L,
(17.45)
along with boundary conditions
U(0, s) = Lt[u(0, t)](s) = Lt[sin t](s) =
1
s2 + 1,
U(L, s) = Lt[u(L, t)](s) = Lt[0](s) = 0.
The solutions of the “ODE” (17.45), that is,
∂2U
∂x2 −s2
c2 U(x, s) = 0,
are
U(x, s) = c1(s) cosh
sx
c
	
+ c2(s) sinh
sx
c
	
,
(17.46)
or by using “clairvoyance” as in Example 9.32 in Section 9.6 and Section 11.3,
U(x, s) = A(s) sinh
s
c(L −x)
	
+ B(s) sinh
sx
c
	
,
(17.47)
for arbitrary functions A(s), B(s). The boundary conditions give
0 = U(L, s) = A(s) · 0 + B(s) sinh
#sL
c
$
and
1
s2 + 1 = U(0, s) = A(s) sinh
#sL
c
$
;
hence, B(s) = 0. The solution of the entire problem has Laplace transform∗
U(x, s) = A(s) sinh
s
c(L −x)
	
=
1
(s2 + 1) sinh

sL
c
	 · sinh
s
c(L −x)
	
.
(17.48)
So, all that remains is to find
u(x, t) = L−1
t

sinh
s
c(L −x)
	 * #
(s2 + 1) sinh
#sL
c
$$
(t).
(17.49)
Recall that for real u, v,
sinh(u + iv) = sinh u cos v + i cosh u sin v.
The poles of U(x, s), as a function of the complex variable s, are where s2 + 1 = 0 or
sinh

sL
c
	
= 0, that is, are at
s0,± = ± i
and
sn,± = ± i nπc
L , n = 1, 2, . . . .
The poles of U(x, s), as a function of s, are depicted in Figure 17.7. Note that s = 0 is a
removable singularity of U(x, s).
∗If we use (17.46), that is, not use clairvoyance as in (17.47), we will still arrive at (17.48) although it would take
more work and use of a hyperbolic trigonometric function identity.

1290
Advanced Engineering Mathematics
Im s
C
Re s
s0, +
s0, –
FIGURE 17.7
Example 17.9.
The assumption that “ L
πc is not an integer” guarantees that all of the poles are simple.
In addition, for any small but positive real number ε, for 0 < x < L, there are left half
circles Sε,k
sinh
s
c(L −x)
	 *
sinh
#sL
c
$
is bounded, so the factor of

1
s2+1
 in |U(x, s)| shows that U(x, s) is of class L in s.
So, we can apply Theorem 17.8 to conclude that
u(x, t) =
∞
)
j=0
)
ϵ=±1
Res

U(x, s)est; sj,ϵ

.
Let us calculate those residues: First, for j = 0 and ϵ = ± 1, s0,ϵ = iϵ, so
Res

U(x, s)est; s0,ϵ

= est sinh
s
c(L −x)
	*#
(s + iϵ) sinh
#sL
c
$$ 
s=iϵ
= eiϵt
i2ϵ · sinh
#iϵ
c (L −x)
$ *
sinh
#iϵL
c
$
= eiϵt
i2ϵ · sin
ϵ
c (L −x)
	 *
sin
#ϵL
c
$
.
= e± it
± i2 ·
#
± sin
#L −x
c
$$
·
#
± sin
#L
c
$$
= ± e± it
i2 · sin
#L −x
c
$
· sin
#L
c
$
.
Add the two residues from the poles s0,± to get
eit −e−it
2i
· sin
#L −x
c
$ *
sin
#L
c
$
,
that is,
sin t · sin
#L −x
c
$ *
sin
#L
c
$
.

Integral Transform Methods
1291
Similarly, for n ≥1 and ϵ = ± 1, sn,ϵ = iϵ · nπc
L , so
Res

U(x, s)est; sn,ϵ

=
lim
s→iϵnπc/L
#
s −iϵnπc
L
$
· est
·

sinh
s
c(L −x)
	 *
1 + s2	
·
#
1
*
sinh
#sL
c
$$
= eiϵnπct/L ·
#
sinh
#iϵnπ(L −x)
L
$*#
1 −
nπc
L
	2$$
·
lim
s→iϵnπc/L
##
s −iϵnπc
L
$ *
sinh
#sL
c
$$
.
L’Hôpital’s rule gives that this residue equals
= eiϵnπct/L ·
#
i sin
#ϵnπ(L −x)
L
$ * #
1 −
nπc
L
	2$$
·
1
L
c cosh(iϵnπ)
= eiϵnπct/L ·
#
iϵ sin
#nπ(L −x)
L
$ * #
1 −
nπc
L
	2$$
·
1
L
c (−1)n .
Noting that i = −1
i , adding the two residues from the poles sn,±, gives
2c
L (−1)n+1 · einπct/L −e−inπct/L
2i
· sin
#nπ(L −x)
L
$ * #
1 −
nπc
L
	2$
,
that is,
2c
L (−1)n+1 · sin
#nπct
L
$
· sin
#nπ(L −x)
L
$ * #
1 −
nπc
L
	2$
.
Putting everything together, we get that the solution of the whole problem is
u(x, t) =
sin

L−x
c
	
sin

L
c
	
· sin t +
2c
L
∞
)
n=1
(−1)n+1
1 −

nπc
L
	2 · sin
#nπ(L −x)
L
$
· sin
#nπct
L
$
. ⃝
Learn More About It
Theorem 17.7, a version of a Laplace inversion theorem, is found on pp. 301–302 of A
First Course in Partial Differential Equations: With Complex Variables and Transform Meth-
ods, by Hans Weinberger, Dover Publications Co. c⃝1995. Problem 17.3.2.11 is in the
exercises of Transform Methods for Solving Partial Differential Equations, 2nd edn., by
Dean G. Duffy, Chapman & Hall/CRC Press c⃝2004. [Problem 17.3.2.11 is a simplified
version of a problem solved in “Thermal effects on fluid flow and hydraulic fracturing
from well bores and cavities in low permeability formations,” Y. Wang and E. Papami-
chos, Int. J. Numer. Anal. Meth. Geomech., 23 (1999) 1819–1834.]
Most of the inverse Laplace transforms given in Table 17.2 involve √s, and their
derivations need integration over a contour that bends around the branch cut on the
half line Re(s) ≤0. Good references for this include Operational mathematics, 3rd edn.,
by Ruel V, Churchill, McGraw-Hill c⃝1971; the books by Weinberger and Duffy cited

1292
Advanced Engineering Mathematics
earlier; and Fundamentals of Complex Analysis..., by Saff and Snider cited in Section
15.10.
A source for Table 17.2 is Handbook of Mathematical Functions, ed., by Milton
Abramowitz and Irene Stegun, Dover Publications c⃝1964.
17.3.2 Problems
Throughout these problems, assume the solution of the PDE satisfies the boundary
condition(s) limx→±∞|u(x, t)|< ∞, limx→±∞|u(x, y)| < ∞, or limy→±∞|u(x, y)| < ∞, as
appropriate for the spatial domain.
In problems 1 and 2, find an appropriate Bromwich contour and use it to explain the
given inverse Laplace transform.
1. L−1
s+1
(s+1)2+4

= e−t cos 2t
2. L−1
 1
s e−as
= step(t −a),
assuming a is a positive constant,
where step
(t −1) =
%0,
t < a
1,
t ≥a
&
In problems 3–5, solve the initial value problem for the wave equation on a finite interval
with time-dependent boundary condition. Assume c =

ϱ/T is constant. For problem 4,
assume L
πc is not an integer; for Problems 3 and 5, assume 2L
πc is not an integer.
TABLE 17.2
Table of Inverse Laplace Transforms, Where a, k Are Nonzero
Constants
Formula
F(s)
f(t) = L−1[F(s)]
L2.1
1
√s
1
√
πt
L2.2
1
√
s3
2 ·

t
π
L2.3
1
a+√s
1
√
πt −a ea2t erfc(a
√
t)
L2.4
√s
s−a2
1
√
πt + a ea2t erfc(a
√
t)
L2.5
e−k√s
k
2
√
πt3 e−k2/(4t)
L2.6
1
√s e−k√s
1
√
πt e−k2/(4t)
L2.7
1
s e−k√s
erfc

k
2
√
t
	
L2.8
1
s√s e−k√s
2 ·

t
π e−k2/(4t) - k erfc

k
2
√
t
	
L2.9
e−k√s
a+√s
1
√
πt e−k2/4t −a eak ea2t erfc

k
2
√
t + a
√
t
	
L2.10
1
√
1+s2
J0(t)
L2.11
1
s e−s
J0(2
√
t)
L2.12
1
s arctan
 k
s

Si(kt) ≜
 kt
0
sin ξ
ξ
dξ

Integral Transform Methods
1293
3.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, 0 < t < ∞
%u(0, t) = sin 2t
u(L, t) = 0
&
, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
4.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, 0 < t < ∞
%u(0, t) = 0
u(L, t) = cos t
&
, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
5.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < L, 0 < t < ∞
%
u(0, t) = 0
∂u
∂x(L, t) = cos t
&
, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < L
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
6. Generalize Theorem 17.8 for finding L−1[F(s)], assuming that exactly two of the
poles, s0,±, of the function F(s) = p(s)
q(s) have order m ≥2 and the other poles are
simple. As in Theorem 17.8, assume F(s) is of class L.
In problems 7 and 8, assume c =

ϱ/T = 1 and solve the initial value problem for the wave
equation on a half line.
7.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < π, 0 < t < ∞
%u(0, t) = sin t
u(π, t) = 0
&
, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
8.
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < π, 0 < t < ∞
%u(0, t) = 0
u(π, t) = cos t
&
, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < π
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
9. Define
F(s) = e−x√s
s −1 .
Use partial fractions [let q = √s] to explain why
F(s) = e−x√s
2
#
1
√s −1 −
1
√s + 1
$
,

1294
Advanced Engineering Mathematics
and then use entry L2.9 of Table 17.2, the table of inverse Laplace transforms, to
explain why this can be evaluated as
L−1[F(s)] = et
2
#
e−xerfc
 x
2
√
t
−
√
t
	
+ exerfc
 x
2
√
t
+
√
t
	$
.
10. Use Laplace transforms to help solve the problem
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂2u
∂t2 = c2 ∂2u
∂x2 , 0 < x < ∞, 0 < t < ∞
⎧
⎨
⎩
∂u
∂x(0, t) = f(t)
limx→∞|u(x, t)| < ∞
⎫
⎬
⎭, 0 < t < ∞
u(x, 0) = ∂u
∂t (x, 0) = 0, 0 < x < ∞
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
.
[Hint: After taking the Laplace transform of the PDE, solve the ODE for functions
of s and apply the boundary condition at x = 0 and the boundedness as x →∞.]
11. Use Table 17.2, a table of inverse Laplace transforms, to help solve the system
⎧
⎪⎨
⎪⎩
∂u
∂t = α ∂2u
∂x2
∂v
∂t = α ∂2v
∂x2 + β ∂u
∂t
⎫
⎪⎬
⎪⎭
, 0 < x < ∞, 0 < t < ∞,
along with the boundary conditions
⎧
⎪⎪⎨
⎪⎪⎩
u(0, t) = 1, ∂v
∂x(0, t) = −1
limx→∞|u(x, t)| < ∞
limx→∞|v(x, t)| < ∞
⎫
⎪⎪⎬
⎪⎪⎭
, 0 < t < ∞.
Assume α and β are positive constants. [Hint: Use Laplace transforms to first solve
the PDE that only involves u(x, t).]
12. Use Corollary 4.1, a consequence of the second shift theorem for Laplace trans-
forms, to explain why
L−1
1
s e−cs−k√s

= erfc
#
k
2√t −c
$
step(t −c).
17.4 Hankel Transforms
In polar coordinates or cylindrical coordinates, the Hankel transform solves differential
equations on the interval 0 ≤r < ∞similarly to the way the Fourier sine or cosine trans-
form solved problems on 0 ≤x < ∞in Section 17.2. Also, we used Laplace transforms on
0 ≤t < ∞in Sections 4.4, 4.5, and 17.3. In all cases, (a) differentiations were turned into

Integral Transform Methods
1295
algebraic operations, (b) the main difficulty was in finding inverse transforms, and (c) it
helped to have tables and complex variable techniques to do that.
Define the Hankel transform of functions f(r) by
F(k) ≜H0

f(r)

≜
∞

0
f(r)J0(kr)r dr.
(17.50)
The Hankel transform inversion is given by
H−1
0
[F(k)] =
∞

0
F(k)J0(rk)k dk.
The outstanding property of the Hankel transform is that
Theorem 17.9
H0
1
r
d
dr

r df
dr(r)

= −k2F(k),
(17.51)
assuming f(r) satisfies the physical boundary conditions
| f(0+) | < ∞, where f(0+) ≜lim
r→0+ f(r)
(17.52)
and
√rf(r) →0 and √rf ′(r) →0 as r →∞.
(17.53)
Why? J0(kr) satisfies Bessel’s differential equation
1
r
d
dr

r dJ0
dr (kr)

+ k2J0(kr) = 0
(17.54)
and also the boundary conditions (Watson 1922) that
√rf(r) and √rf ′(r) are bounded as r →∞.
We calculate that
H0
1
r
d
dr

rdf
dr(r)

=
∞

0 
1
r
d
dr

rdf
dr(r)

J0(kr)r dr =
∞

0
d
dr

r df
dr(r)

J0(kr) dr,

1296
Advanced Engineering Mathematics
so integration by parts and the boundary conditions for f(r) and J0(kr) yield that this
improper integral equals
=
lim
a→0+, b→∞
⎛
⎝

r df
dr(r) J0(kr)
b
a −
b
a
r df
dr(r) · d
dr [J0(kr) ] dr
⎞
⎠
= 0 −0 −
∞

0
df
dr(r)
#
r d
dr [J0(kr)]
$
dr.
A second integration by parts calculates that this equals
= −
lim
a→0+, b→∞f(r)
#
r d
dr [J0(kr)]
$b
a
+
∞

0
f(r)
# d
dr

r dJ0
dr (kr)
$
dr,
so (17.54) implies that this is
= −0 + 0 +
∞

0
f(r)

−rk2J0(kr)
	
dr = −k2
∞

0
f(r)J0(kr)r dr = −k2H0

f(r)

= −k2F(k). 2
Note that if f = f(r) does not depend on θ, that is, f is circularly symmetric, then the
Laplacian operator  = ∇2 on f is given by f(r) = 1
r
d
dr

r df
dr(r)

.
Let’s use the theorem to help solve a PDE in polar coordinates.
Example 17.10
A substance∗initially has a uniform concentration in the disk 0 ≤r < a in the xy-plane.
For t > 0, the substance diffuses in the plane. Find the concentration of the substance for
t > 0 and (x, y) in R2.
Method: Let the concentration of the substance be c. The diffusion equation is
∂c
∂t = D∇2c,
where D is the constant diffusion coefficient, not a differential operator. (The “D” is
for “diffusion.” If the substance that is diffusing happens to be heat, then D = α, in the
notation of Chapters 10 and 11.)
The problem seems to be circularly symmetric, so we assume that c = c(r, t). Using the
Laplacian in polar coordinates, c should satisfy the PDE
∂c
∂t = D · 1
r
∂
∂r

r∂c
∂r(r, t)

and the initial condition
c(r, 0) =
%c0,
0 ≤r < a
0,
a ≤r < ∞
&
(17.55)
∗This is a version of Example 3.5.2 of Duffy (2004).

Integral Transform Methods
1297
and the physical boundary conditions
| c(0+, t) | < ∞
and
lim
r→∞
√r c(r, t) = lim
r→∞
√r ∂c
∂r(r, t) = 0, for t > 0.
Take the Hankel transform of both sides of the PDE and denote C(k, t) ≜H0 [c(r, t)].
Using Theorem 17.9, we get
∂C
∂t (k, t) = −Dk2C(k, t)
whose solution is
C(k, t) = e−Dk2tC(k, 0).
We calculate that
C(k, 0) = H0 [c(r, 0)] =
∞

0
c(r, 0)J0(kr)r dr =
a
0
c0J0(kr)r dr = c0
a
0
J0(kr)r dr.
As it happens (Spiegel 1968),

rJ0(r) dr = rJ1(r), so the change of variables R = kr gives

rJ0(kr) dr =
 R
k J0(R) dR
k = k−2 
RJ0(R) dR = k−2RJ1(R) = k−1rJ0(kr).
So,
C(k, 0) = c0 ·

k−1rJ0(kr)
a
0 = c0k−1aJ1(ka).
The solution of the problem is
c(r, t) = H−1
0

ac0k−1e−Dk2t J1(ka)

;
hence,
c(r, t) =
∞

0
ac0k−1e−Dk2t J1(ka)J0(rk)k dk = ac0
∞

0
e−Dk2t J1(ak)J0(rk)dk. ⃝
(17.56)
In principle, this solution can be computed at any value of (r, t), although the improper
integral over the infinite interval [0, ∞) suggests the calculations could be time consuming
or inaccurate. In the subject of integral transforms, there are many “tricks of the trade,”
as we can see from the Dean G. Duffy’s book mentioned earlier. Surprisingly, it involves
using the general transform C(k, 0) rather than its specific evaluation to be k−1aJ1(ka) !
The solution of the problem can be written as
c(r, t) = H−1
0
[C(k, t) ] = H−1
0

e−Dk2t C(k, 0)

=
∞

0
e−Dk2t
⎛
⎝
∞

0
c(ξ, 0)J0(kξ)ξ dξ
⎞
⎠J0(rk)k dk.
By changing the order of integration, this becomes∗
c(r, t) =
∞

0
⎛
⎝
∞

0
e−Dk2tJ0(kξ)J0(kr)k dk
⎞
⎠c(ξ, 0)ξ dξ.
∗We have corrected the typographical error in Duffy’s book that dk should be k dk.

1298
Advanced Engineering Mathematics
We can take advantage of the evaluation of “Weber’s second exponential integral”∗:
∞

0
e−Dk2tJ0(kξ)J0(kr)k dk =
1
2Dt exp

−r2 + ξ2
4Dt

I0
# rξ
2Dt
$
,
(17.57)
where I0 is the modified Bessel’s function of the first kind, of order zero. (We used the
function I0 in Example 11.21) in Section 11.6.
So, the solution of the PDE is
c(r, t) =
1
2Dt
∞

0
exp

−r2 + ξ2
4Dt

I0
# rξ
2Dt
$
c(ξ, 0)ξ dξ.
Substituting in our specific initial condition given by (17.55), we know the solution of the
problem is given by
c(r, t) = c0
2Dt
a
0
exp

−r2 + ξ2
4Dt

I0
# rξ
2Dt
$
ξ dξ.
It is very good that this form of the solution involves an integration over the finite interval
[0, a). The only thing that might worry us is the factor of t in many of the denominators.
While there appears to be difficulty near t = 0, in fact for r ≥0, ξ > 0, define v = t−1 and
calculate
lim
t→0+
1
2Dt exp

−r2 + ξ2
4Dt

=
lim
v→+∞
v
2D exp

−v · r2 + ξ2
4D

= 0,
by using L’Hôpital’s rule.
Learn More About It
Valuable resources include Transform Methods for Solving Partial Differential Equations,
2nd edn., by Dean G. Duffy, Chapman & Hall/CRC Press c⃝2004, Fourier Transforms,
by Ian N. Sneddon, Dover Publications, Inc. c⃝1995, and The Theory of Bessel Functions,
by G. N. Watson, Cambridge University Press c⃝1922 (reprinted 1996).
17.4.1 Problems
Throughout these problems, assume the solution of the PDE satisfies the boundary con-
dition(s) limr→∞|u(r, z, t)| < ∞, limz→∞|u(r, z, t)| < ∞or limr→∞|u(r, θ, t)| < ∞, as
appropriate for the spatial domain on which the solution is to be defined.
∗See p. 273 of Duffy’s book, p. 137 of Sneddon’s book, or p. 395 of Watson’s book.

Integral Transform Methods
1299
1. Find conditions on functions f(r) so that F(k) ≜H0

f(r)

satisfies
k4F(k) = H0

2f(r)

= H0
1
r
d
dr

r d
dr
1
r
d
dr

rdf
dr(r)

,
where  denotes the Laplacian operator and 2 is the bi-harmonic operator.
2. You may assume that (Watson 1922, page 389)
∞

0
e−akJν(bk)Jν(ck) dk =
1
π
√
bc
· Qν−1
2

a2 + b2 + c2
2bc

,
(17.58)
where Qβ(z) is called the Legendre function of the second kind, β is not an integer,
and a, b, c do not depend on k. Now, for ν = 0, differentiate (17.58) with respect to
a to explain why
∞

0
e−akJ0(bk)J0(ck) k dk = −
1
π
√
bc
· a
bc Q′
−1
2

a2 + b2 + c2
2bc

.
(17.59)
3. Let the concentration of a substance be c = c(r, z). Solve the steady-state problem
in a half space z > 0, that is, {(r, z) : 0 ≤r < ∞, 0 ≤z < ∞}, given by the diffusion
equation
0 = D∇2c, 0 < r < ∞, 0 < z < ∞,
where D is the constant diffusion coefficient and the boundary conditions are
| c(0+, z) | < ∞, and lim
r→∞c(r, z) = 0, for 0 < z < ∞and
c(r, 0) =
%c0,
0 ≤r < a
0,
a ≤r < ∞
&
.
Recall that in cylindrical coordinates with circular symmetry, ∇2c = 1
r
∂
∂r

r ∂c
tialr

+
∂2c
∂z2 . [Hint: (17.59) should be useful.]
4. You will explain why the diffusion problem for 0 < r < ∞, 0 < z < ∞, 0 < t < ∞,
given by
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂c
∂t = D · ∇2c
BCs : | c(0+, z, t) | < ∞
and
lim
r→∞c(r, z, t) = 0, 0 < z < ∞, 0 < t < ∞,
and
c(r, 0, t) = f(r), 0 < r < ∞, 0 < t < ∞
IC : c(r, z, 0) = 0, 0 ≤r < ∞, 0 < z < ∞
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(17.60)

1300
Advanced Engineering Mathematics
has solution
c(r, z, t) =
∞

0
J0(kr)

1 −e−Dk2t	
sinh(kz) F(k) k dk.
(17.61)
(a) The boundary condition at z = 0 suggests using the Fourier sine transform,
so define C(k, ω, t) = H0 [Fs [c(r, z, t)]]. Take both the Hankel and Fourier sine
transforms of both sides of the PDE in (17.60) to get
∂C
∂t (k, ωt) = −D(ω2 + k2)C −

2
π · ω · F(k),
where F(k) ≜H0[f(r)].
(b) Use the initial condition in (17.60) and solve the ODE for C(k, ω, t)’s depen-
dence on t to get
C(k, ω, t) =

2
π ·
ω
ω2 + k2 · F(k) ·

e−D(ω2+k2)t −1
	
.
(c) Use residues to evaluate
F−1
s

ω
ω2 + k2 ·

e−D(ω2+k2)t −1
	
=

2
π
∞

0
ω
ω2 + k2 ·

e−D(ω2+k2)t −1
	
sin ωz dz.
Note that the integrand is an even function, so we can replace
 ∞
0
. . . dω by
1
2
 ∞
−∞. . . dω.
(d) Finally, take the inverse Hankel transform of F−1
s
[C(k, ω, t)] to find that
c(r, z, t) = H−1
0

F−1
s
[c(r, z, t)]

satisfies (17.61).
5. In Section 10.4, we mentioned that a model for the vibration of a plate is given
by ϱ ¨w =−D∇4w + T∇2w −ϱg. Solve the problem
∂2w
∂t2 = −β2∇4w, 0 < r < ∞, 0 < t < ∞,
where w = w(r, t) satisfies appropriate boundary and initial conditions and β
is a positive constant.
Key Terms
bi-harmonic operator: Problem 17.4.1.1
Bromwich contour: Definition 17.4 in Section 17.3

Integral Transform Methods
1301
complementary error function, erfc: (17.33) in Section 17.2
convolution on the real line: end of Section 17.1, Problem 17.1.2.8
countably infinite: before Definition 17.3 in Section 17.3
exponential order: Definition 17.5 in Section 17.3
Fourier cosine transform: Problem 17.1.2.11, (17.14) in Section 17.2
Fourier sine transform: (17.15) in Section 17.2
F(s) →0 uniformly on that family: Definition 17.2 in Section 17.3
Green’s function: after (17.13) in Section 17.2
Hankel transform: (17.50) in Section 17.4
heat kernel: after (17.13) in Section 17.2
class L: Definition 17.3 in Section 17.3
propagator: after (17.13) in Section 17.2
unbounded family of left half circles: Definition 17.1 in Section 17.2
References
Duffy, D.G. Transform Methods for Solving Partial Differential Equations, 2nd edn. Chapman &
Hall/CRC Press, Boca Raton, FL, 2004.
Fisher, S.D. Complex Variables, 2nd edn. Dover Publications, New York, 1999, pp. 100–101.
Spiegel, M. R. Mathematical Handbook of Formulas and Tables, Schaum’s Outlines Series/McGraw-Hill,
New York, 1968.
Watson, G.N. The Theory of Bessel Function. Cambridge University Press, Cambridge, U.K., 1922,
Chapter 7.


18
Nonlinear Ordinary Differential Equations
18.1 Phase Line and Phase Plane
In this chapter, we will learn about nonlinear differential equations. In engineering often
our first mathematical model is linear but a more accurate model is nonlinear.
In general, we have an initial value problem (IVP), that is, a system of differential
equations and initial condition
˙x = f(x, t)
x(t0) = x0

,
(18.1)
and we will assume that we have existence and uniqueness of a solution to the IVP, mean-
ing that given an initial time t0 and an initial value x(t0), (18.1) has exactly one solution
x(t) for t in some interval (t0 −δ, t0 + δ) with δ > 0. This is the generalization to systems of
the results in Section 3.2. In fact, let us assume that f(x, t) is continuously differentiable in
x, which is enough to guarantee the existence and uniqueness property. We will say more
about this in Section 18.7.
Because of the uniqueness, we can notate that the solution of (18.1) is x(t) = x(t; t0, x0),
that is, we acknowledge that the solution of IVP (18.1) depends on the initial condition.
In the special case of initial time t0 = 0, we notate x(t; t0, x0) = x(t; x0); the omission of t0
will imply that the initial time is t0 = 0. Note that x(0; x0) = x0.
We saw in Chapters 3 and 4 that if x(t) = y(t) is one dimensional (1D), that is, x is in R1,
we can show a solution of (18.1) as a graph in the (t, y)-plane. For example, in Figure 18.1
we show a graph of the solution of ˙y + 10y = −2 sin 2t, y(0) = 20/13, which illustrated the
concept of a steady-state oscillation.
Similarly, x(t) in R2 can be graphed in (t, x1(t), x2(t))-space. But suppose the system
is autonomous, meaning that the right-hand side of the system is f(x), which does not
depend on t, that is, we are studying the IVP
 ˙x = f(x)
x(t0) = x0

.
(18.2)
Then, in a sense, all initial times t0 are the same.
Theorem 18.1
(Autonomous systems) If the system is autonomous, then x(t; t0, x0), the solution of IVP
(18.2), satisfies
1303

1304
Advanced Engineering Mathematics
1.5
y
1.0
0.5
2
4
6
8 t
FIGURE 18.1
Steady-state oscillation.
x(t; t0, x0) = x(t −t0; x0).
(18.3)
So, for an autonomous system, we can write the solution with any initial time in terms of
the solution with initial time t = 0 and the same initial value x0.
Why is Theorem 18.1 correct? By uniqueness of the solution of an IVP, it will suffice to
show that x(t −t0; x0) satisfies (18.2) just as x(t; t0, x0) does. First, by the chain rule,
d
dt

x

t −t0; x0

= ˙x(t −t0; x0) · d
dt

t −t0

= f

x(t −t0; x0)

· 1,
so x(t−t0; x0) satisfies the ordinary differential equation (ODE) in (18.2). Second, as for the
initial condition,
x(t −t0; x0)

t=t0 = x(0; x0) ≜x0. 2
For an autonomous system with x in R2, we can display the parametrized curve in R2
given by
C : r = x(t; x0), t in some interval I.
Also, we can display the parametrized curves for several initial values x0. In fact, in
Figure 6.14b, we saw several ellipses that are the parametric curve solutions of the
undamped harmonic oscillator system
	˙x
˙v

=
	
v
−k
m x

.
When we show several parametrized curve solutions corresponding to different choices
of initial values, we get a phase plane picture. Note that because of uniqueness of the
solution of IVPs, two solutions in the phase plane cannot touch.

Nonlinear Ordinary Differential Equations
1305
y.
y
3
3
2
2
1
1
–1
–1
–2
–2
–3
–3
FIGURE 18.2
Phase plane for a damped harmonic oscillator.
Example 18.1
The damped oscillator system
	˙y
˙v

=
	
v
−2y −0.4v

has the phase plane picture shown
in Figure 18.2. As we usually do for parametrized curves, the arrow indicates the
direction of travel as t increases.
The system in Example 18.1 has the origin as a stable spiral point. In general, supposing
A is a real 2 × 2 matrix, the planar, linear, autonomous system
˙x = Ax
has a stable spiral point at the origin if the eigenvalues of A are not real but a com-
plex conjugate pair λ± = α ± iβ with α = Re(λ±) < 0. The origin is an unstable spiral
point if, instead, α > 0.
Example 18.2
The nonlinear system
	˙x
˙y

=
	y −x3
−2x3y

has the phase plane picture shown in Figure 18.3.
Solutions in the phase plane were drawn using MathematicaTM. For example,
s = NDSolve[{x′[t] == y[t] −x[t]3, y′[t] == −2x[t]3, x[0] == 1, y[0] == 0},
{x, y}, {t, −20, 20}]

1306
Advanced Engineering Mathematics
1.0
1.0 x
y
0.5
0.5
–0.5
–0.5
–1.0
–1.0
FIGURE 18.3
Phase plane for a nonlinear system.
found an approximate, numerical solution for one initial point in the phase plane, and then
a = ParametricPlot[Evaluate[{x[t], y[t]}/.s], {t, −20, 20}, PlotStyle−> {Blue, Thick},
PlotRange →{{−1, 1}, {−1.2, 1.2}}, LabelStyle →Directive[FontSize →16], AxesLabel →
{x, y}, AxesStyle →Thickness[0.00315]]
was used to draw the corresponding phase plane curve.
18.1.1 Equilibria
Definition 18.1
System (18.1) has an equilibrium point ¯x if
x(t) ≡¯x
solves (18.1) for t in some open interval.
Other commonly used names for an equilibrium point are constant solution, rest point,
and critical point. The plural of “equilibrium point” is equilibria.
Theorem 18.2
The autonomous system of ODEs (18.2) has equilibrium point ¯x if, and only if, f(¯x) = 0.
Why? x(t) is constant for t in an open interval I if, and only if, 0 ≡˙x(t) = f

x(t)

on I. 2

Nonlinear Ordinary Differential Equations
1307
6
y
4
2
0
–2
0.5
1.0
1.5
2.0
t
FIGURE 18.4
Example 18.3.
18.1.2 Qualitative Study: The Phase Line
Figure 18.4 shows several solutions of a differential equation of the form
˙y = f(y),
(18.4)
that is, an autonomous ODE in R1. For this function f(y), ODE (18.4) has constant solutions
y(t) ≡1 and y(t) ≡4. By Theorem 18.2, f(1) = f(4) = 0.
We also see in Figure 18.4 that when y > 4, the solution y(t) is a decreasing function of
t. Also, when 1 < y < 4, the solution y(t) is an increasing function of t, and when y < 1, the
solution y(t) is a decreasing function of t.
Example 18.3
Find a differentiable function f(y) that has among the solutions of ˙y = f(y) the functions
of t graphed in Figure 18.4.
Method: Because f(1) = f(4) = 0, it makes sense for the differentiable function to have the
form
f(y) = (y −1)(y −4)g(y),
where g(y) is differentiable. We know that the product (y −1)(y −4) has
Sgn

(y −1)(y −4)

=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
+1,
y < 1
0,
y = 1
−1,
1 < y < 4
0,
y = 4
+1,
y > 4
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
From Figure 18.4, we know where y(t) is increasing or decreasing. This implies we need
Sgn

f(y)

=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
−1,
y < 1
0,
y = 1
+1,
1 < y < 4
0,
y = 4
−1,
y > 4
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
,
(18.5)

1308
Advanced Engineering Mathematics
so the simplest thing to do is let g(y) ≡−1. So, one example of the desired function f(y)
is f(y) = −(y −1)(y −4). ⃝
Theorem 18.3
Suppose f(y) is continuously differentiable and ¯y is an equilibrium point for (18.4), that is,
˙y = f(y). Then
(a) If y0 < ¯y, then solution y(t; y0) < ¯y for all t ≥0 for which the solution exists.
(b) If y0 > ¯y, then solution y(t; y0) > ¯y for all t ≥0 for which the solution exists.
Why? (a) Uniqueness of solutions and y0 ̸= ¯y implies that the two solutions y(t; y0) and
y(t; y0) ≡¯y cannot touch in the ty-plane. 2
Theorem 18.3 says that the constant solutions act as “barriers” in the ty-plane.
For a first-order scalar ODE, parametrized curve solutions
C : y = y(t), α ≤t ≤β
exist in R1.
Suppose the ODE is autonomous, that is, is ˙y = f(y). If we put all of the parametrized
curve solutions in one picture on the real line R1, we get the phase line, the 1D analogue
of the phase plane.
Example 18.4
Draw the phase line for the ODE of Example 18.3, that is, ˙y = −(y −1)(y −4).
Method: the Sgn

f(y)

results in (18.5) imply that the phase line is shown in
Figure 18.5. ⃝
y
6
4
2
0
–2
FIGURE 18.5
Example 18.4.

Nonlinear Ordinary Differential Equations
1309
18.1.3 Qualitative Study: The Phase Plane for LCCHS
Suppose A is a 2 × 2 constant matrix. Consider the LCCHS (linear constant coefficients
homogeneous system) of ODEs in R2, that is,
˙x = Ax,
that is,
˙x =
ax + by
˙y =
cx + dy

,
(18.6)
for some real constants a, b, c, and d. We know from Chapter 5 that there are several cases
for the solutions of (18.6) because there are several cases for A’s eigenvalues, λ1 and λ2:
(a) λ1 and λ2 real and not deficient
(b) λ1 and λ2 real, equal, and deficient
(c) λ1 and λ2 not real but a complex conjugate pair.
Particular sub-cases have commonly used names and qualitative description of the
solution curves in the phase plane.
18.1.4 Saddle Point Case
The saddle point case is when λ2 < 0 < λ1. Because the eigenvalues are distinct, there is a
complete set of basic solutions of the ODE system:
x(t) = c1eλ1tv1 + c2eλ2tv2 ,
(18.7)
where c1, c2 are arbitrary constants.
A parametrized curve solution with c1 ̸= 0 and c2 = 0 will lie on the straight line L1
through the origin spanned by v1. Similarly, a parametrized curve solution with c1 = 0
and c2 ̸= 0 will lie on the straight line L2 through the origin spanned by v2. Following the
terminology in the book by Blanchard, Devaney, and Hall mentioned in the following, we
refer to the these two types of solutions as straight line solutions.
If a solution has both c1 ̸= 0 and c2 ̸= 0, we will call it a curved solution.
Because λ2 < 0 < λ1, as t →∞, in a curved solution, the c1eλ1tv1 term will dominate.
In fact,
x(t) −c1eλ1tv1 = c2eλ2tv2 →0, as t →∞.
So, a curved solution will have L1 as an asymptote line, as t →∞.
As t →−∞, in a curved solution, the c2eλ2tv2 term will dominate. In fact,
x(t) −c2eλ2tv2 = c1eλ1tv1 →0, as t →−∞.
So, a curved solution will have L2 as an asymptote line, as t →−∞.

1310
Advanced Engineering Mathematics
y
x
3
3
2
2
1
1
–1
–1
–2
–2
–3
–3
FIGURE 18.6
Example 18.5.
Example 18.5
Analyze and then draw the phase plane for the system ˙x =

1
√
5
√
5
−3

x.
Method: It is easy to calculate that the general solution is
x(t) = c1e2t
√
5
1

+ c2e−4t

1
−
√
5

,
where c1, c2 are arbitrary constants. The straight line solutions are on the lines through
the vectors v1 =
√
5
1

and v2 =

1
−
√
5

. The curved solutions approach the line through
v1 as t →∞and approach the line through v2 as t →−∞. The phase plane is sketched
in Figure 18.6. ⃝
Why do we call the origin a “saddle point” in the case when the two eigenvalues are
λ2 < 0 < λ1? Because the solutions of the ODE system are level sets of a function that has
a saddle point, as we defined it in Section 13.2.
Example 18.6
Analyze and then draw the phase plane for the system ˙x =
−7
3
2
−8

x.
Method: It is easy to calculate that the general solution is
x(t) = c1e−5t
3/2
1

+ c2e−10t
−1
1

,

Nonlinear Ordinary Differential Equations
1311
–3
3
3 x
y
2
2
1
1
–1
–1
–3
–2
–2
FIGURE 18.7
Example 18.6.
where c1, c2 are arbitrary constants. The straight line solutions are on the lines through
the vectors v1 =
3/2
1

and v2 =
−1
1

. The curved solutions approach the origin parallel
to the line through v1 as t →∞, because e−5t goes to zero slower than does e−10t as
t →∞.
The curved solutions are roughly parallel to the line through v2 as t →−∞,
because e−10t is much larger than e−5t as t →−∞. The phase plane is sketched in
Figure 18.7. ⃝
Learn More About It
Either Differential Equations, 4th edn., by Paul Blanchard, Robert L. Devaney, and Glen
R. Hall, Brooks/Cole-Thomson, 2011, or Nonlinear Dynamics and Chaos, by Steven
H. Strogatz, Addison-Wesley, c⃝1994, is a good introduction to qualitative study of
differential equations, including systems.
18.1.5 Problems
For problems 1–5, analyze the system and state what kind of equilibrium point is at the
origin.
1. ˙x =
−1
√
3
√
3
1

x
2. ˙x =
5
−1
3
−1

x

1312
Advanced Engineering Mathematics
3. ˙x =
1
−4
2
−3

x
4. ˙x =
−1
1
0
−2

x
5. ˙x =
1
−4
2
−3

x
For problems 6–8, analyze and then draw the phase plane for the system.
6. ˙x =
−3
√
5
√
5
1

x
7. The system of Problem 18.1.5.1
8. The system of Problem 18.1.5.4
9. The IVP ¨x −x + 3
2 x2 = 0, x(0) = 1, ˙x(0) = 0 has a solution
(⋆)
x(t) = 1 −
1 −et
1 + et
2
.
a. Why does (⋆) give the unique solution of this IVP?
b. Use the aforementioned information to find the unique solution of the IVP
¨x −x + 3
2 x2 = 0, x(2) = 1, ˙x(2) = 0.
18.2 Stability of an Equilibrium Point
In the real world, we sometimes describe a situation as “stable,” meaning that it is steady
or enduring. For example, it is usually considered a good thing if a nation’s economy is
stable in the sense of having steady growth. Likewise, when we drive over a bridge, we
want it to be stable in the sense that the vibrations of the bridge will be so small that it
stands reasonably firm. In these descriptions, we have a qualitative, but not quantitative,
concept of stability.
Mathematically there are many precise, quantitative definitions of stability. We have
given some for linear homogeneous systems in Chapter 5: Specifically, Definition 5.7
in Section 5.3 gave definitions for constant coefficients ODEs, Definition 5.11 in Section
5.7 gave definitions for constant coefficients difference equations, and Definition 5.12 in
Section 5.8 gave definitions for periodic ODEs. It will turn out that these definitions for
linear systems are particularizations of definitions for systems in general, that is, systems
that may not be linear.
We will study the IVP (18.1) in Section 18.1, that is,
˙x = f(x, t)
x(t0) = x0

.
Recall that for a LCCHS (⋆)
˙x = Ax, we said that the system is asymptotically stable if
all its solutions have limt→∞x(t) = 0, and recall that Theorem 5.11(a) in Section 5.3 said
that if all of A’s eigenvalues λ satisfy Re(λ) < 0, then the system is asymptotically stable.

Nonlinear Ordinary Differential Equations
1313
Because 0 is an equilibrium point for LCCHS (⋆), our LCCHS definition of asymptotic
stability really is a statement about convergence to an equilibrium point. This gives us an
idea of how to define stability concepts for the general system of ODEs in (18.1).
Definition 18.2
(a) An equilibrium point ¯x is stable if for all ε > 0, no matter how small, and all initial times
t0, there exists some δ > 0, possibly dependent on ε or t0, such that for all t ≥t0 and initial
values x0 satisfying ||x0 −¯x|| < δ,
||x(t; t0, x0) −¯x|| < ε.
(18.8)
(a) An equilibrium point ¯x is unstable if it is not stable.
The definition of stability given in Definition 18.2 is sometimes called stability in the
sense of Liapunov. The latter was a Russian mathematician of the nineteenth century
who was one of the founders of the modern approach to dynamical systems.
Figure 18.8 illustrates Definition 18.2: A solution that starts in the ball Bδ(¯x) ≜{x : ||x0 −
¯x|| < δ} remains in the ball Bε(¯x) for all t ≥t0. In the figure, a ball in Rn appears to be an
interval on the x-axis. Note in the figure that two solutions appear to intersect at a point
(t1, y), which would violate uniqueness of the solution of IVP (18.1) in Section 18.1, but this
misleading appearance is due to displaying n dimensions of x in a 1D way. To illustrate
the lack of intersection, one of the two solutions is missing part of its graph, around (t1, y).
Example 18.7
The linear, undamped, harmonic oscillator system is
	˙y
˙v

=
⎡
⎣
v
−k
m y
⎤
⎦.
(18.9)
Let x = [y
v]T. We know from Example 6.13 in Section 6.3 that all of the nonzero
solutions of (18.9) in the yv-phase plane lie on an ellipse of the form
y2
a2 +
v2
(ωa)2 = 1,
(18.10)
δ=||x ––x||
(t1, y)
t
δ=||x ––x||
0=||x ––x||
||x– –x||= ε
||x – –x||= ε
||x ––x||=0
t0
t1
t2
FIGURE 18.8
Stability of ¯x.

1314
Advanced Engineering Mathematics
and are depicted in Figure 6.14. Recall that the constant frequency of vibration is
ω =

k/m and a =

y(0)
2 +

v(0)/ω
2 is the amplitude of the undamped harmonic
oscillation.
Linear system (18.9) has exactly one equilibrium point, namely, x = (y, v) = (0, 0). By
Definition 18.2, 0 is stable: For any ε > 0, no matter how small, let δ = ε/(
√
2 · max{1, ω})
to see that if

y(0), v(0)
 < δ, then every point on the solution

y(t), v(t)

, which lies on
the ellipse (18.10), has

y(t), v(t)
 ≤
√
2 · max{1, ω} ·

y(0), v(0)
 < ε.
The generalization of Example 18.7 was given in Theorem 5.11 (a) and (b) in Section 5.3;
LCCHS ˙x = Ax is either asymptotically stable or neutrally stable, that is, all its solutions are
bounded on [0, ∞), if all of A’s eigenvalues λ satisfy Re(λ) ≤0 and no deficient eigenvalue
λ has real part equal to zero. For an LCCHS, being either asymptotically stable or neutrally
stable is the same concept as stability in the sense of Liapunov.
It turns out that the concept of asymptotic stability is more sophisticated for nonlinear
systems than for LCCHS and thus needs to be broken down into two concepts.
Definition 18.3
An equilibrium point ¯x is an attractor if for all initial times t0, there exists some δ > 0,
possibly dependent on t0, such that for all initial values x0 satisfying ||x0 −¯x|| < δ,
lim
t→∞||x(t; t0, x0) −¯x|| = 0.
(18.11)
While condition (18.8) says that solutions stay near the equilibrium point, condition
(18.11) says that solutions get arbitrarily closer and closer to the equilibrium point.
For an LCCHS ˙x = Ax, 0 is the only equilibrium point, and it is an attractor exactly when
all of the eigenvalues of the constant matrix A have negative real part.
Definition 18.4
An equilibrium point ¯x is asymptotically stable if it is both stable and an attractor.
Again, for an LCCHS ˙x = Ax, 0 is the only equilibrium point, and it is an attractor exactly
when all of the eigenvalues of the constant matrix A have negative real part. So, the Defi-
nition 18.4 of asymptotic stability for a general, possibly nonlinear system ˙x = f(x), agrees
with Definition 18.4’s asymptotic stability when the system happens to be an LCCHS.
Unfortunately, for a nonlinear system, if n ≥2, then an equilibrium point in Rn can be
both unstable and an attractor. Suppose 0 is an equilibrium point and x(t) are the solutions.
Intuitively, in two or more dimensions, we can have both (1) x(t) →0 as t →∞, hence 0
is an attractor, and (2) x(t) goes far away from 0 on a finite time interval. If we have a
sequence of solutions x(k)(t) whose initial conditions x(k)(0) →0 as k →∞yet there is a
sequence of times tk at which ||x(k)(tk)|| ≥1, then 0 is unstable. This would correspond to

Nonlinear Ordinary Differential Equations
1315
the colloquial saying, “In the long run, everything will be fine, but in the short run, things
will be ruined.” A specific example of this is mentioned in “Learn More About It.”
18.2.1 Stability from Linearization
Recall that a system of ODEs is “autonomous” if f(x, t) does not depend on t. We say a
system of ODEs is periodic if the dependence of f(x, t) on t is periodic, that is, there is a
T > 0 such that f(x, t + T) ≡f(x, t).
We know a lot about the stability of the equilibrium point 0 of an LCCHS ˙x = Ax, or a
periodic linear homogeneous system ˙x = A(t)x, by using Theorem 5.11 in Section 5.3 or 5.25
in Section 5.8, respectively. Now we will see how to use that knowledge to get conclusions
about the stability of an equilibrium point of a nonlinear system of ODEs ˙x = f(x, t).
Suppose an autonomous system ˙x = f(x) has an equilibrium point ¯x, that is, f(¯x) = 0.
Using linear approximation, we have that
f(x) = f(¯x) + A(x −¯x) + g(x −¯x)
for x near ¯x, where
A ≜
 ∂f
∂x(¯x)

.
(18.12)
Definition 18.5
A quantity ε is o(hp) if there exists limh→0+ ||ε||
hp = 0.
Note that
g(x −¯x) ≜f(x) −f(¯x) −A(x −¯x) = o(||(x −¯x||), as x →¯x.
Why? Because we assumed that f(x) is continuously differentiable.
But, ¯x is an equilibrium point, so f(¯x) = 0. The system ˙x = f(x) is actually
˙y = Ay + g(y),
(18.13)
where y ≜x −¯x gives the displacement from equilibrium.
We call the system
˙y = Ay
(18.14)
the linearization of ˙x = f(x) near the equilibrium point ¯x.
Using (18.13), all textbooks of advanced ODEs establish the following result. We will
explain why it is reasonable in Section 18.3.

1316
Advanced Engineering Mathematics
Theorem 18.4
(Stability from linearization)
(a) If y = 0 is asymptotically stable for the linearization (18.14), that is, all eigenvalues
of the constant matrix A have negative real part, then ¯x is asymptotically stable
for ˙x = f(x).
(b) If the linearization (18.14) has at least one eigenvalue whose real part is positive,
then ¯x is unstable for ˙x = f(x).
Remarks
It is important to understand what Theorem 18.4 says and what it does not say. If the
linearization has an eigenvalue having zero real part, then neither Theorem 18.4(a) nor
Theorem 18.4(b) gives us a conclusion about the original, nonlinear system. The next two
examples show that the situation is complicated.
Example 18.8
In Problem 18.4.5.12, you will explain why the nonlinear system
˙x
˙y

=
y −x3
−2x3

has 0 as
a stable equilibrium point. This is true even though the linearization about 0 is unstable.
The linearization is ˙x = Ax, where
A =
⎡
⎣
∂
∂x

y −x3 
∂
∂y

y −x3 
∂
∂x

−2x3 
∂
∂y

−2x3 
⎤
⎦

(x,y)=(0,0)
=
	−3x2
1
−6x2
0

 
(x,y)=(0,0)
=
	0
1
0
0

.
Because A has a deficient zero eigenvalue, the linearization is unstable. ⃝
Lemma 18.1
If a system has the form
˙x = Ax + g(x −¯x)
(18.15)
and all of the terms in g(x −¯x) are nonlinear in the sense of being o(||x −¯x||), then the
linearization of (18.15) about x = ¯x is
˙x = Ax.
In particular, if g ≡0, then the linearization is ˙x = Ax.

Nonlinear Ordinary Differential Equations
1317
Example 18.9
In Example 18.11, we will explain why the nonlinear system
˙x
˙y

=
 0
1
−1
0
 x
y

+ β(x2 + y2)
x
y

(18.16)
has 0 as an asymptotically stable equilibrium point if the constant β < 0 but has 0
as an unstable equilibrium point if the constant β > 0. This is true even though the
linearization about 0 is stable: The linearization is ˙x = Ax, where
A =
 0
1
−1
0

by Lemma 18.1. Because A has eigenvalues ±i, the linearization is stable. ⃝
Examples 18.8 and 18.9 illustrate that if the linearization about an equilibrium point ¯x
has an eigenvalue whose real part is zero, then the original nonlinear system’s equilib-
rium point may be asymptotically stable or unstable. It is also possible that the original
nonlinear system’s equilibrium point may be stable but not asymptotically stable.
Example 18.10
The undamped nonlinear pendulum of length ℓcan be modeled∗by the system
	 ˙θ
˙v

=
⎡
⎣
1
ℓv
−g sin θ
⎤
⎦.
The inverted position is the equilibrium point (θ, v) = (π, 0). Use linearization to explain
why the inverted pendulum is unstable.
Method: The linearization is
 ˙θ
˙v

= A
θ
v

, where we calculate
A =
⎡
⎢⎣
∂
∂θ

1
ℓv

∂
∂v

1
ℓv

∂
∂θ

−g sin θ

∂
∂v

−g sin θ

⎤
⎥⎦

(θ,v)=(π,0)
=
⎡
⎣
0
1
ℓ
−g cos θ
0
⎤
⎦

(θ,v)=(π,0)
=
⎡
⎣0
1
ℓ
g
0
⎤
⎦.
Because the eigenvalues of A are ±

g/ℓ, the linearization has an eigenvalue with
positive real part. By Theorem 18.4, we conclude that the inverted pendulum is
unstable. ⃝
By the way, ω =

g/ℓis the frequency of the linear ODE for an undamped pendulum
obtained by approximating sin θ ≈θ near the equilibrium (θ, v) = (0, 0).
One elementary topic in control theory is how to apply force to stabilize the inverted
pendulum, for example, how to balance a long stick on your finger.
∗See also Problem 7.2.5.21 for the calculation of the potential energy and see the discussion of calculus of
variations for the pendulum in Section 14.2.

1318
Advanced Engineering Mathematics
18.2.2 Using r(t)
If x(t) is in Rn, define
r(t) ≜||x(t)|| =

x(t)Tx(t).
Some nice examples establish stability properties by using r(t).
Example 18.11
Suppose β is a constant. Study the stability properties of equilibria of the system
	
˙x
˙y

=
	
y + βx

x2 + y2
−x + βy

x2 + y2

.
(18.17)
Method: When β = 0, the only equilibrium point is (x, y) = (0, 0), because 0 = ˙x = y + 0 ·
x

x2 + y2
= y and 0 = ˙y = −x + 0 · y

x2 + y2
= −x imply (x, y) = (0, 0).
If β = 0, (18.17) is the LCCHS
˙x =
 0
1
−1
0

x,
whose eigenvalues ±i. When β = 0, the equilibrium point (0, 0) is stable but not
asymptotically stable, by Theorem 5.11 in Section 5.3.
Define r =

x2 + y2 ≥0.
If β ̸= 0, then an equilibrium point (x, y) must satisfy 0 = ˙x = y + βx

x2 + y2
and
0 = ˙y = −x+βy

x2 + y2
; hence, y = −βx

x2 + y2
= −βxr2 and x = βy

x2 + y2
= βyr2.
It follows that necessarily
r2 = x2 + y2 =

βr22 +

−βr22 = 2β2r4;
hence,
0 = r2(1 −2βr2).
(18.18)
So, there is an equilibrium point with r = 0, that is, at (x, y) = (0, 0), no matter what is the
sign of β.
In addition, (18.8) has another solution r =
1
√
2β , if β > 0. In this case, x = βy · r2 = βy ·
1
2β = y
2 and y = −βx·r2 = −x
2; hence 2x = y = −1
2 x, hence 3
2 x = 0, hence x = 0, and hence
y = 2x = 0, which gives the contradiction 0 <
1
2β = r2 = x2 + y2 = 0. So, (x, y) = (0, 0) is
the only equilibrium point, even if β > 0.
Define
V(t) ≜

r(t)
2 =

x(t)
2 +

y(t)
2.
Fix a value of β ̸= 0. We calculate
˙V(t) = 2x(t)˙x(t) + 2y(t)˙y(t) = 2x(t)


y(t) + βx(t)V(t)

+ 2y(t)

−
x(t) + βy(t)V(t)

= 2βV(t)
!
x(t)
2 +

y(t)
"
= 2β

V(t)
2.

Nonlinear Ordinary Differential Equations
1319
(a)
(b)
2
y
y
1
1
2 x
x
–2
–1
–1
–2
2
1
1
2
–2
–1
–1
–2
FIGURE 18.9
Phase planes for nonlinear systems. (a) β = −0.4 and (b) β = 0.4.
This separable differential equation has solution
V(t) =
1
V0 −2βt,
where V0 ≜V(0).
If β < 0, then ˙V(t) = 2β

V(t)
2 < 0 for all (x(t), y(t)) ̸= (0, 0). It follows that (0, 0) is
stable. In addition,

x(t)
2 +

y(t)
2 = V(t) →0 as t →∞; hence, (0, 0) is an attractor.
It follows that (0, 0) is asymptotically stable if β < 0.
If β > 0, then

x(t)
2 +

y(t)
2 = V(t) =
1
V0 −2βt →∞, as t →V0
2β
+
.
It follows that (0, 0) is unstable if β > 0. ⃝
Figure 18.9 shows the phase plane for (a) β = −0.4 and (b) β = 0.4.
Learn More About It
While the idea of stabilizing an inverted pendulum is an elementary topic, it involves
some sophisticated mathematics of control theory. See Sections 3.3 and 9.1 of Mathe-
matics for Dynamic Modeling, by Edward Beltrami, Academic Press, Inc., c⃝1987.
An equilibrium point that is both unstable and an attractor is given in Example 5.7.6
on pages 130–131 of Ordinary Differential Equations, 2nd ed., by Garrett Birkhoff and
Gian-Carlo Rota, Blaisdell Publishing Co., c⃝1969.
18.2.3 Problems
In problems 1–10, for the given system, (a) find all of the equilibrium points and (b) study
their stability using linearization, if possible.
1.
#
˙x1 =
x1
+
4x2
˙x2 =
−x1
−
3x2
$

1320
Advanced Engineering Mathematics
2.
⎧
⎪⎨
⎪⎩
˙x1 = x1 + 3x2
˙x2 = −x1 −x2
˙x3 = −x3 + 1
⎫
⎪⎬
⎪⎭
3.
#˙x1 =
−x1

1 −x2
1 −x2
2

˙x2 =
−x2

1 −x2
1 −x2
2

$
4.
#
˙x =
−5x + 6 + y2
˙y =
x −y
$
5.
#
˙x =
8x −y2
˙y =
−y + x2
$
6.
#
˙x =
−x −x2 + xy
˙y =
−y + xy −y2
$
[Hint: Factor each of the equilibrium equations.]
7.
#
˙x =
2 sin x + y
˙y =
sin x −3y
$
[Hint: Solve each of the equilibrium equations for y in terms of x and then solve
an equation of the form f(x) = y = g(x) for x.]
8.
⎧
⎨
⎩
˙x =
1
1+2y −0.2
˙y = 0.3x −0.6
⎫
⎬
⎭
[This is a model for a biochemical reaction.]
9.
#˙x = −0.1x + 0.02xy
˙y = 0.2y −0.4xy −0.05y2
$
10.
#˙x = x + y2
˙y = x + y
$
11. The motion of a pendulum in a resisting medium can be modeled by ¨θ + 2b ˙θ +
ω2θ = 0.
(a) Write the second-order ODE as a system of two first-order ODEs.
(b) Using part (a), find all of the equilibrium points and study their stability using
linearization.
12. Suppose f(x) is continuously differentiable at x = c for some constant vector c. If
(⋆) ˙x = f(x) has a solution x(t) satisfying limt→∞x(t) = c, then explain why c must
be an equilibrium point for (⋆).
13. Suppose β is a constant. Study the stability of the origin (x1, x2, x3) = (0, 0, 0) the
system
⎡
⎢⎢⎣
˙x1
˙x2
˙x3
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
x2 −x3 + βx1

x2
1 + x2
2 + x2
3

x3 −x1 + βx2

x2
1 + x2
2 + x2
3

x1 −x2 + βx3

x2
1 + x2
2 + x2
3

⎤
⎥⎥⎦.

Nonlinear Ordinary Differential Equations
1321
18.3 Variation of Parameters Using Linearization
Recall from (5.44) in Section 5.4 that the solution of
˙y = A(t)y(t) + q(t)
(18.19)
is given by
y(t) = e(t−t0)Ay(t0) +
t
t0
e(t−s)Aq(s) ds.
(18.20)
Now consider an almost linear system
˙y = Ay(t) + g(y, t),
(18.21)
where ||g(y, t)|| is o(|| y ||) as || y || →0.
Equation (18.21) is shorthand for
˙y(t) = Ay(t) + g(y(t), t).
(18.22)
If we define q(t) ≜g(y(t), t), we see that the solution of (18.21) is given implicitly by
y(t) = e(t−t0)Ay(t0) +
t
t0
e(t−s)Ag(y(s), s) ds.
(18.23)
From (18.23), we will see why Theorem 18.4 in Section 18.2, that is, “Stability from
linearization,” makes sense in a non-rigorous way.
In the first case, suppose that all eigenvalues of the constant matrix A have negative
real part. Then the matrix exponential etA →O, the zero matrix, as t →∞. So, we have no
difficulty understanding why e(t−t0)Ay(t0), the first term on the right-hand side of (18.23),
goes to 0 as t →∞, no matter what initial value, y(t0), is chosen.
As to why the second term on the right-hand side of (18.23) also goes to 0 as t →∞, the
reasoning is a little more complicated because the only thing we know about g(y(s), s)
is that the dependence of g(y, t) on y is o(|| y ||) as || y || →0. We refer the reader to
“Learn More About It” for references that use more advanced mathematics to clear up
the complications.
18.3.1 Saddle Point Theorem
Suppose ¯x is an equilibrium point for an autonomous system of ODEs
˙x = f(x)
(18.24)

1322
Advanced Engineering Mathematics
in Rn and the constant linearization matrix at ¯x is given in (18.12) in Section 18.2, that is,
A ≜
 ∂f
∂x(¯x)

.
Suppose A has k eigenvalues whose real parts are positive and n −k eigenvalues whose
real parts are negative.
The integer k is to satisfy 0 ≤k ≤n. If k = 0, then ¯x is asymptotically stable, by Theorem
18.4(a) in Section 18.2; if k ≥1, then ¯x is unstable, by Theorem 18.4(b) in Section 18.2.
Note that we are assuming here that no eigenvalue of A is zero or is imaginary.
So far, we have just been discussing stability of ¯x, but we will see that much more is
known about nonlinear systems, even just using the linearization.
In the special case of planar linear system ˙x = Ax, that is, n = 2, ¯x = 0 in R2 is
• A stable node if k = 0
• A saddle point if k = 1
• An unstable node if k = 2
(18.25)
Recall that for a linear system ˙x = Ax, the linearization about the equilibrium point is also
˙x = Ax.
Example 18.12
Consider the planar system
	˙x
˙y

=
	x −y2
−y

.
(18.26)
Find all of the solutions and show the phase plane.
Method: Generally we cannot find the solutions of a nonlinear system of ODEs, but we
can for this example and for some similar examples because one of the ODEs in the
system involves only one of the unknowns.
The second ODE in (18.26) is ˙y = −y, whose solutions are y(t) = y0e−t, where y0 = y(0).
Substitute into the first ODE in (18.26) to get
˙x = x −y2
0e−2t.
This linear ODE can be solved easily by the method of undetermined coefficients or the
method of integrating factor:
x(t) =

x0 −1
3 y2
0

et + 1
3 y2
0e−2t.
So, the solutions are
⎧
⎨
⎩
x(t) =
!
x0 −1
3 y2
0
"
et + 1
3 y2
0e−2t
y(t) = y0e−t
⎫
⎬
⎭.
The solutions behave like
x(t) =

x0 −1
3 y2
0

et,
y(t) = 0, as t →∞,
that is, solutions are asymptotic to the line y = 0.

Nonlinear Ordinary Differential Equations
1323
–1
1
–1
–2
2
y
1
2 x
–2
–1
1
–1
–2
2
y
1
2 x
–2
(a)
(b)
FIGURE 18.10
Example 18.12. (a) ˙x = x −y2, ˙y = −y (b) ˙x = x, ˙y = −y.
The solutions behave like
x(t) = 1
3 y2
0e−2t,
y(t) = y0e−t, as t →−∞,
that is, solutions are asymptotic to the parabola x = 1
3y2.
Figure 18.10a shows the phase plane for system (18.26): Solutions are drawn using
Mathematica. The parametrized curves
x = x(t),
y(t) ≡0
and
x(t) = 1
3y(t)2,
y = y(t)
are also solutions and are drawn in dashed curves. The solutions lying on the parabola
x = 1
3y2 are obtained by taking initial conditions x0 = 1
3 y2
0. ⃝
For Example 18.12, the corresponding linearization x = Ax is ˙x = x, ˙y = −y, and its
phase plane is drawn in Figure 18.10b. The phase plane for the nonlinear system looks
like a distorted, “sheared to the right” version of the phase plane for the linearization.
We will see that this relationship between the nonlinear systems and its linearization
can be made precise and turned into a theorem, whether in R2 or more generally in Rn,
for n ≥2.
It turns out that the dashed solution curves for the nonlinear system are directly
related to the dashed solution curves of the linearization. The linear system ˙x = x, ˙y = −y
has a saddle point at the origin, and the lines y = 0 and x = 0 have straight line solu-
tions corresponding to the eigenvectors
1
0

and
0
1

, respectively. The dashed curves in
Figure 18.10a, that is, the line y = 0 and the parabola x = 1
3 y2, are called the “unstable man-
ifold” and “stable manifold” and are images of the lines, that is, vector subspaces, y = 0
and x = 0, respectively.

1324
Advanced Engineering Mathematics
Consider the case 1 < k < n, so some of the eigenvalues of the linearization, λ1, . . . , λk,
have positive real part and some, λk+1, . . . , λn, have negative real part. We will assume∗
that A has corresponding linearly independent sets of eigenvectors {e(1), . . . , e(k)} and
{e(k+1), . . . , e(n)}.
There is a projection P+ onto V+ ≜Span{e(1), . . . , e(k)}, that is, an n × n matrix satisfying

P+
2 = P+
and
Ap+ is in V+ for all p+ in V+.
The latter property is referred to as “V+ is invariant under A.” The + in the notation refers
to the assumption that the real parts of λ1, . . . , λk are positive.
Correspondingly, the matrix P−≜I −P+ is a projection onto V−≜Span{e(k+1), . . . , e(n)}
and V−is invariant under A.
Definition 18.6
A set S is invariant for (18.24) if for every initial value x0 in S, the solution x(t; x0) is in S
for all t.
Recall that the omission of t0 from the solution notation x(t; x0) implies that the initial
time is t0 = 0.
Definition 18.7
A set S is a twice continuously differentiable surface given by a map  at a point ¯x
tangent to a vector subspace V of Rn if there is a positive constant ε and a continuously
differentiable map  from V to Rn such that
• S = {(¯x + p) : p is in V and ||p|| < ε}.
• (¯x + p) = ¯x + p + o(||p||) as ||p|| →0.
We refer to S as a k-dimensional surface if the dimension of the vector subspace V is k.
A 1D surface is a parametrized curve, and a 2D surface is a parametrized surface.
Example 18.13
Explain why the dashed curves of Example 18.12 are twice continuously differentiable
1D surfaces.
Method: The dashed line y = 0 is the set
U =
 0
0

+ c1
1
0

: −∞< c1 < ∞

,
∗If instead, there is a deficient eigenvalue, then we would need to use a generalized eigenvector(s), for example,
a w with (A −λI) w = v, assuming (A −λI) v = 0.

Nonlinear Ordinary Differential Equations
1325
that is, uses the map
+(0 + p+) ≜0 + p+, for p+ in Span
1
0

.
The parabola x = 1
3 y2 is the set
S =
 0
0

+ c1
0
1

+ 1
3 c2
1
1
0

: −∞< c1 < ∞

,
that is, uses the map
−(0 + p−) ≜0 + p−+ 1
3 ||p−||2
1
0

, for p−in Span
0
1

. ⃝
Definition 18.8
A set S is a stable manifold for (18.24) if there are positive constants ε, r−, K−and map −
such that
• S is invariant for (18.24).
• S is a twice continuously differentiable surface givenbymap −at ¯x tangent to V−.
• ||x(t; (¯x + p−)|| ≤K−e−r−t for all p−in V−with||p−|| < ε, for all t ≥0.
Note that K−e−r−t →0 as t →∞.
Definition 18.9
A set U is an unstable manifold for (18.24) at ¯x if there are positive constants ε, r+, K+
such that
• U is invariant for (18.24).
• U is a twice continuously differentiable surface given by map + at ¯x tangent to V+.
• ||x(t; (¯x + p+)|| ≤K+er+t for all p+ in V+ with ||p+|| < ε, for all t ≤0.
Note that K+er+t →0 as t →−∞.
Theorem 18.5
(Saddle point theorem) Suppose f(x) is twice continuously differentiable for x in some
open ball {x:||x −¯x|| < ε} and the linearization matrix A at ¯x has k eigenvalues whose real
part is positive and n −k eigenvalues whose real part is negative. Then the autonomous
system (18.24) has a k-dimensional unstable manifold and an (n −k)-dimensional stable
manifold at ¯x.

1326
Advanced Engineering Mathematics
The “Learn More About It” section will refer you to where you can find a formal proof
of this fundamental theorem of nonlinear ODEs.
18.3.2 Periodic Solutions
Suppose that in
˙y = A(t)y + q(t),
(18.27)
the matrix A(t) and the vector q(t) are both periodic with period T. Note that if A(t) is
constant, then it is periodic with any period.
Theorem 5.26 in Section 5.8 (noncritical systems) established that if the corresponding
linear homogeneous system of ODEs
˙y = A(t)y
(18.28)
has no T-periodic solution, then the nonhomogeneous system (18.27) is guaranteed to have
a T-periodic solution. In fact, that periodic solution is given by
y(t) = X(t)
⎛
⎝y0 +
t
0
(X(s))−1 q(s)ds
⎞
⎠,
where X(t) is the principal fundamental matrix of (18.28), that is,
˙X(t) = A(t)X(t),
X(0) = I, and
y0 = (I −X(T))−1 X(T)
T
0
(X(s))−1 q(s)ds.
In particular, if A(t) ≡A is constant and has no eigenvalue that is an integer multiple
of i2π/T and q(t) is periodic with period T, then the nonhomogeneous system (18.27) is
guaranteed to have a T-periodic solution given by
y(t) =
!
e−TA −I
"−1 T
0
e(t−s)Aq(s)ds +
t
0
e(t−s)Aq(s)ds.
(18.29)
Using more advanced methods of analysis, specifically fixed point theory, one can
establish the existence of a periodic solution of a nonlinear system of ODEs.
Theorem 18.6
(Weak perturbation of a noncritical system) Suppose g(t, x) satisfies
• g(t, 0) = 0,
• g(t + T, x) ≡g(t, x), and
• g(t, x) is continuously differentiable in x at x = 0,

Nonlinear Ordinary Differential Equations
1327
and the linear homogeneous system of ODEs
˙y = A(t)y
(18.30)
has no T-periodic solution. Then the nonhomogeneous system
˙x = A(t)x(t) + g(t, x)
(18.31)
is guaranteed to have a T-periodic solution. In fact, that periodic solution satisfies
x(t) = X(t)
⎛
⎝x0 +
t
0
(X(s))−1 g

s, x(s)

ds
⎞
⎠
for some appropriate initial value x0, where X(t) is the principal fundamental matrix of
the noncritical linear system (18.30).
Example 18.14
Study (⋆) ˙x =
−1
3
1
1

x +

(cos 2t)x2
1
0

for the existence of a π-periodic solution.
Method: Using the method of Section 5.2, we construct the principal fundamental matrix
for the corresponding linear homogeneous system of ODEs (⋆⋆) ˙x =
−1
3
1
1

x: Using
eigenvalues and eigenvectors, we find the general solution of (⋆⋆):
x = c1e2t
1
1

+ c2e−2t
−3
1

,
which gives a fundamental matrix
Z(t) =
e2t
−3e−2t
e2t
e−2t

and thus principal fundamental matrix
X(t) = Z(t)

Z(0)
−1 =
e2t
−3e−2t
e2t
e−2t
 1
4
 1
3
−1
1

= 1
4
e2t + 3e−2t
3e2t −3e−2t
e2t −e−2t
3e2t + e−2t

.
Because (⋆⋆) has no π-periodic solution, Theorem 18.6 guarantees that the original,
nonlinear system (⋆) has a π-periodic solution x(t) that satisfies
x(t) = 1
4
e2t + 3e−2t
3e2t −3e−2t
e2t −e−2t
3e2t + e−2t

·
·
⎛
⎝x0 +
t
0
1
4
e2s + 3e−2s
3e2s −3e−2s
e2s −e−2s
3e2s + e−2s
−1 
(cos 2s)x2
1(s)
0

ds
⎞
⎠,
that is,
x1(t)
x2(t)

= 1
4
e2t + 3e−2t
3e2t −3e−2t
e2t −e−2t
3e2t + e−2t

·
·
⎛
⎝x0 +
t
0
1
4
3e2s + e−2s
−3e2s + 3e−2s
−e2s + e−2s
e2s + 3e−2s
 
(cos 2s)x2
1(s)
0

ds
⎞
⎠,

1328
Advanced Engineering Mathematics
that is,
x1(t)
x2(t)

= 1
4
e2t + 3e−2t
3e2t −3e−2t
e2t −e−2t
3e2t + e−2t
 ⎛
⎝x0 + 1
4
t
0
	
3e2s + e−2s(cos 2s)x2
1(s)
−e2s + e−2s(cos 2s)x2
1(s)

ds
⎞
⎠.
(18.32)
This is a system of nonlinear Volterra integral equations that can be stated abstractly as
x = N(x). The most straight forward way to find the solution function x(t) is to take a
first guess for the solution as x1(t) ≡v for some constant vector v and then calculate the
sequence of vector-valued functions x2(t) ≜N(x1(t)), x3(t) ≜N(x2(t)), . . . . Hopefully,
the sequence of vector-valued functions will converge to a solution x∞(t) that would be
the π-periodic solution of the original system (⋆). ⃝
Learn More About It
Example 18.12 is essentially Example 9.28 in Dynamics and Bifurcations, by Jack K. Hale
and H¨useyin Koçak, Springer-Verlag, c⃝1991.
Formal statements and explanations of the saddle point theorem 18.5 are found in
Theorem II.6.1 of Ordinary Differential Equations, by Jack K. Hale, Robert E. Krieger
Publishing Company, c⃝1980. Also in that book, existence of invariant manifolds for
periodic systems is established in its Theorem IV.3.1, and its Theorem IV.2.1 has an
explanation of Theorem 18.6.
18.3.3 Problems
In problems 1 and 2, for the planar system, find all of the solutions and show the phase
plane.
1.
˙x
˙y

=

x
−x2 −y

2.
˙x
˙y

=
x −y4
−y

In problems 3 and 4, study the system for the existence of a 2π-periodic solution.
3. ˙x =
1
1
3
−1

x +

x3
1
sin t

4. ˙x =
 0
1
−1
−2

x +
cos t
x2
1

18.4 Liapunov Functions
Liapunov functions can be used to establish stability of an equilibrium point of a system
of nonlinear ODEs. The basic idea is that a Liapunov function can measure something that
“corresponds” to the distance from an equilibrium point. We may calculate that as time
increases that “distance” is nonincreasing or decreasing, in which case we can conclude

Nonlinear Ordinary Differential Equations
1329
that the equilibrium is stable or asymptotically stable. Or we may calculate that as time
increases, that “distance” is increasing, in which case we may be able to conclude that the
equilibrium is unstable.
For linear constant coefficients homogeneous systems, Theorem 5.11 in Section 5.3 may
give definitive conclusions about stability, at least assuming we can find all of the exact
eigenvalues. And we learned in Section 18.2 how to use linearization to get results about
stability of an equilibrium point of a nonlinear system of ODEs; however, in the neu-
trally stable case, linearization fails to give useful conclusions. Liapunov functions give us
another tool for analyzing nonlinear systems even if linearization is not conclusive.
We motivate the concepts with two linear examples.
Example 18.15
Let x = [y
v]T solve the system of differential equations
	
˙y
˙v

=
	
v
−k
m y

.
(18.33)
Physically, this is the undamped, linear problem of vibration of a mass m on a massless
spring whose spring constant is k, so it makes sense to define the total energy of the
system by
E = E(y, v) ≜1
2
!
ky2 + mv2"
,
that is, the sum of the potential energy in the spring and the kinetic energy of the mass
at the end of the spring. Discuss the behavior of the energy of solutions for t ≥0.
Method: Let E(t) ≜E

y(t), v(t)

where x(t) = [y(t)
v(t)]T solves (18.33). The chain
rule gives
˙E(t) = ky(t)˙y(t) + mv(t)˙v(t).
But x(t) solves the system of differential equations, so
˙E(t) = ky(t)v(t) + mv(t)

−k
m y(t)

≡0.
So, the energy is constant in time. ⃝
Physically, this makes perfect sense: Because there is no damping, we have conservation
of the total energy of the system.
In a sense, E = E(y, v) measures the square of the distance between a point (y, v) in the
phase plane and the equilibrium point at (0, 0). The conclusion of Example 18.15 says that
the solution of the system of differential equations (18.33) stays on the same level set of E
as where it begins at t = 0. This is illustrated in Figure 18.11a for the specific example of
k/m = 2.
Example 18.15 also illustrates the subject of Hamiltonian mechanics because ODE system
(18.33) can be rewritten as
⎧
⎪⎪⎨
⎪⎪⎩
˙q = ∂H
∂p
˙p = −∂H
∂q
⎫
⎪⎪⎬
⎪⎪⎭
,

1330
Advanced Engineering Mathematics
(a)
(b)
–4
–2
2
4
y
4
v
2
–2
–4
–4
–2
2
4 y
4
v
2
–2
–4
FIGURE 18.11
(a) Example 18.15 and (b) Example 18.16.
where the momentum is p = mv, the position is q = y, and the Hamiltonian function is
H(p, q) ≜
1
2 kq2 +
1
2m p2. This will be generalized in Problem 18.4.5.20, with Problem
18.4.5.21 as an example.
The first thing to know about Hamiltonian systems is that the Hamiltonian is an invari-
ant, that is, ˙H = 0. As such, they model physical systems without energy dissipation. If
we can show that H is “positive definite,” as defined later in Definition 18.10, then the
Liapunov function theory as follows in Theorem 18.7(a) will establish stability.
Example 18.16
Repeat the instructions of Example 18.15 for the system of differential equations
	˙y
˙v

=
⎡
⎣
v
−k
m y −b
m v
⎤
⎦.
(18.34)
Physically, this is the linear problem of vibration of a mass m on a massless spring whose
spring constant is k and is experiencing a linear damping force whose coefficient is b. As
in Example 18.15 define the total energy of the system by
E = E(y, v) ≜1
2
!
ky2 + mv2"
,
that is, the sum of the potential energy in the spring and the kinetic energy of the mass at
the end of the spring. Discuss the behavior of the energy of solutions for t ≥0.
Method: Let E(t) ≜E

y(t), v(t)

where x(t) = [y(t)
v(t)]T solves (18.34). The chain rule
gives
˙E(t) = ky(t)˙y(t) + mv(t)˙v(t) = ky(t)v(t) + mv(t)

−k
m y(t) −b
m v(t)

= −bv(t)2 ≤0.
So, the energy is nonincreasing in time. ⃝

Nonlinear Ordinary Differential Equations
1331
Physically, this makes perfect sense: Because there is damping, we do not expect the
total energy of the system to be conserved.
The conclusion of Example 18.16 says that the solution of the system of differential equa-
tions (18.34) stays on the same level set of E or moves to a lower energy level as time goes
by. This is illustrated in Figure 18.11b for the specific example of k/m = 2 and b/m = 0.3.
18.4.1 Deﬁnite Functions
Recall from Definition 2.19 in Section 2.6 that a real, square matrix B is positive definite if
xTBx > 0 for all x ̸= 0. The concept of definiteness can be generalized to great use in the
study of stability of differential equations.
Throughout this section, we will assume that V = V(x) is a real-valued function of x in
Rn that satisfies
(1)
V(0) = 0
and
(2)
∇V = ∇V(x) =
 ∂V
∂x1
. . .
∂V
∂xn
T
defined and continuous on some open set S.
(18.35)
Recall that an open ball about ¯x is defined by B ≜BR(¯x) ≜{x : ||x −¯x|| < R}, where
|| x || =

x2
1 + · · · + x2
n
1/2 is the usual Euclidean norm. A set S is open if for every ¯x in
S, there is some R > 0, possibly dependent on ¯x, such that BR(¯x) is contained in S.
Define also the punctured ball )B =)BR(¯x) ≜{x : 0 < ||x −¯x|| < R}.
Definition 18.10
Suppose V(x) satisfies basic requirements (1) and (2). We say V(x) is
(a) Positive definite if V(x) > 0 for all x in the punctured ball )BR(0)
(b) Positive semi-definite if V(x) ≥0 for all x in the ball BR(0)
(c) Negative semi-definite if V(x) ≤0 for all x in BR(0)
(d) Negative definite V(x) < 0 for all x in )BR(0)
For future reference and to connect the general concept to what we have already seen,
note that if B is a real n × n matrix then
V(x) ≜xTBx
(18.36)
is an example of a function that satisfies our basic requirements (1) and (2). A function
V(x) given by (18.36) is called a quadratic form.
Usually the easiest kind of positive definite function to work with is V(x) ≜xTBx, where
B is a positive definite function. And the simplest such choice is V(x) ≜xTx = x2
1 + · · · + x2
n,
that is, the square of the distance from the equilibrium point 0.

1332
Advanced Engineering Mathematics
Example 18.17
(a) E(y, v) ≜1
2

ky2 + mv2
is positive definite on any punctured ball )BR(0).
(b) ˙E(y, v) ≜−bv2 is negative semi-definite on any ball BR(0).
(c) V(x1, x2) ≜x2
1 + (x1 −x2)2 is positive definite on any punctured ball )BR(0).
Method: Only (c) needs explanation as to why it is positive definite rather than merely
positive semi-definite: If V(x1, x2) is not positive at some point (x1, x2), then 0 = x2
1 +
(x1 −x2)2 would imply that both 0 = x1 and 0 = x1−x2; hence, x2 = x1 = 0 also. So, V(x) >
0 at every x ̸= 0, that is, V is positive definite on any punctured ball )BR(0). ⃝
If V(x) is positive definite on some ball punctured )BR(0), then the level sets may not be
as nice as the ellipses we saw in Figure 18.11, but we can still use the values of V(x) to get
enough information about the closeness of x to 0 so as to be useful for studying stability of
an equilibrium point of a system of differential equations.
Suppose we have an autonomous system of differential equations (18.24) in Section
18.3, that is,
˙x = f(x),
and
(18.24) in Section 18.3 has existence and uniqueness of solutions of IVPs and
equilibrium point 0
(18.37)
and suppose V = V(x) satisfies the basic requirements (1) and (2). By the chain rule,
d
dt

V

x(t)
 
= ∂V
∂x1

x(t)

· dx1
dt (t) + · · · + ∂V
∂xn

x(t)

· dxn
dt (t) = ∇V

x(t)

• ˙x(t).
(18.38)
This suggests defining a new function on Rn by
˙Vf(x) ≜f

x

• ∇V(x).
(18.39)
So, (18.38) states that
d
dt

V

x(t)
 
= ˙Vf

x(t)

.
(18.40)
Example 18.18
Find ˙Vf(x) if V(x, y) ≜2x2 + y2 and x(t) ≜[x(t)
y(t)]T solves the system of differential
equations
	˙x
˙y

=
	 −x + y2
−2y −2xy

.
(18.41)
Method: ˙Vf(x) =
	 −x + y2
−2y −2xy

•
	4x
2y

= (−x + y2) · 4x + (−2y −2xy) · 2y = −4x2 −4y2. ⃝

Nonlinear Ordinary Differential Equations
1333
Theorem 18.7
(Liapunov theory) Suppose V(x) is positive definite on a punctured ball )BR(0).
(a) If, in addition, ˙Vf is negative semi-definite on the corresponding ball BR(0), then
0 is stable for (18.24) in Section 18.3.
(b) If, in addition, ˙Vf is negative definite on )BR(0), then 0 is asymptotically stable for
(18.24) in Section 18.3.
Intuitively, if a function V satisfies the properties that it (a) has minimum value of
V(¯x) = 0 at an equilibrium point ¯x, (b) takes on only positive values near the equilibrium
point, and (c) has V

¯x(t)

decreasing for all t > 0, then we expect that solutions tend to ¯x
as t →∞.
The complete mathematical explanation for Theorem 18.7 requires some care because
V

x(t)

does not directly measure distance from the origin. We know V(x) is continuous
on BR(0) because V(x) satisfies (2). What we can use is the fact that V(x) being positive
definite on )BR(0) implies that for any ε with 0 < ε < R,
mε ≜min{V(x) : ε ≤|| x || ≤R}
is positive, because Theorem 13.9 in Section 13.2 implies that V(x) attains its global
minimum on the annulus {x : ε ≤|| x || ≤R}. Also, for any δ with 0 < δ < ε,
Mδ ≜max{V(x) : 0 ≤|| x || ≤δ} < ∞,
again by the continuity of V(x).
Example 18.19
Study the stability of 0 for the system of ODEs (18.41).
Method: The results of Example 18.18 say that V(x, y) ≜2x2 + y2 has ˙Vf(x) = −4x2 −4y2.
It is obvious that V is positive definite on any punctured ball )BR(0) and ˙Vf is negative
definite on )BR(0), so 0 is asymptotically stable for the system of ODEs (18.41). ⃝
Example 18.20
Study the stability of 0 for the system of ODEs (18.34).
Method: Using the physically motivated energy function E = E(y, v) ≜1
2

ky2 + mv2
has
˙Ef = −bv2. Assuming k, m, b > 0 on physical grounds, it is obvious that E is positive
definite on any punctured ball )BR(0) and ˙Ef is negative semi-definite on )BR(0), so 0 is
stable for the system of ODEs (18.34). ⃝
The results of Example 18.20 are unsatisfying because we know that (y, v) = (0, 0) is
asymptotically stable for the linear, damped harmonic oscillator system (18.34), because
the LCCHS has both eigenvalues having negative real part. (See Theorem 5.11 in Section
5.3). So, Theorem 18.7 gives us a correct but fairly useless conclusion about system (18.34).

1334
Advanced Engineering Mathematics
We will see in Section 18.5 that there are better results for the linear, damped harmonic
oscillator system (18.34) using the “LaSalle invariance principle.”
Definition 18.11
Suppose V(x) satisfies basic requirement (2) of (18.35), that is, differentiability, on some
open set G and V(x) has a global minimum value on G. Suppose an autonomous system
˙x = f(x) has existence and uniqueness of solutions for initial values in G. If ˙Vf is negative
semi-definite on G, then we say V(x) is a Liapunov function on G for the system of ODEs
˙x = f(x).
Example 18.21
Use the function V(x, y) = 9
2x2+ 1
2 y2 to get better conclusions about stability of the origin
than we can get using linearization for the system
	˙x
˙y

=
	
y −2x(x2 + y2)
−9x −2y(x2 + y2)

.
Method: V(x, y) = 9
2x2+ 1
2 y2 is positive definite on any punctured ball)BR(0). We calculate
˙Vf(x, y) = 9x˙x + y˙y = 9x

y −2x(x2 + y2)

+ y

−9x −2y(x2 + y2)

= −2

9x2 + y2
x2 + y2
.
Because ˙Vf(x, y) is negative definite on )BR(0), the origin is asymptotically stable.
On the other hand, the linearization about the equilibrium point (0, 0) is ˙x = Ax, where
A =
⎡
⎣
∂
∂x

y −2x(x2 + y2)

∂
∂y

y −2x(x2 + y2)

∂
∂x

−9x −2y(x2 + y2)

∂
∂y

−9x −2y(x2 + y2)

⎤
⎦

(x,y)=(0,0)
=
	−2(x2 + y2) −4x2
1 −4xy
−9 −4xy
−2(x2 + y2) −4y2

 
(x,y)=(0,0)
=
	 0
1
−9
0

.
Because the eigenvalues of A are ±i3, the linearization is neutrally stable, and thus,
Theorem 18.4 in Section 18.2 does not give a conclusion about the stability of the original,
nonlinear system of ODEs. ⃝
18.4.2 Liapunov Functions and Quadratic Forms
Suppose V(x) is a quadratic form, that is, V(x) = xTBx, and the system of ODEs has
the form
˙x = f(x) ≜Ax + g(x),
(18.42)
where A is a real, constant matrix. In Problem 6.7.6.33, you explained why
∇V(x) = ∇xTBx =

B + BT
x.
(18.43)

Nonlinear Ordinary Differential Equations
1335
We calculate
˙Vf(x)≜f

x

•∇V(x)=

Ax + g(x)

•

B + BT
x = xTAT
B + BT
x +

g(x)
T
B + BT
x.
(18.44)
It is common in this subject to rewrite
xTAT
B + BT
x= xTATBx +

Ax)T
BTx

= xTATBx +

BTx)T
Ax

= xTATBx + xTBAx,
so we can rewrite (18.44) as
˙Vf(x)= xT
ATB + BA

x +

g(x)
T
B + BT
x = xTCx +

g(x)
T
B + BT
x,
(18.45)
where
C ≜ATB + BA.
(18.46)
Now, suppose that g(x) is o(|| x ||) as || x || →0, that is, that
lim
|| x ||→0
||g(x)||
|| x ||
= 0.
(18.47)
If, in addition, C is negative definite, then for || x || sufficiently small, ||

g(x)
T
B+BT
x|| →
0 faster than does xTCx, as || x || →0. It follows that if the ball BR(0) is sufficiently small,
that is, if R is sufficiently small, then ˙Vf(x) will be negative definite on )BR(0). This explains
our next result.
Theorem 18.8
Suppose g(x) is o(|| x ||) as || x || →0 and there exists a positive definite constant matrix B
for which ATB + BA is negative definite. Then 0 is asymptotically stable for the system of
ODEs (18.42).
By the way, the matrix equation (18.46), where A and C are given and B is to be solved
for, is called a Liapunov equation.
There is a “converse” of Theorem 18.8:
Theorem 18.9
Suppose 0 is asymptotically stable for the LCCHS ˙x = Ax. Then there exists a real, constant
matrix B for which the quadratic form V(x) ≜xTBx satisfies the hypotheses of Theorem
18.7(a), that is, B is positive definite and ˙Vf(x) is negative definite.

1336
Advanced Engineering Mathematics
Why? Let
B ≜
∞

0
etAT etA dt.
(18.48)
In Problem 18.4.5.14, you will check that for B given in (18.48), V(x) ≜xTBx satisfies the
hypotheses of Theorem 18.7(a). 2
The matrix B is reminiscent of the matrix M defined in (5.74) in Section 5.6 that we used
in studying a control theory problem. Actually, in the history of systems of differential
equations, the solution B of the Liapunov equation (18.46) preceded the matrix M used in
control theory.
18.4.3 Instability
Definition 18.12
Suppose V(x) satisfies our basic requirements (1), that is, V(0) = 0, and (2), that is, contin-
uous differentiability on some open set S. We say a function V(x) is indefinite on S if, in
addition, for every ball BR (contained in S), there are x± such that V(x+) > 0 > V(x−).
Theorem 18.10
(a) Suppose V(x) is indefinite on S and either ˙Vf(x) is positive definite or is negative
definite on some punctured ball )BR(0) ⊆S . Then 0 is unstable for the system of
ODEs (18.42).
(b) Suppose both V(x) and ˙Vf(x) are positive definite on some punctured ball. Then
0 is unstable for the system of ODEs (18.42).
Example 18.22
The undamped nonlinear pendulum of length ℓcan be modeled∗by the system
	 ˙θ
˙v

=
⎡
⎣
1
ℓv
−g sin θ
⎤
⎦.
near the inverted position, that is, near the equilibrium point (θ, v) = (π, 0). Use a
Liapunov function to establish instability of that equilibrium point.
Method: On physical grounds, the energy in the system is E(θ, v) = mgℓ(1−cos θ)+ 1
2 mv2.
It is easy to calculate that ˙Ef(θ, v) ≡0, that is, there is conservation of energy in this
frictionless model. So, we cannot use this function E(θ, v) in Instability Theorem 18.10.
Because the equilibrium point is not 0 = (0, 0), it may help to make a change of
variables: Let ξ = θ −π, and note that sin θ = sin(ξ +π) = −sin ξ, so the system becomes
∗See Problem 7.2.5.21 for the calculation of the potential energy.

Nonlinear Ordinary Differential Equations
1337
	˙ξ
˙v

=
⎡
⎣
1
ℓv
g sin ξ
⎤
⎦,
and we will study the stability of the equilibrium point (ξ, v) = (0, 0).
Noting that cos θ = cos(ξ +π) = −cos ξ, the energy in the system is *V(ξ, v) ≜mgℓ(1+
cos ξ) + 1
2 mv2.
The fact that *V(0, 0) = 2mgℓ̸= 0, that is, (˜1) is not true, is not a major issue: Instead of
working with *V, we can subtract from it 2mgℓ.
But in order to use Instability Theorem 18.10(a) we would need a function V with
either ˙Vf being positive definite or being negative definite. We will see that the artful
addition of an indefinite term to E(θ, v) can also make ˙Vf(θ, v) be definite. Define
V(ξ, v) ≜*V(ξ, v) −2mgℓ+ mgℓv sin ξ = mgℓ(−1 + cos ξ) + 1
2 mv2 + mgℓv sin ξ.
Then, using ˙ξ = v/ℓand ˙v = g sin ξ,
˙Vf(ξ, v) = −mgℓsin ξ(˙ξ) + mv˙v + mgℓ˙v sin ξ + mgℓv cos ξ(˙ξ)
= −
mg(sin ξ)v +
mvg sin ξ + mg2ℓsin2 ξ + mgv2 cos ξ,
that is,
˙Vf(ξ, v) = mg

gℓsin2 ξ + v2 cos ξ

;
hence, ˙Vf(ξ, v) > 0 for all (ξ, v) in the punctured ball )B = {(ξ, v) : 0 <

ξ2 + v2 < π
2 }. So,
˙Vf(ξ, v) is positive definite on )B.
Second, why is V(ξ, v) indefinite with respect to (ξ, v) = (0, 0)? Taylor approximations
give −1 + cos ξ ≈−1
2 ξ2 and sin ξ ≈ξ, so
V(ξ, v) = mgℓ(−1 + cos ξ) + 1
2 mv2 + mgℓv sin ξ ≈−1
2 mgℓξ2 + 1
2 mv2 + mgℓvξ ≜W(ξ, v).
These approximations get better and better as (ξ, v) →(0, 0).
By completing the square, we have
W(ξ, v) = −1
2 mgℓξ2 + mgℓvξ + 1
2 mv2 = −1
2 mgℓ
!
ξ2 −2vξ
"
+ 1
2 mv2
= −1
2 mgℓ(ξ −v)2 + 1
2 m(gℓ+ 1)v2.
On the ray ξ = v, W(ξ, v) = 1
2 m(gℓ+ 1)v2, so we conclude that V takes on some positive
values no matter how small is the ball )Bδ(¯0), that is, no matter how small δ > 0 is chosen.
On the other hand, by completing the square another way, we have
W(ξ, v) = 1
2 mv2 + mgℓvξ −1
2 mgℓξ2 = 1
2 m
!
v2 + 2ℓvξ
"
−1
2 mgℓξ2
= 1
2 m(v + gℓξ)2 −1
2 mgℓ(gℓ+ 1)ξ2.
On the ray v = −gℓξ, W(ξ, v) = −1
2 mgℓ(gℓ+ 1)ξ2, so we conclude that V takes on some
negative values on )Bδ(¯0) no matter how small δ > 0 is chosen.
So, we conclude that V(ξ, v) is indefinite with respect to (ξ, v) = (0, 0); hence, the
inverted undamped nonlinear pendulum is unstable. ⃝

1338
Advanced Engineering Mathematics
Certainly the details of showing instability of the inverted undamped nonlinear pen-
dulum in Example 18.22 were more difficult than in Example 18.10 in Section 18.2. But it
is good to see a physical example of the application of instability theorem, for Liapunov
functions.
Note that to use instability theorem, it would have worked as well to let V(θ, v) ≜
E(θ, v) −2mgℓ−mgℓv sin θ.
18.4.4 Stability of Another Equilibrium Point
When studying the stability of an equilibrium point ¯x, we could make the change of vari-
ables y = x −¯x, that is, x = y + ¯x. Then the stability property(s) of ¯x for ˙x = f(x) is the same
as the stability property(s) of y = 0 for ˙y = f(y + ¯x).
Alternatively, we can define definiteness properties for functions on a ball or on an open
set around a point ¯x, and then the Liapunov theorems apply in this generalization.
We will always assume that
(˜1)
V(¯x) = 0
and
(˜2)
∇V = ∇V(x) =
 ∂V
∂x1
· · ·
∂V
∂xn
T
defined and continuous on some open set S.
Definition 18.13
Suppose V(x) satisfies basic requirements (˜1) and (˜2). We say V(x) is
(a) Positive definite with respect to ¯x if V(x) > 0 for all x in a punctured ball )BR(¯x)
(b) Positive semi-definite with respect to ¯x if V(x) ≥0 for all x in BR(¯x)
(c) Negative semi-definite with respect to ¯x if V(x) ≤0 for all x in BR(¯x)
(d) Negative definite with respect to ¯x if V(x) < 0 for all x in )BR(¯x)
(e) Indefinite with respect to ¯x if for every ball BR(¯x) (contained in S), there are x±
such that V(x+) > 0 > V(x−)
It is also possible to define these concepts using an open set S containing ¯x instead of a
punctured ball, for example, V(x) is positive definite with respect to ¯x if V(x) > 0 for all
x ̸= ¯x in S.
Here is one such result.
Theorem 18.11
Suppose V(x) is positive definite with respect to ¯x and ˙Vf(x) is negative definite with
respect to ¯x. Then ¯x is asymptotically stable for ˙x = f(x).

Nonlinear Ordinary Differential Equations
1339
18.4.5 Problems
1. Examine each of the functions to see if it is indefinite, or positive definite or semi-
definite, or negative definite or semi-definite, or none of the above, with respect
to the given point. Use the definitions to explain your conclusions
(a) V(x, y) = x2 + 3y2 with respect to (0, 0)
(b) V(x, y) = sin2 x + y2 with respect to (π, 0)
(c) V(x, y) = x3 + (y −1)6 with respect to (0, 1)
In problems 2–6, use V(x) = 1
2xTx to study the stability of the origin for the given system.
2.
˙x
˙y

=

−x −1
3 x3 −x cos y
−y −y3

.
3.
˙x
˙y

=
−y −x sin2 x
x −y sin2 x

.
4.
˙x
˙y

=

x −y2
y + xy

.
5.
⎡
⎢⎢⎣
˙x1
˙x2
˙x3
˙x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−x1 −2x2
2
−x2 + 2x1x2
−3x3 + x4
−x3 −2x4
⎤
⎥⎥⎦.
6.
˙x
˙y

=
 xp(x, y)
−yq(x, y)

,
where we assume that p(x, y) is negative definite and q(x, y) is positive definite on
some ball centered at the origin (x, y) = (0, 0).
In problems 7 and 8, use a function of the form V(x, y) = Ax2 + By4, where A, B are
constants, to study the stability of the origin for the given system.
7.
˙x
˙y

=

−x3 −3xy4
x2y −2y3 −y5

.
8.
˙x
˙y

=
−x3 −3xy4
2x2y −5y3

.
9. Use a function of the form V(x, y) = Ax2 + By2, where A, B are constants, to study
the stability of the origin for the system
˙x
˙y

=
 x3y2 −3x3
−5x4y −y

.
10. Use a function of the form V(x, y) = Ax2 + Bx4 + Cy2, where A, B, C are constants,
to study the stability of the origin for the system
˙x
˙y

=

−x3 + xy2
−y −3x2y −5x4y

.
11. Use the function V(x, y) = 9
2y2 + 1
2 v2 to get better conclusions about stability of the
origin than we can get using linearization for the system
	˙y
˙v

=
	 v + y(y2 + v2)
−9y + v(y2 + v2)

.
12. Use the function V(x, y) = x4 + y2 to help conclude that the system

1340
Advanced Engineering Mathematics
˙x
˙y

=
y −x3
−2x3

has 0 as a stable equilibrium point even though the linearization about 0 is
unstable.
13. Use the function V(x, y) = x2−y2 to help conclude that 0 is an unstable equilibrium
point for the system
˙x
˙y

=
−x + y3
x2 + y

.
14. Check that for B given in (18.48), V(x) ≜xTBx satisfies the hypotheses of Theorem
18.7(a).
In problems 15–17, use Theorem 18.9 to find a Liapunov function for
15. The system of Problem 5.3.6.11, that is,
˙x =
 0
1
−1
−2

x.
16. The system of Example 5.13 in Section 5.2, that is,
˙x =
 0
1
−6
−5

x.
17. ˙x =
−3
1
1
−3

x.
18. (Small project) Study the stability of the origin for the system (⋆)
˙x
˙v

=

v
−f(x)

,
using the function H(x, v) ≜1
2 v2 + F(x), where F(x) ≜
 x
0 f(ξ) dξ.
a. Give a condition(s) on the function f(x) that implies that the origin is stable
for (⋆).
b. Give a condition(s) on the function f(x) that implies that the origin is unstable
for (⋆).
Note that for a scalar problem ¨x + f(x) = 0, F(x) has a natural interpretation
as the potential energy and H(x, v) has a natural interpretation as the sum of the
kinetic and potential energies.
19. Assume α is an unspecified constant. Use the function V(x, y) = x2 + y2 to study
the stability of the origin for the system of ODEs
˙x
˙y

=
 3x + αx(x2 + y2)
−3y + αy(x2 + y2)

. [Hint: Consider separately the cases α < 0, α = 0,
and α > 0.]
20. For the “Hamiltonian system” in R2n
⎧
⎨
⎩
˙q = ∂H
∂p
˙p = −∂H
∂q
⎫
⎬
⎭,
suppose H(q, p) is positive definite on the punctured ball )Bδ(0, 0). Explain why
(p, q) = (0, 0) is stable by showing that H(p, q) is a Liapunov function.
21. Consider the two mass and three horizontal springs system of Example 5.5 in
Section 5.1. Rewrite it in the form of a Hamiltonian system, as in Problem 18.4.5.20,

Nonlinear Ordinary Differential Equations
1341
using momentum coordinates p1 = m1˙x1 and p2 = m2˙x2. Use the result of Problem
18.4.5.20 to discuss the stability of (q, p) = (0, 0) in R4.
22. Suppose V(0, 0) = 0, V(x, y) is positive definite on the punctured ball )Bδ(0, 0) for
some δ > 0, and ∇V(0, 0) = 0. For the gradient system of ODEs
⎧
⎨
⎩
˙x = −∂V
∂x
˙y = −∂V
∂y
⎫
⎬
⎭,
(a) Explain why (0, 0) is stable, and (b) discuss whether (0, 0) can fail to be
asymptotically stable.
18.5 Short Take: LaSalle Invariance Principle
Recall that our results for Example 18.20 in Section 18.4 were unsatisfying: We knew that
Liapunov theory did not give as good a conclusion as we could get from using eigenvalues
for that linear system of ODEs.
Suppose we have an autonomous system of differential equations (18.24) in Section 18.3,
that is, ˙x = f(x).
Definition 18.14
A set S is positively invariant for (18.24) in Section 18.3 if for every initial value x0 in S,
then the solution x(t; x0) is in S forward in time, that is, for all t ≥0.
Recall that the omission of t0 from the solution notation x(t; x0) implies that the initial
time is t0 = 0. Recall from Definition 2.32 in Section 2.10 that a set S is closed if every
sequence
+
xk
,∞
k = 1 in S that is convergent has its limit in S.
Definition 18.15
The closure of a set G, denoted by G, consists of all points in G and all limits of sequences
in G, that is,
G = {x : x = lim
k→∞gk for some sequence
+
gk
,∞
k=0 in G}.
Note that every g in G is the limit of the constant sequence
+
g
,∞
k = 0 in G.
Theorem 18.12
A set is closed if, and only if, it equals its closure.

1342
Advanced Engineering Mathematics
Definition 18.16
(a) A set F is a closed, positively invariant subset of S if
F ⊆S,
F is closed,
and
F is positively invariant.
(b) E is a maximal, closed, positively invariant subset of S if E is a closed and
positively invariant subset of S and
F ⊆E whenever F is a closed, positively invariant subset of S.
Definition 18.17
Given a point x and a closed set S, dist(x, S) ≜min{||s −x|| : s is in S}.
Theorem 18.13
(LaSalle invariance principle) Suppose V(x) is a Liapunov function for an autonomous
system (18.24) in Section 18.3, that is, on some open set G in Rn, ˙Vf is negative semi-definite
and V(x) has a global minimum value. Define
S ≜{ x in G : ˙Vf(x) = 0 },
and suppose E is a maximal, closed, positively invariant subset of S. If x(t) is a bounded
solution of ODE system (18.24) in Section 18.3 and x(t) remains in G for t ≥0, then
x(t) →E as t →∞.
(18.49)
One typical application of the LaSalle invariance principle is to show that E consists
of a finite number of equilibrium points of the autonomous ODE system. In this case,
(18.49) implies that every bounded solution of the autonomous system has to converge to
a specific equilibrium point, which may be a very desirable property in a physical system.
Example 18.23
Study the stability of 0 for the damped oscillator problem in Example 18.20 in
Section 18.4.
Method: Assume k, m, b > 0 on physical grounds and let G = R2.
The function
E = E(y, v) ≜1
2

ky2 + mv2
has ˙Ef = −bv2, so S = {(y, v) : v = 0}. The only initial condi-
tion x0 in S that has x(t; x0) remaining in S for all t > 0 is x0 = (y0, 0) = (0, 0), that is, the
only motion that has zero velocity, v(t), for all t ≥0 is the solution that starts at (0, 0).
So, the only closed, positively invariant subset of S is the set {(0, 0)}. So, the maximal,
closed, positively invariant set in S is E = {(0, 0)}.
So, every solution x(t) →0 as t →∞. This says that 0 is a global attractor. We already
knew that 0 is stable, so 0 is asymptotically stable. ⃝

Nonlinear Ordinary Differential Equations
1343
18.5.1 Stability of a Set
Definition 18.18
(a) A set S is stable if for all ε > 0, no matter how small, and all initial times t0, there exists
some δ > 0, possibly dependent on ε or t0, such that for all t ≥t0 and initial values x0
satisfying dist(x0, S) < δ,
dist(x(t; t0, x0), S) < ε.
(18.50)
(a) A set S is unstable if it is not stable.
Definition 18.19
A set S is an attractor if for all initial times t0, there exists some δ > 0, possibly dependent
on t0, such that for all initial values x0 satisfying dist(x0, S) < δ,
lim
t→∞dist(x(t; t0, x0), S) = 0.
(18.51)
Note that the points s(t) in S that minimize the distance to x(t; t0, x0), that is, “shadow”
x(t; t0, x0), may, in principle, not behave in any organized way.
Definition 18.20
A set S is asymptotically stable if it is both stable and an attractor.
18.5.2 Problems
In problems 1–3, use the LaSalle invariance principle and the given function V(x, y) to
study the stability of the origin for the given system.
1.
˙x
˙y

=

y
−x −y3

; V(x, y) = 1
2(x2 + y2)
2.
˙x
˙y

=
y −x3
−2x3

; V(x, y) = x4 + y2
3.
˙x
˙y

=
−y −x sin2 x
x −y sin2 x

; V(x, y) = 1
2(x2 + y2)
4. The second-order nonlinear ODE ¨y + g(y)˙y + y = 0 is equivalent to the nonlin-
ear system
˙y
˙v

=

v
−y −g(y)v

. Assume that g(y), an unspecified function on
R1, is positive definite. Use the LaSalle invariance principle and the function
V(y, v) = 1
2(y2 + v2) to study the stability of the equilibrium point (y, v) = (0, 0).
5. What does the LaSalle invariance principle say about the stability of the set con-
sisting of all of the equilibrium points for the system of Example 5.8 in Section 5.2?

1344
Advanced Engineering Mathematics
6. Study the stability of the equilibria (θ, v) = (a) (0, 0) and (b) (π, 0) for the damped
nonlinear pendulum
 ˙θ
˙v

=

v
−k sin θ −bv

,
where k is an unspecified positive constant.
18.6 Limit Cycles
Suppose a planar autonomous ODE system ˙x = f(x) has a periodic solution x(t) with period
T. It gives a simple, closed, parametrized curve C : x = x(t), 0 ≤t ≤T, in the phase plane.
As in Chapters 6 and 15, we may abuse the definitions by referring to C as both (1) a
parametrized curve, C : x = x(t), 0 ≤t ≤T, and (2) the set {x(t) : 0 ≤t ≤T}, that is, the set
consisting of all of the points x(t), 0 ≤t ≤T.
But, by Theorem 18.1 in Section 18.1, any time translate x(t−t0) will also give a periodic
solution of the same planar autonomous ODE system and will give the same set of points
as does C in the phase plane.
Definition 18.21
For a planar autonomous system ˙x = f(x) on a region D, a periodic solution x(t) is unique
in D, up to time translation, if the only periodic solutions of ˙x = f(x) that are in D are the
time translates of x(t).
Definition 18.22
(a) A planar autonomous ODE system ˙x = f(x) has a limit cycle C if it is not an equi-
librium point, it is the only periodic solution inside some open annulus, up to
time translation, and for some initial value in that open annulus the solution →
(the set C) as t →∞or as t →−∞.
(b) A limit cycle is stable if inside that open annulus all solutions →(the set C) as
t →∞.
(c) A limit cycle is unstable if inside that open annulus all solutions →(the set C) as
t →−∞.
Note: Usually, a solution x(t) does not get closer and closer to a specific point on a
stable limit cycle C but instead get closer and closer to the whole curve, as illustrated in
Figure 18.12. Many people call C the orbit of the periodic solution, and thus, what we call
stable they may call orbitally stable.
In Section 18.2, we learned how to use
r(t) ≜||x(t)|| =

x(t)Tx(t)

Nonlinear Ordinary Differential Equations
1345
y
2
–2
–2
2
4
x
–4
FIGURE 18.12
Example 18.24.
to study stability of the origin for some nonlinear systems of ODEs. Here we will use
similar calculations, along with the phase line for 1D ODEs discussed in Section 18.1, to
study limit cycles and their stability.
Example 18.24
Study limit cycles of the form r(t) ≡constant and their stability for the system
	˙x
˙y

=
	−5x + 2y + x

x2 + y2
−2x −5y + y

x2 + y2

.
(18.52)
Method: Because r2 = x2 + y2, we calculate
r˙r = x˙x + y˙y = x
!
−5x + 2y + x

x2 + y2"
+ y
!
−2x −5y + y

x2 + y2"
= −5r2 + r4,
so
˙r = −5r + r3.
(18.53)
The latter is an autonomous ODE in R1, so we can use the phase line to study its equi-
libria and their stability. Only r = || x || ≥0 is relevant to our problem (18.53). The phase
line is shown in Figure 18.13.
The equilibria of (18.52), r = 0 and r =
√
5, give periodic solutions, x(t)2 +y(t)2 ≡0 and
x(t)2 + y(t)2 ≡5, of (18.52). We see that x2 + y2 ≡5 gives an unstable limit cycle. Note
that the origin is an equilibrium point and is thus not a limit cycle.
Note also that (18.53) has no periodic solution other than the equilibrium at the ori-
gin and on the circle r =
√
5. Why not? Because if the initial value of a solution has
r=0
r= √5
FIGURE 18.13
Example 18.24.

1346
Advanced Engineering Mathematics
0 < r(0) ̸= 5, then the phase line in Figure 18.13 shows that at no time T > 0 can we have
r(T) = r(0), which implies that at no time T > 0 can we have x(T) = x(0). So, no solution
other than on the circle r =
√
5 or at the origin can return to where it started after T units
of time, and thus, the only possible limit cycle is on the circle r =
√
5. ⃝
The phase plane for (18.52) was shown in Figure 18.12.
18.6.1 Periodic Linearization
If the system of ODEs ˙x = f(x, t) is “periodic,” that is, there is a T > 0 such that f(x, t + T) ≡
f(x, t), then the linearization about an equilibrium point ¯x is also periodic. Why? Linear
approximation gives
f(x, t) = f(¯x, t) + A(t)(x −¯x) + g(x −¯x, t)
for x near ¯x, where
A(t) ≜
 ∂f
∂x(¯x, t)

.
(18.54)
The linearization of ˙x = f(x, t) near the equilibrium point ¯x is
˙y = A(t)y.
(18.55)
Using Theorem 5.25 in Section 5.8, we have
Theorem 18.14
(Stability from linearization)
a. If y = 0 is asymptotically stable for the periodic linearization (18.55), that is, all
characteristic multipliers of (18.55) have modulus |μ| < 1, then ¯x is asymptotically
stable for ˙x = f(x, t).
b. If the periodic linearization (18.55) has at least one characteristic multiplier whose
modulus is greater than one, then ¯x is unstable for ˙x = f(x, t).
18.6.2 Linearization about a Periodic Solution
Suppose the system of ODEs (⋆)
˙x = f(x, t) is either autonomous, that is, f(x, t) = f(x), or
periodic with period T > 0. Suppose that (⋆) has a solution ¯x(t) that is also periodic∗with
period T. We can study the stability of the periodic solution by linearization about it.
Linear approximation gives
f(x, t) = f(¯x(t), t) + A(t)

x −¯x(t)

+ g(x −¯x(t), t)
∗If the system is periodic, we assume it is periodic with the same period as the solution x(t).

Nonlinear Ordinary Differential Equations
1347
for x near ¯x(t), where
A(t) ≜
 ∂f
∂x(¯x(t), t)

(18.56)
is a square matrix. The linearization of ˙x = f(x, t) near the periodic solution ¯x(t) is
˙y = A(t)y.
(18.57)
In fact, every equilibrium point ¯x is periodic with any period we want, so linearization
about an equilibrium point is a particular case of linearization about a periodic solution.
Theorem 18.15
For an autonomous system ˙x = f(x), the periodic linearization (18.57) about a nonconstant
periodic solution always has at least one characteristic multiplier equal to one.
Why? Given ¯x(t), a non constant periodic solution of ˙x = f(x), define y(1)(t) ≜˙¯x(t). Using
the chain rule, take the time derivative of ˙¯x = f(x, t) to get
d
dt[y(1)(t)] = ¨¯x = d
dt[ f

¯x(t)

] =
 ∂f
∂x

¯x(t)

˙¯x(t) = A(t)y(1)(t).
(18.58)
We assumed ¯x(t) is not constant, so y(1)(t) is not identically zero. Letting the first column of
a fundamental matrix be y(1)(t), it follows that there is at least one characteristic multiplier
equal to one. 2
18.6.3 Levinson–Smith Theorem
For a special class of second-order autonomous ODEs, there is a theorem that established
the existence of a stable limit cycle. The Lienard equation was motivated by nonlinear
ODEs of electronics and has the form
¨x + f(x)˙x + g(x) = 0.
(18.59)
Define F(x) =
 x
0 f(ξ)dξ and G(x) =
 x
0 g(ξ)dξ.
Basic assumptions about f(x) and its definite integral F(x) are that
⎧
⎪⎪⎨
⎪⎪⎩
• f(x) is even and continuous on −∞< x < ∞,
• There exists a > 0 such that F(x) < 0 for 0 < x < a and F(x) > 0 for a < x,
• F(x) →∞as x →∞.
⎫
⎪⎪⎬
⎪⎪⎭
.
(18.60)
A pictorial example of such a function f(x) is given in Figure 18.14a.

1348
Advanced Engineering Mathematics
4
2
–2
–4
–2
2
4 x
1.0
0.5
–0.5
–4
–2
2
4
x
–4
–1.0
(a)
(b)
FIGURE 18.14
Pictorial examples of (a) f(x) and (b) g(x).
Basic assumptions about g(x) and its definite integral G(x) are that
⎧
⎪⎪⎨
⎪⎪⎩
• g(x) is odd and continuously differentiable on −∞< x < ∞,
• xg(x) > 0 for x > 0
• G(x) →∞as |x| →∞.
⎫
⎪⎪⎬
⎪⎪⎭
.
(18.61)
A pictorial example of such a function g(x) is given in Figure 18.14b.
In Problem 18.6.5.6, you will explain why the famous Van der Pol equation,
¨x −η

1 −x2˙x + x = 0,
(18.62)
is an example of such an ODE if η > 0.
Theorem 18.16
(Levinson–Smith) Assume that f(x) satisfies hypotheses (18.60) and g(x) satisfies hypothe-
ses (18.61). Then the second-order nonlinear ODE (18.59) has a stable limit cycle, and it is
unique in R2, up to translation in time.
A thorough and correct explanation for Theorem 18.16 can be found in many books
on advanced ODEs, including the book by Miller and Michel mentioned in “Learn More
About It.”
But we can give an intuitive argument why such a theorem is plausible. Consider the
oscillator ODE
¨y + p˙y + qy = 0.
If the damping coefficient is positive, all solutions have y(t) →0 as t →∞. If there were
such a thing as “negative damping,” then all solutions would have |y(t)| →∞as t →∞,
except for the equilibrium solution y(t) ≡0.
Consider a solution of the nonlinear ODE (18.59) that starts near (x, ˙x) = (0, 0). Because
p ≜f(x) < 0 for small |x|, it’s as if there is negative damping and the solution in the (x, ˙x)-
phase plane grows away from the origin as t increases.

Nonlinear Ordinary Differential Equations
1349
x.
6
6 x
4
4
2
2
–2
–6
–4
–4
–6
–2
FIGURE 18.15
Levinson–Smith theorem: stable limit cycle.
On the other hand, consider a solution of the nonlinear ODE (18.59) that starts far away
from (x, ˙x) = (0, 0). Because p ≜f(x) > 0 for large |x|, it’s as if there is positive damping and
the solution in the (x, ˙x)-phase plane moves toward the origin as t increases.
Also, because of the assumptions on q ≜g(x), it is as if there is a positive quasi-frequency
of vibration; hence, solutions tend to spiral around the origin.
Put all of this intuition together, and it becomes more reasonable to believe that there is
a closed curve solution in the phase plane, somewhere between “near the origin” and “far
from the origin.” That closed curve would have (x(T), ˙x(T)) = (x(0), ˙x(0)) for some T, which
would be the period of the periodic solution.
Figure 18.15 shows the phase plane for
¨x +
!
x2
0.5 −2.5e−1.4|x|
−1
"
˙x + 0.2x3 + x

2.5e−0.8x2 + 1

2x2 + 1
= 0.
(18.63)
The limit cycle passes through a point (x, ˙x) ≈(−2.4, 0). There is no physical significance to
this particular example, but it demonstrates the conclusion of the Levinson–Smith theorem
for a complicated-looking ODE. The graphs of f(x) and g(x) in Figure 18.14 produced the
stable limit cycle shown in Figure 18.15.
18.6.4 Hopf Bifurcation
A great source of periodic solutions of nonlinear ODEs is the “Poincaré–Andronov–Hopf
bifurcation.”
The word bifurcation refers to a situation where as a parameter, say λ, is varied, the
nature of solutions changes. For example, when you reach a bifurcation point or “fork in
the road,” you have a choice as to which branch of the road to take. Here’s an example.

1350
Advanced Engineering Mathematics
Example 18.25
Suppose λ is a parameter. Study the phase line for the scalar ODE ˙y = y2 −2y + λ. For
which value(s) of λ does the nature of solutions change as λ varies, that is, where is the
bifurcation point(s) of the parameter λ?
Method: This autonomous ODE can be rewritten as ˙y = f(y), where
f(y) = (y2 −2y) + λ = (y2 −2y + 1) −1 + λ = (y −1)2 + (λ −1).
The equilibrium solutions are those values of y for which f(y) = 0, that is,
1. y = 1 ±
√
1 −λ, if λ < 1.
2. y = 0, if λ = 1.
3. D.N.E., if λ > 1.
The phase lines for the three cases are shown in Figure 18.16. For λ < 1, there is one
stable equilibrium and one unstable equilibrium. The equilibria collapse into a single
unstable equilibrium at λ = 1, and then the equilibrium disappears for λ > 1. So, there is
a bifurcation point at λ = 1. ⃝
A Poincaré–Andronov–Hopf bifurcation, or Hopf bifurcation for short, is when a small
periodic solution appears or disappears as λ is varied. The basic situation is that at λ = 0,
the linearization of the ODE system about 0 has an imaginary pair of eigenvalues; hence,
we have a critical case, unlike the situation in Theorem 18.6 in Section 18.3. How that
imaginary pair of eigenvalues may behave as λ varies from 0 is illustrated in Figure 18.17.
To be specific, we have
Theorem 18.17
Assume that δ is a fixed positive number and an n×n matrix A = A(λ) depends on a single
parameter, λ, in the interval −δ < λ < δ. Assume that
y= 0
y=1+√1–λ
y=1–√1–λ
λ <1
λ=1
λ>1
FIGURE 18.16
Example 18.25’s phase lines.

Nonlinear Ordinary Differential Equations
1351
α(λ)+iω(λ)
α(λ)–iω(λ)
α
ω
FIGURE 18.17
Eigenvalues α(λ) ± iω(λ).
• (n −2) eigenvalues of A(λ) have negative real part for −δ < λ < δ.
• A has a pair of eigenvalues α(λ) ± iω(λ) that crosses the imaginary axis with
nonzero speed as λ passes through 0, meaning α(0) = 0, ω(0) = ω0, and dα
dλ(0) ̸= 0.
• g(x, λ) is three times continuously differentiable at (x, λ) = (0, 0).
• g(0, λ) ≡0 and ∂g
∂x(0, λ) ≡O, a zero matrix.
(18.64)
Then there exists a parametrized curve

xa(t), λ(a)

, 0 ≤a ≤a0, of nontrivial periodic
solutions of the system of ODEs
˙x = A(λ)x + g(x, λ).
(18.65)
Further, there is an invertible transformation Pa of Rn so that
Paxa(t) ≈[a cos ω0t
a sin ω0t
0
. . .
0]T, as a →0+.
The “a” is for “approximate amplitude.”
Theorem 18.18
Assume everything is as in Theorem 18.17. Then, generically, λ(a) has λ(a) ≈constant · a2,
and for each fixed value of λ(a), the periodic solution xa(t) is unique for ˙x = A

λ(a)

x +
g(x, λ(a)) in an open ball around 0, up to time translation.
Theorem 18.18 says that generically, which means “usually” is a sense that we will not
make precise, the situation is like that of a planar ODE system that in polar coordinates is
given by the ODE

1352
Advanced Engineering Mathematics
Stable
Stable
Stable
Stable
a
a
Unstable
Unstable
λ
λ
(a)
(b)
FIGURE 18.18
(a) ˙r = r · (λ −r2), (b) ˙r = r · (λ + r2).
˙r = r · (λ −r2)
(18.66)
or by the ODE
˙r = r · (λ + r2).
(18.67)
For (18.66), for each positive value of λ near zero, there is a stable periodic solution whose
amplitude is ≈
√
λ. The bifurcation diagram depicts the change in the nature of peri-
odic solutions as the parameter λ changes. The bifurcation diagram for (18.66) is shown in
Figure 18.18a. Note that the equilibrium point 0 loses stability as λ increases past zero; the
equilibrium point 0 is a periodic solution, too.
For (18.67), the bifurcation diagram in Figure 18.18b shows that for each negative value
of λ, at least near zero, there is an unstable periodic solution whose amplitude is ≈√−λ.
Theorem 18.19
(Bendixson–Dulac theorem) Define x = [x y]T and assume f(x) = [ f(x) g(x) ]T is continu-
ously differentiable in an open set D. If there is a continuously differentiable function μ(x)
such that
∂
∂x

μ(x)f(x)

+ ∂
∂y

μ(x)g(x)

takes on only one sign in D, then the autonomous ODE system ˙x = f(x) has no nonconstant
periodic solution in D.
Why? You will explain this in Problem 18.6.5.10.

Nonlinear Ordinary Differential Equations
1353
Learn More About It
Theorem 18.16 is proven in Theorem 7.3.1 of Ordinary Differential Equations, by Richard
K. Miller and Anthony N. Michel, Dover Publications, 2007, c⃝1982. Theorems 18.17
and 18.18 are Theorems 11.15 and 11.18, respectively, of Dynamics and Bifurcations, by
Jack K. Hale and H¨useyin Koçak, Springer-Verlag, c⃝1991.
A student who wants to explain the Levinson–Smith theorem should work on Problem
16.5, pp. 402–403, of Theory of Ordinary Differential Equations, by Earl A. Coddington and
Norman Levinson, McGraw-Hill, c⃝1955.
18.6.5 Problems
In problems 1–4, study limit cycles of the form r(t) ≡constant and their the stability for
the system. Also, give a rough sketch of the limit cycle(s) and their stability implicit in the
arrow of time on nearby solution curves.
1.
	˙x
˙y

=
⎡
⎢⎢⎣
y + x(1 −

x2 + y2)(2 −

x2 + y2)
−x + y(1 −

x2 + y2)(2 −

x2 + y2)
⎤
⎥⎥⎦
2.
	˙x
˙y

=
	 3x + 2y −x

x2 + y2
−2x + 3y −y

x2 + y2

3.
	˙x
˙y

=
⎡
⎣−x + y + 2 x e−√
x2+y2
−x −y + 2 y e−√
x2+y2
⎤
⎦
4.
	˙x
˙y

=
⎡
⎢⎣
3x + 2y −x

x2 + y2
−2x + 3y −y

x2 + y2
⎤
⎥⎦
5. Suppose f(x, y) is twice continuously differentiable in some open set O and the
gradient system of ODEs given by
(⋆)
⎧
⎨
⎩
˙x = ∂f
∂x
˙y = ∂f
∂y
⎫
⎬
⎭
has a solution x(t) = [x(t)
y(t)]T that is periodic with period T; hence, in partic-
ular x(T) = x(0). Explain why x(t) is in fact constant and thus is an equilibrium
point for (⋆). [Hint: Define h(t) ≜f(x(t), y(t)), explain why ˙h(t) > 0 at all t unless
∇f

x(t), y(t)

≡0, and then use Rolle’s theorem to get a contradiction if x(t) were
not constant.]

1354
Advanced Engineering Mathematics
6. Explain why the Van der Pol equation (18.62) satisfies the hypotheses of the
Levinson–Smith Theorem 18.16 if η > 0.
7. Suppose z(t) satisfies a differential equation of the form (⋆) ¨z + F(˙z) + z = 0, where
F(x) is odd and continuously differentiable on −∞< x < ∞.
(a) Define x(t) ≜˙z(t) and explain why x(t) satisfies the ODE (⋆⋆) ¨x+f(x)˙x+x = 0,
where f(x) ≜F′(x).
(b) Suppose in addition that (1) there is a constant a > 0 such that F(x) < 0 for
0 < x < a and F(x) > 0 for a < x < ∞and (2) F(x) →∞as x →∞. Explain
why ODE (⋆⋆) has a stable limit cycle.
(c) Use part (b) to conclude (⋆) has at most one limit cycle.
8. Explain why ¨x +

x4 −1
 ˙x + x3 = 0 has a stable limit cycle.
9. Explain why ¨x +

x2 −1
 ˙x + 0.01x + sin x = 0 has a stable limit cycle.
10. Explain why Theorem 18.19 is true: If there were a nonconstant periodic solution
C : x = x(t), 0 ≤t ≤T, with period T, in D then define D to be the interior of the
simple, closed, continuously differentiable parametrized curve C. Note that
μ(x(T))f(x(T)) = μ(x(0))f(x(0))
and use the divergence theorem for the vector field μ(x)f(x) in D to get a
contradiction.
11. Apply Theorem 18.19 with μ(x, y) = e−x to explain why the system
	˙x
˙y

=
	
−x3
−x3y −y3

has no periodic solution.
12. Suppose λ is a parameter. Study the phase line for the scalar ODE ˙y = sin y−λ. For
which value(s) of λ does the nature of solutions change as λ varies, that is, where
is the bifurcation point(s) of the parameter λ?
13. Consider the system in Example 18.11 in Section 18.2. Study the limit cycles as the
parameter β varies. For which value(s) of β does the nature of solutions change as
β varies, that is, where is the bifurcation point(s) of the parameter β?
14. Consider the system in Example 18.16 in Section 18.4. Study the limit cycles
as the damping coefficient b varies. For which value(s) of b does the nature
of solutions change as b varies, that is, where is the bifurcation points of the
parameter b?
18.7 Existence, Uniqueness, and Continuous Dependence
Here we will study basic issues concerning solutions of systems of (possibly) nonlin-
ear ODEs. First results are generalizations of the results for scalar ODEs that we saw in
Section 3.2. Throughout we will be studying the system of ODEs

Nonlinear Ordinary Differential Equations
1355
dx
dt = f(t, x)
(18.68)
and also the IVP
⎧
⎨
⎩
dx
dt = f(t, x)
x(t0) = x0
⎫
⎬
⎭.
(18.69)
Definition 18.23
A solution of an ODE system (18.68) is a function x(t) that is defined and differentiable on
an open interval I and satisfies the system of equations (18.68) on I.
Open intervals can be of the form (a, b) = {t : a < t < b}, where a < b, (−∞, ∞) =
{t : −∞< t < ∞}, (−∞, b) = {t : −∞< t < b}, or (a, ∞) = {t : a < t < ∞}.
A solution has to satisfy the system of ODEs on an open interval. A practical way to find
the interval is to substitute the supposed solution into the system of ODEs and during,
or after, that, choose the open interval I. But we usually do not have a formula for the
solution, so we may need to appeal to a theorem to guarantee that a solution exists.
Theorem 18.20
(Peano’s existence theorem) The IVP (18.69) has at least one solution as long as f(t, x) is
continuous in (t, x) in a closed “rectangle” given by
Rα,β(t0, x0) ≜{(t, x) : t0 −α ≤t ≤t0 + α, ||x −x0|| ≤β},
(18.70)
in Rn+1, for some positive scalars α, β.
As far as it goes, this is a good result. However, it doesn’t say how many solutions
there are for the IVP. In science and engineering, we prefer that a mathematical model of a
physical system should give only one prediction about future behavior. Also, the theorem
does not tell us for how long a time interval the solution exists. If it turns out that the
solution only exists for 10−40 s in the future, that would probably not be very useful for
making predictions in a physical problem.
In Section 3.2, we got existence and uniqueness results using conditions of differen-
tiability of the right-hand side of the ODE. It turns out that we can relax the conditions
somewhat using another kind of condition.
Definition 18.24
A function f(t, x) satisfies a local Lipschitz condition in x for (t, x) in an open set D in Rn+1
if there is a positive local Lipschitz constant K such that

1356
Advanced Engineering Mathematics
||f(t, x1) −f(t, x2)|| ≤K||x1 −x2||
(18.71)
for all (t, x1), (t, x2) in D.
Theorem 18.21
If ∂f
∂x(t, x) is continuous on a closed “rectangle” Rα,β(t0, x0), for some positive scalars α, β,
then f(t, x) satisfies a local Lipschitz condition in x for (t, x) in any open set D contained in
Rα,β(t0, x0).
Theorem 18.22
(Picard’s theorem with interval of existence) Suppose that
• f(t, x) is continuous in (t, x) and
• f(t, x) satisfies a local Lipschitz condition, with local Lipschitz constant K, in x
for (t, x) in a closed rectangle Rα,β(t0, x0),
(18.72)
for some positive scalars α, β, where t0 and x0 are the same as in the initial condition
x(t0) = x0. Suppose that there is positive constant M such that for all (t, x) in Rα,β(t0, x0),
we have
||f(t, x)|| ≤M.
If we choose ¯α and ¯β sufficiently small that
0 < ¯α ≤α, 0 < ¯β ≤β, M¯α ≤¯β, and K ¯α < 1,
then IVP (18.69) has exactly one solution on the time interval I¯α ≜[t0 −¯α, t0 + ¯α] and the
points

t, x(t)

remain in the closed “rectangle” R¯α, ¯β for all t in the interval I¯α.
Theorem 18.22 is illustrated in Figure 3.4.
The condition that M¯α ≤¯β has an interesting physical interpretation: ||˙x|| = ||f

t, x(t)

||
is the speed of an object whose position, x(t), satisfies ODE (18.68), so M is an upper bound
on that speed. So, M¯α ≤¯β says, by distance = speed × time, that the object can’t go further
away from x(t0) than a distance of ¯β, that is, ||x(t)−x0|| ≤¯β, that is, the graph of x(t) versus
t stays inside the “rectangle” R¯α, ¯β.
While we will not give a complete explanation for Theorem 18.22, we can mention
“Picard iterates,” a “building block” of an explanation. First, note that IVP (18.69) is
equivalent to the integral equation
x(t) = x0 +
t
t0
f

s, x(s)

ds, t0 −¯α ≤t ≤t0 + ¯α.
(18.73)

Nonlinear Ordinary Differential Equations
1357
By equivalent we mean that a continuously differentiable function x(t) satisfies IVP (18.69)
if, and only if, x(t) satisfies integral equation (18.73).
Let x1(t) be any function that is defined and continuously differentiable on the interval
I¯α that satisfies ||x1(t) −x0|| ≤¯β for all t in I¯α, where ¯α and ¯β are as in Theorem 18.22. We
call x1(t) the first Picard iterate. The second Picard iterate is defined to be
x2(t) ≜x0 +
t
t0
f

s, x1(s)

ds, t0 −¯α ≤t ≤t0 + ¯α,
and succeeding Picard iterates are defined recursively by
xk+1(t) ≜x0 +
t
t0
f

s, xk(s)

ds, t0 −¯α ≤t ≤t0 + ¯α.
An explanation of Theorem 18.22 consists of showing that the sequence of iterates
+
xk(t)
,∞
k=1 (a) converges to a function x∞(t) that is defined and continuously differentiable
on I¯α and (b) x∞(t) satisfies integral equation (18.73). The most efficient way to establish
those things is by using a “contraction mapping theorem.”
Before proceeding we need to know a property about norms and definite integrals of
vector-valued functions: If g(t) is continuous for t in [ a, b ], then


b
a
g(t)dt


≤
b
a
||g(t)|| dt.
Convergence of the sequence of Picard iterates is the same concept as the convergence
of the sequence of partial sums of a Fourier series in Section 9.1
While we will not give those explanations, we can use integral equation (18.73) to give a
brief explanation for the uniqueness conclusion of Theorem 18.22, that is, that (18.73) can
have no more than one solution.
For, if distinct functions x(t) and z(t) both satisfy (18.73), then
||x(t) −z(t)|| =


⎛
⎝

x0 +
t
t0
f

s, x(s)

ds
⎞
⎠−
⎛
⎝

x0 +
t
t0
f

s, z(s)

ds
⎞
⎠


≤

t
t0
||f

s, x(s)

−f

s, z(s)

|| ds

≤

t
t0
K||x(s) −z(s)|| ds

.
(18.74)
If we define the positive constant
 ≜
max
t0−¯α≤t≤t0+¯α ||x(t) −z(t)|| ,

1358
Advanced Engineering Mathematics
then (18.74) implies that for all t in I¯α,
 ≤

t
t0
K ds

= K|t −t0| < K¯α = (K ¯α).
(18.75)
But, a hypotheses of Theorem 18.22 is that K ¯α < 1, so (18.75) implies that
 < 1 · ,
which is absurd. So, the original assumption that distinct functions x(t) and z(t) both sat-
isfy (18.73) was incorrect. This establishes the uniqueness of the solution of the integral
equation (18.73) and thus of the IVP (18.69). □
Example 18.26
For the scalar IVP ˙y = y, y(0) = 1, use y1(t) ≡1 as the initial guess for the solution
and find the Picard iterates y2(t), y3(t), and y4(t). Take a guess about the formula for
the Picard iterate yk(t) and use that to find the Picard iterate yk+1(t). Find y∞(t) ≜
limk→∞yk(t) and compare it to y(t), the exact solution of the IVP.
Method: y1(t) ≡1, y2(t) = y(0) +
 t
0 y1(s)ds = 1 +
 t
0 1ds = 1 + t,
y3(t) = y(0) +
t
0
y2(s)ds = 1 +
t
0
(1 + s)ds = 1 +

s + 1
2 s2 t
0
= 1 + t + 1
2 t2,
y4(t) = y(0) +
t
0
y3(s)ds = 1 +
t
0
!
1 + s + 1
2 s2"
ds = 1 +

s + 1
2! s2 + 1
3! s3 t
0
= 1 + t + 1
2! t2 + 1
3! t3.
We guess
yk(t) = 1 + t + 1
2! t2 + 1
3! t3 + · · · +
1
(k −1)! tk−1,
and then
yk+1(t) = y(0) +
t
0
yk(s)ds = 1 +
t
0
!
1 + s + 1
2 s2 + · · · +
1
(k −1)! sk−1"
ds
= 1 +

s + 1
2! s2 + · · · + 1
k! sk t
0 = 1 + t + 1
2! t2 + · · · + 1
k! tk.
This suggests
y∞(t) ≜lim
k→∞yk(t) = 1 + t + 1
2! t2 + · · · + 1
k! tk + · · · ,
which equals y(t) = et, the exact solution of the IVP. ⃝

Nonlinear Ordinary Differential Equations
1359
4
y
y
3
2
–2
–1
1
y2 (t)
y3 (t)
y4 (t)
y5 (t)
t
t
2
3
–2
–1
20
40
60
80
100
120
140
1
2
3
1
–1
FIGURE 18.19
Picard iterates for ˙y = y, y(0) = 1.
Figure 18.19 shows graphs of the iterates y2(t), y3(t), y4(t), and y5(t). The latter three give
pretty good approximations of the exact solution on a finite time interval. But y4(t) and
y5(t) give very bad approximations for t > 1! While y∞(t) = limk→∞yk(t) is perfect, the
iterates may be troublesome “far” from the initial time.
18.7.1 Continuous Dependence
Recall that an equilibrium point for a system of ODEs is stable if for all nearby initial val-
ues, the solution stays near the equilibrium for all future time. A related concept deals with
how solutions depend, for a short amount of future time, on the initial time, initial value,
and parameters in the system of ODEs. This concept is called “continuous dependence.”
As a first step in this direction, consider how solutions depend on the initial value:
Consider IVP
⎧
⎨
⎩
dx
dt = f(t, x)
x(t1) = x1
⎫
⎬
⎭,
(18.76)
where we are allowed to vary the initial time, t1, and the initial value, x1.
Theorem 18.23
(Continuous dependence on initial condition) Suppose that f(t, x) satisfies hypotheses
(18.72) of Picard’s Theorem 18.22 and let the unique solution of IVP (18.76) be
x(t; t1, x1).
Then x(t; t1, x1) is continuous in (t, t1, x1). Let ¯α be as in the conclusion of Picard’s Theorem
18.22. It follows that for all ε > 0, no matter how small, and all final time T < ¯α, there
exists some δ > 0 such that
|t1 −t0| + ||x1 −x0|| < δ
implies
||x(t; t1, x1) −x(t; t0, x0)|| < ε for 0 ≤t ≤T.
(18.77)

1360
Advanced Engineering Mathematics
Notice that this result, (18.77), does not imply stability, which is a property that is to
hold for all t ≥t0. Indeed, result (18.77) holds whether or not x0 is an equilibrium point.
Nevertheless, continuous dependence on the initial value x1 is useful in theoretical stabil-
ity proofs, including those using Liapunov functions in Section 18.4. In fact, continuous
dependence plays a technical role in many fundamental ODE methods, for example, the
Poincaré map in Section 18.8.
Numerical methods also use continuous dependence. Suppose a numerical method for
an initial value problem for ˙x = f(t, x) produces an approximation xk for x(tk). In order to
be explicit in this discussion, suppose we are using Euler’s method. For the next time step,
there are four types of errors that can make xk+1 differ from x(tk+1):
1. The error in approximating x(tk+1) ≈xk+1+ △t f

tk, x(tk)

because △t is not
arbitrarily small
2. The error in approximating f

tk, x(tk)

≈f(tk, xk) because x(tk) ≈xk
3. Round off error during arithmetic operations, and
4. Approximation errors in the machine’s hard wired function evaluations
The first kind of error can be influenced by the choice of numerical method, for example,
using a Runge-Kutta formula versus Euler’s formula. The second kind of error is kept under
theoretical control using continuous dependence, which says that the error in the previous
step will propagate into the next step but hopefully will not have a drastic effect. Likewise,
continuous dependence helps control the effect of the third and fourth kinds of error.
Also, continuous or differentiable dependence on initial conditions is also useful in the
shooting method briefly discussed in Section 8.8. For example, suppose we want to solve
an ODE-BVP
#
y′′ = f(x, y, y′)
y(a) = α, y(b) = β
$
.
Let y(x; z) be the solution of the IVP
#
y′′ = f(x, y, y′)
y(a) = α, y′(a) = z
$
,
where we have continuous, or better yet, differentiable, dependence of y(x; z) on z. Then
we can use a numerical method for root finding to solve y(b; z) = β for z = z⋆. The solution
of the original ODE-BVP is then y(x; z⋆). In practice, yet another numerical method may be
needed to give an approximation to y(x; z).
Theorem 18.24
(Differentiable dependence on initial condition) Suppose that f(t, x) is continuously dif-
ferentiable on some ball Bδ = {(t, x) : |t −t1| + ||x −x1|| < δ} and let the unique solution of
IVP (18.76) be
x(t; t1, x1).

Nonlinear Ordinary Differential Equations
1361
Then x(t; t1, x1) is differentiable in (t, t1, x1), and the matrix
Y(t) =
 ∂x
∂x1
(t; t1, x1)

satisfies the linear variational equation
¯
˙Y =
 ∂f
∂x

t, x(t; t1, x1)
 
Y
(18.78)
for t in some interval (t −*δ, t +*δ) for some *δ > 0 satisfying 0 < *δ < δ.
We also have results in which we can vary the right-hand sides of the system of ODEs.
It is good that we have such results because in the real world we cannot assume that we
can perfectly measure the initial value x1, the initial time t0, or parameters in the system
of ODEs.
Suppose the system of ODEs is ˙x = f(t, x, λ), where λ are parameters in the system.
Theorem 18.25
(Continuous dependence on everything) Suppose that for (t, x) in a closed rectangle
Rα,β(t0, x0) and all λ with ||λ −λ0|| < α,
• f(t, x, λ) is continuous in (t, x, λ), and
• f(t, x, λ) satisfies a local Lipschitz condition in x, with local Lipschitz constant K,
(18.79)
for some α, β > 0, and let the unique solution of IVP (18.76) be
x(t; t1, x1, λ).
Then x(t; t1, x1, λ) is continuous in (t, t1, x1, λ). Let ¯α be as in the conclusion of Picard’s
Theorem 18.22. It follows that for all ε > 0, no matter how small, and all final time T < ¯α,
there exists some δ > 0 such that ||λ1 −λ0|| + |t1 −t0| + ||x1 −x0|| < δ implies
||x(t; t1, x1, λ1) −x(t; t0, x0, λ0)|| < ε for |t −t1| ≤T.
(18.80)
An informal definition of “chaos” is that there is “sensitive dependence on initial condi-
tions,” in which no matter how close are two initial values, the long-term future behavior
appears to depend randomly on the initial value. This phenomenon will be discussed fur-
ther in Section 18.8. A related concept is that of a “strange or chaotic attractor,” a set that is
an attractor according to Definition 18.19 in Section 18.5 but has a Smale horseshoe map
structure as will be described in Section 18.8.
Note that continuous dependence only applies locally in time, that is, for |t−t1| ≤T < ¯α,
whereas sensitive dependence on initial conditions refers to behavior as t →∞.

1362
Advanced Engineering Mathematics
Continuous dependence on the right-hand sides, f(t, x, λ), is part of studying the “struc-
tural stability” of systems of ODEs. It is a curious intuitive result that systems with chaos
tend to be structurally stable in the sense that small perturbations of chaotic systems tend
to be chaotic and preserve the property of having a strange attractor.
Learn More About It
There is a “uniform contraction mapping theorem” that not only gets the existence
and uniqueness conclusions of Picard’s theorem but also the continuous dependence
conclusions of Theorems 18.23 through 18.25. Textbooks for graduate level study of
ODEs, such as Ordinary Differential Equations, by Jack K. Hale, Robert E. Krieger Pub-
lishing Co., c⃝1980, Section I.3, have such theorems and their justifications.
Problem 18.7.2.7 is a version of Problem 2.3.1 in Differential Equations and Dynami-
cal Systems, 3rd ed., by Lawrence Perko, Springer-Verlag, Inc., c⃝2001.
Ordinary Differential Equations, by Garrett Birkhoff and Gian-Carlo Rota, Blaisdell
Publ. Co., c⃝1969, has a nice result, Theorem 6.4.3, that compares solutions of two
systems, ˙x = f(t, x) and ˙x = g(t, x), without assuming the latter satisfies a local Lipschitz
condition.
18.7.2 Problems
1. For the scalar IVP ˙y = y, y(2) = 1, use y1(t) ≡1 as the initial guess for the solution
and find the Picard iterates y2(t), y3(t), and y4(t). Take a guess about the for-
mula for the Picard iterate yk(t) and use that to find the Picard iterate yk+1(t). Find
y∞(t) ≜limk→∞yk(t) and compare it to y(t), the exact solution of the IVP. [Hint: It
helps to leave things factored, for example,
 t
2(s −2)ds =

1
2 (s −2)2 t
2 = 1
2 (t −2)2
rather than write it as
 t
2(s −2)ds =

1
2 s2 −2s
t
2 = 1
2 (t2 −22) −2(t −2).]
2. For the scalar IVP ˙y = y2, y(2) = 1, use y1(t) ≡1 as the initial guess for the solution
and find the Picard iterates y2(t), y3(t), y4(t), and y5(t). Take a guess about y∞(t) ≜
limk→∞yk(t) and compare it to y(t), the exact solution of the IVP.
[Hint: You may want to use Mathematica to find some of the iterates, for
example, using commands y[1, t ] : = 1, y[2, t] = y[1, t]+
 t
0 (y[1, s])2ds, . . . , y[5, t] =
y[1, t] +
 t
0 (y[4, s])2ds.]
3. For the IVP system ˙x =
 0
1
−1
0

x, x(0) =
1
0

, use x1(t) ≡
1
0

as the initial guess
for the solution and find the Picard iterates x2(t), x3(t), x4(t), and x5(t). Take a
guess about the formula for the Picard iterate xk(t) and use that to find x∞(t) ≜
limk→∞xk(t) and compare it to x(t), the exact solution of the IVP.
4. For the IVP system ˙x =
 0
1
−4
0

x, x(0) =
1
0

, use x1(t) ≡
1
0

as the initial guess
for the solution and find the Picard iterates x2(t), x3(t), x4(t), and x5(t). Take a
guess about the formula for the Picard iterate xk(t) and use that to find x∞(t) ≜
limk→∞xk(t) and compare it to x(t), the exact solution of the IVP.

Nonlinear Ordinary Differential Equations
1363
5. (a) Explain why every continuously differentiable function x(t) that satisfy IVP
(18.69) must also satisfy integral equation (18.73).
(b) Explain why every continuously differentiable function that x(t) satisfies
integral equation (18.73) must also satisfy IVP (18.69).
6. In this problem, you will use Picard’s theorem in the special case of linear systems
in Rn, that is, (⋆)
˙x = A(t)x. Throughout, assume that A(t) is continuous on an
interval I = {t : a ≤t ≤b}. Also, you may assume that (1) the norm of a matrix B
satisfies ||Bx|| ≤|| B || || x || for all x for which the matrix multiplication is defined
and (2) if the matrix A(t) is continuous in t on I, then there exists
K = max
a≤t≤b ||A(t)|| < ∞.
(a) Define f(t, x) = A(t)x. By linearity, A(t)x1 −A(t)x2 = A(t)(x1 −x2). Use this and
the preceding assumptions to explain why f(t, x) satisfies conditions (18.72).
(b) Explain why the other hypotheses of Picard’s theorem are true for (⋆) and thus
conclude that for every t0 satisfying a < t0 < b and every x0 in Rn, there exists
an ¯α such that a solution of IVP ˙x = A(t)x, x(t0) = x0 exists on the time interval
t0 −¯α < t < t0 + ¯α.
(c) Because the solution exists on t0 −¯α < t < t0 + ¯α, explain why we can use
the IVP
˙x = A(t)x, x

t0 + 3¯α
4

given,
to continue the solution to exist on the whole time interval
t0 −¯α < t < t0 + 5¯α
4 ,
and then continue the solution to exist on
t0 −¯α < t < t0 + 7¯α
4 ,
etc. Continuing, we get the solution existing on all of the time interval t0 −¯α <
t < b. Working backward in time, in a similar way, we get the solution existing
on all of the time interval a < t < b.
7. Suppose A is a constant matrix and define x(t; t1, x1) to be the unique solution of
the IVP ˙x = Ax, x(t1) = x1. Express x(t; t1, x1) in terms of etA and then calculate the
matrix Y(t) ≜∂x
∂x1
(t; t1, x1). Compare your conclusion with the results of Theorem
18.24.
8. Suppose A(t) is continuous and periodic with period T. Define x(t; t1, x1) to be
the unique solution of the IVP ˙x = Ax, x(t1) = x1. Express x(t; t1, x1) in terms of the

1364
Advanced Engineering Mathematics
Floquet representation and then calculate the matrix Y(t) ≜∂x
∂x1
(t; t1, x1). Compare
your conclusion with the results of Theorem 18.24.
9. Assume λ is a positive constant and x1 is a fixed but unspecified nonzero vector.
For the ODE-IVP x =

0
1
−λ
1

x, x(0) = x1,
(a) Find the solution x(t; λ).
(b) Calculate
z(t; λ) ≜∂x
∂λ(t; λ).
(c) Find an ODE-IVP that is satisfied by the function z(t) you found in part (b).
10. For the IVP dy
dt = 1 + y2, y(0) = y1,
(a) Use separation of variables to find the solution, y(t; y1), assuming y1 ̸= 0.
(b) find the IVP satisfied by
z(t) ≜∂y
∂y1
(t; y1).
11. For the scalar IVP
⎧
⎨
⎩
dy
dt = 1 + y2
y(0) = 0
⎫
⎬
⎭,
(a) Explain why the method of separation of variables produces a solution y(t)
that exists only on a finite time interval of the form −δ < t < δ.
(b) Explain why Picard’s theorem guarantees the existence and uniqueness of a
solution on an interval of the form −¯α ≤t ≤¯α, and
(c) Compare the largest value of ¯α that Picard’s theorem produces with the value
of δ. Conclude that Picard’s theorem theoretical guarantee of a time interval of
existence is not as good as the actual time interval of existence!
18.8 Short Take: Horseshoe Map and Chaos
Scientist and mathematicians discovered that nonlinear ordinary differential or difference
equations can have solutions that behaved in seemingly unpredictable ways. One such
phenomenon is “sensitive dependence on initial conditions,” in which no matter how close
are two initial values, the long-term future behavior appears to depend randomly on the
initial value. This is called “chaos.”
This was poetically expressed as the “butterfly effect,” the idea that the tiny wind from
the flapping wings of a butterfly in Mexico could have a big effect on the rain falling

Nonlinear Ordinary Differential Equations
1365
in Mongolia. Indeed, one of the earliest works connecting the mathematics of sensitive
dependence to the physical world was Lorenz’s system of three ODEs model (Lorenz,
1963) of the atmosphere.
This is true even though the differential or difference equations themselves were “deter-
ministic,” that is, did not include random variables as terms or factors. But even random
variables, for example, “Brownian motion,” have structure and characteristics, and people
did find structure in deterministic chaos. These structures include “strange attractors.”
Moreover, scientists and engineers learned how to use chaos in mathematical models,
for example, in neurophysiology and communication theory.
One well-developed theory that makes the aforementioned concepts precise is the phe-
nomenon of period doubling: A scalar or system of differential or difference equations can
have periodic solutions whose periods depend on a parameter in the system model, and
those periods could successively double and double as the parameter changes. We have
seen in Section 5.8 that a system could have a specific initial value that gives a periodic
solution. A periodic solution’s period tells us when the state returns to its initial value. It
makes sense that an initial value near that specific value may wander further away the
longer amount of time it has to “wander.” An analogy is that a pet animal left alone can be
harder to find the longer he is left unattended. So, having an ever increasing period could
lead to increasingly sensitive dependence on initial conditions.
What makes the “period doubling route to chaos” even more appealing is that as the
period doubles and doubles endlessly, the number of periodic solutions can increase
endlessly. So, an initial condition could be near many different periodic solutions as it wan-
ders. When the state of chaos is reached, there can be a “dense set” of periodic solutions in
the sense that every initial condition is arbitrarily close to some periodic solution.
One successful model that makes all of the aforementioned text precise is the
Smale horseshoe map. Consider an alphabet consisting of two letters, {A, B} and a
bi-infinite sequence
+
xk
,∞
k = −∞with each xk being either A or B. The set of all such
sequences is infinite. In fact, if we look at their truncations
+
xk
,n
k = −n, then we have Nn = 22n
of them: As the size 2n of the truncations increases endlessly, then Nn, the number of
possible truncated sequences, increases endlessly. In fact,
lim
n→∞
ln Nn
2n
= ln 2
is called the entropy. Because the entropy is positive, a random choice of a bi-infinite
sequence using this alphabet could be anything. This is related to the saying that a “give
an infinite number of monkeys an infinite amount of time with word processing software,
and they will reproduce the works of Shakespeare” mixed in with meaningless streams
of characters such as “vpeiwq jidfkjlasddfkpreoptyiofgnksdlpfbdlkd. . .” I struck my key-
board at random. Mixed in were three words, “as,” “opt,” and “of” as well as a sort of
word “pre(-)op.”
We have seen in Section 6.6 examples of linear mappings of the plane R2. Nonlinear
mappings
x →y = F(x)
can be much more complicated. The horseshoe map bends a region D into a horseshoe
shape, as depicted in Figure 18.20.

1366
Advanced Engineering Mathematics
Horseshoe map
V0
V1
H1
H0
FIGURE 18.20
Horseshoe map.
A map defines a system of nonlinear difference equations
xk+1 = F(xk)
whose solutions are sequences
{x0, x1, x2, . . .},
that is,
+
x0, F(x0), F

F(x0)

, . . .
,
.
For difference equations, there are concepts of stable and unstable manifolds, S and U,
respectively, that is, curves passing through ¯x along which solutions
+
xk
,∞
k = 1 either tend
toward or away from ¯x as k →∞. The stable and unstable manifolds are invariant forward
in time, k: For example, if your initial value x0 starts on S, then the iterates x1, x2, . . . are on
S, that is, F(S) ⊆S.
The horseshoe map on D takes two vertical strips V0, V1 into two horizontal strips
H0, H1, respectively, for example, F(V0) ⊆H0. The action of the horseshoe map on D
squeezes space in one direction tangent to the stable manifold and elongates space in the
other direction tangent to the unstable manifold. We can imagine that this has the effect
that points that are initially close can move much further away as time, k, increases. This
gives sensitive dependence on initial conditions. The details of how this happens require
some lengthy explanations, but we will only mention the conclusions that in part the map
behaves like it was acting on the space of bi-infinite sequences of letters chosen from {A, B}.
This gives both a dense set of periodic solutions and the kind of randomness that follows
from having positive entropy.
ODEs
So, what does a horseshoe map have to do with ODEs? The answer is that the solutions
of a system of ODEs can define a nonlinear map.
The first way that we can get a map of the plane is if we have an autonomous system of
three ODEs, that is, our state space is R3. Suppose that we have a periodic solution curve
C : x(t), −∞< t < ∞that passes through a point x(0) and we have a plane  that is
perpendicular to C at x(0), that is, the tangent vector ˙x(0) is normal to . Solutions that
start on  and are near to x(0) will return to  in roughly the same amount of time as it
takes for x(t) to return to x(0), because of continuous dependence of solutions on the initial
value. So, a point x on , near to x(0), will have an image F(x) on . F is called the induced
Poincaré map. If the Poincaré map behaves like the horseshoe map on a region on , then
the system of ODEs in R3 will exhibit chaos.

Nonlinear Ordinary Differential Equations
1367
People have been able to show that under special circumstances a “homoclinic orbit”
will induce a horseshoe map and thus chaos. A homoclinic orbit is a solution curve C :
x = x(t), −∞< t < ∞that is not periodic but instead has
lim
t→∞x(t) = ¯x = lim
t→−∞x(t).
Because of uniqueness of solutions, the homoclinic orbit never actually touches the equilib-
rium point ¯x. One special circumstance we want is that the homoclinic orbit should be on
both the unstable and stable manifolds of ¯x. Because the state space is R3, the stable mani-
fold or the unstable manifold will have dimension two, and the other will have dimension
one. The other special circumstance we want is that the tangent spaces to those two man-
ifolds should intersect transversally, meaning that the tangent vectors to the stable and
unstable manifolds should, together, span the whole space.
The second way that we can get a map of the plane is if we have a periodic, nonau-
tonomous system of two ODEs that is a small perturbation of an autonomous planar
ODE. Because of periodicity, solutions do depend on the initial time. Suppose we have
a homoclinic or heteroclinic orbit for the corresponding autonomous planar ODE. Then
it is possible to show that there may be a dense set of periodic solutions and sensitive
dependence on initial conditions, that is, chaos.
Learn More About It
A useful book of illustrations for dynamics is Dynamics: The Geometry of Behavior, par-
ticularly Part Three: Global Behavior, by Ralph H. Abraham and Christopher D. Shaw,
Aerial Press, Inc.
A nice exposition of the Smale horseshoe map is in Section 3.5 of An Introduc-
tion to Dynamical Systems, by D. K. Arrowsmith and C. M. Place, Cambridge University
Press, c⃝1990. Other good references for period doubling and chaos and their mathe-
matical foundations are Section 11.3 of Methods of Bifurcation Theory, by Shui-Nee Chow
and Jack K. Hale, Springer-Verlag, c⃝1982; Nonlinear Dynamics and Chaos, by Steven H.
Strogatz, Addison-Wesley, c⃝1994; and Section 12.3 of Nonlinear Dynamics and Chaos,
by J. M. T. Thompson and H. B. Stewart, John Wiley & Sons, c⃝1986.
A famous result of chaos in nonlinear difference equations is in “Period three implies
chaos,” by Tien-Yien Li and James A. Yorke, Am. Math. Mon. 82 (1975), 985–992.
18.9 Short Take: Delay Equations
An example of a delay equation is
d
dt

x(t)

= ax(t) + bx(t −r),
(18.81)
where r is a positive constant and a, b are constant coefficients. In order to know the value
of the derivative ˙x(t) for times in the interval 0 ≤t ≤r, we need to know the values of

1368
Advanced Engineering Mathematics
ax(t) + bx(t −r) for 0 ≤t ≤r; hence, we need to know the values of
x(θ) for −r ≤θ ≤0.
So, in order to solve the delay equation (18.81) over as large a time interval as possible, we
need to know not just the initial value x(0) but also the history of x(t) for −r ≤t ≤0.
A useful notation is
xt(θ) ≜x(t + θ) for −r ≤θ ≤0.
(18.82)
We see that the initial history is given by
x0(θ) ≜x(θ) for −r ≤θ ≤0.
(18.83)
We can think of a delay equation as describing the time evolution of xt(θ) as a function
of t, and the state space in which this is taking place is the vector space C consisting of all
functions φ(θ) that are defined and continuous in θ for −r ≤θ ≤0, that is,
C ≜C[ −r, 0 ].
C is a complete normed vector space when given the norm
||φ||∞≜max
−r≤θ≤0 |φ(θ)|.
We can study the existence and uniqueness issues of delay equations using Picard iter-
ates just as we did for ODEs in Section 18.7. For example, the scalar delay equation (18.81),
along with the initial history x0(θ), is equivalent to the integral equation
x(t) = x(0) +
t
0

ax(s) + bx(s −r)

ds.
(18.84)
So, at a time t > 0, the history xt = xt(θ) is given by
xt(θ) = x(0) +
t+θ

0

ax(s) + bx(s −r)

ds, −r ≤θ ≤0.
Theorem 18.26
(Existence and uniqueness) For the IVP
⎧
⎨
⎩
d
dt

x(t)

= f(t, x(t), xt)
xt0(θ) ≜φ0(θ), −r ≤θ ≤0
⎫
⎬
⎭,
(18.85)

Nonlinear Ordinary Differential Equations
1369
suppose that for all (t, x) in a closed rectangle Rα,β

t0, φ0(0)

and all ϕ with ||ϕ −φ0||∞< β,
• f(t, x, φ) is continuous in (t, x, φ) and
• f(t, x, φ) satisfies local Lipschitz condition
|f(t, x, φ) −f(t, y, ψ)| ≤K

|x −y| + ||φ −ψ||∞

,
(18.86)
for some α, β > 0. Then for some ¯α > 0, IVP (18.85) has a unique solution xt = xt(θ), −r ≤
θ ≤0 defined for t0 < t < t0 + ¯α.
You may have noticed that Theorem 18.81 concludes with the existence (and uniqueness)
of a solution only forward in time, that is, for t0 < t < t0 + ¯α. It is an unfortunate reality
that we cannot expect to have the existence and uniqueness of a solution backward in time.
Reference to an example of this will be in “Learn More About It” at the end of the section.
If the initial history x0 = x0(θ) ≜φ(θ) is in C, that is, is continuous on the interval −r ≤
θ ≤0, it makes sense that there should be a unique solution of (18.81) that exists as a
continuously differentiable function for 0 < t < ∞. In addition, as long as the initial
history is differentiable from the left at θ = 0 and satisfies the “compatibility condition”
lim
△θ→0−
φ(△θ) −φ(0)
△θ
≜˙φ(0−) = aφ(0) + bφ(−r),
then the solution x(t) will be differentiable at t = 0+.
18.9.1 Characteristic Equation
For the delay equation (18.81), try to find solution(s) in the form x(t) = eλt where λ is a
constant. Substituting this into the delay equation, we get
λeλt = ˙x(t) = ax(t) + bx(t −r) = aeλt + beλ(t−r),
and then multiply through by e−λt to get
λ = a + be−λr.
(18.87)
Unfortunately, this is not an algebraic equation in λ. In fact, there can be infinitely many
distinct roots λ1, λ2, . . . , so delay equation (18.81) can have general solution
x(t) =
∞
-
n=1
cneλnt,
where c1, c2, . . . are arbitrary constants. Indeed, in order to satisfy the initial history
condition, we need to choose c1, c2, . . . so that
φ(θ) = x0(θ) =
∞
-
n=1
cneλnθ, −r ≤θ ≤0.

1370
Advanced Engineering Mathematics
We have seen in the study of Fourier series that, in principle, we need an infinite family of
functions in order to expand a function in a series of functions.
In fact, delay equations are like PDEs in this respect. Recall that when solving the heat
equation, we need to specify the initial heat distribution over space; for a delay equation,
we need to specify the initial history over time.
18.9.2 Euler’s Method
The simplest numerical method for ODEs is Euler’s method. We can use it also for delay
equations: Partition the history time interval −r ≤t ≤0 into subintervals of equal length
△t = r
N and define
xi = x(ti),
for
i = −N, −N + 1, . . . , 0, 1, 2, . . . .
Note that the initial history function φ(θ) = x0(θ) = x(θ) gives the values
xi = φ(i △t),
for
i = −N, −N + 1, . . . , 0.
For i = 1, 2, . . . we use the delay equation to approximate ˙x(ti). For example, for the delay
equation (18.81), Euler’s method would give
xi+1 = xi + △t ·

axi + bxi−N

,
for
i = 0, 1, 2, . . . .
Any other finite differences numerical method, such as a Runge–Kutta method, can be
implemented in a similar way.
Learn More About It
Introduction to Functional Differential Equations, by Jack K. Hale and Sjoerd M. Verduyn
Lunel, Springer-Verlag, c⃝1993, is a good reference for the study of all kinds of delay
equations. Property 4.2 on pp. 68–69 gives an example where uniqueness backward in
time fails to be true even though there is uniqueness forward in time, thus illustrating
one of the many ways that delay equations can be much different from ODEs.
There are also models that involve both partial derivative and delays, as in Theory
and Applications of Partial Functional Differential Equations, by Jianhong Wu, Springer-
Verlag, c⃝1996.
18.9.3 Problems
In problems 1–3, find the characteristic equation of the delay equation or system of
equations.
1.
d
dt

x(t)

= ax(t) + bx

t −r
2

+ cx(t −r)
2.
d2
dt2

x(t)

= ax(t) + bx(t −r)
3.
d
dt

x(t)

= Ax(t) + Bx(t −r), where x is in Rn and A and B are n × n matrices.

Nonlinear Ordinary Differential Equations
1371
Key Terms
almost linear: (18.21) in Section 18.3
alphabet: Section 18.8
asymptotically stable: Definition 18.4 in Section 18.2
attractor: Definition 18.3 in Section 18.2
autonomous: before (18.2) in Section 18.1
bifurcation: before Figure 18.16
bifurcation diagram: Figure 18.18(a)
closed: after Definition 18.14 in Section 18.5
closure: Definition 18.15 in Section 18.5
constant solution: Definition 18.1 in Section 18.1
continuous dependence: Theorem 18.23 in Section 18.7
differentiable dependence: Theorem 18.24 in Section 18.7
critical point: Definition 18.1 in Section 18.1
curved solution: after (18.7) in Section 18.1
entropy: Section 18.8
equilibrium point, equilibria: Definition 18.1 in Section 18.1
generically: Theorem 18.18 in Section 18.6
gradient system: Problems 18.4.5.22, 18.6.5.5
history: after (18.81) in Section 18.9
homoclinic orbit: Section 18.8
Hopf bifurcation: before Figure 18.17
indefinite function: Definition 18.12 in Section 18.4, Definition 18.13 in Section 18.4
invariant set: Definition 18.6 in Section 18.3
k-dimensional surface: Definition 18.7 in Section 18.3
Liapunov equation: (18.46) in Section 18.4
Liapunov function: Definition 18.11 in Section 18.4
Lienard equation: (18.59) in Section 18.6
limit cycle: Definition 18.22 in Section 18.6
linear variational equation: Theorem 18.24 in Section 18.7
Lipschitz condition: Definition 18.24 in Section 18.7
Lipschitz constant: Definition 18.24 in Section 18.7
linearization: (18.14) in Section 18.2, (18.55) in Section 18.6, (18.57) in Section 18.6
maximal, closed, positively invariant: Definition 18.18 in Section 18.5
negative definite: Definition 18.10 in Section 18.4, Definition 18.13 in Section 18.4
negative semi-definite: Definition 18.10 in Section 18.4, Definition 18.13 in Section 18.4
open set: before Definition 18.10 in Section 18.4
orbit: after Definition 18.22 in Section 18.6
orbitally stable: after Definition 18.22 in Section 18.6
period doubling: beginning of Section 18.8
periodic: before Definition 18.5 in Section 18.2
phase line: before Example 18.4 in Section 18.1
phase plane: before Example 18.1 in Section 18.1
Picard iterates: after (18.73) in Section 18.7
Poincaré–Andronov–Hopf bifurcation: before Figure 18.17
Poincaré map: Section 18.8
positive definite: Definition 18.10 in Section 18.4, Definition 18.13 in Section 18.4
positive semi-definite: Definition 18.10 in Section 18.4, Definition 18.13 in Section 18.4

1372
Advanced Engineering Mathematics
positively invariant: Definition 18.14 in Section 18.5
punctured ball: before Definition 18.10 in Section 18.4
quadratic form: (18.36) in Section 18.4
rest point: Definition 18.1 in Section 18.1
saddle point: before (18.7) in Section 18.1
Smale horseshoe map: Section 18.8
solution: Definition 18.23 in Section 18.7
stability in the sense of Liapunov: Definition 18.2 in Section 18.2
stable: Definition 18.2 in Section 18.2, Definition 18.22 in Section 18.6
stable manifold: Definition 18.8 in Section 18.3
stable spiral point: Example 18.1 in Section 18.1
straight line solutions: after (18.7) in Section 18.1
time translate: before Definition 18.21 in Section 18.6
transversally: Section 18.8
twice continuously differentiable surface: Definition 18.7 in Section 18.3
unique up to time translation: Definition 18.21 in Section 18.6
unstable: Definition 18.2 in Section 18.2, Definition 18.22 in Section 18.6
unstable manifold: Definition 18.9 in Section 18.3
unstable spiral point: after Example 18.1 in Section 18.1
Van der Pol equation: (18.62) in Section 18.6
Mathematica Commands
s = NDSolve[{x′[t] == y[t] −x[t]3, y′[t] == −2x[t]3, x[0] == 1, y[0] == 0}, {x, y},
{t, −20, 20}]: after Example 18.2
a = ParametricPlot[Evaluate[{x[t], y[t]}/.s], {t, −20, 20}, PlotStyle−> {Blue, Thick},
PlotRange →{{−1, 1}, {−1.2, 1.2}}, LabelStyle →Directive[FontSize →16],
AxesLabel →{x, y}, AxesStyle →Thickness[0.00315]]: after Example 18.2
y[1, t ]
:=
1, y[2, t] = y[1, t]
+
 t
0 (y[1, s])2ds, . . . , y[5, t] = y[1, t]
+
 t
0 (y[4, s])2ds:
Problem 18.7.2.3
Reference
Lorenz, E.N. Deterministic nonperiodic flow. J. Atmos. Sci. 20, 130–141, 1963.

Appendix A: Partial Fractions
A.1 Partial Fractions Expansions
A partial fractions expansion is a way to decompose a rational function p(x)/q(x) into sim-
pler rational functions. A rational function is proper if the degree of the numerator is
strictly less than the degree of the denominator. For example,
p(x)
q(x) = −x4 + 5x2 −17x + 11
x(x + 3)2 
x2 + x + 1

is a proper rational function because the degree of the denominator is five and the degree
of the numerator is four.
A.1.1 Real Form
First, a real linear factor is a first-degree polynomial (ax + b), where a and b are real
and a ̸= 0. A quadratic irreducible over the reals, or irreducible quadratic for short, is
a second-degree polynomial (ax2 + bx + c) that has no real roots and for which a ̸= 0. For
example, x2 + 2x + 5 is an irreducible quadratic.
Given a proper rational function p(x)/q(x), in principle, we can factor the denominator,
q(x), as a product of linear factors and irreducible quadratic factors. For example,
q(x) = x5 + 7x4 + 16x3 + 15x2 + 9x = · · · = x(x + 3)2 
x2 + x + 1

.
If the denominator q(x)’s factors include (ax + b)m, the mth power of a linear factor, but
not (ax+b)m+1, then the form of the partial fractions expansion of a proper rational function
p(x)/q(x) will include the terms
A1
(ax + b) +
A2
(ax + b)2 + · · · +
Am
(ax + b)m
and no other terms having (ax + b) in a denominator. If m = 1, then the terms reduce to the
single term A1/(ax + b).
If an irreducible quadratic (ax2 + bx + c) is a factor of q(x), then the form of the partial
fractions expansion of a proper rational function p(x)/q(x) will include the term
C1x + E1
(ax2 + bx + c) .
1373

1374
Appendix A: Partial Fractions
If the mth power of an irreducible quadratic (ax2 + bx + c) is a factor of q(x) but (ax2 + bx +
c)m+1 is not a factor of q(x), then the form of the partial fractions expansion of a proper
rational function p(x)/q(x) will include the terms
C1x + E1
(ax2 + bx + c) + · · · +
Cmx + Em
(ax2 + bx + c)m
and no other terms having (ax2 + bx + c) in a denominator.
Example A.1
The form of the partial fractions expansion of −x4 + 5x2 −17x + 11
x(x + 3)2 
x2 + x + 1
 is
−x4 + 5x2 −17x + 11
x(x + 3)2 
x2 + x + 1
 = A
x +
B1
(x + 3) +
B2
(x + 3)2 +
Cx + E

x2 + x + 1
,
(A.1)
where A, B1, B2, C, E are constants.
Example A.2
Find the partial fractions expansion of x2 −17x + 11
x2(x + 1)3
.
Method: The partial fractions expansion is
x2 −17x + 11
x2(x + 1)3
= A1
x + A2
x2 +
B1
(x + 1) +
B2
(x + 1)2 +
B3
(x + 1)3 ,
(A.2)
where A1, A2, B1, B2, B3 are constants to be found. (Note that x2 = (1 · x + 0)2 is a power
of a linear factor.) How can we find the constants’ values? The first step is to multiply
through (A.2) by the denominator of the left-hand side (LHS), that is, by q(x) = x2(x+1)3.
This gives
x2 −17x + 11 = A1x(x+ 1)3+ A2(x+ 1)3 + B1x2(x+ 1)2+ B2x2(x+ 1)+ B3x2.
(A.3)
The easiest constants to find are A2 and B3, by substituting in the real zeros of q(x): At
x = 0, (A.3) is
02 −17 · 0 + 11 = A · 0 + A2 · (0 + 1)3 + B1 · 0 + B2 · 0 + B3 · 0 = A2,
so A2 = 11. At x = −1, (A.3) is
29 = (−1)2 −17 · (−1) + 11 = A1 · 0 + A2 · 0 + B1 · 0 + B2 · 0 + B3 · (−1)2 = B3;
hence, B3 = 29.
To find the remaining constants A1, B1, B2, there are many methods. One method is to
separate the terms in (A.3) by powers of x. In principle, this gives a system of five linear
equations in unknowns A1, A2, B1, B2, B3, but we can substitute the values A2 = 11 and
B3 = 29 that we already found by substituting in x = 0 and x = −1. That would reduce
the work to solving a system of three linear equations in the three unknowns A1, B1, B2.

Appendix A: Partial Fractions
1375
A method we prefer is to substitute the values A2 = 11 and B3 = 29 into (A.3) and
then keep on the right-hand side (RHS) only the terms involving the three remaining
unknowns A1, B1, B2:
x2 −17x + 11 = A1x(x+ 1)3+ 11(x+ 1)3+ B1x2(x+ 1)2+ B2x2(x+ 1)+ 29x2,
which implies
x2 −17x + 11 −11(x3 + 3x2 + 3x + 1) −29x2
= x2 −17x + 11 −11(x+ 1)3 −29x2 = A1x(x+ 1)3+ B1x2(x+ 1)2+ B2x2(x+ 1),
that is,
−11x3 −61x2 −50x = A1x(x+ 1)3+ B1x2(x+ 1)2+ B2x2(x+ 1).
Notice that on the RHS, we can factor out x(x + 1). In order to have the LHS = RHS, it
follows that we must be able to factor out x(x + 3) from the LHS. This gives
1
x(x + 1) ·

−11x3 −61x2 −50x

= A1(x + 1)2 + B1x(x + 1) + B2x,
which can be rewritten as
−(11x + 50) = −11x2 + 61x + 50x
x + 1
= A1(x + 1)2 + B1x(x + 1) + B2x .
(A.4)
To solve
−11x −50 = A1(x + 1)2 + B1x(x + 1) + B2x
(A.5)
for A1, B1, B2, substitute in x = 0 to get −50 = A1, and substitute x = −1 into (A.5) to get
−39 = −B2; hence, B2 = 39. Substitute the values of A1 and B2 into (A.5) to get
−11x −50 = −50(x + 1)2 + B1x(x + 1) + 39x,
so
−11x −50 + 50(x2 + 2x + 1) −39x = B1x(x + 1);
hence,
50 = 50x2 + 50x
x(x + 1)
= B1.
So, the partial fractions expansion is
x2 −17x + 11
x2(x + 1)3
= −50
x
+ 11
x2 +
50
(x + 1) +
39
(x + 1)2 +
29
(x + 1)3 . ⃝
Note that if we had made an error before arriving at (A.5), we might have heard an
“alarm bell ringing.” For example, if instead of (A.5) we had
−11x −50 +
̸= 0
x(x + 1) = A1(x + 1)2 + B1x(x + 1) + B2x,
then we would have reached a contradiction, because the polynomial on the RHS cannot
equal a non-polynomial on the LHS. So, if the division that arrives at (A.5) has a nonzero
remainder, then an alarm bell has rung.

1376
Appendix A: Partial Fractions
When an alarm bell such as that rings, it tells us that an error has happened somewhere;
but, unfortunately, it does not say what went wrong.
Example A.3
Find the partial fractions expansion of (x2 −3x + 5)/((x + 1)(x2 + x + 1)).
Method: The partial fractions expansion is
x2 −3x + 5
(x + 1)(x2 + x + 1) =
A
(x + 1) +
Cx + E
(x2 + x + 1),
(A.6)
where A, C, E are constants to be found. How? The first step is to multiply through (A.6)
by the denominator of the LHS, that is, by q(x) = (x + 1)(x2 + x + 1). This gives
x2 −3x + 5 = A(x2 + x + 1) + (Cx + E)(x + 1).
(A.7)
Substitute x = −1 into (A.7) to get 9 = A, and then substitute that into (A.7) to get
x2 −3x + 5 = 9(x2 + x + 1) + (Cx + E)(x + 1).
(A.8)
After that, we could leave on the RHS of (A.8) only what involves C and E:
−8x2 −12x −4 = x2 −3x + 5 −9(x2 + x + 1) = (Cx + E)(x + 1);
hence,
−8x −4 = −8x2 −12x −4
x + 1
= (Cx + E).
Because there is no remainder after the division, an alarm bell did not ring.
So, the partial fractions expansion is (x2 −3x + 5)/((x + 1)(x2 + x + 1)) = 9A/(x+1)+
(−8x −4)/(x2 + x + 1). ⃝
An alternative method of solving (A.7), that is, x2−3x+5 = A(x2+x+1)+(Cx+E)(x+1),
is to sort by powers of x:
1 · x2 −3x + 5 = A(x2 + x + 1) + (Cx + E)(x + 1) = Ax2 + Ax + A + Cx2 + Cx + Ex + E
= (A + C)x2 + (A + C + E)x + (A + E).
The x2 terms matched between the LHS and the RHS require that 1 = A + C; matching the
x1 terms requires that −3 = A+C+E. Matching the x0 terms requires that 5 = A+E. So, we
would need to solve the system of three equations in three unknowns given in compact
form by
⎡
⎣
1
1
0
1
1
1
1
0
1
⎤
⎦
⎡
⎣
A
C
E
⎤
⎦=
⎡
⎣
1
−3
5
⎤
⎦.
So,
⎡
⎣
A
C
E
⎤
⎦=
⎡
⎣
1
1
0
1
1
1
1
0
1
⎤
⎦
−1 ⎡
⎣
1
−3
5
⎤
⎦=
⎡
⎣
9
−8
−4
⎤
⎦.

Appendix A: Partial Fractions
1377
This gives the same conclusion as in Example A.3. Sorting by powers of x can be a useful
method. Its only disadvantage is that there is no alarm bell that can ring. If we use partial
fractions as one step in the middle of a long problem, it is a good idea to check that the
conclusion of the partial fractions expansion is correct.
A.1.2 Complex Form
If p(z)/q(z) is a proper rational function, then in principle∗in practice, we can factor g(z)
into a product of linear factors (z −a). So, we may choose not to worry about quadratic
factors that are irreducible over the reals and instead use only linear factors, possibly
involving complex numbers.
Example A.4
Find the complex partial fractions expansion of (4z −i7)/(z3 + 2z2 + 5z).
Method: (4z −i7)/(z3 + 2z2 + 5z)
=
4z −i7
z(z + 1 −i2)(z + 1 + i2) = A
z +
B
z + 1 −i2 +
C
z + 1 + i2
4z −i7 = A(z + 1 −i2)(z + 1 + i2) + Bz(z + 1 + i2) + Cz(z + 1 −i2).
Substituting in z = 0, z = −1 + i2, and z = −1 −i2 successively yields
A = −i7
5 ,
B =
−4 + i
(−1 + i2)i4 = · · · = 7 −i6
20
,
and
C =
−4 −i15
(−1 −i2)(−i4) = · · · = −7 + i34
20
.
So, we have the partial fractions expansion
4z −i7
z3 + 2z2 + 5z = 1
20

−i28
z
+
7 −i6
z + 1 −i2 + −7 + i34
z + 1 + i2

. ⃝
∗But, as we’ve seen in Chapter 8, solving a nonlinear equation may be difficult, even if the equation is of the
form polynomial = 0, if the polynomial has a high degree.


Appendix B: Laplace Transforms Deﬁnitions and
Derivations
B.1 Laplace Transforms
Definition B.1
L[ f(t) ](s) ≜
 ∞
0
f(t)e−st dt, for those values of s for which the improper integral converges.
Definition B.2
A function f(t) is of exponential order α if there is a positive constant M such that |f(t)| ≤
Meαt for all t ≥0. Here, α is a real number.
Theorem B.1
If f(t) is of exponential order α, then the improper integral L[ f(t)](s) converges for real
numbers s > α.
B.1.1 Properties and Their Derivations
Note that implicitly, each identity is asserted to be true only when both sides of the
identity exist.
1. L[ f(t) + g(t)](s) = L[ f(t) ](s) + L[ g(t)](s).
Derivation:
For
those
s
for
which
improper
integrals
 ∞
0
f(t)e−st dt
and
 ∞
0
g(t)e−st dt both exist, there exists
L[ f(t) ](s) + L[ g(t)](s) = lim
b→∞
b
0
f(t)e−st dt + lim
b→∞
b
0
g(t)e−st dt
= lim
b→∞
⎛
⎝
b
0
f(t)e−st dt +
b
0
g(t)e−st dt
⎞
⎠= lim
b→∞
b
0

f(t) + g(t)

e−st dt
≜
∞

0

f(t) + g(t)

e−st dt ≜L[ f(t) + g(t) ](s) . 2
1379

1380
Appendix B: Laplace Transforms Deﬁnitions and Derivations
2. L[ cf(t) ](s) = cL[ f(t) ](s), any constant c.
Derivation: For those s for which the improper integral
 ∞
0
f(t)e−st dt exists, there
exists
cL[ f(t) ](s) = c · lim
b→∞
b
0
f(t)e−st dt = lim
b→∞
b
0
cf(t)e−st dt ≜
∞

0
cf(t)e−st dt
≜L[ cf(t)](s) . 2
3. L[ eat ](s) =
1
s−a.
Derivation: For s > a, there exists
L[ e at ](s) ≜lim
b→∞
b
0
e ate−st dt = lim
b→∞
b
0
e−(s−a)t dt = lim
b→∞
 −1
s −a e−(s−a)t 	b
0
= lim
b→∞
−1
s −a

e−(s−a)b −1

= −1
s −a (0 −1) =
1
s −a . 2
4. L[ ˙y ](s) = sL[ y(t) ](s) −y(0).
Derivation: Integration by parts gives
L[ ˙y ](s) ≜lim
b→∞
b
0
˙y(t)e−st dt = lim
b→∞
⎛
⎝

y(t) e−st 	b
0 −
b
0
y(t)

−se−st
dt
⎞
⎠
= lim
b→∞
⎛
⎝y(b) e−sb −y(0) + s
b
0
y(t)e−st dt
⎞
⎠= 0 −y(0) + s
∞

0
y(t)e−st dt
= sL[ y(t) ](s) −y(0) . 2
5. L[ tn ](s) = n!/sn, n = 0, 1, 2, . . ..
Derivation: For n = 0,
L[ t0 ](s) = lim
b→∞
b
0
1 · e−st dt = lim
b→∞
 e−st
−s
	b
0 = lim
b→∞

e−sb
−s + 1
s

= 1
s .

Appendix B: Laplace Transforms Deﬁnitions and Derivations
1381
For integer n ≥1, define an ≜L[ tn ](s). Integration by parts gives
an ≜lim
b→∞
b
0
tne−st dt = lim
b→∞

tn · e−st
−s
	b
0 −
b
0
n tn−1 · e−st
−s dt
= lim
b→∞

tn · e−st
−s
	b
0 + n
s · L[ tn−1](s)
= lim
b→∞

−bn
3esb −0

+ n
s · an−1 = 0 + n
s · an−1,
using L’Hôpital’s rule. Induction explains why an = n!/sn. 2
6. L[ cos ωt ](s) =
s
s2 + ω2
Derivation: Using (3.9) in Section 3.1, that is,

eat cos btdt =
1
a2 + b2 eat(b sin bt +
a cos bt) + c,
L[ cos ωt ](s) ≜lim
b→∞
b
0
cos(ωt)e−st dt = lim
b→∞

e−st
(−s)2 + ω2 (ω sin ωt −s cos ωt)
b
0
=
1
s2 + ω2 lim
b→∞

e−sb(ω sin ωb −s cos ωb) + s

=
1
s2 + ω2 (0 + s) =
s
s2 + ω2 . 2
7. L[ sin ωt ](s) =
ω
s2 + ω2 .
Derivation: Using (3.10) in Section 3.1, that is,

eat sin btdt =
1
a2 + b2 eat(a sin bt −
b cos bt) + c,
L[ sin ωt ](s) ≜lim
b→∞
b
0
sin(ωt)e−st dt = lim
b→∞

e−st
(−s)2 + ω2 (−s sin ωt −ω cos ωt)
b
0
=
1
s2 + ω2 lim
b→∞

e−sb(−s sin ωb −ω cos ωb) + ω

=
1
s2 + ω2 (0 + ω) =
ω
s2 + ω2 . 2

1382
Appendix B: Laplace Transforms Deﬁnitions and Derivations
8. L[ eatf(t)](s) = F(s −a), where F(s) = L[ f(t) ](s).
Derivation:
L[ eatf(t) ](s) ≜lim
b→∞
b
0
eatf(t)e−st dt = lim
b→∞
b
0
f(t)e−(s−a)t dt
=
⎛
⎝lim
b→∞
b
0
f(t)e−st dt
⎞
⎠

s→(s−a) = L[ eatf(t) ]

s→(s−a) = F(s −a),
where F(s) = L[ f(t)](s). 2
9. L[ ¨y ](s) = s2L[ y](s) −sy(0) −˙y(0).
Derivation: Using property (4),
L[ ¨y](s) = L
 d
dt
 ˙y
 	
(s) = sL[ ˙y(t)](s) −˙y(0) = s

sL[ y(t)](s) −y(0)

−˙y(0)
= s2L[ y](s) −sy(0) −˙y(0). 2
10. L[ step(t −c)](s) = e−cs
s .
Derivation: For s > 0,
L[ step(t −c)](s) ≜lim
b→∞
b
0
step(t −c)e−st dt = lim
b→∞
⎛
⎝
c
0
0 · e−st dt +
b
c
1 · e−st dt
⎞
⎠
= lim
b→∞

0 +
 e−st
−s
	b
c

= lim
b→∞
e−sc
−s −1
−s

= e−cs
s
. 2
11. L[ g(t) step(t −c)](s) = e−cs · L[ g(t + c)](s).
Derivation:
L[ g(t) step(t −c)](s) ≜lim
b→∞
b
0
g(t)step(t −c)e−st dt
= lim
b→∞
⎛
⎝
c
0
0 · g(t) · e−st dt +
b
c
1 · g(t)e−st dt
⎞
⎠.

Appendix B: Laplace Transforms Deﬁnitions and Derivations
1383
Using the change of variables τ = t −c,
L[ g(t)step(t −c)](s) = lim
b→∞
b−c

0
1 · g(τ + c)e−s(τ+c) dτ = lim
b→∞
b−c

0
g(τ + c)e−sτ e−cs dτ
= e−cs
lim
(b−c)→∞
b−c

0
g(τ + c)e−sτ dτ = e−cs
lim
(b−c)→∞
b−c

0
g(t + c)e−st dt
≜e−csL[ g(t + c)](s) . 2
12. L[( f ∗g)(t)](s) = L[ f(t)](s) · L[ g(t)](s).
Derivation:
L[( f ∗g)(t)](s) ≜lim
b→∞
b
0
⎛
⎝
t
0
f(t −u)g(u)du
⎞
⎠e−st dt.
The triangle in the (t, u)-plane described by 0 ≤t ≤b, 0 ≤u ≤t can be described
as 0 ≤u ≤b, u ≤t ≤b, so changing the order of integration gives
L[( f ∗g)(t)](s) = lim
b→∞
b
0
⎛
⎝
b
u
f(t −u)g(u) dt
⎞
⎠e−st du
= lim
b→∞
b
0
⎛
⎝
b
u
f(t −u) e−s(t−u) e−su dt
⎞
⎠g(u) du.
The change of variables τ = t −u in the inner integral implies
L[( f ∗g)(t)](s) = lim
b→∞
b
0
⎛
⎝
b−u

0
f(τ)e−sτ dτ
⎞
⎠g(u)e−su du
= lim
b→∞
b
0

≈F(s)

g(u)e−su du,
where the ≈acknowledges that the inner integral is approaching F(s) ≜L[ f(t)](s)
as b →∞. So,
L[( f ∗g)(t)](s) = lim
b→∞

≈F(s)
 b
0
g(u)e−su du = F(s)G(s). 2

1384
Appendix B: Laplace Transforms Deﬁnitions and Derivations
13. L[ t sin ωt](s) = 2ωs/(s2 + ω2)2.
Derivation: Using integration by parts with u = t and dv = e−st sin ωt dt, and (3.10)
in Section 3.1, that is,

eat sin btdt =
1
a2 + b2 eat
a sin bt −b cos bt

+ c,
L[ t sin ωt](s) ≜lim
b→∞
b
0
t sin ωt e−st dt
= lim
b→∞

t ·
e−st
(−s)2 + ω2 (−s sin ωt −ω cos ωt)
	b
0
−
b
0
e−st
(−s)2 + ω2 (−s sin ωt −ω cos ωt) dt
⎞
⎠
= lim
b→∞

be−sb
s2 + ω2

−s sin ωb −ω cos ωb

−0

−
1
s2 + ω2 (−sL[ sin ωt](s) −ωL[ cos ωt](s))
= 0 −
1
s2 + ω2

−s ·
ω
s2 + ω2 −ω ·
s
s2 + ω2

=
2ωs
(s2 + ω2)2 . 2
14. L[ t cos ωt](s) = (s2 −ω2)/(s2 + ω2)2.
Derivation: Using integration by parts with u = t and dv = e−st cos ωt dt, and (3.9) in
Section 3.1, that is,

eat cos btdt =
1
a2 + b2 eat 
b sin bt + a cos bt

+ c,
L[ t cos ωt](s) ≜lim
b→∞
b
0
t cos ωt e−st dt
= lim
b→∞

t ·
e−st
(−s)2 + ω2 (ω sin ωt −s cos ωt)
	b
0
−
b
0
e−st
(−s)2 + ω2 (ω sin ωt −s cos ωt) dt
⎞
⎠
= lim
b→∞

be−sb
s2 + ω2

ω sin ωb −s cos ωb

−0


Appendix B: Laplace Transforms Deﬁnitions and Derivations
1385
−
1
s2 + ω2

ωL[ sin ωt](s) −sL[ cos ωt](s)

= 0 −
1
s2 + ω2

ω ·
ω
s2 + ω2 −s ·
s
s2 + ω2

=
s2 −ω2
(s2 + ω2)2 . 2
15. L[ δ(t −c)](s) = e−cs.
Derivation: Using “approximate delta functions,” we calculate
L[ δ(t −c)](s) = lim
n→∞L[ δn(t)](s)
= lim
n→∞L

n ·

step

t −

c −1
2n

−step

t −

c + 1
2n

(s)
= lim
n→∞n · 1
s ·

e−

c−1
2n

s −e−

c+ 1
2n

s

.
Substitute ε = 1/2n and use L’Hôpital’s rule to get
L[ δ(t −c)](s) = 1
s · lim
ε→0+
e−

c−ε

s −e−

c+ε

s
2ε
= 1
s · lim
ε→0+
se−

c−ε

s + se−

c+ε

s
2
= 1
s · se−cs + se−cs
2
= e−cs . 2
16. L[ f(t)](s) =
1
1 −e−sT
 T
0 f(t)e−st dt, for s > 0, if f(t) is periodic with period T.
Derivation:
L[ f(t)](s) ≜lim
b→∞
b
0
f(t)e−st dt = lim
N→∞
(N+1)T

0
f(t)e−st dt
= lim
N→∞
N
n=0
(n+1)T

nT
f(t)e−st dt.
Using periodicity and the substitution τ = t −nT, we have
(n+1)T

nT
f(t)e−st dt =
T
0
f(τ + nT)e−s(τ+nT) dτ =
T
0
f(τ)e−sτ e−nsT dτ
= e−nsT
T
0
f(τ)e−sτ dτ.

1386
Appendix B: Laplace Transforms Deﬁnitions and Derivations
So, noting that |e−sT| < 1 for s > 0, using the sum of a geometric series, we calculate
L[ f(t)](s) = lim
N→∞
N
n=0
⎛
⎝e−nsT
T
0
f(τ)e−sτ dτ
⎞
⎠=
⎛
⎝
T
0
f(τ)e−sτ dτ
⎞
⎠lim
N→∞
N
n=0

e−sTn
=
⎛
⎝
T
0
f(τ)e−sτ dτ
⎞
⎠
∞

n=0

e−sTn
=
⎛
⎝
T
0
f(τ)e−sτ dτ
⎞
⎠·
1
1 −e−sT . 2
When s is a complex variable, all of the aforementioned results are also true if
s > α is replaced by Re(s) > α.

Appendix C: Series Solutions of ODEs
C.1 Power Series Solutions of ODEs
We will start with a simple example that demonstrates the basic methods.
Example C.1
Find a power series solution of the ODE ˙y = 2y.
Method: Substitute a series solution in the form
y(t) =
∞

n=0
cntn
(C.1)
into the ODE to get
˙y =
∞

n=0
n cntn−1 =
∞

n=0
2cntn = 2y .
(C.2)
But, on the left-hand side, we really have
˙y = d
dt

c0t0 + c1t1 + c2t2 + c3t3 + · · ·

= 0 + c1t0 + 2c2t1 + 3c3t2 + · · · ,
which can be rewritten as
˙y =
∞

n=0
(n + 1)cn+1tn.
So, to solve our ODE means to solve
∞

n=0
(n + 1)cn+1tn =
∞

n=0
2cntn,
that is,
∞

n=0

(n + 1)cn+1 −2cn

tn = 0,
for the coefficients c0, c1, c2, . . . . This implies the recursion relation
(n + 1)cn+1 −2cn = 0,
1387

1388
Appendix C: Series Solutions of ODEs
that is,
cn+1 =
2
n + 1 cn,
which is actually a first-order difference equation. Unfortunately, we cannot use a char-
acteristic equation to help solve it. But fortunately, it is simple enough that we can guess
the solution: Because
c1 = 2
1 c0 = 2c0,
c2 = 2
2 c1 = 22
2 c0,
and
c3 = 2
3 c2 =
23
3 · 2 c0, . . . ,
we guess that
cn = 2n
n! c0, n = 1, 2, . . .
We can easily check that it satisfies the recursion relation. Thus, the solutions of the
ODE are
y(t) =
∞

n=0
cntn = c0t0 + c1t1 + c2t2 + c3t3 + · · · = c0 ·

1 + 21
1! t1 + 22
2! t2 + 23
3! t3 + · · ·

= c0

1 + 2t
1! + (2t)2
2!
+ (2t)3
3!
+ · · ·

= c0 · e2t,
where c0 is an arbitrary constant. Of course, we knew this already, but it is good to see
that our new method gives a result that agrees with our old method for solving ˙y = 2y. ⃝
Example C.2
Find a power series solution of the ODE ¨y + 4y = 0.
Method: Substitute a series solution in the form
y(t) =
∞

n=0
cntn
(C.3)
into the ODE to get
∞

n=0
n(n −1) cntn−2 +
∞

n=0
4cntn = 0 .
(C.4)
But the first term is
¨y = d2
dt2

c0t0 + c1t1 + c2t2 + c3t3 + · · ·

= 0 + 0 + 2 · 1c2t0 + 3 · 2c3t1 + · · ·
=
∞

n=2
n · (n −1) cntn−2 ,
which can be rewritten as
¨y =
∞

n=0
(n + 2)(n + 1)cn+2tn.

Appendix C: Series Solutions of ODEs
1389
So, to solve our ODE means to solve
∞

n=0
(n + 2)(n + 1)cn+2tn +
∞

n=0
4cntn = 0,
that is,
∞

n=0

(n + 2)(n + 1)cn+2 + 4cn

tn = 0,
for the coefficients c0, c1, c2, . . .. This implies the recursion relation
(n + 2)(n + 1)cn+2 + 4cn = 0,
that is,
cn+2 = −
4
(n + 2)(n + 1) cn,
which is actually a second-order difference equation. Unfortunately, we cannot use a
characteristic equation to help solve it. Fortunately, it is simple enough that we can
guess the solution: Because
c2 = −4
2 · 1 c0,
c4 = −4
4 · 3 c2 =
42
4 · 3 · 2 · 1 c0,
and
c6 = −4
6 · 5 c4 = · · · ,
we guess that
c2k = (−1)k 22k
(2k)! c0, k = 1, 2, . . . .
We can easily check that it satisfies the recursion relation. Thus, among the solutions
of the ODE are
y(t) =
∞

k=0
c2kt2k = c0t0 + c2t2 + c4t4 + c6t6 + · · ·
= c0 ·

1 −22
2! t1 + 24
4! t4 −26
6! t6 + · · ·

= c0y1(t),
where c0 is an arbitrary constant and y1(t) ≜cos 2t. Of course, we knew this already, but
it’s good to see that our new method gives a result that agrees with our old method for
solving ¨y + 4y = 0.
But, wait, there are other solutions: Because
c3 = −4
3 · 2 c1,
c5 = −4
5 · 4 c3 =
42
5 · 4 · 3 · 2 c1,
and
c7 = −4
7 · 6 c5 = · · · ,
we guess that
c2k+1 = (−1)k
22k
(2k + 1)! c1, k = 1, 2, . . . .

1390
Appendix C: Series Solutions of ODEs
We can easily check that it satisfies the recursion relation. Thus, among the solutions
of the ODE are
y(t) =
∞

k=0
c2k+1t2k+1 = c1t1 + c3t3 + c3t3 + c5t5 + · · ·
= c1 ·

t1 −22
3! t3 + 24
5! t5 −26
7! t7 + · · ·

= 1
2 c1 ·

2t −23
3! t3 + 25
5! t5 −27
7! t7 + · · ·

= c1y2(t),
where c1 is an arbitrary constant and y2(t) ≜1
2 sin 2t. Of course, we knew this already,
but it’s good to see that our new method gives a result that agrees with our old method
for solving ¨y + 4y = 0.
Putting all of the solutions together, we see that the general solution of the ODE ¨y +
4y = 0 is
y(t) =
∞

k=0
c2k t2k +
∞

k=0
c2k+1 t2k+1 = c0y1(t) + c1y2(t) = c0 · cos 2t + c1 · 1
2 sin 2t,
where the functions y1(t) and y2(t) were defined earlier. ⃝
Interestingly, the two solutions y1(t) and y2(t) are the basic solutions predicted theoreti-
cally by Theorem 3.9 in Section 3.3.
C.1.1 Convergence
First, we recall some definitions and results concerning
∞

n=0
an,
(C.5)
a series of real numbers.
Definition C.1
(a) The sequence of partial sums is
	
sN

∞
N=0, defined by
sN ≜
N
n=0
an.
(C.6)
(b) Series (C.5) converges if the sequence of partial sums converges.

Appendix C: Series Solutions of ODEs
1391
(c) Series (C.5) converges absolutely if the series
∞

n=0
|an|
converges.
(d) Series (C.5) diverges if the sequence of partial sums does not converge.
Theorem C.1
Suppose ∞
n=0 an and ∞
n=0bn are absolutely convergent series and ∞
n=0 hn is not
absolutely convergent. Then for all complex constants β and γ ,
(a) ∞
n=0(βan + γ bn) is absolutely convergent.
(b) ∞
n=0(βan + γ hn) is not absolutely convergent unless γ = 0.
Given a series ∞
k=1 ak, if possible, we calculate
L ≜lim
k→∞

ak+1
ak
 .
(C.7)
Theorem C.2
(a) If L < 1 exists, then series (C.5) converges absolutely.
(b) If L > 1 exists, then series (C.5) diverges.
Note that Theorem C.2 does not discuss what happens if L does not exist or if L = 1.
Definition C.2
(a) A finite number R is called the radius of convergence for power series
∞

n=0
cntn,
(C.8)
if it both (a) converges absolutely for |t −t0| < R and (b) diverges for |t −t0| > R.
(b) The radius of convergence of power series (C.8) is defined to be R = ∞, and
we say the radius of convergence is infinite, if the power series (C.8) converges
at all t.

1392
Appendix C: Series Solutions of ODEs
Given a power series (C.8), for any fixed value of t, we can define the corresponding
series of real numbers
∞

n=0
antn,
where an ≜cntn.
Theorem C.3
Suppose there exists
α ≜lim
k→∞

ck+1
ck
 .
(C.9)
(a) If 0 < α < ∞, then the power series (C.8) converges absolutely for |z−z0| < R ≜1
α
and diverges for |z −z0| > R, hence has radius of convergence equal to R.
(b) If α = 0, then the power series (C.8) converges at all z, hence has infinite radius of
convergence.
Example C.3
Find the radii of convergence of the two solutions of ¨y + 4y = 0 we found in C.2.
Method: The first solution is
y1(t) = c0
∞

k=0
Ck t2k = c0 · cos 2t,
where we define Ck ≜c2k/c1 = (−1)k 22k
(2k)!. We calculate
α ≜lim
k→∞
Ck+1 ÷ Ck
 = lim
k→∞
(−1)k+1
22(k+1)
(2(k + 1))! ÷

(−1)k 22k
(2k)!

= lim
k→∞
(−1)
22
(2k + 2)(2k + 1)
 = 0,
so y1(t) has radius of convergence R = ∞.
The second solution is
y2(t) = c1
∞

k=0
(−1)k
22k
(2k + 1)! t2k+1 = 1
2 c1 · sin 2t,
where we define Ck ≜= c2k+1/c1 = (−1)k
22k
(2k+1)!. We could use similar work to explain
why y2(t) has radius of convergence R = ∞. Instead, let us use the recursion relation to
find α:

Appendix C: Series Solutions of ODEs
1393
α ≜lim
k→∞
Ck+1 ÷ Ck
 = lim
k→∞
c2(k+1)+1 ÷ c2k+1
 = lim
k→∞
c(2k+1)+2 ÷ c2k+1

= lim
k→∞
−
4
(2k + 1 + 2)(2k + 1 + 1) c2k+1 ÷ c2k+1
 = lim
k→∞

4
(2k + 3)(2k + 2)
 = 0.
So, the radius of convergence of y2(t) is R = ∞. ⃝
C.1.2 Nonconstant Coefﬁcients ODEs
We know from Chapter 3 how to solve all constant coefficients scalar ODEs. The
power series method gives us a way to systematically look for solutions of nonconstant
coefficients ODEs.
To do that, it helps to become skilled at some manipulations of power series. The first
manipulation involves multiplying a power series by a polynomial.
Example C.4
If y(t) = ∞
n=0 cntn, rewrite (t + 2)˙y in terms of power series that uses tn in the
summation(s).
Method: Without rigorous analysis of convergence, we have
(t + 2)˙y = (t + 2) d
dt
⎡
⎣
∞

n=0
cntn
⎤
⎦= (t + 2)
∞

n=0
d
dt

cntn
= (t + 2)
∞

n=1
ncntn−1.
Note that in the sum, the term for n = 0 drops out because ncntn = 0 when n = 0.
Another way of seeing this is to note that the n = 0 term in y(t) = ∞
n=0 cntn is the
constant term c0t0 = c0, whose derivative with respect to t is zero.
Continuing, we calculate
(t + 2)˙y = t
∞

n=1
ncntn−1 + 2
∞

n=1
ncntn−1 =
∞

n=1
ncntn +
∞

n=1
2ncntn−1 .
The first term is a power series that uses tn in the summation, but the second is not. To
fix this, we use a shift of index by substituting m = n −1, that is, n = m + 1:
∞

n=1
2ncntn−1 =
∞

m=0
2(m + 1)cm+1tm,
after noting that m+1 = n = 1, 2, . . . corresponds to m = 0, 1, . . . . To get back to n-powers
rather m-powers, substitute n = m to get
∞

m=0
2(m + 1)cm+1tm =
∞

n=0
2(n + 1)cn+1tn.
Putting the two terms back together, we have
(t + 2)˙y =
∞

n=1
ncntn +
∞

n=0
2(n + 1)cn+1tn . ⃝
(C.10)

1394
Appendix C: Series Solutions of ODEs
When we solve an ODE in Example C.5 as follows, we will want to rewrite (C.10) in
terms of a single power series that starts at n = 1. To do that, we “peel off” the first term
from the second series in (C.10):
∞

n=0
2(n + 1)cn+1tn = 2(0 + 1)c0+1t0 +
∞

n=1
2(n + 1)cn+1tn.
So, (C.10) can be rewritten as
(t + 2)˙y =
∞

n=1
ncntn + 2c1 +
∞

n=1
2(n + 1)cn+1tn = 2c1 +
∞

n=1
(ncn + 2(n + 1)cn+1) tn .
Example C.5
Find a power series solution of the ODE (t + 2)˙y + y = 0.
Method:
Substitute the usual power series solution in the form (C.4), that is, y(t) =
∞
n=0 cntn, into the ODE and do the manipulations as in Example C.3:
0 = (t + 2)˙y + y = (t + 2) d
dt
⎡
⎣
∞

n=0
cntn
⎤
⎦+
∞

n=0
cntn
= t
∞

n=1
ncntn−1 + 2
∞

n=1
ncntn−1 +
∞

n=0
cntn
=
∞

n=1
ncntn +
∞

n=1
2ncntn−1 +
∞

n=0
cntn
=
∞

n=1
ncntn +
∞

n=0
2(n + 1)cn+1tn +
∞

n=0
cntn .
Recall that the second term was written in terms of powers tn rather than powers tn−1
by doing a change of index from n to m = n −1 and then back to n.
In order to get a recursion relation, we need to combine all of the series into one series.
In order to do that, we peel off all of the t0 terms to get
0 = (t + 2)˙y + y =
∞

n=1
ncntn + 2c1 +
∞

n=1
2(n + 1)cn+1tn + c0 +
∞

n=1
cntn .
So, the ODE is
0 = (t + 2)˙y + y = 2c1 + c0 +
∞

n=1

ncn + 2(n + 1)cn+1 + cn

tn .
Sorting by powers of t gives
0 = 2c1 + c0
(C.11)

Appendix C: Series Solutions of ODEs
1395
and
0 = ncn + 2(n + 1)cn+1 + cn,
for n = 1, 2, . . . .
The latter can be rewritten as
0 = (n + 1)cn + 2(n + 1)cn+1,
for n = 1, 2, . . . ;
hence,
cn+1 = −(n + 1)
2(n + 1) cn = −1
2 cn,
for n = 1, 2, . . . .
(C.12)
Combining (C.11) and (C.12), this situation is simple enough that we can guess the
solution: Because
c1 = −1
2 c0,
c2 = −2
2 c1 = (−1)2
22
c0,
and
c3 = −1
2 c2 = (−1)3
23
c0, . . . ,
we guess that
cn = (−1)n
2n
c0, n = 1, 2, . . . .
We can easily check that it satisfies the recursion relation. Thus, the solutions of the
ODE are
y(t) =
∞

n=0
cntn = c0t0 + c1t1 + c2t2 + c3t3 + · · · = c0 ·

1 + (−1)1
21
t1 + (−1)2
22
t2 + · · ·

= c0
∞

n=0
−t
2
n
= c0 ·
1
1 −(−t/2) = c0 ·
2
2 + t .
where c0 is an arbitrary constant. The solutions of (t + 2)˙y + y = 0 are
y(t) = c0y1(t),
where c0 is an arbitrary constant and y1(t) ≜
2
2+t. ⃝
In fact, this first-order ODE can be solved by the method of separation of variables and
gives the same conclusion. Note that using the recursion relation, we can calculate that the
radius of convergence of y1(t) is
R =

lim
k→∞
ck+1 ÷ ck

−1
=

lim
k→∞
−1
2 ck ÷ ck

−1
=

lim
k→∞
−1
2

−1
= 2.
This makes sense because y1(t) blows up at t = −2.

1396
Appendix C: Series Solutions of ODEs
Example C.6
Find a power series solution of the ODE (1 + x2)y′′ + 10xy′ + 20y = 0.
This problem is more complicated than the previous examples because the ODE is
second-order and it has nonconstant coefficients.
Method:
Substitute the usual power series solution in the form (C.3),
that is,
y(x) = ∞
n=0 cnxn, into the ODE and do manipulations similar to those in Example C.3
and afterward:
0 = (1 + x2)y′′ + 10xy′ + 20y =
⎛
⎝(1 + x2)
∞

n=2
cnn(n −1)xn−2
⎞
⎠
+ 10x
∞

n=1
cnnxn−1 + 20
∞

n=0
cnxn
=
⎛
⎝
∞

n=2
n(n −1)cnxn−2 +
∞

n=2
n(n −1)cnxn
⎞
⎠+
∞

n=1
10ncnxn +
∞

n=0
20cnxn
=
∞

n=0
(n + 2)(n + 1)cn+2xn +
∞

n=2
n(n −1)cnxn +
∞

n=1
10ncnxn +
∞

n=0
20cnxn
= 2c2 + 6c3x +
∞

n=2
(n + 2)(n + 1)cn+2xn +
∞

n=2
n(n −1)cnxn + 10c1x +
∞

n=2
10ncnxn
+ 20c0 + 20c1x +
∞

n=0
20cnxn
= 2c2 + 20c0 + (6c3+10c1+20c1) x
+
∞

n=2

(n + 2)(n + 1)cn+2 + n(n −1)cn + 10ncn + 20cn

xn .
Sorting by powers of x gives
0 = 2c2 + 20c0,
0 = 6c3+10c1+20c1 = 6c3 + 30c1,
(C.13)
and
0 = (n + 2)(n + 1)cn+2 + n(n −1)cn + 10ncn + 20cn, for n = 2, 3, . . . .
The latter can be rewritten as
cn+2 = −n(n −1) + 10n + 20
(n + 2)(n + 1)
cn = −(n + 5)(n + 4)
(n + 2)(n + 1) cn, for n = 2, 3, . . . .
(C.14)

Appendix C: Series Solutions of ODEs
1397
Combining (C.13) and (C.14), this situation is simple enough that we can guess a
solution: Because
c2 = −20
2 c0 = −5 · 4
2 · 1 c0
and
c4 = −7 · 6
4 · 3 c2 =

−7 · 6
4 · 3
 
−5 · 4
2 · 1 c0

= (−1)2 7 · 6 · 5 · 4
4 · 3 · 2 · 1 c0,
c6 = −9 · 8
6 · 5 c4 =

−9 · 8
6 · 5
 7 · 6 · 5 · 4
4 · 3 · 2 · 1 c0

= (−1)3
9 · 8 · 7 · 6 · 5 · 4
6 · 5 · 4 · 3 · 2 · 1

c0 ,
we guess that
c2k = (−1)k (2k + 3)!
3!(2k)!
c0 = (−1)k (2k + 3) · (2k + 2) · (2k + 1)
3 · 2 · 1
c0, k = 1, 2, . . . .
So, the ODE has solutions y(x) = c0y1(x), where c0 is an arbitrary constant and
y1(x) = 1
c0
∞

k=0
c2k x2k = 1
6
∞

k=0
(−1)k (2k + 3)(2k + 2)(2k + 1)x2k .
Similarly, the odd powers naturally group together to give
c3 = −30
6 c1 = −6 · 5
3 · 2 c1,
c5 = −8 · 7
5 · 4 c3 =

−8 · 7
5 · 4

f
6 · 5
3 · 2 c1

= (−1)2 8 · 7 · 6 · 5
5 · 4 · 3 · 2 c1,
and
c7 = −10 · 9
7 · 6 c5 =

−10 · 9
7 · 6
 8 · 7 · 6 · 5
5 · 4 · 3 · 2 c0

= (−1)3
10 · 9 · 8 · 7 · 6 · 5
7 · 6 · 5 · 4 · 3 · 2

c0.
We guess that
c2k+1 = (−1)k
(2k + 4)!
4!(2k + 1)! c1 = (−1)k (2k + 4) · (2k + 3) · (2k + 2)
4 · 3 · 2 · 1
c1, k = 1, 2, . . . .
So, the ODE has solutions y(x) = c1y2(x), where c1 is an arbitrary constant and
y2(x) = 1
c1
∞

k=0
c2k+1 x2k+1 = 1
24
∞

k=0
(−1)k (2k + 4)(2k + 3)(2k + 2)x2k.
The general solution of the ODE is
y(x) =
∞

k=0
c2k x2k +
∞

k=0
c2k+1 x2k+1 = c0y1(x) + c1y2(x),
where the functions y1(x) and y2(x) were defined earlier in terms of power series.
C.1.3 Power Series in (x −a)
It may make sense to take the solution form to be a power series in powers of (x−a) rather
than powers of (x −0), that is, to assume
y(x) =
∞

n=0
cn(x −a)n.

1398
Appendix C: Series Solutions of ODEs
C.2 Method of Frobenius
The method of Frobenius is a modification of the power series solution method that turns
out to be very useful for classical special functions, such as Bessel functions, used in
mathematical physics.
In a sense, the method is a generalization of the method we used in Section 3.5 to solve
Cauchy–Euler ODEs
x2y′′ + p0xy′ + q0y = 0, where ′ = d
dx and p0, q0 are constants,
in Section 3.5. Recall that we substituted in a desired solution in the form y(x) = xn, a
single power of x, and we got a characteristic equation n(n −1) + p0n + q0 = 0, assuming
p0 and q0 are constants.
Definition C.3
A function p(x) is real analytic on an open interval −δ < x < δ if there is a power series
whose radius of convergence is greater than or equal to δ, for which p(x) equals that power
series for all x in the interval −δ < x < δ.
Definition C.4
An ODE of the form
x2y′′ + xp(x)y′ + q(x)y = 0
(C.15)
has x = 0 as a regular singular point if p(x) and q(x) are real analytic on an open interval
−δ < x < δ.
The method of Frobenius consists of substituting a solution in the form
y(x) = xn
∞

m=0
cm xm
(C.16)
into ODE (C.15).
Expand p(x) and q(x) in power series:
p(x) =
∞

m=0
pm xm,
and
q(x) =
∞

m=0
qm xm.

Appendix C: Series Solutions of ODEs
1399
Substitute (C.16), that is, y(x) = ∞
m=0 cm xm+n, into ODE (C.15) to get
0 = x2 ·
∞

m=0
(m + n)(m + n −1)cm xm+n−2
+

p0 + p1x + p2x2 + · · ·

· x ·
∞

m=0
(m + n)cm xm+n−1
+

q0 + q1x + q2x2 + · · ·

·
∞

m=0
cm xm+n
=
∞

m=0
(m + n)(m + n −1)cm xm+n + p0
∞

m=0
(m + n)cm xm+n
+ p1x ·
∞

m=0
(m + n)cm xm+n + · · · + q0
∞

m=0
cm xm+n + q1x ·
∞

m=0
cm xm+n + · · · .
Sort by powers of x and isolate the x0+n terms, which correspond to m = 0, to get
0 = xn ·

(0 + n)(0 + n −1)c0 + p0(0 + n)c0 + q0c0

= c0xn ·

n(n −1) + np0 + q0

.
This implies that n, the power of x that multiplies the power series, must satisfy the
indicial equation
n(n −1) + np0 + q0 = 0.
(C.17)
This is a generalization of the characteristic equation for Cauchy–Euler equations. Why?
Because if p(x) ≡p0 and q(x) ≡q0 are constants, then ODE (C.15) is a Cauchy–Euler
equation whose characteristic equation is (C.17).
Note that
p0 = lim
x→0 p(x)
and
q0 = lim
x→0 q(x) ,
(C.18)
because we assumed p(x) and q(x) are real analytic on −δ < x < δ.
Theorem C.4
Suppose x = 0 is a regular singular point for ODE (C.15), where p(x) and q(x) are real
analytic on interval −δ < x < δ and p0, q0 are as in (C.18). Suppose n1 and n2 are the two
solutions of indicial equation (C.17), with Re(n1) ≥Re(n2). Then ODE (C.15) has a basic
solution in the form
y1(x) = |x|n1
∞

m=0
cmxm
that is convergent for 0 < |x| < δ and has c0 = 1.

1400
Appendix C: Series Solutions of ODEs
In addition, there is a second basic solution y2(x) that is convergent for 0 < |x| < δ:
(a) If n1 −n2 is not an integer, then y2(x) is in the form
y2(x) = |x|n2
∞

m=0
cmxm,
wherec0 = 1.
(b) If n1 = n2, which we call the first exceptional case, then y2(x) is in the form
y2(x) = |x|n1+1
∞

m=0
ˆcmxm + (ln |x|)y1(x),
where ˆc0 ̸= 0.
(c) If n1 −n2 is a nonzero integer, which we call the second exceptional case, then
y2(x) is in the form
y2(x) = |x|n2
∞

m=0
˘cmxm + c(ln |x|)y1(x),
where ˘c0 ̸= 0 and c is a constant, possibly equal to zero.
C.2.1 Bessel’s Equation of Order α
The ODE
x2y′′ + xy′ + (x2 −α2)y = 0
(C.19)
is called Bessel’s equation of order α, if α is a nonnegative real constant.
Example C.7
For Bessel’s equation of order one, find a power series solution involving only positive
powers.
Method: The ODE is
x2y′′ + xy′ + (x2 −1)y = 0,
(C.20)
and it has a regular singular point at x = 0. The indicial equation is
0 = n(n −1) −n −1 = n2 −1,
so the method of Frobenius says there is a solution of the form
y(x) = x1 ·
∞

m=0
cm xm =
∞

m=0
cm xm+1, for 0 < x < δ,
(C.21)

Appendix C: Series Solutions of ODEs
1401
for some δ > 0. Substitute this into ODE (C.20) to get
0 = x2 ·
∞

m=1
(m + 1)mcm xm−1 + x ·
∞

m=0
(m + 1)cm xm + (x2 −1) ·
∞

m=0
cm xm+1
=
∞

m=1
(m + 1)mcm xm+1 +
∞

m=0
(m + 1)cm xm+1 +
∞

m=0
cm xm+3 −
∞

m=0
cm xm+1 .
Because so many of the series are in terms of powers xm+1, it makes sense to get all of
the series in terms of powers xm+1 instead of powers xm. This gives us
0 =
∞

m=1
(m + 1)mcm xm+1 +
∞

m=0
(m + 1)cm xm+1 +
∞

m=2
cm−2 xm+1 −
∞

m=0
cm xm+1 .
Among the four series, the highest first power is x2+1, that is, xm+1 for m = 2. So, we
peel off x0+1 and x1+1 powers so that all of the series will start with the index m = 2:
0 = 2c1x2 +
∞

m=2
(m + 1)mcm xm+1 + c0x + 2c1x2 +
∞

m=2
(m + 1)cm xm+1 +
∞

m=2
cm−2 xm+1
−c0x −c1x2 −
∞

m=2
cm xm+1
= (c0 −c0)x + (2c1 + 2c1 −c1)x2 +
∞

m=2

(m + 1)mcm + (m + 1)cm + cm−2 −cm

xm+1
= 3c1x2 +
∞

m=2

(m + 2)mcm + cm−2

xm+1 .
Sorting by powers of x gives
c1 = 0
(C.22)
and
(m + 2)mcm + cm−2 = 0 for m = 2, 3, . . . .
The latter can be rewritten as
cm+2 = −
1
(m + 4)(m + 2) cm, for m = 2, 3, . . . .
(C.23)
Combining (C.22) and (C.23), we can guess the solution: Note first that
c2 = −1
4 · 2 c0 = −
1
2 · 2 · (2 · 1) c0
and
c3 = −1
5 · 3 c1 = 0.
Using (C.23), we see that all of the odd-indexed coefficients are zero, and the even-
numbered coefficients are
c4 = −1
6 · 4 c2 = (−1)2
1
6 · 4 ·
1
2 · 2 · (2 · 1) c0 = (−1)2
1
2 · 2 · (3 · 2) · 2 · 2 · (2 · 1) c0
= (−1)2 1
24 ·
1
3!2! c0 .

1402
Appendix C: Series Solutions of ODEs
Next, we have
c6 = −1
8 · 6 c4 = −
1
22 · 4 · 3 (−1)2 1
24 ·
1
3!2! c0 = (−1)3 1
26 ·
1
4!3! c0 .
We guess that
c2k =
(−1)k
22k(k + 1)!k!.
So, the ODE has solution y(x) = c0y1(x), where c0 is an arbitrary constant and
y1(x) = x ·
∞

k=0
(−1)k
k!(k + 1)!
x
2
2k
. ⃝
The Bessel function of the first kind and order one is defined by
J1(x) ≜x
2 ·
∞

k=0
(−1)k
k!(k + 1)!
x
2
2k
= 1
2 y1(x).
More generally, for any nonnegative real number α, the Bessel function of the first kind
and order α is defined by
Jα(x) ≜
x
2
α
·
∞

k=0
(−1)k
k!(k + α + 1)
x
2
2k
.
Using the method of Frobenius, Jα(x) is derived to be the first solution, y1(x), of Bessel’s
equation of order α, that is,
x2y′′ + xy′ + (x2 −α2)y = 0,
(C.24)
corresponding to the greater root n = α of the indicial equation n(n+1)−n−α2 = 0. Note
that the gamma function is defined by
(z) ≜
∞

0
e−xxz−1 dx
(C.25)
and satisfies (ℓ+ 1) = ℓ! for nonnegative integers ℓ.
The second solution of Bessel’s equation of order α is notated Yα(x). If α is a nonnegative
integer, we have the first exceptional case (when α = 0) or the second exceptional case of
Theorem C.4. It turns out that
lim
x→0+ |Yα(x)| = ∞
when α is a nonnegative integer.

Appendix C: Series Solutions of ODEs
1403
The modified Bessel equation of order α is
x2y′′ + xy′ −(x2 + α2)y = 0
(C.26)
and has solutions Iα(x) and Kα(x), called modified Bessel functions of order α of the
first and second kind, respectively. They have the useful properties that Iα(x) > 0 for all
x > 0 and limx→0+ |Kα(x)| = ∞.
C.3 Polynomial Solutions of ODEs
Sometimes when we substitute a power series solution into an ODE, the recursion relation
gives us a solution where all of the coefficients of the power series are zero past a certain
index. Such a solution is a polynomial. It turns out that such polynomials are very useful
classical special functions, such as Legendre polynomials, used in mathematical physics.
Example C.8
For Legendre’s equation
(1 −x2)y′′ −2xy′ + ν(ν + 1)y = 0,
(C.27)
where ν is a constant parameter, along with boundary conditions
|y(−1+)| < ∞, |y(1−)| < ∞,
(C.28)
find the five polynomial solutions of lowest degree.
Method: Substitute the usual power series (C.3), that is,
y(x) =
∞

n=0
cnxn,
into the ODE to get
0 = (1 −x2)
∞

n=2
n(n −1)cnxn−2 −2x
∞

n=1
ncnxn−1 + ν(ν + 1)
∞

n=0
cnxn
=
∞

n=2
n(n −1)cnxn−2 −
∞

n=2
n(n −1)cnxn −
∞

n=1
2ncnxn +
∞

n=0
ν(ν + 1)cnxn
=
∞

n=0
(n + 2)(n + 1)cn+2xn −
∞

n=2
n(n −1)cnxn −
∞

n=1
2ncnxn +
∞

n=0
ν(ν + 1)cnxn
= 2c2 + 6c3x +
∞

n=2
(n + 2)(n + 1)cn+2xn −
∞

n=2
n(n −1)cnxn −2c1x −
∞

n=2
2ncnxn
+ ν(ν + 1)c0 + ν(ν + 1)c1x +
∞

n=2
ν(ν + 1)cnxn .

1404
Appendix C: Series Solutions of ODEs
Sorting by powers of x gives
0 = 2c2 + ν(ν + 1)c0,
0 = 6c3 −2c1 + ν(ν + 1)c1
(C.29)
and
0 = (n + 2)(n + 1)cn+2 −n(n −1)cn −2ncn + ν(ν + 1)cn for n = 2, 3, . . . .
The latter can be rewritten as the recursion relation
cn+2 = n(n + 1) −ν(ν + 1)
(n + 2)(n + 1)
cn, for n = 0, 1, . . . .
(C.30)
For any positive integer n, when ν = n, the recursion relation shows that c2k+2 = 0 for
all k ≥n. We have even polynomials
For ν = 0,
y(x) = c0.
For ν = 2,
y(x) = c0 + c2x2 = c0

1 −3x2
.
For ν = 4,
y(x) = c0 + c2x2 = c0

1 −10x2 + 70
6 x4

.
and odd polynomials
For ν = 1,
y(x) = c1.
For ν = 3,
y(x) = c1x + c3x3 = c1

x −5
3 x3

. ⃝
The corresponding Legendre polynomials Pn(x) are constant multiples of these polyno-
mials “normalized” to have Pn(1) = 1:
For ν = 0,
P0(x) ≡1.
For ν = 1,
P1(x) = x.
For ν = 2,
P2(x) = −1
2

1 −3x2
.
For ν = 3,
P3(x) = −3
2

x −5
3 x3

.
For ν = 4,
P4(x) = 3
8

1 −10x2 + 35
3 x4

.
Learn More about It
The Handbook of Mathematical Functions, edited by Milton Abramowitz and Irene
Stegun, Dover Publications c⃝1964, is a good reference for all special functions, such
as Bessel functions of all kinds and Legendre polynomials. An Introduction to Ordinary
Differential Equations, by Earl A. Coddington, Prentice-Hall c⃝1961, and Intermedi-
ate Differential Equations, 2nd edition, by Earl D. Rainville, The Macmillan Co. c⃝1964,
are very good textbooks for learning about special functions, including the explanation
for Theorem C.4 on the method of Frobenius. The Theory of Bessel Functions, by G. N.
Watson, Cambridge University Press c⃝1922 (reprinted 1996), is a standard reference
for all things concerning those functions.

Appendix C: Series Solutions of ODEs
1405
C.3.1 Problems
In problems 1–4, find the recursion relation when a power series about x = 0 is used to
solve
1. (2x2 + 3)y′′ −5xy′ + 7y = 0
2. (x2 + 1)y′′ + 5xy′ −xy = 0
3. (x2 + 3)y′ + y = 0
4. Bessel’s equation of order α, where α is an unspecified nonnegative number
In problems 5–9, solve the IVP.
5. (x2 + 1)y′′ + 6y′ + 6y = 0, y(0) = −1, y′(0) = 0
6. (1 −x2)y′′ −2xy′ + 20y = 0, y(0) = −2, y′(0) = 0 [Hint: Use the initial conditions
to find c0, c1 early in your work.]
7. (x2 + 1)y′′ + 6y′ + 6y = 0, y(0) = 0, y′(0) = 1
8. (x2 + 1)y′′ −3y + 3y = 0, y(0) = 0, y′(0) = 7
9. (x2 + 1)y′′ −26y = 0, y(0) = 0, y′(0) = 1
10. Express the general solution of y′′+xy′+2y = 0 in the form y(x) = c0z0(x)+c1z1(x).
Write down formulas for the functions z0(x) and z1(x).
In problems 11 and 12, find polynomial approximations of two linearly independent
solutions z0(x) and z1(x). Use all powers of x up to and including x8.
11. (1 + 2x2)y′′ + 6xy′ + 2y = 0
12. (x2 + 1)y′′ + 6xy′ + 4y = 0
13. Solve (x2 + 5)y′′ + xy′ −y = 0. State clearly which constants are arbitrary. State the
interval of validity of the solution.
14. Solve (x2 + 4)y′′ + 6xy′ + 4y = 0. State clearly which constants are arbitrary. State
the interval of validity of the solution.
In problems 15–17, find a complete set of basic solutions of the ODE and find the radius of
convergence of your choice of one of those solutions.
15. (x2 + 1)y′′ + 6xy′ −6y = 0
16. x2y′′ −2y = 0
17. Bessel’s equation of order two
18. Suppose that ψ(z) ≜Jn(z) satisfies ODE z d
dz(z dψ
dz ) + (z2 −n2)ψ(z) = 0, z = ex, and
y(x) ≜ψ(ex). What ODE does y(x) satisfy?


Index
A
Abel’s theorem, 238, 332–333, 371, 426
Adams–Bashforth method, 705
Adams–Moulton formula, 706
Adaptive step size, 708
Adjugate matrix, 53–54, 79–82
Aitken’s δ2 method, 656–657
Algebraic inverse, 183
Algebraic multiplicity, 81, 132, 138–139, 387
Ampère—Maxwell’s law, 840
Ampère’s circuital law, 566
Amplification factor, 1003
Amplitude, 232–235, 268–269, 275, 277–278, 284
Analog-to-digital converter (A/D or ADC), 340
Angle of attack, 1257
Annihilator method, 93
Anti-derivative, 551–552
Anti-partial derivative, 572, 601
Areal coordinates, 731–732
Augmented matrix, 5–6, 10, 40–41, 63
B
Banach contraction mapping theorem, 666
Banach spaces, 666
Barycentric coordinates, 732
Beats phenomenon, 274, 283–287
Bendixson–Dulac theorem, 1352
Bessel function of the first kind and
order one, 1402
Bessel’s equation of order α, 1400–1403
Bessel’s equation of order zero, 714
Bi-harmonic operator, 1299
Block matrices, 43, 153–154
Boole’s rule, 671–673
Bounded as t →∞, 202–203
Bounded function, 202
Bounded linear functionals and operator
adjoint, 186–187
Brachistochrone problem, 1083–1085
Bromwich contour, 1285, 1287
B-spline curve, 729–731
Butterfly effect, 1364
BVP, see Two-point boundary value
problem (BVP)
C
Calculus of variations
constraints
differential equation, 1103–1105
integral, 1100
isoperimetric problem, 1100
Lagrange multiplier (λ), 1101–1103
eigenvalue problems
Laplacian, 1108–1109
ODE-BVP, 1106–1108
Rayleigh–Ritz method, 1106
Sturm–Liouville problem, 1109–1110
Euler–Lagrange equations
Dirichlet boundary condition, 1093
divergence theorem, 1093
Fourier series, 1090
linear approximation, 1090
variation, 1089–1090
variational method, 1091
finite element methods (see Finite element
methods)
Hamilton’s principle
continuous media, 1097
Hamilton’s equations, 1095
Lagrangian, 1095
pendulum, 1095
total action during time interval, 1096
MacLaurin series, 1088
minimization
admissible function, 1081
brachistochrone problem, 1083–1085
functional, 1081
natural boundary conditions, 1082
nonlinear ODE-BVP, 1083
potential energy, 1082–1083
Rayleigh–Ritz method, 1085–1087
natural boundary condition, 1082, 1094–1095
stationarity, 1088
Camber line, 1255
Camber ratio, 1255, 1257–1258
Casorati determinant, 425–427
forward difference operator, 330
higher order difference operators, 331
n-th order linear homogeneous difference
equation, 332
Wronskian determinant, 331
Cauchy complete, 183–185
Cauchy equation, 845
1407

1408
Index
Cauchy–Euler ODEs
boundary conditions, 259
characteristic polynomial, 255–258
standard form, 255
Cauchy–Goursat integral theorem, 1200–1204,
1278
Cauchy principal value, 1218–1219
Cauchy–Riemann equations, 1140–1143, 1256
Cauchy–Schwarz inequality, 103–105, 111–112
Cauchy’s residue theorem, 1212
Cayley–Hamilton theorem, 415–417
CDF, see Cumulative distribution function
(CDF)
Central difference operator, 696
Characteristic equation, 76, 78, 227, 245, 255, 258
Characteristic multipliers, 442, 444
Characteristic polynomials, 76, 227, 326–327, 397
Cholesky factorization, 168–169
Chord length, 1257
Clairaut’s theorem, 212, 485, 583
Closed form solution, 204, 297
Closed subspace, 185
Collinear, 1234
Collocation method, 726
Column space, 115
Compartment models, 358–360
Complementary error function (erfc), 1281
Complex eigenvalues, 83–84, 381–385, 427–428
Complex inner product space, 176
Complex integration methods
Cauchy principal value, 1218–1219
Hilbert transform, 1219–1220
improper integrals, 1215–1217
integration of periodic functions, 1214–1215
Complex numbers, 82–83, 1171
Complex variable, 1130–1132
analytic function, 1148
Cauchy–Goursat integral theorem,
1200–1204
Cauchy–Riemann equations, 1140–1143
Cauchy’s integral formulas
Dumb-bell contour, 1207–1209
Laurent series integration, 1210–1212
partial fractions, 1206
Cauchy’s residue theorem, 1212
closed contour integration, 1198–1200
complex conjugate, 1123
complex integration methods
Cauchy principal value, 1218–1219
Hilbert transform, 1219–1220
improper integrals, 1215–1217
integration of periodic functions,
1214–1215
complex plane, 1194
continuous at z0, 1137
contour, 1194–1195
converges absolutely, 1175
converges uniformly, 1177
derivatives, 1138–1139
diverges, 1175
elementary functions
arg(z), 1157–1159
branches of logarithms, 1164
exp(z), 1159–1160
log(z), 1160–1164
power functions, 1164–1169
entire function, 1149
geometric series, 1175
harmonic conjugate, 1152–1155
harmonic functions, 1150–1152
holomorphic, 1148
imaginary part, 1123
initial point and terminal point, 1194
Laurent series
domains, 1183
geometric series, 1180
Maclaurin series, 1184–1185
open annulus, 1180–1182
power series, 1179
removable singularity, 1184
L’Hôpital’s rule, 1150
modulus/absolute value, 1123
open disk, 1135
orthogonal families, 1143–1144
parametrized curve, 1194, 1198
polar form, complex number
DeMoivre’s theorem, 1125
Euler’s formula, 1124
exponential form, 1125
polar coordinates, 1124
polygonal curve approximation, 1195
polynomial in z of degree, 1138
positively oriented, 1194–1195
power series about z0, 1175
punctured disk, 1135
radius of convergence, 1177
Riemann sum, 1196
roots, 1127–1130
streamlined version, 1197
Taylor series
Cauchy’s integral formula, 1178
open disk D, 1179
product of, 1185
real analytic, 1179
trigonometric functions
complex numbers, 1171

Index
1409
Euler’s formula, 1171
set union, 1174
zeros and poles
singularities, 1188–1193
zero of order, 1186–1188
Conformal mappings
elementary functions, 1227
harmonic functions, 1226–1227
Laplace’s equation
boundary conditions, 1247
boundary values, circles,
1248–1251
Dirichlet boundary condition, 1246
electric field lines, 1248
Joukowsky map, 1251–1253
Neumann boundary condition, 1246
open mapping theorem, 1246
Zhukovskii airfoils (see Zhukovskii
airfoils)
linear mapping
boundary point, 1224, 1226
definition, 1223
domain D, 1224, 1226
magnification, 1223, 1225
punctured line, 1232
rotation, 1223–1225
tangent vectors, 1223, 1226
translation, 1223, 1225
Möbius transformations
circles, 1230–1232
concentric circles, 1233–1240
cross-ratio method, 1240–1242
definition, 1228
distinct points, 1240
inversion mapping, 1230
lines, 1232–1233, 1241–1243
orientations, 1242
test value, 1243
Conjugate gradient method, 1046
Constant coefficient matrix, 355
Contact resistance, 854
Continuum mechanics
body forces, 844
Cauchy equation, 845
conservation of mass, 842–843
contact force, 843
displacement/deformation, 843
divergence theorem, 844–845
Jacobian determinant, 842
Lagrangian description, 840–841
reference state, 840
rigid rotation, 841–842
stress tensor, 843
stress/traction vector, 843
total linear momentum, 843
Contour plot, 502, 940–945
Convergence acceleration technique, 656
Converges, 1390
Convolution, 348, 1268
definition, 312
Laplace transform properties, 312
particular solution, 314–318
properties, 312, 314
reality check, 313
Convolution on the real line, 643
Coriolis acceleration, 503, 542–543
Correlation coefficient, 105
Countably infinite, 1284
Courant–Friedrichs–Lewy (CFL) criterion, 1017
Cox–de Boor recursive, 728
Cramer’s rule, 54–56
Crank–Nicholson method, 1005–1006
Critically damped case
homogeneous ordinary differential
equations, 225–228, 237, 256
second-order, linear, constant coefficients
homogeneous ordinary differential
equations (LCCHODEs), 237
Cross ratio, 1240–1242
Cubic basis polynomials, 732
Cubic B-splines
blending function, 723
chain rule, 724
collocation method, 726
cubic spline uniform basis functions, 723
horizontal translation, 723
linear combination, 724
ODE-BVP, 725–726
solid curve, 726–727
Cubic spline uniform basis functions, 723, 1115
Cumulative distribution function (CDF),
636–637
Cyclotron motion, 463
D
D’Alembert wave solutions, 1014
real line, 882–883
step function, 888
support (x), 888–889
zero initial displacement
intersection symbol, 891
interval cases, 891–893
IVP, 889–890

1410
Index
piecewise continuous function, 890–891
transitional cases, 893–895
zero initial velocity
chain rule, 883–884
decomposition, 884–885
graphical method, 884
moving interval, 885–886
overlap, 886–887
Dantzig’s simplex procedure
pivoting process, 1055–1056
unit cost reduction
basic feasible solution, 1057–1058
Mathematica command, 1060
matrix-tableau form, 1057
maximum unit reduced cost, 1058
minimum reduction, 1056–1058
row operations, 1058
swapping variable, 1059
wiggle room, 1059
Decay constant, 219
Definite integral, 400–401, 549–551, 639, 672
Deflation of polynomials/deflation, 690
Delta functions, 318–320, 787, 1385
DeMoivre’s theorem, 1125
Density function, 559, 595, 597, 636
Determinant, 48–52, 77, 141–143
De-tuning parameter, 280
Diagonal matrix, 21–22, 85–86, 95–98, 132
Differential equation constraints, 1103–1105
Differential operator, higher-order linear
ordinary differential equations (ODE),
251–253
Directional derivative, 488–489
Direct methods, 682
Dirichlet boundary condition, 1093, 1108, 1246
Dirichlet problem, 864–865, 1249–1251
Discrete convolution, 798, 1185
Discrete Fourier transform (DFT)
aliasing property, 794–796
analysis equation, 791
digital filter, 792
discrete convolution, 798–799
Euler’s formula, 794–795
fast Fourier transform
discrete convolution, 802
divide and conquer strategy, 801–802
even integer, 800–801
indirect method, 802
odd integer, 800–801
Hermitian conjugate, 792
MathematicaTM command and MATLAB®,
796–798
N-th root of unity, 791
power spectrum, 796
Riemann sum, 790
symmetric matrix, 791–792
time series, 791
unitary matrix, 792–793
Discretization matrix, 993, 1002
Discretized eigenvalue problem, 715
Dispersion equation, 1015
Divergence theorem, 621–624
Diverges, 1391
Dot product, 101, 457–458
Double Fourier series, 951
Double integral
anti-partial derivative, 572
double Riemann sum, 571
element of area, 572
Fubini’s theorem, 574
iterated integral, 572
mass density, 571, 573
polar coordinates, 580–582
volume
average value of a function, 580
double Riemann sum, 575
horizontal line segment, 579
mass density, 577
solid tetrahedron, 577–578
triple integrals, 575
type II region, 579
vertical line segment, 576–577, 579
Double Riemann sum, 571, 575
2D stokes potential fluid, 1155
Dual basis, 1022
Dumb-bell contour, 1207–1209
E
Eccentric circles, 1233
Eigenspaces, 91–92
Eigenvalues and eigenvectors
adjugate matrix method, 79–82
annihilator method, 93
characteristic equation, 76, 78
characteristic polynomial, 76
column operation, 77
complex conjugate pairs, 83–84, 94
complex numbers, 82–83
deficient eigenvalues, 98
determinant, 77
eigenspaces, 90–92
elementary row operations, 77
geometry, 75–76

Index
1411
linear homogenous systems, 362
MATLAB® and MathematicaTM, 86–87
nonresonant nonhomogenous systems, 362
triangular and diagonal matrices, 85–86
trivial solution, 79
Electric field lines, 1248
Electrostatic potential difference, 566
Elementary column operations, 52
Elementary functions, complex variable
arg(z), 1157–1159
branches of logarithms, 1164
exp(z), 1159–1160
log(z), 1160–1164
power functions, 1164–1169
Elementary matrices, 24–25
Elementary row operations, 3, 77
Empty set, 892, 894
Energy density function, 881
Envelope, solution graph, 235
Equivalent system, 1
Error tolerance, 183–184
Euclidean norm, 664
Euler–Lagrange equations
Dirichlet boundary condition, 1093
divergence theorem, 1093
Fourier series, 1090
linear approximation, 1090
variation, 1089–1090
variational method, 1090
Euler’s conservation equation, 624–627
Euler’s formula, 230–231, 1171
Even function, 744–747, 756
Exactness criterion, 212–214, 492
Exact ordinary differential equation (ODE)
Clairaut’s theorem, 212
definition, 211
exactness criterion, 212–214
explicit solution, 214
partial derivatives, 211–212
verification, 211
Explicit solution, exact ordinary differential
equation (ODE), 214
Exponential order, 1286, 1379
F
False position method, 654
Fast Fourier transform
discrete convolution, 802
divide and conquer strategy, 801–802
even integer, 800–801
indirect method, 802
odd integer, 800–801
Fick’s law of diffusion, 523
Finite difference methods
heat equations
boundary conditions, 996
central difference, 991
Dirichlet boundary conditions, 997
finite difference stencil, 992
forward difference, 991
Neumann/Robin boundary
conditions, 997
nonlinearity, 998–1000
PDE-BVP-IVP, 992–996
stencil/replacement equations, 992
time-dependent boundary conditions, 997
potential equations
boundary conditions, 1010–1011
central difference approximations, 1007
finite differences approximate solution,
1008–1010
five-point Laplacian stencil, 1008
replacement equations, 1007
wave equations
D’Alembert wave solutions, 1014
Lax scheme, 1016–1017
scalar hyperbolic problem, 1014–1016
wave speeds, 1014
Finite element methods
Galerkin method, 1116–1117
least squares, 1117–1118
linear differential operator, 1112
Mathematica commands, 1112–1115
PDEs, 1118
Rayleigh–Ritz method, 1111–1112, 1117
trial functions, 1111
uniform tent basis spline functions, 1111
Finite jump discontinuity, 740
First exceptional case, 1400
Five-point Laplacian stencil, 1008
Fixed end boundary condition, 1094
Floquet representation, 442–445
Forced oscillations
beats phenomenon, 283–287
examples, 273–274
forced spring-mass-damper
system, 273–274
maximum frequency response, 281–283
resonance case, 274–276
steady-state oscillation, 276–281
Forced spring-mass-damper system, 273–274
Forward difference operator, 331, 695
Forward Euler scheme, 1015–1016

1412
Index
Fourier cosine series
arbitrary function f(t), 762–763
coefficients, 756
convergence, 757–758
even function, 756–757
finite sum y(t), 764–765
IVP, 765–767
ODE, 763–764
steady-state solution, 765
Fourier cosine transform, 1269, 1272–1278
Fourier series, 1090
complex Fourier series, 778–780, 788–789
convolution
definition, 785–786
delta function, 787–788
nonzero constants, 787
ramp function, 786
cosine series
arbitrary function f(t), 762–763
coefficients, 756
convergence, 757–758
even function, 756–757
finite sum y(t), 764–765
IVP, 765–767
ODE, 763–764
steady-state solution, 765
DFT (see Discrete Fourier transform (DFT))
Fourier transform
absolutely integrable, 780–781
finite pulse, 783–784
finite wave train, 784–785
inversion theorem, 781–782, 789
Laplace transform operator, 783
non-rigorous way, 782–783
principal value, 782
Riemann sum, 782
generalized series expansion
characteristic equation, 769
coefficients bn, 770
eigenfunction, 768, 771
eigenvalue, 768–769, 771
equilibrium temperature distribution,
772–773
graphical method, 774–775
ODE-BVP, 765, 773–774
periodic boundary condition, 775–776
square integrability condition, 769
orthogonality and coefficients (see Fourier
series coefficients)
Parseval’s theorem (see Parseval’s theorem)
Rayleigh quotient
first integral, 818
linear function X(x), 822
quadratic functionals, 820
regular Sturm–Liouville problem, 819–820
second integral, 819
single trial function X(x), 820–822
separation of variables, 755
sine series
arbitrary function f(t), 762–763
coefficients, 758
convergence, 759–760
finite sum y(t), 764–765
IVP, 763–767
least period, 760–761
nonzero terms, 761–762
odd function, 758–759
ODE, 763–764
steady-state solution, 765
Sturm–Liouville problems (see
Sturm–Liouville problems)
Fourier series coefficients
average value, 743
complex Fourier series, 751–753
convergence of, 739–741
even and odd function, 744–747
extend periodically, 748
Fourier expansion, 742
Fourier representation of, 752–753
function graph, 749–751
harmonic components, 737
inner product function, 741–743
integral calculation, 748
interval of length, 754–755
MathematicaTM command, 738–739
nice function, 743–744
orthogonality relations, 741–742
partial sums, 738–739
periodic extension, 748–749
span vector, 741
trigonometric function identity, 753–754
Fourier sine series
arbitrary function f(t), 762–763
coefficients, 758
convergence, 759–760
finite sum y(t), 764–765
IVP, 765–767
least period, 760–761
nonzero terms, 761–762
odd function, 758–759
ODE, 763–764
steady-state solution, 765
Fourier sine transform, 1272–1278
Fourier’s law of heat conduction, 523, 846
Fourier transform
absolutely integrable, 780–781

Index
1413
convolution, 1268
definition, 1263
F(ω), 1267
finite pulse, 783–784
finite wave train, 784–785
inversion theorem, 781–782, 789
Laplace transform operator, 783
non-rigorous way, 782–783
operator symbols, 1263–1264
PDEs
first-order ODE, exponential decay, 1271
Fourier cosine and sine transforms,
1272–1278
heat kernel, 1272
Parseval’s theorem, 1279
separation of variables, 1270
spatial variable, 1270
principal value, 782
rectangular contour, 1266
residues and real integral method, 1264
Riemann sum, 782
semicircular contours, 1264–1265
Fréchet derivative, 666
Free end boundary condition, 1094
Frequency response, 232, 279, 281–283
Frobenius method
Bessel’s equation of order α, 1400–1402
definition, 1398–1400
Frobenius norm, 181, 679
F(s) →0 uniformly on that family, 1284
Fubini’s theorem, 574
Fully plastic moment, 1054
Fundamental theorem of calculus, 552
Fundamental theorem of line integrals, 562–564
G
Galerkin method, 1116–1117
coercive, 1019–1020
differential operator, 1020–1021
Euclidean inner product, 1022
N-dimensional vector, 1019
nonlinear problems, 1023–1024
ODE-BVP, 1018–1019
PDE, 1022–1023
space of functions, 1018
Gaussian distribution, 641
Gaussian elimination methods, 6, 9, 679
Gaussian error function, 642
Gauss–Jordan method, 7, 11–12, 124
Gauss–Seidel iteration, 683–684
Gauss’s theorem, 621
Generalized eigenvalue problem, 141
Generalized eigenvector, 389
Generalized normal equations, 147, 1047
Geometrical calculus
arctan, 464
area of the triangle, 505
change of variables, area, and volume,
516–519
contour plot, 502
Coriolis acceleration, 503
cross product, 457–458
curves
ellipse in the plane, 472–473
figure eight curve, 474–475
line segment, 471
ODE system, 476–477
parametrized, 470
Pythagorean, 472
tangent vector, 474–475
vector field, 476
Zhukovskii airfoil, 477–478
cyclotron motion, 463
cylindrical coordinates, 465–466
differential operators and curvilinear
coordinates
Clairaut’s theorem, 528
curl operator, 523
cylindrical coordinates, 533
div, curl, and Laplacian operators, 532
divergence, 523
Fick’s law of diffusion, 523
Fourier’s law of heat conduction, 523
gradient operator, 522
incompressible, 523
intersect orthogonally, 525
irrotational, 523
Laplacian operator, 524, 527–528
matrix-vector equation:, 530
multivariable chain rule, 526–527
properties of, 524
scale/length factors, 529
spherical coordinates, 532
three curves, 525–526
vorticity, 523
dot product, 457
element of surface area, 519–520
equation of the plane, 462
geostrophic wind, 503
isobars, 502–503
level curves, 502, 505
level sets, 502
linear transformations

1414
Index
area and volume, 514–516
mapping, 511–512
stretching and/or compressing
transformations, 513
transformation or map, 511
unit square, 512
lines, 459
orthogonal matrix, 509–510
orthonormal basis, spherical coordinates,
468–470
parallelogram, 505–507
parametric equations, 462
partial derivatives
Clairaut’s theorem, 485
definition, 485
gradient vector in R3, 492
linear approximation, 486–489
multivariable chain rules, 489–491
scalar potential functions, 493–497
planes, 459–461
polar coordinates, 463
range/image, 463
relationships to the standard o.n. basis, 470
right-handed orthogonal bases for R3,
467–468
rotating coordinate frames (see Rotating
coordinate frames)
spherical coordinates, 465–467
surfaces
cylindrical coordinates, 479
definition, 482
double cusped horn, 481–482
paraboloidal, 478
parametrized surface, 478
sphere and cone intersection, 480
sphere of radius, 481
spherical coordinates, 481
tangent plane and normal vector, 500–502
triangle with vertices, 506
trigonometric functions, 463
volume of parallelepiped, 507–509
Geometric LU factorization, 165–167
Geometric multiplicity, 81–82
Geostrophic wind, 503
Gill’s method, 711
Global optimization
constrained optimization problem,
1038–1039
global express guaranteed shipping,
1040–1042
Lagrange multiplier, 1038–1039
unit basis vector, 1042–1043
Gradient vector, 487
Gramian/Gram matrix, 125
Gram–Schmidt process, 106–109, 118–119,
148–149
Green’s function, 948; see also Heat kernel
Green’s theorem
annulus, 585–586
Clairaut’s theorem, 583
planar vector field, 582–583
polar coordinates, 586–587
rectangle, 587–588
H
Half-life, 219
Half-range expansion, see Fourier cosine series;
Fourier sine series
Hamilton’s principle
continuous media, 1097
Hamilton’s equations, 1095
Lagrangian, 1095
pendulum, 1095
total action during time interval, 1096
Hankel transforms
Bessel’s differential equation, 1295–1296
bi-harmonic operator, 1299
boundary conditions, 1295–1296
definition, 1295
integral transforms, 1297
inversion, 1295
Laplacian operator, 1296
PDE solution, 1298
polar/cylindrical coordinates, 1294
Weber’s second exponential integral, 1298
Heat equations
arbitrary constant, 909
boundary conditions, 996
boundary value problem, 853–854
central difference, 991
composite rod, 854, 913–914
Dirichlet boundary conditions, 997
equilibrium solution, 907–908
finite difference stencil, 992
first-order constant coefficient, 904
forward difference, 991
Fourier series problem, 911
Fourier sine series, 905–907
Fourier’s law of heat conduction, 846
heat flux, 903
inhomogeneous boundary condition,
907–908
initial conditions, 911–913

Index
1415
Neumann boundary conditions, 848, 997
nonlinearity, 998–1000
ODE, 909–910
PDE-BVP-IVP, 992–996
polar coordinates, 967–969
product solution, 904–905
rate of decay, 907
spatial dimension, 850–851
specific heat temperature, 846–847
steady-state temperature, 849
stencil/replacement equations, 992
temperature distribution, 851–853
temperature evolution, 908–909
thermal conductivity, 846–847
thermal diffusivity, 847
time constant, 907
time-dependent boundary conditions,
914–918, 997
xt-plane, 903–904
Heat kernel, 1272
Hessian matrix, 1063
Higher-order linear ordinary differential
equations (ODE)
Abel’s theorem, 248–249
basic solutions, 245
complete set of basic solutions, 245, 247–249
differential operator notation D, 251–253
general solution, 245
LCCHODEs, 250–251, 253
shift theorem, 253
solution for DC circuit, 246–247
standard form, 244
Wronskian determinant, 248
Hilbert space, 185, 188, 1077–1078, 1116
Hilbert transform, 1219–1220
Hill’s equation, 447–448
Homogeneous Dirichlet boundary
conditions, 997
Householder matrix, 120, 691–693
I
Identity matrix, 21
Implicit partial difference equation, 1005
Implicit partial pivoting, 682
Implicit solution, 196, 216
Improper integrals, 553–554, 1215–1217
Improved tangent method (ITM), 702
Incompressible fluid, 1144
Indefinite integral, 551
Inertia matrix, 140
Initial condition, 196, 200–201, 214, 216–217,
224–225, 237, 248
Initial value problem (IVP)
Peano’s existence theorem, 215–217
Picard’s existence and uniqueness
theorem, 217
Picard’s theorem with interval of existence,
217–218
Integrals
anti-derivative, 551–552
area calculation, 549–550
average value of a function, 553
constraints, 1100
divergence theorem, 621–624
double (see Double integral)
Euler’s conservation equation, 624–626
fundamental theorem of calculus, 552,
554–555
Gauss’s theorem, 621
Green’s theorem (see Green’s theorem)
improper integrals, 553–554
indefinite integral, 551
integration by parts, 552
line integrals
Ampère’s circuital law, 566
arclength function, 556
density function, 559
electrostatic potential difference, 566
element of arclength, 557
fundamental theorem, 563–564
helix, 559–560
kinetic energy (KE), 567
linear approximation of a curve, 556
magnetic flux density, 567
magnetic permeability, 566
Newton’s second law of motion, 567
path direction, 565
tangent vector, 565–566
vector-valued functions, 560–562
piecewise-defined function, 551
regular partition, 549
Riemann sum, 549
Stokes’ theorem (see Stokes’ theorem)
substitution method, 552, 554–555
surface integrals and applications (see
Surface integrals)
triple integrals and applications (see Triple
integrals and applications)
union of intervals, 550
Integral transform methods
Fourier transform (see Fourier transform)
Hankel transforms (see Hankel transforms)

1416
Index
inverse Laplace transform (see Inverse
Laplace transform)
Integrating factor, 197–198, 200–201, 204, 208
Integration by parts, 552
Inverse Fourier transform, see Fourier transform
Inverse Laplace transform, 301–303
Bromwich contour, 1285, 1287
class L, 1284
contour integration, 1283
countably infinite, 1284
exponential order, 1286
F(s) →0 uniformly on that family, 1284
Heaviside’s method, 1288
poles, 1284
rational function, 1288
table lookup method, 1283
unbounded family of left half circles, 1284
wave equation, 1288–1291
Inverse matrix
partitioned matrix, 43–44
row reduction algorithm, 40–43
square matrices, 37–40
Inverse power method, 670
Inversion mapping, 1131–1132, 1230
Irreducible quadratic, 1373–1374
Isolated singularity, 1188
Isoperimetric problem, 1100
Iterated integral, 572
J
Jacobian matrix, 516, 664, 666–667
Jacobi iteration converges, 685
Joint probability density (function)., 643
Joukowsky map, 1251–1253
K
Karush–Kuhn–Tucker (KKT) condition,
1064–1065
Kronecker delta, 106
Kutta condition, 1257
L
Lagrange multipliers (λ), 1101; see also Global
optimization
Lagrangian (L), 1095
Lamé constants, 878
Laplace equation, 849
arbitrary constants, 1247
boundary values, circles, 1248–1251
contour/3D plot
homogeneous boundary condition,
943–945
isotherms, 941
Mathematica command, 940–941
rotated graph solution, 942
Dirichlet boundary condition, 1246
electric field lines, 1248
electrostatic potential,935–937
equilibrium heat problem, 932–933
Joukowsky map, 1251–1253
Neumann boundary condition, 1246
open mapping theorem, 1246
parallel sides, 933–935
polar coordinates
Cauchy–Euler equation, 960
clairvoyance, 964–966
continuity boundary condition, 962
double root, 963–964
eigenvalue problem, 959–960
equilibrium heat problem, 958–959
flux vector, 961
Fourier sine series, 961
linearity solution, 960–961
nontrivial product solution, Rn(r), 960
periodic boundary condition,962–963
physical boundary, disk, 964
temperature distribution, 961
using clairvoyance, 937–940
Zhukovskii airfoils (see Zhukovskii airfoils)
Laplace transforms, 305–306
definitions, 1379
inverse Laplace transform (see Inverse
Laplace transform)
linear systems, 390–391
operator, 783
of periodic function, 321
properties and derivations, 299, 301–302, 312
L[cf(t)](s) = cL[f(t)](s), any
constant c, 1380
L[cos ωt](s) =
s
s2+ω2 , 1381
L[δ(t −c)](s) = e−cs, 1385
L[eatf(t)](s) = F(s −a), where
F(s) = L[f(t)](s), 1382
L[eat](s) =
1
s−a, 1380
L[f(t)](s) =
1
1−e−sT
 T
0 f (t) e−st dt, for s > 0,
1385–1386
L[g(t) step(t −c)](s) = e−cs. L[g(t + c)](s),
1382–1383
L[(f ∗g)(t)](s) = L[f(t)](s). L[g(t)](s), 1383

Index
1417
L[sin ωt](s) =
ω
s2+ω2 , 1381
L[step(t −c)](s) = e−cs
s , 1382
L[t cos ωt](s) = (s2 −ω2)/(s2 + ω2)2,
1384–1385
L[t](s) = n!/sn, n = 0, 1, 2, . . ., 1380–1381
L[t sin ωt](s) = 2ωs/(s2 + ω2)2, 1384
L[¨y](s) = s2L[y](s) −sy(0) −˙y(0), 1382
L[˙y](s) = sL[y(t)](s) −y(0), 1380
solution in frequency domain, 300
steady-state solution, 304–305
unit step function (see Unit step function)
Laplacian operator, 524
LaSalle invariance principle, 1341–1343
Laurent series
domains, 1183
geometric series, 1180
integration of, 1210
Maclaurin series, 1183–1185
open annulus, 1180–1182
power series, 1179
removable singularity, 1184
Lax scheme, 1016–1017
LCCHODEs, see Second-order, linear, constant
coefficients homogeneous ordinary
differential equations
Leading entry, 2
Least squares, 1117–1118
Least squares solutions (L.S.S.), 158–159
best approximation problem, 122
normal equations
basic theoretical method, 123–124
Gramian/Gram matrix, 125
orthogonal matrices, 127–129
regression line, 125–127
PA projection, 121–122
residual, 121
vector subspace, 122
Legendre polynomials, 1404
Leibniz’s rule, 497
Levinson–Smith theorem, 1347–1349
L’Hôpital’s rule, 639, 1150
Liapunov functions
autonomous system, 1332
coefficients homogeneous systems, 1329
differential equations, 1329
Hamiltonian mechanics, 1329
instability, 1336–1338
Liapunov theory, 1333–1334
negative definite, 1331
negative semi-definite, 1331
nonlinear ODEs, 1328–1329
positive definite, 1331
positive semi-definite, 1331
quadratic forms, 1331, 1334–1336
real-valued function, 1331
stability equilibrium point, 1338
Liapunov stability, 1313
Liapunov theory, 1333–1334
Linear constant coefficients homogeneous
difference equation (LCCHE),
325–327, 333–334, 340, 343–347
Linear, constant coefficients homogeneous
ordinary differential equations
(LCCHODEs), 264–265, 372–375
Linear constant coefficients homogeneous
system (LCCHS), 372–375, 1309,
1313–1314
Linear, constant coefficients, homogeneous
system of difference equations
(LCCHS), 422, 427
Linear control theory
Cayley–Hamilton theorem, 415–417
controllability condition, 418–419
multi variable control system, 417, 420
observability criterion, 421
optimal control, 420
piecewise continuous, 414
Riemann sums, 415
scalar-valued function, 412–414, 419
single input control system, 412, 420
vector-valued function, 412
Linear convergence, 655
Linear first-order ordinary differential
equations (ODE)
basic solutions, 199
complete set of basic solutions, 199
general solution, 199, 201–202
integrating factor, 197–199
particular solution, 199–200
standard form, 196
Linear homogenous systems
arbitrary constants, 365–366
complete set of basic solutions,
364–365
eigenvalues and eigenvectors, 362
fundamental matrix, 368–372
general solution, 364–366
initial conditions, 363
initial value problem, 366–367
Maclaurin series, 375–376
nonconstant coefficients, 376–377
second-order LCCHODE and LCCHS,
372–375
unique solution, 363
Linearity principle, 27

1418
Index
Linear mapping
boundary point, 1224, 1226
definition, 1223
domain D, 1224, 1226
magnification, 1223, 1225
punctured line, 1232
rotation, 1223–1225
tangent vectors, 1223, 1226
translation, 1223, 1225
Linear programming (LP) problem
Acme company, 1049–1050
convex set and lines, 1048–1049
linear inequalities, 1048
slack variable and standard form
feasible solution, 1051–1052
nonnegative number, 1050–1051
total cost, 1053–1054
wheat bran, oat flour, and rice flour
proportions, 1052–1053
structural optimization, 1054
Linear superposition principle, 31
Linear system, 353
compartment models, 358–360
complex eigenvalues, 381–385
constant coefficient matrix, 355
deficient eigenvalues, 387–390
of difference equations
Casorati determinant, 425–427
color blindness, 423–425
complex eigenvalues, 427–428
ladder network electrical circuits, 429–434
LCCHS, 422
second-order scalar difference equation,
428–429
stability, 434–436
homogeneous systems of second-order
equations, 385–387
Laplace transforms, 390–391
linear control theory
Cayley–Hamilton theorem, 415–417
controllability condition, 418–419
multivariable control system, 417, 420
observability criterion, 421
optimal control, 420
piecewise continuous, 414
Riemann sums, 415
scalar-valued function, 412–414, 419
single input control system, 412, 420
vector-valued function, 412
linear homogenous systems
arbitrary constants, 365–366
complete set of basic solutions, 364–365
eigenvalues and eigenvectors, 362
fundamental matrix, 368–372
general solution, 364–366
initial conditions, 363
initial value problem, 366–367
Maclaurin series, 375–376
nonconstant coefficients, 376–377
second-order LCCHODE and LCCHS,
372–375
unique solution, 363
matrix–vector form, 354–355
nonhomogeneous linear systems
characteristic polynomial, 397
definite and indefinite integration, 396
fundamental matrix, 395–396, 398–400
law of exponents, 397
solution, 400
variation of constants, 397
variation of parameters, 397
nonresonant nonhomogeneous systems
computer hardware obsolescence model,
407–408
constant forcing function, 406
eigenvalues and eigenvectors, 404
general solution, 404–405, 407
MathematicaTM, 407
nonresonance assumption, 405
particular solution, 405
sinusoidal forcing, 408–411
undetermined coefficients method, 403
periodic linear differential equations
Floquet representation, 442–445
Hill’s equation, 447–448
period doubling, 440
principal fundamental matrix, 440
stability, 445–447
stroboscopic/return map, 441–442
T-periodic solution, 440, 448–451
RLC series circuit, 353–354
RLC two-loop circuit, 355–356
second-order equations, 357–358
spring–mass–damper system, 356
stability, 391–392
Linear transformations, 511–513
Line integrals
Ampère’s circuital law, 566
arclength function, 556
density function, 559
electrostatic potential difference, 566
element of arclength, 557
fundamental theorem, 563–564
helix, 559–560
kinetic energy (KE), 567
linear approximation of a curve, 556

Index
1419
magnetic flux density, 567
magnetic permeability,566
Newton’s second law of motion, 567
path direction, 565
tangent vector, 565–566
vector-valued functions, 560–563
Line search, 1046
Liouville’s theorem, 371
Logarithmic decrement, 235–236
Lorentz force, 458
Lower triangular matrix, 22–23
M
Maclaurin series, 375–376, 400, 1088, 1184–1185
Magnetic permeability, 566
Magnification, 1223, 1225
Mass matrix, 140
MathematicaTM, 64, 86–87, 214, 407, 477–478, 557,
559–560, 601, 603, 605, 623, 673, 716,
738–739, 796–797, 822, 924, 1040, 1086,
1305
MATLAB®, 86–87, 597, 662, 796–798, 1010
Matrix
addition, 16–17
adjugate matrix, 53–54
augmented matrix, 5–6
basis, 64–65
bounded linear functionals and operator
adjoint, 186–187
Cauchy criterion, 185
Cauchy–Schwarz inequality, 103–105,
111–112
Cholesky factorization, 168–169
convergence of sequences, 183–184
Cramer’s rule, 54–56
determinant, 48–52
diagonal matrix, 21–22, 95–98
direction vector, 103
dot product, 101
eigenvalues and eigenvectors
adjugate matrix method, 79–82
annihilator method, 93
characteristic equation, 76, 78
characteristic polynomial, 76
column operation, 77
complex conjugate pairs, 83–84, 94
complex numbers, 82–83
deficient eigenvalues, 98
determinant, 77
eigenspaces, 91–92
elementary row operations, 77
geometry, 75–76
MathematicaTM, 86–87
MATLAB®, 86–87
triangular and diagonal matrices, 85–86
trivial solution, 79
elementary matrices, 24–25
elementary row operations, 3–4
Gram–Schmidt process, 106–109, 118–119
Hilbert space, 185, 188
homogeneous system
Ax = 0 form, 26
basic solutions, 30–31
general linear combination of vectors, 27
general solution, 28
linearity principle, 27
nullity, 31–32
spanning set, 28–30
trivial solution, 27
identity matrix, 21
inner product, 102, 175–177
inverse matrix
partitioned matrix, 43–44
row reduction algorithm, 40–43
square matrices, 37–40
leading entry, 2
least squares solutions (see Least squares
solutions)
linear functionals and operators, 177–179
linear independence, 62–64
lower inequality, 145
lower triangular matrix, 22–23
LU factorization, 165–168
Moore–Penrose generalized inverse, 160–162
multiplication, 18–20
nonhomogeneous system, 33–36
nonzero row, 2
norm and bounded linear operators, 179–183
orthogonal matrix, 116–118
orthogonal projections, 109–111
orthogonal set of vectors, 105–106
orthogonal sets and bases, 114–116
pivot column, 3
pivot positions, 2
positive definite matrix (see Positive
definite matrix)
positive semi-definite matrix, 136
powers, 23–24
QR factorization, 148–151
rank of C, 3
Rayleigh quotient
continuous systems, 170
property, 172

1420
Index
real matrix, 171
theorem, 172–174
row echelon form, 2
RREF
basic variable, 8
free variable, 8
Gaussian elimination methods, 9
Gauss–Jordan method, 6–7
lemma, 10–11
linear system, 11–13
matrix of coefficients, 10
signal restoration, 187–188
SVD
block matrices, 153–154
construction, 152
factorization, 154–158
least squares solution (L.S.S.), 158–159
positive eigenvalues, 152
pseudo-diagonal matrix, 151, 153
uniqueness, 155
zero eigenvalues, 152
symmetric matrices
A2, A−1,
√
A formulas, 137
diagonal matrix, 132
orthogonality for eigenvectors, 132–134
orthogonally similar, 131
spectral formula, 135
unique least squares solution, 138–139
transpose, 24
triangle inequality, 103–104, 145
upper inequality, 145
upper triangular matrix, 22–23
vector spaces and subspaces, 65–69
weak convergence and compactness, 189–191
zero matrix, 21
zero row, 2
Matrix addition, 16–17
Matrix multiplication, 18–20
Matrix norm induced, 664
Matrix–vector form, 354–355
Maxwell’s integral equations
Ampère—Maxwell’s law, 840
electromotive force, 838
magnetic flux density, 838–840
magnetomotive force, 840
physical law, 839
Method of successive approximations, 656
Midpoint method, 702
Möbius transformations, 1228–1229
circles, 1230–1232
concentric circles
centered, 1234
collinear, 1234
eccentric circles, 1233
interior vs. exterior relationship,
1236–1239
positive and negative orientation, 1238
quadratic equation, 1235
cross-ratio method, 1240–1242
distinct points, 1240
inversion mapping, 1230
lines, 1232–1233
orientations, 1242
test value, 1243
Modified Bessel equation of order α, 1403
Modified Newton’s method, 653
Modified (or corrected) Newton’s
method, 653
Momentum density function, 881
Monodromy matrix, 442
Moore–Penrose generalized inverse, 139,
160–163
Multi-index, 732
Multivariable chain rules, 489–491
N
Natural frequency, 232
Neumann boundary condition, 1246
Neumann–Goldstine criterion, 1003–1005
Newton–Cotes formula, 670
Newton–Raphson method
in Rn, 661–664
scalar equation
bisection method, 652–653
cycling in, 652–653
iterate, 650–651
point-slope formula, 649
quadratic convergence, 651
software implementation, 651
tangent line, 649
Newton’s law of cooling, 357
Nonhomogeneous Dirichlet boundary
conditions, 997
Nonhomogeneous linear systems
characteristic polynomial, 397
definite and indefinite integration, 396
fundamental matrix, 395–396, 398–400
law of exponents, 397
solution, 400
variation of parameters/constants, 397
Nonlinear differential equations
continuous dependence, 1359–1361
delay equations

Index
1421
characteristic equation, 1369–1370
Euler’s method, 1370
horseshoe map and chaos
Brownian motion, 1365
butterfly effect, 1364
entropy, 1365
homoclinic orbit, 1367
periodic solution, 1365
Poincaré map, 1366
Smale horseshoe map, 1365–1366
initial value problem (IVP), 1303
LaSalle invariance principle, 1341–1343
Liapunov functions (see Liapunov functions)
limit cycles
Hopf bifurcation, 1349–1350
Levinson–Smith theorem, 1347–1349
orbitally stable, 1344
periodic linearization, 1346
periodic solution, 1346–1347
stable and unstable, 1344
time translation, 1344
local Lipschitz condition, 1355–1356
Peano’s existence theorem, 1355
phase plane
autonomous systems, 1304–1306
damped oscillator system, 1305
equilibria, 1306
initial value problem (IVP), 1303
LCCHS, 1309
nonlinear system, 1305–1306
phase line, 1307–1309
saddle point case, 1309–1311
stable spiral point, 1305
steady-state oscillation, 1303–1304
undamped harmonic oscillator
system, 1304
unstable spiral point, 1305
Picard’s theorem, 1356–1358
stability of equilibrium point
asymptotically stable, 1314
attractor, 1314
LCCHS, 1313–1314
Liapunov stability, 1313
linearization stability, 1315–1317
linear system, 1314
stable and unstable, 1313
undamped harmonic oscillation, 1314
using r(t), 1318–1319
variation of parameters, linearization
almost linear system, 1321
periodic solutions, 1326–1327
saddle point theorem (see Saddle point
theorem)
Nonlinear programming problem
complementarity condition, 1066–1067
convex programming (CP) problem, 1064
convex set, 1063
dual LP problem, 1068–1071
feasible region, 1064
geometric dimensioning/tolerancing,
1071–1072
Hessian matrix, 1063
KKT condition, 1064–1065
Lagrangian, 1064
line segment, 1062
real, constant, positive semi-definite
matrix, 1063
Slater’s condition, 1064
stationarity condition, 1066–1068
Nonlinear Volterra integral equations, 1328
Nonresonant nonhomogeneous systems
computer hardware obsolescence
model, 407–408
constant forcing function, 406
eigenvalues and eigenvectors, 404
general solution, 404–405, 407
MathematicaTM, 407
nonresonance assumption, 405
particular solution, 405
sinusoidal forcing, 408–411
undetermined coefficients method, 403
Nontrivial solution, 715
Nonuniform tent–spline functions, 727–729
Normal equations
basic theoretical method, 123–124
Gramian/Gram matrix, 125
orthogonal matrices, 127–129
regression line, 125–127
Normal random variable, 640–642
Norm and bounded linear operators, 179–183
Numerical methods
Ax = b
absolute error, 678
column diagonally dominant, 685
convergence and stability, 677
Frobenius norm, 684, 686
Gaussian elimination algorithm, 677
Gauss–Jordan algorithm, 679–682
Hilbert matrix, 686
induced matrix norm, 684–685
iterative methods, 682–684
Jacobi iteration converges, 685
perfectly conditioned matrix, 679
relative error, 678
row diagonally dominant, 685
standard condition number, 677

1422
Index
BVP (see Two-point boundary value problem
(BVP))
derivative approximations
central difference approximation, 699
central difference operator, 696
forward difference operator, 695
linear approximation, 695
Taylor series, 696–698
finite difference methods (see Finite
difference methods)
Galerkin method (see Galerkin method)
integrals approximation
Boole’s rule, 671–672
iteration of quadrature, 674–675
midpoint rule, 670, 672
Newton–Cotes formula of
closed type, 670
numerical drift, 675
quadrature rules, 670, 672–673
Riemann sums, 669–670
Simpson’s rule, 670, 672
step size, 670
trapezoidal rule, 670, 672
two-edged sword, 675
linear algebraic eigenvalue problems
almost triangular matrix, 692
deflation and similarity, 690
elementary method, 687–688
extended sign function, 691
Householder matrix, 691–693
lower Hessenberg form, 692
power methods, 688–690
QR algorithm, 693
similarity transformations, 690–691
upper Hessenberg form, 692
numerical stability
amplification factor, 1003
consistent, 1001
convergent method, 1001
Crank–Nicholson method, 1005–1006
discretization matrix, 1002
Neumann–Goldstine criterion, 1003–1005
stable, 1002
sum of sines formula, 1003
ODE-IVPs
Euler’s method (EM), 700, 702
global error, 702
ITM, 702
linear approximate solution graph, 700
local error, 701
multistep methods, 705
multivariable chain rule, 701, 703
numerical instability and stiffness,
707–708
Predictor–Corrector methods, 706
Runge–Kutta methods, 704–705
Taylor’s formula method, 706–707
Taylor’s theorem, 703
scalar equation (see Scalar equation)
splines
cubic B-splines (see Cubic B-splines)
curve in Rn, 729–730
horizontal translation, 721
nonuniform splines, 727–729
piecewise linear approximation, 720
piecewise polynomial, 720
support interval I, 722
surface, 731
tent–spline function, 721
triangular surface patches, 731–732
uniform tent basis functions, 721
uniform tent–spline functions, 720
system of equations (see System of equations)
O
Odd function, 744–747
ODE, see Ordinary differential equation (ODE)
One compartment model, 207
One-sided z-transforms, 340–341
Optimization
continuous function, 1028–1029
convex function, 1029–1030
critical number, 1028
global minimum (value), 1027
insulation, critical radius of, 1029
local minimum (value), 1027
LP problem (see Linear programming (LP)
problem)
nonlinear programming
complementarity condition, 1066–1067
convex programming (CP) problem, 1064
convex set, 1063
dual LP problem, 1068–1071
feasible region, 1064
geometric dimensioning/tolerancing,
1071–1072
Hessian matrix, 1063
KKT condition, 1064–1065
Lagrangian, 1064
line segment, 1062
real, constant, positive semi-definite
matrix, 1063

Index
1423
Slater’s condition, 1064
stationarity condition, 1066–1068
Rayleigh–Ritz method
dot product, 1076
global minimizer, 1076–1077
Hilbert space eigenvalue problem,
1077–1078
maximization problem, 1075–1076
real symmetric matrix, 1073–1075
simplex procedure (see Dantzig’s simplex
procedure)
variables function
critical/stationary point, 1033–1034
feasible direction, 1032–1033
Fermat’s theorem, 1034–1035
global optimization (see Global
optimization)
Hessian matrix, 1035
interior, 1033
local minimizer, 1033
minimization problem, 1032
numerical minimization, 1045–1046
objective function, 1032
saddle point, 1035
second derivative test, 1036–1038
steepest descent method, 1045–1046
Taylor’s theorem, 1036
Order, definition, 195
Ordinary differential equation (ODE)
Fourier sine series, 763–764
linear system (see Linear system)
polynomial solutions, 1403–1404
power series solutions (see Power series
solutions, ODEs)
Orthogonal matrix, 116–118, 127–129
Orthogonal projections, 109–111
Orthogonal sets and bases, 114–116
Overdamped, 225–228, 237
Over-relaxation methods, 683
P
Parametric equations, 459
Parseval identity, 116, 160, 830
Parseval’s theorem
Bessel’s inequality, 829
best approximation, 829
complex Fourier series, 829–830
Fourier cosine series, 826–827
Fourier series, 827
Fourier transforms, 831–832
mass density, 824–825
square integrable, 827–828
Partial differential equations (PDEs)
cylindrical coordinates
Bessel’s equation of order zero, 974–975
double integrals, 980–981
eigenfunctions, 979
Fourier–Bessel coefficients, 977–978
modified Bessel’s equation of order n,
980–981
nonhomogeneous boundary conditions,
976
orthogonality relation, 977
periodicity, 973–974
θ-dependence, 973
using clairvoyance, 975–976
zero component, 978–979
D’Alembert wave solution (see D’Alembert
wave solution)
energy conservation, 897–900
first-order ODE, exponential decay, 1271
Fourier cosine and sine transforms,
1272–1279
heat equations
arbitrary constant, 909
boundary value problem, 854–855
composite rod, 854, 913–914
equilibrium solution, 907–908
first-order constant coefficient, 904
Fourier series problem, 911
Fourier sine series, 905–907
Fourier’s law of heat conduction, 846
heat flux, 903
inhomogeneous boundary condition,
907–908
initial conditions, 912–913
Neumann boundary condition, 848
ODE, 909–910
product solution, 904–905
rate of decay, 907
spatial dimension, 850–851
steady-state temperature, 849
temperature distribution, 853–855
thermal conductivity, 846–847
thermal diffusivity, 847
time constant, 907
time-dependent boundary condition,
914–918
time evolution, 908–909
xt-plane, 903–904
heat kernel, 1272
integral equation
arbitrary control volume, 835, 837–838

1424
Index
continuum mechanics (see Continuum
mechanics)
divergence theorem, 835–837
fluid mass, 835
heat energy density, 836
heat flux vector, 836
Maxwell’s equation (see Maxwell’s
integral equations)
Stokes’ theorem, 838
Laplace equation (see Laplace equation)
Laplacian and application
eigenfunction, 949–950
steady-state temperature, 953–954
surface waves, 954–956
temperature distribution function, 952
time-dependent heat flow, slab, 951–952
transverse vibrations, rectangular
membrane, 952–953
Parseval’s theorem, 1279
polar coordinates
heat equation, 967–969
Laplace equation (see Laplace equation,
polar coordinates)
potential equation
boundary condition, 861
closed ball, 865
Dirichlet problem, 865
irrotational, 860
Laplace’s equation, 861
magnetostatics, 860–861
Poisson’s formula, disk, 864–865
potential flow, 860
solvability condition, 862–864
vorticity, 860
separation of variables, 1270
spatial variable, 1270
spherical coordinates
electric potential, u, 982
Legendre polynomials, 983–984
Legendre’s equation, 982–983
Neumann problem, 985
orthogonality relation, 984
ρ- and φ-dependence, 982
solvability condition, 984–985
wave equation (see Wave equation)
Partial fractions
complex form, 1377
proper rational function, 1373–1377
real form, 1373–1377
Partial pivoting, 681
Partial sums, 738–739
Partitioned matrix, 17, 43–44
PDEs, see Partial differential equations (PDEs)
Peano’s existence theorem, 215, 1355–1359
Pendulum, 1095
Period doubling, 440, 1367
Periodic function, 286
Periodic linear differential equations
Floquet representation, 442–446
Hill’s equation, 447–449
period doubling, 440
principal fundamental matrix, 440
stability, 445–447
stroboscopic/return map, 441–442
T-periodic solution, 440, 448–451
Phase form, 232–234
Picard’s theorem, 1358–1361
Piecewise continuous, 414
Piecewise smooth, 740
Plane wave, 897
Plasticity, 1054
Poincaré–Andronov–Hopf bifurcation,
1351–1352
Poincaré map, 1368
Poisson’s equation, 849, 860
Poisson’s ratio, 879
Position vector, 459
Positive definite matrix, 136
generalized eigenvalue problem, 140
positive definiteness and determinants,
141–143
scalars, 139
vibrations, 140
Positive semi-definite matrix, 136
Potential equations
boundary conditions, 1010–1011
central difference, 1007
finite differences approximate solution,
1008–1010
five-point Laplacian stencil, 1008
replacement equations, 1007
Potential flow, 627, 860, 1144
Power functions, 1164–1169
Power series solutions, ODEs
basic methods, 1387–1390
convergence
converges, 1390
converges absolutely, 1391
diverges, 1391
radius of convergence, 1391–1393
sequence of partial sums, 1390
Frobenius method
Bessel’s equation of order α, 1400–1403
definition, 1398–1400
nonconstant coefficients, 1393–1397
x −a, 1397

Index
1425
Practical resonance, 280
Predictor–Corrector methods, 706
Principal minors, 142
Principal nth root of unity, 1129
Probability density function (PDF), 636
Probability distributions
bell curve, 641
CDF, 636–637
expectation, 638
exponential distribution, 638
frequency interpretation, 635
Gaussian distribution, 641
Gaussian error function, 642
joint distribution, 643–644
L’Hôpital’s rule, 639
mean of random variable, 640
mode of continuous random variable, 640
normal random variable, 640
PDF, 636
problems, 645
random variable, 635
standard deviation, 640
Propagator, see Heat kernel
Pseudo-diagonal matrix, 151, 153
Punctured line, 1232
Pythagorean theorem, 463
Q
QR algorithm, 693
Quarter-chord point, 1255
Quasi-frequency, 232
Quasiperiodic function, 236, 286
R
Radius of convergence, 1177, 1391–1393
Rayleigh quotient
continuous systems, 170
first integral, 818
linear function X(x), 822
property, 172
quadratic functionals, 820
real matrix, 171
regular Sturm–Liouville problem, 919–820
second integral, 819
single trial function X(x), 820–822
theorem, 172–174
Rayleigh–Ritz method
dot product, 1076
eigenvalue problems, 1106
finite element methods, 1111–1113, 1117
global minimizer, 1076–1077
Hilbert space eigenvalue problem, 1077–1078
maximization problem, 1075–1076
minimization, 1085–1087
real symmetric matrix, 1073–1075
Real analytic function, 1179, 1398
Real inner product space, 176
Real linear factor, 1373
Reciprocal/dual lattice, 522
Recursion relation, 1387, 1389
Reference configuration, 869–870
Reflection transformations, 513
Regular singular point, 1398
Relaxation method, 683
Replacement equations, 992
Riemann sums, 341, 415, 549, 574, 670, 1196
Root mean square (RMS), 830
Rotating coordinate frames
ODEs, 537–539
velocity and acceleration
acceleration of the particle relative, 542
angular acceleration vector, 543
angular velocity vector, 543
constant angular speed, 540
Coriolis acceleration, 541
Earth’s equator, 542–543
intermediate frame, 543
rotating coordinate frame, 541
Rotation transformations, 513
Round-off-error, 654–655
Row reduced echelon form (RREF)
basic variable, 8
free variable, 8
Gaussian elimination methods, 9
Gauss–Jordan method, 6–7
lemma, 10–11
linear system, 11–13
matrix of coefficients, 10
Row reduction algorithm, 40–43
Runge–Kutta method of order four, 711
Runge–Kutta method of order three, 711
Runge–Kutta method of order two, 711
Runge–Kutta methods, 704–705
S
Saddle point theorem
dashed solution curves, 1323
differentiable surface, 1324

1426
Index
invariant, 1324
k-dimensional surface, 1324
ODE, 1322
phase plane, 1323
planar linear system, 1322
stable and unstable manifold, 1325
twice continuously differentiable surface,
1324
Sawtooth function, 323
Scalar equation
Aitken’s δ2 method, 656–657
fixed point problem iteration, 656
modified Newton’s method, 653
Newton–Raphson method
bisection method, 652–653
cycling in, 652–653
iterate, 650–651
point-slope formula, 649
quadratic convergence, 651
software implementation, 651
tangent line, 649
secant method, 654–656
Scalar hyperbolic problem, 1015–1016
Scalar ordinary differential equation (ODE)
Casorati determinant
forward difference operator, 330
higher order difference operators, 331
n-th order linear homogeneous difference
equation, 332
Wronskian determinant, 331
Cauchy–Euler equations, 325
characteristic polynomials, 326–327
complex number, polar form, 327
complex roots, 328
Fibonacci numbers, 324
forced oscillations
beats phenomenon, 283–287
examples, 273–274
forced spring-mass-damper system,
273–274
maximum frequency response, 281–283
resonance case, 274–276
steady-state oscillation, 276–281
homogeneous problems, 195–196
Cauchy–Euler ODEs, 255–259
exact ODEs (see Exact ordinary
differential equation (ODE))
higher-order linear ODEs (see
Higher-order linear ordinary
differential equations (ODE))
initial value problem (IVP), 215–217
linear first-order ODE (see Linear
first-order ordinary differential
equations (ODE))
second-order LCCHODEs (see
Second-order, linear, constant
coefficients homogeneous ordinary
differential equations (LCCHODEs))
separable ordinary differential equation
(ODE), 209–211
steady-state solution, 203
transient solutions, 202, 204
Laplace transforms, 305–306
inverse Laplace transform, 301–303
of periodic function, 321
properties, 299, 301–302
solution in frequency domain, 300
steady-state solution, 304–305
unit step function (see Unit step function)
LCCHE, 325–327
nonhomogeneous
general solution, 263–264, 269
LCCHODEs, 264–265
method of undetermined coefficients,
265–269
particular solution, 263–264, 269
RHS function, 264
shift theorem, 270–271
nonhomogeneous linear difference equation,
334
n-th order difference equation, 324
polar coordinates/trigonometry, 327
second-order difference equation, 329
undetermined coefficients method, 334–337
variation of parameters
general solution, 293
methods, 294–297
particular solution, 294
second-order nonhomogeneous ODE, 292
z-transforms (see z-transforms)
Scalar potential functions, 493–497
Scalar triple product, 508
Secant method
scalar equation, 654–656
system of equations, 666–668
Second-order equations, 357–358
Second-order, linear, constant coefficients
homogeneous ordinary differential
equations (LCCHODEs)
Abel’s theorem, 238–329
amplitude and phase form, 232–234
basic solutions, 239
critically damped case, 237
existence and uniqueness, 229

Index
1427
existence of complete set of basic solutions,
229–230
general solution, 228–229
series RLC circuit, 225–228
solution graph
envelope, 235
logarithmic decrement, 235
quasi-period, 236
spring–mass–damper systems, 222–225
standard form, 222
underdamped case
basic solution, 230–232
Euler’s formula, 230–231
Wronskian determinant, 238
Separable ordinary differential equations
(ODE), 209–211
Separation of variables method, see Partial
differential equations (PDEs)
Sequence of partial sums, 1390
Series RLC circuit, 225–228
Set union, 1174
Shear modulus, 878
Shear transformations, 24, 513, 515
Shift operator, 341
Shift theorem, 253, 270–271
Shooting methods, 717
Signal restoration, 187–188
Similarity transformation, 690–691
Simpson’s rule, 670, 672–673, 1001
Singular value decomposition (SVD)
block matrices, 153–154
construction, 152
factorization, 154–158
least squares solution (L.S.S.), 158–159
positive eigenvalues, 152
pseudo-diagonal matrix, 151, 153
uniqueness, 155
zero eigenvalues, 152
Slater’s condition, 1064
Smale horseshoe map, 1365–1366
Sobolev space, 1109
Solution of ODE, definition, 195
Spanning set, 28–30
Spectral formula, 135
Splines
cubic B-splines (see Cubic B-splines)
curve in Rn, 729–730
horizontal translation, 721
nonuniform splines, 727–729
piecewise linear approximation, 720
piecewise polynomial, 720
support interval I, 722
surface, 730
tent–spline function, 721
triangular surface patches, 731–732
uniform tent basis functions, 721
uniform tent–spline functions, 720
Spring–mass–damper systems, 222–225
Square matrices, 37–40
Square wave function, 316, 450, 747
Standard form
Cauchy–Euler ODEs, 255
higher-order linear ordinary differential
equations (ODE), 244
linear first-order ordinary differential
equations (ODE), 196
second-order, linear, constant coefficients
homogeneous ordinary differential
equations (LCCHODEs), 222
Standard orthonormal, 467
Steady-state oscillation, 276–280, 1306
Steady-state solution, 203, 269, 276–278, 304,
345–348, 849
Steffensen’s method, 657
Stencil equations, 992
Stiffness matrix, 140
Stokes’ theorem
chain of three curves, 630
Green’s theorem, 627, 634
inverse square law force field, 632
Poisson’s equation, 632
positively oriented boundary curve, 627–629
total flux of curl (F), 631–632
Streamlines, 1144
Stress/traction, vector, 843
Stretching and/or compressing
transformations, 513
Stroboscopic/return map, 441–442
Sturm–Liouville problems, 1109–1110
adjugate matrix method, 812
chain rule, 811–812
eigenvalues, 806
fourth-order ODE-BVP, 813–815
interface condition, 810
match, 810–811
ODE, 805
orthogonality relation, 806–807
regular, 805
singular Sturm–Liouville problem, 808–810
Super linear convergence, 655
Surface integrals
annulus geometry, 609–610
computer algebra system, 619
element of surface area, 609–611
parametric surface, 607–608
scalar-valued function, 611–613

1428
Index
surfaces of revolution, 620–621
twisted cylinder, 617–618
vector field
closed surface, 616
flux of F across, 614
iterated double integral, 614
piecewise smooth, 615
positive orientation, 616–617
tangent plane, 613
total flux of electric field, 615
upward orientation, 616
vector-valued function, 613–614
z-coordinate centroid, 618–619
SVD, see Singular value decomposition (SVD)
Symmetric matrices
A2, A−1,
√
A formulas, 137
diagonal matrix, 132
orthogonality for eigenvectors, 132–134
orthogonally similar, 131
positive definite matrix, 136
positive semi-definite matrix, 136
spectral formula, 135
unique least squares solution, 138–139
Symmetric space, 1116
System of equations
fixed point problem iteration, 666
Newton–Kantorovich theorem, 664–666
Newton’s method in Rn
iterates converge, 662–663
Jacobian matrix, 661
MATLAB®, 662
quadratic convergence, 662–664
software implementation, 662
secant method, 666–668
T
Taylor series, 706–707, 874
Cauchy’s integral formula, 1178
open disk D, 1179
product of, 1185
real analytic, 1179
Taylor’s theorem, 697–698, 702–703, 1035–1036
Tent basis functions, 727
Thermal conductivity, 846–847
Thermal diffusivity, 847, 908, 915
Thickness distribution, 1255
Thickness ratio, 1255, 1258
Time constant, 227–228, 251, 447, 907
Torque, 458
Total linear momentum, 843, 871
Transient solutions, 202, 204
Translation, 1223, 1225
Transverse shear forces, 879–880
Triangle inequality, 103–104, 145
Triangular and diagonal matrices, 85–86
Trigonometric functions
complex numbers, 1171
Euler’s formula, 1171
set union, 1174
Triple integrals and applications
cylindrical coordinates
average value of a function, 601
boneshaft, 597–598
circular symmetry, 597
cortical bone material, 597
solid, 598–600
total mass of the bone, 598
trabecular bone material, 597
polar moment of inertia, 595–594
solid tetrahedron’s centroid, 596
spherical coordinates, 602–605
triple Riemann sum, 595
Triple Riemann sum, 595
Trivial solution, 27
Two-point boundary value problem (BVP)
Bessel’s equation of order zero, 714
finite difference method, 712
IVP numerical methods, 716–717
ODE-BVP eigenvalue problem, 715–716
periodic solutions of linear problems,
717–718
replacement equations, 712–713
U
Undamped natural frequency, 232
Underdamped, 225–226, 230–232
Undetermined coefficients method, 265–269, 403
Uniaxial stretching, 879
Uniform tent basis functions, 721, 1112
Unilateral z-transforms, 340–341, 350
Unit square, 512
Unit step function
convolution
definition, 312
Laplace transform properties, 312
particular solution, 314–318
properties, 312, 314
reality check, 313
delta functions, 318–320
graphical method, 309–312

Index
1429
periodic function, 321
properties, 307–308
Upper triangular matrix, 22–23
V
Variation of parameters
general solution, 293
methods, 294–297
particular solution, 294
second-order nonhomogeneous
ODE, 292
Vector field conservative, 564
Virtual work principle, 881–882
W
Wave equations
D’Alembert wave solutions, 1014
Fourier sine series problem, 925–926
guitar string, 868–869, 926–927
infinite series, 925
Lax scheme, 1016–1017
linear elasticity
linearized strain tensor, 876–877
plane strain, 877–878
principal strains, 877
shear modulus, 878
linear elastostatics, 879–880
product solution, 925
scalar hyperbolic problem (see Scalar
hyperbolic problem)
speed of sound, 924
adiabatic conditions, 875–876
chain rule and Taylor series, 874
continuity equation, 87
linearization system, 874
mass density, 873
momentum equation, 872
perturbation method, 875
small disturbances, 873
taut string, 927–928
undamped harmonic oscillator, 924
vibrating string
contact/tension force, 870
elastic, 870, 872
local elongation, 870
Newton’s second law, 871
reference configuration, 869–870
wave speeds, 1014
Wave number, 897
Wronskian determinant, 238–239, 248,
295, 331, 368
Y
Yield stress, 1054
Young’s modulus, 879
Z
Zero matrix, 21
Zhukovskii airfoils, 477–478
camber line, 1255
camber ratio, 1255
centered at the origin, 1253–1254
law of cosines, 1253–1254
lift force
angle of attack, 1257
Cauchy–Riemann equations, 1256
chord length, 1257
complex potential function, 1256
complex velocity, 1256
2D flow, 1255–1257
Kutta condition, 1257
potential flow, 1255
stagnation point, 1257
nondimensional parameter, 1253
quarter-chord point, 1255
small parameter, 1253
thickness distribution, 1255
thickness ratio, 1255
Zoo of Solutions of LCCHODEs, 250–251, 253
z-transforms
convolution, 348
geometric series, 340
LCCHE, 342–344
linearity properties, 342
Riemann sum, 341
shift operator, 341
sinusoidal signals, 344–345
steady-state solution, 345–347
transfer function, 348–349
unilateral/one-sided, 340–341


K11552
Beginning with linear algebra and later expanding into calculus of varia-
tions, Advanced Engineering Mathematics provides accessible and 
comprehensive mathematical preparation for advanced undergraduate 
and beginning graduate students taking engineering courses. This book 
offers a review of standard mathematics coursework while effectively 
integrating science and engineering throughout the text. It explores the 
use of engineering applications, carefully explains links to engineering 
practice, and introduces the mathematical tools required for understand-
ing and utilizing software packages.
• Provides comprehensive coverage of mathematics used by 
engineering students
• Combines stimulating examples with formal exposition and 
provides context for the mathematics presented
• Contains a wide variety of applications and homework problems 
• Includes over 300 figures, more than 40 tables, and over 
1500 equations
• Introduces useful Mathematica™ and MATLAB® procedures
• Presents faculty and student ancillaries, including an online 
student solutions manual, full solutions manual for instructors, 
and full-color figure slides for classroom presentations
Advanced Engineering Mathematics covers ordinary and partial 
differential equations, matrix/linear algebra, Fourier series and transforms, 
and numerical methods. Examples include the singular value decomposi-
tion for matrices, least squares solutions, difference equations, the 
z-transform, Rayleigh methods for matrices and boundary value problems, 
the Galerkin method, numerical stability, splines, numerical linear algebra, 
curvilinear coordinates, calculus of variations, Liapunov functions, 
controllability, and conformal mapping.
This text also serves as a good reference book for students seeking 
additional information. It incorporates Short Takes sections, describing 
more advanced topics to readers, and Learn More about It sections with 
direct references for readers wanting more in-depth information.
ADVANCED ENGINEERING
MATHEMATICS
Mathematics for Engineering 


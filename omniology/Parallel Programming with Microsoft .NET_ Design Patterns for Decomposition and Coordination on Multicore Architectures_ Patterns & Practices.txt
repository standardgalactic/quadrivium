 001
•  •  •  •  •  •
•  •  •  •  •  •  •  •
•  •  •  •  •  •  •
•  •  •  •  •
P A R A L L E L 
P R O G R A M M I N G 
M I C R O S O F T
®   .N E T
Design Patterns for 
Decomposition and Coordination 
on Multicore Architectures
Colin Campbell
Ralph Johnson
Ade Miller
Stephen Toub
Foreword by
Tony Hey
WITH

a guide to parallel programming


Parallel Programming 
with Microsoft
®.NET
Design Patterns for Decomposition and
Coordination on Multicore Architectures
Colin Campbell
Ralph Johnson
Ade Miller
Stephen Toub

ISBN 9780735640603
This document is provided “as-is.” Information and views expressed in this 
document, including URL and other Internet website references, may change 
without notice. You bear the risk of using it. Unless otherwise noted, the 
companies, organizations, products, domain names, email addresses, logos, 
people, places, and events depicted in examples herein are fictitious. No 
association with any real company, organization, product, domain name,  
email address, logo, person, place, or event is intended or should be inferred. 
Complying with all applicable copyright laws is the responsibility of the user. 
Without limiting the rights under copyright, no part of this document may be 
reproduced, stored in or introduced into a retrieval system, or transmitted in  
any form or by any means (electronic, mechanical, photocopying, recording,  
or otherwise), or for any purpose, without the express written permission of 
Microsoft Corporation. 
Microsoft may have patents, patent applications, trademarks, copyrights, or 
other intellectual property rights covering subject matter in this document. 
Except as expressly provided in any written license agreement from Microsoft, 
the furnishing of this document does not give you any license to these patents, 
trademarks, copyrights, or other intellectual property.
© 2010 Microsoft Corporation. All rights reserved. 
Microsoft, MSDN, Visual Basic, Visual C#, Visual Studio, Windows, Windows 
Live, Windows Server, and Windows Vista are trademarks of the Microsoft 
group of companies. 
All other trademarks are property of their respective owners.

Contents
Foreword	
xi	
Tony Hey
Preface	
xiii
Who This Book Is For	
xiii
Why This Book Is Pertinent Now	
xiv
What You Need to Use the Code	
xiv
How to Use This Book	
xv
Introduction	
xvi
Parallelism with Control Dependencies Only	
xvi
Parallelism with Control and Data Dependencies	
xvi
Dynamic Task Parallelism and Pipelines	
xvi
Supporting Material	
xvii
What Is Not Covered	
xviii
Goals	
xviii
Acknowledgments	
xix
1	
Introduction	
1
The Importance of Potential Parallelism	
2
Decomposition, Coordination,  
and Scalable Sharing	
3
Understanding Tasks	
3
Coordinating Tasks	
4
Scalable Sharing of Data	
5
Design Approaches	
6
Selecting the Right Pattern	
7
A Word About Terminology	
7
The Limits of Parallelism	
8
A Few Tips 	
10
Exercises	
11
For More Information	
11

vi
2	
Parallel Loops	
13
The Basics	
14
Parallel for Loops	
14
Parallel for Each	
15
Parallel Linq (PLINQ)	
16
What to Expect	
16
An Example	
18
Sequential Credit Review Example	
19
Credit Review Example Using  
Parallel.For Each	
19
Credit Review Example with PLINQ	
20
Performance Comparison 	
21
Variations	
21
Breaking Out of Loops Early	
21
Parallel Break	
21
Parallel Stop	
23
External Loop Cancellation	
24
Exception Handling	
26
Special Handling of Small Loop Bodies	
26
Controlling the Degree of Parallelism	
28
Using Task-Local State in a Loop Body	
29
Using a Custom Task Scheduler  
For a Parallel Loop	
31
Anti-Patterns	
32
Step Size Other than One	
32
Hidden Loop Body Dependencies	
32
Small Loop Bodies with Few Iterations	
32
Processor Oversubscription  
And Undersubscription	
33
Mixing the Parallel Class and PLINQ	
33
Duplicates in the Input Enumeration	
34
Design Notes	
34
Adaptive Partitioning	
34
Adaptive Concurrency	
34
Support for Nested Loops and Server Applications	
35
Related Patterns	
35
Exercises	
35
Further Reading	
37
3	
Parallel Tasks	
39
	
The Basics	
40
An Example	
41

 vii
Variations	
43
Canceling a Task	
43
Handling Exceptions	
44
Ways to Observe an Unhandled Task Exception	
45
Aggregate Exceptions	
45
The Handle Method	
46
The Flatten Method	
47
Waiting for the First Task to Complete	
48
Speculative Execution	
49
Creating Tasks with Custom Scheduling	
50
Anti-Patterns	
51
Variables Captured by Closures	
51
Disposing a Resource Needed by a Task	
52
Avoid Thread Abort	
53
Design Notes	
53
Tasks and Threads	
53
Task Life Cycle	
53
Writing a Custom Task Scheduler	
54
Unobserved Task Exceptions	
55
Relationship Between Data Parallelism  
and Task Parallelism	
56
The Default Task Scheduler	
56
The Thread Pool 	
57
Decentralized Scheduling Techniques	
58
Work Stealing	
59
Top-Level Tasks in the Global Queue	
60
Subtasks in a Local Queue	
60
Inlined Execution of Subtasks	
60
Thread Injection	
61
Bypassing the Thread Pool	
63
Exercises	
64
Further Reading	
65
4	
Parallel Aggregation	
67
The Basics	
68
An Example	
69
Variations	
73
Using Parallel Loops for Aggregation	
73
Using A Range Partitioner for Aggregation	
76
Using Plinq Aggregation with Range Selection	
77
Design Notes	
80
Related Patterns	
82
Exercises	
82
Further Reading	
83

viii
5	
Futures	
85
	
The Basics	
86
Futures	
86
Continuation Tasks	
88
Example: The Adatum Financial Dashboard 	
89
The Business Objects	
91
The Analysis Engine	
92
Loading External Data	
95
Merging	
95
Normalizing	
96
Analysis and Model Creation	
96
Processing Historical Data	
96
Comparing Models	
96
View And View Model	
97
Variations	
97
Canceling Futures and Continuation Tasks	
97
Continue When “At Least One” Antecedent Completes	 97
Using .Net Asynchronous Calls with Futures	
97
Removing Bottlenecks	
98
Modifying the Graph at Run Time	
98
Design Notes	
99
Decomposition into Futures  
And Continuation Tasks	
99
Functional Style	
99
Related Patterns	
100
Pipeline Pattern	
100
Master/Worker Pattern	
100
Dynamic Task Parallelism Pattern	
100
Discrete Event Pattern	
100
Exercises	
101
Further Reading	
101
6	
Dynamic Task Parallelism	
103
	
The Basics	
103
An Example	
105
Variations	
107
Parallel While-Not-Empty 	
107
Task Chaining with Parent/Child Tasks	
108
Design Notes	
109
Exercises	
110
Further Reading	
110

 ix
7	
Pipelines	
113
The Basics	
113
An Example	
117
Sequential Image Processing	
117
The Image Pipeline	
119
Performance Characteristics	
120
Variations	
122
Canceling a Pipeline	
122
Handling Pipeline Exceptions	
124
Load Balancing Using Multiple Producers	
126
Pipelines and Streams	
129
Asynchronous Pipelines	
129
Anti-Patterns	
129
Thread Starvation	
129
Infinite Blocking Collection Waits	
130
Forgetting GetConsumingEnumerable()	
130
Using Other Producer/Consumer  
Collections	
130
Design Notes	
131
Related Patterns	
131
Exercises	
132
Further Reading	
132
Appendices	
a	 Adapting Object-Oriented Patterns   	
133
Structural Patterns	
133
Façade	
134
Example	
134
Guidelines	
134
Decorators	
134
Example	
135
Guidelines	
136
Adapters	
136
Example	
137
Guidelines	
138
Repositories And Parallel Data Access	
138
Example	
139
Guidelines	
139
Singletons and Service Locators	
139
Implementing a Singleton with the Lazy<T> Class	
140
Notes	
141
Guidelines	
141

x
Model-View-ViewModel 	
142
Example	
143
The Dashboard’s User Interface	
144
Guidelines	
147
Immutable Types	
148
Example	
149
Immutable Types as Value Types	
150
Compound Values	
152
Guidelines	
152
Shared Data Classes	
153
Guidelines	
153
Iterators	
154
Example	
154
Lists and Enumerables	
155
Further Reading	
156
Structural Patterns	
156
Singleton	
156
Model-View-ViewModel 	
157
Immutable Types	
158
b	 Debugging and Profiling 
	
Parallel Applications	
159
The Parallel Tasks and Parallel Stacks Windows	
159
The Concurrency Visualizer	
162
Visual Patterns	
167
Oversubscription	
167
Lock Contention and Serialization	
168
Load Imbalance	
169
Further Reading	
172
c	 Technology Overview	
173
Further Reading	
175
Glossary	
177
References	
187
Other Online Sources	
189
Index	
191

xi
Foreword
At its inception some 40 or so years ago, parallel computing was the 
province of experts who applied it to exotic fields, such as high en-
ergy physics, and to engineering applications, such as computational 
fluid dynamics. We’ve come a long way since those early days.
This change is being driven by hardware trends. The days of per-
petually increasing processor clock speeds are now at an end. Instead, 
the increased chip densities that Moore’s Law predicts are being used 
to create multicore processors, or single chips with multiple processor 
cores. Quad-core processors are now common, and this trend will 
continue, with 10’s of cores available on the hardware in the not-too-
distant future.
In the last five years, Microsoft has taken advantage of this tech-
nological shift to create a variety of parallel implementations. These 
include the Windows High Performance Cluster (HPC) technology 
for message-passing interface (MPI) programs, Dryad, which offers a 
Map-Reduce style of parallel data processing, the Windows Azure 
platform, which can supply compute cores on demand, the Parallel 
Patterns Library (PPL) for native code, and the parallel extensions of 
the .NET Framework 4.
Multicore computation affects the whole spectrum of applica-
tions, from complex scientific and design problems to consumer  
applications and new human/computer interfaces. We used to joke 
that “parallel computing is the future, and always will be,” but the 
pessimists have been proven wrong. Parallel computing has at last 
moved from being a niche technology to being center stage for both 
application developers and the IT industry.
But, there is a catch. To obtain any speed-up of an application, 
programmers now have to divide the computational work to make 
efficient use of the power of multicore processors, a skill that still 
belongs to experts. Parallel programming presents a massive challenge 
for the majority of developers, many of whom are encountering it for 
the first time. There is an urgent need to educate them in practical 

xii
ways so that they can incorporate parallelism into their applications.
Two possible approaches are popular with some of my computer 
science colleagues: either design a new parallel programming language 
or develop a “heroic” parallelizing compiler. While both are certainly 
interesting academically, neither has had much success in popularizing 
and simplifying the task of parallel programming for non-experts. In 
contrast, a more pragmatic approach is to provide programmers with 
a library that hides much of parallel programming’s complexity and to 
teach programmers how to use it.
To that end, the Microsoft .NET Framework parallel extensions 
present a higher-level programming model than earlier APIs. Program-
mers can, for example, think in terms of tasks rather than threads and 
can avoid the complexities of managing threads. Parallel Programming 
with Microsoft .NET teaches programmers how to use these libraries 
by putting them in the context of design patterns. As a result, applica-
tion developers can quickly learn to write parallel programs and gain 
immediate performance benefits.
I believe that this book, with its emphasis on parallel design pat-
terns and an up-to-date programming model, represents an important 
first step in moving parallel programming into the mainstream.
Tony Hey
Corporate Vice President, Microsoft Research
foreword

xiii
Preface
This book describes patterns for parallel programming, with code  
examples, that use the new parallel programming support in the  
Microsoft® .NET Framework 4. This support is commonly referred to 
as the Parallel Extensions. You can use the patterns described in this 
book to improve your application’s performance on multicore com-
puters. Adopting the patterns in your code makes your application run 
faster today and also helps prepare for future hardware environments, 
which are expected to have an increasingly parallel computing  
architecture.
Who This Book Is For
The book is intended for programmers who write managed code for 
the .NET Framework on the Microsoft Windows® operating system. 
This includes programmers who write in Microsoft Visual C#® 
development tool, Microsoft Visual Basic® development system, and 
Microsoft Visual F#. No prior knowledge of parallel programming 
techniques is assumed. However, readers need to be familiar with  
features of C# such as delegates, lambda expressions, generic types, 
and Language Integrated Query (LINQ) expressions. Readers should 
also have at least a basic familiarity with the concepts of processes 
and threads of execution.
Note: The examples in this book are written in C# and use the 
features of the .NET Framework 4, including the Task Parallel 
Library (TPL) and Parallel LINQ (PLINQ). However, you can use 
the concepts presented here with other frameworks and libraries 
and with other languages. 
Complete code solutions are posted on CodePlex. See  
http://parallelpatterns.codeplex.com/. There is a C# version  
for every example. In addition to the C# example code, there  
are also versions of the examples in Visual Basic and F#. 

xiv
Why This Book Is Pertinent Now
The advanced parallel programming features that are delivered with 
Visual Studio® 2010 development system make it easier than ever to 
get started with parallel programming.
The Task Parallel Library (TPL) is for .NET programmers who 
want to write parallel programs. It simplifies the process of adding 
parallelism and concurrency to applications. The TPL dynamically 
scales the degree of parallelism to most efficiently use all the proces-
sors that are available. In addition, the TPL assists in the partitioning 
of work and the scheduling of tasks in the .NET thread pool. The  
library provides cancellation support, state management, and other 
services.
Parallel LINQ (PLINQ) is a parallel implementation of LINQ to 
Objects. PLINQ implements the full set of LINQ standard query  
operators as extension methods for the System.Linq namespace and 
has additional operators for parallel operations. PLINQ is a declara-
tive, high-level interface with query capabilities for operations such as 
filtering, projection, and aggregation.
Visual Studio 2010 includes tools for debugging parallel applica-
tions. The Parallel Stacks window shows call stack information for  
all the threads in your application. It lets you navigate between  
threads and stack frames on those threads. The Parallel Tasks window 
resembles the Threads window, except that it shows information 
about each task instead of each thread. The Concurrency Visualizer 
views in the Visual Studio profiler enable you to see how your applica-
tion interacts with the hardware, the operating system, and other 
processes on the computer. You can use the Concurrency Visualizer 
to locate performance bottlenecks, processor underutilization, thread 
contention, cross-core thread migration, synchronization delays, areas 
of overlapped I/O, and other information.
For a complete overview of the parallel technologies available 
from Microsoft, see Appendix C, “Technology Overview.”
What You Need to Use the Code
The code that is used as examples in this book is at http://parallel 
patterns.codeplex.com/. These are the system requirements:
•	 Microsoft Windows Vista® SP1, Windows 7, Microsoft 
Windows Server® 2008, or Windows XP SP3 (32-bit or 64-bit) 
operating system
•	 Microsoft Visual Studio 2010 (Ultimate or Premium edition  
is required for the Concurrency Visualizer, which allows  
you to analyze the performance of your application); this 
includes the .NET Framework 4, which is required to run  
the samples
preface

 xv
How to Use This Book
This book presents parallel programming techniques in terms of  
particular patterns. Figure 1 shows the different patterns and their 
relationships to each other. The numbers refer to the chapters in this 
book where the patterns are described.
figure 1
Parallel programming patterns
After the introduction, the book has one branch that discusses data 
parallelism and another that discusses task parallelism.
Both parallel loops and parallel tasks use only the program’s  
control flow as the means to coordinate and order tasks. The other 
patterns use both control flow and data flow for coordination.  
Control flow refers to the steps of an algorithm. Data flow refers to 
the availability of inputs and outputs.
Data Parallelism
Task Parallelism
Coordinated by
control ﬂow only
Coordinated by control
ﬂow and data ﬂow
5 Futures 
7 Pipelines
6 Dynamic Task Parallelism
4 Parallel Aggregation
2 Parallel Loops
3 Parallel Tasks
1 Introduction

xvi
introduction
Chapter 1 introduces the common problems faced by developers  
who want to use parallelism to make their applications run faster. It  
explains basic concepts and prepares you for the remaining chapters. 
There is a table in the “Design Approaches” section of Chapter 1 that 
can help you select the right patterns for your application.
parallelism with control dependencies only
Chapters 2 and 3 deal with cases where asynchronous operations are 
ordered only by control flow constraints:
•	 Chapter 2, “Parallel Loops.” Use parallel loops when you want 
to perform the same calculation on each member of a collection 
or for a range of indices, and where there are no dependencies 
between the members of the collection. For loops with depen-
dencies, see Chapter 4, “Parallel Aggregation.”
•	 Chapter 3, “Parallel Tasks.” Use parallel tasks when you have 
several distinct asynchronous operations to perform. This chap-
ter explains why tasks and threads serve two distinct purposes.
parallelism with control and  
data dependencies
Chapters 4 and 5 show patterns for concurrent operations that are 
constrained by both control flow and data flow:
•	 Chapter 4, “Parallel Aggregation.” Patterns for parallel aggre-
gation are appropriate when the body of a parallel loop includes 
data dependencies, such as when calculating a sum or searching 
a collection for a maximum value.
•	 Chapter 5, “Futures.” The Futures pattern occurs when opera-
tions produce some outputs that are needed as inputs to other 
operations. The order of operations is constrained by a directed 
graph of data dependencies. Some operations are performed in 
parallel and some serially, depending on when inputs become 
available.
dynamic task parallelism and pipelines
Chapters 6 and 7 discuss some more advanced scenarios:
•	 Chapter 6, “Dynamic Task Parallelism.” In some cases, 
operations are dynamically added to the backlog of work  
as the computation proceeds. This pattern applies to several 
domains, including graph algorithms and sorting.
•	 Chapter 7, “Pipelines.” Use pipelines to feed successive 
outputs of one component to the input queue of another 
component, in the style of an assembly line. Parallelism  
results when the pipeline fills, and when more than one  
component is simultaneously active.
preface

 xvii
supporting material
In addition to the patterns, there are several appendices:
•	 Appendix A, “Adapting Object-Oriented Patterns.” 
This appendix gives tips for adapting some of the common 
object-oriented patterns, such as facades, decorators, and 
repositories, to multicore architectures.
•	 Appendix B, “Debugging and Profiling Parallel Applications.” 
This appendix gives you an overview of how to debug and 
profile parallel applications in Visual Studio 2010.
•	 Appendix C, “Technology Roadmap.” This appendix describes 
the various Microsoft technologies and frameworks for parallel 
programming.
•	 Glossary. The glossary contains definitions of the terms used 
in this book.
•	 References. The references cite the works mentioned in this 
book.
Everyone should read Chapters 1, 2, and 3 for an introduction and 
overview of the basic principles. Although the succeeding material is 
presented in a logical order, each chapter, from Chapter 4 on, can be 
read independently.
Callouts in a distinctive style, such as the one shown in the margin, 
alert you to things you should watch out for.
It’s very tempting to take a new tool or technology and try and 
use it to solve whatever problem is confronting you, regardless of the 
tool’s applicability. As the saying goes, “when all you have is a hammer, 
everything looks like a nail.” The “everything’s a nail” mentality can 
lead to very unfortunate results, which one hopes the bunny in Figure 
2 will be able to avoid.
You also want to avoid unfortunate results in your parallel pro-
grams. Adding parallelism to your application costs time and adds 
complexity. For good results, you should only parallelize the parts of 
your application where the benefits outweigh the costs.
figure 2
“When all you have is a hammer, everything looks like a nail.”
Don’t apply the patterns  
in this book blindly to your 
applications.

xviii
What Is Not Covered
This book focuses more on processor-bound workloads than on  
I/O-bound workloads. The goal is to make computationally intensive 
applications run faster by making better use of the computer’s avail-
able cores. As a result, the book does not focus as much on the issue 
of I/O latency. Nonetheless, there is some discussion of balanced 
workloads that are both processor intensive and have large amounts 
of I/O (see Chapter 7, “Pipelines”). There is also an important example 
for user interfaces in Chapter 5, “Futures,” that illustrates concurrency 
for tasks with I/O.
The book describes parallelism within a single multicore node 
with shared memory instead of the cluster, High Performance  
Computing (HPC) Server approach that uses networked nodes with 
distributed memory. However, cluster programmers who want to take 
advantage of parallelism within a node may find the examples in  
this book helpful, because each node of a cluster can have multiple 
processing units.
Goals
After reading this book, you should be able to:
•	 Answer the questions at the end of each chapter.
•	 Figure out if your application fits one of the book’s patterns 
and, if it does, know if there’s a good chance of implementing  
a straightforward parallel implementation.
•	 Understand when your application doesn’t fit one of these 
patterns. At that point, you either have to do more reading  
and research, or enlist the help of an expert.
•	 Have an idea of the likely causes, such as conflicting  
dependencies or erroneously sharing data between tasks,  
if your implementation of a pattern doesn’t work.
•	 Use the “Further Reading” sections to find more material.
preface

xix
Acknowledgments
Writing a technical book is a communal effort. The patterns & prac-
tices group always involves both experts and the broader community 
in its projects. Although this makes the writing process lengthier and 
more complex, the end result is always more relevant. The authors 
drove this book’s direction and developed its content, but they want 
to acknowledge the other people who contributed in various ways.
The following subject matter experts were key contributors: 
Nicholas Chen, Daniel Dig, Munawar Hafiz, Fredrik Berg Kjolstad and 
Samira Tasharofi, (University of Illinois at Urbana Champaign), Reed 
Copsey, Jr. (C Tech Development Corporation), and Daan Leijen  
(Microsoft Research). Judith Bishop (Microsoft Research) reviewed 
the text and also gave us her valuable perspective as an author. Our 
schedule was aggressive, but the reviewers worked extra hard to help 
us meet it. Thank you.
Jon Jacky (Modeled Computation LLC) created many of the  
programming samples and contributed to the text. Rick Carr (DCB 
Software Testing, Inc) tested the samples and content.
Many other people reviewed sections of the book or gave us 
feedback on early outlines and drafts. They include Chris Tavares, 
Niklas Gustafson, Dana Groff, Wenming Ye, and David Callahan  
(Microsoft), Justin Bozonier (MG-ALFA / Milliman, Inc.), Tim Mattson 
(Intel), Kurt Keutzer (UC Berkeley), Joe Hummel, Ian Griffiths and 
Mike Woodring (Pluralsight, LLC).
There were a great many people who spoke to us about the book 
and provided feedback. They include the attendees at the ParaPLoP 
2010 workshop and TechEd 2010 conference, as well as contributors 
to discussions on the book’s CodePlex site. The work at UC Berkeley 
and University of Illinois at Urbana Champaign was supported in part 
by the Universal Parallel Computing Research Center initiative.
Tiberiu Covaci (Many-core.se) also deserves special mention for 
generating interest in the book during his numerous speaking engage-
ments on “Patterns for Parallel Programming” in the U.S. and Europe.

xx
A team of technical writers and editors worked to make the prose 
readable and interesting. They include Roberta Leibovitz (Modeled 
Computation LLC), Tina Burden (TinaTech Inc.), and RoAnn Corbisier 
(Microsoft).
The innovative visual design concept used for this guide was  
developed by Roberta Leibovitz and Colin Campbell (Modeled  
Computation LLC) who worked with a group of talented designers 
and illustrators. The book design was created by John Hubbard (Eson). 
The cartoons that face the chapters were drawn by the award-winning 
Seattle-based cartoonist Ellen Forney. The technical illustrations were 
done by Katie Niemer (TinaTech Inc.).
acknowledgments

1
Introduction
Parallel programming  
uses multiple cores at  
the same time to improve 
your application’s speed.
Writing parallel programs 
has the reputation of being 
hard, but help has arrived.
The CPU meter shows the problem. One core is running at 100 per-
cent, but all the other cores are idle. Your application is CPU-bound, 
but you are using only a fraction of the computing power of your 
multicore system. What next?
The answer, in a nutshell, is parallel programming. Where you once 
would have written the kind of sequential code that is familiar to all 
programmers, you now find that this no longer meets your perfor-
mance goals. To use your system’s CPU resources efficiently, you need 
to split your application into pieces that can run at the same time.
This is easier said than done. Parallel programming has a  
reputation for being the domain of experts and a minefield of subtle, 
hard-to-reproduce software defects. Everyone seems to have a favor-
ite story about a parallel program that did not behave as expected 
because of a mysterious bug.
These stories should inspire a healthy respect for the difficulty  
of the problems you face in writing your own parallel programs.  
Fortunately, help has arrived. The Microsoft® .NET Framework 4 in-
troduces a new programming model for parallelism that significantly 
simplifies the job. Behind the scenes are supporting libraries with  
sophisticated algorithms that dynamically distribute computations on 
multicore architectures. In addition, Microsoft Visual Studio® 2010 
development system includes debugging and analysis tools to support 
the new parallel programming model.
Proven design patterns are another source of help. This guide  
introduces you to the most important and frequently used patterns 
of parallel programming and gives executable code samples for them, 
using the Task Parallel Library (TPL) and Parallel LINQ (PLINQ). When 
thinking about where to begin, a good place to start is to review the 
patterns in this book. See if your problem has any attributes that 
match the six patterns presented in the following chapters. If it does, 
delve more deeply into the relevant pattern or patterns and study the 
sample code.
1

2
chapter one
Most parallel programs conform to these patterns, and it’s  
very likely you’ll be successful in finding a match to your particular 
problem. If you can’t use these patterns, you’ve probably encountered 
one of the more difficult cases, and you’ll need to hire an expert or 
consult the academic literature.
The code examples for this guide are online at http://parallel 
patterns.codeplex.com.
The Importance of Potential Parallelism
The patterns in this book are ways to express potential parallelism. This 
means that your program is written so that it runs faster when parallel 
hardware is available and roughly the same as an equivalent sequential 
program when it’s not. If you correctly structure your code, the  
run-time environment can automatically adapt to the workload on a 
particular computer. This is why the patterns in this book only express 
potential parallelism. They do not guarantee parallel execution in  
every situation. Expressing potential parallelism is a central organizing 
principle behind the programming model of .NET. It deserves some 
explanation.
Some parallel applications can be written for specific hardware. 
For example, creators of programs for a console gaming platform have 
detailed knowledge about the hardware resources that will be  
available at run time. They know the number of cores and the details 
of the memory architecture in advance. The game can be written to 
exploit the exact level of parallelism provided by the platform. Com-
plete knowledge of the hardware environment is also a characteristic 
of some embedded applications, such as industrial control. The life 
cycle of such programs matches the life cycle of the specific hardware 
they were designed to use.
In contrast, when you write programs that run on general-purpose 
computing platforms, such as desktop workstations and servers, there 
is less predictability about the hardware features. You may not always 
know how many cores will be available. You also may be unable to 
predict what other software could be running at the same time as 
your application.
Even if you initially know your application’s environment, it can 
change over time. In the past, programmers assumed that their  
applications would automatically run faster on later generations of 
hardware. You could rely on this assumption because processor clock 
speeds kept increasing. With multicore processors, clock speeds are 
not increasing with newer hardware as much as in the past. Instead, 
the trend in processor design is toward more cores. If you want your 
application to benefit from hardware advances in the multicore world, 
you need to adapt your programming model. You should expect that 
Declaring the potential  
parallelism of your program 
allows the execution environ-
ment to run it on all available 
cores, whether one or many.
Don’t hard code the degree of 
parallelism in an application. 
You can’t always predict how 
many cores will be available  
at run time.

 3
introduction
the programs you write today will run on computers with many more 
cores within a few years. Focusing on potential parallelism helps to 
“future proof” your program.
Finally, you must plan for these contingencies in a way that does 
not penalize users who might not have access to the latest hardware. 
You want your parallel application to run as fast on a single-core com-
puter as an application that was written using only sequential code. In 
other words, you want scalable performance from one to many cores. 
Allowing your application to adapt to varying hardware capabilities, 
both now and in the future, is the motivation for potential parallelism.
An example of potential parallelism is the parallel loop pattern 
described in Chapter 2, “Parallel Loops.” If you have a for loop that 
performs a million independent iterations, it makes sense to divide 
those iterations among the available cores and do the work in parallel. 
It’s easy to see that how you divide the work should depend on the 
number of cores. For many common scenarios, the speed of the loop 
will be approximately proportional to the number of cores.
Decomposition, Coordination,  
and Scalable Sharing
The patterns in this book contain some common themes. You’ll see 
that the process of designing and implementing a parallel application 
involves three aspects: methods for decomposing the work into dis-
crete units known as tasks, ways of coordinating these tasks as they 
run in parallel, and scalable techniques for sharing the data needed to 
perform the tasks.
The patterns described in this guide are design patterns. You can 
apply them when you design and implement your algorithms and 
when you think about the overall structure of your application.  
Although the example applications are small, the principles they dem-
onstrate apply equally well to the architectures of large applications.
understanding tasks
Tasks are sequential operations that work together to perform a 
larger operation. When you think about how to structure a parallel 
program, it’s important to identify tasks at a level of granularity that 
results in efficient use of hardware resources. If the chosen granular-
ity is too fine, the overhead of managing tasks will dominate. If it’s too 
coarse, opportunities for parallelism may be lost because cores that 
could otherwise be used remain idle. In general, tasks should be  
as large as possible, but they should remain independent of each 
other, and there should be enough tasks to keep the cores busy. You 
may also need to consider the heuristics that will be used for task 
Hardware trends predict 
more cores instead of  
faster clock speeds.
A well-written parallel 
program runs at approxi-
mately the same speed  
as a sequential program  
when there is only one  
core available.
Tasks are sequential units of 
work. Tasks should be large, 
independent, and numerous 
enough to keep all cores busy.

4
chapter one
scheduling. Meeting all these goals sometimes involves design  
tradeoffs. Decomposing a problem into tasks requires a good under-
standing of the algorithmic and structural aspects of your application.
An example of these guidelines is a parallel ray tracing application. 
A ray tracer constructs a synthetic image by simulating the path of 
each ray of light in a scene. The individual ray simulations are a good 
level of granularity for parallelism. Breaking the tasks into smaller 
units, for example, by trying to decompose the ray simulation itself 
into independent tasks, only adds overhead, because the number of 
ray simulations is already large enough to keep all cores occupied. If 
your tasks vary greatly in size, you generally want more of them in 
order to fill in the gaps.
Another advantage to grouping work into larger and fewer tasks 
is that such tasks are often more independent of each other than 
smaller but more numerous tasks. Larger tasks are less likely than 
smaller tasks to share local variables or fields. Unfortunately, in  
applications that rely on large mutable object graphs, such as applica-
tions that expose a large object model with many public classes, 
methods, and properties, the opposite may be true. In these cases, the 
larger the task, the more chance there is for unexpected sharing of 
data or other side effects.
The overall goal is to decompose the problem into independent 
tasks that do not share data, while providing sufficient tasks to  
occupy the number of cores available. When considering the number 
of cores, you should take into account that future generations of 
hardware will have more cores.
coordinating tasks
It’s often possible that more than one task can run at the same time. 
Tasks that are independent of one another can run in parallel, while 
some tasks can begin only after other tasks complete. The order of 
execution and the degree of parallelism are constrained by the appli-
cation’s underlying algorithms. Constraints can arise from control 
flow (the steps of the algorithm) or data flow (the availability of inputs 
and outputs).
Various mechanisms for coordinating tasks are possible. The way 
tasks are coordinated depends on which parallel pattern you use. For 
example, the pipeline pattern described in Chapter 7, “Pipelines,” is 
distinguished by its use of concurrent queues to coordinate tasks. 
Regardless of the mechanism you choose for coordinating tasks, in 
order to have a successful design, you must understand the dependen-
cies between tasks.
Keep in mind that tasks  
are not threads. Tasks and 
threads take very different 
approaches to scheduling. 
Tasks are much more compat-
ible with the concept of 
potential parallelism than 
threads are. While a new 
thread immediately introduces 
additional concurrency to your 
application, a new task 
introduces only the potential 
for additional concurrency. A 
task’s potential for additional 
concurrency will be realized 
only when there are enough 
available cores.

 5
introduction
scalable sharing of data
Tasks often need to share data. The problem is that when a program 
is running in parallel, different parts of the program may be racing 
against each other to perform updates on the same location of 
memory. The result of such unintended data races can be catastroph-
ic. The solution to the problem of data races includes techniques for 
synchronizing threads.
You may already be familiar with techniques that synchronize 
concurrent threads by blocking their execution in certain circum-
stances. Examples include locks, atomic compare-and-swap opera-
tions, and semaphores. All of these techniques have the effect of  
serializing access to shared resources. Although your first impulse for 
data sharing might be to add locks or other kinds of synchronization, 
adding synchronization reduces the parallelism of your application. 
Every form of synchronization is a form of serialization. Your tasks 
can end up contending over the locks instead of doing the work you 
want them to do. Programming with locks is also error-prone.
Fortunately, there are a number of techniques that allow data to 
be shared that don’t degrade performance or make your program 
prone to error. These techniques include the use of immutable, read-
only data, limiting your program’s reliance on shared variables, and 
introducing new steps in your algorithm that merge local versions of 
mutable state at appropriate checkpoints. Techniques for scalable 
sharing may involve changes to an existing algorithm.
Conventional object-oriented designs can have complex and 
highly interconnected in-memory graphs of object references. As a 
result, traditional object-oriented programming styles can be very 
difficult to adapt to scalable parallel execution. Your first impulse 
might be to consider all fields of a large, interconnected object graph 
as mutable shared state, and to wrap access to these fields in serial-
izing locks whenever there is the possibility that they may be shared 
by multiple tasks. Unfortunately, this is not a scalable approach to 
sharing. Locks can often negatively affect the performance of all 
cores. Locks force cores to pause and communicate, which takes time, 
and they introduce serial regions in the code, which reduces the  
potential for parallelism. As the number of cores gets larger, the cost 
of lock contention can increase. As more and more tasks are added 
that share the same data, the overhead associated with locks can 
dominate the computation.
In addition to performance problems, programs that rely on com-
plex synchronization are prone to a variety of problems, including 
deadlock. This occurs when two or more tasks are waiting for each 
other to release a lock. Most of the horror stories about parallel  
programming are actually about the incorrect use of shared mutable 
state or locking protocols.
For more about the impor-
tance of immutable types in 
parallel programs, see the 
section, “Immutable Types,”  
in Appendix A.
Scalable sharing may involve 
changes to your algorithm.
Adding synchronization 
(locks) can reduce the 
scalability of your  
application.

6
chapter one
Nonetheless, synchronizing elements in an object graph plays a 
legitimate, if limited, role in scalable parallel programs. This book uses 
synchronization sparingly. You should, too. Locks can be thought of 
as the goto statements of parallel programming: they are error prone 
but necessary in certain situations, and they are best left, when  
possible, to compilers and libraries.
No one is advocating the removal, in the name of performance, of 
synchronization that’s necessary for correctness. First and foremost, 
the code still needs to be correct. However, it’s important to incorpo-
rate design principles into the design process that limit the need for 
synchronization. Don’t add synchronization to your application as an 
afterthought.
design approaches
It’s common for developers to identify one problem area, parallelize 
the code to improve performance, and then repeat the process for the 
next bottleneck. This is a particularly tempting approach when you 
parallelize an existing sequential application. Although this may give 
you some initial improvements in performance, it has many pitfalls, 
and it may not produce the best results. A far better approach is to 
understand your problem or application and look for potential  
parallelism across the entire application as a whole. What you dis-
cover may lead you to adopt a different architecture or algorithm that 
better exposes the areas of potential parallelism in your application. 
Don’t simply identify bottlenecks and parallelize them. Instead, pre-
pare your program for parallel execution by making structural changes.
Techniques for decomposition, coordination, and scalable sharing 
are interrelated. There’s a circular dependency. You need to consider 
all of these aspects together when choosing your approach for a  
particular application.
After reading the preceding description, you might complain that 
it all seems vague. How specifically do you divide your problem into 
tasks? Exactly what kinds of coordination techniques should you use? 
Questions like these are best answered by the patterns described 
in this book. Patterns are a true shortcut to understanding. As you 
begin to see the design motivations behind the patterns, you will also 
develop your intuition about how the patterns and their variations can 
be applied to your own applications. The following section gives more 
details about how to select the right pattern.
Think in terms of data 
structures and algorithms; 
don’t just identify bottlenecks.
Use patterns.

 7
introduction
Selecting the Right Pattern
To select the relevant pattern, use the following table.
Application characteristic
Relevant pattern
Do you have sequential loops where there’s no  
communication among the steps of each iteration?
The Parallel Loop pattern (Chapter 2).
Parallel loops apply an independent operation to multiple 
inputs simultaneously.
Do you have distinct operations with well-defined  
control dependencies? Are these operations largely free  
of serializing dependencies?
The Parallel Task pattern (Chapter 3)
Parallel tasks allow you to establish parallel control flow  
in the style of fork and join.
Do you need to summarize data by applying some kind  
of combination operator? Do you have loops with steps 
that are not fully independent? 
The Parallel Aggregation pattern (Chapter 4)
Parallel aggregation introduces special steps in the 
algorithm for merging partial results. This pattern 
expresses a reduction operation and includes map/reduce 
as one of its variations.
Does the ordering of steps in your algorithm depend  
on data flow constraints? 
The Futures pattern (Chapter 5)
Futures make the data flow dependencies between tasks 
explicit. This pattern is also referred to as the Task Graph 
pattern.
Does your algorithm divide the problem domain  
dynamically during the run? Do you operate on recursive 
data structures such as graphs? 
The Dynamic Task Parallelism pattern (Chapter 6) 
This pattern takes a divide-and-conquer approach and 
spawns new tasks on demand.
Does your application perform a sequence of operations 
repetitively? Does the input data have streaming  
characteristics? Does the order of processing matter?
The Pipelines pattern (Chapter 7) 
Pipelines consist of components that are connected by 
queues, in the style of producers and consumers. All  
the components run in parallel even though the order  
of inputs is respected.
One way to become familiar with the possibilities of the six patterns 
is to read the first page or two of each chapter. This gives you an 
overview of approaches that have been proven to work in a wide va-
riety of applications. Then go back and more deeply explore patterns 
that may apply in your situation.
A Word About Terminology
You’ll often hear the words parallelism and concurrency used as syn-
onyms. This book makes a distinction between the two terms.
Concurrency is a concept related to multitasking and asynchro-
nous input-output (I/O). It usually refers to the existence of multiple 
threads of execution that may each get a slice of time to execute  
before being preempted by another thread, which also gets a slice of 
time. Concurrency is necessary in order for a program to react to  
external stimuli such as user input, devices, and sensors. Operating  
systems and games, by their very nature, are concurrent, even on  
one core.

8
chapter one
With parallelism, concurrent threads execute at the same time on 
multiple cores. Parallel programming focuses on improving the perfor-
mance of applications that use a lot of processor power and are not 
constantly interrupted when multiple cores are available.
The goals of concurrency and parallelism are distinct. The main 
goal of concurrency is to reduce latency by never allowing long peri-
ods of time to go by without at least some computation being  
performed by each unblocked thread. In other words, the goal of 
concurrency is to prevent thread starvation.
Concurrency is required operationally. For example, an operating 
system with a graphical user interface must support concurrency if 
more than one window at a time can update its display area on a sin-
gle-core computer. Parallelism, on the other hand, is only about 
throughput. It’s an optimization, not a functional requirement. Its goal 
is to maximize processor usage across all available cores; to do this, it 
uses scheduling algorithms that are not preemptive, such as algorithms 
that process queues or stacks of work to be done.
The Limits of Parallelism
A theoretical result known as Amdahl’s law says that the amount of 
performance improvement that parallelism provides is limited by the 
amount of sequential processing in your application. This may, at first, 
seem counterintuitive.
Amdahl’s law says that no matter how many cores you have, the 
maximum speedup you can ever achieve is (1 / percent of time spent 
in sequential processing). Figure 1 illustrates this.
figure 1
Amdahl’s law for an 
application with 25 
percent sequential 
processing
Execution Speed
4
3.5
3
2.5
2
1.5
1
0.5
0
0
6
11
16
Number of processors

 9
introduction
For example, with 11 processors, the application runs slightly more 
than three times faster than it would if it were entirely sequential.
Even with fewer cores, you can see that the expected speedup is 
not linear. Figure 2 illustrates this.
figure 2
Per-core performance 
improvement for a 25 
percent sequential 
application
Figure 2 shows that as the number of cores (and overall application 
speed) increases the percentage of time spent in the sequential part 
of the application increases. (The elapsed time spent in sequential 
processing is constant.) The illustration also shows why you might be 
satisfied with a 2x speedup on a four-core computer for actual ap-
plications, as opposed to sample programs. The important question is 
always how scalable the application is. Scalability depends on the 
amount of time spent doing work that is inherently sequential in na-
ture.
Another implication of Amdahl’s law is that for some problems, 
you may want to create additional features in the parts of an applica-
tion that are amenable to parallel execution. For example, a developer 
of a computer game might find that it’s possible to make increasingly 
sophisticated graphics for newer multicore computers by using the 
parallel hardware, even if it’s not as feasible to make the game logic 
(the artificial intelligence engine) run in parallel. Performance can in-
fluence the mix of application features.
The speedup you can achieve in practice is usually somewhat 
worse than Amdahl’s law would predict. As the number of cores  
% Parallel
% Sequential
KEY
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Number of cores
Speedup

10
chapter one
increases, the overhead incurred by accessing shared memory also 
increases. Also, parallel algorithms may include overhead for coordina-
tion that would not be necessary for the sequential case. Profiling 
tools, such as the Visual Studio Concurrency Visualizer, can help you 
understand how effective your use of parallelism is.
In summary, because an application consists of parts that must 
run sequentially as well as parts that can run in parallel, the application 
overall will rarely see a linear increase in performance with a linear 
increase in the number of cores, even if certain parts of the applica-
tion see a near linear speedup. Understanding the structure of your 
application, and its algorithms—that is, which parts of your applica-
tion are suitable for parallel execution—is a step that can’t be skipped 
when analyzing performance.
A Few Tips 
Always try for the simplest approach. Here are some basic precepts:
•	 Whenever possible, stay at the highest possible level of abstrac-
tion and use constructs or a library that does the parallel work 
for you.
•	 Use your application server’s inherent parallelism; for example, 
use the parallelism that is incorporated into a web server or 
database.
•	 Use an API to encapsulate parallelism, such as Microsoft Parallel 
Extensions for .NET (TPL and PLINQ). These libraries were 
written by experts and have been thoroughly tested; they help 
you to avoid many of the common problems that arise in parallel 
programming.
•	 Consider the overall architecture of your application when 
thinking about how to parallelize it. It’s tempting to simply look 
for the performance hotspots and focus on improving them. 
While this may improve things, it does not necessarily give you 
the best results.
•	 Use patterns, such as the ones described in this book.
•	 Often, restructuring your algorithm (for example, to eliminate 
the need for shared data) is better than making low-level 
improvements to code that was originally designed to run 
serially.
•	 Don’t share data among concurrent tasks unless absolutely 
necessary. If you do share data, use one of the containers 
provided by the API you are using, such as a shared queue.
•	 Use low-level primitives, such as threads and locks, only as  
a last resort. Raise the level of abstraction from threads to  
tasks in your applications.

 11
introduction
Exercises
1.	 What are some of the tradeoffs between decomposing  
a problem into many small tasks versus decomposing it  
into larger tasks?
2.	 What is the maximum potential speedup of a program  
that spends 10 percent of its time in sequential processing 
when you move it from one to four cores?
3.	 What is the difference between parallelism and  
concurrency?
For More Information
If you are interested in better understanding the terminology used in 
the text, refer to the glossary at the end of this book.
The design patterns presented in this book are consistent with 
classifications of parallel patterns developed by groups in both indus-
try and academia. In the terminology of these groups, the patterns in 
this book would be considered to be algorithm or implementation 
patterns. Classification approaches for parallel patterns can be found 
in the book by Mattson, et al. and at the Our Pattern Language (OPL) 
web site. This book attempts to be consistent with the terminology 
of these sources. In cases where this is not possible, an explanation 
appears in the text.
For a detailed discussion of parallelism on the Windows platform, 
see the book by Duffy. An overview of threading and synchronization 
in .NET can be found in Albahari.
J. Albahari and B. Albahari. C# 4 in a Nutshell. O’Reilly, fourth 
edition, 2010.
J. Duffy. Concurrent Programming on Windows. Addison-Wesley, 
2008.
T. G. Mattson, B. A. Sanders, and B. L. Massingill. Patterns for 
Parallel Programming. Addison-Wesley, 2004.
“Our Pattern Language for Parallel Programming Ver 2.0.” 
http://parlab.eecs.berkeley.edu/wiki/patterns


13
Parallel Loops
2
The Parallel Loop pattern 
independently applies an 
operation to multiple data 
elements. It’s an example  
of data parallelism.
Use the Parallel Loop pattern when you need to perform the same 
independent operation for each element of a collection or for a fixed 
number of iterations. The steps of a loop are independent if they 
don’t write to memory locations or files that are read by other steps.
The syntax of a parallel loop is very similar to the for and foreach 
loops you already know, but the parallel loop runs faster on a com-
puter that has available cores. Another difference is that, unlike a se-
quential loop, the order of execution isn’t defined for a parallel loop. 
Steps often take place at the same time, in parallel. Sometimes, two 
steps take place in the opposite order than they would if the loop 
were sequential. The only guarantee is that all of the loop’s iterations 
will have run by the time the loop finishes.
It’s easy to change a sequential loop into a parallel loop. However, 
it’s also easy to use a parallel loop when you shouldn’t. This is because 
it can be hard to tell if the steps are actually independent of each 
other. It takes practice to learn how to recognize when one step is 
dependent on another step. Sometimes, using this pattern on a loop 
with dependent steps causes the program to behave in a completely 
unexpected way, and perhaps to stop responding. Other times, it in-
troduces a subtle bug that only appears once in a million runs. In 
other words, the word “independent” is a key part of the definition of 
this pattern, and one that this chapter explains in detail.
For parallel loops, the degree of parallelism doesn’t need to be 
specified by your code. Instead, the run-time environment executes 
the steps of the loop at the same time on as many cores as it can. The 
loop works correctly no matter how many cores are available. If there 
is only one core, the performance is close to (perhaps within a few 
percentage points of) the sequential equivalent. If there are multiple 
cores, performance improves; in many cases, performance improves 
proportionately with the number of cores.

14
chapter two
The Basics
The .NET Framework includes both parallel For and parallel ForEach 
loops and is also implemented in the Parallel LINQ (PLINQ) query 
language. Use the Parallel.For method to iterate over a range of inte-
ger indices and the Parallel.ForEach method to iterate over user-
provided values. Use PLINQ if you prefer a high-level, declarative style 
for describing loops or if you want to take advantage of PLINQ’s 
convenience and flexibility.
parallel for loops
Here’s an example of a sequential for loop in C#. 
int n = ...
for (int i = 0; i < n; i++)
{
   // ... 
}
To take advantage of multiple cores, replace the for keyword with a 
call to the Parallel.For method and convert the body of the loop into 
a lambda expression.
int n = ...
Parallel.For(0, n, i =>
{
   // ... 
});
Parallel.For is a static method with overloaded versions. Here’s the 
signature of the version of Parallel.For that’s used in the example.
Parallel.For(int fromInclusive, 
             int toExclusive, 
             Action<int> body);
In the example, the first two arguments specify the iteration limits. 
The first argument is the lowest index of the loop. The second argu-
ment is the exclusive upper bound, or the largest index plus one. The 
third argument is an action that’s invoked once per iteration. The ac-
tion takes the iteration’s index as its argument and executes the loop 
body once for each index.
The Parallel.For method has additional overloaded versions. 
These are covered in the section, “Variations,” later in this chapter and 
in Chapter 4, “Parallel Aggregation.”
The example includes a lambda expression in the form args => 
body as the third argument to the Parallel.For invocation. Lambda 
expressions are unnamed methods that can capture variables from 
To make for and foreach 
loops with independent 
iterations run faster on 
multicore computers, use 
their parallel counterparts.
Don’t forget that the steps  
of the loop body must be 
independent of one another  
if you want to use a parallel 
loop. The steps must not 
communicate by writing  
to shared variables.
Parallel.For uses multiple 
cores to operate over an index 
range.
The Parallel.For method does 
not guarantee any particular 
order of execution. Unlike a 
sequential loop, some 
higher-valued indices may be 
processed before some 
lower-valued indices.

 15
parallel loops
their enclosing scope. Of course, the body parameter could also be an 
instance of a delegate type, an anonymous method (using the delegate 
keyword) or an ordinary named method. In other words, you don’t 
have to use lambda expressions if you don’t want to. Examples in this 
book use lambda expressions because they keep the code within the 
body of the loop, and they are easier to read when the number of lines 
of code is small.
parallel for each
Here’s an example of a sequential foreach loop in C#.
IEnumerable<MyObject> myEnumerable = ...
foreach (var obj in myEnumerable)
{
   // ... 
}
To take advantage of multiple cores, replace the foreach keyword 
with a call to the Parallel.ForEach method.
IEnumerable<MyObject> myEnumerable = ...
Parallel.ForEach(myEnumerable, obj =>
{
   // ... 
});
Parallel.ForEach is a static method with overloaded versions. Here’s 
the signature of the version of Parallel.ForEach that was used in the 
example.
ForEach<TSource>(IEnumerable<TSource> source, 
                 Action<TSource> body);
In the example, the first argument is an object that implements the 
IEnumerable<MyObject> interface. The second argument is a method 
that’s invoked for each element of the input collection.
The Parallel.ForEach method does not guarantee the order of 
execution. Unlike a sequential ForEach loop, the incoming values 
aren’t always processed in order.
The Parallel.ForEach method has additional overloaded versions. 
These are covered in the section, “Variations,” later in this chapter and 
in Chapter 4, “Parallel Aggregation.”
If you’re unfamiliar with the 
syntax for lambda expressions, 
see “Further Reading” at the 
end of this chapter. After you 
use lambda expressions, you’ll 
wonder how you ever lived 
without them.
Parallel.ForEach runs the 
loop body for each element in 
a collection.
Don’t forget that iterations 
need to be independent. The 
loop body must only make 
updates to fields of the 
particular instance that’s 
passed to it.

16
chapter two
parallel linq (plinq)
The Language Integrated Query (LINQ) feature of the .NET Frame-
work includes a parallel version named PLINQ (Parallel LINQ). There 
are many options and variations for expressing PLINQ queries but al-
most all LINQ-to-Objects expressions can easily be converted to their 
parallel counterpart by adding a call to the AsParallel extension 
method. Here’s an example that shows both the LINQ and PLINQ 
versions.
IEnumerable<MyObject> source = ...
// LINQ
var query1 = from i in source select Normalize(i);
// PLINQ
var query2 = from i in source.AsParallel() 
                                       select Normalize(i);
This code example creates two queries that transform values of the 
enumerable object source. The PLINQ version uses multiple cores if 
they’re available.
You can also use PLINQ’s ForAll extension method in cases 
where you want to iterate over the input values but you don’t want 
to select output values to return. This is shown in the following code.
IEnumerable<MyObject> myEnumerable = ...
myEnumerable.AsParallel().ForAll(obj => DoWork(obj));
The ForAll extension method is the PLINQ equivalent of Parallel.
ForEach.
what to expect
By default, the degree of parallelism (that is, how many iterations run 
at the same time in hardware) depends on the number of available 
cores. In typical scenarios, the more cores you have, the faster your 
loop executes, until you reach the point of diminishing returns that 
Amdahl’s Law predicts. How much faster depends on the kind of 
work your loop does.
The .NET implementation of the Parallel Loop pattern ensures 
that exceptions that are thrown during the execution of a loop body 
are not lost. For both the Parallel.For and Parallel.ForEach methods 
as well as for PLINQ, exceptions are collected into an AggregateEx-
ception object and rethrown in the context of the calling thread. All 
exceptions are propagated back to you. To learn more about excep-
tion handling for parallel loops, see the section, “Variations,” later in 
this chapter.
You can convert LINQ 
expressions to parallel code 
with the AsParallel 
extension method.
It’s important to use PLINQ’s 
ForAll extension method 
instead of giving a PLINQ 
query as an argument to the 
Parallel.ForEach method. For 
more information, see the 
section, “Mixing the Parallel 
Class and PLINQ,” later in  
this chapter.
Adding cores makes your loop 
run faster; however, there’s 
always an upper limit.
You must choose the correct 
granularity. Too many small 
parallel loops can reach a point 
of over-decomposition where 
the multicore speedup is more 
than offset by the parallel 
loop’s overhead.

 17
parallel loops
Parallel loops have many variations. There are 12 overloaded 
methods for Parallel.For and 20 overloaded methods for Parallel.
ForEach. PLINQ has close to 200 extension methods. Although there 
are many overloaded versions of For and ForEach, you can think of 
the overloads as providing optional configuration options. Two ex-
amples are a maximum degree of parallelism and hooks for external 
cancellation. These options allow the loop body to monitor the prog-
ress of other steps (for example, to see if exceptions are pending) and 
to manage task-local state. They are sometimes needed in advanced 
scenarios. To learn about the most important cases, see the section, 
“Variations,” later in this chapter.
If you convert a sequential loop to a parallel loop and then find 
that your program does not behave as expected, the mostly likely 
problem is that the loop’s steps are not independent. Here are some 
common examples of dependent loop bodies:
•	 Writing to shared variables. If the body of a loop writes to 
a shared variable, there is a loop body dependency. This is a 
common case that occurs when you are aggregating values.  
Here is an example, where total is shared across iterations.
for(int i = 1; i < n; i++)
  total += data[i];
If you encounter this situation, see Chapter 4, “Parallel Aggregation.”
Shared variables come in many flavors. Any variable that is 
declared outside of the scope of the loop body is a shared 
variable. Shared references to types such as classes or arrays  
will implicitly allow all fields or array elements to be shared. 
Parameters that are declared using the keyword ref result in 
shared variables. Even reading and writing files can have the 
same effect as shared variables.
•	 Using properties of an object model. If the object being 
processed by a loop body exposes properties, you need to  
know whether those properties refer to shared state or state 
that’s local to the object itself. For example, a property named 
Parent is likely to refer to global state. Here’s an example.
for(int i = 0; i < n; i++)
  SomeObject[i].Parent.Update();
In this example, it’s likely that the loop iterations are not independent. 
For all values of i, SomeObject[i].Parent is a reference to a single 
shared object.
Robust exception handling  
is an important aspect of 
parallel loop processing.
Check carefully for dependen-
cies between loop iterations! 
Not noticing dependencies 
between steps is by far the 
most common mistake you’ll 
make with parallel loops.

18
chapter two
•	 Referencing data types that are not thread safe. If the body of 
the parallel loop uses a data type that is not thread safe, the 
loop body is not independent (there is an implicit dependency 
on the thread context). An example of this case, along with a 
solution, is shown in “Using Task-Local State in a Loop Body” in 
the section, “Variations,” later in this chapter.
•	 Loop-carried dependence. If the body of a parallel for loop 
performs arithmetic on the loop index, there is likely to be a 
dependency that is known as loop-carried dependence. This is 
shown in the following code example. The loop body references 
data[i] and data[i – 1]. If Parallel.For is used here, there’s no 
guarantee that the loop body that updates data[i – 1] has 
executed before the loop for data[i].
for(int i = 1; i < N; i++)
  data[i] = data[i] + data[i - 1]; 
Sometimes, it’s possible to use a parallel algorithm in cases of 
loop-carried dependence, but this is outside the scope of this 
book. Your best bet is to look elsewhere in your program for 
opportunities for parallelism or to analyze your algorithm and 
see if it matches some of the advanced parallel patterns that 
occur in scientific computing. Parallel scan and parallel dynamic 
programming are examples of these patterns.
When you look for opportunities for parallelism, profiling your ap-
plication is a way to deepen your understanding of where your  
application spends its time; however, profiling is not a substitute for 
understanding your application’s structure and algorithms. For exam-
ple, profiling doesn’t tell you whether loop bodies are independent.
An Example
Here’s an example of when to use a parallel loop. Fabrikam Shipping 
extends credit to its commercial accounts. It uses customer credit 
trends to identify accounts that might pose a credit risk. Each cus-
tomer account includes a history of past balance-due amounts. Fabri-
kam has noticed that customers who don’t pay their bills often have 
histories of steadily increasing balances over a period of several 
months before they default.
To identify at-risk accounts, Fabrikam uses statistical trend analy-
sis to calculate a projected credit balance for each account. If the 
analysis predicts that a customer account will exceed its credit limit 
within three months, the account is flagged for manual review by one 
of Fabrikam’s credit analysts. 
Arithmetic on loop index 
variables, especially addition or 
subtraction, usually indicates 
loop-carried dependence.
Don’t expect miracles from 
profiling—it can’t analyze your 
algorithms for you. Only you 
can do that.
You must be extremely 
cautious when getting data 
from properties and methods. 
Large object models are known 
for sharing mutable state in 
unbelievably devious ways.

 19
parallel loops
In the application, a top-level loop iterates over customers in the 
account repository. The body of the loop fits a trend line to the bal-
ance history, extrapolates the projected balance, compares it to the 
credit limit, and assigns the warning flag if necessary.
An important aspect of this application is that each customer’s 
credit status can be independently calculated. The credit status of one 
customer doesn’t depend on the credit status of any other customer. 
Because the operations are independent, making the credit analysis 
application run faster is simply a matter of replacing a sequential 
foreach loop with a parallel loop.
The complete source code for this example is online at http://
parallelpatterns.codeplex.com in the Chapter2\CreditReview project.
sequential credit review example
Here’s the sequential version of the credit analysis operation.
static void UpdatePredictionsSequential(
                                   AccountRepository accounts)
{
  foreach (Account account in accounts.AllAccounts)
  {
    Trend trend = SampleUtilities.Fit(account.Balance);
    double prediction = trend.Predict(
                     account.Balance.Length + NumberOfMonths); 
    account.SeqPrediction = prediction;
    account.SeqWarning = prediction < account.Overdraft;
  }
}
The UpdatePredictionsSequential method processes each account 
from the application’s account repository. The Fit method is a utility 
function that uses the statistical least squares method to create a 
trend line from an array of numbers. The Fit method is a pure func-
tion. This means that it doesn’t modify any state.
The prediction is a three-month projection based on the trend. If 
a prediction is more negative than the overdraft limit (credit balances 
are negative numbers in the accounting system), the account is flagged 
for review.
credit review example using  
parallel.for each
The parallel version of the credit scoring analysis is very similar to the 
sequential version.

20
chapter two
static void UpdatePredictionsParallel(AccountRepository accounts)
{
  Parallel.ForEach(accounts.AllAccounts, account =>
  {
    Trend trend = SampleUtilities.Fit(account.Balance);
    double prediction = trend.Predict(
                       account.Balance.Length + NumberOfMonths);
    account.ParPrediction = prediction;
    account.ParWarning = prediction < account.Overdraft;
  });
}
The UpdatePredictionsParallel method is identical to the Up-
datePredictionsSequential method, except that the Parallel.ForEach 
method replaces the foreach operator.
credit review example with plinq
You can also use PLINQ to express a parallel loop. Here’s an example.
static void UpdatePredictionsPlinq(AccountRepository accounts)
{            
  accounts.AllAccounts
  .AsParallel()
  .ForAll(account =>
  {
    Trend trend = SampleUtilities.Fit(account.Balance);
    double prediction = trend.Predict(
                        account.Balance.Length + NumberOfMonths);
    account.PlinqPrediction = prediction;
    account.PlinqWarning = prediction < account.Overdraft;         
   });            
}
Using PLINQ is almost exactly like using LINQ-to-Objects. PLINQ 
provides a ParallelEnumerable class that defines extension methods 
for various types in a manner very similar to LINQ’s Enumerable class. 
One of the methods of ParallelEnumerable is the AsParallel exten-
sion method.
The AsParallel extension method allows you to convert a se-
quential collection of type IEnumerable<T> into a ParallelQuery<T> 
object. Applying AsParallel to the accounts.AllAccounts collection 
returns an object of type ParallelQuery<AccountRecord>.
PLINQ’s ParallelEnumerable class has close to 200 extension 
methods that provide parallel queries for ParallelQuery<T> objects. 
In addition to parallel implementations of LINQ methods, such as 

 21
parallel loops
Use Break to exit a loop 
early while ensuring that 
lower-indexed steps complete.
Select and Where, PLINQ provides a ForAll extension method that 
invokes a delegate method in parallel for every element.
In the PLINQ prediction example, the argument to ForAll is a 
lambda expression that performs the credit analysis for a specified 
account. The body is the same as in the sequential version.
performance comparison 
Running the credit review example on a quad-core computer shows 
that the Parallel.ForEach and PLINQ versions run slightly less than 
four times as fast as the sequential version. Timing numbers vary; you 
may want to run the online samples on your own computer.
Variations
The credit analysis example shows a typical way to use parallel loops, 
but there can be variations. This section introduces some of the most 
important ones. You won’t always need to use these variations, but 
you should be aware that they are available.
breaking out of loops early
Breaking out of loops is a familiar part of sequential iteration. It’s less 
common in parallel loops, but you’ll sometimes need to do it. Here’s 
an example of the sequential case.
int n = ...
for (int i = 0; i < n; i++)
{
  // ... 
  if (/* stopping condition is true */)
    break;   
}
The situation is more complicated with parallel loops because more 
than one step may be active at the same time, and steps of a parallel 
loop are not necessarily executed in any predetermined order. Conse-
quently, parallel loops have two ways to break or stop a loop instead 
of just one. Parallel break allows all steps with indices lower than the 
break index to run before terminating the loop. Parallel stop termi-
nates the loop without allowing any new steps to begin.
Parallel Break
The Parallel.For method has an overload that provides a Parallel
LoopState object as a second argument to the loop body. You can ask 
the loop to break by calling the Break method of the ParallelLoop
State object. Here’s an example.

22
chapter two
int n = ...
Parallel.For(0, n, (i, loopState) =>
{
  // ... 
  if (/* stopping condition is true */)
  {
    loopState.Break();
    return;   
  }
});
This example uses an overloaded version of Parallel.For that passes a 
“loop state” object to each step. Here’s the signature of the version of 
the Parallel.For method that was used in the example.
Parallel.For(int fromInclusive, 
             int toExclusive, 
             Action<int, ParallelLoopState> body);   
The object that’s passed to the loopState argument is an instance of 
the ParallelLoopState class that was created by the parallel loop for 
use within the loop body.
Calling the Break method of the ParallelLoopState object begins 
an orderly shutdown of the loop processing. Any steps that are run-
ning as of the call to Break will run to completion.
You may want to check for a break condition in long-running loop 
bodies and exit that step immediately if a break was requested. If you 
don’t do this, the step will continue to run until it finishes. To see if 
another step running in parallel has requested a break, retrieve the 
value of the parallel loop state’s LowestBreakIteration property. If 
this returns a nullable long integer whose HasValue property is true, 
you know that a break has been requested. You can also read the 
ShouldExitCurrentIteration property of the loop state object, which 
checks for breaks as well as other stopping conditions.
During the processing of a call to the Break method, iterations 
with an index value less than the current index will be allowed to start 
(if they have not already started), but iterations with an index value 
greater than the current index will not be started. This ensures that all 
iterations below the break point will complete.
Because of parallel execution, it’s possible that more than one 
step may call Break. In that case, the lowest index will be used 
to determine which steps will be allowed to start after the break  
occurred.
The Parallel.For and Parallel.ForEach methods return an object 
of type ParallelLoopResult. You can find out if a loop terminated 
with a break by examining the values of two of the loop result proper-
Calling Break doesn’t stop 
other steps that might have 
already started running.
Don’t forget that all steps with 
an index value less than the 
step that invoked the Break 
method will be allowed to run 
normally, even after you call 
Break.

 23
parallel loops
ties. If the IsCompleted property is false and the LowestBreak
Iteration property returns an object whose HasValue property is 
true, you know that the loop terminated by a call to the Break 
method. You can query for the specific index with the loop result’s 
LowestBreakIteration property. Here’s an example.
int n = ...
var result = new double[n];
var loopResult = Parallel.For(0, n, (i, loopState) =>
{
   if (/* break condition is true */)
   {
      loopState.Break();
      return;
   }
   result[i] = DoWork(i);
});
if (!loopResult.IsCompleted && 
        loopResult.LowestBreakIteration.HasValue)
{
   Console.WriteLine(“Loop encountered a break at {0}”, 
                      loopResult.LowestBreakIteration.Value);
}
The Break method ensures that data up to a particular iteration index 
value will be processed. Depending on how the iterations are sched-
uled, it may be possible that some steps with a higher index value than 
the one that called the Break method may have been started before 
the call to Break occurs.
The Parallel.ForEach method also supports the loop state Break 
method. The parallel loop assigns items a sequence number, starting 
from zero, as it pulls them from the enumerable input. This sequence 
number is used as the iteration index for the LowestBreakIteration 
property.
Parallel Stop
There are also situations, such as unordered searches, where you want 
the loop to stop as quickly as possible after the stopping condition is 
met. The difference between “break” and “stop” is that, with stop, no 
attempt is made to execute loop iterations less than the stopping in-
dex if they have not already run. To stop a loop in this way, call the 
ParallelLoopState class’s Stop method instead of the Break method. 
Here is an example of parallel stop.
Be aware that some steps with 
index values higher than the 
step that called the Break 
method might be run. There’s 
no way of predicting when or 
if this might happen.
The Parallel.ForEach 
method also supports the  
loop state Break method.
Use Stop to exit a loop early 
when you don’t need all 
lower-indexed iterations  
to run before terminating  
the loop.

24
chapter two
var n = ...
var loopResult = Parallel.For(0, n, (i, loopState) =>
{
   if (/* stopping condition is true */)
   {
      loopState.Stop();
      return;
   }
   result[i] = DoWork(i);
});
if (!loopResult.IsCompleted && 
         !loopResult.LowestBreakIteration.HasValue)
{
   Console.WriteLine(“Loop was stopped”);
}
When the Stop method is called, the index value of the iteration 
that caused the stop isn’t available.
You cannot call both Break and Stop during the same parallel 
loop. You have to choose which of the two loop exit behaviors you 
want to use. If you call both Break and Stop in the same parallel loop, 
an exception will be thrown.
Parallel programs use Stop more often than Break. Processing all 
iterations with indices less than the stopping iteration is usually not 
necessary when the loop bodies are independent of each other. It’s 
also true that Stop shuts down a loop more quickly than Break.
There’s no Stop method for a PLINQ query, but you can use the 
WithCancellation extension method and then use cancellation as a 
way to stop PLINQ execution. For more information, see the next 
section, “External Loop Cancellation.” 
external loop cancellation
In some scenarios, you may want to cancel a parallel loop because of 
an external request. For example, you may need to respond to a re-
quest from a user interface to stop what you’re doing.
In .NET, you use the CancellationTokenSource class to signal 
cancellation and the CancellationToken structure to detect and re-
spond to a cancellation request. The structure allows you to find out 
if there is a pending cancellation request. The class lets you signal that 
cancellation should occur.
The Parallel.For and Parallel.ForEach methods include over-
loaded versions that accept parallel loop options as one of the argu-
ments. You can specify a cancellation token as one of these options. 
You’ll probably use Stop 
more often than Break.

 25
parallel loops
If you provide a cancellation token as an option to a parallel loop, the 
loop will use that token to look for a cancellation request. Here’s an 
example.
void DoLoop(CancellationTokenSource cts)
{
  int n = ...
  CancellationToken token = cts.Token;
  var options = new ParallelOptions 
                        { CancellationToken = token };
  try
  {
    Parallel.For(0, n, options, (i) =>
    {
      // ... 
      // ... optionally check to see if cancellation happened
      if (token.IsCancellationRequested)
      {
         // ... optionally exit this iteration early
         return;
      }
    });
  }
  catch (OperationCanceledException ex)
  {
     // ... handle the loop cancellation
  }
}
Here is the signature of the Parallel.For method that was used in the 
example.
Parallel.For(int fromInclusive, 
             int toExclusive, 
             ParallelOptions parallelOptions, 
             Action<int> body);   
When the caller of the DoLoop method is ready to cancel, it invokes 
the Cancel method of the CancellationTokenSource class that was 
provided as an argument to the DoLoop method. The parallel loop 
will allow currently running iterations to complete and then throw an 
OperationCanceledException. No new iterations will start after 
cancellation begins.
External cancellation 
requires a cancellation  
token source object.

26
chapter two
If external cancellation has been signaled and your loop has called 
either the Break or the Stop method of the ParallelLoopState object, 
a race occurs to see which will be recognized first. The parallel loop 
will either throw an OperationCanceledException or it will termi-
nate using the mechanism for Break and Stop that is described in the 
section, “Breaking Out of Loops Early,” earlier in this chapter.
You can use the WithCancellation extension method to add 
external cancellation capabilities to a PLINQ query.
exception handling
If the body of a parallel loop throws an unhandled exception, the 
parallel loop no longer begins any new steps. By default, iterations 
that are executing at the time of the exception, other than the itera-
tion that threw the exception, will complete. After they finish, the 
parallel loop will throw an exception in the context of the thread that 
invoked it. Long-running iterations may want to test to see whether 
an exception is pending in another iteration. They can do this with  
the ParallelLoopState class’s IsExceptional property. This property 
returns true if an exception is pending.
Because more than one exception may occur during parallel exe-
cution, exceptions are grouped using an exception type known as an 
aggregate exception. The AggregateException class has an Inner
Exceptions property that contains a collection of all the exceptions 
that occurred during the execution of the parallel loop. Because the 
loop runs in parallel, there may be more than one exception.
Exceptions take priority over external cancellations and termina-
tions of a loop initiated by calling the Break or Stop methods of the 
ParallelLoopState object.
For a code example and more information about handling aggre-
gate exceptions, see the section, “Exception Handling,” in Chapter 3, 
“Parallel Tasks.”
special handling of small loop bodies
If the body of the loop performs only a small amount of work, you 
may find that you achieve better performance by partitioning the it-
erations into larger units of work. The reason for this is that there are 
two types of overhead that are introduced when processing a loop: 
the cost of managing worker threads and the cost of invoking a del-
egate method. In most situations, these costs are negligible, but with 
very small loop bodies they can be significant.
The parallel extensions of .NET Framework 4 include support for 
custom partitioning. A Partitioner object divides the indices into 
non-overlapping intervals named ranges. With partitioners, each par-
allel loop step handles a range of indices instead of individual indices. 
Throwing an unhandled 
exception prevents new 
iterations from starting.
Consider using a  
Partitioner object when 
you have many iterations 
that each perform a small 
amount of work.

 27
parallel loops
By grouping iterations into ranges, you can avoid some of the over-
head of a normal parallel loop. Here’s an example.
int n = ...
double[] result = new double[n];
Parallel.ForEach(Partitioner.Create(0, n), 
    (range) =>
    {
       for (int i = range.Item1; i < range.Item2; i++)
       {
         // very small, equally sized blocks of work
         result[i] = (double)(i * i); 
       }
    });
Here’s the signature of the Parallel.For method that was used in the 
example.
Parallel.ForEach<TSource>(
     Partitioner<TSource> source, 
     Action<TSource> body);
In this example, you can think of the result of the Partitioner.Create 
method as an object that acts like an instance of IEnumerable
<Tuple<int, int>> (the actual type is Partitioner<Tuple<int, int>>). 
In other words, Create gives you access to a collection of tuples 
(unnamed records) with two integer field values. Each tuple represents 
a range of index values that should be processed in a single iteration 
of the parallel loop. Each iteration of the parallel loop contains a 
nested sequential for loop that processes each index in the range.
The partition-based syntax for parallel loops is more complicated 
than the syntax for other parallel loops in .NET, and when the amount 
of work in each iteration is large (or of uneven size across iterations), 
it may not result in better performance. Generally, you would only use 
the more complicated syntax after profiling or in the case where loop 
bodies are extremely small and the number of iterations large.
The number of ranges that will be created by a Partitioner object 
depends on the number of cores in your computer. The default num-
ber of ranges is approximately three times the number of those cores.
If you know how big you want your ranges to be, you can use an 
overloaded version of the Partitioner.Create method that allows you 
to specify the size of each range. Here’s an example.
double[] result = new double[1000000];
Parallel.ForEach(Partitioner.Create(0, 1000000, 50000), 
    (range) =>

28
chapter two
    {
       for (int i = range.Item1; i < range.Item2; i++)
       {
         // small, equally sized blocks of work
         result[i] = (double)(i * i); 
       }
    });
In this example, each range will span 50,000 index values. In other 
words, for a million iterations, the system will use twenty parallel  
iterations (1,000,000 / 50,000). These iterations will be spread out 
among all the available cores.
Custom partitioning is an extension point in the API for parallel 
loops. You can implement your own partitioning strategies. For more 
information about this topic, see the section, “Further Reading,” at the 
end of this chapter.
controlling the degree of parallelism
Although you usually let the system manage how iterations of a paral-
lel loop are mapped to your computer’s cores, in some cases, you may 
want additional control.
You’ll see this variation of the Parallel Loop pattern in a variety of 
circumstances. Reducing the degree of parallelism is often used in 
performance testing to simulate less capable hardware. Increasing the 
degree of parallelism to a number larger than the number of cores can 
be appropriate when iterations of your loop spend a lot of time wait-
ing for I/O operations to complete.
The term degree of parallelism can be used in two senses. In the 
simplest case, it refers to the number of cores that are used to process 
iterations simultaneously. However, .NET also uses this term to refer 
to the number of tasks that can be used simultaneously by the parallel 
loop. For example, the MaxDegreeOfParallelism property of the 
ParallelOptions object refers to the maximum number of worker 
tasks that will be scheduled at any one time by a parallel loop.
For efficient use of hardware resources, the number of tasks is 
often greater than the number of available cores. For example, parallel 
loops may use additional tasks in cases where there are blocking I/O 
operations that do not require processor resources to run. The degree 
of parallelism is automatically managed by the underlying components 
of the system; the implementation of the Parallel class, the default 
task scheduler, and the .NET thread pool all play a role in optimizing 
throughput under a wide range of conditions.
You can limit the maximum number of tasks used concurrently  
by specifying the MaxDegreeOfParallelism property of a Parallel
Options object. Here is an example.
You can control the  
maximum number of  
threads used concurrently  
by a parallel loop.

 29
parallel loops
var n = ...
var options = new ParallelOptions() 
                      { MaxDegreeOfParallelism = 2};
Parallel.For(0, n, options, i =>
{
  // ... 
});
In the preceding code example, the parallel loop will run using at most 
two tasks at any one time. Here’s the signature of the Parallel.For 
method that was used in the example.
Parallel.For(int fromInclusive, 
             int toExclusive, 
             ParallelOptions parallelOptions, 
             Action<int> body);
You can also configure the maximum number of worker threads for 
PLINQ queries by setting the WithDegreeOfParallelism property of 
a ParallelQuery<T> object. Here’s an example.
IEnumerable<T> myCollection = // ...
myCollection.AsParallel()
            .WithDegreeOfParallelism(8)
            .ForAll(obj => /* ... */);
The query in the code example will run with a maximum of eight tasks 
at any one time.
If you specify a larger degree of parallelism, you may also want to 
use the ThreadPool class’s SetMinThreads method so that these 
threads are created without delay. If you don’t do this, the thread 
pool’s thread injection algorithm may limit how quickly threads can 
be added to the pool of worker threads that is used by the parallel 
loop. It may take more time than you want to create the required 
number of threads.
using task-local state in a loop body
Occasionally, you’ll need to maintain thread-local state during the 
execution of a parallel loop. For example, you might want to use a 
parallel loop to initialize each element of a large array with random 
values. The .NET Framework Random class does not support multi-
threaded access. Therefore, you need a separate instance of the  
random number generator for each thread.
Here’s an example that uses one of the overloads of the Parallel.
ForEach method. The example uses a Partitioner object to decom-
You must use task-local state 
for loop bodies that make  
calls to methods that are not 
thread safe.

30
chapter two
pose the work into relatively large pieces, because the amount of 
work performed by each step is small, and there are a large number  
of steps.
int numberOfSteps = 10000000;
double[] result = new double[numberOfSteps];
Parallel.ForEach(
     Partitioner.Create(0, numberOfSteps), 
     new ParallelOptions(),
     () => { return new Random(MakeRandomSeed()); },
     (range, loopState, random) =>
     {
       for (int i = range.Item1; i < range.Item2; i++)
         result[i] = random.NextDouble();
       return random;
     },
     _ => {});
Here’s the signature of the overload of version of the Parallel.ForEach 
method that was used in the example.
ForEach<TSource, TLocal>(
   OrderablePartitioner<TSource> source, 
   ParallelOptions parallelOptions, 
   Func<TLocal> localInit, 
   Func<TSource, ParallelLoopState, TLocal, TLocal> body, 
   Action<TLocal> localFinally)
The Parallel.ForEach loop will create a new instance of the Random 
class for each of its worker tasks. This instance will be passed as an 
argument to each partitioned iteration. Each partitioned iteration is 
responsible for returning the next value of the thread-local state. In 
this example, the returned value is always the same object that was 
passed in.
The default constructor of the Random class uses the system’s 
clock to generate a random seed. It’s possible that the default con-
structor will use the same seed if the constructor is called in twice in 
short succession. Therefore, the code makes sure that a distinct seed 
value is used for each new Random object by passing a unique seed 
value as an argument to the constructor.
Task-local state is available  
if your loop body needs it.
Calling the default Random 
constructor twice in short 
succession may use the same 
random seed. Provide your 
own random seed to prevent 
duplicate random sequences.

 31
parallel loops
This example is designed to show the use of task-local state. In 
the Parallel class, thread-local state is provided by task-local state. A 
task is guaranteed to execute on just one thread throughout its entire 
run.
Generating pseudorandom numbers using the Random class in 
parallel can result in overlapping sequences of numbers on different 
threads. If your application really requires statistically robust pseudo-
random values, you should consider using the RNGCryptoService
Provider class or a third-party library. For more information about 
generating random numbers, see the section, “Further Reading,” at the 
end of this chapter.
using a custom task scheduler  
for a parallel loop
You can substitute custom task scheduling logic for the default task 
scheduler that uses ThreadPool worker threads. For example, you can 
do this to enable a maximum degree of parallelism that operates 
across multiple loops instead of over only a single loop, which is the 
default. Additionally, you can use a custom task scheduler if you want 
to use a dedicated set of threads to process a parallel loop instead of 
pulling worker threads from the shared pool. These are just a few of 
the advanced scenarios where a custom task scheduler might be 
needed.
To specify a custom task scheduler, set the TaskScheduler prop-
erty of a ParallelOptions object. Here’s an example.
int n = ...
TaskScheduler myScheduler = ...
var options = new ParallelOptions() 
                      { TaskScheduler = myScheduler };
Parallel.For(0, n, options, i =>
{
  // ... 
});
Here is the signature of the Parallel.For that was used in the code 
example.
Parallel.For(int fromInclusive, 
             int toExclusive, 
             ParallelOptions parallelOptions, 
             Action<int> body);
For more information about tasks and task schedulers, see the sec-
tion, “Design Notes,” in Chapter3, “Parallel Tasks.”
It isn’t possible to specify a custom task scheduler for PLINQ 
queries.
The Random class isn’t the 
right random generator for all 
simulation scenarios. It’s used 
here only as an example of a 
class that isn’t thread safe.
You can use a custom task 
scheduler if the built-in task 
schedulers don’t meet your 
needs. This is one of the 
parallel loop’s extension 
mechanisms.

32
chapter two
Anti-Patterns
Anti-patterns are cautionary tales. They highlight issues that need to 
be carefully considered as well as problem areas.
step size other than one
If you need to use a step size other than one, transform the loop index 
in the body of the loop or use parallel foreach with the values you 
want. Also, be aware that a step size other than one often indicates a 
data dependency. Carefully analyze such a computation before you 
convert it into a parallel loop.
In sequential programs, you rarely iterate from high to low values 
unless the order matters. If your loop has a negative step size, the 
ordering of the loop is probably significant, and the iterations may not 
be independent.
Here’s a helpful technique. Before you convert a sequential loop 
to a parallel loop, you can temporarily reverse the order of the indices 
(for example, so that they go from high to low). If your sequential 
code still runs correctly, that’s a bit of evidence that the steps of  
the loop are independent. (The test isn’t foolproof; you still need to 
understand the algorithm.)
hidden loop body dependencies
Incorrect analysis of loop dependencies is a frequent source of soft-
ware defects. Be careful that all parallel loop bodies do not contain 
hidden dependencies. This is a mistake that’s easy to make.
The case of trying to share an instance of a class such as Random 
or DbConnection, which are not thread safe, across parallel iterations 
is an example of a subtle dependency.
When loop bodies are not fully independent of each other, it may 
still be possible to use parallel loops. However, in these cases, you 
must make sure that all shared variables are protected and synchro-
nized, and you must understand the performance characteristics of 
any synchronization you add. Adding synchronization can greatly 
reduce the performance of a parallel program, but forgetting to add 
necessary synchronization can result in a program with bugs that are 
catastrophic and difficult to reproduce.
If the loop body is not independent, for example, when you use 
an iteration to calculate a sum, you may need to apply the variation on 
a parallel loop that’s described in Chapter 4, “Parallel Aggregation.”
small loop bodies with few iterations
You’ll probably not get performance improvements if you use a paral-
lel loop for very small loop bodies with only a limited number of  
data elements to process. In this case, the overhead required by the 
Beware! A step size other  
than one can indicate a  
loop dependency.

 33
parallel loops
parallel loop itself will dominate the calculation. Simply changing  
every sequential for loop to Parallel.For will not necessarily produce 
good results.
processor oversubscription  
and undersubscription
Arbitrarily increasing the degree of parallelism puts you at risk of 
processor oversubscription, a situation that occurs when there are many 
more compute-intensive worker threads than there are cores. Tests 
have shown that, in general, the optimum number of worker threads 
for a parallel task equals the number of available cores divided by the 
average fraction of core utilization per task. For example, with four 
cores and 50 percent average core utilization per task, you need eight 
worker threads for maximum throughput. Increasing the number of 
worker threads above this number begins to incur extra cost from 
additional context switching without any improvement in processor 
utilization.
On the other hand, arbitrarily restricting the degree of parallelism 
can result in processor undersubscription. Having too few tasks misses 
an opportunity to effectively use the available processor cores. One 
situation where you may want to restrict the degree of parallelism is 
if you know that other tasks in your application are also running in 
parallel.
In most cases, the built-in load balancing algorithms in the .NET 
Framework are the most effective way to manage these tradeoffs. 
They coordinate resources among parallel loops and other tasks that 
are running concurrently. 
mixing the parallel class and plinq
PLINQ queries are represented by instances of the ParallelQuery<T> 
class. This class implements the IEnumerable<T> interface, so it’s pos-
sible to use a PLINQ query as the source collection for a Parallel.
ForEach loop, but it’s not recommended. Instead, use PLINQ’s ForAll 
extension method for the ParallelQuery<T> instance. PLINQ’s ForAll 
extension method performs the same kind of iteration as Parallel.
ForEach, but it avoids unnecessary merge and repartition steps that 
would otherwise occur.
Here’s an example of how to use the ForAll extension method.
var q = from d in data.AsParallel() ... select F(d);
q.ForAll(item =>
{
  // ... process item
});
Use ForAll if you need to 
iterate over a PLINQ result. 
Don’t use ForEach in this case.

34
chapter two
Don’t allow duplicate instances 
in parallel loops. If an object 
appears more than once in the 
input to a loop, it’s possible 
that two parallel threads could 
update the object at the same 
time.
The Parallel class adapts 
to the number of iterations, 
from only a few to many 
millions.
If the individual steps of your 
loop take a long time, you may 
see more worker threads than 
you intend.
duplicates in the input enumeration
If you’re using the Parallel.ForEach method, duplicate objects in 
the enumeration often indicate an unsafe race condition in your code. 
If an object (that is, an instance of a class) appears more than once in 
the input to the loop, it’s possible that two parallel threads could try 
to update that object at the same time.
Design Notes
Here are some fine points about using parallel loops.
adaptive partitioning
Parallel loops in .NET use an adaptive partitioning technique where 
the size of units of work increases over time. Adaptive partitioning is 
able to meet the needs of both small and large input ranges.
adaptive concurrency
The Parallel class and PLINQ work on slightly different threading 
models in the .NET Framework 4. PLINQ uses a fixed number of tasks 
to execute a query; by default, it creates the same number of tasks as 
there are logical cores in the computer, or it uses the value passed to 
the WithDegreeOfParallelism method if one was specified.
Conversely, by default, the Parallel.ForEach and Parallel.For 
methods can use a variable number of tasks. That’s why, for example, 
the ParallelOptions class has a MaxDegreeOfParallelism property 
instead of a “MinDegreeOfParallelism” property. The idea is that the 
system can use fewer threads than requested to process a loop.
The .NET thread pool adapts dynamically to changing workloads 
by allowing the number of worker threads for parallel tasks to change 
over time. At run time, the system observes whether increasing the 
number of threads improves or degrades overall throughput and ad-
justs the number of worker threads accordingly.
Be careful if you use parallel loops with individual steps that take 
several seconds or more. This can occur with I/O-bound workloads as 
well as lengthy calculations. If the loops take a long time, you may 
experience an unbounded growth of worker threads due to a heuristic 
for preventing thread starvation that’s used by the .NET ThreadPool 
class’s thread injection logic. This heuristic steadily increases the num-
ber of worker threads when work items of the current pool run for 
long periods of time. The motivation is to add more threads in cases 
where everything in the thread pool is blocked. Unfortunately, if work 
is actually proceeding, more threads may not necessarily be what  
you want. The .NET Framework can’t distinguish between these two 
situations.

 35
parallel loops
You can limit the number of threads with the ParallelOptions 
class if you observe, through profiling, that the number of threads 
chosen by the system is too large. You can also limit the number of 
threads with the SetMaxThreads method of the ThreadPool class. 
Generally, using ParallelOptions is preferred because SetMaxThreads 
has a global, process-wide effect that you may not want.
support for nested loops and  
server applications
The .NET Framework Parallel Extensions allow you to nest parallel 
loops. In this situation, the run-time environment coordinates the use 
of processor resources. You’ll find that with nested parallel loops, 
each loop uses fewer threads than in a non-nested loop.
A related problem is the handling of parallel loops in server  
applications. The Parallel class attempts to deal with multiple App
Domains in server applications in exactly the same manner that nested 
loops are handled. If the server application is already using all the 
available thread pool threads to process other ASP.NET requests,  
a parallel loop will only run on the thread that invoked it. If the  
workload decreases and other threads become available and there’s 
no other ASP.NET work to be processed, the loop will start using 
additional threads.
Related Patterns
The Parallel Loop pattern is the basis of the parallel aggregation  
pattern, which is the subject of Chapter 4, “Parallel Aggregation.”
Exercises
1.	 Which of the following problems could be solved using the 
parallel loop techniques taught in this chapter?
a.	 Sorting an in-memory array of numbers with a million 
elements
b.	 Putting the words in each line read from a text file in 
alphabetical order
c.	 Adding together all the numbers in one collection,  
to obtain a single sum
d.	 Adding numbers from two collections pair-wise,  
to obtain a collection of sums

36
chapter two
e.	 Counting the total number of occurrences of each word 
in a collection of text files
f.	 Finding the word that occurs most frequently in each 
file in a collection of text files
2.	 Choose a suitable problem from Exercise 1. Code three 
solutions, using a sequential loop, a parallel loop, and PLINQ.
3.	 In the credit review example, what is the type of account? 
What is the type of account.AllAccounts? What is the type 
of account.AllAccounts.AsParallel()?
4.	 Is it possible to use a PLINQ query as the source for a 
Parallel.ForEach loop? Is this recommended? Explain your 
answers.
5.	 Do a performance analysis of the credit review example code 
on the CodePlex site http://parallelpatterns.codeplex.com. 
Use command line options to independently vary the number 
of iterations (the number of accounts) and the amount of 
work done in the body of each iteration (the number of 
months in the credit history). Record the execution times 
reported by the program for all three versions, using several 
different combinations of numbers of accounts and months. 
Repeat the tests on different computers with different 
numbers of cores and with different execution loads (from 
other applications).
6.	 Modify the credit review example from CodePlex so that you 
can set the MaxDegreeOfParallelism property. Observe the 
effect of different values of this property on the execution 
time when running on computers with different numbers of 
cores.

 37
parallel loops
Further Reading
The examples in this book use features and libraries of recent  
C# versions. Some helpful books are by Bishop, and Albahari and  
Albahari. These sources contain information about lambda expres-
sions. The book by Mattson, et al. describes software design patterns 
for parallel programming that are not specialized for a particular  
language or library. Many of the cases in the “Variations” section of 
this chapter are from the report by Toub, which contains additional 
examples and variations. For more information about partitioners, 
refer to the in-depth article on this subject by Toub.
J. Albahari and B. Albahari. C# 4 in a Nutshell. O’Reilly, fourth 
edition, 2010.
J. Bishop. C# 3.0 Design Patterns. O’Reilly, 2008.
T. G. Mattson, B. A. Sanders, and B. L. Massingill. Patterns for 
Parallel Programming. Addison-Wesley, 2004.
T. G. Mattson, “Use and Abuse of Random Numbers” (video), 
Feb 14, 2008, http://software.intel.com/en-us/videos/tim-
mattson-use-and-abuse-of-random-numbers/
S. Toub. “Patterns of Parallel Programming: Understanding and 
Applying Parallel Patterns with the .NET Framework 4 and C#.” 
2009. http://www.microsoft.com/downloads/details.
aspx?FamilyID=86b3d32b-ad26-4bb8-a3ae-c1637026c3ee 
&displaylang=en
S. Toub. “Custom Parallel Partitioning With .NET 4.” April 26, 
2010. http://www.drdobbs.com/visualstudio/224600406


39
Parallel Tasks
Parallel tasks are  
asynchronous operations  
that can run at the  
same time. This approach  
is also known as task 
parallelism.
Parallel tasks in  
.NET are implemented  
by the Task class.
Chapter 2 shows how you can use a parallel loop to apply a single 
operation to many data elements. This is data parallelism. Chapter 3 
explains what happens when there are distinct asynchronous opera-
tions that can run simultaneously. In this situation, you can temporar-
ily fork a program’s flow of control with tasks that can potentially 
execute in parallel. This is task parallelism. The Parallel Tasks pattern 
is sometimes known as the Fork/Join pattern or the Master/Worker 
pattern.
Data parallelism and task parallelism are two ends of a spectrum. 
Data parallelism occurs when a single operation is applied to many in-
puts. Task parallelism uses multiple operations, each with its own input.
In the Microsoft® .NET Framework, tasks are implemented by 
the Task class in the System.Threading.Tasks namespace. The static 
Task.Factory property contains an instance of the TaskFactory class, 
which is used to create and schedule new tasks. You can wait for a 
task to complete by invoking the task’s Wait method. It’s also possible 
to wait for multiple tasks to complete. If you think in terms of fork/
join, the StartNew method is the fork operation and the Wait 
method is the join operation.
Scheduling is an important aspect of parallel tasks. Unlike threads, 
new tasks don’t necessarily begin executing immediately. Instead, they 
are placed in a work queue. Tasks run when their associated task 
scheduler removes them from the queue, usually as cores become 
available. The task scheduler attempts to optimize overall throughput 
by controlling the system’s degree of concurrency. As long as there are 
enough tasks and the tasks are sufficiently free of serializing depen-
dencies, the program’s performance scales with the number of avail-
able cores. In this way, tasks embody the concept of potential paral-
lelism that was introduced in Chapter 1.
Another important aspect of task-based applications is how they 
handle exceptions. In .NET, an unhandled exception that occurs  
during the execution of a task is deferred for later observation. For 
3

40
chapter three
example, the deferred exception is automatically observed at a later 
time when you call one of the task class’s wait methods. At that time, 
the exception is rethrown in the calling context of the wait method. 
This allows you to use the same exception handling approach in paral-
lel programs that you use in sequential programs.
The Basics
Each task is a sequential operation; however, tasks can often run in 
parallel. In .NET, a task is also an object with properties and methods 
of its own. Here’s some sequential code.
  DoLeft();
  DoRight();
Let’s assume that the methods DoLeft and DoRight are independent. 
This means that neither method writes to memory locations or files 
that the other method might read. Because the methods are indepen-
dent, you can use the Invoke method of the Parallel class to call them 
in parallel. This is shown in the following code.
  Parallel.Invoke(DoLeft, DoRight);
Parallel.Invoke is the simplest expression of the parallel task pattern. 
It creates new parallel tasks for each delegate method that is in its 
params array argument list. The Invoke method returns when all the 
tasks are finished.
You can’t assume that all parallel tasks will immediately run. De-
pending on the current work load and system configuration, tasks 
might be scheduled to run one after another, or they might run at the 
same time. For more information about how tasks are scheduled, see 
the section, “The Default Task Scheduler,” later in this chapter.
The delegate methods of Parallel.Invoke can either complete 
normally or finish by throwing an exception. Any exceptions that oc-
cur during the execution of Parallel.Invoke are deferred and rethrown 
when all tasks finish. All exceptions are rethrown as inner exceptions 
of an AggregateException instance. For more information and a code 
example, see the section, “Handling Exceptions,” later in this chapter.
Internally, Parallel.Invoke creates new tasks and waits for them. 
It uses methods of the Task class to do this. Here’s an example.
  Task t1 = Task.Factory.StartNew(DoLeft);  
  Task t2 = Task.Factory.StartNew(DoRight);
  Task.WaitAll(t1, t2);
Tasks in .NET defer  
exceptions and rethrow 
them when one of the task’s 
wait methods is invoked.
Parallel.Invoke is a method 
in .NET that creates a list of 
parallel tasks and waits for 
them all to complete.
Two tasks, t1 and t2, are 
needed in this example for 
correct exception handling. 
Without the second task, t2, 
the code would not have 
guaranteed that exceptions 
thrown during the execution 
of task t1 would be observed. 
For more information, see the 
section, “Handling Exceptions,” 
later in this chapter.

 41
parallel tasks
The StartNew method of the TaskFactory class creates and sched-
ules a new task that executes the delegate method that is provided as 
its argument. You can wait for parallel tasks to complete by calling the 
Task.WaitAll method. (Use the Wait method if you want to wait on 
a single task.)
When you use StartNew to create a task, the new task is added 
to a work queue for eventual execution, but it does not start to run 
until its task scheduler takes it out of the work queue, which can 
happen immediately or can occur at some point in the future.
The examples you’ve seen so far are simple, but they’re powerful 
enough to handle many scenarios. For more ways to use tasks, see the 
section, “Variations,” later in this chapter.
An Example
An example of task parallelism is an image processing application 
where images are created with layers. Separate images from different 
sources are processed independently and then combined with a  
process known as alpha blending. This process superimposes 
semitransparent layers to form a single image.
The source images that are combined are different, and different 
image processing operations are performed on each of them. This 
means that the image processing operations must be performed  
separately on each source image and must be complete before the 
images can be blended. In the example, there are only two source 
images, and the operations are simple: conversion to gray scale and 
rotation. In a more realistic example, there might be more source  
images and more complicated operations.
Here’s the sequential code. The source code for the complete 
example is located at http://parallelpatterns.codeplex.com in the 
Chapter3\ImageBlender folder.
static int SeqentialImageProcessing(Bitmap source1, Bitmap source2, 
                                    Bitmap layer1, Bitmap layer2, 
                                    Graphics blender)
{
  SetToGray(source1, layer1);
  Rotate(source2, layer2);
  Blend(layer1, layer2, blender);
  return source1.Width;
}
In this example, source1 and source2 are bitmaps that are the original 
source images, layer1 and layer2 are bitmaps that have been prepared 
with additional information needed to blend the images, and blender 
Use the StartNew factory 
method to create a task and 
schedule it to run. Use Task.
WaitAll to wait for more 
than one task to complete 
before proceeding.

42
chapter three
is a Graphics instance that performs the blending and references the 
bitmap with the final blended image. Internally, SetToGray, Rotate, 
and Blend use methods from the .NET System.Drawing namespace 
to perform the image processing. The last statement returns an inte-
ger that the caller uses to print a progress message.
The SetToGray and Rotate methods are entirely independent of 
each other. This means that you can execute them in separate tasks. 
If two or more cores are available, the tasks might run in parallel, and 
the image processing operations might complete in less elapsed time 
than a sequential version would.
The parallel code uses tasks explicitly.
static int ParallelTaskImageProcessing(Bitmap source1, 
   Bitmap source2, Bitmap layer1, Bitmap layer2, 
   Graphics blender)
{
  Task toGray = Task.Factory.StartNew(() => 
     SetToGray(source1, layer1));
  Task rotate = Task.Factory.StartNew(() => 
     Rotate(source2, layer2));
  Task.WaitAll(toGray, rotate);
  Blend(layer1, layer2, blender);
  return source1.Width;
}
This code calls Task.Factory.StartNew to create and run two tasks 
that execute SetToGray and Rotate, and calls Task.WaitAll to wait 
for both tasks to complete before blending the processed images.
You can also use the Parallel.Invoke method to achieve parallel-
ism. The Parallel.Invoke method has very convenient syntax. This is 
shown in the following code.
static int ParallelInvokeImageProcessing(Bitmap source1, 
   Bitmap source2, Bitmap layer1, Bitmap layer2, 
   Graphics blender)
{
  Parallel.Invoke(
    () => SetToGray(source1, layer1),
    () => Rotate(source2, layer2));
  Blend(layer1, layer2, blender);
  return source1.Width;
}
Here the tasks are identified implicitly by the arguments to Parallel.
Invoke. This call does not return until all of the tasks complete.

 43
parallel tasks
Variations
This section describes variations of the .NET implementation of the 
Parallel Task pattern.
canceling a task
In .NET, a cancellation request does not forcibly end a task. Instead, 
tasks use a cooperative cancellation model. This means that a running 
task must poll for the existence of a cancellation request at appropri-
ate intervals and then shut itself down by calling back into the library.
The .NET Framework uses separate types. One allows a program 
request cancellation, and the other checks for cancellation requests. 
Instances of the CancellationTokenSource class are used to request 
cancellation, while CancellationToken values indicate whether 
cancellation has been requested. Here’s an example:
CancellationTokenSource cts = new CancellationTokenSource();
CancellationToken token = cts.Token;
Task myTask = Task.Factory.StartNew(() =>
{
    for (...)
    {
        token.ThrowIfCancellationRequested();
        // Body of for loop.
    }
}, token);
// ... elsewhere ...
cts.Cancel();
The CancellationTokenSource object contains a cancellation token. 
This token is used in two places. It’s passed as an argument to the 
StartNew factory method, and its ThrowIfCancellationRequested 
method is invoked to detect and process a cancellation request.  
Although the example doesn’t show it, you can also read the  
cancellation token’s IsCancellationRequested property to test for 
a cancellation request. For example, you would do this if you need  
to perform local cleanup actions for a task that’s in the process of 
being canceled.
If the cancellation token indicates that a cancellation has been 
requested, the ThrowIfCancellationRequested method creates an 
OperationCanceledException instance and passes in the cancellation 
token. It then throws the exception. This exception is the signal that 
notifies the .NET Framework that the task has been canceled; there-
Tasks in .NET  
use a cooperative  
cancellation model.

44
chapter three
There are several requirements 
that must be met before a task 
will transition to status 
Canceled.
Whether or not a task gets the 
status Canceled can be 
important. If you don’t do this 
correctly, the process might be 
unexpectedly terminated when 
the improperly canceled task is 
garbage collected. A typical 
mistake is forgetting to pass 
the cancellation token to the 
StartNew method. Another 
typical mistake is forgetting to 
throw the OperationCancele-
dException that contains the 
required cancellation token.
fore, the OperationCanceledException should not be handled by 
user code within the task (it is often handled, however, outside of the 
task that was canceled). If you follow the steps that have been  
described in this section, the task will be stopped and its Status 
property will be set to the enumerated value TaskStatus.Canceled.
To ensure that a running task transitions to status Canceled, 
a cancellation token must have been given as an argument to the 
StartNew method and an unhandled OperationCanceledException 
exception that contains that same cancellation token must be thrown 
by the task.
Checking for cancellation within a loop with many iterations that 
each perform a small amount of work can negatively affect your  
application’s performance, but checking only infrequently for cancel-
lation can introduce unacceptable latency in your application’s  
response to cancellation requests. For example, in an interactive  
GUI-based application, checking for cancellation more than once per 
second is probably a good idea. An application that runs in the  
background could poll for cancellation much less frequently, perhaps 
every two to ten seconds. Profiling your application can give you 
performance data that you can use when determining the best places 
to test for cancellation requests in your code. You need to understand 
which code locations will allow you to poll for cancellation at evenly 
spaced intervals of your choosing, and profiling can help you do this.
If a cancellation token is passed to the StartNew method and 
cancellation is signaled on that token before the new task begins to 
execute, the new task’s status will transition to Canceled without 
ever running.
For a little fine print about the behavior of cancellation, see the 
sections, “Task Life Cycle” and “Unobserved Task Exceptions,” later in 
this chapter.
handling exceptions
If the execution of a Task object’s delegate throws an unhandled 
exception, the task terminates and its Status property is set to the 
enumerated value TaskStatus.Faulted. The unhandled exception is 
temporarily unobserved. This means that the Task Parallel Library 
(TPL) catches the exception and records its details in internal data 
structures associated with the task object. Recovering a deferred  
exception and embedding it in a new exception is known as  
“observing an unhandled task exception.” In many cases, unhandled 
task exceptions will be observed in a different thread than the one 
that executed the task.

 45
parallel tasks
You must take care to observe 
the unhandled exceptions of 
each task. If you don’t do this, 
.NET’s exception escalation 
policy can terminate your 
process when the task is 
garbage collected.
The Task Parallel Library 
uses AggegrateException 
objects to propagate  
exceptions.
Ways to Observe an Unhandled Task Exception
There are several ways to observe an unhandled task exception.
Invoking the faulted task’s Wait method causes the task’s 
unhandled exception to be observed. The exception is also thrown in 
the calling context of the Wait method. The Task class’s static 
WaitAll method allows you to observe the unhandled exceptions of 
more than one task with a single method invocation. The Parallel.
Invoke method includes an implicit call to WaitAll. Exceptions from 
all of the tasks are grouped together in an AggregateException 
object and thrown in the calling context of the WaitAll or Wait 
method.
Getting the Exception property of a faulted task causes the 
task’s unhandled exception to be observed. The property returns the 
aggregate exception object. Getting the value does not automatically 
cause the exception to be thrown; however, the exception is consid-
ered to have been observed when you get the value of the Exception 
property. Use the Exception property instead of the Wait method or 
WaitAll method when you want to examine the unhandled exception 
but do not want it to be rethrown in the current context.
Special handling occurs if a faulted task’s unhandled exceptions 
are not observed by the time the task object is garbage-collected. For 
more information, see the section, “Unobserved Task Exceptions,” 
later in this chapter.
Aggregate Exceptions
In some cases, such as when you’re waiting for more than one task, 
there can be multiple unhandled exceptions that need to be observed. 
For this reason, the runtime collects the unhandled task exceptions 
and wraps them in an AggregateException instance. TPL throws the 
aggregate exception in the context of the observer (that is, in the 
waiting thread). You can catch the aggregate exception and examine 
its InnerExceptions property to handle the original exceptions indi-
vidually. Even if the unhandled exception from just one task is ob-
served, it’s still wrapped in an AggregateException. Throwing an 
aggregate exception preserves the inner exception’s stack trace, which 
would otherwise be overwritten if the exception were rethrown in 
the current context.
Like all exceptions that are raised inside of a task, an Operation-
CanceledException instance that’s created by canceling a task will 
become an inner exception of an AggregateException thrown by the 
task in the context of the thread that calls the task’s Wait method. 
However, if you follow the cancellation rules that are described in the 
section, “Canceling a Task,” earlier in this chapter, the task’s final  
status will be TaskStatus.Canceled instead of TaskStatus.Faulted.

46
chapter three
The Handle Method
The AggregateException class has several helper methods to make 
handling inner exceptions easier. The Handle method of the 
AggregateException class invokes a user-provided delegate method 
for each of the inner exceptions of an aggregate exception. The  
delegate should return true if it has handled the inner exception and 
false if it has not handled the inner exception. Here’s an example.
try
{
  Task t = Task.Factory.StartNew( ... );
  // ... 
  t.Wait();
}
catch (AggregateException ae)
{
  ae.Handle(e =>
  {
    if (e is MyException)
    {
      // ... handle exception ...
      return true;
    }
    else
    { 
      return false;
    }
  });
}
When the Handle method invokes the user-provided delegate for 
each inner exception, it keeps track of the results of the invocation. 
After it processes all inner exceptions, it checks to see if the results 
for one or more of the inner exceptions were false, which indicates 
that they have not been handled. If there are any unhandled excep-
tions, the Handle method creates and throws a new aggregate 
exception that contains the unhandled ones as inner exceptions.
If the Handle method doesn’t meet your needs, you can write 
your own code that iterates through the InnerExceptions property 
of an AggregateException object.
Use the Handle method 
to iterate through inner 
exceptions.

 47
parallel tasks
The Flatten method 
transforms a tree of  
exceptions into a  
sequence.
The Flatten Method
The Flatten method of the AggregateException class is useful when 
tasks are nested within other tasks. In this case, it’s possible that an 
aggregate exception can contain other aggregate exceptions as inner 
exceptions. The Flatten method produces a new exception object 
that merges the inner exceptions of all nested aggregate exceptions 
into the inner exceptions of the top-level aggregate exception. In 
other words, it converts a tree of inner exceptions into a flat sequence 
of inner exceptions. It’s typical to use the Flatten and Handle 
methods together. This is shown in the following example.
try
{
  Task t1 = Task.Factory.StartNew(() =>
  {
    Task t2 = Task.Factory.StartNew(() =>
    { 
       // ...
       throw new MyException();
    });
    // ...
    t2.Wait();
  });
  // ... 
  t1.Wait();
}
catch (AggregateException ae)
{
  ae.Flatten().Handle(e =>
  {
    if (e is MyException)
    {
      // ... handle exception ...
      return true;
    }
    else
    { 
      return false;
    }
  });
}

48
chapter three
waiting for the first task to complete
The Task class includes a method named Task.WaitAll that allows 
you to wait for a set of tasks to complete. However, you can also wait 
for the first task to complete by calling the Task.WaitAny method.
Here’s an example.
  var taskIndex = -1;
  Task[] tasks = new Task[]
                   {
                     Task.Factory.StartNew(DoLeft),
                     Task.Factory.StartNew(DoRight),
                     Task.Factory.StartNew(DoCenter)
                   };
  Task[] allTasks = tasks;
  // Print completion notices one by one as tasks finish.
  while (tasks.Length > 0)
  {
    taskIndex = Task.WaitAny(tasks);
    Console.WriteLine(“Finished task {0}.”, taskIndex + 1);
    tasks = tasks.Where((t) => t != tasks[taskIndex]).ToArray();
  }
  // Observe any exceptions that might have occurred.
  try
  {
    Task.WaitAll(allTasks);
  }
  catch (AggregateException ae)
  {
    ...
  }
This example shows how you can use the WaitAny method to be 
notified when the first of a group of tasks completes. The code prints 
a line to the console output after each task finishes. The while loop 
exits after all the tasks finish.
You need to observe any exceptions that may have occurred, but 
note that exceptions are never observed by the WaitAny method. 
Instead, you should add a step that checks for exceptions when- 
ever you use the WaitAny method. Although this example uses 
the WaitAll method to observe exceptions, you could also use the 
Exception property or the Wait method for this purpose. The 
IsFaulted property of the Task class can be used to check to see 
whether an unhandled exception occurred within a task.
Exceptions are never observed 
by the WaitAny method. 
However, you must make sure 
that all exceptions are 
eventually observed using one 
of the techniques that are 
described in the section, 
“Handling Exceptions,” earlier 
in this chapter.
Use Task.WaitAny as 
a way to wait for at least 
one task in a set of tasks  
to complete.

 49
parallel tasks
speculative execution
Speculative execution is a variation that occurs where you perform an 
operation in anticipation of a particular result. For example, you might 
predict that the current computation being run will output the value 
42, and you start the next computation that depends on this compu-
tation’s result by using 42 as the input. If the first computation ends 
with 42, you’ve now gained parallelism by successfully starting the 
dependent operation well in advance of when you otherwise could 
have. If the first computation results in something other than 42, you 
can restart the second operation using the correct value.
Another example of speculative execution occurs when you exe-
cute more than one asynchronous operation in parallel but need just 
one of the operations to complete before proceeding. Imagine, for 
example, that you use three different search tasks to search for an 
item. After the fastest task finds the item, you don’t need to wait for 
the other searches to complete. In cases like, this you wait for the first 
task to complete and usually cancel the remaining tasks. However, you 
should always observe any exceptions that might have occurred in any 
of the tasks.
Here’s an example.
  SpeculativeInvoke(SearchLeft, SearchRight, SearchCenter);
This example executes three delegate methods in parallel. Only one 
of the delegates needs to complete for the operation to succeed, so 
the tasks that did not finish first are canceled. The SpeculativeInvoke 
method executes each of the delegates in the params array argument. 
Its implementation is shown in the following code.
public static void SpeculativeInvoke(
                     params Action<CancellationToken>[] actions)
{
  var cts = new CancellationTokenSource();
  var token = cts.Token;
  var tasks = 
    (from a in actions
     select Task.Factory.StartNew(() => a(token), token))
    .ToArray();
  // Wait for fastest task to complete.
  Task.WaitAny(tasks);
  // Cancel all of the slower tasks.
  cts.Cancel();
  // Wait for cancellation to finish and observe exceptions.

50
chapter three
  try 
  { 
    Task.WaitAll(tasks); 
  }
  catch (AggregateException ae)
  {
    // Filter out the exception caused by cancellation itself.
    ae.Flatten().Handle(e => e is OperationCanceledException);
  }
  finally
  {
    if (cts != null) cts.Dispose();
  }
}
The WaitAny method does not observe unhandled task exceptions. 
This is because you need the returned index from WaitAny to know 
which task completed (potentially by throwing an exception).  
Therefore, this example also makes a call to the WaitAll method after 
canceling the tasks that did not finish first. This causes all unhandled 
task exceptions to be observed in the current thread. The code 
catches the aggregate exception that is thrown by WaitAll and 
removes the OperationCanceledException instances. If there are no 
remaining unhandled exceptions, the code proceeds normally. Other-
wise, the remaining unhandled exceptions are rethrown as the inner 
exceptions of a new aggregate exception.
creating tasks with custom scheduling
You can customize the details of how tasks in .NET are scheduled and 
run by overriding the default task scheduler that’s used by the task 
factory methods. For example, you can provide a custom task sched-
uler as an argument to one of the overloaded versions of the Task
Factory.StartNew method.
There are some cases where you might want to override the de-
fault scheduler. The most common case occurs when you want your 
task to run in a particular thread context. This can happen when you 
use objects provided by libraries, such as Windows Presentation 
Foundation (WPF), that impose thread affinity constraints. Other 
cases occur when the load-balancing heuristics of the default task 
scheduler don’t work well for your application. For more information, 
see the section, “Thread Injection,” later in this chapter.
Unless you specify otherwise, any new tasks will use the current 
task scheduler, which is the value of the static property Task
Scheduler.Current. In other words, subtasks inherit the task sched-
You can customize the way 
tasks are scheduled.

 51
parallel tasks
uler of the enclosing task context. Unless otherwise specified, new 
top-level tasks (and, consequently, their subtasks) use the default task 
scheduler, which is the value of the static property TaskScheduler.
Default. The default task scheduler is tightly integrated with the 
.NET thread pool and handles a wide variety of operating conditions. 
It’s a good choice for most applications. For more information, see the 
section, “The Default Task Scheduler,” later in this chapter. The Task
Scheduler class’s FromCurrentSynchronizationContext static 
method returns a task scheduler object that schedules the task to run 
wherever the current thread’s SynchronizationContext property 
indicates. This is sometimes the current thread, but not necessarily. 
This task scheduler is useful in cases of thread affinity. Not every 
thread has a current synchronization context, and there is no API to 
get a task scheduler that runs in the synchronization context of a 
thread other than the current thread. For example, you can’t use the 
FromCurrentSynchronizationContext method if you’re trying to 
schedule a task in a Windows service’s main thread.
You can implement your own task scheduler class. For more  
information, see the section, “Writing a Custom Task Scheduler,” later 
in this chapter.
Anti-Patterns
Here are some things to watch out for when you use tasks.
variables captured by closures
In C#, a closure can be created with a lambda expression in the form 
args => body that represents an unnamed (anonymous) delegate. A 
unique feature of closures is that they may refer to variables defined 
outside their lexical scope, such as local variables that were declared 
in a scope that contains the closure.
The semantics of closures in C# may not be intuitive to some 
programmers and it’s easy to make mistakes. Unless you understand 
the semantics, you may find that captured variables don’t behave as 
you expect, especially in parallel programs.
Problems occur when you reference a variable without consider-
ing its scope. Here’s an example.
  for (int i = 0; i < 4; i++)
  {
     // WARNING: BUGGY CODE, i has unexpected value
     Task.Factory.StartNew(() => Console.WriteLine(i));
  }
Capturing a loop index variable 
in a closure is usually a bug. Be 
on the lookout for this very 
common coding error.

52
chapter three
You might think that this code sample would print the numbers 1, 2, 
3, 4 in some arbitrary order, but it can print other values, depending 
on how the threads happen to run. For example, you might see 4, 4, 4, 
4. The reason is that the variable i is shared by all the closures created 
by the steps of the for loop. By the time the tasks start, the value of 
the single, shared variable i will probably be different from the value 
of i when the task was created. All the tasks are sharing the same 
variable.
The solution is to introduce an additional temporary variable in 
the appropriate scope.
  for (int i = 0; i < 4; i++)
  {
     var tmp = i;
     Task.Factory.StartNew(() => Console.WriteLine(tmp));
  }
This version prints the numbers 1, 2, 3, 4 in an arbitrary order, but each 
number will be printed. The reason is that the variable tmp is declared 
within the block scope of the for loop’s body. This causes a new 
variable named tmp to be instantiated with each iteration of the for 
loop. (In contrast, all iterations of the for loop share a single instance 
of the variable i.)
This bug is one reason why you should use Parallel.For instead of 
coding a loop yourself. It’s also one of the most common mistakes 
made by programmers who are new to tasks.
disposing a resource needed by a task
When you create a task, don’t forget that you can’t call the Dispose 
method on the objects that the task needs to do its work. Careless 
use of the C# using keyword is a common way to make this mistake. 
Here’s an example.
Task<string> t;
using (var file = new StringReader(“text”))
{
  t = Task<string>.Factory.StartNew(() => file.ReadLine());
}
// WARNING: BUGGY CODE, file has been disposed
Console.WriteLine(t.Result);
The using keyword introduces an implicit try/finally block that 
invokes the Dispose method on the value of the variable when it exits 
its scope. The using keyword can’t be used with captured variables. 
Instead, you need to call the Dispose method when you know that 
the disposable object is no longer needed. In this example, Dispose 
can only be called after the task’s Result property is read.
Be careful not to dispose 
resources needed by a  
pending task.

 53
parallel tasks
avoid thread abort
Terminating tasks with the Thread.Abort method leaves the App
Domain in a potentially unusable state. Also, aborting a thread pool 
worker thread is never recommended. If you need to cancel a task, use 
the technique described in the section, “Canceling a Task,” earlier in 
this chapter. Do not abort the task’s thread.
Design Notes
This section describes some of the design considerations that were 
used to create the Task Parallel Library.
tasks and threads
When a task begins to run, the applicable task scheduler invokes the 
task’s user delegate in a thread of its choosing.
The task will not migrate among threads at run time. This is a 
useful guarantee because it lets you use thread-affine abstractions, 
such as the Microsoft Windows® operating system’s critical sections, 
without having to worry, for example, that the Monitor.Enter func-
tion will be executed in a different thread than the Monitor.Exit 
function.
task life cycle
The Status property of a Task instance tracks its life cycle. Figure 1 
shows the life cycle of the tasks that are described in this chapter:
figure 1
Task life cycle
The TaskFactory.StartNew method creates and schedules a new 
task, which results in the status TaskStatus.WaitingToRun. Eventu-
ally, the TaskScheduler instance that is responsible for managing the 
task begins to execute the task’s user delegate on a thread of its 
choosing. At this point, the task’s status transitions to TaskStatus.
Running. After the task begins to run, it has three possible outcomes. 
If the task’s user delegate exits normally, the task’s status transitions 
Never attempt to cancel a task 
by calling the Abort method of 
the thread that is executing 
the task.
WaitingToRun
Running
Faulted
RanToCompletion
Canceled

54
chapter three
to TaskStatus.RanToCompletion. If the task’s user delegate throws 
an unhandled exception, the task’s status becomes Task.Faulted.
It’s also possible for a task to end in TaskStatus.Canceled. For 
this to occur, you must pass a CancellationToken as an argument to 
the factory method that created the task. If that token signals a  
request for cancellation before the task begins to execute, the task 
won’t be allowed to run. The task’s Status property will transition 
directly to TaskStatus.Canceled without ever invoking the task’s user 
delegate. If the token signals a cancellation request after the task 
begins to execute, the task’s Status property will only transition to 
TaskStatus.Canceled if the user delegate throws an Operation
CanceledException and that exception’s CancellationToken prop-
erty contains the token that was given when the task was created.
Later in this book, you’ll see two variations of the task life cycle. 
Chapter 5, “Futures,” introduces continuation tasks whose life cycles 
also depend on antecedent tasks. Chapter 6, “Dynamic Task Parallel-
ism,” describes tasks whose life cycles depend on subtasks that have 
been created with the AttachedToParent task creation option.
There is one more task status, TaskStatus.Created. This is the 
status of a task immediately after it’s created by the Task class’s 
constructor; however, it’s recommended that you use a factory 
method to create tasks instead of the new operator.
writing a custom task scheduler
The .NET Framework includes two task scheduler implementations: 
the default task scheduler and a task scheduler that runs tasks on a 
target synchronization context. If these task schedulers don’t meet 
your application’s needs, it’s possible to implement a custom task 
scheduler.
There are a number of advanced scenarios where creating a  
custom task scheduler is relevant. For example, you could implement 
a custom task scheduler if you wanted to enable a single maximum 
degree of parallelism across multiple loops and tasks instead of across 
a single loop. You could also create a custom task scheduler if you 
want to implement alternative scheduling algorithms, for example, to 
ensure fairness among batches of work. Finally, you can implement a 
custom task scheduler if you want to use a specific set of threads, 
such as Single Thread Apartment (STA) threads, instead of thread pool 
worker threads.
You can see how to create a custom task scheduler by looking at 
the ParallelExtensionsExtras project that is part of the Microsoft 
Parallel Samples package. For details of where to get these additional 

 55
parallel tasks
samples, see the section, “Further Reading,” at the end of this chapter. 
This package includes an implementation of a QueuedTaskScheduler 
class that provides multiple global task queues that use round-robin 
scheduling. A custom scheduler doesn’t need to use its own threads; 
it can simply limit the number of tasks that are allowed to run concur-
rently but still use the thread pool. This is the approach taken by  
the LimitedConcurrencyLevelTaskScheduler class in the Parallel
ExtensionsExtras project.
unobserved task exceptions
If you don’t give a faulted task the opportunity to propagate its ex-
ceptions (for example, by calling the Wait method), the runtime will 
escalate the task’s unobserved exceptions according to the current 
.NET exception policy when the task is garbage-collected. Unob-
served task exceptions will eventually be observed in the finalizer 
thread context. The finalizer thread is the system thread that invokes 
the Finalize method of objects that are ready to be garbage-collected. 
If an unhandled exception is thrown during the execution of a Final-
ize method, the runtime will, by default, terminate the current pro-
cess, and no active try/finally blocks or additional finalizers will be 
executed, including finalizers that release handles to unmanaged re-
sources. To prevent this from happening, you should be very careful 
that your application never leaks unobserved task exceptions. You can 
also elect to receive notification of any unobserved task exceptions 
by subscribing to the UnobservedTaskException event of the Task-
Scheduler class and choose to handle them as they propagate into the 
finalizer context.
This last technique can be useful in scenarios such as hosting 
untrusted plug-ins that have benign exceptions that would be cum-
bersome to observe. For more information, see the section, “Further 
Reading,” at the end of this chapter.
During finalization, tasks that have a Status property of Faulted 
are treated differently from tasks with the status Canceled. The task’s 
status determines how unobserved task exceptions that arise from 
task cancellation are treated during finalization. If the cancellation 
token that was passed as an argument to the StartNew method is the 
same token as the one embedded in the unobserved Operation
CanceledException instance, the task does not propagate the opera-
tion-canceled exception to the UnobservedTaskException event or 
to the finalizer thread context. In other words, if you follow the can-
cellation protocol described in this chapter, unobserved cancellation 
exceptions will not be escalated into the finalizer’s thread context.

56
chapter three
relationship between data parallelism 
and task parallelism
The Parallel.Invoke method was described as creating tasks for each 
of the delegates passed in its params array. As a mental model, this is 
fine. In practice, this isn’t always how it’s actually executed. For  
example, with enough delegates in the params list, TPL may, for 
performance reasons, use a parallel loop to invoke all the delegates.
This highlights how task parallelism and data parallelism are  
related. If you represent each operation to be performed as a delegate, 
you can then use data parallelism to perform the same operation (in-
voking the delegate) on each piece of data (the delegate). Conversely, 
if you have a data parallel problem, you can approach it in a task- 
parallel manner by spinning up a task to process each individual  
operation.
The Default Task Scheduler
All parallel libraries rely on a scheduler to organize and order the ex-
ecution of tasks and threads, and TPL is no exception. This section is 
a behind-the-scenes look at the .NET Framework 4 implementation 
of task scheduling. The material in this section can help you under-
stand the performance characteristics that you’ll observe when using 
TPL and PLINQ. Be aware that the scheduling algorithms described 
here represent an implementation choice. They aren’t constraints 
imposed by TPL itself, and future versions of .NET might optimize 
task execution differently.
In .NET Framework 4, the default task scheduler is tightly inte-
grated with the thread pool. If you use the default task scheduler, the 
worker threads that execute parallel tasks are managed by the .NET 
ThreadPool class. Generally, there are at least as many worker threads 
as cores on your computer. When there are more tasks than available 
worker threads, some tasks will be queued and wait until the thread 
pool provides an available worker thread.
An example of this approach is the thread pool’s QueueUser
WorkItem method. In fact, you can think of the default task sched-
uler as an improved thread pool where work items return a handle 
that a thread can use as a wait condition, and where unhandled task 
exceptions are forwarded to the wait context. The handles to the 
work items are called tasks, and the wait condition occurs when you 
call the task’s Wait method. In addition to these enhancements, the 
default thread scheduler is capable of better performance than  
the thread pool alone as the number of cores increases. Here’s how 
this works.
The behind-the-scenes 
behavior described in this 
section applies to .NET 
Framework 4. There’s no 
guarantee that future releases 
of the runtime or the frame-
work won’t behave differently.

 57
parallel tasks
the thread pool 
In its simplest form, a thread pool consists of a global queue of  
pending work items and a set of threads that process the work items, 
usually on a first-in first-out (FIFO) basis. This is shown in Figure 2.
figure 2
A thread pool
Thread pools have problems scaling to large numbers of cores. The 
main reason is that the thread pool has a single, global work queue. 
Each end of the global queue can be accessed by only one thread at a 
time, and this can become a bottleneck. When there are only a few, 
coarse-grained work items and a limited number of cores, the syn-
chronization overhead of a global queue (that is, the cost of ensuring 
that only one thread at a time has access) is small. For example, the 
overhead is negligible when the number of cores is four or fewer and 
when each work item takes many thousands of processor cycles. 
However, as the number of cores increases and the amount of work 
that you want to do in each work item decreases (due to the finer-
grained parallelism that is needed to exploit more of the cores), the 
synchronization cost of the traditional thread pool design begins to 
dominate.
Synchronization is an umbrella term that includes many tech-
niques for coordinating the activities of multi-threaded applications. 
Locks are a familiar example of a synchronization technique. Threads 
use locks to make sure that they don’t try to modify a location of 
memory at the same time as another thread. All types of synchroniza-
tion have the potential of causing a thread to block (that is, to do no 
work) until a condition is met.
Tasks in .NET are designed to scale to large numbers of cores. 
They are also lightweight enough to perform very small units of work, 
in the hundreds or low thousands of CPU cycles. In .NET, it’s possible 
for an application to run efficiently with millions of tasks. To handle 
this kind of scale, a more decentralized approach to scheduling than 
one that uses a single global queue is needed.
Add top−level task
Global queue
Run top−level tasks in FIFO order
Worker thread 1
Worker thread n
...

58
chapter three
decentralized scheduling techniques
The .NET Framework provides local task queues for each worker 
thread in the thread pool. Local task queues distribute the responsibil-
ity for queue management and avoid much of the serial access  
required by a global queue of work items. Giving different parts of the 
application their own work queues helps avoid a central bottleneck. 
This is shown Figure 3.
figure 3
Per-thread local task queues
You can see that there are as many task queues as there are worker 
threads, plus one global queue. All of these queues operate concur-
rently. The basic idea is that when a new task needs to be added, it 
can sometimes be added to a thread-local queue instead of to the 
global queue, and when a thread is ready for a new task to run, it can 
sometimes find one waiting in its local queue instead of having to go 
to the global queue. Of course, any work that comes from a thread 
that is not one of the thread pool’s worker threads still has to be 
placed on the global queue, which always incurs heavier synchroniza-
tion costs than adding to a local queue.
In the typical case, accessing the local queue needs only very 
limited synchronization. Items can be locally added and removed  
very quickly. The reason for this efficiency is that the local queues  
are implemented using a special concurrent data structure known as 
a work-stealing queue. A work-stealing queue is a double-ended 
queue that has a private end and a public end. The queue allows  
lock-free pushes and pops from the private end but requires costlier 
synchronization for operations at the public end. When the length of 
the queue is small, synchronization is required from both ends due to 
the locking strategy used by the implementation.
Moving to a distributed scheduling approach makes the order of 
task execution less predictable than with a single global queue.  
Worker thread 1
Worker thread n
Run top−level tasks in FIFO order
Per−thread local queue of tasks
Push /pop local subtasks in LIFO order
Global queue
Add top−level task
...

 59
parallel tasks
Although the global queue executes work in FIFO order, the local  
work-stealing queues use LIFO order to avoid synchronization costs. 
However, the overall throughput is likely to be better because a thread 
only uses the relatively more expensive global queue when it runs out 
of work from its local queue.
work stealing
What happens when a thread’s local work queue is empty and the 
global queue is also empty? There still may be work on the local 
queues of other worker threads. This is where work stealing comes 
into play. This is shown Figure 4.
figure 4
Work stealing
The diagram shows that when a thread has no items in its local queue, 
and there are also no items in the global queue, the system “steals” 
work from the local queue of one of the other worker threads. To 
minimize the amount of synchronization, the system takes the task 
from the public end of the second thread’s work-stealing queue. This 
means that, unless the queue is very short, the second thread can 
continue to push and pop from the private end of its local queue with 
minimal overhead for synchronization.
This mix of LIFO and FIFO ordering has other interesting benefits 
that arise from the work distribution patterns of typical applications. 
It turns out that LIFO order makes sense for local queues because it 
reduces the likelihood of a cache miss. Something that has just been 
placed in a work queue has a good chance of referencing objects that 
are still present in the system’s memory caches. One way to take  
advantage of the cache is by prioritizing recently added tasks.
Many parallel algorithms have a divide-and-conquer approach 
similar to recursion, as you’ll see later in Chapter 6, “Dynamic Task 
Parallelism.” The largest chunks of work tend to get pushed onto the 
queue before smaller subtasks. With FIFO ordering, these larger 
chunks are the first to be removed by other threads. Transferring a 
Empty local queue for worker thread 1
Steal from neighbor in FIFO order
Local task queue for worker thread 2
Push /pop local tasks in LIFO order
Worker thread 2
Worker thread 1
Empty global queue

60
chapter three
larger task to another thread reduces the need for stealing additional 
tasks in the future. As one of these larger tasks executes, it pushes and 
pops its subtasks in the new thread’s local queue. This is a very effi-
cient way to schedule these kinds of tasks.
top-level tasks in the global queue
Tasks are placed in the global queue whenever a task factory method 
is invoked from a thread that is not one of the thread pool worker 
threads. (Of course, the factory method must be allowed to use the 
default task scheduler for this to be true. The information in this  
section applies only to tasks managed by the default task scheduler.)
You can also force the default task scheduler to place a task in the 
global queue by passing the task creation option PreferFairness to 
the factory method.
In this book, tasks in the global queue are called top-level tasks. 
Top-level tasks have approximately the same performance character-
istics as work items that have been created with the thread pool’s 
QueueUserWorkItem method.
subtasks in a local queue
When one of the task factory methods is called from within a thread 
pool worker thread, the default task scheduler places the new task in 
that thread’s local task queue. This is a faster operation than placing 
it in the global queue.
The default task scheduler assumes that minimizing the worst-
case latency for subtasks isn’t important. Instead, its goal is to opti-
mize overall system throughput. This makes sense if you believe that 
any time you create a task from within another task or from within a 
thread pool work item, you are performing an operation that is part 
of some larger computation (such as a top-level task). In this case, the 
only latency that matters is that of the top-level task. Therefore, the 
default task scheduler doesn’t care about FIFO ordering of subtasks. 
Although these assumptions don’t hold in all cases, understanding 
them will help you use the features of the default task scheduler in 
the most efficient way for your application.
In this book, tasks in a local queue are known as subtasks. The 
motivation for this term is that most tasks that end up in a local queue 
are created while executing the user delegate of some other task.
inlined execution of subtasks
It’s often the case that a task must wait for a second task to complete 
before the first task can continue. If the second task hasn’t begun to 
execute, you might imagine that the thread that is executing the first 
task blocks until the second task is eventually allowed to run and 

 61
parallel tasks
complete. An unlucky queue position for the second task can make 
this an arbitrarily long wait, and in extreme cases can even result in 
deadlock if all other worker threads are busy.
Fortunately, the TPL can detect whether the second task has 
begun to execute. If the second task is not yet running, the default 
task scheduler can sometimes execute it immediately in the first task’s 
thread context. This technique, known as inlined execution, enables the 
reuse of a thread that would otherwise be blocked. It also eliminates 
the possibility of deadlock due to thread starvation. A nice side effect 
is that inline execution can reduce overall latency by acting as a sched-
uling short cut for urgent tasks.
The default task scheduler in .NET Framework 4 inlines a pending 
subtask if Task.Wait or Task.WaitAll is called from within the 
worker thread whose local queue contains that subtask. Inlining also 
applies to tasks created by methods of the Parallel class if these 
methods are called from within a worker thread. In other words, a 
thread pool worker thread can perform inline execution of tasks that 
it created. Top-level tasks in the global queue are never eligible to be 
inlined, and tasks created with the LongRunning task creation option 
do not inline other tasks.
The default task scheduler’s policy for inline execution was moti-
vated by the synchronization requirements of the work-stealing 
queues. Removing or marking a task in another local queue as  
“processed” would require additional, expensive cross-thread synchro-
nization. Also, it turns out that typical applications almost never need 
cross-thread inlining. The most common coding patterns result in 
subtasks that reside in the local queue of the thread that executes the 
parent task.
thread injection
The .NET thread pool automatically manages the number of worker 
threads in the pool. It adds and removes threads according to built-in 
heuristics. The .NET thread pool has two main mechanisms for inject-
ing threads: a starvation-avoidance mechanism that adds worker 
threads if it sees no progress being made on queued items and a hill-
climbing heuristic that tries to maximize throughput while using as 
few threads as possible.
The goal of starvation avoidance is to prevent deadlock. This kind 
of deadlock can occur when a worker thread waits for a synchroniza-
tion event that can only be satisfied by a work item that is still pend-
ing in the thread pool’s global or local queues. If there were a fixed 
number of worker threads, and all of those threads were similarly 
blocked, the system would be unable to ever make further progress. 
Adding a new worker thread resolves the problem.

62
chapter three
A goal of the hill-climbing heuristic is to improve the utilization 
of cores when threads are blocked by I/O or other wait conditions 
that stall the processor. By default, the managed thread pool has one 
worker thread per core. If one of these worker threads becomes 
blocked, there’s a chance that a core might be underutilized, depend-
ing on the computer’s overall workload. The thread injection logic 
doesn’t distinguish between a thread that’s blocked and a thread 
that’s performing a lengthy, processor-intensive operation. Therefore, 
whenever the thread pool’s global or local queues contain pending 
work items, active work items that take a long time to run (more than 
a half second) can trigger the creation of new thread pool worker 
threads.
The .NET thread pool has an opportunity to inject threads every 
time a work item completes or at 500 millisecond intervals, whichever 
is shorter. The thread pool uses this opportunity to try adding threads 
(or taking them away), guided by feedback from previous changes in 
the thread count. If adding threads seems to be helping throughput, 
the thread pool adds more; otherwise, it reduces the number of 
worker threads. This technique is called the hill-climbing heuristic.
Therefore, one reason to keep individual tasks short is to avoid 
“starvation detection,” but another reason to keep them short is to 
give the thread pool more opportunities to improve throughput by 
adjusting the thread count. The shorter the duration of individual 
tasks, the more often the thread pool can measure throughput and 
adjust the thread count accordingly.
To make this concrete, consider an extreme example. Suppose 
that you have a complex financial simulation with 500 processor-in-
tensive operations, each one of which takes ten minutes on average 
to complete. If you create top-level tasks in the global queue for each 
of these operations, you will find that after about five minutes the 
thread pool will grow to 500 worker threads. The reason is that the 
thread pool sees all of the tasks as blocked and begins to add new 
threads at the rate of approximately two threads per second.
What’s wrong with 500 worker threads? In principle, nothing, if 
you have 500 cores for them to use and vast amounts of system 
memory. In fact, this is the long-term vision of parallel computing. 
However, if you don’t have that many cores on your computer, you are 
in a situation where many threads are competing for time slices. This 
situation is known as processor oversubscription. Allowing many 
processor-intensive threads to compete for time on a single core adds 
context switching overhead that can severely reduce overall system 
throughput. Even if you don’t run out of memory, performance in this 

 63
parallel tasks
situation can be much, much worse than in sequential computation. 
(Each context switch takes between 6,000 and 8,000 processor cy-
cles.) The cost of context switching is not the only source of overhead. 
A managed thread in .NET consumes roughly a megabyte of stack 
space, whether or not that space is used for currently executing func-
tions. It takes about 200,000 CPU cycles to create a new thread, and 
about 100,000 cycles to retire a thread. These are expensive opera-
tions.
As long as your tasks don’t each take minutes, the thread pool’s 
hill-climbing algorithm will eventually realize it has too many threads 
and cut back on its own accord. However, if you do have tasks that 
occupy a worker thread for many seconds or minutes or hours, that 
will throw off the thread pool’s heuristics, and at that point you 
should consider an alternative.
The first option is to decompose your application into shorter 
tasks that complete fast enough for the thread pool to successfully 
control the number of threads for optimal throughput.
A second possibility is to implement your own task scheduler 
object that does not perform thread injection. If your tasks are of long 
duration, you don’t need a highly optimized task scheduler because 
the cost of scheduling will be negligible compared to the execution 
time of the task. MSDN® developer program has an example of a 
simple task scheduler implementation that limits the maximum degree 
of concurrency. For more information, see the section, “Further Read-
ing,” at the end of this chapter.
As a last resort, you can use the SetMaxThreads method to 
configure the ThreadPool class with an upper limit for the number 
of worker threads, usually equal to the number of cores (this is the 
Environment.ProcessorCount property). This upper limit applies for 
the entire process, including all AppDomains.
bypassing the thread pool
If you don’t want a task to use a worker thread form the thread pool, 
you can create a new thread for its dedicated use. The new thread will 
not be a thread pool worker thread. To do this, include the Long
Running task creation option as an argument to one of the task 
factory methods. The option is mostly used for tasks with long I/O- 
related wait conditions and for tasks that act as background helpers.
A disadvantage of bypassing the thread pool is that, unlike a 
worker thread that is created by the thread pool’s thread injection 
logic, a thread created with the LongRunning option cannot use 
inline execution for its subtasks.
The SetMaxThreads method 
can cause deadlock if thread 
pool worker threads are 
waiting for scheduled work 
items to run. Use it with 
extreme caution.

64
chapter three
Exercises
1.	 The image blender example in this chapter uses task  
parallelism: a different task processes each image layer.  
A typical strategy in image processing uses data parallelism: 
the same computation processes different portions of  
an image or different images. Is there a way to use data 
parallelism in the image blender example? If there is,  
what are the advantages and disadvantages, compared  
to the task parallelism discussed here?
2.	 In the image blender sample, the image processing methods 
SetToGray and Rotate are void methods that do not return 
results, but they save their results by updating their second 
argument. Why don’t they return their results?
3.	 In the image blender sample that uses Task.Factory.
StartNew, what happens if one of the parallel tasks throws 
an exception? Answer the same question for the sample that 
uses Parallel.Invoke.

 65
parallel tasks
Further Reading
Leijen et al. discusses design considerations, including scheduling and 
work stealing. Hoag provides a detailed discussion of task creation 
options. Microsoft’s Parallel Samples package is ParExtSamples at 
MSDN. The how-to article on MSDN is a good resource for writing 
custom task schedulers. The Task Scheduler Events page on MSDN 
provides more information about unobserved task exceptions.
D. Leijen, W. Schulte, and S. Burckhardt. “The Design of a  
Task Parallel Library.” S. Arora and G.T. Leavens, editors, OOP-
SLA 2009: Proceedings of the 24th Annual ACM SIGPLAN 
Conference on Object-Oriented Programming, Systems,  
Languages, and Applications, pages 227–242. ACM, 2009.
J. E. Hoag. “A Tour of Various TPL Options.” April 2009. 
http://blogs.msdn.com/b/pfxteam/archive/2010/04/19/9997552.
aspx
ParExtSamples software. “Samples for Parallel Programming 
with the .NET Framework 4.” 
http://code.msdn.microsoft.com/ParExtSamples
“How to: Create a Task Scheduler That Limits the Degree  
of Concurrency.” 
http://msdn.microsoft.com/en-us/library/ee789351.aspx
.NET Framework Class Library. “TaskScheduler Events.” 
http://msdn.microsoft.com/en-us/library/system.threading.tasks.
taskscheduler_events.aspx


67
Parallel Aggregation
4
The Parallel Aggregation 
pattern lets you use multiple 
cores to calculate sums and 
other types of accumulations 
that are based on associative 
operators.
Chapter 2 shows how to use parallel techniques that apply the same 
independent operation to many input values. However, not all parallel 
loops have loop bodies that execute independently. For example, a 
loop that calculates a sum does not have independent steps. All the 
steps accumulate their results in a single variable that represents  
the sum calculated up to that point. This accumulated value is an  
aggregation.
Nonetheless, there is a way for an aggregation operation to use a 
parallel loop. This is the Parallel Aggregation pattern.
Although calculating a sum is an example of aggregation, the pat-
tern is more general than that. It works for any binary operation that 
is associative. The .NET implementation of the Parallel Aggregation 
pattern also expects the operations to be commutative.
The Parallel Aggregation pattern uses unshared, local variables 
that are merged at the end of the computation to give the final result. 
Using unshared, local variables for partial, locally calculated results  
is how the steps of a loop can become independent of each other. 
Parallel aggregation demonstrates the principle that it’s usually better 
to make changes to your algorithm than to add synchronization 
primitives to an existing algorithm. For more information about the 
algorithmic aspects of this pattern, see the section, “Design Notes,” 
later in this chapter.
The Parallel Aggregation pattern is also known as the Parallel 
Reduction pattern because it combines multiple inputs into a single 
output.

68
chapter four
The Basics
The most familiar application of aggregation is calculating a sum. 
Here’s a sequential version.
double[] sequence = ...
double sum = 0.0d;
for (int i = 0; i < sequence.Length; i++)
{
  sum += Normalize(sequence[i]);
}
return sum;
This is a typical sequential for loop. In this example and the ones that 
follow, Normalize is a user-provided method that transforms the 
input values in some way, such as converting them to an appropriate 
scale. The result is the sum of the transformed values.
The Microsoft® .NET Framework Language Integrated Query 
(LINQ) provides a very simple way to express this kind of aggregation. 
Languages such as C#, F#, and Microsoft Visual Basic® development 
system provide special syntax for LINQ. The following LINQ expres-
sion calculates a sum.
  double[] sequence = ...
  return (from x in sequence select Normalize(x)).Sum();
The LINQ expression is a sequential operation whose performance is 
comparable to the sequential for loop shown in previous example.
To convert a LINQ-to-Objects expression into a parallel query is 
extremely easy. The following code gives an example.
  double[] sequence = ...
  return (from x in sequence.AsParallel() 
          select Normalize(x)).Sum();
If you invoke the AsParallel extension method, you’re instructing the 
compiler to bind to PLINQ instead of to LINQ. The program will use 
the parallel versions of all further query operations within the expres-
sion. The Sum extension method executes the query and (behind the 
scenes and in parallel) calculates the sum of the selected, transformed 
values. For an introduction to PLINQ, see Chapter 2, “Parallel Loops.”
This example uses addition as the underlying aggregation opera-
tor, but there are many others. For example, PLINQ has built-in stan-
dard query operators that count the number of elements and calculate 
the average, maximum, or minimum. PLINQ also has operators that 
create and combine sets (duplicate elimination, union, intersection, 
and difference), transform sequences (concatenation, filtering, and 
partitioning) and group (projection). These standard query operators 

 69
parallel aggregation
are sufficient for many types of aggregation tasks, and with PLINQ 
they all can efficiently use the hardware resources of a multicore 
computer.
If PLINQ’s standard query operators aren’t what you need, you 
can also use the Aggregate extension method to define your own 
aggregation operators. Here’s an example.
  double[] sequence = ...
  return (from x in sequence.AsParallel() select Normalize(x))
             .Aggregate(1.0d, (y1, y2) => y1 * y2);
This code shows one of the overloaded versions of the Aggregate 
extension method. It applies a user-provided transformation to each 
element of the input sequence and then returns the mathematical 
product of the transformed values.
PLINQ is usually the recommended approach whenever you need 
to apply the parallel aggregation pattern to .NET applications. Its 
declarative nature makes it less prone to error than other approaches, 
and its performance on multicore computers is competitive with 
them. Implementing parallel aggregation with PLINQ doesn’t require 
adding locks in your code. Instead, all the synchronization occurs in-
ternally, within PLINQ.
If PLINQ doesn’t meet your needs or if you prefer a less declara-
tive style of coding, you can also use Parallel.For or Parallel.ForEach 
to implement the parallel aggregation pattern. The Parallel.For and 
Parallel.ForEach methods require more complex code than PLINQ. 
For example, the Parallel.ForEach method requires your code to in-
clude synchronization primitives to implement parallel aggregation. 
For examples and more information, see the section, “Using Parallel 
Loops for Aggregation,” under “Variations” later in this chapter.
An Example
Aggregation doesn’t only apply to numeric values. It’s a more general 
pattern that arises in many application contexts. The following ex-
ample shows how a variation of parallel aggregation known as map/
reduce is used to aggregate nonscalar data types.
The example is of a social network service, where subscribers can 
designate other subscribers as friends. The site recommends new 
friends to each subscriber by identifying other subscribers who are 
friends of friends. To limit the number of recommendations, the ser-
vice only recommends the candidates who have the largest number of 
mutual friends. Candidates can be identified in independent parallel 
operations, and then candidates are ranked and selected in an aggre-
gation operation.

70
chapter four
Here’s how the data structures and algorithms that are used  
by the recommendation service work. Subscribers are identified by 
integer ID numbers. A subscriber’s friends are represented by the  
collection of their IDs. The collection is a set because each element 
(a friend’s ID number) occurs only once and the order of the elements 
doesn’t matter. For example, the subscriber whose ID is 0 has two 
friends whose IDs are 1 and 2. This can be written as:
0 -> { 1, 2 }
The social network repository stores an entry like this for every  
subscriber. In order to recommend friends to a subscriber, the recom-
mendation service must consider a subscriber’s entry, as well as the 
entries for all of that subscriber’s friends. For example, to recommend 
friends for subscriber 0, the pertinent entries in the repository are:
0 -> { 1, 2 }
1 -> { 0, 2, 3 }
2 -> { 0, 1, 3, 4 }
You can see that the service should recommend subscribers 3 and 4 to 
subscriber 0 because they appear among the friends of subscribers 1 
and 2, who are already friends of 0. In addition, the recommendation 
service should rank subscriber 3 higher than 4, because 3 is a friend of 
both of 0’s friends, while 4 is a friend of only one of them. You can 
write the results like this:
{ 3(2), 4(1) }
This means that subscriber 3 shares two mutual friends with sub-
scriber 0, and subscriber 4 shares one. This is an example of a type of 
collection known as a multiset. In a multiset, each element (3 and 4 
in this example) is associated with a multiplicity, which is the number 
of times it occurs in the collection (2 and 1, respectively). So a multiset 
is a collection where each element only occurs once, yet it can repre-
sent duplicates (or larger multiplicities). The order of elements in a 
multiset doesn’t matter.
The recommendation service uses map/reduce, which has several 
phases. In the first phase, which is the map phase, the service creates 
a collection of candidates that can contain duplicates — the same 
candidate’s ID can occur several times in the list (once for each  
mutual friend). In the second phase, which is the reduce phase, the 
service aggregates this collection to create a multiset where each 
candidate’s ID occurs only once, but is associated with its multiplicity 
in the first collection (the number of mutual friends). There is also a 
postprocessing phase where the service ranks candidates by sorting 
them according to their multiplicity and selects only the candidates 
with the largest multiplicities.

 71
parallel aggregation
An important feature of map/reduce is that the result of the map 
stage is a collection of items that is compatible with the reduce stage. 
The reduce stage uses multisets; therefore, the map stage does not 
produce only a list of candidate IDs; instead, it produces a collection 
of multisets, where each multiset contains only one candidate with a 
multiplicity of one. In this example, the output of the map stage is a 
collection of two multisets. The subscribers are the numbers 3 and 4.
{ 3(1) }, { 3(1) , 4(1) } 
Here, the first multiset contains friends of subscriber 1, and the  
second multiset contains friends of subscriber 2.
Another important feature of map/reduce is that the aggregation 
in the reduce phase is performed by applying a binary operation to 
pairs of elements from the collection that is produced by the map 
phase. In this example, the operation is a multiset union, which 
combines two multisets by collecting the elements and adding their 
multiplicities. The result of applying the multiset union operation to 
the two multisets in the preceding collection is:
{ 3(2), 4(1) }
Now that there is only one multiset, the reduce phase is complete. By 
repeatedly applying the multiset union operation, the reduce phase 
can aggregate any collection of multisets, no matter how large, into 
one multiset.
Here is the code for the sequential version.
public IDMultisetItemList PotentialFriendsSequential(
                                         SubscriberID id,
                                         int maxCandidates)
{
  // Map
  var foafsList = new List<IDMultiset>();
  foreach (SubscriberID friend in subscribers[id].Friends)
  {
    var foafs = subscribers[friend].FriendsCopy();
    foafs.RemoveWhere(foaf => foaf == id || 
                      subscribers[id].Friends.Contains(foaf));
    foafsList.Add(Multiset.Create(foafs));
  }
  // Reduce
  IDMultiset candidates = new IDMultiset();
  foreach (IDMultiset foafs in foafsList)
  {
    candidates = Multiset.Union(foafs, candidates);

72
chapter four
  }
  // Postprocess
  return Multiset.MostNumerous(candidates, maxCandidates);
}
In the map phase, this code loops sequentially over the subscriber’s 
friends and builds a collection of multisets of candidates (a foaf is a 
friend of a friend). In the reduce phase, the code loops sequentially 
over those multisets and aggregates them with the multiset union 
operation. If this code executes with the few subscribers in the ex-
ample, the id argument is 0 and subscribers[id].Friends is { 1, 2}. 
When the map phase completes, foafsList is { 3(1) }, { 3(1) , 4(1) }, and 
when the reduce phase completes, candidates is { 3(2), 4(1) }.
Multiset union is commutative: the result does not depend on the 
order of its arguments. Multiset union is also associative: if you ag-
gregate several multisets into one by successively forming unions in a 
pair-wise manner, the final result does not depend on the order of the 
union operations. If the aggregation function is not associative, it 
can’t be done in parallel without potentially getting different results. 
If it’s not commutative, the potential for parallelism is greatly reduced.
Here’s how to use PLINQ to apply map/reduce to the social net-
working example.
public IDMultisetItemList PotentialFriendsPLinq(SubscriberID id, 
                                           int maxCandidates)
{
  var candidates = 
    subscribers[id].Friends.AsParallel()                    
    .SelectMany(friend => subscribers[friend].Friends)
    .Where(foaf => foaf != id && 
           !(subscribers[id].Friends.Contains(foaf)))
    .GroupBy(foaf => foaf)                               
    .Select(foafGroup => new IDMultisetItem(foafGroup.Key,
                                       foafGroup.Count()));
  return Multiset.MostNumerous(candidates, maxCandidates);
}
Recall that in map/reduce, independent parallel operations (the map 
phase) are followed by aggregation (the reduce phase). In the map 
phase, the parallel operations iterate over all the friends of subscriber 
0. The map phase is performed by the SelectMany method, which 
finds all the friends of each friend of the subscriber, and the Where 
method, which prevents redundant recommendations by removing 
the subscriber and the subscriber’s own friends. The output of the 
map phase is a collection of candidate IDs, including duplicates. The 
Strictly speaking, floating-
point arithmetic is neither 
commutative nor associative. 
From run to run, parallel 
computations over floats or 
doubles may end up with 
slightly different results.

 73
parallel aggregation
reduce phase is performed by the GroupBy method, which collects 
duplicate candidate IDs into groups, and the Select method, which 
converts each group into a multiset item that associates the candidate 
ID with a multiplicity (or Count). The return statement performs the 
final postprocessing step that selects the candidates with the highest 
multiplicities.
When map/reduce is implemented with PLINQ, it need not be a 
line-by-line translation of the foreach version. In the PLINQ example, 
the output of the map stage is not a collection of multisets, but a 
collection with duplicates. The multiset is not formed until the reduce 
stage.
The online source code for this example also includes map/reduce 
implemented with the Parallel.ForEach method.
Variations
This section contains some common variations of the parallel aggrega-
tion pattern.
using parallel loops for aggregation
The Parallel.ForEach and Parallel.For methods have overloaded ver-
sions that can implement the parallel aggregation pattern. Here’s an 
example.
  double[] sequence = ...
  object lockObject = new object();
  double sum = 0.0d;
  
  Parallel.ForEach(
    // The values to be aggregated 
    sequence,
    
    // The local initial partial result
    () => 0.0d,
    // The loop body
    (x, loopState, partialResult) =>
    {
       return Normalize(x) + partialResult;
    },
    
    // The final step of each local context            
    (localPartialSum) =>
    {
       // Enforce serial access to single, shared result

74
chapter four
       lock (lockObject)
       {
         sum += localPartialSum;
       }
    });
  return sum;
Parallel.ForEach partitions the input based on the desired degree 
of parallelism and creates parallel tasks to perform the work. Each 
parallel task has local state that isn’t shared with other tasks. The loop 
body updates only the task-local state. In other words, the loop body 
accumulates its value first into a subtotal and not directly into the  
final total. When each task finishes, it adds its subtotal into the final 
sum. For an illustration of these steps, see Figure 1 in the section, 
“Design Notes,” later in this chapter.
Here is the signature of the overloaded version of Parallel.
ForEach that’s used in this example.
Parallel.ForEach<TSource, TLocal>(
  IEnumerable<TSource> source, 
  Func<TLocal> localInit, 
  Func<TSource, ParallelLoopState, TLocal, TLocal> body, 
  Action<TLocal> localFinally);
There are four arguments. The first argument is the data values that 
will be aggregated. The second argument is a delegate that returns the 
initial value of the aggregation. The third argument is a delegate  
that combines one of the input data values with a partial sum from a 
previous iteration.
The Task Parallel Library (TPL) implementation of Parallel.
ForEach creates tasks in order to execute the parallel loop. When the 
loop creates multiple tasks, there will be one partial result value for 
each task. The number of worker tasks is determined by the parallel 
loop implementation, based on heuristics. Generally, there will be at 
least one worker task per available core, but there could be more. As 
the loop progresses, TPL will sometimes retire tasks and create new 
ones.
When the parallel loop is ready to finish, it must merge the partial 
results from all of its worker tasks together to produce the final result. 
The fourth argument to Parallel.ForEach is a delegate that performs 
the merge step. The delegate is invoked once for each of the worker 
tasks. The argument to the delegate is the partial result that was cal-
culated by the task. This delegate locks the shared result variable and 
combines the partial sum with the result.
This example uses the C# lock keyword to enforce interleaved, 
sequential access to variables that are shared by concurrent threads. 

 75
parallel aggregation
There are other synchronization techniques that could be used in this 
situation, but they are outside the scope of this book. Be aware that 
locking is cooperative; that is, unless all threads that access the shared 
variable use locking consistently and correctly, serial access to the 
variable is not guaranteed.
The syntax for locking in C# is lock ( object ) { body }. The object 
uniquely identifies the lock. All cooperating threads must use the 
same synchronizing object, which must be a reference type such as 
Object and not a value type such as int or double. When you use lock 
with Parallel.For or Parallel.ForEach you should create a dummy 
object and set it as the value of a captured local variable dedicated to 
this purpose. (A captured variable is a local variable from the enclosing 
scope that is referenced in the body of a lambda expression.) The 
lock’s body is the region of code that will be protected by the lock. 
The body should take only a small amount of execution time. Which 
shared variables are protected by the lock object varies by application 
and is something that all programmers whose code accesses those 
variables must be careful not to contradict. In this example, the lock 
object ensures serial access to the sum variable.
When a thread encounters a lock statement, it attempts to  
acquire the lock associated with the synchronizing object. Only one 
thread at a time can hold this lock. If another thread has already  
acquired the lock but has not yet released it, the current thread will 
block until the lock becomes available. When multiple threads  
compete for a lock, the order in which they acquire the lock is unpre-
dictable; it’s not necessarily in FIFO order. After the thread success-
fully acquires the lock, it executes the statements inside of the body. 
When the thread exits the lock body (whether normally or by throw-
ing an exception), it releases the lock. For more information about 
locking, see the section, “Design Notes,” later in this chapter.
The localFinally delegate (argument four) is executed once at the 
end of each task. Therefore, the number of locks taken will be equal 
to the number of tasks that are used to execute the parallel loop. You 
can’t predict how many tasks will be used. For example, you shouldn’t 
assume that the number of tasks will be less than or equal to the 
number that you provided with the parallel option MaxDegree
OfParallelism (or its default value). The reason for this is that the 
parallel loop sometimes shuts down tasks during the execution of the 
loop and continues with new tasks. This cooperative yielding of con-
trol can in some cases reduce system latency by allowing other tasks 
to run. It also helps prevent an unintended increase in the number of 
thread pool worker threads. You shouldn’t consider the state of the 
accumulator to be strictly “thread local.” It’s actually “task local.” (The 
distinction between thread local and task local won’t normally affect 
You should document the 
variables that are protected by 
a lock with a comment in the 
code. It’s easy to make a 
mistake.

76
chapter four
a program’s logic because a task is guaranteed to execute from start 
to finish on only one thread.)
For comparison, here’s the PLINQ version of the example used in 
this variation.
  double[] sequence = ...
  return sequence.AsParallel().Select(Normalize).Sum();
using a range partitioner for aggregation
When you have a loop body with a very small amount of work to do, 
and there are many iterations to perform, it’s possible that the over-
head of Parallel.ForEach is large compared to the cost of executing 
the loop body.
In this case, it’s sometimes more efficient to use a Partitioner 
object for the loop. A Partitioner object allows you to embed a 
sequential for loop inside of your Parallel.ForEach loop and reduce 
the number of iterations of the Parallel.ForEach loop. Generally, you 
should profile the application in order to decide whether to use a 
Partitioner object.
Here’s an example.
  double[] sequence = ...
  object lockObject = new object();
  double sum = 0.0d;
  var rangePartitioner = Partitioner.Create(0, sequence.Length);
            
  Parallel.ForEach(
    // The input intervals
    rangePartitioner, 
   
    // The local initial partial result
    () => 0.0, 
    // The loop body for each interval
    (range, loopState, initialValue) =>
    {
      double partialSum = initialValue;
      for (int i = range.Item1; i < range.Item2; i++)
      {
        partialSum += Normalize(sequence[i]);
      }
      return partialSum;
    },
    
    // The final step of each local context

 77
parallel aggregation
    (localPartialSum) =>
    {
      // Use lock to enforce serial access to shared result
      lock (lockObject)
      {
        sum += localPartialSum;
      }
    });
  return sum;
This code is very similar to the example in the section, “Using Parallel 
Loops for Aggregation,” earlier in this chapter. The main difference is 
that the Parallel.ForEach loop uses a sequence of index intervals, 
which are generated by the Partitioner object, as its input instead of 
individual values. This can avoid some of the overhead involved in in-
voking delegate methods. You’ll notice a benefit only when the 
amount of work in each step is small and there are many steps.
Here’s the signature of the overloaded version of the Parallel.
ForEach method that was used.
Parallel.ForEach<TSource, TLocal>(
  Partitioner<TSource> source, 
  Func<TLocal> localInit, 
  Func<TSource, ParallelLoopState, TLocal, TLocal> body, 
  Action<TLocal> localFinally);
using plinq aggregation with  
range selection
The PLINQ Aggregate extension method includes an overloaded ver-
sion that allows a very general application of the parallel aggregation 
pattern. Here’s an example from an application that does financial 
simulation. The method performs repeated simulation trials and ag-
gregates results into a histogram. There are two dependencies that 
must be handled by this code. They are the accumulation of partial 
results into the result histogram and the use of instances of the  
Random class. (An instance of Random cannot be shared across 
multiple threads.)
  int[] histogram = MakeEmptyHistogram();
  
  return ParallelEnumerable.Range(0, count).Aggregate(
    // 1- Create an empty local accumulator object
    //    that includes all task-local state.
    () => new Tuple<int[], Random>(

78
chapter four
             MakeEmptyHistogram(), 
             new Random(SampleUtilities.MakeRandomSeed())),
    // 2- Run the simulation, adding result to local accumulator. 
    (localAccumulator, i) =>
    {
      // With each iteration, get the next random value. 
      var sample = localAccumulator.Item2.NextDouble();
      if (sample > 0.0 && sample < 1.0)
      {
        // Perform a simulation trial for the sample value.
        var simulationResult = 
                            DoSimulation(sample, mean, stdDev);
        // Add result to the histogram of the local accumulator.
        int histogramBucket = 
                 (int)Math.Floor(simulationResult / BucketSize);
        if (0 <= histogramBucket && histogramBucket < TableSize)
          localAccumulator.Item1[histogramBucket] += 1;                       
      }
      return localAccumulator;
    },
    // 3- Combine local results pair-wise.
    (localAccumulator1, localAccumulator2) =>
    {
      return new Tuple<int[], Random>(                           
        CombineHistograms(localAccumulator1.Item1,                                           
                          localAccumulator2.Item1), 
        null);
    },
    // 4- Extract answer from final combination.
    finalAccumulator => finalAccumulator.Item1
    ); // Aggregate
The data source in this example is a parallel query that’s created with 
the Range static method of the ParallelEnumerable class. Here’s the 
signature of the overloaded version of the Aggregate extension 
method that was used in this example.
For information about the 
statistical limitations of the 
Random class when used in 
parallel programs, see Chapter 
3, “Parallel Tasks.”

 79
parallel aggregation
Aggregate<TSource, TAccumulate, TResult>(
  this ParallelQuery<TSource> source, 
  Func<TAccumulate> seedFactory, 
  Func<TAccumulate, TSource, TAccumulate> updateAccumulatorFunc, 
  Func<TAccumulate, TAccumulate, TAccumulate> 
                                         combineAccumulatorsFunc, 
  Func<TAccumulate, TResult> resultSelector);
There are four arguments. Each of the arguments is a delegate method.
The first argument is a delegate that establishes the local state of 
each worker task that the query creates. This is called once, at the 
beginning of the task. In this example, the delegate returns a tuple 
(unnamed record) instance that contains two fields. The first field is 
an empty histogram. This will accumulate the local partial results of 
the simulation in this task. The second field is an instance of the  
Random class. It’s part of the local state to ensure that the simulation 
does not violate the requirements of the Random class by sharing 
instances across threads. Note that you can store virtually any kind of 
local state in the object you create.
The second argument is the loop body. This delegate is invoked 
once per data element in the partition. In this example, the loop body 
creates a new random sample and performs the simulation experi-
ment. Then it classifies the result of the simulation into buckets used 
by the histogram and increments the appropriate bucket of the task’s 
local histogram.
The third argument is invoked for pair-wise combinations of local 
partial results. The delegate merges the input histograms (it adds their 
corresponding bucket values and returns a new histogram with the 
sum). It returns a new tuple. The null argument reflects the fact that 
the random number generator is no longer needed. The combination 
delegate is invoked as many times as necessary to consolidate all the 
local partial results.
The fourth argument selects the result from the final, merged 
local state object.
This variation can be adapted to many situations that use the 
Parallel Aggregation pattern. Note that this implementation doesn’t 
require locks.

80
chapter four
Design Notes
If you compare the sequential and parallel versions of the aggregation 
pattern, you see that the design of the parallel version includes an 
additional step in the algorithm that merges partial results. Figure 1, 
which illustrates the Parallel.ForEach and Parallel.For methods, 
shows this.
figure 1
Aggregation using Parallel For and ForEach 
Figure 1 shows that instead of placing the accumulation in a single, 
shared result, the parallel loop uses unshared local storage for partial 
results (these are named subtotals in Figure 1). Each task processes a 
single partition of the input values. The number of partitions depends 
on the degree of parallelism that’s needed to efficiently use the com-
puter’s available cores. After a task finishes accumulating the values in 
its assigned partition, it merges its local result into the final, global 
result. The final result is shared by all tasks. Locks are required to en-
sure that updates are consistent.
The reason that this approach is fast is because there are very few 
locks. In normal cases, the number of elements to be processed is 
many times larger than the number of tasks and partitions. The cost 
of serializing locks can be amortized over many individual accumula-
tion operations.
The algorithm used by PLINQ differs slightly from what’s shown 
in Figure 1. Aggregation in PLINQ does not require the developer to 
use locks. Instead, the final merge step is expressed as a binary opera-
tor that combines any two partial result values (that is, two of the 
subtotals) into another partial result. Repeating this process on subse-
quent pairs of partial results eventually converges on a final result. 
One of the advantages of the PLINQ approach is that it requires less 
synchronization, so it’s more scalable. The binary combinations do not 
...
...
...
Accumulate 
into subtotal
Accumulate 
into subtotal
Accumulate 
into subtotal
Add subtotal
to result
Add subtotal
to result
Add subtotal
to result
Partitioned
input values
Independent 
tasks
Coordinated
by serializing
locks

 81
parallel aggregation
You can’t just add locks  
and expect to get good  
performance. You also  
need to think about the 
algorithm.
require the system to lock the final result; they can be performed in 
parallel. (This is possible because the binary combination operation is 
associative, so the order of these operations doesn’t affect the result.)
This discussion shows that the parallel aggregation pattern is a 
good example of why changes to your algorithm are often needed 
when moving from a sequential to parallel approach.
To make this point clear, here’s an example of what parallel ag-
gregation would look like if you simply added locks to the existing 
sequential algorithm. You just need to convert sequential for to 
Parallel.For and add one lock statement.
  // Do not copy this code. This version will run much slower
  // than the sequential version. It’s included here to 
  // illustrate what not to do.
  double[] sequence = ...
  object lockObject = new object();
  double sum = 0.0d;
  // BUG – don’t do this
  Parallel.For(0, sequence.Length, i =>
  {
     // BUG – don’t do this
     lock (lockObject)
     {
       sum += sequence[i];
     }
  });
  return sum;
If you forget to add the lock statement, this code fails to calculate the 
correct sum on a multicore computer. Adding the lock statement 
makes this code example correct with respect to serialization. If you 
run this code, it produces the expected sum. However, it fails  
completely as an optimization. This code is many times slower than 
the sequential version it attempted to optimize! The reason for the 
poor performance is the cost of synchronization.
In contrast, the examples of the parallel aggregation pattern that 
you have seen elsewhere in this chapter will run much faster on  
multicore computers than the sequential equivalent, and their perfor-
mance also improves in approximate proportion to the number of 
cores.
It might at first seem counterintuitive that adding additional steps 
can make an algorithm go faster, but it’s true. If you introduce extra 
work and that work has the effect of preventing data dependencies 
between parallel tasks, you often benefit in terms of performance.

82
chapter four
Related Patterns
There’s a group of patterns related to summarizing data in a collec-
tion. Aggregation (also known as Reduce) is one of them. The others 
include Scan and Pack. The Scan pattern occurs when each iteration 
of a loop depends on data computed in the previous iteration. The 
Pack pattern uses a parallel loop to select elements to retain or dis-
card. The result of a pack operation is a subset of the original input. 
These patterns can be combined, as in the Fold and Scan pattern.  
For more information about these related patterns, see the section, 
“Further Reading,” at the end of this chapter.
Exercises
1.	 Consider the small social network example (with subscribers 
0, 1, 2). What constraints exist in the data? How are these 
constraints observed in the sample code?
2.	 In the social network example, there’s a separate 
postprocessing step where the multiset of candidates, which 
is an unordered collection, is transformed into a sequence 
that is sorted by the number of mutual friends, and then the 
top N candidates are selected. Could some or all of this 
postprocessing be incorporated into the reduction step? 
Provide answers for both the PLINQ and Parallel.ForEach 
versions.
3.	 In the standard reference on map/reduce (see the section, 
“Further Reading”), the map phase executes a map function 
that takes an input pair and produces a set of intermediate 
key/value pairs. All pairs for the same intermediate key are 
passed to the reduce phase. That reduce phase executes a 
reduce function that merges all the values for the same 
intermediate key to a possibly smaller set of values. The signa-
tures of these functions can be expressed as: map (k1,v1)  -> 
list(k2,v2) and reduce (k2,list(v2))  -> list(v2). In the social 
network example, what are the types of k1, v1, k2, and v2? 
What are the map and reduce functions? Provide answers for 
both the PLINQ and Parallel.ForEach versions.

 83
parallel aggregation
Further Reading
MSDN® describes the standard query operators of LINQ and PLINQ. 
A thorough treatment of synchronization techniques appears in the 
book by Duffy. The related patterns of Stencil, Scan, and Pack are 
discussed by McCool. The standard reference on map/reduce is the 
paper by Dean and Ghemawat. Other cases of algorithms that use 
parallel loops with some dependencies between steps are described 
by Toub. These include fold-and-scan and dynamic programming.
J. Dean and S. Ghemawat. “MapReduce: Simplified Data  
Processing on Large Clusters.” In OSDI ‘04: Sixth Symposium on 
Operating System Design and Implementation, 137–150, 2004.
J. Duffy. Concurrent Programming on Windows, Addison-Wesley, 
2008.
M. McCool. “Structured Patterns: An Overview.” December 
2009. http://www.ddj.com/go-parallel/article/showArticle.
jhtml?articleID=223101515
S. Toub. “Patterns of Parallel Programming: Understanding and 
Applying Parallel Patterns with the .NET Framework 4.” 2009.  
http://www.microsoft.com/downloads/details.aspx?FamilyID= 
86b3d32b-ad26-4bb8-a3ae-c1637026c3ee&displaylang=en
“Standard Query Operators Overview.” 
http://msdn.microsoft.com/en-us/library/bb397896.aspx


85
Futures
5
Futures are asynchronous 
functions.
A future in .NET is a task 
that returns a value. Futures 
are implemented by the 
Task<TResult> class.
In Chapter 3, “Parallel Tasks,” you saw how the Parallel Task pattern 
allows you to fork the flow of control in a program. In this chapter, 
you’ll see how control flow and data flow can be integrated with the 
Futures pattern.
A future is a stand-in for a computational result that is initially 
unknown but becomes available at a later time. The process of calcu-
lating the result can occur in parallel with other computations. The 
Futures pattern integrates task parallelism with the familiar world of 
arguments and return values. If the parallel tasks described in Chapter 
3 are asynchronous actions, futures are asynchronous functions. (Recall 
that actions don’t return values, but functions do.)
Futures express the concept of potential parallelism that was in-
troduced in Chapter 1, “Introduction.” Decomposing a sequential 
operation with futures can result in faster execution if hardware re-
sources are available for parallel execution. However, if all cores are 
otherwise occupied, futures will be evaluated without parallelism.
In the Microsoft® .NET Framework, futures are implemented 
with the Task<TResult> class, where the type parameter TResult 
gives the type of the result. In other words, a future in .NET is a task 
that returns a value. Instead of explicitly waiting for the task to com-
plete using a method such as Wait, you simply ask the task for its 
result when you are ready to use it. If the task has already finished, its 
result is waiting for you and is immediately returned. If the task is 
running but has not yet finished, the thread that needs the result 
blocks until the result value becomes available. (While the thread is 
blocked, the core is available for other work.) If the task hasn’t started 
yet, the task will be executed inline in the current thread context, if 
possible.
The .NET Framework also implements a variation of the Futures 
pattern known as continuation tasks. A .NET continuation task is a 
task that automatically starts when other tasks, known as its anteced-
ents, complete. In many cases, the antecedents consist of futures 

86
chapter five
whose result values are used as input by the continuation task. An 
antecedent may have more than one continuation task.
Continuation tasks represent the nested application of asynchro-
nous functions. In some ways, continuation tasks act like callback 
methods—in both cases, you register an operation that will be  
automatically invoked at a specified point in the future.
The Futures pattern discussed in this chapter is closely related to 
what is sometimes known as a task graph. When futures provide 
results that are the inputs to other futures, this can be seen as a  
directed graph. The nodes are tasks, and the arcs are values that act as 
inputs and outputs of the tasks.
The Basics
When you think about the Parallel Task pattern described in Chapter 
3, you see that, in many cases, the purpose of a task is to calculate a 
result. In other words, asynchronous operations often act like  
functions. Of course, tasks can also do other things, such as reorder-
ing values in an array, but calculating new values is common enough 
to warrant a pattern tailored to it. It’s also much easier to reason 
about pure functions, which don’t have side effects and therefore 
exist purely for their results. This simplicity becomes very useful as 
the number of cores becomes large.
futures
The following example is from the body of a sequential method.
  var b = F1(a); 
  var c = F2(a); 
  var d = F3(c); 
  var f = F4(b, d);
  return f;
Suppose that F1, F2, F3, and F4 are processor-intensive functions that 
communicate with one another using arguments and return values 
instead of reading and updating shared state variables.
Suppose, also, that you want to distribute the work of these func-
tions across available cores, and you want your code to run correctly 
no matter how many cores are available. When you look at the inputs 
and outputs, you can see that F1 can run in parallel with F2 and F3 but 
that F3 can’t start until after F2 finishes. How do you know this? The 
possible orderings become apparent when you visualize the function 
calls as a graph. Figure 1 illustrates this.
A continuation task  
automatically starts  
when specified antecedent 
tasks finish.
Be careful not to confuse 
futures with pipelines. As you 
will see in Chapter 7, “Pipe-
lines,” pipeline tasks are also 
nodes of a directed graph, but 
the arcs that connect stages of 
the pipeline are concurrent 
queues that convey a series of 
values, like an assembly line or 
data stream. In contrast, with 
futures, nodes of the task 
graph are connected by 
singleton values, similar to 
arguments and return values.

 87
futures
figure 1
A task graph for calculating f
The nodes of the graph are the functions F1, F2, F3, and F4. The in-
coming arrows for each node are the inputs required by the function, 
and the outgoing arrows are values calculated by each function. It’s 
easy to see that F1 and F2 can run at the same time but that F3 must 
follow F2.
Here’s an example that shows how to create futures for this  
example. For simplicity, the code assumes that the values being  
calculated are integers and that the value of variable a has already 
been supplied, perhaps as an argument to the current method.
  Task<int> futureB = Task.Factory.StartNew<int>(() => F1(a));
  int c = F2(a); 
  int d = F3(c); 
  int f = F4(futureB.Result, d);
  return f;
This code creates a future that begins to asynchronously calculate the 
value of F1(a). On a multicore system, F1 will be able to run in parallel 
with the current thread. This means that F2 can begin executing with-
out waiting for F1. The function F4 will execute as soon as the data it 
needs becomes available. It doesn’t matter whether F1 or F3 finishes 
first, because the results of both functions are required before F4 can 
be invoked. (Recall that the Result property does not return until the 
future’s value is available.) Note that the calls to F2, F3, and F4 do not 
need to be wrapped inside of a future because a single additional 
asynchronous operation is all that is needed to take advantage of the 
parallelism of this example.
F1
F2
F3
F4
a
a
c
d
b
f
The Result property returns 
a precalculated value 
immediately or waits until 
the value becomes available.

88
chapter five
Of course, you could equivalently have put F2 and F3 inside of a 
future, as shown here.
  Task<int> futureD = Task.Factory.StartNew<int>(
                                            () => F3(F2(a)));
  int b = F1(a);
  int f = F4(b, futureD.Result);
  return f;
It doesn’t matter which branch of the task graph shown in the figure 
runs asynchronously.
An important point of this example is that exceptions that occur 
during the execution of a future are thrown by the Result property. 
This makes exception handling easy, even in cases with many futures 
and complex chains of continuation tasks. You can think of futures as 
either returning a result or throwing an exception. Conceptually, this 
is very similar to the way any .NET function works. Here is an example.
  Task<int> futureD = Task.Factory.StartNew<int>(
                                            () => F3(F2(a)));
  try 
  {
    int b = F1(a);
    int f = F4(b, futureD.Result);
    return f;
  }
  catch (MyException)
  {
     Console.WriteLine(“Saw MyException exception”);
     return -1;
  }
If an exception of type MyException were thrown in F2 or F3, 
it would be deferred and rethrown when the Result property of 
futureD is read. Getting the value of the Result property occurs 
within a try block, which means that the exception can be handled in 
the corresponding catch block.
continuation tasks
It’s very common for one asynchronous operation to invoke a second 
asynchronous operation and pass data to it. Continuation tasks make 
the dependencies among futures apparent to the run-time environ-
ment that is responsible for scheduling them. This helps to allocate 
work efficiently among cores.
Futures, as implemented by 
the Task<TResult> class, 
defer exceptions until the 
Result property is read.

 89
futures
For example, if you want to update the user interface (UI) with 
the result produced by the function F4 from the previous section, you 
can use the following code.
TextBox myTextBox = ...;
var futureB = Task.Factory.StartNew<int>(() => F1(a));
var futureD = Task.Factory.StartNew<int>(() => F3(F2(a)));
var futureF = Task.Factory.ContinueWhenAll<int, int>(
                 new[] { futureB, futureD },
                 (tasks) => F4(futureB.Result, futureD.Result));
  
futureF.ContinueWith((t) =>
  myTextBox.Dispatcher.Invoke(
       (Action)(() => { myTextBox.Text = t.Result.ToString(); }))
            );
This code structures the computation into four tasks. The system 
understands the ordering dependencies between continuation tasks 
and their antecedents. It makes sure that the continuation tasks will 
start only after their antecedent tasks complete.
The first task, futureB, calculates the value of b. The second, 
futureD, task calculates the value of d. These two tasks can run in 
parallel. The third task, futureF, calculates the value of f. It can run 
only after the first two tasks are complete. Finally, the fourth task 
takes the value calculated by F4 and updates a text box on the user 
interface.
The ContinueWith method creates a continuation task with 
a single antecedent. The ContinueWhenAll<TAntecedentResult, 
TResult> method of the Task.Factory object allows you to create a 
continuation task that depends on more than one antecedent task.
Example: The Adatum Financial Dashboard 
Here’s an example of how the Futures pattern can be used in an ap-
plication. The example shows how you can run computationally inten-
sive operations in parallel in an application that uses a graphical user 
interface (GUI).
Adatum is a financial services firm that provides a financial dash-
board application to its employees. The application, known as the 
Adatum Dashboard, allows employees to perform analyses of financial 
markets. The dashboard application runs on an employee’s desktop 
workstation. The Adatum Dashboard analyzes historical data instead 
of a stream of real-time price data. The analysis it performs is compu-
tationally intensive, but there is also some I/O latency because the 

90
chapter five
Adatum Dashboard application collects input data from several 
sources over the network.
After the application has the market data, it merges the datasets 
together. The application normalizes the merged market data and 
then performs an analysis step. After the analysis, it creates a market 
model. It also performs these same steps for historical market data 
from the Federal Reserve System. After the current and historical 
models are ready, the application compares the two models and makes 
a market recommendation of “buy,” “sell,” or “hold.” You can visualize 
these steps as a graph. Figure 2 illustrates this.
figure 2
Adatum Dashboard tasks
The tasks in this diagram communicate by specific types of business 
objects. These are implemented as .NET classes in the Adatum Dash-
board application.
Load NYSE
Merge
Normalize
Analyze
Compare
Load Fed
Historical Data
Normalize
Analyze
Create Model
Create Model
Load Nasdaq
StockDataCollection
StockDataCollection
StockDataCollection
StockDataCollection
StockDataCollection
StockDataCollection
MarketModel
StockAnalysisCollection
StockAnalysisCollection
MarketModel
MarketRecommendation

 91
futures
View
ViewModel
Analysis Engine
Business Objects
You can download the source code for the Adatum Dashboard 
application from the CodePlex site at http://parallelpatterns.codeplex.
com in the Chapter5\A-Dash project. The application consists of four 
parts: the business object definitions, an analysis engine, a view 
model, and the user interface, or the view. Figure 3 illustrates this.
figure 3
Adatum Dashboard application
the business objects
The Adatum Dashboard uses immutable data types. Objects of these 
types cannot be modified after they are created, which makes them 
well suited to parallel applications.
The StockDataCollection class represents a time series of closing 
prices for a group of securities. You can think of this as a dictionary 
indexed by a stock symbol. Conceptually, the values are arrays of 
prices for each security. You can merge StockDataCollection values 
as long as the stock symbols don’t overlap. The result of the merge 
operation is a new StockDataCollection value that contains the time 
series of the inputs.
The StockAnalysisCollection class is the result of the analysis 
step. Similarly, the MarketModel and MarketRecommendation 
classes are the outputs of the modeling and the comparison phases of 
the application. The MarketRecommendation class has a property 
that contains a “buy, hold, or sell” decision. 
For more information about how to implement your own immu-
table types, see the section, “Immutable Types,” in Appendix A, 
“Adapting Object-Oriented Patterns.”

92
chapter five
the analysis engine
The Adatum Dashboard’s AnalysisEngine class produces a market 
recommendation from the market data it receives.
The sequential process is shown in the following code.
public MarketRecommendation DoAnalysisSequential()
{
  StockDataCollection nyseData = 
    LoadNyseData();
  StockDataCollection nasdaqData = 
    LoadNasdaqData();
  StockDataCollection mergedMarketData = 
    MergeMarketData(new[]{nyseData, nasdaqData});
  StockDataCollection normalizedMarketData = 
    NormalizeData(mergedMarketData);
  StockDataCollection fedHistoricalData = 
    LoadFedHistoricalData();
  StockDataCollection normalizedHistoricalData = 
    NormalizeData(fedHistoricalData);
  StockAnalysisCollection analyzedStockData = 
    AnalyzeData(normalizedMarketData);
  MarketModel modeledMarketData = 
    RunModel(analyzedStockData);
  StockAnalysisCollection analyzedHistoricalData = 
    AnalyzeData(normalizedHistoricalData);
  MarketModel modeledHistoricalData =
    RunModel(analyzedHistoricalData);
  MarketRecommendation recommendation = 
    CompareModels(new[] { modeledMarketData,
                          modeledHistoricalData });
  return recommendation;
}
The final result of the computation is a MarketRecommendation 
object. Each of the method calls returns data that becomes the input 
to the operation that invokes it. When you use method invocations in 
this way, you are limited to sequential execution. The DoAnalysis
Sequential method returns only after all the dependent operations 
complete.

 93
futures
The parallel version uses futures and continuation tasks for each 
of the operational steps. Here’s the code.
public AnalysisTasks DoAnalysisParallel()
{
  TaskFactory factory = Task.Factory;
  // ...
  Task<StockDataCollection> loadNyseData =
    Task<StockDataCollection>.Factory.StartNew(
      () => LoadNyseData(),
      TaskCreationOptions.LongRunning);
  Task<StockDataCollection> loadNasdaqData =
    Task<StockDataCollection>.Factory.StartNew(
      () => LoadNasdaqData(),
      TaskCreationOptions.LongRunning);
  Task<StockDataCollection> mergeMarketData =
    factory.ContinueWhenAll<StockDataCollection,
                            StockDataCollection>(
      new[] { loadNyseData, loadNasdaqData },
      (tasks) => MergeMarketData(
                        from t in tasks select t.Result));
  Task<StockDataCollection> normalizeMarketData =
    mergeMarketData.ContinueWith(
      (t) => NormalizeData(t.Result));
  Task<StockDataCollection> loadFedHistoricalData =
    Task<StockDataCollection>.Factory.StartNew(
      () => LoadFedHistoricalData(),
      TaskCreationOptions.LongRunning);
  Task<StockDataCollection> normalizeHistoricalData =
    loadFedHistoricalData.ContinueWith(
      (t) => NormalizeData(t.Result));
  Task<StockAnalysisCollection> analyzeMarketData =
    normalizeMarketData.ContinueWith(
      (t) => AnalyzeData(t.Result));
  Task<MarketModel> modelMarketData =
    analyzeMarketData.ContinueWith(
      (t) => RunModel(t.Result));

94
chapter five
  Task<StockAnalysisCollection> analyzeHistoricalData =
    normalizeHistoricalData.ContinueWith(
      (t) => AnalyzeData(t.Result));
  Task<MarketModel> modelHistoricalData =
    analyzeHistoricalData.ContinueWith(
      (t) => RunModel(t.Result));
  Task<MarketRecommendation> compareModels =
    factory.ContinueWhenAll<MarketModel, MarketRecommendation>(
      new[] { modelMarketData, modelHistoricalData },
      (tasks) => CompareModels(from t in tasks select t.Result));
  Task errorHandler = CreateErrorHandler(loadNyseData, 
    loadNasdaqData, loadFedHistoricalData, 
    mergeMarketData, normalizeHistoricalData, 
    normalizeMarketData, analyzeHistoricalData, 
    analyzeMarketData, modelHistoricalData, 
    modelMarketData, compareModels);
  return new AnalysisTasks()
    {
      LoadNyseData = loadNyseData,
      LoadNasdaqData = loadNasdaqData,
      MergeMarketData = mergeMarketData,
      NormalizeMarketData = normalizeMarketData,
      LoadFedHistoricalData = loadFedHistoricalData,
      NormalizeHistoricalData = normalizeHistoricalData,
      AnalyzeMarketData = analyzeMarketData,
      AnalyzeHistoricalData = analyzeHistoricalData,
      ModelMarketData = modelMarketData,
      ModelHistoricalData = modelHistoricalData,
      CompareModels = compareModels,
      ErrorHandler = errorHandler
    };
} 
The parallel version, provided by the DoAnalysisParallel method, 
is similar to the sequential version, except that the synchronous 
method calls have been replaced with futures and continuation tasks. 
The method returns an AnalysisTasks object that contains the tasks 
associated with each step of the calculation. The DoAnalysisParallel 
method returns immediately, leaving the tasks running. The next  
sections describe how each of the tasks is created.

 95
futures
Loading External Data
The methods that gather the external data from the network are long-
running, I/O intensive operations. Unlike the other steps, they are not 
particularly processor intensive, but they may take a relatively long 
time to complete. Most of their time is spent waiting for I/O opera-
tions to finish. You create these tasks with a factory object, and you 
use an argument to specify that the tasks are of long duration. This 
temporarily increases the degree of concurrency that is allowed by the 
system. The following code shows how to load the external data.
  Task<StockDataCollection> loadNyseData =
    Task<StockDataCollection>.Factory.StartNew(
      () => LoadNyseData(),
      TaskCreationOptions.LongRunning);
  Task<StockDataCollection> loadNasdaqData =
    Task<StockDataCollection>.Factory.StartNew(
      () => LoadNasdaqData(),
      TaskCreationOptions.LongRunning); 
Note that the factory creates futures that return values of type Stock
DataCollection. The TaskCreationOptions.LongRunning enumer-
ated value tells the task library that you want more concurrency. To 
prevent underutilization of processor resources, the task library may 
choose to run tasks like these in additional threads.
Merging
The merge operation takes inputs from both the loadNyseData and 
the loadNasdaqData tasks. It’s a continuation task that depends on 
two antecedent tasks, as shown in the following code.
  Task<StockDataCollection> mergeMarketData =
    factory.ContinueWhenAll<StockDataCollection, 
                            StockDataCollection>(
      new[] { loadNyseData, loadNasdaqData },
      (tasks) => MergeMarketData(
          from t in tasks select t.Result));
After the loadNyseData and loadNasdaqData tasks complete, the 
MergeMarketData method given as an argument is invoked. At that 
point, the tasks parameter will be an array of antecedent tasks, which 
are the loadNyseData and loadNasdaqData tasks.
The MergeMarketData method takes an array of StockData
Collection objects as its input. The LINQ expression from t in tasks 
select t.Result maps the input array of futures into a collection of 
StockDataCollection objects by getting the Result property of each 
future.
Use “long running” tasks  
in cases where you want 
additional concurrency,  
such as when there are 
long-running I/O  
operations.

96
chapter five
Normalizing
After the market data is merged, it undergoes a normalization step.
Task<StockDataCollection> normalizeMarketData =
     mergeMarketData.ContinueWith(
       (t) => NormalizeData(t.Result));
The ContinueWith method creates a continuation task with a single 
antecedent. The continuation task gets the result value from the  
task referenced by the mergeMarketData variable and invokes the 
NormalizeData method.
Analysis and Model Creation
After the market data is normalized, the application performs an 
analysis step. This takes an object of type StockAnalysisCollection 
as input and returns an object of type MarketAnalysis, as shown in 
the following code.
Task<StockAnalysisCollection> analyzeMarketData =
    normalizeMarketData.ContinueWith(
      (t) => AnalyzeData(t.Result));
Task<MarketModel> modelMarketData =
    analyzeMarketData.ContinueWith(
      (t) => RunModel(t.Result));
Processing Historical Data
The application also creates a model of historical data. The steps that 
create the tasks are similar to those for the current market data.  
However, because these steps are performed by tasks, they may be 
run in parallel if the hardware resources allow it.
Comparing Models
Here is the code that compares the two models.
Task<MarketRecommendation> compareModels =
    factory.ContinueWhenAll<MarketModel, MarketRecommendation>(
        new[] { modelMarketData, modelHistoricalData },
        (tasks) => CompareModels(
            from t in tasks select t.Result));
The “compare models” step compares the current and historical  
market models and produces the final result. 

 97
futures
view and view model
The Adatum Dashboard is a GUI-based application that also uses the 
Model-View-ViewModel (MVVM) pattern. It breaks the parallel com-
putation into subtasks whose status can be independently seen from 
the user interface. For more information about how the dashboard 
application interacts with the view model and view, see the section, 
“Model-View-ViewModel,” in Appendix A, “Adapting Object-Orient-
ed Patterns.”
Variations
So far, you’ve seen some of the most common ways to use futures and 
continuation tasks to create tasks. This section describes some other 
ways to use them.
canceling futures and continuation tasks
There are several ways to cancel futures and continuation tasks. You 
can handle cancellation entirely from within the task, as the Adatum 
Dashboard does, or you can pass cancellation tokens when the tasks 
are created.
The Adatum Dashboard application supports cancellation from 
the user interface. It does this by calling the Cancel method of 
the CancellationTokenSource class. This sets the IsCancellation
Requested property of the cancellation token to true.
The application checks for this condition at various checkpoints. 
If a cancellation has been requested, the operation is canceled. For  
a code example, see the section, “Canceling a Task,” in Chapter 3,  
“Parallel Tasks.”
continue when “at least one” antecedent 
completes
It’s possible to invoke a continuation task when the first of multiple 
antecedents completes. To do this, use the Task.Factory object’s 
ContinueWhenAny method. The ContinueWhenAny method is use-
ful when the result of any of the tasks will do. For example, you may 
have an application where each task queries a Web service that gives 
the local weather. The application returns the first answer it receives 
to the user.
using .net asynchronous calls with futures
Tasks are similar in some ways to asynchronous methods that use the 
.NET Asynchronous Programming Model (APM) pattern and the 
IAsyncResult interface. In fact, tasks in .NET Framework 4 are IAsync
Result objects. They implement this interface. This allows you to use 
the Task class when you implement the APM pattern.

98
chapter five
You can convert a pair of begin/end methods that use IAsync
Result into a task. To do this, use the Task.Factory object’s From
Async method.
In general, tasks can be easier to use than other implementations 
of IAsyncResult because futures rethrow exceptions when the result 
is requested.
removing bottlenecks
The idea of a critical path is familiar from project management. A 
“path” is any sequence of tasks from the beginning of the work to the 
end result. A task graph may contain more than one path. For example, 
look at the task graph that is shown in Figure 2. You can see that there 
are three paths, beginning with “Load NYSE,” “Load Nasdaq,” and 
“Load Fed Historical Data” tasks. Each path ends with the “Compare” 
task.
The duration of a path is the sum of the execution time for each 
task in the path. The critical path is the path with the longest dura-
tion. The amount of time needed to calculate the end result depends 
only on the critical path. As long as there are enough resources (that 
is, available cores), the noncritical paths don’t affect the overall execu-
tion time.
If you want to make your task graph run faster, you need to find 
a way to reduce the duration of the critical path. To do this, you can 
organize the work more efficiently. You can break down the slowest 
tasks into additional tasks, which can then execute in parallel. You can 
also modify a particularly time-consuming task so that it executes in 
parallel internally using any of the patterns that are described in this 
book.
The Adatum Dashboard example doesn’t offer much opportunity 
for breaking down the slowest tasks into additional tasks that execute 
in parallel. This is because the paths are linear. However, you can use 
the Parallel Loops and Parallel Aggregation patterns to exploit more 
of the potential parallelism within each of the Analyze tasks (see Fig-
ure 2) if they take the most time. The task graph remains unchanged, 
but the tasks within it are now also parallelized.
modifying the graph at run time
The code in the financial program’s analysis engine creates a static 
task graph. In other words, the graph of task dependencies is reflected 
directly in the code. By reading the implementation of the analysis 
engine, you can determine that there are a fixed number of tasks with 
a fixed set of dependencies among them.
The extension of the analysis tasks in the UI layer is an example 
of dynamic task creation. The UI augments the graph of tasks by add-
Tasks in .NET implement the 
IAsyncResult interface.

 99
futures
ing continuation tasks programmatically, outside of the context where 
these tasks were originally created.
Dynamically created tasks are also a way to structure algorithms 
used for sorting, searching, and graph traversal. For examples, see 
chapter 6, “Dynamic Task Parallelism.”
Design Notes
There are several ideas behind the design of the Adatum Dashboard 
application.
decomposition into futures  
and continuation tasks
The first design decision is the most obvious one: the Adatum Dash-
board introduces parallelism by means of futures and continuation 
tasks. This makes sense because the problem space could be decom-
posed into operations with well-defined inputs and outputs.
functional style
There are implicit and explicit approaches to synchronizing data be-
tween tasks. In this chapter, the examples used an explicit approach. 
Data is passed between tasks as parameters, which makes the data 
dependencies very obvious to the programmer. Alternatively, as you 
saw in Chapter 3, “Parallel Tasks,” it’s also possible for tasks to com-
municate with side effects that modify shared data structures. In this 
case, you rely on the tasks to use control dependencies that block 
appropriately. However, in general, explicit data flow is less prone to 
error that implied data flow.
You can see this by analogy. In principle, there’s no need for a 
programming language to support methods with return values. Pro-
grammers can always use methods without return values and perform 
updates on shared global variables as a way of communicating the 
results of a computation to other components of an application. 
However, in practice, using return values is considered to be a much 
less error-prone way to write programs. Programmers tend to make 
more mistakes with global variables than with return values.
Similarly, futures (tasks that return values) can reduce the possibil-
ity of error in a parallel program as compared to tasks that communi-
cate results by modifying shared global state. In addition, tasks that 
return values can often require less synchronization than tasks that 
globally access state variables, and they are much easier to understand. 
Futures also promote a natural kind of data isolation, similar to 
what is found in functional programming, which relies on operations 
that communicate with input and output values. Functional programs 

100
chapter five
are very easy to adapt to multicore environments. In general, futures 
should only communicate with the outside world by means of their 
return values. It’s also a good practice to use immutable types for  
return values. 
Related Patterns
There are a number of patterns that have some similarities to the  
Futures pattern, but they also have some important differences. This 
section provides a brief comparison.
pipelines pattern
The Pipelines pattern is described in Chapter 7, “Pipelines.” It differs 
in several important respects from a task graph. The pipeline focuses 
on data flow by means of queues (message buffers), instead of task  
dependencies. In a pipeline, the same task is executed on multiple data 
items.
master/worker pattern
Tasks within the Master/Worker pattern have a child/parent relation-
ship instead of the antecedent/dependent relationship that continua-
tion tasks have. The master task creates the worker tasks, passes  
data to them, and waits for a result to be returned. Typically, worker 
tasks all execute the same computation against different data. The 
implementation of parallel loops in .NET Framework 4 uses the  
Master/Worker pattern internally.
dynamic task parallelism pattern
This is also known as the Divide and Conquer pattern and is the  
subject of Chapter 6, “Dynamic Task Parallelism.” Dynamic task paral-
lelism creates trees of tasks on the fly in a manner similar to recursion. 
If futures are asynchronous functions, dynamic task parallelism  
produces asynchronous recursive functions.
discrete event pattern
The Discrete Event pattern focuses on sending messages between 
tasks. There is no limitation on the number of events a task raises or 
when it raises them. Events can also pass between tasks in either  
direction; there is no antecedent/dependency relationship. The Dis-
crete Event pattern can be used to implement a task graph by placing 
additional restrictions on it.
Communicating among tasks 
by means of arguments and 
return values scales well as 
the number of cores increases.

 101
futures
Exercises
1.	 Suppose you parallelize the following sequential code using 
futures in the style of the first example in the section, “The 
Basics.”
var b = F1(a);  
var d = F2(c);  
var e = F3(b, d);  
var f = F4(e);  
var g = F5(e); 
var h = F6(f, g);
Draw the task graph. In order to achieve the greatest degree of 
concurrency, what is the minimum number of futures you must 
define? What is the largest number of these futures that can 
be running at the same time?
2.	 Modify the BasicFutures sample from CodePlex so that one 
of the futures throws an exception. What should happen? 
Observe the behavior when you execute the modified 
sample.
Further Reading
Leijen describes the motivation for including futures in the Task  
Parallel Library (TPL) and has references to other work, especially in 
functional languages. The NModel framework provides a C# library of 
immutable collection types including set, bag, sequence, and map.
D. Leijen, W. Schulte, and S. Burckhardt. “The Design of a  
Task Parallel Library.”  S. Arora and G.T. Leavens, editors, OOP-
SLA 2009: Proceedings of the 24th Annual ACM SIGPLAN 
Conference on Object-Oriented Programming, Systems,  
Languages, and Applications, pages 227–242. ACM, 2009.
NModel software. 2008. http://nmodel.codeplex.com/ 


103
Dynamic Task Parallelism
6
Dynamic task parallelism is 
similar to recursion. Tasks 
create sub tasks on the fly to 
solve subproblems as needed.
This chapter covers situations where tasks are dynamically added to 
the work queue as the computation proceeds. This is known as dy-
namic task parallelism. A simple example of dynamic task parallelism 
occurs in cases where the sequential version of an algorithm includes 
recursion.
Dynamic task parallelism is also known as recursive decomposi-
tion or “divide and conquer.”
Dynamic task parallelism applies to problems that are solved by 
first solving smaller, related problems. For example, when you count 
the number of nodes in a data structure that represents a binary tree, 
you can count the nodes in the left and right subtrees and then add 
the results.
Applications that use data structures such as trees and graphs are 
typical examples of dynamic task parallelism. It’s also used for applica-
tions that have geographic or geometric aspects, where the problem 
can be partitioned spatially.
The Basics
The following code shows a binary tree.
public class Tree<T>
{
  public T       Data  { get; set; }
  public Tree<T> Left  { get; set; }
  public Tree<T> Right { get; set; }
}
If you want to perform an action on each data value in the tree, you 
need to visit each node. This is known as walking the tree, which is a 
naturally recursive operation. Here’s an example that uses sequential 
code.

104
chapter six
static void SequentialWalk<T>(Tree<T> tree, Action<T> action)
{
  if (tree == null) return;
  action(tree.Data);
  SequentialWalk(tree.Left, action);
  SequentialWalk(tree.Right, action);
}
You can also use parallel tasks to walk the tree.
static void ParallelWalk<T>(Tree<T> tree, Action<T> action)
{
  if (tree == null) return;
  var t1 = Task.Factory.StartNew(
             () => action(tree.Data));
  var t2 = Task.Factory.StartNew(
             () => ParallelWalk(tree.Left, action));
  var t3 = Task.Factory.StartNew(
             () => ParallelWalk(tree.Right, action)); 
  Task.WaitAll(t1, t2, t3);
}
You don’t need to create a new task to walk the right side of the tree. 
If you want, you can use the current task; however, using tasks for 
both the left and right walks makes sure that exceptions that are 
thrown by either will be observed. For more information about how 
to observe unhandled task exceptions, see the section, “Handling 
Exceptions,” in Chapter 3, “Parallel Tasks.”
When you use dynamic task parallelism to perform a tree walk, 
you no longer visit nodes in a fully predictable order. If you need to 
visit nodes in a sequential order, such as with a preorder, inorder, or 
postorder traversal, you may want to consider the Pipeline pattern 
that’s described in Chapter 7, “Pipelines.”
In this example, the number of tasks is three times the number of 
nodes in the tree, which could be a large number. The Task Parallel 
Library (TPL) is designed to handle this type of situation, but you may 
want to read the section, “Design Notes,” later in this chapter for 
some performance tips.
Dynamic task parallelism 
results in a less predictable 
order of execution than 
sequential code.

 105
dynamic task parallelism
An Example
An example of dynamic task parallelism is when you sort a list with an 
algorithm such as QuickSort. This algorithm first divides an unsorted 
array of values, and then it orders and recombines the pieces. Here’s a 
sequential implementation.
static void SequentialQuickSort(int[] array, int from, int to)
{
  if (to - from <= Threshold)
  {
    InsertionSort(array, from, to);
  }
  else
  {
    int pivot = from + (to - from) / 2;
    pivot = Partition(array, from, to, pivot);
    SequentialQuickSort(array, from, pivot);
    SequentialQuickSort(array, pivot + 1, to);
  }
}
This method sorts array in place, instead of returning a sorted array. 
The from and to arguments identify the segment that will be sorted. 
The code includes an optimization. It’s not efficient to use the recur-
sive algorithm on short segments, so the method calls the non- 
recursive InsertionSort method on segments that are not longer than 
Threshold, which is set in a global variable. This optimization applies 
equally to the sequential and parallel versions.
If the segment is longer than Threshold, the recursive algorithm 
is used. The element in the middle of the segment (at pivot) is chosen. 
The Partition method moves all the array elements not greater than 
the element at pivot to the segment that precedes pivot, and leaves 
the greater elements in the segment that follows pivot (pivot itself 
may be moved). Then the method recursively calls QuickSort on both 
segments.
The following code shows a parallel implementation.
static void ParallelQuickSort(int[] array, int from, 
                              int to, int depthRemaining)
{
  if (to - from <= Threshold)
  {
    InsertionSort(array, from, to);
Sorting is a typical applica-
tion that can benefit from 
dynamic task parallelism.

106
chapter six
  }
  else
  {
    int pivot = from + (to - from) / 2; 
    pivot = Partition(array, from, to, pivot);
    if (depthRemaining > 0)
    {
      Parallel.Invoke(
        () => ParallelQuickSort(array, from, pivot, 
                       depthRemaining - 1),
        () => ParallelQuickSort(array, pivot + 1, to, 
                       depthRemaining - 1));
    }
    else
    {
      ParallelQuickSort(array, from, pivot, 0);
      ParallelQuickSort(array, pivot + 1, to, 0);
    }
  }
}
This version uses Parallel.Invoke to execute the recursive calls in 
tasks that can run in parallel. Tasks are created dynamically with each 
recursive call; if the array is large, many tasks might be created.
The parallel version includes an additional optimization. It’s gener-
ally not useful to create many more tasks than there are processors to 
run them. So, the ParallelQuickSort method includes an additional 
argument to limit task creation. The depthRemaining argument is 
decremented on each recursive call, and tasks are created only when 
this argument exceeds zero. The following code shows how to calcu-
late an appropriate depth (the depthRemaining argument) from the 
number of processors.
public static void ParallelQuickSort(int[] array)
{
  ParallelQuickSort(array, 0, array.Length, 
      (int) Math.Log(Environment.ProcessorCount, 2) + 4);
}
One relevant factor in selecting the number of tasks is how similar the 
predicted run times of the tasks will be. In the case of QuickSort, the 
duration of the tasks may vary a great deal because the pivot points 
depend on the unsorted data. They don’t necessarily result in seg-
ments of equal size (in fact, the sizes could vary widely). To compen-
sate for the uneven sizes of the tasks, the formula in this example that 
calculates the depthRemaining argument produces more tasks than 
cores. The formula limits the number of tasks to approximately  

 107
dynamic task parallelism
sixteen times the number of cores. This is because the number of 
tasks can be no larger than 2 ^ depthRemaining. If you substitute 
depthRemaining = log2(NCores) + 4 and simplify the expression, you 
see that the number of tasks is 16 x NCores. (Recall that for any value 
a, 2 ^ (a + 4) is the same as 16 times 2^a and that if a = log2(b), 
2^a = b.)
Variations
Dynamic task parallelism has several variations.
parallel while-not-empty 
The examples shown so far in this chapter use techniques that are the 
parallel analogs of sequential depth-first traversal. There are also par-
allel algorithms for other types of traversals. These techniques rely on 
concurrent collections to keep track of the remaining work to be 
done. Here’s an example.
public static void ParallelWhileNotEmpty<T>(
  IEnumerable<T> initialValues, 
  Action<T, Action<T>> body)
{
  var from = new ConcurrentQueue<T>(initialValues);  
  while (!from.IsEmpty)
  { 
    var to = new ConcurrentQueue<T>();
    Action<T> addMethod = to.Enqueue; 
    Parallel.ForEach(from, v => body(v, addMethod));        
    from = to;
  }
}
This method shows how you can use Parallel.ForEach to process an 
initial collection of values. While processing the values, additional 
values to process may be discovered. The additional values are placed 
in the to queue. After the first batch of values processes, the method 
starts processing the additional values, which may again result in more 
values to process. This process repeats until no additional values can 
be produced.
A method that walks a binary tree can use the ParallelWhileNot
Empty method.
static void ParallelWalk4<T>(Tree<T> tree, Action<T> action)
{
  if (tree == null) return;
  ParallelWhileNotEmpty(new[] { tree }, (item, adder) =>
Limiting the number of 
subtasks by measuring the 
recursion depth is an extremely 
important technique.

108
chapter six
  {
    if (item.Left != null) adder(item.Left);
    if (item.Right != null) adder(item.Right);
    action(item.Data);
  });
}
An example of a problem that would be an appropriate use of the 
ParallelWhileNotEmpty method is a website link checking tool. The 
walk task loads the initial page and searches it for links. Each link is 
checked and removed from the list, and additional links to unchecked 
pages from the same site are added to the list. Eventually, there are no 
more unchecked links and the application stops.
task chaining with parent/child tasks
The TPL includes a task creation option named AttachedToParent. 
This option is used most frequently in code that uses the dynamic task 
parallelism pattern. The purpose of the AttachedToParent option is 
to link a subtask to the task that created it. In this case, the subtask is 
known as a child task and the task that created the child task is known 
as a parent task.
You use the AttachedToParent option in two situations. The first 
is when you want to link the status of a parent task to the status of 
its child tasks. The second is when you want to use the Microsoft® 
Visual Studio® development system debugger to see the parent/
child relationships. The order of execution is not changed by using the 
AttachedToParent option.
The following code shows an example.
static void ParallelWalk2<T>(Tree<T> tree, Action<T> action)
{
  if (tree == null) return;
  var t1 = Task.Factory.StartNew(
              () => action(tree.Data),                                        
              TaskCreationOptions.AttachedToParent);
  var t2 = Task.Factory.StartNew(
              () => ParallelWalk2(tree.Left, action),                             
              TaskCreationOptions.AttachedToParent);
  var t3 = Task.Factory.StartNew(
              () => ParallelWalk2(tree.Right, action),
              TaskCreationOptions.AttachedToParent);
  Task.WaitAll(t1, t2, t3);
}

 109
dynamic task parallelism
The AttachedToParent option affects the behavior of the parent 
task. If a parent task with at least one running child task finishes  
running for any reason, its Status property becomes WaitingFor
ChildrenToComplete. Only when all of its attached children are no 
longer running will the parent task’s Status property transition to one 
of the three final states. Figure 1 illustrates this.
figure 1
Life cycle of a task that has attached children
Exceptions from attached children are observed in the parent task.
To access the parent/child view in the Visual Studio debugger, 
right-click the row headings in the Parallel Tasks window, and then 
click Parent Child View.
Design Notes
The behavior of the Microsoft .NET Framework default task sched-
uler was described in Chapter 3, “Parallel Tasks.” The default task 
scheduler uses fast local task queues for each of its worker threads as 
well as a work stealing algorithm for load balancing. These features are 
of particular importance to the performance of applications that use 
dynamic task parallelism.
The default task scheduler’s thread injection heuristics aren’t ap-
propriate for every case of dynamic task parallelism. In particular, if 
you have long-running, processor-intensive tasks, profiling your ap-
plication may show processor oversubscription (too many worker 
threads). If this happens, consider decomposing your problem into 
more tasks with shorter durations. For example, in the parallel quick 
sort example, you can adjust the initial value of the depthRemaining 
parameter to create more or fewer tasks.
You can also write a custom thread scheduler. For more informa-
tion about task scheduling, see Chapter 3, “Parallel Tasks.”
When you use the default 
task scheduler, watch  
out for long-running, 
processor-intensive tasks.
Running
WaitingForChildrenToComplete
Canceled
RanToCompletion
Faulted

110
chapter six
Exercises
1.	 The sample code on CodePlex assigns a particular default 
value for the Threshold segment length. At this point, the 
QuickSort methods switch to the non-recursive Insertion-
Sort algorithm. Use the command line argument to assign 
different values for the Threshold value, and then observe 
the execution times for the sequential version to sort differ-
ent array sizes. What do you expect to see? What’s the best 
value for Threshold on your system?
2.	 Use the command line argument to vary the array size, and 
then observe the execution time as a function of array size 
for the sequential and parallel versions. What do you expect? 
Can you explain your observations?
3.	 Suggest other measures, besides the number of cores,  
to limit the number of tasks.
Further Reading
Toub discusses additional variations on parallel QuickSort.
S. Toub. “Patterns of Parallel Programming: Understanding and 
Applying Parallel Patterns with the .NET Framework 4.” 2009.  
http://www.microsoft.com/downloads/details.
aspx?FamilyID=86b3d32b-ad26-4bb8-a3ae-
c1637026c3ee&displaylang=en



113
Pipelines
7
A pipeline is a series of 
parallel tasks that are 
connected by buffers.
Don’t confuse pipelines and 
parallel loops. Pipelines are 
used in cases where a parallel 
loop can’t be used. With the 
Pipeline pattern, the data is 
processed in sequential order. 
The first input is transformed 
into the first output, the 
second input into the second 
output, and so on.
The Pipeline pattern uses parallel tasks and concurrent queues to 
process a sequence of input values. Each task implements a stage of 
the pipeline, and the queues act as buffers that allow the stages of the 
pipeline to execute concurrently, even though the values are pro-
cessed in order. You can think of software pipelines as analogous to 
assembly lines in a factory, where each item in the assembly line is 
constructed in stages. The partially assembled item is passed from one 
assembly stage to another. The outputs of the assembly line occur in 
the same order as that of the inputs.
A pipeline is composed of a series of producer/consumer stages, 
each one depending on the output of its predecessor. Pipelines allow 
you to use parallelism in cases where there are too many dependen-
cies to use a parallel loop.
There are many ways to use pipelines. Pipelines are often useful 
when the data elements are received from a real-time event stream, 
such as values on stock ticker tapes, user-generated mouse click 
events, and packets that arrive over the network. Pipelines are also 
used to process elements from a data stream, as is done with compres-
sion and encryption, or processing streams of video frames. In all of 
these cases, it’s important that the data elements are processed in 
sequential order.
Pipelines are also useful for processor-intensive applications and 
for applications where the cost of I/O is significant.
The Basics
In the Microsoft® .NET Framework, the buffers that connect stages 
of a software pipeline are usually based on the BlockingCollection<T> 
class.

114
chapter seven
Figure 1 illustrates an example of a pipeline that has four stages. 
It reads words and sentence fragments from a data source, it corrects 
the punctuation and capitalization, it groups them into complete 
sentences, and it writes the sentences to a disk file.
figure 1
Sample pipeline
Each stage of the pipeline reads from a dedicated input and writes to 
a particular output. For example, the “Read Strings” task reads from a 
source and writes to buffer 1. All the stages of the pipeline can exe-
cute at the same time because concurrent queues buffer any shared 
inputs and outputs. If there are four available cores, the stages can run 
in parallel. As long as there is room in its output buffer, a stage of the 
pipeline can add the value it produces to its output queue. If the out-
put buffer is full, the producer of the new value waits until space  
becomes available. Stages can also wait (that is, block) on inputs.  
An input wait is familiar from other programming contexts—if an 
enumeration or a stream does not have a value, the consumer of that 
enumeration or stream waits until a value is available or an “end of 
file” condition occurs. Blocking a collection works the same way.  
Using buffers that hold more than one value at a time compensates 
for variability in the time it takes to process each value.
The BlockingCollection<T> class lets you signal the “end of file” 
condition with the CompleteAdding method. This method tells the 
consumer that it can end its processing loop after all the data previ-
ously added to the collection is removed or processed.
The following code demonstrates how to implement a pipeline 
that uses the BlockingCollection class for the buffers and tasks for 
the stages of the pipeline.
If the queue is full, the 
producer blocks. If the  
queue is empty, the  
consumer blocks.
Read Strings
Correct Case
Create Sentences
Write Sentences
Buffer 1
Buffer 2
Buffer 3
input
output

 115
pipelines
int seed = ...
int BufferSize = ...
var buffer1 = new BlockingCollection<string>(BufferSize);
var buffer2 = new BlockingCollection<string>(BufferSize);
var buffer3 = new BlockingCollection<string>(BufferSize);
var f = new TaskFactory(TaskCreationOptions.LongRunning, 
                                                     TaskContinuationOptions.None);
var stage1 = f.StartNew(() => ReadStrings(buffer1, ...));
var stage2 = f.StartNew(() => CorrectCase(buffer1, buffer2));
var stage3 = f.StartNew(() => CreateSentences(buffer2, buffer3));
var stage4 = f.StartNew(() => WriteSentences(buffer3));
Task.WaitAll(stage1, stage2, stage3, stage4);
The first stage produces the input strings and places them in the first 
buffer. The second stage transforms the strings. The third stage com-
bines the strings and produces sentences. The final stage writes the 
corrected sentences to a file.
The buffers are instances of the BlockingCollection<string> 
class. The argument to the constructor specifies the maximum num-
ber of values that can be buffered at any one time. In this case, the 
value is 32 for each buffer. For more information about how to choose 
the buffer size of a blocking collection, see the section, “Design 
Notes,” later in this chapter.
As this example shows, tasks in a pipeline are usually created with 
the LongRunning option. For more information, see the section, 
“Anti-Patterns,” later in this chapter.
The first stage of the pipeline includes a sequential loop that 
writes to an output buffer.
static void ReadStrings(BlockingCollection<string> output,
                        int seed)
{
  try
  {
    foreach (var phrase in PhraseSource(seed))
    {
      Stage1AdditionalWork();
      output.Add(phrase);
    }
  }
  finally
  {
Don’t worry if you forget the 
LongRunning option. If you 
use the default task scheduler, 
the .NET thread pool will 
automatically add additional 
worker threads as needed at 
the rate of about 2 per second.

116
chapter seven
    output.CompleteAdding();
  }
}
The sequential loop populates the output buffer with values. The 
values come from an external data source that’s accessed by the 
PhraseSource method, which returns an ordinary single-threaded in-
stance of IEnumerable<string>. The producer places a value in the 
blocking collection with the blocking collection’s Add method. This 
method can potentially block if the queue is full. This is a way to 
limit stages of the pipeline that are executing faster than other stages.
The call to the CompleteAdding method is usually inside of a fi-
nally block so that it will execute even if an exception occurs.
Stages in the middle of the pipeline consume values from an input 
buffer and also produce values and place them into an output buffer. 
The following code shows how the stages are structured.
void DoStage(BlockingCollection<T> input,
             BlockingCollection<T> output)
  try
  {
     foreach (var item in input.GetConsumingEnumerable())
     {
        var result = ...
        output.Add(result);
     }
  }
  finally
  {
     output.CompleteAdding();
  }
}
You can look at the online source code to see the implementa-
tions of the CorrectCase and CreateSentences methods that make 
up stages 2 and 3 of this pipeline. They are structured in a very similar 
way to this example. The important point of this code is that the input 
blocking collection’s GetConsumingEnumerable method returns an 
enumeration that a consumer can use to “take” the values. There may 
be many consumers of a single producer. When a consumer “takes” a 
value, no other consumer will see it.
Although this example doesn’t show it, a blocking collection’s 
GetConsumingEnumerable method can be called by more than one 
consumer. This allows values from the producer to be divided among 
multiple recipients. If a recipient gets a value from the blocking col-
lection, no other consumer will also get that value.
Use a sequential loop to 
process steps in a pipeline 
stage. Call the Comple-
teAdding method once a 
pipeline stage will produce  
no more values.
A blocking collection can 
have more than one consumer.

 117
pipelines
The last stage of the pipeline consumes values from a blocking 
collection but doesn’t produce values. Instead, it writes values to a 
stream. Here’s the code.
static void WriteSentences(BlockingCollection<string> input)
{
  using (StreamWriter outfile = 
             new StreamWriter(PathForPipelineResults))
  {
    // ...
    foreach (var sentence in input.GetConsumingEnumerable())
    {
       var printSentence = ...
       outfile.WriteLine(printSentence);
    }
  }
}
One of the things that makes a pipeline easy to write in .NET is that 
you can rely on familiar sequential techniques such as iteration by 
using the IEnumerable<T> class. There is some synchronization, but 
it’s hidden inside the implementation of the BlockingCollection<T> 
class.
(Some details of error handling, cancellation and the collection of 
performance data have been omitted from this example for clarity. To 
see error handling and cancellation code, review the full ImagePipeline 
sample that’s mentioned later in this chapter.)
An Example
The online samples include an application named ImagePipeline. This 
application takes a directory of JPEG images and generates thumbnail 
versions, which are also post-processed with several image-enhancing 
filters. The resulting processed images are displayed as a slideshow. 
Each image in the slideshow is displayed in alphabetical file name order.
sequential image processing
Each image is processed in four stages: the large color image is loaded 
from a file, a small thumbnail with a picture frame is generated from 
it, noise is added to the image to create a speckling effect, and then 
the processed image displays as the next picture in the slideshow. 
Figure 2 illustrates this sequence.
You can’t use a parallel loop for 
this example because the 
application has a requirement 
that images must be processed 
in sequence. Parallel loops do 
not guarantee any particular 
processing order.

118
chapter seven
Load Image
Scale Image
Filter Image
Display Image
figure 2
Sequential image processing
Here’s the code for the sequential version.
string sourceDir = ...
IEnumerable<string> fileNames = ...
int count = 0;
ImageInfo info = null;
foreach (var fileName in fileNames)
{
  /// ...
  info = LoadImage(fileName, sourceDir, count, ...);
  ScaleImage(info);
  FilterImage(info);           
  // ...         
  DisplayImage(info, count + 1, displayFn, ...);
  // ...
                    
  count += 1;
  info = null;
}
The four steps are performed by the LoadImage, ScaleImage, Fil-
terImage, and DisplayImage methods. This example is slightly 
abridged for clarity. The code that deals with cancellation, error hand-
ing (the disposal of handles to unmanaged objects), and the capture 
of performance measurements is omitted. You can refer to the online 
samples to see these details. For more information about cancellation 
and error handling, see the section, “Variations,” later in this chapter.

 119
pipelines
input
Load Image
Scale Image
Filter Image
Display Image
Thumbnail
images
Original 
images
Filtered
images
the image pipeline
The sequential loop can process only one image at a time; each image 
must complete all four stages before work can begin on the next im-
age, and the stages themselves are sequentially linked. In fact, this 
example seems intractably sequential—the top-level loop has the 
restriction that images must be displayed in order, and within each 
step are substeps that require inputs from previous substeps. You 
can’t display an image until after the filter is applied to it. You can’t 
apply the filter until after the image is scaled to thumbnail size. You 
can’t do the scaling until after the original image loads.
However, the pipeline pattern can introduce parallelism into this 
example. Each image still passes through all four stages in sequence, 
but the stages can execute in parallel on different images. Figure 3  
illustrates the image pipeline.
figure 3
Image pipeline
The following code shows the parallel version.
IEnumerable<string> fileNames = ...
string sourceDir = ...
Action<ImageInfo> displayFn = ...
int limit = ...
var originalImages = new BlockingCollection<ImageInfo>(limit);
var thumbnailImages = new BlockingCollection<ImageInfo>(limit);
var filteredImages =  new BlockingCollection<ImageInfo>(limit);
try
{

120
chapter seven
  var f = new TaskFactory(TaskCreationOptions.LongRunning,       
                          TaskContinuationOptions.None);
  // ...
  var loadTask = f.StartNew(() =>
                    LoadPipelinedImages(fileNames, sourceDir, 
                                        originalImages, ...));
  var scaleTask = f.StartNew(() =>
                    ScalePipelinedImages(originalImages, 
                                         thumbnailImages, ...));
  var filterTask = f.StartNew(() =>
                    FilterPipelinedImages(thumbnailImages, 
                                          filteredImages, ...));
  var displayTask = f.StartNew(() =>
                    DisplayPipelinedImages(
                        filteredImages.GetConsumingEnumerable(), 
                        displayFn, ...));
  Task.WaitAll(loadTask, scaleTask, filterTask, displayTask);
}
finally
{
  // ... release handles to unmanaged resources ...
}
(Some details of error handling, cancellation, and the collection of 
performance data have been omitted here for clarity. Refer to the 
online sample for the complete implementation.)
There are three blocking collections that act as buffers between 
the stages of the pipeline. The four stages are the same as in the  
sequential version. A call to the task factory StartNew method 
launches each processing stage as a long-running task.
The code calls Task.WaitAll to defer cleanup until after all stages 
complete processing all images.
performance characteristics
To understand the performance characteristics of the sequential and 
pipelined versions, it’s useful to look at a scheduling diagram such as 
the one that Figure 4 illustrates.

 121
pipelines
figure 4
Image pipeline with stages of equal speed
Figure 4 shows how the tasks in the image pipeline example execute 
over time. For example, the top row shows that stage 1 processes 
image 1 starting at time t0 and image 2 starting at time t1. Stage 2 
begins processing image 1 at time t1. Assume for a moment that the 
pipeline is perfectly balanced; that is, each stage of the pipeline takes 
exactly the same amount of time to do its work. Call that duration T. 
Therefore, in the figure, t1 occurs after T units of time elapse, t2 after 
2 x T units of time elapse, and so on.
If there are enough available cores to allow the pipeline’s tasks to 
run in parallel, the figure shows that the expected execution time for 
six images in a pipeline with four stages is approximately 9 x T. In 
contrast, the sequential version takes approximately 24 x T because 
each of the 24 steps must be processed one after another.
The average performance improves as more images are processed. 
The reason for this, as Figure 4 illustrates, is that some cores are idle 
as the pipeline fills during startup and drains during shutdown. With 
a large number of images, the startup and shutdown time becomes 
relatively insignificant. The average time per image would approach T.
However, there’s one catch: the assumption that all the pipeline 
steps take exactly the same amount of time isn’t always true. Figure 5 
shows the scheduling diagram that occurs when the filter stage takes 
twice as long as the other stages.
If there are enough available 
cores and if all stages of  
a pipeline take an equal 
amount of time, the execution 
time for the pipeline as a 
whole is the same as the  
time for just one stage.
Image
 1
Image
 2
Image
 3
Image
 3
t0
t1
t2
t3
t4
t5
t6
t7
t8
t9
idle
idle
idle
idle
idle
idle
idle
idle
idle
idle
idle
idle
Image
 1
Image
 1
Image
 1
Image
 2
Image
 2
Image
 2
Image
 3
Image
 3
Image
 4
Image
 4
Image
 4
Image
 4
Image
5
Image
5
Image
5
Image
5
Image
 6
Image
 6
Image
 6
Image
 6
Stage 1: Load
Stage 2: Scale
Stage 3: Filter
Stage 4 Display

122
chapter seven
figure 5
Image pipeline with unequal stages
When one of the stages takes 2 x T units of time while the other 
stages take T units of time, you can see that it’s not possible to keep 
all of the cores completely busy. On average (with a large number of 
images), the time to process an images is 2 x T. In other words, when 
there are enough cores for each pipeline stage, the speed of a pipeline 
is approximately equal to the speed of its slowest stage.
If you run the ImagePipeline application in the Microsoft® Visual 
Studio® development system debugger, you can see this effect for 
yourself. The ImagePipeline sample has a user interface (UI) feature 
that reports the average length of time in milliseconds for each of the 
stages of the pipeline. It also reports the overall average length of time 
that’s needed to process each image. When you run the sample in 
sequential mode (by selecting the Sequential radio button), you’ll 
notice that the steady-state elapsed time per image equals the sum of 
all the stages. When you run in pipeline mode, the average elapsed 
time per image converges to approximately the same amount of time 
as slowest stage. The most efficient pipelines have stages of equal 
speed. You won’t always achieve this, but it’s a worthy goal.
Variations
There are several variations to the pipeline pattern.
canceling a pipeline
Pipeline tasks work together to perform their work; they must also 
work together when they respond to a cancellation.
In the standard cancellation scenario, which was explained in 
Chapter 3, your task is passed a CancellationToken value from a 
t0
t1
t2
t3
t4
t5
t6
t7
t8
t9
Stage 1: Load
Stage 2: Scale
Stage 3: Filter
Stage 4 Display
Image
 1
Image
 1
Image
 1
Image
 1
Image
2
Image
2
Image
2
Image
2
Image
3
Image
3
Image
3
Image
3
When the stages of a pipeline 
don’t take the same amount 
of time, the speed of a 
pipeline is approximately 
equal to the speed of its 
slowest stage.

 123
pipelines
higher layer of the application, such as the UI. In the case of a pipeline 
stage, you need to observe this cancellation token in two places. This 
is shown in the following code.
void DoStage(BlockingCollection<T> input,
             BlockingCollection<T> output,
             CancellationToken token)
  try
  {
     foreach (var item in input.GetConsumingEnumerable())
     {
        if (token.IsCancellationRequested) break;
        var result = ...
        output.Add(result, token);
     }
  }
  catch (OperationCanceledException) { }
  finally
  {
     output.CompleteAdding();
  }
}
A natural place to check for cancellation is at the beginning of the 
loop that processes items from the blocking collection. At this point, 
you only need to break from the loop, as shown in the following code. 
  if (token.IsCancellationRequested) break;
The second place is less obvious. You use an overloaded version of the 
BlockingCollection<T> class’s Add method that accepts a cancella-
tion token as an argument. The following code shows how to do this.
  output.Add(result, token);
If you give the Add method a cancellation token, the blocking collec-
tion will be able to detect the cancellation request while it’s waiting 
to add a new value.
To understand why it’s necessary to add such detailed cancella-
tion logic, recall that it’s possible that a pipeline stage that’s creating 
values can be blocked when it calls the Add method of its output 
queue. (If the output queue is full, the call to Add waits until space is 
available before returning.) If a cancellation request happened in this 
situation and you didn’t check for it, it would be possible for your 
program to experience deadlock. Deadlock could happen because 
stages of the pipeline that consume values produced by this stage can 

124
chapter seven
also be canceled; therefore, it’s very possible that later stages in the 
pipeline would terminate without draining the queue. The producer, 
which is blocked while waiting for space to become available, would 
have no way to proceed.
The solution, as shown in the example, is to use the overloaded 
version of the blocking collection’s Add method that accepts a can-
cellation token as an argument. If a cancellation is requested while the 
producer waits for space to become available, the blocking collection 
creates and throws an OperationCanceledException instance.
If the loop exits normally, from the break keyword, or from a 
cancellation exception, the finally clause guarantees that the output 
buffer will be marked as completed. This will unblock any consumers 
that might be waiting for input, and they can then process the cancel-
lation request.
Although it’s also possible to check for a cancellation token while 
waiting for input from a blocking collection (there’s an overloaded 
version of the GetConsumingEnumerable method that accepts a 
cancellation token), you don’t need to do this if you use the tech-
niques described in this section.
Be aware that if the type T implements the IDispose interface, 
under .NET coding conventions, you also must call the Dispose 
method on cancellation. You need to dispose of the current iteration’s 
object as well as instances of T stored in the blocking queues. The 
online source code of the ImagePipeline example shows how to do 
this.
handling pipeline exceptions
Exceptions are similar to cancellations. The difference between the 
two is that when an unhandled exception occurs within one of the 
pipeline stages, the tasks that execute the other stages don’t by de-
fault receive notification that an exception has occurred elsewhere. 
Without such notification, there are several ways for the application 
to deadlock.
Use a special instantiation of the CancellationTokenSource class 
to allow your application to coordinate the shutdown of all the pipe-
line stages when an exception occurs in one of them. Here’s an ex-
ample.
static void DoPipeline(CancellationToken token)
{
  using (CancellationTokenSource cts = 
          CancellationTokenSource.CreateLinkedTokenSource(token))
  {
Check for cancellation at  
the beginning of a stage’s 
main loop and also when 
adding values to a  
blocking collection.
Improperly canceling a pipeline 
can cause your program to 
experience deadlock. Follow 
the guidelines in this section 
carefully.
When there is an unhandled 
exception in one pipeline 
stage, you should cancel the 
other stages. If you don’t do 
this, deadlock can occur. 
Follow the guidelines in this 
section carefully.

 125
pipelines
     var f = new TaskFactory(TaskCreationOptions.LongRunning,                                                      
                             TaskContinuationOptions.None);
     var stage1 = f.StartNew(() => DoStage1(..., cts));
     var stage2 = f.StartNew(() => DoStage2(..., cts));
     var stage3 = f.StartNew(() => DoStage3(..., cts));
     var stage4 = f.StartNew(() => DoStage4(..., cts));
  }
  Task.WaitAll(stage1, stage2, stage3, stage4);
}
The CreateLinkedTokenSource method of the CancellationToken-
Source class creates a handle that allows you to respond to an exter-
nal cancellation request and also to initiate (and recognize) an internal 
cancellation request of your own. You pass the linked cancellation 
token source as an argument to the methods that execute your pipe-
line stages, so each stage can use the token source to initiate cancel-
lation. 
Here’s an example.
void DoStage(BlockingCollection<T> input,
             BlockingCollection<T> output,
             CancellationTokenSource cts)
  try
  {
     var token = cts.Token;
     foreach (var item in input.GetConsumingEnumerable())
     {
        if (token.IsCancellationRequested) break;
        var result = ...
        output.Add(result, token);
     }
  }
  catch (Exception e)
  {
    // If an exception occurs, notify all other pipeline stages.
    cts.Cancel(); 
    if (!(e is OperationCanceledException))
      throw;
  }
  finally
  {
     output.CompleteAdding();
  }
}

126
chapter seven
This code is similar to the cancellation variation described earlier, ex-
cept that when an unhandled exception occurs, the exception is inter-
cepted by the catch block, which also signals cancellation for all of 
the other pipeline stages. Consequently, each pipeline stage will begin 
an orderly shutdown.
After all pipeline tasks stop, the original exception, wrapped as an 
inner exception of an AggregateException instance, is thrown by the 
Task.WaitAll method. You should be sure to include a catch or fi-
nally block to do any cleanup, such as releasing handles to unmanaged 
resources.
You might ask why you can’t just pass a CancellationToken-
Source object to the DoPipeline method instead of a Cancellation-
Token value. By convention, only the cancellation token is passed as 
an argument. Passing a CancellationToken value allows lower-level 
library code to respond to an external cancellation but prevents 
lower-level libraries from initiating the cancellation of higher-level 
components. In other words, responding to a cancellation request 
requires less privilege than initiating such a request.
load balancing using multiple producers
The BlockingCollection<T> class allows you to read values from more 
than one producer. This feature is provided by the TakeFromAny 
static method and its variants. You can use TakeFromAny to imple-
ment load balancing strategies for some pipeline scenarios (but not 
all). This variation is sometimes known as a nonlinear pipeline.
The image pipeline example described earlier in this chapter re-
quires that the slideshow of thumbnail images be performed in the 
same order as the input files. This is a constraint that’s common to 
many pipeline scenarios, such as processing a series of video frames. 
However, in the case of the image pipeline example, the filter opera-
tions on successive images are independent of each other. In this case, 
you can insert an additional pipeline task. This is shown in Figure 6.
It is sometimes possible to 
implement load balancing  
by increasing the number of 
tasks used for a particular 
pipeline stage.

 127
pipelines
figure 6
Consuming values from  
multiple producers
Figure 6 shows what happens when you add an additional filter task. 
Both of the filter tasks take images produced by the previous stage of 
the pipeline. The order in which they consume these images is not 
fully determined, although from a filter’s local point of view, no input 
image ever arrives out of order.
Each of the filter stages has its own blocking collection for plac-
ing in queues the elements that it produces. The consumer of these 
queues is a component known as a multiplexer, which combines the 
inputs from all of the producers. The multiplexer allows its consumer, 
which in this case is the display stage of the pipeline, to receive the 
images in the correct sequential order. The images don’t need to be 
sorted or reordered. Instead, the fact that each producer queue is lo-
cally ordered allows the multiplexer to look for the next value in the 
sequence by simultaneously monitoring the heads of all of the pro-
ducer queues. This is where the blocking collection’s TakeFromAny 
method comes into play. The method allows the multiplexer to block 
until any of the producer queues has a value to read.
Here’s an example to make this more concrete. Suppose that each 
image has a unique sequence number that’s available as a property. 
The image numbers start with 1 and increase sequentially. As Figure 6 
shows, the first filter might process images that are numbered 1, 4, 
and 5, while the second filter processes images with sequence num-
bers 2, 3, 6, and 7. Each load-balanced filter stage collects its output 
images into two queues. The two output queues are correctly ordered 
(that is, no higher numbered image comes before a lower numbered 
image), but there are gaps in the sequence of numbers. For example, 
if you take values from the first filter’s output queue, you get image 1, 
followed by image 4, followed by image 5. Images 2 and 3 are missing 
because they’re found in the second filter’s output queue.
Display image
Multiplexer
Queue
Queue
Queue
Filter
Filter
1, 2, 3, 4, 5, 6, 7...
1, 2, 3, 4, 5, 6, 7...
2, 3 ,6 ,7...
2, 3 ,6 ,7...
2, 3 ,6 ,7...
1, 4, 5...
1, 4, 5...
1, 4, 5...

128
chapter seven
The gaps are a problem. The next stage of the pipeline, the Dis-
play Image stage, needs to show images in order and without gaps in 
the sequence. This is where the multiplexer comes in. Using the Take-
FromAny method, the multiplexer waits for input from both of the 
filter stage producer queues. When an image arrives, the multiplexer 
looks to see if the image’s sequence number is the next in the ex-
pected sequence. If it is, the multiplexer passes it to the Display Image 
stage. If the image is not the next in the sequence, the multiplexer 
holds the value in an internal look-ahead buffer and repeats the take 
operation for the input queue that does not have a look-ahead value. 
This algorithm allows the multiplexer to put together the inputs from 
the incoming producer queues in a way that ensures sequential order 
without sorting the values.
Figure 7 shows the performance benefit of doubling the number 
of filter stages when the filter operation is twice as expensive as the 
other pipeline stages.
figure 7
Image pipeline with load balancing
If all pipeline stages, except the filter stage, take T units of time to 
process an image, and the filter stage takes 2 x T units of time, using 
two filter stages and two producer queues to load balance the pipe-
line results in an overall speed of approximately T units of time per 
image as the number of images grows. If you run the ImagePipeline 
sample and select the Load Balanced radio button, you’ll see this ef-
fect. The speed of the pipeline (after a suitable number of images are 
processed) will converge on the average time of the slowest single-
instance stage or on one-half of the average filter time, whichever is 
greater.
Image
 1
Image
 1
Image
 1
t0
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10
Filter
Display 
Load
Resize
Image
 1
Image 
2
Image 
2
Image 
2
Image 
2
Image 
3
Image 
3
Image 
3
Image 
3
Image
4
Image
4
Image
4
Image
4
Image
5
Image
5
Image
5
Image
5
Image
6
Image
6
Image
6
Image
6

 129
pipelines
The queue wait time of Queue 3, which is displayed on the Image-
Pipeline sample’s UI, indicates the overhead that’s introduced by wait-
ing on multiple producer queues. This is an example of how adding 
overhead to a parallel computation can actually increase the overall 
speed if the change also allows more efficient use of the available 
cores.
pipelines and streams
You may have noticed that blocking collections and streams have 
some similarities. It’s sometimes useful to treat a blocking collection 
as a stream, and vice versa. For example, you may want to use a Pipe-
line pattern with library methods that read and write to streams. 
Suppose that you want to compress a file and then encrypt it. Both 
compression and encryption are supported by the .NET Framework, 
but the methods expect streams, not blocking collections, as input.
It’s possible to create a stream whose underlying implementation 
relies on tasks and a blocking collection. For more information, see 
the section, “Further Reading,” at the end of this chapter.
asynchronous pipelines
The pipelines that have been described so far are synchronous. Pro-
ducers and consumers are long-running tasks that block on inputs and 
outputs. You could also have an asynchronous pipeline, where tasks 
aren’t created until data becomes available. For that, you could use the 
AsyncCall class that’s found in the ParallelExtensionsExtras sample 
project. (For information about where to get this sample, see the sec-
tion, “Further Reading,” at the end of this chapter.) The AsyncCall 
class is a queue that a producer puts data into; if there’s currently no 
task processing the queue when data arrives, a new task is spun up to 
process the queue, and it is active as long as there’s incoming data to 
process. If it ever finds that there is no more data, the task goes away. 
If more data arrives, a new task starts.
Anti-Patterns
There are a few things to watch out for when implementing a pipeline.
thread starvation
A pipeline requires all of its tasks to be executing concurrently. If 
there are not enough threads to run all pipeline tasks, the blocking 
collections can fill and block indefinitely. Task inlining, which was 
described in Chapter 3, doesn’t help. To guarantee that a thread will 
be available to run each pipeline task, you can use the default task 

130
chapter seven
scheduler’s LongRunning task creation option. If you forget this op-
tion and are using the default task scheduler, you’re still safe. The 
thread pool will eventually notice that early stages of the pipeline are 
not making progress and inject the additional threads that are needed 
for the later pipeline stages.
infinite blocking collection waits
If a pipeline task throws an exception, it will no longer take values 
from its input blocking collection. If that blocking collection happens 
to be full, the task that writes values to that collection will be blocked 
indefinitely. You can avoid this situation by using the technique that 
was described in the section, “Canceling a Pipeline,” earlier in this 
chapter.
forgetting getconsumingenumerable()
Blocking collections implement IEnumerable<T>, so it’s easy to forget 
to call the GetConsumingEnumerable method. If you make this mis-
take, the enumeration will be a snapshot of the blocking collection’s 
state, and enumerating the results won’t consume from the collection 
or modify it in any way, which means that multiple consumers could 
get the same items.
using other producer/consumer  
collections
The BlockingCollection<T> class uses a concurrent queue as its de-
fault storage mechanism. However, you can also specify your own 
storage mechanism. The only requirement is that the underlying stor-
age must implement the IProducerConsumerCollection interface. In 
general, using the blocking collections provided by the .NET Frame-
work is easier and safer than writing your own implementation.
The .NET Framework provides several implementations of the 
IProducerConsumerCollection interface. These include the Con
currentBag and the ConcurrentStack classes. Therefore, in principle, 
you could use bag (unordered) or stack (last-in, first-out [LIFO])  
semantics for the buffers between your pipeline’s stages.
Generally, only the default first-in, first-out (FIFO) ordering is 
recommended. If you use a concurrent bag, the outputs of your pipe-
line stages don’t depend on order. In this case, a parallel loop could be 
used instead of a pipeline. Parallel loops are faster and easier to code. 
The fact that you are using unordered buffers is evidence that you’re 
using the wrong pattern.
It’s easy to forget to call the 
GetConsumingEnumerable 
because the Blocking
Collection class implements 
IEnumerable<T>. Enumerating 
over the blocking collection 
instance won’t consume  
values. Watch out for this!

 131
pipelines
Design Notes
When you use the Pipeline pattern to decompose a problem, you 
need to consider how many pipeline stages to use. This depends on 
the number of cores you expect to have available at run time. Unlike 
most of the other patterns in this book, the Pipeline pattern doesn’t 
automatically scale with the number of cores. This is one of its limita-
tions, unless additional parallelism is introduced within a pipeline 
stage itself.
More stages work well unless the overhead of adding and remov-
ing elements from the buffers becomes significant. This is usually only 
a problem for stages that perform very small amounts of work.
To achieve a high degree of parallelism, you need to be careful 
that all the stages in the pipeline take approximately the same amount 
of time to perform their work. If they don’t, the pipeline will be gated 
by the slowest component and may experience processor undersub-
scription.
Buffer size is also important for overall performance. Very small 
buffers will block the pipeline, especially if their buffer elements take 
a variable amount of processing time. Allocating larger buffers accom-
modates the variability in processing time. The size of the buffer can 
also depend on the size of the objects it contains. You would probably 
want to use a buffer with fewer entries if each element contained an 
object such as a large bitmapped image that required a great deal of 
memory.
In general, buffers should be large enough to absorb variability in 
the pipeline flow, but no larger. Use the profiler to understand the 
throughput characteristics of the pipeline and modify the buffer sizes 
to minimize the amount of time each stage is blocked by I/O waits.
Related Patterns
Pipelines have a lot in common with operating system concepts of 
pipes and filters. Pipelines are also related to streaming concepts.
Pipelines are expressions of a general technique known as pro-
ducer/cotnsumer. The pipeline is composed of a series of producer/
consumers, each one depending on the output of its predecessor.

132
chapter seven
Exercises
1.	 Write your own pipeline by modifying the example shown in 
the first section of this chapter. 
2.	 Execute the code with the Concurrency Visualizer. View  
and interpret the results.
Further Reading
The use of streams that are implemented on top of tasks and blocking 
collections is discussed by Toub. This reference uses the term “transfer 
stream” for this concept. Multiplexing inputs from multiple producer 
queues is covered by Campbell. A description of the pipes and filters 
pattern used by command shells for operating systems is described by 
Buschmann. The Microsoft Parallel Samples package is ParExtSamples 
on MSDN.
F. Buschmann, R. Meunier, H. Rohnert, P. Sommerlad, and M. 
Stal. Pattern-Oriented Software Architecture Volume 1: A System 
of Patterns. Wiley, 1996.
C. Campbell, M. Veanes, J. Huo, and A. Petrenko. “Multiplexing 
of Partially Ordered Events.” TestCom 2005, Springer Verlag, June 
2005.  
http://research.microsoft.com/apps/pubs/default.aspx?id=77808 
S. Toub. “Patterns of Parallel Programming: Understanding and 
Applying Parallel Patterns with the .NET Framework 4.” 2009.  
http://www.microsoft.com/downloads/details.
aspx?FamilyID=86b3d32b-ad26-4bb8-a3ae-
c1637026c3ee&displaylang=en
ParExtSamples software. “Samples for Parallel Programming 
with the .NET Framework 4.” 
http://code.msdn.microsoft.com/ParExtSamples

133
Adapting  
Object-Oriented  
Patterns
Appendix A
Façades, decorators, and repositories are examples of well established 
object-oriented patterns that may already be part of your develop-
ment toolkit. In this appendix, you’ll learn how to adapt these pat-
terns and others to work with parallel programs. By applying some of 
these patterns, you can reduce the complexity of your parallel code. 
You’ll also learn about some pitfalls to avoid. This appendix isn’t a 
complete list of object-oriented patterns. It highlights some of the 
more common patterns and discusses the considerations for using 
them in a parallel program.
If you’re not already familiar with object-oriented patterns, see 
the section, “Further Reading,” later in this appendix for more infor-
mation.
Structural Patterns
Façades, adapters, and decorators are all structural patterns that can 
hide the complexities of parallelism behind a simpler interface. Of 
course, any time you add another layer of abstraction, there’s a trad-
eoff between simplicity and performance. Consider this tradeoff be-
fore you apply these patterns.
When you use any structural pattern to hide the underlying paral-
lelism, methods and properties should copy values or expose immu-
table types instead of providing references to shared objects with 
mutable state. Using immutable types or copied objects removes de-
pendencies and prevents the possibility of sharing the same object 
(reference) across parallel tasks. This is especially important if you’re 
hiding parallelism from the developers who wrote the calling code. 
They may be unaware of the problems with passing shared references 
or mutable objects to code that then executes in parallel. Your imple-
mentation may require additional type mapping code to protect the 
caller from these problems. For more information about this approach, 
see the section, “Immutable Types,” later in this appendix.

134
appendix a
façade
The Façade pattern presents a simplified view of a larger system. You 
may want to use this pattern, or libraries that use this pattern, to hide 
the complexities of parallelism from other parts of an application. You 
can use a façade to hide your parallel functionality behind a serial API 
that protects the developer from any side effects caused by parallel-
ism.
A façade adds to the complexity of an application’s architecture; 
it makes things simpler only if the system behind the façade is already 
very complex. If you’re adding a façade to hide parallelism, you prob-
ably needed one in any case!
Example
In the application example shown in Chapter 5, “Futures,” the view 
model acts as a façade to the underlying application model. The view 
model abstracts the inner workings of the user interface layer of the 
application. The view has no knowledge of the inner workings of the 
view model. Instead, it passively depends on the view model to set its 
properties. For more information, see the section, “Model-View-
ViewModel,” later in this appendix.
Guidelines
Here are some guidelines for using façades:
•	 The façade reduces complexity by hiding the details of a parallel 
implementation behind a simpler interface.
•	 A façade can easily be refactored to allow different (parallel) 
implementations of particular sub-systems to be used, depend-
ing on the available hardware or problem size, without altering 
the calling code.
•	 The previous two points are general properties of the façade 
pattern. They’re particularly valuable when dealing with the 
complexity of parallelism.
decorators
The Decorator pattern overrides the behavior of an underlying class. 
Decorators use a “contains” relationship and inheritance. You can 
decorate serial implementations to create parallel ones. Like the Fa-
çade pattern, the Decorator pattern can hide the underlying complex-
ity of parallelism from the calling code.
One seemingly obvious application of decorators is to write a 
locking decorator for a type that’s not thread safe. There are two 
reasons to avoid this approach. A locking decorator is unlikely to have 
good parallel performance unless the locks are infrequently used. 

 135
adapting object-oriented patterns
Also, while adding locks to individual methods may appear to add 
thread safety, combinations of those methods may still require addi-
tional locking to ensure correctness.
Example
The following example shows a decorated IImageEditor interface. 
The underlying SerialEditor implementation is decorated by the 
ParallelEditor class, a parallel implementation that calls the decorated 
type.
public interface IImageEditor
{
  void Rotate(RotateFlipType rotation, 
              IEnumerable<Bitmap> images);
}
public class SerialEditor : IImageEditor
{
  public void Rotate(RotateFlipType rotation,  
                     IEnumerable<Bitmap> images)
  {
    foreach (Bitmap b in images)
      b.RotateFlip(rotation);
  }
}
public class ParallelEditor : IImageEditor
{
  private IImageEditor decorated;
  public ParallelEditor(IImageEditor decorated)
  {
    this.decorated = decorated;
  }
  // ... Modified behavior
  public void Rotate(RotateFlipType rotation, 
                     IEnumerable<Bitmap> images)
  {
    if (decorated == null)
      return;
    Parallel.ForEach(images, b =>
      {
        b.RotateFlip(rotation);

136
appendix a
      });
  }
  // Additional behavior...
}
The example creates a decorated ParallelEditor object and passes an 
instance of SerialEditor to the constructor. The ParallelEditor 
modifies the behavior of the Rotate method to run using Parallel.
ForEach, but it doesn’t replace any other methods of the SerialEditor 
class. This is shown in the following code.
IList<Bitmap> images = new List<Bitmap>(); 
// Load images...
IImageEditor parallel = new ParallelEditor(new SerialEditor());
parallel.Rotate(RotateFlipType.RotateNoneFlipX, images); 
It’s important that the decorated Rotate method does not access 
mutable shared state. Each step of the Parallel.ForEach loop must be 
independent.
Guidelines
Here are some guidelines for using decorators:
•	 The principle advantage of the decorator is that it encapsulates 
parallelism while preserving the existing interface. The calling 
code remains largely unchanged.
•	 A decorator allows different parallel implementations to be 
used, depending on the available hardware or problem size, 
without altering the calling code.
•	 The decorator separates concerns. The decorated type imple-
ments the work being done, while the decorator is only respon-
sible for parallelizing the work.
•	 The first two points are general properties of the decorator 
pattern. They’re particularly valuable when dealing with the 
complexity of parallelism.
adapters
The Adapter pattern translates from one interface to another. This 
pattern is applicable to parallelism because, like with the Façade pat-
tern, it allows you to hide complexity and to expose an interface that’s 
easier for developers to use.

 137
adapting object-oriented patterns
Example
The following code exposes an interface named IWithEvents that’s 
based on events, and an interface named IWithFutures that uses fu-
tures. The goal is to use interfaces that are more familiar to developers 
who are not parallel programming experts or who prefer an event-
based model.
public interface IWithFutures
{
  Task<int> Start();
}
public class FuturesBased : IWithFutures
{
  public Task<int> Start()
  {
    return Task<int>.Factory.StartNew(() =>
      {
        // ...
      });
  }
}
public interface IWithEvents
{
  void Start();
  event EventHandler<CompletedEventArgs> Completed;
}
public class EventBased : IWithEvents
{
  readonly IWithFutures instance = new FuturesBased();
  public void Start()
  {
    Task<int> task = instance.Start();
    task.ContinueWith((t) =>
        {
          var evt = Completed;
          if (evt != null)

138
appendix a
            evt(this, new CompletedEventArgs(t.Result));
        });
  }
  public event EventHandler<CompletedEventArgs> Completed;
}
The event-based implementation adapts the IWithFutures interface 
and allows results to be handled by an event handler. This is shown in 
the following code.
IWithEvents model = new EventBased();
bool completed = false;
model.Completed += (s, e) =>
  {
    Console.WriteLine(“Completed Event: Result = {0}”, e.Result);
    completed = true;
  };
// Start model and wait for Completed event...
model.Start();
Guidelines
Use the adapter pattern to modify an existing interface to make it 
easier for developers who are not familiar with parallelism to use.
repositories and parallel data access
A repository mediates between the application logic and the data ac-
cess layers. You can think of a repository as an application of the Fa-
çade pattern to one or more data sources. Repositories in the Micro-
soft® .NET Framework commonly use ADO.NET to access a database. 
This section discusses what to think about when you call an ADO.
NET-based repository from a parallel application, instead of imple-
menting the Repository pattern itself.
ADO.NET is optimized for throughput and scalability. ADO.NET 
objects don’t lock resources and must be used on only a single thread. 
The ADO.NET Entity Framework (EF) is built on top of ADO.NET 
and has the same limitation. Considering these constraints, how can 
your application access a repository from multiple tasks, and when 
would it be appropriate to do this? 
Accessing data from a set of parallel tasks is very similar to access-
ing it from the ASP.NET thread pool. Much of the same ASP.NET and 
data access guidance applies. Specifically, when connecting to a data-
base from multiple tasks, don’t share connections between tasks and 

 139
adapting object-oriented patterns
keep connections open for as little time as possible so that they can 
be reused.
Connecting from many parallel tasks to a single database may 
seem like a good way to improve performance. For example, you may 
wonder why there’s no such thing as “PLINQ-to-SQL.” In fact, using 
multiple tasks for data access can reduce the time taken by individual 
clients to access a database, but it negatively affects the overall 
throughput of the database. In other words, individual clients will 
each use more database resources, which may make them individually 
faster, but it’s at the expense of the number of clients that can access 
the database.
Parallelism is appropriate for applications that merge the results 
of queries from several different data sources. Applications that run 
on Windows Azure are good examples. Windows Azure applications 
often store data in a mixture of table storage and blob storage and 
may break data up between databases for reasons of scalability and 
performance.
Example
For examples of how to connect to several different databases in 
parallel, see the section, “Further Reading,” later in this appendix.
Guidelines
Here are some guidelines for accessing data:
•	 Don’t share ADO.NET connections between tasks. ADO.NET is 
not thread safe. Each task should use its own connection.
•	 Keep connections to the data source for as little time as pos-
sible. Explicitly close connections to ensure that unused  
connections aren’t left open. Don’t rely on garbage collection  
to do this for you.
•	 Use tasks to parallelize connections to different databases. 
Using tasks to open multiple database connections to the same 
database may have significant performance implications.
Singletons and Service Locators
Singletons are classes with only one instance. Service locators are 
singletons that control object resolution or that map interfaces to 
specific implementation types at run time. Service locators support 
unit testing and composite or composable applications. Querying a 
service locator for an instance that provides a particular interface can 
return a single shared object, a new instance that isn’t shared, or a 
cached instance from a pool. This is often referred to as “resolving” an 
instance.

140
appendix a
Normally, instances of an object are created using the class  
constructor. This is shown in the following example.
MyClass variable = new MyClass();
Instead, you want to be able to obtain the single instance, as shown 
here.
MyClass variable = MyClass.Instance;
Here, Instance is a static read-only property of the class, and each 
time it’s called, it’ll refer to the same single instance of MyClass. By 
adding this special Instance property, MyClass is transformed into a 
singleton class.
The instance returned by the singleton is evaluated lazily. Lazy 
evaluation means that the instance is created only when it is needed. 
Your program avoids creating an object that either uses many re-
sources or is time consuming to create until it’s actually needed.
The use of singletons is sometimes criticized because it can lead 
to shared state. As you’ve seen throughout this book, mutable shared 
state prevents parallel execution. However, singletons are a good way 
to easily identify global state. Remember that when you see singletons 
and service locators in an application, it’s probable that there’s shared 
state. Shared state will likely impact the scalability of your application, 
so it’s worth considering if the dependency on shared state can be 
removed before simply applying the singleton or service locator  
pattern.
For various approaches to implementing thread-safe singletons, 
see the section, “Further Reading,” later in this appendix.
implementing a singleton with the 
lazy<t> class
The .NET Framework 4 makes it easy to implement singletons. It pro-
vides a class named Lazy<T>. Here’s an example.
public sealed class LazySingleton
{
  private readonly static Lazy<LazySingleton> instance = 
      new Lazy<LazySingleton>(() => new LazySingleton() );
  private LazySingleton() { }
  public static LazySingleton Instance
  {
    get { return instance.Value; }
  }
}

 141
adapting object-oriented patterns
The Lazy<T> not only handles the locks necessary for the creation of 
the instance value, but it also throws any exceptions from the  
instance constructor to the calling code. The class uses the sealed 
keyword to prevent further inheritance and unintended modifications 
to the class.
notes
The Lazy<T> class provides a thread-safe way to resolve an instance 
of your class, but it does nothing to make the remainder of your class 
thread safe. Singletons imply shared state, so further synchronization 
may still be required.
Implementations of service locators also need to deal with syn-
chronization issues. If you’ve written your own service locator or de-
pendency injection container and you want to make it thread safe, 
you need to either ensure that multiple resolves do not interfere with 
each other or implement locking so that that only one thread resolves 
a type at a time. If you’re using a third-party dependency injection 
container, check that its resolution-related classes are thread safe. 
Most dependency injection containers, such as the Unity Application 
Block (Unity), support thread-safe resolution but not thread-safe 
configuration.
guidelines
Here are some guidelines for using singletons and service locators in 
parallel programs:
•	 Inclusion of service locators or singletons strongly suggests that 
your program relies on a shared resource or shared data. Con-
sider modifying your algorithm to remove the need for shared 
resources or data where possible.
•	 Shared resources may impact the performance of your parallel 
application because tasks that access the shared resource have 
to wait their turns. The degree of impact depends on the 
amount of time your tasks spend waiting relative to the amount 
of time spent doing other work.
•	 Ask yourself if your singleton is necessary and if all the data 
inside the singleton needs to be there. A singleton should 
contain only essential data.
•	 Evaluate how often the data that must be protected is accessed. 
If the singleton contains different kinds of data that are ac-
cessed with different levels of frequency, you’ll probably need 
to take too many locks.
•	 If you’re using a service locator, whether from a third party or 
one you wrote yourself, make sure that its object-resolution 
functionality is thread safe. Remember that this may impact 

142
appendix a
performance if the locator serializes access from multiple 
threads to ensure correctness.
•	 The examples in this section demonstrate how to create thread-
safe singletons. You must also think about parallel access when 
you design the shared objects that they return.
Model-View-ViewModel 
Model-View-Presenter (MVP) and Model-View-ViewModel (MVVM) 
are both patterns that separate the domain logic from the user inter-
face (UI). This separation makes your application easier to understand 
and test. The model encapsulates the application logic and state. The 
view model exposes appropriate state to the view in a bindable form. 
It supports the view by managing state and interaction logic. The view 
model acts as a façade to the underlying model. The view model may 
add additional behaviors that are required by the view. The view 
model may also implement a Composite pattern, composing itself 
from more than one model object.
MVVM is particularly applicable to Windows Presentation Foun-
dation (WPF)–based applications because it takes advantage of 
WPF’s extensive support of data binding. In WPF, the view can be 
extremely simple and contain little or no code. It relies on data and 
command binding to connect it to the view model. The MVVM pat-
tern is a specialization of the Presentation Model pattern and is tai-
lored for WPF.
MVP and MVVM separate the concerns of the domain logic from 
the UI logic, but they don’t rely on data binding. In MVP, the pre-
senter acts as an intermediary between the model and the view. This 
section only shows examples of the MVVM pattern and not MVP, but 
much of the MVVM example also applies to MVP. For more informa-
tion about view models and presenters, see the section, “Further 
Reading,” later in this appendix.
From a parallel programmer’s perspective, the biggest challenge is 
to ensure that your parallel code, which runs in the model, handles the 
UI thread correctly, updates the UI, and exposes any data in a thread-
safe way. This section discusses how to use continuation tasks to do 
this. There are also other issues related to concurrency and how to 
build multithreaded UIs that are responsive to users, but these are 
outside the scope of this book.

 143
adapting object-oriented patterns
example
The Adatum Dashboard application shown in Chapter 5 is an example 
of a parallel MVVM application. To understand this explanation, refer 
to the source code that’s located in the project Chapter5\A-Dash di-
rectory of the online examples. Figure 1 illustrates the architecture.
figure 1
The Model View-View  
Model pattern
Employees expect the application to be responsive regardless of com-
putational load and I/O latencies. Blocking the UI for more than a 
fraction of a second is unacceptable. Because of this requirement, 
even a program that takes advantage of any potential parallelism to 
perform the computations would still need to use background threads 
to ensure that the UI remains responsive. The Adatum Dashboard 
example demonstrates how to execute the model on the .NET thread 
pool and how to use the view model to expose the model’s state to 
the view.
The DoAnalysisParallel method returns an AnalysisTasks object 
that contains futures for each of the operations in the task graph. For 
example, the CompareModels property contains a future that returns 
a market recommendation value. The future is implemented as a 
Task<MarketRecommendation> object. The following code retrieves 
the final result of the analysis.
AnalysisTasks tasks = engine.DoAnalysisParallel();
This code immediately returns an AnalysisTasks object that contains 
a Task<> object for each step of the calculation.
View (XAML)
View Model
Model
MainWindow
MainWindowViewModel
AnalysisEngine
Commands
Data Building

144
appendix a
If you’re interested in examining any partial results that are avail-
able before the entire analysis finishes, you can do this. For example, 
the following code retrieves the information produced by the task 
that loads the NYSE market data from the network.
MarketData nyseData = tasks.LoadNyseData.Result;
the dashboard’s user interface
Futures and continuation tasks are also used in the Adatum Dash-
board UI, which relies on WPF. The Adatum Dashboard UI is designed 
so that the result of each step in the analysis can be viewed by the 
user as the computation progresses. There are individual buttons for 
each step. As each result becomes available, the corresponding button 
is enabled. It’s also possible to cancel the analysis from the UI. The 
Adatum Dashboard application demonstrates how an application can 
use continuation tasks to keep the UI up to date.
The application uses continuation tasks instead of event handlers 
or other callback mechanisms. In fact, you can think of a continuation 
task as a kind of callback.
Here’s a description of how the notification works. The following 
code is for the main window view model.
public class MainWindowViewModel : IMainWindowViewModel
               INotifyPropertyChanged, IDisposable
{
  // ...
  
  void OnRequestCalculate()
  {
    // ...
    AnalysisTasks tasks = engine.DoAnalysisParallel();
    AddButtonContinuations(tasks); 
  }
  // ...
}
The user interface uses the MVVM pattern. The main window’s  
view model has a CalculateCommand that invokes the OnRequest
Calculate method when a user clicks the Calculate button.
Instead of starting background tasks with a method such as 
QueueUserWorkItem from the ThreadPool class, the view model 
asks the analysis engine to create an AnalysisTasks object that con-
tains tasks that correspond to each of the analysis results.

 145
adapting object-oriented patterns
This architecture demonstrates decoupling. The developer who 
wrote the application’s analysis engine didn’t need to know how 
other parts of the application would use the results of the analysis.
Next, the handler uses the AddButtonContinuations method to 
create UI-specific continuation tasks. This is shown in the following 
code.
void AddButtonContinuations(AnalysisTasks tasks)
{
  AddButtonContinuation(tasks.LoadNyseData, 
     t => { NyseMarketData = t.Result; });
  AddButtonContinuation(tasks.LoadNasdaqData,
     t => { NasdaqMarketData = t.Result; });
  AddButtonContinuation(tasks.LoadFedHistoricalData,
     t => { FedHistoricalData = t.Result; });
  AddButtonContinuation(tasks.MergeMarketData,
     t => { MergedMarketData = t.Result; });
  AddButtonContinuation(tasks.NormalizeHistoricalData,
     t => { NormalizedHistoricalData = t.Result; });
  AddButtonContinuation(tasks.NormalizeMarketData,
     t => { NormalizedMarketData = t.Result; });
  AddButtonContinuation(tasks.AnalyzeHistoricalData,
     t => { AnalyzedHistoricalData = t.Result; });
  AddButtonContinuation(tasks.AnalyzeMarketData,
     t => { AnalyzedStockData = t.Result; });
  AddButtonContinuation(tasks.ModelHistoricalData,
     t => { ModeledHistoricalData = t.Result; });
  AddButtonContinuation(tasks.ModelMarketData,
     t => { ModeledMarketData = t.Result; });
  AddButtonContinuation(tasks.CompareModels,
     t =>
     {
       this.Recommendation = t.Result;
       this.StatusTextBoxText =

146
appendix a
          (this.Recommendation == null) ?
              “Canceled” : this.Recommendation.Value;
       this.ModelState = State.Ready;
     });
  tasks.ErrorHandler.ContinueWith(
    t =>
    {
      if (t.Status == TaskStatus.Faulted)
        this.StatusTextBoxText = “Error”;
      this.ModelState = State.Ready;
    }, TaskScheduler.FromCurrentSynchronizationContext());
}
The AddButtonContinuations method uses a special scheduler to 
create continuation tasks that automatically run in the UI thread after 
each task finishes and has results that are ready to view. The result of 
the FromCurrentSynchronizationContext method of the Task-
Scheduler class is a TaskScheduler object that allows its tasks to run 
only in the synchronization context specified by the current thread 
(in this example, this is the UI thread).
Each continuation task sets a property on the view model that is 
based on the task’s result. When the final CompareModels task 
finishes, the UI view model is updated with the final recommendation. 
Callbacks registered with the view model notify the UI of the chang-
es so that they’ll be reflected in the UI.
The last continuation task is the error handler. If an application 
error occurs, the Adatum dashboard application observes the excep-
tion using a continuation task that is dedicated to error handling.  
The error handling continuation task updates the UI with a second 
continuation task that runs in the UI thread.
The helper method AddButtonContinuation is a simple applica-
tion of the ContinueWith method and creates continuation tasks. It’s 
shown in the following code.
void AddButtonContinuation<T>(Task<T> task, 
                              Action<Task<T>> action)
{
  task.ContinueWith(
          action,
          CancellationToken.None,
          TaskContinuationOptions.OnlyOnRanToCompletion,
          TaskScheduler.FromCurrentSynchronizationContext());
}

 147
adapting object-oriented patterns
Tasks make it easy to satisfy 
thread affinity constraints.
Before incorporating parallelism into the application, Adatum used 
background worker threads to handle the computationally intensive 
parts of the dashboard. However, the Adatum Dashboard application 
has some requirements, such as the use of WPF for the UI, that make 
continuation tasks more appropriate.
One of the reasons that the Futures pattern works for the Ada-
tum Dashboard is because it satisfies the thread affinity requirements 
imposed by WPF. Some frameworks place this constraint on objects 
they expose. For example, in WPF, you have to run all methods of a UI 
object on the same thread that you used to create that object. Typi-
cally, applications use the Dispatcher class to route work that will be 
processed on the UI thread. For more information, see the section, 
“Further Reading,” later in this appendix.
The Futures pattern makes it easy to deal with thread affinity. 
Continuation tasks can be configured with a task scheduler that runs 
the task on a particular thread. Antecedents of the task don’t need to 
run on the same thread as the continuation task. This feature allowed 
Adatum’s developers to distribute computationally intensive work 
among many cores and to ensure that the calculated values appeared 
in the UI without violating WPF’s thread affinity constraint.
guidelines
Here are some guidelines for using futures and continuations:
•	 Use continuation tasks that are attached to the model’s tasks  
to update the view model. This approach helps to decouple the 
model from the view.
•	 Use immutable types to pass results from the continuation tasks 
to the UI thread. This makes it easier to ensure that your code is 
correct.
•	 Use continuation tasks instead of background worker threads 
when you want to execute several different tasks in the back-
ground and merge the results before passing them back to the 
UI thread.
•	 Use the correct task scheduler to make sure that modifications 
to UI objects, which have thread affinity, take place on the main 
UI thread.

148
appendix a
Don’t overlook the importance 
of immutable types in parallel 
programming. They can 
improve reliability as well  
as performance.
Immutable Types
When writing parallel code, you can minimize the chances of one task 
interfering with another by not sharing between tasks any objects 
that have mutable fields. Instead, use immutable types, which are also 
sometimes known as purely functional data structures. You can use im-
mutable types to implement the (Immutable) Value Object pattern.
A familiar example of an immutable data type is the .NET string 
class. The System.String class has many operations for creating 
strings, but none of these operations allow you to modify a string. 
Instead, the operations create a new string. Another example of an 
immutable type is the .NET System.Uri class. Instances of this class 
represent a Uniform Resource Identifier (URI), with methods and 
properties that let you easily create a URI and query it for its con-
stituent elements. You don’t need to worry that another thread can 
modify a URI while you are using it.
Immutable types are a very useful form of scalable sharing. For 
example, when you append to a string, the result is a string that in-
cludes the addition. The original string is unmodified. This means that 
you can always use a particular string value in parallel code without 
worrying that another task or thread is modifying it. The combination 
of immutable types and the .NET Framework concurrent collection 
classes is particularly useful. For more information about why immu-
table types are a good idea for parallel programs, see the section, 
“Scalable Sharing of Data,” in Chapter 1, “Introduction.”
To be considered immutable, a type must meet the following 
conditions:
•	 The type must contain only fields that aren’t modified outside 
the constructor. This means that an object’s state can’t be 
changed after it’s been constructed. The C# readonly keyword 
can be used to enforce this at compile time.
•	 The type must contain only fields that are also immutable types.
•	 The type must only inherit from other immutable types.
•	 The type can’t be inherited by mutable types. Although this 
might seem unnecessary, it prevents a mutable type from 
inheriting an immutable type and altering the immutable type’s 
behavior by overriding a virtual method. You may want to mark 
your immutable types with the sealed keyword to enforce this.
•	 An instance of the type can’t publish references to itself until 
after construction is complete. If an object exposes a reference 
to itself (its this reference) during construction, 
it might be possible for the reference to be accessed by another 

 149
adapting object-oriented patterns
thread before construction is completed. It would then be 
possible for the second thread to observe an object whose 
internal state was changing during construction. In other words, 
the assumption of immutability would be broken.
These conditions can be relaxed if you only require an object with 
observational immutability; this is an object that appears immutable 
publically but contains some private mutable state. If you choose this 
approach, your implementation must ensure that changes to the pri-
vate mutable state of the object are done in a thread-safe manner. The 
ReadOnlyCollection<T> and Lazy<T> classes are both examples of 
observationally immutable types.
Objects in C# are not immutable by default, but it’s possible to 
implement immutable types. Languages such as F# include built-in 
support for creating immutable types.
example
Many of the types used by the Adatum Dashboard example in Chap-
ter 5 are immutable. Values of these types communicate the result of 
the financial analysis. For example, here’s an implementation of the 
StockData class. Instances of this class represent the price history of 
a financial asset. The Adatum Dashboard example passes StockData 
and StockDataCollection objects between tasks. No locking is re-
quired because the types themselves are immutable and therefore 
free of side effects.
public sealed class StockData
{
  readonly string name;
  readonly ReadOnlyCollection<double> priceHistory;
  public string Name
  {
    get { return name; }
  }
  public ReadOnlyCollection<double> PriceHistory
  {
    get { return priceHistory; }
  }
  public StockData(string name, double[] priceHistory)
  {
     this.name = name;

150
appendix a
     this.priceHistory = new
               ReadOnlyCollection<double>(priceHistory);
  }
  // ...
}
The StockData class uses the readonly keyword to ensure that the 
class doesn’t inadvertently modify its fields. If you try to modify a 
read-only field, the compiler will issue an error.
Although the StockData class doesn’t do this, you could also have 
defined the Name property as an automatic property with a private 
setter. Here’s an example.
class StockData2
{
   public string Name { get; private set; }
   // ...
}
An advantage of automatic properties is that they’re concise and easy 
to specify. A disadvantage is that you must manually ensure that none 
of the other operations of the class modify the property. Marking the 
setter as private only prevents modification of the property by callers 
that are outside of the class.
immutable types as value types
Value types use copy semantics for assignment. Two values of such a 
type are equal if all of their corresponding fields are equal. If two 
values are equal, their hash codes are also equal. The .NET type Sys-
tem.Double is an example of a value type. You can contrast value 
types with reference types. Classes in C# are reference types. By de-
fault, instances of a class are equal only if they’re the result of the 
same invocation of the new operator.
You often want immutable types to behave like value types even 
though they’re implemented with reference types. For example, you 
might want to use instances of your type as keys of a dictionary. The 
System.String class is an example of a reference type that in most 
ways emulates the behavior of a value type. (You can bypass the 
“value type” abstraction of the String class by invoking the Object.
ReferenceEquals method.)
You can make an immutable type behave like a value type by im-
plementing an Equals method that uses structural equality (a field-by-
field comparison) instead of reference equality (the test used by the 
Object.ReferenceEquals method). If you implement the Equals 
method in your type, you must also implement the GetHashCode 
method so that values that are equal always have the same hash code. 

 151
adapting object-oriented patterns
It’s also recommended that you implement the equals operator (==). 
Here’s an example.
public sealed class StockData
{
  // ...
  public static bool operator ==(StockData a, StockData b)
  {
    if (System.Object.ReferenceEquals(a, b))
      return true;
    if (((object)a == null) || ((object)b == null))
      return false;
    if (a.Name != b.Name)
      return false;
    if (a.PriceHistory.Count != b.PriceHistory.Count)
      return false;
    for (int i = 0; i < a.PriceHistory.Count; i++)
    {
      if (a.PriceHistory[i] != b.PriceHistory[i])
        return false;
    }
    return true;
  }
  public static bool operator !=(StockData a, StockData b)
  {
    return !(a == b);
  }
  public override bool Equals(object obj)
  {
    if (obj == null)
      return false;
    return Equals(obj as StockData);
  }
  public bool Equals(StockData d)
  {
    if (d == null)
      return false;
    return (this == d);
  }
  public override int GetHashCode()

152
appendix a
  {
    int result = name.GetHashCode() ^ priceHistory.Count;
    for (int i = 0; i < priceHistory.Count; i++)
      result ^= priceHistory[i].GetHashCode();
    return result;
  }
}
For more information about equality implementations, see the sec-
tion, “Further Reading,” later in this appendix.
compound values
Immutable types are not limited to only records with fixed numbers 
of fields. It’s also possible to implement complex structures, such as 
tables, trees, and sequences as immutable or purely functional data 
types. For example, the “Insert node” operation on an immutable tree 
data type returns a new tree.
At first, you might think that tables and trees implemented as 
immutable types would be very inefficient. However, there are  
implementation techniques that don’t require you to copy the entire 
structure with each insertion or deletion operation. It’s possible for 
an implementation to use a significant amount of structure sharing. 
For more information, see the section, “Further Reading,” later in this 
appendix.
guidelines
Here are some guidelines for using immutable types:
•	 Consider using immutable types when sharing data between 
threads. You may want to use them in conjunction with the 
shared data structures discussed in the next section of this 
appendix.
•	 For more complex types you may want to consider implement-
ing “freezable” or “popsicle” immutability. Such types are 
mutable when they are created but become immutable when 
the user calls a “freeze” method. WPF uses this approach and 
provides a System.Windows.Freezable base class. You can 
also use a Builder pattern to provide a way to incrementally 
create an immutable value before freezing it.
For more information about creating immutable types in C#, see the 
section, “Further Reading,” later in this appendix.

 153
adapting object-oriented patterns
Shared Data Classes
Many of the patterns discussed in this book deliberately try to mini-
mize the amount of shared data. The .NET Framework 4 includes a 
System.Collections.Concurrent namespace that contains several 
thread-safe data structures. If you decide that your application needs 
to share state, first review the collections provided by the .NET 
Framework 4 before writing your own implementation. The challenge 
of writing shared data structures that are both correct and perform 
well shouldn’t be underestimated. The following table describes the 
.NET Framework 4 data structures.
Type
Description
BlockingCollection<T>
This data structure implements both a bounded 
and an unbounded producer/consumer. Use it to 
connect tasks in the style of producers and 
consumers. The Pipeline pattern in Chapter 7, 
“Pipelines,” uses blocking collections to connect 
pipeline stages.
ConcurrentBag<T>
This data structure is an unordered collection  
of objects that is useful for storing data where 
ordering is not important. The code samples in 
Chapter 6, “Dynamic Task Parallelism,” include 
examples of storing unordered results in a 
concurrent bag.
ConcurrentDictionary<T>
This data structure is a concurrent dictionary for 
storing and retrieving key/value pairs.
ConcurrentQueue<T>
This data structure is a concurrent and scalable 
first-in, first-out (FIFO) non-blocking queue.  
Use it to connect different tasks or to give work 
to tasks. Chapter 6 discusses variations that use 
queues to provide values to tasks.
ConcurrentStack<T>
This data structure is a concurrent and scalable 
last-in, first-out (LIFO) stack. Use it when the last 
item added is the most important one to process 
next.
	
guidelines
Here are some guidelines for using shared data:
•	 Think twice before using sharing data. Many of the patterns 
described in this book either try to limit sharing or avoid it 
entirely.
•	 Where possible, use shared data collections in preference to 
locks.
•	 Use the shared data classes provided by the .NET Framework 4 
in preference to writing your own classes.
Collections from the 
namespace System.
Collections.Generic are 
not thread safe. You must  
use the collections in  
System.Collections. 
Concurrent whenever 
collections are shared by  
more than one thread.	

154
appendix a
Iterators
Iterators let you program the control flow of another method. Itera-
tors are similar to what are known as coroutines in computer science. 
The C# language provides built-in support for writing iterators. Here’s 
an example.
public static IEnumerable<string> GetImageFilenames(
                             string sourceDir, int maxImages)
{
  var names = GetImageFilenamesList(sourceDir, maxImages);
  while (true)
  {
    foreach (var name in names)
      yield return name;
  }
}
This example produces a non-terminating list of file names.
example
Iterators are very useful when combined with parallel loops. In fact, a 
custom iterator is a kind of extension method for parallel loops. For 
example, the following code shows a binary tree.
class Tree<T>
{
  public Tree<T> Left, Right;
  public T Data;
}
You can implement a custom iterator for the Tree<T> class. This itera-
tor can be used by a parallel loop to access the tree’s nodes. This is 
shown in the following code.
public IEnumerable<Tree<T>> Iterate<T>()
{
  var queue = new Queue<Tree<T>>();
  queue.Enqueue(this);
  while (queue.Count > 0)
  {
    var node = queue.Dequeue();
    yield return node;
    if (node.Left != null) queue.Enqueue(node.Left);
    if (node.Right != null) queue.Enqueue(node.Right);
  }
}
Custom iterators can be used 
with parallel loops.

 155
adapting object-oriented patterns
The custom iterator can be used with the Parallel.ForEach method.
Tree<T> myTree = ...
Parallel.ForEach(myTree.Iterator(), node =>
{
  // ... process node in parallel
}); 
A more advanced variation would create partitions based on subtrees. 
For example, you could implement a Partitioner object that divided 
the tree into subtrees whose roots corresponded to nodes of a certain 
depth in the original tree. This would be especially efficient if, for 
example, the locations in memory of the subtrees improved memory 
cache performance.
Lists and Enumerables
The .NET type IList<T> defines the functionality of indexed lists. The 
type IEnumerable<T> is used for unindexed iteration.
You should be aware of how these two types interact with paral-
lel loops. In certain rare cases, the parallel loop’s default handling of 
the IList<T> type may not be what you want. This can occur when the 
IList<T> implementation has unfavorable random-access performance 
characteristics or when there are race conditions that result from 
multiple threads attempting to perform lazy loading at the same time.
You can override Parallel.ForEach’s default handling of a source 
that provides the IList<T> interface.
The Parallel.ForEach method requires its sources to provide the 
IEnumerable<T> interface; however, it also checks to see whether its 
source provides the IList<T> interface. If it does, Parallel.ForEach 
uses this interface by default. In most cases, using IList<T> to access 
elements of the collection results in more efficient partitioning strat-
egies because it provides random (that is, indexed) access to the items 
in the collection. In contrast, IEnumerable<T> only supports access 
that walks the collection using the MoveNext method to retrieve 
successive elements. In almost all cases, the default behavior results in 
better performance.
A few types that provide IList<T> do so in a way that makes con-
current indexing an expensive or even incorrect operation. For these 
types, MoveNext is a better accessor. You can use a Partitioner ob-
ject to force Parallel.ForEach to use the IEnumerable<T> interface, 
even if IList<T> is available. Here’s an example.
You can control whether  
a parallel loop uses IList 
or IEnumerable in cases 
where both are available. 
This is needed only in  
certain rare cases.

156
appendix a
IEnumerable<T> source = ...;
// Will always use source’s IEnumerable<T> implementation.
Parallel.ForEach(Partitioner.Create(source), 
                 item => { /*... do work ... */ });
The System.Data.Linq.EntitySet<TEntity> class is an example of a 
type that should use IEnumerable<T> for parallel iteration. This is 
because of the type’s lazy loading semantics. (If two threads attempt 
to access the indexer concurrently, the EntitySet<TEntity> instance 
may become corrupted.)
Further Reading
Here is some further reading about the topics discussed in this  
appendix:
structural patterns
For best practices for using ADO.NET, see “Best Practices for 
Using ADO.NET” on MSDN: 
http://msdn.microsoft.com/en-us/library/ms971481.aspx 
For ADO.NET performance guidelines, see Chapter 12, “Improv-
ing ADO.NET Performance” in Improving .NET Application 
Performance and Scalability on MSDN:
http://msdn.microsoft.com/en-us/library/ff647768.aspx
For an overview of the ADO.NET Entity Framework, see “ADO.
NET Entity Framework” on MSDN: 
http://msdn.microsoft.com/en-us/library/bb399572.aspx
For information about the Repository pattern, see Patterns 
of Enterprise Application Architecture. Martin Fowler. Patterns of 
Enterprise Application Architecture. Addison-Wesley Professional, 
2002.
For information about Windows® Azure, including a discussion 
about repositories with parallel database access, see the pat-
terns & practices Windows Azure Guidance site: 
http://wag.codeplex.com/
singleton
For the original “Gang of Four” patterns, see Design Patterns: 
Elements of Reusable Object-Oriented Software.

 157
adapting object-oriented patterns
Gamma, Erich, Richard Helm, Ralph Johnson, John M. Vlissides. 
Design Patterns: Elements of Reusable Object-Oriented Software. 
Addison-Wesley Professional, 1994.
For information about implementing singletons and other design 
patterns for C#, see C# 3.0 Design Patterns.
J. Bishop. C# 3.0 Design Patterns. O’Reilly, 2008.
For readers who want considerably more detail, Duffy (2008), 
Chapter 10, contains an excellent in-depth description of 
singletons. 
J. Duffy. Concurrent Programming on Windows, Addison-Wesley, 
2008.
For documentation for the Unity dependency injection con-
tainer and a discussion of containers, see “Unity Application 
Block” on MSDN: 
http://msdn.microsoft.com/unity
For further discussion of lazy initialization, see “Lazy Initializa-
tion” on MSDN: 
http://msdn.microsoft.com/en-us/library/dd997286.aspx
model-view-viewmodel 
For an in-depth discussion of the MVVM pattern, see the article, 
“WPF Apps With The Model-View-ViewModel Design Pattern,” 
in MSDN Magazine:
http://msdn.microsoft.com/en-us/magazine/dd419663.aspx
For information about the MVP pattern, see the article, “Model 
View Presenter,” in MSDN Magazine:
http://msdn.microsoft.com/en-us/magazine/cc188690.aspx
For an overview of the WPF threading model and how to 
update WPF objects with the Dispatcher class, see the article, 
“Build More Responsive Apps With The Dispatcher,” in  
MSDN Magazine:
http://msdn.microsoft.com/en-us/magazine/cc163328.aspx 
For information about the BackgroundWorker class, including 
how to use it and an example, see “BackgroundWorker Class”  
on MSDN: 
http://msdn.microsoft.com/en-us/library/system. 
componentmodel.backgroundworker.aspx

158
appendix a
immutable types
Eric Lippert’s blog includes an excellent series of posts on 
immutability in C#: 
http://blogs.msdn.com/b/ericlippert/archive/tags/immutability/
Joe Duffy’s blog also covers the requirements for immutability 
from a parallel perspective: 
http://www.bluebytesoftware.com/blog/2007/11/11/ 
ImmutableTypesForC.aspx 
For details of the requirements for implementing equality, see 
“Guidelines for Overriding Equals() and Operator == (C# 
Programming Guide)” on MSDN: 
http://msdn.microsoft.com/en-us/library/ms173147(VS.90).aspx
For more information about implementing immutable types, 
including complex structures, such as sequences, tables, and 
trees, see Purely Functional Data Structures. Chris Okasaki. Purely 
Functional Data Structures. Cambridge University Press, 1998. 
For an example of a C# implementation of immutable types  
for sequences, sets, and maps, see the NModel framework on 
CodePlex: 
http://nmodel.codeplex.com

159
Appendix B
Debugging and Profiling 
Parallel Applications
The Microsoft® Visual Studio® 2010 development system debugger 
includes two windows that assist with parallel programming: the Par-
allel Stacks window and the Parallel Tasks window. In addition, the 
Premium and Ultimate editions of Visual Studio 2010 include a profil-
ing tool. This appendix gives examples of how to use these windows 
and the profiler to visualize the execution of a parallel program and to 
confirm that it’s working as you expect. After you gain some experi-
ence at this, you’ll be able to use these tools to help identify and fix 
problems.
The Parallel Tasks and Parallel Stacks Windows
In Visual Studio, open the parallel guide samples solution. Set the 
ImagePipeline project from Chapter 7, “Pipelines,” to be the startup 
project. Open ImagePipeline.cs and find the LoadPipelinedImages 
method. This is the method executed by the first task in the pipeline. 
Insert a breakpoint at the first statement in the body of the foreach 
loop. This is the loop that reads images from disk and fills the pipeline 
as it iterates over the images.
Start the debugging process. You can either press F5 or click Start 
Debugging on the Debug menu. The ImagePipeline sample begins to 
run and opens its graphical user interface window on the desktop. 
Select the Pipelined option, and then click Start. When execution 
reaches the breakpoint, all tasks stop and the familiar Call Stack win-
dow appears. On the Debug menu, point to Windows, and then click 
Parallel Tasks.
When execution first reaches the breakpoint, the Parallel Tasks 
window shows that the first pipeline task, LoadPipelinedImages, is 
running and all the other tasks are waiting. This is because there are 
no images in the pipeline yet. Pressing F5 several times (or clicking 
Continue on the Debug menu) causes several images to be loaded, so 
the pipeline starts to fill and other tasks can run. This is shown in 

160
appendix b
Figure 1. Recall that each task runs in a thread. The Parallel Tasks 
window shows the assignment of tasks to threads. More than one 
task can run in a thread when task inlining occurs, so it’s possible that 
a thread may be running when one of its tasks is blocked.
figure 1
The Parallel Tasks window
On the Debug menu, point to Windows, and then click Parallel 
Stacks. In the Parallel Stacks window, from the drop-down menu in 
the upper-left corner, click Tasks, and then right-click the background 
of the Parallel Tasks window and click Show External Code. (You may 
first need to disable the Enable Just My Code (Managed only) op-
tion. To locate this option, click the Tools menu, click Options, and 
click Debugging. This window shows the stack for each of the tasks.) 
In Figure 2, the window contents have been enlarged (point to the 
zoom control in the left side of the window and use the slider) so only 
two stacks appear, but all stacks can be accessed.
figure 2
The Parallel Stacks window
As you continue to press F5, the contents of each window changes as 
the buffers between pipeline stages empty and fill. This is the ex-
pected behavior, so no bugs are indicated. These windows can also 
reveal unexpected behavior that can help you identify and fix perfor-

 161
debugging and profiling parallel applications
mance problems and synchronization errors. For example, the Parallel 
Tasks and Parallel Stacks windows can help to identify common con-
currency problems such as deadlocks. The following code shows this 
problem.
static void Deadlock()
{
  object obj1 = new object();
  object obj2 = new object();
  Parallel.Invoke(
    () => {
      for (int i = 0; ; i++) {
        lock (obj1) {
          Console.WriteLine(“Got 1 at {0}”, i);
          lock (obj2) Console.WriteLine(“Got 2 at {0}”, i);
        }
      }
    },
    () => {
      for (int i = 0; ; i++) {
        lock (obj2) {
          Console.WriteLine(“Got 2 at {0}”, i);
          lock (obj1) Console.WriteLine(“Got 1 at {0}”, i);
        }
      }
    });
}
This code is a classic example of a deadlock. Two tasks each at-
tempt to acquire locks in an order that leads to a cycle. This eventu-
ally results in deadlock. At some point while running this code, the 
application will stop making progress (this will be obvious because 
there will be no more new console output). At that point, if you use 
the Break All option on the Debug menu, and open the Parallel Tasks 
window, you’ll see something like what’s shown in Figure 3.
figure 3
Parallel Tasks window showing deadlock

162
appendix b
The Concurrency Visualizer
The Visual Studio 2010 profiler includes the Concurrency Visualizer. It 
shows how parallel code uses resources as it runs: how many cores it 
uses, how threads are distributed among cores, and the activity of 
each thread. This information helps you to confirm that your parallel 
code is behaving as you intend, and it can help you diagnose perfor-
mance problems.
The Concurrency Visualizer has two stages: data collection and 
visualization. In the collection stage, you enable data collection and 
run your application. In the visualization stage, you examine the data 
you collected. This appendix uses the Concurrency Visualizer to pro-
file the ImagePipeline sample from Chapter 7 on a computer with two 
cores.
You first perform the data collection stage. To do this, you must 
run Visual Studio as an administrator because data collection uses 
kernel-level logging. Open the parallel guide samples solution in Vi-
sual Studio. There are several ways to start a data collection run. One 
way is to click Start Performance Analysis on the Visual Studio De-
bug menu. The Performance Wizard begins. Click Concurrency, and 
then select Visualize the behavior of a multithreaded application. 
The next page of the wizard shows the solution that is currently open 
in Visual Studio. Select the project you want to profile, which is Im-
agePipeline. Click Next. The last page of the wizard asks if you want 
to begin profiling after the wizard finishes. This check box is selected 
by default. Click Next. The Visual Studio profiler window appears and 
indicates it’s currently profiling. The ImagePipeline sample begins to 
run and opens its graphical user interface window on the desktop. To 
maximize processor utilitization, select the Load Balanced option, 
and then click Start. In order to collect plenty of data to visualize, let 
the Images counter (on the graphical interface) reach at least 20. Then 
click Stop Profiling in the Visual Studio profiler window.
During data collection, the performance analyzer frequently takes 
a sample of data (known as a snapshot) that records the state of your 
running parallel code. Each data collection run writes several data 
files, including a .vsp file. A single data collection run can write files 
that are hundreds of megabytes. Data collected during separate runs 
of the same program can differ because of uncontrolled factors such 
as other processes running on the same computer.
You can run the visualization stage whenever the files are avail-
able. You don’t need to be running Visual Studio as an administrator 
to do this. There are several ways to begin visualization. You can re-
quest the Performance Wizard to start visualization as soon as data 
collection finishes. Alternatively, you can simply open any .vsp file in 
Visual Studio. If you select the first option, you’ll see a summary re-

 163
debugging and profiling parallel applications
port after the data is collected and analyzed. The summary report 
shows the different views you can see. These include a Threads view, 
a CPU Utilization view, and a Cores view. 
Figure 4 shows the Threads view. Each task is executed in a thread. 
The Concurrency Visualizer shows the thread for each task (remember 
that there may be more than one task per thread because of inline 
tasks).
figure 4
Threads view of the Concurrency Visualizer
The Concurrency Visualizer screens contain many details that may not 
be clear in these figures, which are reduced in size and are not in full 
color. A version of this appendix that contains the full color screen 
shots is available on the CodePlex site at http://parallelpatterns.code-
plex.com/.

164
appendix b
Figure 5 illustrates the CPU Utilization view. The CPU Utilization 
view shows how many processors (logical cores) the entire application 
(all tasks) uses, as a function of time. On the computer used for this 
example, there are two logical cores. Other processes not related to 
the application are also shown. For each process, there’s a graph that 
shows how many processors it’s using at each point in time. To make 
the processes easier to distinguish, the area under each process’s graph 
appears in a different color (some colors may not be reproduced ac-
curately in this figure). Some data points show a fraction, not 0, 1, or 
2, because each point represents an average calculated over the sam-
pling interval.
During this particular data collection run, the graphical user inter-
face showed that the first few images appeared slowly, and the re-
maining images appeared more rapidly. (This behavior didn’t occur on 
every run). The view reflects the behavior. Early in the run (before 
about 20,000 on the time scale) the application runs in bursts, and fills 
the pipeline as it loads images. When there are several images in the 
pipeline (after about 20,000), pipeline tasks can run in parallel and the 
application uses two logical cores. This view also shows intervals be-
tween bursts (before 20,000) where the application gets no proces-
sors. During these intervals the application is blocked or is preempted 
by other processes.
figure 5
Detail of CPU Utilization view

 165
debugging and profiling parallel applications
Figure 6 illustrates the Cores view. The Cores view shows how the 
application uses the available cores. There is a timeline for each core, 
with a color-coded band that indicates when each thread is running 
(a different color indicates each thread.) Between 10 and 22 on the 
time scale, the application runs in bursts and the empty intervals indi-
cate when there was no work from this process running on either 
core. Between 22 and 28 the pipeline is filled and more tasks are eli-
gible to run than there are cores. Several threads alternate on each 
core and the table beneath the graph shows that there is a great deal 
of context switching.
figure 6
Detail of Cores view
Figure 7 illustrates the Threads view. The Threads view shows how 
each thread spends its time. There is a timeline with color-coded 
bands that indicate different kinds of activity. For example, red indi-
cates when the thread is synchronizing (waiting for something). This 
view initially shows idle threads in the thread pool. (You can hide 
them by right-clicking the view and then clicking Hide). This view 
shows that the main thread is active throughout; the green color indi-
cates user interface activity.

166
appendix b
figure 7
Detail of Threads view
The pipeline threads execute in bursts before time 22, alternating 
between running and synchronizing, as they wait for the pipeline to 
fill. After 22, some pipeline threads execute frequently and others 
execute almost continuously. There are more pipeline threads than 
cores, so some pipeline threads must alternate between running and 
being preempted.
You can use the Scenario library to mark different phases of com-
plex applications. The following code shows an example. (The Sce-
nario library is a free download on the MSDN® Code Gallery website. 
For more information, see “Scenario Marker Support” on MSDN 
[http://msdn.microsoft.com/en-us/library/dd984115.aspx].)
Scenario.Scenario myScenario = new Scenario.Scenario();
myScenario.Begin(0, “Main Calculation”);
// Main calculation phase...
myScenario.End(0, “Main Calculation”);
These markers will be displayed in the Threads view and CPU Utiliza-
tion view. Don’t use too many markers as they can easily swamp the 

 167
debugging and profiling parallel applications
visualization and make it hard to read. The tool may hide some mark-
ers to improve visibility. You can use the zoom feature to increase the 
magnification and see the hidden markers for a specific section of the 
view.
Visual Patterns
The patterns discussed in this book primarily focus on ways to express 
potential parallelism. However, there are other kinds of patterns that 
are useful in parallel development. The human mind is very good at 
recognizing visual patterns, and the Concurrency Visualizer takes ad-
vantage of this. You can learn to identify some common visual pat-
terns that occur when an application has specific performance prob-
lems. This section describes visual patterns that will help you to 
recognize and fix oversubscription, lock contention, and load imbal-
ances.
oversubscription
Oversubscription occurs when there are more threads than logical 
processors to run them. Oversubscription can cause poor perfor-
mance because of the high number of context switches, each of which 
takes some processing time and which can decrease the benefits pro-
vided by memory caches.
The Concurrency Visualizer makes it easy to recognize oversub-
scription because it causes there to be large numbers of yellow regions 
in the profiler trace. Yellow means that a thread was preempted (the 
thread was switched out). When traced, the following code yields a 
quintessential depiction of oversubscription.
static void Oversubscription()
{
    for (int i = 0; i < (Environment.ProcessorCount * 4); i++)
    {
        new Thread(() => {
            // Do work 
            for (int j = 0; j < 1000000000; j++) ;
        }).Start();
    }
}
Figure 8 illustrates the Threads view from one run of this function on 
a quad-core system. It produces a very distinct pattern.

168
appendix b
figure 8
Threads view that shows oversubscription
lock contention and serialization
Contention occurs when a thread attempts to acquire a lock that is 
held by another thread. In many cases, this results in the first thread 
blocking until the lock is released. The Threads view of the Concur-
rency Visualizer depicts blocking in red. It is often a sign of decreased 
performance. In extreme cases, an application can be fully serialized 
by one or more locks, even though multiple threads are being used.
The following method produces a lock convoy, which leads to 
significant lock contention and serialization of the program even 
though multiple threads are in use. A lock convoy is a performance 
problem that occurs when multiple threads contend for a frequently 
shared resource.
static void LockContention()
{
    object syncObj = new object();
    for (int p = 0; p < Environment.ProcessorCount; p++)
    {
        new Thread(() => {
            for(int i=0; i<50; i++)

 169
debugging and profiling parallel applications
            {
                // Do work
                for (int j = 0; j < 1000; j++);
                // Do protected work
                lock (syncObj)
                    for (int j = 0; j < 100000000; j++);
            }
        }).Start();
    }
}
Figure 9 illustrates the pattern this code produced in the Threads view 
of the Concurrency Visualizer.
figure 9
Threads view showing lock convoy
load imbalance
A load imbalance occurs when work is unevenly distributed across all 
the worker threads that are involved in a parallel operation. Load im-
balances mean that the system is underutilized, because some threads 
or cores sit idly while others finish processing the operation. The vi-
sual pattern produced by a load imbalance is recognizable in several 
of the Concurrency Visualizer views. The following code is designed 
to create a load imbalance.

170
appendix b
static void LoadImbalance()
{
  const int loadFactor = 10;
  ParallelEnumerable.Range(0, 100000).ForAll(i =>
  {
    for (int j = 0; j < (i * loadFactor); j++) ;
  });
}
Although most of the parallelism support in .NET Framework 4 uses 
dynamic partitioning to apportion work between worker tasks, the 
ParallelEnumerable.Range method from PLINQ uses static partition-
ing. This example, on a system with eight logical cores, causes ele-
ments [0, 12499] to be processed by one task, and elements [12500, 
24999] to be processed by another task, and so on. The body of the 
workload simply iterates from 0 to the current index value, which 
means that the amount of work to be done is proportional to the in-
dex. Workers that process lower ranges will have significantly less 
work to do than the workers that process the upper ranges. Figure 10, 
which is the CPU Utilization view in the Concurrency Visualizer, il-
lustrates this.

 171
debugging and profiling parallel applications
figure 10
CPU view that shows a load imbalance
When the method begins to execute, all eight logical cores on the 
system are being used. However, after a period of time, usage drops as 
each core completes its work. This yields a “stair-step pattern,” as 
threads are dropped after they complete their portion of the work. 
The Threads view confirms this analysis. Figure 11 illustrates this.

172
appendix b
figure 11
Threads view that shows a load imbalance
The Threads view shows that after completing a portion of the work, 
the worker threads were idle while they waited for CLR Worker 
Thread 6340 to complete the remaining work.
Further Reading
The Parallel Performance Analysis blog at MSDN discusses many tech-
niques and examples. MSDN also provides information about the 
Scenario library.
Parallel Performance Analysis in Visual Studio 2010 blog on 
MSDN:  
http://blogs.msdn.com/b/visualizeparallel/
“Performance Tuning with the Concurrency Visualizer in Visual 
Studio 2010” on MSDN: 
http://msdn.microsoft.com/en-us/magazine/ee336027.aspx
Scenario Home Page on MSDN: 
http://code.msdn.microsoft.com/scenario
	

173
Technology Overview
Appendix C
Tools
Concurrency
Visualizer
Parallel
Debugger
Windows
CHESS
Managed Languages
Visual F#
Axum
Managed Libraries
DryadLINQ
Parallel LINQ
Task Parallel Library
Rx
Data Structures
Data Structures
Managed Concurrency Runtime
ThreadPool
Native Libraries
Parallel
Pattern
Library
Async
Agents
Library
Native Concurrency Runtime
Task Scheduler
Resource Manager
Operating 
System
HPC Server
Threads
UMS Threads
Key:
Research/ Incubation
Visual Studio 2010/ .NET 4
Windows 7/ Server 2008 R2
Appendix C describes some of the parallel computing resources  
offered by Microsoft® that are not covered in this guide. The 
“Further Reading” section includes URLs for websites that have more 
information. Figure 1 illustrates the different offerings and how they 
are related. (Note that a UMS thread is a user-mode scheduling thread. 
This is a mechanism that applications can use to schedule their own 
threads.)
figure 1
Microsoft parallel programming resources

174
appendix c
This guide covers the Task Parallel Library (TPL) and Parallel LINQ 
(PLINQ), which are managed libraries that ship as part of the Micro-
soft .NET Framework 4. They rely on the .NET ThreadPool class and 
the concurrent data structures of .NET Framework 4.
The F# language also ships with the Microsoft Visual Studio® 
2010 development system. It exposes a more functional approach to 
parallelism than do TPL or PLINQ, and it emphasizes immutable data 
types. However, F#’s runtime libraries build on and integrate with TPL, 
and the F# PowerPack includes parallelization support that is built on 
top of PLINQ. The samples in this guide have been ported to F#.
For Microsoft Visual C++® development system developers, there 
are the Parallel Pattern and Asynchronous Agents libraries. These use 
the native Concurrency Runtime, which includes a powerful sched-
uler and resource manager that are designed to efficiently execute 
native parallel workloads on multicore architectures. The Visual C++ 
version of this guide will be available soon.
Visual Studio 2010 contains several tools for debugging and pro-
filing parallel applications. For examples of how to use them, see Ap-
pendix B, “Debugging and Profiling Parallel Applications.” You can also 
use the CHESS tools from Microsoft Research to detect bugs in your 
parallel code.
In addition to the technologies that ship with Visual Studio 2010 
and the .NET Framework 4, there are other interesting research proj-
ects that focus on parallelism and concurrency. They include the 
Axum language, the Reactive Extensions for .NET (Rx), and Dryad-
LINQ. Axum is a concurrent programming language built on the prin-
ciples of isolation, actors, and message-passing to increase application 
safety, responsiveness, scalability, and developer productivity. Rx is a 
library for composing asynchronous and event-based programs using 
observable collections. DryadLINQ is a programming environment for 
writing large-scale data parallel applications that run on large PC clus-
ters. Accelerator (not shown in Figure 1) is a library that uses a data 
parallelism programming model and targets both graphics processing 
unit GPUs and multicore CPUs.
Axum, Rx, Accelerator, and DryadLINQ are incubation or research 
projects and Microsoft has made no commitment to ship them. How-
ever, they contain many new ideas and approaches that will interest 
anyone who has read this far in the book. You’re encouraged to down-
load them, evaluate them, and provide the respective teams with 
feedback.
All of the above technologies, with the exception of Axum and 
DryadLINQ, are largely for parallelism on a single computer. Windows 
High Performance Computing (HPC) Server 2008 targets clusters of 
servers and supports scale out across many computers. Although the 

 175
technology overview
technologies are very different for clustered computing, some of the 
fundamental patterns discussed in this book, such as Parallel Tasks and 
Parallel Aggregation, are still applicable.
Further Reading
The MSDN Parallel Computing Developer Center covers parallel  
development on both the managed and native concurrency 
runtimes, as well as the Visual Studio 2010 tools that support 
writing parallel programs:  
http://msdn.microsoft.com/concurrency
For information about F# including the language reference and 
walkthroughs, see the Microsoft F# Developer Center: 
http://msdn.microsoft.com/fsharp 
F# versions of the samples in this guide are on CodePlex: 
http://parallelpatterns.codeplex.com/
Windows HPC Server 2008 product information and developer 
resources are available on the Windows HPC Server site: 
http://www.microsoft.com/hpc
Details of the Microsoft Research projects Accelerator and 
DryadLINQ are available: 
http://research.microsoft.com/accelerator 
http://research.microsoft.com/en-us/projects/DryadLINQ
Descriptions and downloads for Rx, Axum, and CHESS, which 
are all part of the Microsoft DevLabs project, are available: 
http://msdn.microsoft.com/devlabs


177
Glossary
aggregate exception. An exception that contains one or more inner 
exceptions, instead of only one inner exception.
aggregation. To combine multiple data items into a single result.
alpha blending. Merging different images into a single image by 
superimposing them in semitransparent layers.
antecedent task. A task whose completion starts a continuation 
task. An antecedent is often a future whose result value is used as 
input to the continuation task.
asynchronous. An operation that that does not block the current 
thread of control when the operation starts. 
background thread. A thread that stops when a process shuts 
down. A running background thread does not keep a process 
running. Threads in the thread pool are background threads. Con-
trast to foreground thread.
barrier. A synchronization point where all participating threads must 
stop and wait until every thread reaches it. Supported in .NET by 
the System.Threading.Barrier class.
block. To pause execution while waiting for some event or condi-
tion.
blocking collection. A collection where removal attempts block 
until data is available to be removed, and additional attempts are 
blocked until space is available. Supported in .NET by the System.
Collections.Concurrent.BlockingCollection<T> class.
captured variable. A variable defined outside a lambda expression 
that is used in the lambda expression. The lambda expression can 
update the captured variable.

178
glossary
cluster. A parallel computing system composed of multiple comput-
ers connected by a network, not multiple cores in a single physical 
processor.
closure. A lambda expression that captures variables.
concurrency. Programming with multiple activities at the same time. 
Concurrency enables programs to respond promptly to external 
stimuli; its goal is to reduce latency. Concurrency can be imple-
mented with asynchronous operations or with threads, where it is 
expected that threads will take turns executing on processors. 
Compare to parallelism.
Concurrency Visualizer. An addition to the Visual Studio profiler 
that collects and displays information about the execution and 
performance of parallel programs.
context switch. When one thread stops executing on a processor 
and a different thread resumes. Excessive context switching can 
occur when processors are oversubscribed and can result in poor 
performance.
continuation task. A task that automatically starts when other 
tasks, known as its antecedents, complete.
control flow. A basis for coordination where tasks execute accord-
ing to the steps of an algorithm, as in a parallel loop.
coordination. Arranging for tasks to work together to ensure a 
correct outcome. Coordination can be based on data flow or control 
flow.
core. The part of a physical processor that executes instructions. 
Most recent physical processor models have more than one core, so 
they can execute tasks in parallel.
coroutine. A generalized subroutine (method) that supports mul-
tiple entry points and can return more than once, yielding different 
return values. In C#, iterators are often implemented as coroutines 
using the yield keyword.
data flow. A basis for coordination where tasks execute when data 
becomes available, as in a pipeline or task graph.
data parallelism. A form of parallel processing where the same 
computation executes in parallel on different data. Data parallelism 
is supported in the Microsoft .NET Framework by the Parallel.For 
and Parallel.ForEach methods and by PLINQ. Compare to task 
parallelism.

 179
data partitioning. Dividing a collection of data into parts, in order 
to use data parallelism.
data race. When more than one concurrent thread reads and writes 
data without synchronization.
deadlock. When execution stops and cannot resume because the 
system is waiting for a condition that cannot occur. Threads can 
deadlock when one holds resources that another needs.
decomposition. To break a problem into smaller parts. For parallel 
processing, decomposition can be by data or by task.
degree of parallelism. The number of parallel tasks that may 
execute concurrently at any one time. The degree of parallelism can 
be set in PLINQ by the WithDegreeOfParallelism method and can 
be limited in parallel loops by the MaxDegreeOfParallelism option.
dependency. When one operation uses the results of another. When 
there is a dependency between operations, they cannot run in 
parallel. Contrast to independent.
dependency injection (DI) container. A framework or library for 
building graphs of objects by passing in (injecting) each object’s 
dependencies. Object lifetimes are handled by the container instead 
of by the consuming object.
double-checked locking. First testing a condition, then, only if the 
condition is true, acquiring a lock and testing the same condition 
again, this time to determine whether to update shared data. This 
maneuver can often avoid the expensive operation of acquiring a 
lock when it will not be useful.
dynamic partitioning. Data partitioning where the parts are se-
lected as the parallel tasks execute. Contrast to static partitioning.
foreground thread. A thread that keeps a process running. After all 
its foreground threads stop, the process shuts down. Contrast to 
background thread.
fork/join. A parallel computing pattern that uses task parallelism. 
Fork occurs when tasks start; join occurs when all tasks finish.
future. A task that returns a value. In the .NET Framework, futures 
are implemented with the Task<TResult> class.
granularity. The quantity of data in a partition or work in a task. 
Equivalently, the number of data partitions or tasks. A coarse level 
of granularity has a few large partitions or tasks; a fine level of 
granularity has many small partitions or tasks.

180
glossary
hardware thread. An execution pipeline on a core. Simultaneous 
multithreading (also sometimes known as hyperthreading) enables 
more than one hardware thread to execute on a single core. Each 
hardware thread is considered a separate logical processor.
immutable. Data that cannot be modified after it’s created. 
For example, .NET strings are immutable. Compare to mutable.
immutable type. A type whose instances are immutable. Its 
instances are purely functional data structures.
independent. When one operation does not use the results of 
another. Independent operations can execute in parallel. Contrast 
with dependency.
inner exception. An exception thrown and wrapped in another 
exception.
lambda expression. An anonymous function that can contain 
expressions or statements.
lazy initialization. Where data is not initialized until it is needed. 
Lazy initialization is supported in .NET by the Lazy<T> and Lazy
Initalizer classes.
livelock. When execution continues but does not make progress 
toward its goal.
load balancing. When similar amounts of work are assigned to 
different tasks, so that the available processors are used efficiently. 
Compare to load imbalance.
load imbalance. When different amounts of work are assigned to 
different tasks, so that some tasks don’t have enough work to do, 
and the available processors are not used efficiently. Compare to 
load balancing.
lock. A synchronization mechanism that ensures that only one 
thread can execute a particular section of code at a time.
lock convoy. When multiple tasks contend repeatedly for the 
same lock. Frequent failures to acquire the lock can result in poor 
performance.
logical processor. The processor associated with a single hardware 
thread. In .NET, System.Environment.ProcessorCount returns the 
number of logical processors. Compare to physical processor.
manycore. Multicore, usually with more than eight logical proces-
sors.

 181
map. A parallel computation where multiple tasks independently 
perform the same transformation on different data. An example of 
data parallelism.
map/reduce. A parallel programming pattern where a data parallel 
phase (map) is followed by an aggregation phase (reduce).
memory barrier. A machine instruction that enforces an ordering 
constraint on memory operations. Memory operations that precede 
the barrier are guaranteed to occur before operations that follow 
the barrier.
multicore. Having more than one core, able to execute parallel tasks. 
Most recent physical processor models are multicore.
multiplicity. The number of times an element occurs in a multiset.
multiset. An unordered collection that may contain duplicates. 
Each element in the collection is associated with a multiplicity (or 
count) that indicates how many times it occurs. Contrast to set.
multiset union. An operation that combines multisets by merging 
their elements and adding the multiplicities of each element.
mutable. Data that can be modified after it is created. For example, 
.NET arrays are mutable. Compare to immutable.
mutable type. A type whose instances are mutable.
nested parallelism. When one parallel programming construct 
appears within another. In .NET, when you use a Parallel.For loop 
within another Parallel.For loop, they coordinate with each other 
to share threading resources.
node. A computer in a cluster.
nonblocking algorithm. An algorithm that allows multiple tasks 
to make progress on a problem without ever blocking each other. 
object graph. A data structure consisting of objects that reference 
each other. Object graphs are often shared mutable data that can 
complicate parallel programming.
overlapped I/O. I/O operations that proceed (or wait) while other 
tasks are executing.
oversubscription. When there are more threads than processors 
available to run them. Oversubscription can result in poor perfor-
mance because time is spent context switching.

182
glossary
parallelism. Programming with multiple threads, where it is 
expected that threads will execute at the same time on multiple 
processors. Its goal is to increase throughput. Compare to  
concurrency.
partitioning. Dividing data into parts in order to use data parallel-
ism.
physical processor. A processor chip, also known as a package or 
socket. Most recent physical processor models have more than one 
core and more than one logical processor per core.
pipeline. A series of producer/consumers, where each one consumes 
the output produced by its predecessor.
priority inversion. When a lower priority thread runs while a higher 
priority thread waits. This can occur when the lower priority thread 
holds a resource that the higher-priority thread requires.
process. A running application. Processes can run in parallel and are 
isolated from one another (they usually do not share data). A process 
can include several (or many) threads or tasks. In .NET, processes can 
be monitored and controlled with the System.Diagnostics.Process 
class. Compare to thread, task.
profiler. A tool that collects and displays information for perfor-
mance analysis. The Concurrency Visualizer is a profiler for concur-
rent and parallel programs.
pure function. A function (or method or operation) that has no side 
effects (does not update any data nor produce any output) but 
returns only a value.
purely functional data structure. A data structure that can only 
be accessed by pure functions. An instance of an immutable type.
race. When the outcome of a computation depends on which 
statement executes first, but the order of execution is not con-
trolled or synchronized.
race condition. A condition where a race occurs. Race conditions 
are usually errors; good programming practices prevent them.
recursive decomposition. In parallel programming, where the tasks 
themselves can start more tasks.
reduce. A kind of aggregation where data is combined by an opera-
tion that is associative, which often makes it possible to perform 
much of the reduction in parallel.

 183
round robin. A scheduling algorithm where each thread is given its 
turn to run in a fixed order in a repeating cycle, so that during each 
cycle, each thread runs once.
scalable. A parallel computation whose performance increases when 
more processors are available.
semaphore. A synchronization mechanism that ensures not more 
than a specified number of threads can execute a particular section 
of code at a time. Compare to lock.
serialize. To run in sequence, not in parallel.
set. An unordered collection without duplicates. Compare to 
multiset.
shared data. Data used by more than one thread. Access to mutable 
shared data requires synchronization.
single-threaded data type. A type that is not thread-safe. It cannot 
be accessed by multiple threads unless there is additional synchroni-
zation in user code.
simultaneous multithreading (SMT). A technique for executing 
multiple threads on a single core.
socket. Physical processor.
speculative execution. To execute tasks even though their results 
may not be needed.
static partitioning. Data partitioning where the parts are selected 
before the program executes. Contrast to dynamic partitioning.
synchronization. Coordinating the actions of threads to ensure a 
correct outcome. A lock is an example of a synchronization mecha-
nism.
task. A parallelizable unit of work. A task executes in a thread but is 
not the same as a thread; it is at a higher level of abstraction. Tasks 
are recommended for parallel programming in .NET with the Task 
Parallel Library in the System.Threading.Tasks namespace. Compare 
to thread, process.
task graph. When tasks provide results that are the inputs to other 
tasks, this can be seen as a directed graph. The nodes are tasks, and 
the arcs are values that act as inputs and outputs of the tasks.
task inlining. When more than one task executes in a single thread 
concurrently due to one task requesting that the other task run 
synchronously at the current point of execution.

184
glossary
task parallelism. A form of parallel processing where different 
computations execute in parallel on different data. Task parallelism 
is supported in .NET by the Parallel.Invoke and Task.Factory.
StartNew methods. Compare to data parallelism.
thread. An executing sequence of statements. Several (or many) 
threads can run within a single process. Threads are not isolated;  
all the threads in a process share data. A thread can run a task but is 
not the same as a task; it is at a lower level of abstraction. Threads 
are supported in .NET by the System.Threading namespace. Com-
pare to process, task.
thread affinity. When certain operations must only be performed 
by a particular thread. For example, a Windows Presentation 
Foundation (WPF) control must be accessed from the same thread 
that created it.
thread injection. To add threads to the thread pool.
thread pool. A collection of threads managed by .NET to avoid the 
overhead of creating and disposing threads. Tasks are usually run by 
threads in the thread pool.
thread starvation. When tasks are unable to run because there are 
no threads available to run them in the thread pool.
thread-local state. Variables that are accessed by only one thread. 
No locking or other synchronization is needed to safely access 
thread-local state. Thread-local state is supported in .NET by the 
ThreadStatic attribute and the ThreadLocal class.
thread-safe. A type that can be used concurrently by multiple 
threads, without requiring additional synchronization in user code.  
A thread-safe type ensures that its data can be accessed by only  
one thread at a time, and its operations are atomic with respect to 
multiple threads. The .NET System.Collections.Concurrent 
namespace provides several thread-safe collection types. Compare 
to single-threaded data type.
torn read. When reading a variable requires more than one machine 
instruction, and another task writes to the variable between the 
read instructions.
torn write. When writing a variable requires more than one machine 
instruction, and another task reads the variable between the write 
instructions.
tuple. An unnamed record containing an ordered list of elements. 
Tuples are supported in .NET by the Tuple classes.

 185
two-step dance. To signal an event while holding a lock, when the 
waking thread needs to acquire that lock. It will wake only to find 
that it must wait again. This can cause context switching and poor 
performance.
undersubscription. When there are fewer tasks than there are 
processors available to run them, so processors remain idle.
virtual core. Logical processor.
volatile. A keyword that tells the C# compiler that a field can be 
modified by multiple threads, the operating system, or other hard-
ware.
work stealing. When a thread executes a task queued for another 
thread in the thread pool, in order to remain busy.
XAML. A declarative markup language used in the .NET Framework 
programming model to specify the user interface (UI) for .NET 
applications.


187
References
J. Albahari and B. Albahari. C# 4 in a Nutshell. O’Reilly, fourth edition, 
2010.
J. Bishop. C# 3.0 Design Patterns. O’Reilly, 2008.
J. Boodhoo. “Model View Presenter.” 
http://msdn.microsoft.com/en-us/magazine/cc188690.aspx
C. Campbell, M. Veanes, J. Huo, and A. Petrenko. “Multiplexing of 
Partially Ordered Events,” in TestCom 2005, Springer Verlag, June 
2005.  
http://research.microsoft.com/apps/pubs/default.aspx?id=77808
J. Dean and S. Ghemawat. “MapReduce: simplified data processing 
on large clusters.” In OSDI ‘04: Sixth Symposium on Operating System 
Design and Implementation, pages 137–150, 2004. 
J. Duffy. Concurrent Programming on Windows. Addison-Wesley, 
2008.
J. Duffy. “Immutable types for C#.” 
http://www.bluebytesoftware.com/blog/2007/11/11/Immutable-
TypesForC.aspx
M. Fowler. Patterns of Enterprise Application Architecture. Addison 
Wesley, 2003.
E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: 
Elements of Reusable Object-Oriented Software. Addison-Wesley, 
1995.
J. E. Hoag. “A Tour of Various TPL Options.” April 2009.  
http://blogs.msdn.com/b/pfxteam/archive/2010/04/19/9997552.
aspx

188
references
J. Jacky, M. Veanes, C. Campbell, and W. Schulte. Model-Based 
Software Testing and Analysis with C#. Cambridge University Press, 
2008.
D. Leijen, W. Schulte, and S. Burckhardt. “The design of a task 
parallel library.” In S. Arora and G.T. Leavens, editors, OOPSLA 2009: 
Proceedings of the 24th Annual ACM SIGPLAN Conference on Object-
Oriented Programming, Systems, Languages, and Applications, pages 
227–242. ACM, 2009.
E. Lippert. “Fabulous Adventures in Coding.”  
http://blogs.msdn.com/b/ericlippert/archive/tags/immutability/
M. McCool. “Structured Patterns for Parallel Computation,” Decem-
ber 2009.  
http://www.ddj.com/go-parallel/article/showArticle.
jhtml?articleID=223101515
T. G. Mattson, B. A. Sanders, and B. L. Massingill. Patterns for Parallel 
Programming. Addison-Wesley, 2004.
C. Okasaki. Purely Functional Data Structures. Cambridge University 
Press, 1998.
J. Smith. “WPF Apps With the Model-View-ViewModel Design 
Pattern.”  
http://msdn.microsoft.com/en-us/magazine/dd419663.aspx
S. Toub. “Patterns of Parallel Programming: Understanding and 
Applying Parallel Patterns with the .NET Framework 4 and C#,” 
2009.  
http://www.microsoft.com/downloads/details.
aspx?FamilyID=86b3d32b-ad26-4bb8-a3ae-
c1637026c3ee&displaylang=en
S. Toub. “Custom Parallel Partitioning With .NET 4,” April 26, 2010.  
http://www.drdobbs.com/visualstudio/224600406
S. Wildermuth. “WPF Threads: Build More Responsive Apps With 
The Dispatcher.”  
http://msdn.microsoft.com/en-us/magazine/cc163328.aspx

 189
Other Online Sources
Accelerator: 
http://research.microsoft.com/en-us/projects/accelerator/
“The ADO.NET Entity Framework Overview”: 
http://msdn.microsoft.com/en-us/library/aa697427(VS.80).aspx
“Best Practices for using ADO.NET”: 
http://msdn.microsoft.com/en-us/library/ms971481.aspx
DevLabs Portal: 
http://msdn.microsoft.com/en-us/devlabs/default.aspx
DryadLINQ: 
http://research.microsoft.com/en-us/projects/DryadLINQ/
“Parallel Programming with Microsoft .NET”: 
http://parallelpatterns.codeplex.com/
“Guidelines for Overriding Equals() and Operator == (C# Program-
ming Guide)”: 
http://msdn.microsoft.com/en-us/library/ms173147(VS.90).aspx
“How to: Create a Task Scheduler That Limits the Degree of Con-
currency”: 
http://msdn.microsoft.com/en-us/library/ee789351.aspx
“Improving ADO.NET Performance”: 
http://msdn.microsoft.com/en-us/library/ff647768.aspx
“Lazy Initialization”: 
http://msdn.microsoft.com/en-us/library/dd997286.aspx
Microsoft F# Developer Center: 
http://msdn.microsoft.com/en-us/fsharp/default.aspx
NModel software: 
http://nmodel.codeplex.com/
“Our Pattern Language for Parallel Programming Ver 2.0”: 
http://parlab.eecs.berkeley.edu/wiki/patterns
Parallel Computing: 
http://msdn.microsoft.com/en-us/concurrency/default.aspx

190
references
Parallel Performance Analysis in Visual Studio 2010: 
http://blogs.msdn.com/b/visualizeparallel/
Samples for Parallel Programming with the .NET Framework 4: 
http://code.msdn.microsoft.com/ParExtSamples
Windows Azure Guidance 
http://wag.codeplex.com/
“Performance Tuning with the Concurrency Visualizer in Visual 
Studio 2010”: 
http://msdn.microsoft.com/en-us/magazine/ee336027.aspx
Scenario Home Page: 
http://code.msdn.microsoft.com/scenario
“Standard Query Operators Overview”: 
http://msdn.microsoft.com/en-us/library/bb397896.aspx
“TaskScheduler Events”: 
http://msdn.microsoft.com/en-us/library/system.threading.tasks.
taskscheduler_events.aspx
Unity Application Block: 
http://msdn.microsoft.com/en-us/library/dd203101.aspx
Windows HPC Server 2008: 
http://www.microsoft.com/hpc/en/us/default.aspx

191
Index
A
.NET asynchronous calls with futures, 
97–98
Adapter pattern, 136–138
adapting object-oriented patterns, 
133–158
Adapter pattern, 136–138
BlockingCollection<T> data 
structure, 153
ConcurrentBag<T> data structure, 
153
ConcurrentDictionary<T> data 
structure, 153
ConcurrentQueue<T> data 
structure, 153
ConcurrentStack<T> data 
structure, 153
dashboard’s user interface, 
144–147
Decorator pattern, 134–136
Façade pattern, 134
further reading, 156–158
futures and continuations, 147
IEnumerable<T>, 155–156
IList<T> type, 155–156
immutable types, 148–152, 158
iterators, 154–155
lazy<t> class, 140–142
lists and enumerables, 155–156
Model-View-Presenter (MVP) 
pattern, 142–147, 157
Model-View-ViewModel (MVVM) 
pattern, 142–147
purely functional data structures, 
148
repositories and parallel data 
access, 138–139
shared data classes, 152–156
singletons and service locators, 
139–142
structural patterns, 156–157
value types, 150–152
adaptive concurrency, 34–35
adaptive partitioning, 34
Adatum Dashboard tasks, 90–91
Adatum Financial Dashboard example, 
89–97
AddButtonContinuations method, 
145–147
AggregateException instance, 45
Aggregate method, 77–79
alpha blending, 41
Parallel Task pattern, 41–42
Amdahl’s law, 8–10
analysis and model creation, 96
AnalysisEngine class, 92–94
AnalysisTasks object, 144
antecedents, 97
anti-patterns
Parallel Loop pattern, 32–34
Parallel Task pattern, 51–53
Pipelines pattern, 129–130
appendices, 133–175
appendices see adapting object-
oriented patterns; debugging and 
profiling parallel applications; 
technology overview
asynchronous pipelines, 129
AttachedToParent option, 108–109
audience, xiii

192
index
B
BlockingCollection class, 113–117,  
126, 130
BlockingCollection<T> data structure, 
153
bottlenecks, 98
Break method, 21–23
C
CancellationTokenSource class
Futures pattern, 97
Parallel Loops pattern, 24–26
Parallel Task pattern, 43–44
Pipelines pattern, 124–126
closures, 51–52
Concurrency Visualizer, xiv, 162–167
concurrency vs. parallelism, 78
ConcurrentBag<T> data structure, 153
ConcurrentDictionary<T> data 
structure, 153
ConcurrentQueue<T> data structure, 
153
ConcurrentStack<T> data structure, 153
continuations and futures, 147
continuation tasks, 85–86, 88–89
ContinueWhenAny method, 97
contributors and reviewers, xixxx
coordination, 3
copyright, iv
custom task scheduler, 31, 54–55
D
dashboard’s user interface, 144–147
data classes, 152–156
debugging and profiling parallel 
applications, 159–172
Concurrency Visualizer, 162–167
load imbalance, 169–172
lock contention and serialization, 
168–169
oversubscription, 167–168
parallel tasks and parallel stacks 
windows, 159–161
visual patterns, 167–172
decomposition, 3
Decorator pattern, 134–136
degree of parallelism, 28–29
dependent loop bodies, 17–18
depthRemaining argument, 106–107
design approaches, 6
design notes
Dynamic Task Parallelism pattern, 
109
Futures pattern, 99–100
Parallel Aggregation pattern, 
80–81
Parallel Loop pattern, 34–36
Parallel Task pattern, 53–56
Pipelines pattern, 131
Dispose method, 52
DoAnalysisParallel method, 94
duplicates in the input enumeration, 34
Dynamic Task Parallelism pattern, 
103–110
AttachedToParent option, 
108–109
design notes, 109
example, 105–107
further reading, 110
Futures pattern, 100
ParallelWhileNotEmpty method, 
107–108
E
examples
Dynamic Task Parallelism pattern, 
105–107
Pipelines pattern, 117–122
exceptions, 26, 44–47
F
Façade pattern, 134
Flatten method, 47
ForAll method, 33
forward, xi–xii
further reading
adapting object-oriented 
patterns, 156–158
Dynamic Task Parallelism pattern, 
110
Futures pattern, 101
Parallel Aggregation pattern, 83
Parallel Loop pattern, 37
Pipelines pattern, 132
future defined, 85

 193
futures and continuations, 147
cancelling tasks, 97
Futures pattern, 85–101
Adatum Dashboard tasks, 90–91
Adatum Financial Dashboard 
example, 89–97
analysis and model creation, 96
AnalysisEngine class, 92–94
antecedents, 97
bottlenecks, 98
canceling futures and  
continuation tasks, 97
comparing models, 96
continuation tasks, 88–89
design notes, 99–100
Discrete Event pattern, 100
Dynamic Task Parallelism pattern, 
100
further reading, 101
future defined, 85
Loading External Data, 95
Master/Worker pattern, 100
merging, 95
modifying the graph at run time, 
98–99
normalizing, 96
Pipeline pattern, 100
processing historical data, 96
related patterns, 100
task graph, 86–88
using .NET asynchronous calls 
with futures, 97–98
variations, 97–99
view and view model, 97
G
GetConsumingEnumerable method, 130
global queue, 60
glossary, 177–185
goals of this guide, xviii
guide, xv–xvii
H
hammers and nails, xvii
Handle method, 46
hidden loop body dependencies, 32
historical data, 96
how to use this guide, xv–xvii
I
IAsyncResult interface, 97–98
IEnumerable<T>, 155–156
IImageEditor interface, 135–136
IList<T> type, 155–156
image pipelines, 119–120
with stages of equal speed, 121
with unequal stages, 122
immutable types, 148–152, 158
infinite blocking collection waits, 130
inlined execution of subtasks, 60–61
introduction, 111
iterators, 154–155
IWithFutures interface, 137–138
L
lambda expressions, 14–15
lazy<t> class, 140–142
LimitedConcurrencyLevelTask 
Scheduler class, 55
lists and enumerables, 155–156
load balancing, 126–129
load imbalancing, 169–172
Loading External Data, 95
local queue, 60
lock contention and serialization, 
168–169
lock keyword, 74–75
LongRunning option, 63, 115
M
map phase, 70, 72
Master/Worker pattern, 100
Microsoft, iv
parallel programming resources, 
173
model creation and analysis, 96
Model-View-ViewModel (MVVM) 
pattern, 142–147
more information, 11
multisets, 70–73
N
nested loops, 35
.NET asynchronous calls with futures, 
97–98
normalization, 96
Normalize method, 68

194
index
O
online sources, 189–190
out-of-scope material, xviii
oversubscription, 167–168
P
Parallel Aggregation pattern, 67–83
design notes, 80–81
further reading, 83
multisets, 70–73
Normalize method, 68
parallel loops, 73–76
Partitioner object, 76–77
PLINQ Aggregate method, 77–79
Random class, 77–79
range partitioner, 76–77
Task Parallel Library (TPL), 74
parallel class, 33–34
parallel data access, 138–139
Parallel.ForEach method, 15, 80–81
Parallel.For method, 1415, 80–81
Parallel.Invoke method, 56
parallelism
limitations, 8–10
vs. concurrency, 78
Parallel LINQ see PLINQ
Parallel Loop pattern, 13–37
adaptive concurrency, 34–35
adaptive partitioning, 34
anti-patterns, 32–34
custom task scheduler, 31
design notes, 34–36
duplicates in the input  
enumeration, 34
exception handling, 26
external loop cancellation, 24–26
further reading, 37
hidden loop body dependencies, 
32
mixing the parallel class and 
PLINQ, 33–34
nested loops, 35
Parallel.ForEach method, 15
Parallel.For method, 14–15
processor oversubscription, 33
processor undersubscription, 33
related patterns, 35
small loop bodies, 26–28
small loop bodies with few 
iterations, 32–33
step size other than one, 32
task-local state, 29–31
what to expect, 16–18
parallel loops, 114
for aggregation, 73–76
vs. pipelines, 113
parallel programming overview, 111
parallel stacks windows, 159–161
Parallel Task pattern, 39–65
AggregateException instance, 45
alpha blending, 41–42
anti-patterns, 51–53
CancellationTokenSource class, 
43–44
custom task scheduler, 54–55
decentralized scheduling 
techniques, 58–59
default task scheduler, 56–64
design notes, 53–56
Dispose method, 52
disposing a resource needed  
by a task, 52
exceptions, 44–47
Flatten method, 47
further reading, 65
global queue, 60
Handle method, 46
inlined execution of subtasks, 
60–61
local queue, 60
Parallel.Invoke method, 56
SetMaxThreads method, 63
speculative execution, 49–50
task cancelation, 43–44
Task class, 48
task creation with custom 
scheduling, 50–51
task life cycle, 53–54
tasks and threads, 53
task vs. data parallelism, 56
Thread.Abort method, 53
thread injection, 61–63
thread pool bypassing, 63
thread pools, 56–57
unobserved task exceptions, 55
variables captured by closures, 
51–52

 195
WaitAny method, 48
ways to observe an unhandled 
task exception, 45
work stealing, 59–60
parallel tasks, 159–161
ParallelWhileNotEmpty method, 
107–108
Partitioner object, 76–77
patterns, 7
pipelines
exceptions, 124–126
and streams, 129
Pipelines pattern, 100, 113–132
anti-patterns, 129–130
asynchronous pipelines, 129
BlockingCollection class, 
 114–117
CancellationTokenSource class, 
124–126
design notes, 131
example, 117–122
forgetting getconsuminge 
numerable(), 130
further reading, 132
GetConsumingEnumerable 
method, 130
image pipeline, 119–120
image pipeline with stages  
of equal speed, 121
image pipeline with unequal 
stages, 122
infinite blocking collection  
waits, 130
load balancing using multiple 
producers, 126–129
LongRunning option, 115
parallel loops, 114
parallel loops vs. pipelines, 113
performance characteristics, 
120–122
pipeline cancelation, 122–124
pipeline exceptions, 124–126
pipelines and streams, 129
related patterns, 131
sequential image processing, 
117–118
TakeFromAny method, 126–129
thread starvation, 129–130
using other producer/consumer 
collections, 130
variations, 122–129
PLINQ, xiv, 16
and parallel class, 33–34
PLINQ Aggregate method, 77–79
potential parallelism, 23
preface, xiii–xviii
prerequisites, xiv
processor oversubscription, 33
processor undersubscription, 33
producer/consumer collections, 130
purely functional data structures, 148
Q
QueuedTaskScheduler class, 55
QueueUserWorkItem method, 56
R
Random class, 29–31
Parallel Aggregation pattern, 
77–79
range partitioners, 76–77
references, 187–188
related patterns
Futures pattern, 100
Parallel Loop pattern, 35
Pipelines pattern, 131
relevence, xiv
reviewers, xix–xx
S
scalable sharing, 3, 56
sequential image processing, 117–118
serialization, 168–169
service locators, 139–142
SetMaxThreads method, 63
shared data classes, 152156
singletons and service locators, 139–142
small loop bodies
with few iterations, 32–33
Parallel Loop pattern, 26–28
sources, 189–190
speculative execution, 49–50
step size other than one, 32
StockData class, 149–150
StockDataCollection class, 91
Stop method, 23–24

196
index
streams and pipelines, 129
structural patterns, 156–157
support, 3
System.Collections.Concurrent 
namespace, 153
System.String class, 148–149
T
TakeFromAny method, 126–129
Task class, 48
task graph, 86–88
task-local state, 29–31
Task Parallel Library (TPL), 74
tasks
described, 34
vs. threads, 4, 53
task schedulers, 31, 54–55
Task<TResult> class, 85
task vs. data parallelism, 56
team, xix–xx
technology overview, 173–175
further reading, 175
Microsoft parallel programming 
resources, 173
terminology, 78
Thread.Abort method, 53
thread injection, 61–63
thread pools, 56–57
bypassing, 63
thread starvation, 8
Pipelines pattern, 129–130
threads vs. tasks, 4
tips, 10–11
top-level tasks, 60
U
UpdatePredictionsSequential method, 
19–21
V
value types, 150–152
variations
Futures pattern, 97–99
Pipelines pattern, 122–129
view and view model, 97
visual patterns, 167–172
W
WaitAny method, 48
work stealing, 59–60

